job_title,company_name,experience,salary,location,industry,job_description,skills,scraped_at
Data Engineer,Accenture India,2-5 Years,,Jaipur,"Consulting, Information Services","Job Description: Data Engineer\nProject Role: Data Engineer\nProject Role Description: Design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform, and load) processes to migrate and deploy data across systems.\nMust-Have Skills: SAP HCM On Premise ABAP\nGood-to-Have Skills: SAP HCM Organizational Management\nExperience Required: Minimum 5 years\nEducational Qualification: 15 years full-time education\nJob Summary:\nAs a Data Engineer, your main responsibility will be to design, develop, and maintain data solutions for generating, collecting, and processing data. You'll work on creating data pipelines, ensuring data quality, and implementing ETL processes to move and deploy data across systems. Your role will be crucial in managing and analyzing data to support business decisions and provide data-driven insights.\nRoles & Responsibilities:\nSubject Matter Expert (SME): Expected to be an SME, collaborate and manage the team to deliver results. Responsible for making key team decisions.\nCollaboration & Teamwork: Work closely with multiple teams and contribute to important decisions and problem-solving efforts.\nData Solutions: Design, develop, and maintain data solutions that support data generation, collection, and processing.\nData Pipelines: Create and manage data pipelines to extract, transform, and load data (ETL processes) across systems.\nData Quality & Integrity: Ensure data quality and integrity throughout the data lifecycle.\nETL Processes: Implement and optimize ETL processes to efficiently migrate and deploy data across systems and platforms.\nProfessional & Technical Skills:\nMust-Have Skills:\nStrong proficiency in SAP HCM On Premise ABAP.\nExperience with data modeling and database design.\nFamiliarity with ETL tools and processes.\nExpertise in programming languages such as ABAP, SQL, and Python.\nGood-to-Have Skills:\nExperience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nAdditional Information:\nExperience Required: A minimum of 5 years of experience in SAP HCM On Premise ABAP.\nEducational Requirement: The candidate should have a minimum of 15 years of full-time education.","['Sql', 'Python', 'Abap']",2025-06-10 19:14:08
Data Engineer,Accenture India,2-5 Years,,Jaipur,"Consulting, Information Services","Job Description: Data Engineer\nProject Role: Data Engineer\nProject Role Description: Design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform, and load) processes to migrate and deploy data across systems.\nMust-Have Skills: SAP HCM On Premise ABAP\nGood-to-Have Skills: SAP HCM Organizational Management\nExperience Required: Minimum 5 years\nEducational Qualification: 15 years full-time education\nJob Summary:\nAs a Data Engineer, your main responsibility will be to design, develop, and maintain data solutions for generating, collecting, and processing data. You'll work on creating data pipelines, ensuring data quality, and implementing ETL processes to move and deploy data across systems. Your role will be crucial in managing and analyzing data to support business decisions and provide data-driven insights.\nRoles & Responsibilities:\nSubject Matter Expert (SME): Expected to be an SME, collaborate and manage the team to deliver results. Responsible for making key team decisions.\nCollaboration & Teamwork: Work closely with multiple teams and contribute to important decisions and problem-solving efforts.\nData Solutions: Design, develop, and maintain data solutions that support data generation, collection, and processing.\nData Pipelines: Create and manage data pipelines to extract, transform, and load data (ETL processes) across systems.\nData Quality & Integrity: Ensure data quality and integrity throughout the data lifecycle.\nETL Processes: Implement and optimize ETL processes to efficiently migrate and deploy data across systems and platforms.\nProfessional & Technical Skills:\nMust-Have Skills:\nStrong proficiency in SAP HCM On Premise ABAP.\nExperience with data modeling and database design.\nFamiliarity with ETL tools and processes.\nExpertise in programming languages such as ABAP, SQL, and Python.\nGood-to-Have Skills:\nExperience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nAdditional Information:\nExperience Required: A minimum of 5 years of experience in SAP HCM On Premise ABAP.\nEducational Requirement: The candidate should have a minimum of 15 years of full-time education.","['Sql', 'Python', 'Abap']",2025-06-10 19:14:09
Data Engineer,Accenture India,2-5 Years,,Jaipur,"Consulting, Information Services","Job Description: Data Engineer\nProject Role: Data Engineer\nProject Role Description: Design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform, and load) processes to migrate and deploy data across systems.\nMust-Have Skills: SAP HCM On Premise ABAP\nGood-to-Have Skills: SAP HCM Organizational Management\nExperience Required: Minimum 5 years\nEducational Qualification: 15 years full-time education\nJob Summary:\nAs a Data Engineer, your main responsibility will be to design, develop, and maintain data solutions for generating, collecting, and processing data. You'll work on creating data pipelines, ensuring data quality, and implementing ETL processes to move and deploy data across systems. Your role will be crucial in managing and analyzing data to support business decisions and provide data-driven insights.\nRoles & Responsibilities:\nSubject Matter Expert (SME): Expected to be an SME, collaborate and manage the team to deliver results. Responsible for making key team decisions.\nCollaboration & Teamwork: Work closely with multiple teams and contribute to important decisions and problem-solving efforts.\nData Solutions: Design, develop, and maintain data solutions that support data generation, collection, and processing.\nData Pipelines: Create and manage data pipelines to extract, transform, and load data (ETL processes) across systems.\nData Quality & Integrity: Ensure data quality and integrity throughout the data lifecycle.\nETL Processes: Implement and optimize ETL processes to efficiently migrate and deploy data across systems and platforms.\nProfessional & Technical Skills:\nMust-Have Skills:\nStrong proficiency in SAP HCM On Premise ABAP.\nExperience with data modeling and database design.\nFamiliarity with ETL tools and processes.\nExpertise in programming languages such as ABAP, SQL, and Python.\nGood-to-Have Skills:\nExperience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nAdditional Information:\nExperience Required: A minimum of 5 years of experience in SAP HCM On Premise ABAP.\nEducational Requirement: The candidate should have a minimum of 15 years of full-time education.","['Sql', 'Python', 'Abap']",2025-06-10 19:14:10
Data Engineer,Accenture India,2-5 Years,,Jaipur,"Consulting, Information Services","Job Description: Data Engineer\nProject Role: Data Engineer\nProject Role Description: Design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform, and load) processes to migrate and deploy data across systems.\nMust-Have Skills: SAP HCM On Premise ABAP\nGood-to-Have Skills: SAP HCM Organizational Management\nExperience Required: Minimum 5 years\nEducational Qualification: 15 years full-time education\nJob Summary:\nAs a Data Engineer, your main responsibility will be to design, develop, and maintain data solutions for generating, collecting, and processing data. You'll work on creating data pipelines, ensuring data quality, and implementing ETL processes to move and deploy data across systems. Your role will be crucial in managing and analyzing data to support business decisions and provide data-driven insights.\nRoles & Responsibilities:\nSubject Matter Expert (SME): Expected to be an SME, collaborate and manage the team to deliver results. Responsible for making key team decisions.\nCollaboration & Teamwork: Work closely with multiple teams and contribute to important decisions and problem-solving efforts.\nData Solutions: Design, develop, and maintain data solutions that support data generation, collection, and processing.\nData Pipelines: Create and manage data pipelines to extract, transform, and load data (ETL processes) across systems.\nData Quality & Integrity: Ensure data quality and integrity throughout the data lifecycle.\nETL Processes: Implement and optimize ETL processes to efficiently migrate and deploy data across systems and platforms.\nProfessional & Technical Skills:\nMust-Have Skills:\nStrong proficiency in SAP HCM On Premise ABAP.\nExperience with data modeling and database design.\nFamiliarity with ETL tools and processes.\nExpertise in programming languages such as ABAP, SQL, and Python.\nGood-to-Have Skills:\nExperience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nAdditional Information:\nExperience Required: A minimum of 5 years of experience in SAP HCM On Premise ABAP.\nEducational Requirement: The candidate should have a minimum of 15 years of full-time education.","['Sql', 'Python', 'Abap']",2025-06-10 19:14:10
Data Engineer,Accenture India,2-5 Years,,Jaipur,"Consulting, Information Services","Job Description: Data Engineer\nProject Role: Data Engineer\nProject Role Description: Design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform, and load) processes to migrate and deploy data across systems.\nMust-Have Skills: SAP HCM On Premise ABAP\nGood-to-Have Skills: SAP HCM Organizational Management\nExperience Required: Minimum 5 years\nEducational Qualification: 15 years full-time education\nJob Summary:\nAs a Data Engineer, your main responsibility will be to design, develop, and maintain data solutions for generating, collecting, and processing data. You'll work on creating data pipelines, ensuring data quality, and implementing ETL processes to move and deploy data across systems. Your role will be crucial in managing and analyzing data to support business decisions and provide data-driven insights.\nRoles & Responsibilities:\nSubject Matter Expert (SME): Expected to be an SME, collaborate and manage the team to deliver results. Responsible for making key team decisions.\nCollaboration & Teamwork: Work closely with multiple teams and contribute to important decisions and problem-solving efforts.\nData Solutions: Design, develop, and maintain data solutions that support data generation, collection, and processing.\nData Pipelines: Create and manage data pipelines to extract, transform, and load data (ETL processes) across systems.\nData Quality & Integrity: Ensure data quality and integrity throughout the data lifecycle.\nETL Processes: Implement and optimize ETL processes to efficiently migrate and deploy data across systems and platforms.\nProfessional & Technical Skills:\nMust-Have Skills:\nStrong proficiency in SAP HCM On Premise ABAP.\nExperience with data modeling and database design.\nFamiliarity with ETL tools and processes.\nExpertise in programming languages such as ABAP, SQL, and Python.\nGood-to-Have Skills:\nExperience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nAdditional Information:\nExperience Required: A minimum of 5 years of experience in SAP HCM On Premise ABAP.\nEducational Requirement: The candidate should have a minimum of 15 years of full-time education.","['Sql', 'Python', 'Abap']",2025-06-10 19:14:11
Data Engineer,Accenture India,2-5 Years,,Jaipur,"Consulting, Information Services","Job Description: Data Engineer\nProject Role: Data Engineer\nProject Role Description: Design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform, and load) processes to migrate and deploy data across systems.\nMust-Have Skills: SAP HCM On Premise ABAP\nGood-to-Have Skills: SAP HCM Organizational Management\nExperience Required: Minimum 5 years\nEducational Qualification: 15 years full-time education\nJob Summary:\nAs a Data Engineer, your main responsibility will be to design, develop, and maintain data solutions for generating, collecting, and processing data. You'll work on creating data pipelines, ensuring data quality, and implementing ETL processes to move and deploy data across systems. Your role will be crucial in managing and analyzing data to support business decisions and provide data-driven insights.\nRoles & Responsibilities:\nSubject Matter Expert (SME): Expected to be an SME, collaborate and manage the team to deliver results. Responsible for making key team decisions.\nCollaboration & Teamwork: Work closely with multiple teams and contribute to important decisions and problem-solving efforts.\nData Solutions: Design, develop, and maintain data solutions that support data generation, collection, and processing.\nData Pipelines: Create and manage data pipelines to extract, transform, and load data (ETL processes) across systems.\nData Quality & Integrity: Ensure data quality and integrity throughout the data lifecycle.\nETL Processes: Implement and optimize ETL processes to efficiently migrate and deploy data across systems and platforms.\nProfessional & Technical Skills:\nMust-Have Skills:\nStrong proficiency in SAP HCM On Premise ABAP.\nExperience with data modeling and database design.\nFamiliarity with ETL tools and processes.\nExpertise in programming languages such as ABAP, SQL, and Python.\nGood-to-Have Skills:\nExperience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nAdditional Information:\nExperience Required: A minimum of 5 years of experience in SAP HCM On Premise ABAP.\nEducational Requirement: The candidate should have a minimum of 15 years of full-time education.","['Sql', 'Python', 'Abap']",2025-06-10 19:14:11
Data Engineer,Accenture India,2-5 Years,,Jaipur,"Consulting, Information Services","Job Description: Data Engineer\nProject Role: Data Engineer\nProject Role Description: Design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform, and load) processes to migrate and deploy data across systems.\nMust-Have Skills: SAP HCM On Premise ABAP\nGood-to-Have Skills: SAP HCM Organizational Management\nExperience Required: Minimum 5 years\nEducational Qualification: 15 years full-time education\nJob Summary:\nAs a Data Engineer, your main responsibility will be to design, develop, and maintain data solutions for generating, collecting, and processing data. You'll work on creating data pipelines, ensuring data quality, and implementing ETL processes to move and deploy data across systems. Your role will be crucial in managing and analyzing data to support business decisions and provide data-driven insights.\nRoles & Responsibilities:\nSubject Matter Expert (SME): Expected to be an SME, collaborate and manage the team to deliver results. Responsible for making key team decisions.\nCollaboration & Teamwork: Work closely with multiple teams and contribute to important decisions and problem-solving efforts.\nData Solutions: Design, develop, and maintain data solutions that support data generation, collection, and processing.\nData Pipelines: Create and manage data pipelines to extract, transform, and load data (ETL processes) across systems.\nData Quality & Integrity: Ensure data quality and integrity throughout the data lifecycle.\nETL Processes: Implement and optimize ETL processes to efficiently migrate and deploy data across systems and platforms.\nProfessional & Technical Skills:\nMust-Have Skills:\nStrong proficiency in SAP HCM On Premise ABAP.\nExperience with data modeling and database design.\nFamiliarity with ETL tools and processes.\nExpertise in programming languages such as ABAP, SQL, and Python.\nGood-to-Have Skills:\nExperience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nAdditional Information:\nExperience Required: A minimum of 5 years of experience in SAP HCM On Premise ABAP.\nEducational Requirement: The candidate should have a minimum of 15 years of full-time education.","['Sql', 'Python', 'Abap']",2025-06-10 19:14:12
Data Engineer,Accenture India,2-5 Years,,Jaipur,"Consulting, Information Services","Job Description: Data Engineer\nProject Role: Data Engineer\nProject Role Description: Design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform, and load) processes to migrate and deploy data across systems.\nMust-Have Skills: SAP HCM On Premise ABAP\nGood-to-Have Skills: SAP HCM Organizational Management\nExperience Required: Minimum 5 years\nEducational Qualification: 15 years full-time education\nJob Summary:\nAs a Data Engineer, your main responsibility will be to design, develop, and maintain data solutions for generating, collecting, and processing data. You'll work on creating data pipelines, ensuring data quality, and implementing ETL processes to move and deploy data across systems. Your role will be crucial in managing and analyzing data to support business decisions and provide data-driven insights.\nRoles & Responsibilities:\nSubject Matter Expert (SME): Expected to be an SME, collaborate and manage the team to deliver results. Responsible for making key team decisions.\nCollaboration & Teamwork: Work closely with multiple teams and contribute to important decisions and problem-solving efforts.\nData Solutions: Design, develop, and maintain data solutions that support data generation, collection, and processing.\nData Pipelines: Create and manage data pipelines to extract, transform, and load data (ETL processes) across systems.\nData Quality & Integrity: Ensure data quality and integrity throughout the data lifecycle.\nETL Processes: Implement and optimize ETL processes to efficiently migrate and deploy data across systems and platforms.\nProfessional & Technical Skills:\nMust-Have Skills:\nStrong proficiency in SAP HCM On Premise ABAP.\nExperience with data modeling and database design.\nFamiliarity with ETL tools and processes.\nExpertise in programming languages such as ABAP, SQL, and Python.\nGood-to-Have Skills:\nExperience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nAdditional Information:\nExperience Required: A minimum of 5 years of experience in SAP HCM On Premise ABAP.\nEducational Requirement: The candidate should have a minimum of 15 years of full-time education.","['Sql', 'Python', 'Abap']",2025-06-10 19:14:13
Data Engineer,Accenture India,2-5 Years,,Jaipur,"Consulting, Information Services","Job Description: Data Engineer\nProject Role: Data Engineer\nProject Role Description: Design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform, and load) processes to migrate and deploy data across systems.\nMust-Have Skills: SAP HCM On Premise ABAP\nGood-to-Have Skills: SAP HCM Organizational Management\nExperience Required: Minimum 5 years\nEducational Qualification: 15 years full-time education\nJob Summary:\nAs a Data Engineer, your main responsibility will be to design, develop, and maintain data solutions for generating, collecting, and processing data. You'll work on creating data pipelines, ensuring data quality, and implementing ETL processes to move and deploy data across systems. Your role will be crucial in managing and analyzing data to support business decisions and provide data-driven insights.\nRoles & Responsibilities:\nSubject Matter Expert (SME): Expected to be an SME, collaborate and manage the team to deliver results. Responsible for making key team decisions.\nCollaboration & Teamwork: Work closely with multiple teams and contribute to important decisions and problem-solving efforts.\nData Solutions: Design, develop, and maintain data solutions that support data generation, collection, and processing.\nData Pipelines: Create and manage data pipelines to extract, transform, and load data (ETL processes) across systems.\nData Quality & Integrity: Ensure data quality and integrity throughout the data lifecycle.\nETL Processes: Implement and optimize ETL processes to efficiently migrate and deploy data across systems and platforms.\nProfessional & Technical Skills:\nMust-Have Skills:\nStrong proficiency in SAP HCM On Premise ABAP.\nExperience with data modeling and database design.\nFamiliarity with ETL tools and processes.\nExpertise in programming languages such as ABAP, SQL, and Python.\nGood-to-Have Skills:\nExperience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nAdditional Information:\nExperience Required: A minimum of 5 years of experience in SAP HCM On Premise ABAP.\nEducational Requirement: The candidate should have a minimum of 15 years of full-time education.","['Sql', 'Python', 'Abap']",2025-06-10 19:14:13
Data Engineer,Accenture India,2-5 Years,,Jaipur,"Consulting, Information Services","Job Description: Data Engineer\nProject Role: Data Engineer\nProject Role Description: Design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform, and load) processes to migrate and deploy data across systems.\nMust-Have Skills: SAP HCM On Premise ABAP\nGood-to-Have Skills: SAP HCM Organizational Management\nExperience Required: Minimum 5 years\nEducational Qualification: 15 years full-time education\nJob Summary:\nAs a Data Engineer, your main responsibility will be to design, develop, and maintain data solutions for generating, collecting, and processing data. You'll work on creating data pipelines, ensuring data quality, and implementing ETL processes to move and deploy data across systems. Your role will be crucial in managing and analyzing data to support business decisions and provide data-driven insights.\nRoles & Responsibilities:\nSubject Matter Expert (SME): Expected to be an SME, collaborate and manage the team to deliver results. Responsible for making key team decisions.\nCollaboration & Teamwork: Work closely with multiple teams and contribute to important decisions and problem-solving efforts.\nData Solutions: Design, develop, and maintain data solutions that support data generation, collection, and processing.\nData Pipelines: Create and manage data pipelines to extract, transform, and load data (ETL processes) across systems.\nData Quality & Integrity: Ensure data quality and integrity throughout the data lifecycle.\nETL Processes: Implement and optimize ETL processes to efficiently migrate and deploy data across systems and platforms.\nProfessional & Technical Skills:\nMust-Have Skills:\nStrong proficiency in SAP HCM On Premise ABAP.\nExperience with data modeling and database design.\nFamiliarity with ETL tools and processes.\nExpertise in programming languages such as ABAP, SQL, and Python.\nGood-to-Have Skills:\nExperience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nAdditional Information:\nExperience Required: A minimum of 5 years of experience in SAP HCM On Premise ABAP.\nEducational Requirement: The candidate should have a minimum of 15 years of full-time education.","['Sql', 'Python', 'Abap']",2025-06-10 19:14:14
Data Engineer,Accenture India,2-5 Years,,Jaipur,"Consulting, Information Services","Job Description: Data Engineer\nProject Role: Data Engineer\nProject Role Description: Design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform, and load) processes to migrate and deploy data across systems.\nMust-Have Skills: SAP HCM On Premise ABAP\nGood-to-Have Skills: SAP HCM Organizational Management\nExperience Required: Minimum 5 years\nEducational Qualification: 15 years full-time education\nJob Summary:\nAs a Data Engineer, your main responsibility will be to design, develop, and maintain data solutions for generating, collecting, and processing data. You'll work on creating data pipelines, ensuring data quality, and implementing ETL processes to move and deploy data across systems. Your role will be crucial in managing and analyzing data to support business decisions and provide data-driven insights.\nRoles & Responsibilities:\nSubject Matter Expert (SME): Expected to be an SME, collaborate and manage the team to deliver results. Responsible for making key team decisions.\nCollaboration & Teamwork: Work closely with multiple teams and contribute to important decisions and problem-solving efforts.\nData Solutions: Design, develop, and maintain data solutions that support data generation, collection, and processing.\nData Pipelines: Create and manage data pipelines to extract, transform, and load data (ETL processes) across systems.\nData Quality & Integrity: Ensure data quality and integrity throughout the data lifecycle.\nETL Processes: Implement and optimize ETL processes to efficiently migrate and deploy data across systems and platforms.\nProfessional & Technical Skills:\nMust-Have Skills:\nStrong proficiency in SAP HCM On Premise ABAP.\nExperience with data modeling and database design.\nFamiliarity with ETL tools and processes.\nExpertise in programming languages such as ABAP, SQL, and Python.\nGood-to-Have Skills:\nExperience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nAdditional Information:\nExperience Required: A minimum of 5 years of experience in SAP HCM On Premise ABAP.\nEducational Requirement: The candidate should have a minimum of 15 years of full-time education.","['Sql', 'Python', 'Abap']",2025-06-10 19:14:15
Data Engineer,Accenture India,2-5 Years,,Jaipur,"Consulting, Information Services","Job Description: Data Engineer\nProject Role: Data Engineer\nProject Role Description: Design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform, and load) processes to migrate and deploy data across systems.\nMust-Have Skills: SAP HCM On Premise ABAP\nGood-to-Have Skills: SAP HCM Organizational Management\nExperience Required: Minimum 5 years\nEducational Qualification: 15 years full-time education\nJob Summary:\nAs a Data Engineer, your main responsibility will be to design, develop, and maintain data solutions for generating, collecting, and processing data. You'll work on creating data pipelines, ensuring data quality, and implementing ETL processes to move and deploy data across systems. Your role will be crucial in managing and analyzing data to support business decisions and provide data-driven insights.\nRoles & Responsibilities:\nSubject Matter Expert (SME): Expected to be an SME, collaborate and manage the team to deliver results. Responsible for making key team decisions.\nCollaboration & Teamwork: Work closely with multiple teams and contribute to important decisions and problem-solving efforts.\nData Solutions: Design, develop, and maintain data solutions that support data generation, collection, and processing.\nData Pipelines: Create and manage data pipelines to extract, transform, and load data (ETL processes) across systems.\nData Quality & Integrity: Ensure data quality and integrity throughout the data lifecycle.\nETL Processes: Implement and optimize ETL processes to efficiently migrate and deploy data across systems and platforms.\nProfessional & Technical Skills:\nMust-Have Skills:\nStrong proficiency in SAP HCM On Premise ABAP.\nExperience with data modeling and database design.\nFamiliarity with ETL tools and processes.\nExpertise in programming languages such as ABAP, SQL, and Python.\nGood-to-Have Skills:\nExperience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nAdditional Information:\nExperience Required: A minimum of 5 years of experience in SAP HCM On Premise ABAP.\nEducational Requirement: The candidate should have a minimum of 15 years of full-time education.","['Sql', 'Python', 'Abap']",2025-06-10 19:14:15
Data Engineer,Accenture India,2-5 Years,,Jaipur,"Consulting, Information Services","Job Description: Data Engineer\nProject Role: Data Engineer\nProject Role Description: Design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform, and load) processes to migrate and deploy data across systems.\nMust-Have Skills: SAP HCM On Premise ABAP\nGood-to-Have Skills: SAP HCM Organizational Management\nExperience Required: Minimum 5 years\nEducational Qualification: 15 years full-time education\nJob Summary:\nAs a Data Engineer, your main responsibility will be to design, develop, and maintain data solutions for generating, collecting, and processing data. You'll work on creating data pipelines, ensuring data quality, and implementing ETL processes to move and deploy data across systems. Your role will be crucial in managing and analyzing data to support business decisions and provide data-driven insights.\nRoles & Responsibilities:\nSubject Matter Expert (SME): Expected to be an SME, collaborate and manage the team to deliver results. Responsible for making key team decisions.\nCollaboration & Teamwork: Work closely with multiple teams and contribute to important decisions and problem-solving efforts.\nData Solutions: Design, develop, and maintain data solutions that support data generation, collection, and processing.\nData Pipelines: Create and manage data pipelines to extract, transform, and load data (ETL processes) across systems.\nData Quality & Integrity: Ensure data quality and integrity throughout the data lifecycle.\nETL Processes: Implement and optimize ETL processes to efficiently migrate and deploy data across systems and platforms.\nProfessional & Technical Skills:\nMust-Have Skills:\nStrong proficiency in SAP HCM On Premise ABAP.\nExperience with data modeling and database design.\nFamiliarity with ETL tools and processes.\nExpertise in programming languages such as ABAP, SQL, and Python.\nGood-to-Have Skills:\nExperience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nAdditional Information:\nExperience Required: A minimum of 5 years of experience in SAP HCM On Premise ABAP.\nEducational Requirement: The candidate should have a minimum of 15 years of full-time education.","['Sql', 'Python', 'Abap']",2025-06-10 19:14:16
Data Engineer,Accenture India,2-5 Years,,Jaipur,"Consulting, Information Services","Job Description: Data Engineer\nProject Role: Data Engineer\nProject Role Description: Design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform, and load) processes to migrate and deploy data across systems.\nMust-Have Skills: SAP HCM On Premise ABAP\nGood-to-Have Skills: SAP HCM Organizational Management\nExperience Required: Minimum 5 years\nEducational Qualification: 15 years full-time education\nJob Summary:\nAs a Data Engineer, your main responsibility will be to design, develop, and maintain data solutions for generating, collecting, and processing data. You'll work on creating data pipelines, ensuring data quality, and implementing ETL processes to move and deploy data across systems. Your role will be crucial in managing and analyzing data to support business decisions and provide data-driven insights.\nRoles & Responsibilities:\nSubject Matter Expert (SME): Expected to be an SME, collaborate and manage the team to deliver results. Responsible for making key team decisions.\nCollaboration & Teamwork: Work closely with multiple teams and contribute to important decisions and problem-solving efforts.\nData Solutions: Design, develop, and maintain data solutions that support data generation, collection, and processing.\nData Pipelines: Create and manage data pipelines to extract, transform, and load data (ETL processes) across systems.\nData Quality & Integrity: Ensure data quality and integrity throughout the data lifecycle.\nETL Processes: Implement and optimize ETL processes to efficiently migrate and deploy data across systems and platforms.\nProfessional & Technical Skills:\nMust-Have Skills:\nStrong proficiency in SAP HCM On Premise ABAP.\nExperience with data modeling and database design.\nFamiliarity with ETL tools and processes.\nExpertise in programming languages such as ABAP, SQL, and Python.\nGood-to-Have Skills:\nExperience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nAdditional Information:\nExperience Required: A minimum of 5 years of experience in SAP HCM On Premise ABAP.\nEducational Requirement: The candidate should have a minimum of 15 years of full-time education.","['Sql', 'Python', 'Abap']",2025-06-10 19:14:17
Data Engineer,Accenture India,2-5 Years,,Jaipur,"Consulting, Information Services","Job Description: Data Engineer\nProject Role: Data Engineer\nProject Role Description: Design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform, and load) processes to migrate and deploy data across systems.\nMust-Have Skills: SAP HCM On Premise ABAP\nGood-to-Have Skills: SAP HCM Organizational Management\nExperience Required: Minimum 5 years\nEducational Qualification: 15 years full-time education\nJob Summary:\nAs a Data Engineer, your main responsibility will be to design, develop, and maintain data solutions for generating, collecting, and processing data. You'll work on creating data pipelines, ensuring data quality, and implementing ETL processes to move and deploy data across systems. Your role will be crucial in managing and analyzing data to support business decisions and provide data-driven insights.\nRoles & Responsibilities:\nSubject Matter Expert (SME): Expected to be an SME, collaborate and manage the team to deliver results. Responsible for making key team decisions.\nCollaboration & Teamwork: Work closely with multiple teams and contribute to important decisions and problem-solving efforts.\nData Solutions: Design, develop, and maintain data solutions that support data generation, collection, and processing.\nData Pipelines: Create and manage data pipelines to extract, transform, and load data (ETL processes) across systems.\nData Quality & Integrity: Ensure data quality and integrity throughout the data lifecycle.\nETL Processes: Implement and optimize ETL processes to efficiently migrate and deploy data across systems and platforms.\nProfessional & Technical Skills:\nMust-Have Skills:\nStrong proficiency in SAP HCM On Premise ABAP.\nExperience with data modeling and database design.\nFamiliarity with ETL tools and processes.\nExpertise in programming languages such as ABAP, SQL, and Python.\nGood-to-Have Skills:\nExperience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nAdditional Information:\nExperience Required: A minimum of 5 years of experience in SAP HCM On Premise ABAP.\nEducational Requirement: The candidate should have a minimum of 15 years of full-time education.","['Sql', 'Python', 'Abap']",2025-06-10 19:14:18
Data Engineer,Accenture India,5-10 Years,,Jaipur,"Consulting, Information Services","Job Description: Data Engineer\nProject Role: Data Engineer\nProject Role Description: Design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform, and load) processes to migrate and deploy data across systems.\nMust-Have Skills: SAP HCM On Premise ABAP\nGood-to-Have Skills: SAP HCM Organizational Management\nExperience Required: Minimum 5 years\nEducational Qualification: 15 years full-time education\nJob Summary:\nAs a Data Engineer, your main responsibility will be to design, develop, and maintain data solutions for generating, collecting, and processing data. You'll work on creating data pipelines, ensuring data quality, and implementing ETL processes to move and deploy data across systems. Your role will be crucial in managing and analyzing data to support business decisions and provide data-driven insights.\nRoles & Responsibilities:\nSubject Matter Expert (SME): Expected to be an SME, collaborate and manage the team to deliver results. Responsible for making key team decisions.\nCollaboration & Teamwork: Work closely with multiple teams and contribute to important decisions and problem-solving efforts.\nData Solutions: Design, develop, and maintain data solutions that support data generation, collection, and processing.\nData Pipelines: Create and manage data pipelines to extract, transform, and load data (ETL processes) across systems.\nData Quality & Integrity: Ensure data quality and integrity throughout the data lifecycle.\nETL Processes: Implement and optimize ETL processes to efficiently migrate and deploy data across systems and platforms.\nProfessional & Technical Skills:\nMust-Have Skills:\nStrong proficiency in SAP HCM On Premise ABAP.\nExperience with data modeling and database design.\nFamiliarity with ETL tools and processes.\nExpertise in programming languages such as ABAP, SQL, and Python.\nGood-to-Have Skills:\nExperience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nAdditional Information:\nExperience Required: A minimum of 5 years of experience in SAP HCM On Premise ABAP.\nEducational Requirement: The candidate should have a minimum of 15 years of full-time education.","['Sql', 'Python', 'Abap']",2025-06-10 19:35:55
Data Engineer,Accenture India,5-10 Years,,Jaipur,"Consulting, Information Services","Job Description: Data Engineer\nProject Role: Data Engineer\nProject Role Description: Design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform, and load) processes to migrate and deploy data across systems.\nMust-Have Skills: SAP HCM On Premise ABAP\nGood-to-Have Skills: SAP HCM Organizational Management\nExperience Required: Minimum 5 years\nEducational Qualification: 15 years full-time education\nJob Summary:\nAs a Data Engineer, your main responsibility will be to design, develop, and maintain data solutions for generating, collecting, and processing data. You'll work on creating data pipelines, ensuring data quality, and implementing ETL processes to move and deploy data across systems. Your role will be crucial in managing and analyzing data to support business decisions and provide data-driven insights.\nRoles & Responsibilities:\nSubject Matter Expert (SME): Expected to be an SME, collaborate and manage the team to deliver results. Responsible for making key team decisions.\nCollaboration & Teamwork: Work closely with multiple teams and contribute to important decisions and problem-solving efforts.\nData Solutions: Design, develop, and maintain data solutions that support data generation, collection, and processing.\nData Pipelines: Create and manage data pipelines to extract, transform, and load data (ETL processes) across systems.\nData Quality & Integrity: Ensure data quality and integrity throughout the data lifecycle.\nETL Processes: Implement and optimize ETL processes to efficiently migrate and deploy data across systems and platforms.\nProfessional & Technical Skills:\nMust-Have Skills:\nStrong proficiency in SAP HCM On Premise ABAP.\nExperience with data modeling and database design.\nFamiliarity with ETL tools and processes.\nExpertise in programming languages such as ABAP, SQL, and Python.\nGood-to-Have Skills:\nExperience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nAdditional Information:\nExperience Required: A minimum of 5 years of experience in SAP HCM On Premise ABAP.\nEducational Requirement: The candidate should have a minimum of 15 years of full-time education.","['Sql', 'Python', 'Abap']",2025-06-10 19:35:58
Data Engineer,Accenture India,5-10 Years,,Jaipur,"Consulting, Information Services","Job Description: Data Engineer\nProject Role: Data Engineer\nProject Role Description: Design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform, and load) processes to migrate and deploy data across systems.\nMust-Have Skills: SAP HCM On Premise ABAP\nGood-to-Have Skills: SAP HCM Organizational Management\nExperience Required: Minimum 5 years\nEducational Qualification: 15 years full-time education\nJob Summary:\nAs a Data Engineer, your main responsibility will be to design, develop, and maintain data solutions for generating, collecting, and processing data. You'll work on creating data pipelines, ensuring data quality, and implementing ETL processes to move and deploy data across systems. Your role will be crucial in managing and analyzing data to support business decisions and provide data-driven insights.\nRoles & Responsibilities:\nSubject Matter Expert (SME): Expected to be an SME, collaborate and manage the team to deliver results. Responsible for making key team decisions.\nCollaboration & Teamwork: Work closely with multiple teams and contribute to important decisions and problem-solving efforts.\nData Solutions: Design, develop, and maintain data solutions that support data generation, collection, and processing.\nData Pipelines: Create and manage data pipelines to extract, transform, and load data (ETL processes) across systems.\nData Quality & Integrity: Ensure data quality and integrity throughout the data lifecycle.\nETL Processes: Implement and optimize ETL processes to efficiently migrate and deploy data across systems and platforms.\nProfessional & Technical Skills:\nMust-Have Skills:\nStrong proficiency in SAP HCM On Premise ABAP.\nExperience with data modeling and database design.\nFamiliarity with ETL tools and processes.\nExpertise in programming languages such as ABAP, SQL, and Python.\nGood-to-Have Skills:\nExperience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nAdditional Information:\nExperience Required: A minimum of 5 years of experience in SAP HCM On Premise ABAP.\nEducational Requirement: The candidate should have a minimum of 15 years of full-time education.","['Sql', 'Python', 'Abap']",2025-06-10 19:36:01
Data Engineer,Accenture India,5-10 Years,,Jaipur,"Consulting, Information Services","Job Description: Data Engineer\nProject Role: Data Engineer\nProject Role Description: Design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform, and load) processes to migrate and deploy data across systems.\nMust-Have Skills: SAP HCM On Premise ABAP\nGood-to-Have Skills: SAP HCM Organizational Management\nExperience Required: Minimum 5 years\nEducational Qualification: 15 years full-time education\nJob Summary:\nAs a Data Engineer, your main responsibility will be to design, develop, and maintain data solutions for generating, collecting, and processing data. You'll work on creating data pipelines, ensuring data quality, and implementing ETL processes to move and deploy data across systems. Your role will be crucial in managing and analyzing data to support business decisions and provide data-driven insights.\nRoles & Responsibilities:\nSubject Matter Expert (SME): Expected to be an SME, collaborate and manage the team to deliver results. Responsible for making key team decisions.\nCollaboration & Teamwork: Work closely with multiple teams and contribute to important decisions and problem-solving efforts.\nData Solutions: Design, develop, and maintain data solutions that support data generation, collection, and processing.\nData Pipelines: Create and manage data pipelines to extract, transform, and load data (ETL processes) across systems.\nData Quality & Integrity: Ensure data quality and integrity throughout the data lifecycle.\nETL Processes: Implement and optimize ETL processes to efficiently migrate and deploy data across systems and platforms.\nProfessional & Technical Skills:\nMust-Have Skills:\nStrong proficiency in SAP HCM On Premise ABAP.\nExperience with data modeling and database design.\nFamiliarity with ETL tools and processes.\nExpertise in programming languages such as ABAP, SQL, and Python.\nGood-to-Have Skills:\nExperience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nAdditional Information:\nExperience Required: A minimum of 5 years of experience in SAP HCM On Premise ABAP.\nEducational Requirement: The candidate should have a minimum of 15 years of full-time education.","['Sql', 'Python', 'Abap']",2025-06-10 19:36:02
Data Engineer,Accenture India,5-10 Years,,Jaipur,"Consulting, Information Services","Job Description: Data Engineer\nProject Role: Data Engineer\nProject Role Description: Design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform, and load) processes to migrate and deploy data across systems.\nMust-Have Skills: SAP HCM On Premise ABAP\nGood-to-Have Skills: SAP HCM Organizational Management\nExperience Required: Minimum 5 years\nEducational Qualification: 15 years full-time education\nJob Summary:\nAs a Data Engineer, your main responsibility will be to design, develop, and maintain data solutions for generating, collecting, and processing data. You'll work on creating data pipelines, ensuring data quality, and implementing ETL processes to move and deploy data across systems. Your role will be crucial in managing and analyzing data to support business decisions and provide data-driven insights.\nRoles & Responsibilities:\nSubject Matter Expert (SME): Expected to be an SME, collaborate and manage the team to deliver results. Responsible for making key team decisions.\nCollaboration & Teamwork: Work closely with multiple teams and contribute to important decisions and problem-solving efforts.\nData Solutions: Design, develop, and maintain data solutions that support data generation, collection, and processing.\nData Pipelines: Create and manage data pipelines to extract, transform, and load data (ETL processes) across systems.\nData Quality & Integrity: Ensure data quality and integrity throughout the data lifecycle.\nETL Processes: Implement and optimize ETL processes to efficiently migrate and deploy data across systems and platforms.\nProfessional & Technical Skills:\nMust-Have Skills:\nStrong proficiency in SAP HCM On Premise ABAP.\nExperience with data modeling and database design.\nFamiliarity with ETL tools and processes.\nExpertise in programming languages such as ABAP, SQL, and Python.\nGood-to-Have Skills:\nExperience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nAdditional Information:\nExperience Required: A minimum of 5 years of experience in SAP HCM On Premise ABAP.\nEducational Requirement: The candidate should have a minimum of 15 years of full-time education.","['Sql', 'Python', 'Abap']",2025-06-10 19:36:03
Data Engineer,Accenture India,5-10 Years,,Jaipur,"Consulting, Information Services","Job Description: Data Engineer\nProject Role: Data Engineer\nProject Role Description: Design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform, and load) processes to migrate and deploy data across systems.\nMust-Have Skills: SAP HCM On Premise ABAP\nGood-to-Have Skills: SAP HCM Organizational Management\nExperience Required: Minimum 5 years\nEducational Qualification: 15 years full-time education\nJob Summary:\nAs a Data Engineer, your main responsibility will be to design, develop, and maintain data solutions for generating, collecting, and processing data. You'll work on creating data pipelines, ensuring data quality, and implementing ETL processes to move and deploy data across systems. Your role will be crucial in managing and analyzing data to support business decisions and provide data-driven insights.\nRoles & Responsibilities:\nSubject Matter Expert (SME): Expected to be an SME, collaborate and manage the team to deliver results. Responsible for making key team decisions.\nCollaboration & Teamwork: Work closely with multiple teams and contribute to important decisions and problem-solving efforts.\nData Solutions: Design, develop, and maintain data solutions that support data generation, collection, and processing.\nData Pipelines: Create and manage data pipelines to extract, transform, and load data (ETL processes) across systems.\nData Quality & Integrity: Ensure data quality and integrity throughout the data lifecycle.\nETL Processes: Implement and optimize ETL processes to efficiently migrate and deploy data across systems and platforms.\nProfessional & Technical Skills:\nMust-Have Skills:\nStrong proficiency in SAP HCM On Premise ABAP.\nExperience with data modeling and database design.\nFamiliarity with ETL tools and processes.\nExpertise in programming languages such as ABAP, SQL, and Python.\nGood-to-Have Skills:\nExperience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nAdditional Information:\nExperience Required: A minimum of 5 years of experience in SAP HCM On Premise ABAP.\nEducational Requirement: The candidate should have a minimum of 15 years of full-time education.","['Sql', 'Python', 'Abap']",2025-06-10 19:36:03
Data Engineer,Accenture India,5-10 Years,,Jaipur,"Consulting, Information Services","Job Description: Data Engineer\nProject Role: Data Engineer\nProject Role Description: Design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform, and load) processes to migrate and deploy data across systems.\nMust-Have Skills: SAP HCM On Premise ABAP\nGood-to-Have Skills: SAP HCM Organizational Management\nExperience Required: Minimum 5 years\nEducational Qualification: 15 years full-time education\nJob Summary:\nAs a Data Engineer, your main responsibility will be to design, develop, and maintain data solutions for generating, collecting, and processing data. You'll work on creating data pipelines, ensuring data quality, and implementing ETL processes to move and deploy data across systems. Your role will be crucial in managing and analyzing data to support business decisions and provide data-driven insights.\nRoles & Responsibilities:\nSubject Matter Expert (SME): Expected to be an SME, collaborate and manage the team to deliver results. Responsible for making key team decisions.\nCollaboration & Teamwork: Work closely with multiple teams and contribute to important decisions and problem-solving efforts.\nData Solutions: Design, develop, and maintain data solutions that support data generation, collection, and processing.\nData Pipelines: Create and manage data pipelines to extract, transform, and load data (ETL processes) across systems.\nData Quality & Integrity: Ensure data quality and integrity throughout the data lifecycle.\nETL Processes: Implement and optimize ETL processes to efficiently migrate and deploy data across systems and platforms.\nProfessional & Technical Skills:\nMust-Have Skills:\nStrong proficiency in SAP HCM On Premise ABAP.\nExperience with data modeling and database design.\nFamiliarity with ETL tools and processes.\nExpertise in programming languages such as ABAP, SQL, and Python.\nGood-to-Have Skills:\nExperience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nAdditional Information:\nExperience Required: A minimum of 5 years of experience in SAP HCM On Premise ABAP.\nEducational Requirement: The candidate should have a minimum of 15 years of full-time education.","['Sql', 'Python', 'Abap']",2025-06-10 19:36:04
Data Engineer,Accenture India,5-10 Years,,Jaipur,"Consulting, Information Services","Job Description: Data Engineer\nProject Role: Data Engineer\nProject Role Description: Design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform, and load) processes to migrate and deploy data across systems.\nMust-Have Skills: SAP HCM On Premise ABAP\nGood-to-Have Skills: SAP HCM Organizational Management\nExperience Required: Minimum 5 years\nEducational Qualification: 15 years full-time education\nJob Summary:\nAs a Data Engineer, your main responsibility will be to design, develop, and maintain data solutions for generating, collecting, and processing data. You'll work on creating data pipelines, ensuring data quality, and implementing ETL processes to move and deploy data across systems. Your role will be crucial in managing and analyzing data to support business decisions and provide data-driven insights.\nRoles & Responsibilities:\nSubject Matter Expert (SME): Expected to be an SME, collaborate and manage the team to deliver results. Responsible for making key team decisions.\nCollaboration & Teamwork: Work closely with multiple teams and contribute to important decisions and problem-solving efforts.\nData Solutions: Design, develop, and maintain data solutions that support data generation, collection, and processing.\nData Pipelines: Create and manage data pipelines to extract, transform, and load data (ETL processes) across systems.\nData Quality & Integrity: Ensure data quality and integrity throughout the data lifecycle.\nETL Processes: Implement and optimize ETL processes to efficiently migrate and deploy data across systems and platforms.\nProfessional & Technical Skills:\nMust-Have Skills:\nStrong proficiency in SAP HCM On Premise ABAP.\nExperience with data modeling and database design.\nFamiliarity with ETL tools and processes.\nExpertise in programming languages such as ABAP, SQL, and Python.\nGood-to-Have Skills:\nExperience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nAdditional Information:\nExperience Required: A minimum of 5 years of experience in SAP HCM On Premise ABAP.\nEducational Requirement: The candidate should have a minimum of 15 years of full-time education.","['Sql', 'Python', 'Abap']",2025-06-10 19:36:04
Data Engineer,Accenture India,5-10 Years,,Jaipur,"Consulting, Information Services","Job Description: Data Engineer\nProject Role: Data Engineer\nProject Role Description: Design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform, and load) processes to migrate and deploy data across systems.\nMust-Have Skills: SAP HCM On Premise ABAP\nGood-to-Have Skills: SAP HCM Organizational Management\nExperience Required: Minimum 5 years\nEducational Qualification: 15 years full-time education\nJob Summary:\nAs a Data Engineer, your main responsibility will be to design, develop, and maintain data solutions for generating, collecting, and processing data. You'll work on creating data pipelines, ensuring data quality, and implementing ETL processes to move and deploy data across systems. Your role will be crucial in managing and analyzing data to support business decisions and provide data-driven insights.\nRoles & Responsibilities:\nSubject Matter Expert (SME): Expected to be an SME, collaborate and manage the team to deliver results. Responsible for making key team decisions.\nCollaboration & Teamwork: Work closely with multiple teams and contribute to important decisions and problem-solving efforts.\nData Solutions: Design, develop, and maintain data solutions that support data generation, collection, and processing.\nData Pipelines: Create and manage data pipelines to extract, transform, and load data (ETL processes) across systems.\nData Quality & Integrity: Ensure data quality and integrity throughout the data lifecycle.\nETL Processes: Implement and optimize ETL processes to efficiently migrate and deploy data across systems and platforms.\nProfessional & Technical Skills:\nMust-Have Skills:\nStrong proficiency in SAP HCM On Premise ABAP.\nExperience with data modeling and database design.\nFamiliarity with ETL tools and processes.\nExpertise in programming languages such as ABAP, SQL, and Python.\nGood-to-Have Skills:\nExperience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nAdditional Information:\nExperience Required: A minimum of 5 years of experience in SAP HCM On Premise ABAP.\nEducational Requirement: The candidate should have a minimum of 15 years of full-time education.","['Sql', 'Python', 'Abap']",2025-06-10 19:36:05
Data Engineer,Accenture India,5-10 Years,,Jaipur,"Consulting, Information Services","Job Description: Data Engineer\nProject Role: Data Engineer\nProject Role Description: Design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform, and load) processes to migrate and deploy data across systems.\nMust-Have Skills: SAP HCM On Premise ABAP\nGood-to-Have Skills: SAP HCM Organizational Management\nExperience Required: Minimum 5 years\nEducational Qualification: 15 years full-time education\nJob Summary:\nAs a Data Engineer, your main responsibility will be to design, develop, and maintain data solutions for generating, collecting, and processing data. You'll work on creating data pipelines, ensuring data quality, and implementing ETL processes to move and deploy data across systems. Your role will be crucial in managing and analyzing data to support business decisions and provide data-driven insights.\nRoles & Responsibilities:\nSubject Matter Expert (SME): Expected to be an SME, collaborate and manage the team to deliver results. Responsible for making key team decisions.\nCollaboration & Teamwork: Work closely with multiple teams and contribute to important decisions and problem-solving efforts.\nData Solutions: Design, develop, and maintain data solutions that support data generation, collection, and processing.\nData Pipelines: Create and manage data pipelines to extract, transform, and load data (ETL processes) across systems.\nData Quality & Integrity: Ensure data quality and integrity throughout the data lifecycle.\nETL Processes: Implement and optimize ETL processes to efficiently migrate and deploy data across systems and platforms.\nProfessional & Technical Skills:\nMust-Have Skills:\nStrong proficiency in SAP HCM On Premise ABAP.\nExperience with data modeling and database design.\nFamiliarity with ETL tools and processes.\nExpertise in programming languages such as ABAP, SQL, and Python.\nGood-to-Have Skills:\nExperience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nAdditional Information:\nExperience Required: A minimum of 5 years of experience in SAP HCM On Premise ABAP.\nEducational Requirement: The candidate should have a minimum of 15 years of full-time education.","['Sql', 'Python', 'Abap']",2025-06-10 19:36:06
Data Engineer,Accenture India,5-10 Years,,Jaipur,"Consulting, Information Services","Job Description: Data Engineer\nProject Role: Data Engineer\nProject Role Description: Design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform, and load) processes to migrate and deploy data across systems.\nMust-Have Skills: SAP HCM On Premise ABAP\nGood-to-Have Skills: SAP HCM Organizational Management\nExperience Required: Minimum 5 years\nEducational Qualification: 15 years full-time education\nJob Summary:\nAs a Data Engineer, your main responsibility will be to design, develop, and maintain data solutions for generating, collecting, and processing data. You'll work on creating data pipelines, ensuring data quality, and implementing ETL processes to move and deploy data across systems. Your role will be crucial in managing and analyzing data to support business decisions and provide data-driven insights.\nRoles & Responsibilities:\nSubject Matter Expert (SME): Expected to be an SME, collaborate and manage the team to deliver results. Responsible for making key team decisions.\nCollaboration & Teamwork: Work closely with multiple teams and contribute to important decisions and problem-solving efforts.\nData Solutions: Design, develop, and maintain data solutions that support data generation, collection, and processing.\nData Pipelines: Create and manage data pipelines to extract, transform, and load data (ETL processes) across systems.\nData Quality & Integrity: Ensure data quality and integrity throughout the data lifecycle.\nETL Processes: Implement and optimize ETL processes to efficiently migrate and deploy data across systems and platforms.\nProfessional & Technical Skills:\nMust-Have Skills:\nStrong proficiency in SAP HCM On Premise ABAP.\nExperience with data modeling and database design.\nFamiliarity with ETL tools and processes.\nExpertise in programming languages such as ABAP, SQL, and Python.\nGood-to-Have Skills:\nExperience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nAdditional Information:\nExperience Required: A minimum of 5 years of experience in SAP HCM On Premise ABAP.\nEducational Requirement: The candidate should have a minimum of 15 years of full-time education.","['Sql', 'Python', 'Abap']",2025-06-10 19:36:06
Data Engineer,Accenture India,5-10 Years,,Jaipur,"Consulting, Information Services","Job Description: Data Engineer\nProject Role: Data Engineer\nProject Role Description: Design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform, and load) processes to migrate and deploy data across systems.\nMust-Have Skills: SAP HCM On Premise ABAP\nGood-to-Have Skills: SAP HCM Organizational Management\nExperience Required: Minimum 5 years\nEducational Qualification: 15 years full-time education\nJob Summary:\nAs a Data Engineer, your main responsibility will be to design, develop, and maintain data solutions for generating, collecting, and processing data. You'll work on creating data pipelines, ensuring data quality, and implementing ETL processes to move and deploy data across systems. Your role will be crucial in managing and analyzing data to support business decisions and provide data-driven insights.\nRoles & Responsibilities:\nSubject Matter Expert (SME): Expected to be an SME, collaborate and manage the team to deliver results. Responsible for making key team decisions.\nCollaboration & Teamwork: Work closely with multiple teams and contribute to important decisions and problem-solving efforts.\nData Solutions: Design, develop, and maintain data solutions that support data generation, collection, and processing.\nData Pipelines: Create and manage data pipelines to extract, transform, and load data (ETL processes) across systems.\nData Quality & Integrity: Ensure data quality and integrity throughout the data lifecycle.\nETL Processes: Implement and optimize ETL processes to efficiently migrate and deploy data across systems and platforms.\nProfessional & Technical Skills:\nMust-Have Skills:\nStrong proficiency in SAP HCM On Premise ABAP.\nExperience with data modeling and database design.\nFamiliarity with ETL tools and processes.\nExpertise in programming languages such as ABAP, SQL, and Python.\nGood-to-Have Skills:\nExperience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nAdditional Information:\nExperience Required: A minimum of 5 years of experience in SAP HCM On Premise ABAP.\nEducational Requirement: The candidate should have a minimum of 15 years of full-time education.","['Sql', 'Python', 'Abap']",2025-06-10 19:36:07
Data Engineer,Accenture India,5-10 Years,,Jaipur,"Consulting, Information Services","Job Description: Data Engineer\nProject Role: Data Engineer\nProject Role Description: Design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform, and load) processes to migrate and deploy data across systems.\nMust-Have Skills: SAP HCM On Premise ABAP\nGood-to-Have Skills: SAP HCM Organizational Management\nExperience Required: Minimum 5 years\nEducational Qualification: 15 years full-time education\nJob Summary:\nAs a Data Engineer, your main responsibility will be to design, develop, and maintain data solutions for generating, collecting, and processing data. You'll work on creating data pipelines, ensuring data quality, and implementing ETL processes to move and deploy data across systems. Your role will be crucial in managing and analyzing data to support business decisions and provide data-driven insights.\nRoles & Responsibilities:\nSubject Matter Expert (SME): Expected to be an SME, collaborate and manage the team to deliver results. Responsible for making key team decisions.\nCollaboration & Teamwork: Work closely with multiple teams and contribute to important decisions and problem-solving efforts.\nData Solutions: Design, develop, and maintain data solutions that support data generation, collection, and processing.\nData Pipelines: Create and manage data pipelines to extract, transform, and load data (ETL processes) across systems.\nData Quality & Integrity: Ensure data quality and integrity throughout the data lifecycle.\nETL Processes: Implement and optimize ETL processes to efficiently migrate and deploy data across systems and platforms.\nProfessional & Technical Skills:\nMust-Have Skills:\nStrong proficiency in SAP HCM On Premise ABAP.\nExperience with data modeling and database design.\nFamiliarity with ETL tools and processes.\nExpertise in programming languages such as ABAP, SQL, and Python.\nGood-to-Have Skills:\nExperience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nAdditional Information:\nExperience Required: A minimum of 5 years of experience in SAP HCM On Premise ABAP.\nEducational Requirement: The candidate should have a minimum of 15 years of full-time education.","['Sql', 'Python', 'Abap']",2025-06-10 19:36:08
Data Engineer,Accenture India,5-10 Years,,Jaipur,"Consulting, Information Services","Job Description: Data Engineer\nProject Role: Data Engineer\nProject Role Description: Design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform, and load) processes to migrate and deploy data across systems.\nMust-Have Skills: SAP HCM On Premise ABAP\nGood-to-Have Skills: SAP HCM Organizational Management\nExperience Required: Minimum 5 years\nEducational Qualification: 15 years full-time education\nJob Summary:\nAs a Data Engineer, your main responsibility will be to design, develop, and maintain data solutions for generating, collecting, and processing data. You'll work on creating data pipelines, ensuring data quality, and implementing ETL processes to move and deploy data across systems. Your role will be crucial in managing and analyzing data to support business decisions and provide data-driven insights.\nRoles & Responsibilities:\nSubject Matter Expert (SME): Expected to be an SME, collaborate and manage the team to deliver results. Responsible for making key team decisions.\nCollaboration & Teamwork: Work closely with multiple teams and contribute to important decisions and problem-solving efforts.\nData Solutions: Design, develop, and maintain data solutions that support data generation, collection, and processing.\nData Pipelines: Create and manage data pipelines to extract, transform, and load data (ETL processes) across systems.\nData Quality & Integrity: Ensure data quality and integrity throughout the data lifecycle.\nETL Processes: Implement and optimize ETL processes to efficiently migrate and deploy data across systems and platforms.\nProfessional & Technical Skills:\nMust-Have Skills:\nStrong proficiency in SAP HCM On Premise ABAP.\nExperience with data modeling and database design.\nFamiliarity with ETL tools and processes.\nExpertise in programming languages such as ABAP, SQL, and Python.\nGood-to-Have Skills:\nExperience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nAdditional Information:\nExperience Required: A minimum of 5 years of experience in SAP HCM On Premise ABAP.\nEducational Requirement: The candidate should have a minimum of 15 years of full-time education.","['Sql', 'Python', 'Abap']",2025-06-10 19:36:09
Data Engineer,Accenture India,5-10 Years,,Jaipur,"Consulting, Information Services","Job Description: Data Engineer\nProject Role: Data Engineer\nProject Role Description: Design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform, and load) processes to migrate and deploy data across systems.\nMust-Have Skills: SAP HCM On Premise ABAP\nGood-to-Have Skills: SAP HCM Organizational Management\nExperience Required: Minimum 5 years\nEducational Qualification: 15 years full-time education\nJob Summary:\nAs a Data Engineer, your main responsibility will be to design, develop, and maintain data solutions for generating, collecting, and processing data. You'll work on creating data pipelines, ensuring data quality, and implementing ETL processes to move and deploy data across systems. Your role will be crucial in managing and analyzing data to support business decisions and provide data-driven insights.\nRoles & Responsibilities:\nSubject Matter Expert (SME): Expected to be an SME, collaborate and manage the team to deliver results. Responsible for making key team decisions.\nCollaboration & Teamwork: Work closely with multiple teams and contribute to important decisions and problem-solving efforts.\nData Solutions: Design, develop, and maintain data solutions that support data generation, collection, and processing.\nData Pipelines: Create and manage data pipelines to extract, transform, and load data (ETL processes) across systems.\nData Quality & Integrity: Ensure data quality and integrity throughout the data lifecycle.\nETL Processes: Implement and optimize ETL processes to efficiently migrate and deploy data across systems and platforms.\nProfessional & Technical Skills:\nMust-Have Skills:\nStrong proficiency in SAP HCM On Premise ABAP.\nExperience with data modeling and database design.\nFamiliarity with ETL tools and processes.\nExpertise in programming languages such as ABAP, SQL, and Python.\nGood-to-Have Skills:\nExperience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nAdditional Information:\nExperience Required: A minimum of 5 years of experience in SAP HCM On Premise ABAP.\nEducational Requirement: The candidate should have a minimum of 15 years of full-time education.","['Sql', 'Python', 'Abap']",2025-06-10 19:36:09
Azure Databricks Data Engineer,Tata Consultancy Services Limited,4-12 Years,,Bengaluru,Software,"Job Description\nImplementation, and operations of OLTP, OLAP, DW technologies such as Azure SQL, Azure SQL DW, , Azure Data Lake Store , Azure Data Factory and understanding of Microsoft Azure PaaS features.\nAzure Cloud ,Azure Databricks, Data Factory knowledge are good to have, otherwise any cloud exposure\nAbility to gather requirements from client side and explain to tech team members.\nResolve conflicts in terms of bandwidth or design issues.\nGood understanding of data modeling, dataanalysis, data governance.\nVery good communication skills and client handlingskills","['Olap', 'Azure', 'Sql']",2025-06-10 19:36:17
Azure Databricks Data Engineer,Tata Consultancy Services Limited,4-12 Years,,Bengaluru,Software,"Job Description\nImplementation, and operations of OLTP, OLAP, DW technologies such as Azure SQL, Azure SQL DW, , Azure Data Lake Store , Azure Data Factory and understanding of Microsoft Azure PaaS features.\nAzure Cloud ,Azure Databricks, Data Factory knowledge are good to have, otherwise any cloud exposure\nAbility to gather requirements from client side and explain to tech team members.\nResolve conflicts in terms of bandwidth or design issues.\nGood understanding of data modeling, dataanalysis, data governance.\nVery good communication skills and client handlingskills","['Olap', 'Azure', 'Sql']",2025-06-10 19:36:19
Azure Databricks Data Engineer,Tata Consultancy Services Limited,4-12 Years,,Bengaluru,Software,"Job Description\nImplementation, and operations of OLTP, OLAP, DW technologies such as Azure SQL, Azure SQL DW, , Azure Data Lake Store , Azure Data Factory and understanding of Microsoft Azure PaaS features.\nAzure Cloud ,Azure Databricks, Data Factory knowledge are good to have, otherwise any cloud exposure\nAbility to gather requirements from client side and explain to tech team members.\nResolve conflicts in terms of bandwidth or design issues.\nGood understanding of data modeling, dataanalysis, data governance.\nVery good communication skills and client handlingskills","['Olap', 'Azure', 'Sql']",2025-06-10 19:36:22
Azure Databricks Data Engineer,Tata Consultancy Services Limited,4-12 Years,,Bengaluru,Software,"Job Description\nImplementation, and operations of OLTP, OLAP, DW technologies such as Azure SQL, Azure SQL DW, , Azure Data Lake Store , Azure Data Factory and understanding of Microsoft Azure PaaS features.\nAzure Cloud ,Azure Databricks, Data Factory knowledge are good to have, otherwise any cloud exposure\nAbility to gather requirements from client side and explain to tech team members.\nResolve conflicts in terms of bandwidth or design issues.\nGood understanding of data modeling, dataanalysis, data governance.\nVery good communication skills and client handlingskills","['Olap', 'Azure', 'Sql']",2025-06-10 19:36:24
Azure Databricks Data Engineer,Tata Consultancy Services Limited,4-12 Years,,Bengaluru,Software,"Job Description\nImplementation, and operations of OLTP, OLAP, DW technologies such as Azure SQL, Azure SQL DW, , Azure Data Lake Store , Azure Data Factory and understanding of Microsoft Azure PaaS features.\nAzure Cloud ,Azure Databricks, Data Factory knowledge are good to have, otherwise any cloud exposure\nAbility to gather requirements from client side and explain to tech team members.\nResolve conflicts in terms of bandwidth or design issues.\nGood understanding of data modeling, dataanalysis, data governance.\nVery good communication skills and client handlingskills","['Olap', 'Azure', 'Sql']",2025-06-10 19:36:25
Azure Databricks Data Engineer,Tata Consultancy Services Limited,4-12 Years,,Bengaluru,Software,"Job Description\nImplementation, and operations of OLTP, OLAP, DW technologies such as Azure SQL, Azure SQL DW, , Azure Data Lake Store , Azure Data Factory and understanding of Microsoft Azure PaaS features.\nAzure Cloud ,Azure Databricks, Data Factory knowledge are good to have, otherwise any cloud exposure\nAbility to gather requirements from client side and explain to tech team members.\nResolve conflicts in terms of bandwidth or design issues.\nGood understanding of data modeling, dataanalysis, data governance.\nVery good communication skills and client handlingskills","['Olap', 'Azure', 'Sql']",2025-06-10 19:36:25
Azure Databricks Data Engineer,Tata Consultancy Services Limited,4-12 Years,,Bengaluru,Software,"Job Description\nImplementation, and operations of OLTP, OLAP, DW technologies such as Azure SQL, Azure SQL DW, , Azure Data Lake Store , Azure Data Factory and understanding of Microsoft Azure PaaS features.\nAzure Cloud ,Azure Databricks, Data Factory knowledge are good to have, otherwise any cloud exposure\nAbility to gather requirements from client side and explain to tech team members.\nResolve conflicts in terms of bandwidth or design issues.\nGood understanding of data modeling, dataanalysis, data governance.\nVery good communication skills and client handlingskills","['Olap', 'Azure', 'Sql']",2025-06-10 19:36:26
Azure Databricks Data Engineer,Tata Consultancy Services Limited,4-12 Years,,Bengaluru,Software,"Job Description\nImplementation, and operations of OLTP, OLAP, DW technologies such as Azure SQL, Azure SQL DW, , Azure Data Lake Store , Azure Data Factory and understanding of Microsoft Azure PaaS features.\nAzure Cloud ,Azure Databricks, Data Factory knowledge are good to have, otherwise any cloud exposure\nAbility to gather requirements from client side and explain to tech team members.\nResolve conflicts in terms of bandwidth or design issues.\nGood understanding of data modeling, dataanalysis, data governance.\nVery good communication skills and client handlingskills","['Olap', 'Azure', 'Sql']",2025-06-10 19:36:27
Azure Databricks Data Engineer,Tata Consultancy Services Limited,4-12 Years,,Bengaluru,Software,"Job Description\nImplementation, and operations of OLTP, OLAP, DW technologies such as Azure SQL, Azure SQL DW, , Azure Data Lake Store , Azure Data Factory and understanding of Microsoft Azure PaaS features.\nAzure Cloud ,Azure Databricks, Data Factory knowledge are good to have, otherwise any cloud exposure\nAbility to gather requirements from client side and explain to tech team members.\nResolve conflicts in terms of bandwidth or design issues.\nGood understanding of data modeling, dataanalysis, data governance.\nVery good communication skills and client handlingskills","['Olap', 'Azure', 'Sql']",2025-06-10 19:36:28
Azure Databricks Data Engineer,Tata Consultancy Services Limited,4-12 Years,,Bengaluru,Software,"Job Description\nImplementation, and operations of OLTP, OLAP, DW technologies such as Azure SQL, Azure SQL DW, , Azure Data Lake Store , Azure Data Factory and understanding of Microsoft Azure PaaS features.\nAzure Cloud ,Azure Databricks, Data Factory knowledge are good to have, otherwise any cloud exposure\nAbility to gather requirements from client side and explain to tech team members.\nResolve conflicts in terms of bandwidth or design issues.\nGood understanding of data modeling, dataanalysis, data governance.\nVery good communication skills and client handlingskills","['Olap', 'Azure', 'Sql']",2025-06-10 19:36:28
Azure Databricks Data Engineer,Tata Consultancy Services Limited,4-12 Years,,Bengaluru,Software,"Job Description\nImplementation, and operations of OLTP, OLAP, DW technologies such as Azure SQL, Azure SQL DW, , Azure Data Lake Store , Azure Data Factory and understanding of Microsoft Azure PaaS features.\nAzure Cloud ,Azure Databricks, Data Factory knowledge are good to have, otherwise any cloud exposure\nAbility to gather requirements from client side and explain to tech team members.\nResolve conflicts in terms of bandwidth or design issues.\nGood understanding of data modeling, dataanalysis, data governance.\nVery good communication skills and client handlingskills","['Olap', 'Azure', 'Sql']",2025-06-10 19:36:29
Azure Databricks Data Engineer,Tata Consultancy Services Limited,4-12 Years,,Bengaluru,Software,"Job Description\nImplementation, and operations of OLTP, OLAP, DW technologies such as Azure SQL, Azure SQL DW, , Azure Data Lake Store , Azure Data Factory and understanding of Microsoft Azure PaaS features.\nAzure Cloud ,Azure Databricks, Data Factory knowledge are good to have, otherwise any cloud exposure\nAbility to gather requirements from client side and explain to tech team members.\nResolve conflicts in terms of bandwidth or design issues.\nGood understanding of data modeling, dataanalysis, data governance.\nVery good communication skills and client handlingskills","['Olap', 'Azure', 'Sql']",2025-06-10 19:36:30
Azure Databricks Data Engineer,Tata Consultancy Services Limited,4-12 Years,,Bengaluru,Software,"Job Description\nImplementation, and operations of OLTP, OLAP, DW technologies such as Azure SQL, Azure SQL DW, , Azure Data Lake Store , Azure Data Factory and understanding of Microsoft Azure PaaS features.\nAzure Cloud ,Azure Databricks, Data Factory knowledge are good to have, otherwise any cloud exposure\nAbility to gather requirements from client side and explain to tech team members.\nResolve conflicts in terms of bandwidth or design issues.\nGood understanding of data modeling, dataanalysis, data governance.\nVery good communication skills and client handlingskills","['Olap', 'Azure', 'Sql']",2025-06-10 19:36:31
Azure Databricks Data Engineer,Tata Consultancy Services Limited,4-12 Years,,Bengaluru,Software,"Job Description\nImplementation, and operations of OLTP, OLAP, DW technologies such as Azure SQL, Azure SQL DW, , Azure Data Lake Store , Azure Data Factory and understanding of Microsoft Azure PaaS features.\nAzure Cloud ,Azure Databricks, Data Factory knowledge are good to have, otherwise any cloud exposure\nAbility to gather requirements from client side and explain to tech team members.\nResolve conflicts in terms of bandwidth or design issues.\nGood understanding of data modeling, dataanalysis, data governance.\nVery good communication skills and client handlingskills","['Olap', 'Azure', 'Sql']",2025-06-10 19:36:32
Azure Databricks Data Engineer,Tata Consultancy Services Limited,4-12 Years,,Bengaluru,Software,"Job Description\nImplementation, and operations of OLTP, OLAP, DW technologies such as Azure SQL, Azure SQL DW, , Azure Data Lake Store , Azure Data Factory and understanding of Microsoft Azure PaaS features.\nAzure Cloud ,Azure Databricks, Data Factory knowledge are good to have, otherwise any cloud exposure\nAbility to gather requirements from client side and explain to tech team members.\nResolve conflicts in terms of bandwidth or design issues.\nGood understanding of data modeling, dataanalysis, data governance.\nVery good communication skills and client handlingskills","['Olap', 'Azure', 'Sql']",2025-06-10 19:36:32
Data Engineer-Data Platforms-Azure,IBM,3-6 Years,,Bengaluru,Software,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nWe are seeking a skilled Azure Data Engineer with 5+ years of experience\nIncluding 3+ years of hands-on experience with ADF/Databricks\nThe ideal candidate Data bricks, Data Lake, Python programming skills.\nThe candidate will also have experience for deploying to data bricks.\nFamiliarity with Azure Data Factory\nPreferred technical and professional experience\nGood communication skills.\n3+ years of experience with ADF/DB/DataLake.\nAbility to communicate results to technical and non-technical audiences","['Azure', 'Databricks', 'Data Lake', 'Python', 'Adf', 'Elasticsearch']",2025-06-10 19:36:40
Data Engineer-Data Platforms-Azure,IBM,3-6 Years,,Bengaluru,Software,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nWe are seeking a skilled Azure Data Engineer with 5+ years of experience\nIncluding 3+ years of hands-on experience with ADF/Databricks\nThe ideal candidate Data bricks, Data Lake, Python programming skills.\nThe candidate will also have experience for deploying to data bricks.\nFamiliarity with Azure Data Factory\nPreferred technical and professional experience\nGood communication skills.\n3+ years of experience with ADF/DB/DataLake.\nAbility to communicate results to technical and non-technical audiences","['Azure', 'Databricks', 'Data Lake', 'Python', 'Adf', 'Elasticsearch']",2025-06-10 19:36:44
Data Engineer-Data Platforms-Azure,IBM,3-6 Years,,Bengaluru,Software,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nWe are seeking a skilled Azure Data Engineer with 5+ years of experience\nIncluding 3+ years of hands-on experience with ADF/Databricks\nThe ideal candidate Data bricks, Data Lake, Python programming skills.\nThe candidate will also have experience for deploying to data bricks.\nFamiliarity with Azure Data Factory\nPreferred technical and professional experience\nGood communication skills.\n3+ years of experience with ADF/DB/DataLake.\nAbility to communicate results to technical and non-technical audiences","['Azure', 'Databricks', 'Data Lake', 'Python', 'Adf', 'Elasticsearch']",2025-06-10 19:36:46
Data Engineer-Data Platforms-Azure,IBM,3-6 Years,,Bengaluru,Software,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nWe are seeking a skilled Azure Data Engineer with 5+ years of experience\nIncluding 3+ years of hands-on experience with ADF/Databricks\nThe ideal candidate Data bricks, Data Lake, Python programming skills.\nThe candidate will also have experience for deploying to data bricks.\nFamiliarity with Azure Data Factory\nPreferred technical and professional experience\nGood communication skills.\n3+ years of experience with ADF/DB/DataLake.\nAbility to communicate results to technical and non-technical audiences","['Azure', 'Databricks', 'Data Lake', 'Python', 'Adf', 'Elasticsearch']",2025-06-10 19:36:47
Data Engineer-Data Platforms-Azure,IBM,3-6 Years,,Bengaluru,Software,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nWe are seeking a skilled Azure Data Engineer with 5+ years of experience\nIncluding 3+ years of hands-on experience with ADF/Databricks\nThe ideal candidate Data bricks, Data Lake, Python programming skills.\nThe candidate will also have experience for deploying to data bricks.\nFamiliarity with Azure Data Factory\nPreferred technical and professional experience\nGood communication skills.\n3+ years of experience with ADF/DB/DataLake.\nAbility to communicate results to technical and non-technical audiences","['Azure', 'Databricks', 'Data Lake', 'Python', 'Adf', 'Elasticsearch']",2025-06-10 19:36:48
Data Engineer-Data Platforms-Azure,IBM,3-6 Years,,Bengaluru,Software,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nWe are seeking a skilled Azure Data Engineer with 5+ years of experience\nIncluding 3+ years of hands-on experience with ADF/Databricks\nThe ideal candidate Data bricks, Data Lake, Python programming skills.\nThe candidate will also have experience for deploying to data bricks.\nFamiliarity with Azure Data Factory\nPreferred technical and professional experience\nGood communication skills.\n3+ years of experience with ADF/DB/DataLake.\nAbility to communicate results to technical and non-technical audiences","['Azure', 'Databricks', 'Data Lake', 'Python', 'Adf', 'Elasticsearch']",2025-06-10 19:36:48
Data Engineer-Data Platforms-Azure,IBM,3-6 Years,,Bengaluru,Software,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nWe are seeking a skilled Azure Data Engineer with 5+ years of experience\nIncluding 3+ years of hands-on experience with ADF/Databricks\nThe ideal candidate Data bricks, Data Lake, Python programming skills.\nThe candidate will also have experience for deploying to data bricks.\nFamiliarity with Azure Data Factory\nPreferred technical and professional experience\nGood communication skills.\n3+ years of experience with ADF/DB/DataLake.\nAbility to communicate results to technical and non-technical audiences","['Azure', 'Databricks', 'Data Lake', 'Python', 'Adf', 'Elasticsearch']",2025-06-10 19:36:49
Data Engineer-Data Platforms-Azure,IBM,3-6 Years,,Bengaluru,Software,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nWe are seeking a skilled Azure Data Engineer with 5+ years of experience\nIncluding 3+ years of hands-on experience with ADF/Databricks\nThe ideal candidate Data bricks, Data Lake, Python programming skills.\nThe candidate will also have experience for deploying to data bricks.\nFamiliarity with Azure Data Factory\nPreferred technical and professional experience\nGood communication skills.\n3+ years of experience with ADF/DB/DataLake.\nAbility to communicate results to technical and non-technical audiences","['Azure', 'Databricks', 'Data Lake', 'Python', 'Adf', 'Elasticsearch']",2025-06-10 19:36:50
Data Engineer-Data Platforms-Azure,IBM,3-6 Years,,Bengaluru,Software,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nWe are seeking a skilled Azure Data Engineer with 5+ years of experience\nIncluding 3+ years of hands-on experience with ADF/Databricks\nThe ideal candidate Data bricks, Data Lake, Python programming skills.\nThe candidate will also have experience for deploying to data bricks.\nFamiliarity with Azure Data Factory\nPreferred technical and professional experience\nGood communication skills.\n3+ years of experience with ADF/DB/DataLake.\nAbility to communicate results to technical and non-technical audiences","['Azure', 'Databricks', 'Data Lake', 'Python', 'Adf', 'Elasticsearch']",2025-06-10 19:36:51
Data Engineer-Data Platforms-Azure,IBM,3-6 Years,,Bengaluru,Software,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nWe are seeking a skilled Azure Data Engineer with 5+ years of experience\nIncluding 3+ years of hands-on experience with ADF/Databricks\nThe ideal candidate Data bricks, Data Lake, Python programming skills.\nThe candidate will also have experience for deploying to data bricks.\nFamiliarity with Azure Data Factory\nPreferred technical and professional experience\nGood communication skills.\n3+ years of experience with ADF/DB/DataLake.\nAbility to communicate results to technical and non-technical audiences","['Azure', 'Databricks', 'Data Lake', 'Python', 'Adf', 'Elasticsearch']",2025-06-10 19:36:52
Data Engineer-Data Platforms-Azure,IBM,3-6 Years,,Bengaluru,Software,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nWe are seeking a skilled Azure Data Engineer with 5+ years of experience\nIncluding 3+ years of hands-on experience with ADF/Databricks\nThe ideal candidate Data bricks, Data Lake, Python programming skills.\nThe candidate will also have experience for deploying to data bricks.\nFamiliarity with Azure Data Factory\nPreferred technical and professional experience\nGood communication skills.\n3+ years of experience with ADF/DB/DataLake.\nAbility to communicate results to technical and non-technical audiences","['Azure', 'Databricks', 'Data Lake', 'Python', 'Adf', 'Elasticsearch']",2025-06-10 19:36:52
Data Engineer-Data Platforms-Azure,IBM,3-6 Years,,Bengaluru,Software,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nWe are seeking a skilled Azure Data Engineer with 5+ years of experience\nIncluding 3+ years of hands-on experience with ADF/Databricks\nThe ideal candidate Data bricks, Data Lake, Python programming skills.\nThe candidate will also have experience for deploying to data bricks.\nFamiliarity with Azure Data Factory\nPreferred technical and professional experience\nGood communication skills.\n3+ years of experience with ADF/DB/DataLake.\nAbility to communicate results to technical and non-technical audiences","['Azure', 'Databricks', 'Data Lake', 'Python', 'Adf', 'Elasticsearch']",2025-06-10 19:36:53
Data Engineer-Data Platforms-Azure,IBM,3-6 Years,,Bengaluru,Software,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nWe are seeking a skilled Azure Data Engineer with 5+ years of experience\nIncluding 3+ years of hands-on experience with ADF/Databricks\nThe ideal candidate Data bricks, Data Lake, Python programming skills.\nThe candidate will also have experience for deploying to data bricks.\nFamiliarity with Azure Data Factory\nPreferred technical and professional experience\nGood communication skills.\n3+ years of experience with ADF/DB/DataLake.\nAbility to communicate results to technical and non-technical audiences","['Azure', 'Databricks', 'Data Lake', 'Python', 'Adf', 'Elasticsearch']",2025-06-10 19:36:54
Data Engineer-Data Platforms-Azure,IBM,3-6 Years,,Bengaluru,Software,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nWe are seeking a skilled Azure Data Engineer with 5+ years of experience\nIncluding 3+ years of hands-on experience with ADF/Databricks\nThe ideal candidate Data bricks, Data Lake, Python programming skills.\nThe candidate will also have experience for deploying to data bricks.\nFamiliarity with Azure Data Factory\nPreferred technical and professional experience\nGood communication skills.\n3+ years of experience with ADF/DB/DataLake.\nAbility to communicate results to technical and non-technical audiences","['Azure', 'Databricks', 'Data Lake', 'Python', 'Adf', 'Elasticsearch']",2025-06-10 19:36:55
Data Engineer-Data Platforms-Azure,IBM,3-6 Years,,Bengaluru,Software,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nWe are seeking a skilled Azure Data Engineer with 5+ years of experience\nIncluding 3+ years of hands-on experience with ADF/Databricks\nThe ideal candidate Data bricks, Data Lake, Python programming skills.\nThe candidate will also have experience for deploying to data bricks.\nFamiliarity with Azure Data Factory\nPreferred technical and professional experience\nGood communication skills.\n3+ years of experience with ADF/DB/DataLake.\nAbility to communicate results to technical and non-technical audiences","['Azure', 'Databricks', 'Data Lake', 'Python', 'Adf', 'Elasticsearch']",2025-06-10 19:36:56
Senior Data Engineer ( Fabric ),Aspire Systems India Private Limited,10-15 Years,,Chennai,Software,"We are looking for a Senior Data Engineer with extensive experience in developing ETL processes using PySpark Notebooks and Microsoft Fabric, and supporting existing legacy SQL Server environments. The ideal candidate will possess a strong background in Spark-based development, demonstrate a high proficiency in SQL, and be comfortable working independently, collaboratively within a team, or leading other developers when required, coupled with strong communication skills.\nRequirements:\nWe are looking for a Senior Data Engineer with extensive experience in developing ETL processes using PySpark Notebooks and Microsoft Fabric, and supporting existing legacy SQL Server environments. The ideal candidate will possess a strong background in Spark-based development, demonstrate a high proficiency in SQL, and be comfortable working independently, collaboratively within a team, or leading other developers when required, coupled with strong communication skills. The ideal candidate will possess Experience with Azure Data Services, including Azure Data Factory, Azure Synapse or similar tools,Experience of creating DAGs, implementing activities, and running Apache Airflow and Familiarity with DevOps practices, CI/CD pipelines and Azure DevOps.\nThe ideal candidate should have:\nKey Responsibilities:\nDesign, develop, and maintain ETL Notebook orchestration pipelines using PySpark and Microsoft Fabric.\nWorking with Apache Delta Lake tables, Change Data Feed (CDF), Lakehouses and custom libraries\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver efficient data solutions.\nMigrate and integrate data from legacy SQL Server environments into modern data platforms.\nOptimize data pipelines and workflows for scalability, efficiency, and reliability.\nProvide technical leadership and mentorship to junior developers and other team members.\nTroubleshoot and resolve complex data engineering issues related to performance, data quality, and system scalability.\nDebugging of code, breaking down to test components, identify issues and resolve\nDevelop, maintain, and enforce data engineering best practices, coding standards, and documentation.\nConduct code reviews and provide constructive feedback to improve team productivity and code quality.\nSupport data-driven decision-making processes by ensuring data integrity, availability, and consistency across different platforms.\nQualifications:\nBachelor s or Master s degree in Computer Science, Data Science, Engineering, or a related field.\n10+ years of experience in data engineering, with a strong focus on ETL development using PySpark or other Spark-based tools.\nProficiency in SQL with extensive experience in complex queries, performance tuning, and data modeling.\nExperience with Microsoft Fabric or similar cloud-based data integration platforms is a plus.\nStrong knowledge of data warehousing concepts, ETL frameworks, and big data processing.\nFamiliarity with other data processing technologies (e.g., Hadoop, Hive, Kafka) is an advantage.\nExperience working with both structured and unstructured data sources.\nExcellent problem-solving skills and the ability to troubleshoot complex data engineering issues.\nExperience with Azure Data Services, including Azure Data Factory, Azure Synapse, or similar tools.\nExperience of creating DAGs, implementing activities, and running Apache Airflow\nFamiliarity with DevOps practices, CI/CD pipelines and Azure DevOps.","['CI/CD', 'Senior Data Engineer ( Fabric )', 'Sql', 'Etl']",2025-06-10 19:37:04
Senior Data Engineer ( Fabric ),Aspire Systems India Private Limited,10-15 Years,,Chennai,Software,"We are looking for a Senior Data Engineer with extensive experience in developing ETL processes using PySpark Notebooks and Microsoft Fabric, and supporting existing legacy SQL Server environments. The ideal candidate will possess a strong background in Spark-based development, demonstrate a high proficiency in SQL, and be comfortable working independently, collaboratively within a team, or leading other developers when required, coupled with strong communication skills.\nRequirements:\nWe are looking for a Senior Data Engineer with extensive experience in developing ETL processes using PySpark Notebooks and Microsoft Fabric, and supporting existing legacy SQL Server environments. The ideal candidate will possess a strong background in Spark-based development, demonstrate a high proficiency in SQL, and be comfortable working independently, collaboratively within a team, or leading other developers when required, coupled with strong communication skills. The ideal candidate will possess Experience with Azure Data Services, including Azure Data Factory, Azure Synapse or similar tools,Experience of creating DAGs, implementing activities, and running Apache Airflow and Familiarity with DevOps practices, CI/CD pipelines and Azure DevOps.\nThe ideal candidate should have:\nKey Responsibilities:\nDesign, develop, and maintain ETL Notebook orchestration pipelines using PySpark and Microsoft Fabric.\nWorking with Apache Delta Lake tables, Change Data Feed (CDF), Lakehouses and custom libraries\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver efficient data solutions.\nMigrate and integrate data from legacy SQL Server environments into modern data platforms.\nOptimize data pipelines and workflows for scalability, efficiency, and reliability.\nProvide technical leadership and mentorship to junior developers and other team members.\nTroubleshoot and resolve complex data engineering issues related to performance, data quality, and system scalability.\nDebugging of code, breaking down to test components, identify issues and resolve\nDevelop, maintain, and enforce data engineering best practices, coding standards, and documentation.\nConduct code reviews and provide constructive feedback to improve team productivity and code quality.\nSupport data-driven decision-making processes by ensuring data integrity, availability, and consistency across different platforms.\nQualifications:\nBachelor s or Master s degree in Computer Science, Data Science, Engineering, or a related field.\n10+ years of experience in data engineering, with a strong focus on ETL development using PySpark or other Spark-based tools.\nProficiency in SQL with extensive experience in complex queries, performance tuning, and data modeling.\nExperience with Microsoft Fabric or similar cloud-based data integration platforms is a plus.\nStrong knowledge of data warehousing concepts, ETL frameworks, and big data processing.\nFamiliarity with other data processing technologies (e.g., Hadoop, Hive, Kafka) is an advantage.\nExperience working with both structured and unstructured data sources.\nExcellent problem-solving skills and the ability to troubleshoot complex data engineering issues.\nExperience with Azure Data Services, including Azure Data Factory, Azure Synapse, or similar tools.\nExperience of creating DAGs, implementing activities, and running Apache Airflow\nFamiliarity with DevOps practices, CI/CD pipelines and Azure DevOps.","['CI/CD', 'Senior Data Engineer ( Fabric )', 'Sql', 'Etl']",2025-06-10 19:37:07
Senior Data Engineer ( Fabric ),Aspire Systems India Private Limited,10-15 Years,,Chennai,Software,"We are looking for a Senior Data Engineer with extensive experience in developing ETL processes using PySpark Notebooks and Microsoft Fabric, and supporting existing legacy SQL Server environments. The ideal candidate will possess a strong background in Spark-based development, demonstrate a high proficiency in SQL, and be comfortable working independently, collaboratively within a team, or leading other developers when required, coupled with strong communication skills.\nRequirements:\nWe are looking for a Senior Data Engineer with extensive experience in developing ETL processes using PySpark Notebooks and Microsoft Fabric, and supporting existing legacy SQL Server environments. The ideal candidate will possess a strong background in Spark-based development, demonstrate a high proficiency in SQL, and be comfortable working independently, collaboratively within a team, or leading other developers when required, coupled with strong communication skills. The ideal candidate will possess Experience with Azure Data Services, including Azure Data Factory, Azure Synapse or similar tools,Experience of creating DAGs, implementing activities, and running Apache Airflow and Familiarity with DevOps practices, CI/CD pipelines and Azure DevOps.\nThe ideal candidate should have:\nKey Responsibilities:\nDesign, develop, and maintain ETL Notebook orchestration pipelines using PySpark and Microsoft Fabric.\nWorking with Apache Delta Lake tables, Change Data Feed (CDF), Lakehouses and custom libraries\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver efficient data solutions.\nMigrate and integrate data from legacy SQL Server environments into modern data platforms.\nOptimize data pipelines and workflows for scalability, efficiency, and reliability.\nProvide technical leadership and mentorship to junior developers and other team members.\nTroubleshoot and resolve complex data engineering issues related to performance, data quality, and system scalability.\nDebugging of code, breaking down to test components, identify issues and resolve\nDevelop, maintain, and enforce data engineering best practices, coding standards, and documentation.\nConduct code reviews and provide constructive feedback to improve team productivity and code quality.\nSupport data-driven decision-making processes by ensuring data integrity, availability, and consistency across different platforms.\nQualifications:\nBachelor s or Master s degree in Computer Science, Data Science, Engineering, or a related field.\n10+ years of experience in data engineering, with a strong focus on ETL development using PySpark or other Spark-based tools.\nProficiency in SQL with extensive experience in complex queries, performance tuning, and data modeling.\nExperience with Microsoft Fabric or similar cloud-based data integration platforms is a plus.\nStrong knowledge of data warehousing concepts, ETL frameworks, and big data processing.\nFamiliarity with other data processing technologies (e.g., Hadoop, Hive, Kafka) is an advantage.\nExperience working with both structured and unstructured data sources.\nExcellent problem-solving skills and the ability to troubleshoot complex data engineering issues.\nExperience with Azure Data Services, including Azure Data Factory, Azure Synapse, or similar tools.\nExperience of creating DAGs, implementing activities, and running Apache Airflow\nFamiliarity with DevOps practices, CI/CD pipelines and Azure DevOps.","['CI/CD', 'Senior Data Engineer ( Fabric )', 'Sql', 'Etl']",2025-06-10 19:37:10
Senior Data Engineer ( Fabric ),Aspire Systems India Private Limited,10-15 Years,,Chennai,Software,"We are looking for a Senior Data Engineer with extensive experience in developing ETL processes using PySpark Notebooks and Microsoft Fabric, and supporting existing legacy SQL Server environments. The ideal candidate will possess a strong background in Spark-based development, demonstrate a high proficiency in SQL, and be comfortable working independently, collaboratively within a team, or leading other developers when required, coupled with strong communication skills.\nRequirements:\nWe are looking for a Senior Data Engineer with extensive experience in developing ETL processes using PySpark Notebooks and Microsoft Fabric, and supporting existing legacy SQL Server environments. The ideal candidate will possess a strong background in Spark-based development, demonstrate a high proficiency in SQL, and be comfortable working independently, collaboratively within a team, or leading other developers when required, coupled with strong communication skills. The ideal candidate will possess Experience with Azure Data Services, including Azure Data Factory, Azure Synapse or similar tools,Experience of creating DAGs, implementing activities, and running Apache Airflow and Familiarity with DevOps practices, CI/CD pipelines and Azure DevOps.\nThe ideal candidate should have:\nKey Responsibilities:\nDesign, develop, and maintain ETL Notebook orchestration pipelines using PySpark and Microsoft Fabric.\nWorking with Apache Delta Lake tables, Change Data Feed (CDF), Lakehouses and custom libraries\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver efficient data solutions.\nMigrate and integrate data from legacy SQL Server environments into modern data platforms.\nOptimize data pipelines and workflows for scalability, efficiency, and reliability.\nProvide technical leadership and mentorship to junior developers and other team members.\nTroubleshoot and resolve complex data engineering issues related to performance, data quality, and system scalability.\nDebugging of code, breaking down to test components, identify issues and resolve\nDevelop, maintain, and enforce data engineering best practices, coding standards, and documentation.\nConduct code reviews and provide constructive feedback to improve team productivity and code quality.\nSupport data-driven decision-making processes by ensuring data integrity, availability, and consistency across different platforms.\nQualifications:\nBachelor s or Master s degree in Computer Science, Data Science, Engineering, or a related field.\n10+ years of experience in data engineering, with a strong focus on ETL development using PySpark or other Spark-based tools.\nProficiency in SQL with extensive experience in complex queries, performance tuning, and data modeling.\nExperience with Microsoft Fabric or similar cloud-based data integration platforms is a plus.\nStrong knowledge of data warehousing concepts, ETL frameworks, and big data processing.\nFamiliarity with other data processing technologies (e.g., Hadoop, Hive, Kafka) is an advantage.\nExperience working with both structured and unstructured data sources.\nExcellent problem-solving skills and the ability to troubleshoot complex data engineering issues.\nExperience with Azure Data Services, including Azure Data Factory, Azure Synapse, or similar tools.\nExperience of creating DAGs, implementing activities, and running Apache Airflow\nFamiliarity with DevOps practices, CI/CD pipelines and Azure DevOps.","['CI/CD', 'Senior Data Engineer ( Fabric )', 'Sql', 'Etl']",2025-06-10 19:37:11
Senior Data Engineer ( Fabric ),Aspire Systems India Private Limited,10-15 Years,,Chennai,Software,"We are looking for a Senior Data Engineer with extensive experience in developing ETL processes using PySpark Notebooks and Microsoft Fabric, and supporting existing legacy SQL Server environments. The ideal candidate will possess a strong background in Spark-based development, demonstrate a high proficiency in SQL, and be comfortable working independently, collaboratively within a team, or leading other developers when required, coupled with strong communication skills.\nRequirements:\nWe are looking for a Senior Data Engineer with extensive experience in developing ETL processes using PySpark Notebooks and Microsoft Fabric, and supporting existing legacy SQL Server environments. The ideal candidate will possess a strong background in Spark-based development, demonstrate a high proficiency in SQL, and be comfortable working independently, collaboratively within a team, or leading other developers when required, coupled with strong communication skills. The ideal candidate will possess Experience with Azure Data Services, including Azure Data Factory, Azure Synapse or similar tools,Experience of creating DAGs, implementing activities, and running Apache Airflow and Familiarity with DevOps practices, CI/CD pipelines and Azure DevOps.\nThe ideal candidate should have:\nKey Responsibilities:\nDesign, develop, and maintain ETL Notebook orchestration pipelines using PySpark and Microsoft Fabric.\nWorking with Apache Delta Lake tables, Change Data Feed (CDF), Lakehouses and custom libraries\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver efficient data solutions.\nMigrate and integrate data from legacy SQL Server environments into modern data platforms.\nOptimize data pipelines and workflows for scalability, efficiency, and reliability.\nProvide technical leadership and mentorship to junior developers and other team members.\nTroubleshoot and resolve complex data engineering issues related to performance, data quality, and system scalability.\nDebugging of code, breaking down to test components, identify issues and resolve\nDevelop, maintain, and enforce data engineering best practices, coding standards, and documentation.\nConduct code reviews and provide constructive feedback to improve team productivity and code quality.\nSupport data-driven decision-making processes by ensuring data integrity, availability, and consistency across different platforms.\nQualifications:\nBachelor s or Master s degree in Computer Science, Data Science, Engineering, or a related field.\n10+ years of experience in data engineering, with a strong focus on ETL development using PySpark or other Spark-based tools.\nProficiency in SQL with extensive experience in complex queries, performance tuning, and data modeling.\nExperience with Microsoft Fabric or similar cloud-based data integration platforms is a plus.\nStrong knowledge of data warehousing concepts, ETL frameworks, and big data processing.\nFamiliarity with other data processing technologies (e.g., Hadoop, Hive, Kafka) is an advantage.\nExperience working with both structured and unstructured data sources.\nExcellent problem-solving skills and the ability to troubleshoot complex data engineering issues.\nExperience with Azure Data Services, including Azure Data Factory, Azure Synapse, or similar tools.\nExperience of creating DAGs, implementing activities, and running Apache Airflow\nFamiliarity with DevOps practices, CI/CD pipelines and Azure DevOps.","['CI/CD', 'Senior Data Engineer ( Fabric )', 'Sql', 'Etl']",2025-06-10 19:37:12
Senior Data Engineer ( Fabric ),Aspire Systems India Private Limited,10-15 Years,,Chennai,Software,"We are looking for a Senior Data Engineer with extensive experience in developing ETL processes using PySpark Notebooks and Microsoft Fabric, and supporting existing legacy SQL Server environments. The ideal candidate will possess a strong background in Spark-based development, demonstrate a high proficiency in SQL, and be comfortable working independently, collaboratively within a team, or leading other developers when required, coupled with strong communication skills.\nRequirements:\nWe are looking for a Senior Data Engineer with extensive experience in developing ETL processes using PySpark Notebooks and Microsoft Fabric, and supporting existing legacy SQL Server environments. The ideal candidate will possess a strong background in Spark-based development, demonstrate a high proficiency in SQL, and be comfortable working independently, collaboratively within a team, or leading other developers when required, coupled with strong communication skills. The ideal candidate will possess Experience with Azure Data Services, including Azure Data Factory, Azure Synapse or similar tools,Experience of creating DAGs, implementing activities, and running Apache Airflow and Familiarity with DevOps practices, CI/CD pipelines and Azure DevOps.\nThe ideal candidate should have:\nKey Responsibilities:\nDesign, develop, and maintain ETL Notebook orchestration pipelines using PySpark and Microsoft Fabric.\nWorking with Apache Delta Lake tables, Change Data Feed (CDF), Lakehouses and custom libraries\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver efficient data solutions.\nMigrate and integrate data from legacy SQL Server environments into modern data platforms.\nOptimize data pipelines and workflows for scalability, efficiency, and reliability.\nProvide technical leadership and mentorship to junior developers and other team members.\nTroubleshoot and resolve complex data engineering issues related to performance, data quality, and system scalability.\nDebugging of code, breaking down to test components, identify issues and resolve\nDevelop, maintain, and enforce data engineering best practices, coding standards, and documentation.\nConduct code reviews and provide constructive feedback to improve team productivity and code quality.\nSupport data-driven decision-making processes by ensuring data integrity, availability, and consistency across different platforms.\nQualifications:\nBachelor s or Master s degree in Computer Science, Data Science, Engineering, or a related field.\n10+ years of experience in data engineering, with a strong focus on ETL development using PySpark or other Spark-based tools.\nProficiency in SQL with extensive experience in complex queries, performance tuning, and data modeling.\nExperience with Microsoft Fabric or similar cloud-based data integration platforms is a plus.\nStrong knowledge of data warehousing concepts, ETL frameworks, and big data processing.\nFamiliarity with other data processing technologies (e.g., Hadoop, Hive, Kafka) is an advantage.\nExperience working with both structured and unstructured data sources.\nExcellent problem-solving skills and the ability to troubleshoot complex data engineering issues.\nExperience with Azure Data Services, including Azure Data Factory, Azure Synapse, or similar tools.\nExperience of creating DAGs, implementing activities, and running Apache Airflow\nFamiliarity with DevOps practices, CI/CD pipelines and Azure DevOps.","['CI/CD', 'Senior Data Engineer ( Fabric )', 'Sql', 'Etl']",2025-06-10 19:37:12
Senior Data Engineer ( Fabric ),Aspire Systems India Private Limited,10-15 Years,,Chennai,Software,"We are looking for a Senior Data Engineer with extensive experience in developing ETL processes using PySpark Notebooks and Microsoft Fabric, and supporting existing legacy SQL Server environments. The ideal candidate will possess a strong background in Spark-based development, demonstrate a high proficiency in SQL, and be comfortable working independently, collaboratively within a team, or leading other developers when required, coupled with strong communication skills.\nRequirements:\nWe are looking for a Senior Data Engineer with extensive experience in developing ETL processes using PySpark Notebooks and Microsoft Fabric, and supporting existing legacy SQL Server environments. The ideal candidate will possess a strong background in Spark-based development, demonstrate a high proficiency in SQL, and be comfortable working independently, collaboratively within a team, or leading other developers when required, coupled with strong communication skills. The ideal candidate will possess Experience with Azure Data Services, including Azure Data Factory, Azure Synapse or similar tools,Experience of creating DAGs, implementing activities, and running Apache Airflow and Familiarity with DevOps practices, CI/CD pipelines and Azure DevOps.\nThe ideal candidate should have:\nKey Responsibilities:\nDesign, develop, and maintain ETL Notebook orchestration pipelines using PySpark and Microsoft Fabric.\nWorking with Apache Delta Lake tables, Change Data Feed (CDF), Lakehouses and custom libraries\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver efficient data solutions.\nMigrate and integrate data from legacy SQL Server environments into modern data platforms.\nOptimize data pipelines and workflows for scalability, efficiency, and reliability.\nProvide technical leadership and mentorship to junior developers and other team members.\nTroubleshoot and resolve complex data engineering issues related to performance, data quality, and system scalability.\nDebugging of code, breaking down to test components, identify issues and resolve\nDevelop, maintain, and enforce data engineering best practices, coding standards, and documentation.\nConduct code reviews and provide constructive feedback to improve team productivity and code quality.\nSupport data-driven decision-making processes by ensuring data integrity, availability, and consistency across different platforms.\nQualifications:\nBachelor s or Master s degree in Computer Science, Data Science, Engineering, or a related field.\n10+ years of experience in data engineering, with a strong focus on ETL development using PySpark or other Spark-based tools.\nProficiency in SQL with extensive experience in complex queries, performance tuning, and data modeling.\nExperience with Microsoft Fabric or similar cloud-based data integration platforms is a plus.\nStrong knowledge of data warehousing concepts, ETL frameworks, and big data processing.\nFamiliarity with other data processing technologies (e.g., Hadoop, Hive, Kafka) is an advantage.\nExperience working with both structured and unstructured data sources.\nExcellent problem-solving skills and the ability to troubleshoot complex data engineering issues.\nExperience with Azure Data Services, including Azure Data Factory, Azure Synapse, or similar tools.\nExperience of creating DAGs, implementing activities, and running Apache Airflow\nFamiliarity with DevOps practices, CI/CD pipelines and Azure DevOps.","['CI/CD', 'Senior Data Engineer ( Fabric )', 'Sql', 'Etl']",2025-06-10 19:37:13
Senior Data Engineer ( Fabric ),Aspire Systems India Private Limited,10-15 Years,,Chennai,Software,"We are looking for a Senior Data Engineer with extensive experience in developing ETL processes using PySpark Notebooks and Microsoft Fabric, and supporting existing legacy SQL Server environments. The ideal candidate will possess a strong background in Spark-based development, demonstrate a high proficiency in SQL, and be comfortable working independently, collaboratively within a team, or leading other developers when required, coupled with strong communication skills.\nRequirements:\nWe are looking for a Senior Data Engineer with extensive experience in developing ETL processes using PySpark Notebooks and Microsoft Fabric, and supporting existing legacy SQL Server environments. The ideal candidate will possess a strong background in Spark-based development, demonstrate a high proficiency in SQL, and be comfortable working independently, collaboratively within a team, or leading other developers when required, coupled with strong communication skills. The ideal candidate will possess Experience with Azure Data Services, including Azure Data Factory, Azure Synapse or similar tools,Experience of creating DAGs, implementing activities, and running Apache Airflow and Familiarity with DevOps practices, CI/CD pipelines and Azure DevOps.\nThe ideal candidate should have:\nKey Responsibilities:\nDesign, develop, and maintain ETL Notebook orchestration pipelines using PySpark and Microsoft Fabric.\nWorking with Apache Delta Lake tables, Change Data Feed (CDF), Lakehouses and custom libraries\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver efficient data solutions.\nMigrate and integrate data from legacy SQL Server environments into modern data platforms.\nOptimize data pipelines and workflows for scalability, efficiency, and reliability.\nProvide technical leadership and mentorship to junior developers and other team members.\nTroubleshoot and resolve complex data engineering issues related to performance, data quality, and system scalability.\nDebugging of code, breaking down to test components, identify issues and resolve\nDevelop, maintain, and enforce data engineering best practices, coding standards, and documentation.\nConduct code reviews and provide constructive feedback to improve team productivity and code quality.\nSupport data-driven decision-making processes by ensuring data integrity, availability, and consistency across different platforms.\nQualifications:\nBachelor s or Master s degree in Computer Science, Data Science, Engineering, or a related field.\n10+ years of experience in data engineering, with a strong focus on ETL development using PySpark or other Spark-based tools.\nProficiency in SQL with extensive experience in complex queries, performance tuning, and data modeling.\nExperience with Microsoft Fabric or similar cloud-based data integration platforms is a plus.\nStrong knowledge of data warehousing concepts, ETL frameworks, and big data processing.\nFamiliarity with other data processing technologies (e.g., Hadoop, Hive, Kafka) is an advantage.\nExperience working with both structured and unstructured data sources.\nExcellent problem-solving skills and the ability to troubleshoot complex data engineering issues.\nExperience with Azure Data Services, including Azure Data Factory, Azure Synapse, or similar tools.\nExperience of creating DAGs, implementing activities, and running Apache Airflow\nFamiliarity with DevOps practices, CI/CD pipelines and Azure DevOps.","['CI/CD', 'Senior Data Engineer ( Fabric )', 'Sql', 'Etl']",2025-06-10 19:37:14
Senior Data Engineer ( Fabric ),Aspire Systems India Private Limited,10-15 Years,,Chennai,Software,"We are looking for a Senior Data Engineer with extensive experience in developing ETL processes using PySpark Notebooks and Microsoft Fabric, and supporting existing legacy SQL Server environments. The ideal candidate will possess a strong background in Spark-based development, demonstrate a high proficiency in SQL, and be comfortable working independently, collaboratively within a team, or leading other developers when required, coupled with strong communication skills.\nRequirements:\nWe are looking for a Senior Data Engineer with extensive experience in developing ETL processes using PySpark Notebooks and Microsoft Fabric, and supporting existing legacy SQL Server environments. The ideal candidate will possess a strong background in Spark-based development, demonstrate a high proficiency in SQL, and be comfortable working independently, collaboratively within a team, or leading other developers when required, coupled with strong communication skills. The ideal candidate will possess Experience with Azure Data Services, including Azure Data Factory, Azure Synapse or similar tools,Experience of creating DAGs, implementing activities, and running Apache Airflow and Familiarity with DevOps practices, CI/CD pipelines and Azure DevOps.\nThe ideal candidate should have:\nKey Responsibilities:\nDesign, develop, and maintain ETL Notebook orchestration pipelines using PySpark and Microsoft Fabric.\nWorking with Apache Delta Lake tables, Change Data Feed (CDF), Lakehouses and custom libraries\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver efficient data solutions.\nMigrate and integrate data from legacy SQL Server environments into modern data platforms.\nOptimize data pipelines and workflows for scalability, efficiency, and reliability.\nProvide technical leadership and mentorship to junior developers and other team members.\nTroubleshoot and resolve complex data engineering issues related to performance, data quality, and system scalability.\nDebugging of code, breaking down to test components, identify issues and resolve\nDevelop, maintain, and enforce data engineering best practices, coding standards, and documentation.\nConduct code reviews and provide constructive feedback to improve team productivity and code quality.\nSupport data-driven decision-making processes by ensuring data integrity, availability, and consistency across different platforms.\nQualifications:\nBachelor s or Master s degree in Computer Science, Data Science, Engineering, or a related field.\n10+ years of experience in data engineering, with a strong focus on ETL development using PySpark or other Spark-based tools.\nProficiency in SQL with extensive experience in complex queries, performance tuning, and data modeling.\nExperience with Microsoft Fabric or similar cloud-based data integration platforms is a plus.\nStrong knowledge of data warehousing concepts, ETL frameworks, and big data processing.\nFamiliarity with other data processing technologies (e.g., Hadoop, Hive, Kafka) is an advantage.\nExperience working with both structured and unstructured data sources.\nExcellent problem-solving skills and the ability to troubleshoot complex data engineering issues.\nExperience with Azure Data Services, including Azure Data Factory, Azure Synapse, or similar tools.\nExperience of creating DAGs, implementing activities, and running Apache Airflow\nFamiliarity with DevOps practices, CI/CD pipelines and Azure DevOps.","['CI/CD', 'Senior Data Engineer ( Fabric )', 'Sql', 'Etl']",2025-06-10 19:37:15
Senior Data Engineer ( Fabric ),Aspire Systems India Private Limited,10-15 Years,,Chennai,Software,"We are looking for a Senior Data Engineer with extensive experience in developing ETL processes using PySpark Notebooks and Microsoft Fabric, and supporting existing legacy SQL Server environments. The ideal candidate will possess a strong background in Spark-based development, demonstrate a high proficiency in SQL, and be comfortable working independently, collaboratively within a team, or leading other developers when required, coupled with strong communication skills.\nRequirements:\nWe are looking for a Senior Data Engineer with extensive experience in developing ETL processes using PySpark Notebooks and Microsoft Fabric, and supporting existing legacy SQL Server environments. The ideal candidate will possess a strong background in Spark-based development, demonstrate a high proficiency in SQL, and be comfortable working independently, collaboratively within a team, or leading other developers when required, coupled with strong communication skills. The ideal candidate will possess Experience with Azure Data Services, including Azure Data Factory, Azure Synapse or similar tools,Experience of creating DAGs, implementing activities, and running Apache Airflow and Familiarity with DevOps practices, CI/CD pipelines and Azure DevOps.\nThe ideal candidate should have:\nKey Responsibilities:\nDesign, develop, and maintain ETL Notebook orchestration pipelines using PySpark and Microsoft Fabric.\nWorking with Apache Delta Lake tables, Change Data Feed (CDF), Lakehouses and custom libraries\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver efficient data solutions.\nMigrate and integrate data from legacy SQL Server environments into modern data platforms.\nOptimize data pipelines and workflows for scalability, efficiency, and reliability.\nProvide technical leadership and mentorship to junior developers and other team members.\nTroubleshoot and resolve complex data engineering issues related to performance, data quality, and system scalability.\nDebugging of code, breaking down to test components, identify issues and resolve\nDevelop, maintain, and enforce data engineering best practices, coding standards, and documentation.\nConduct code reviews and provide constructive feedback to improve team productivity and code quality.\nSupport data-driven decision-making processes by ensuring data integrity, availability, and consistency across different platforms.\nQualifications:\nBachelor s or Master s degree in Computer Science, Data Science, Engineering, or a related field.\n10+ years of experience in data engineering, with a strong focus on ETL development using PySpark or other Spark-based tools.\nProficiency in SQL with extensive experience in complex queries, performance tuning, and data modeling.\nExperience with Microsoft Fabric or similar cloud-based data integration platforms is a plus.\nStrong knowledge of data warehousing concepts, ETL frameworks, and big data processing.\nFamiliarity with other data processing technologies (e.g., Hadoop, Hive, Kafka) is an advantage.\nExperience working with both structured and unstructured data sources.\nExcellent problem-solving skills and the ability to troubleshoot complex data engineering issues.\nExperience with Azure Data Services, including Azure Data Factory, Azure Synapse, or similar tools.\nExperience of creating DAGs, implementing activities, and running Apache Airflow\nFamiliarity with DevOps practices, CI/CD pipelines and Azure DevOps.","['CI/CD', 'Senior Data Engineer ( Fabric )', 'Sql', 'Etl']",2025-06-10 19:37:16
Senior Data Engineer ( Fabric ),Aspire Systems India Private Limited,10-15 Years,,Chennai,Software,"We are looking for a Senior Data Engineer with extensive experience in developing ETL processes using PySpark Notebooks and Microsoft Fabric, and supporting existing legacy SQL Server environments. The ideal candidate will possess a strong background in Spark-based development, demonstrate a high proficiency in SQL, and be comfortable working independently, collaboratively within a team, or leading other developers when required, coupled with strong communication skills.\nRequirements:\nWe are looking for a Senior Data Engineer with extensive experience in developing ETL processes using PySpark Notebooks and Microsoft Fabric, and supporting existing legacy SQL Server environments. The ideal candidate will possess a strong background in Spark-based development, demonstrate a high proficiency in SQL, and be comfortable working independently, collaboratively within a team, or leading other developers when required, coupled with strong communication skills. The ideal candidate will possess Experience with Azure Data Services, including Azure Data Factory, Azure Synapse or similar tools,Experience of creating DAGs, implementing activities, and running Apache Airflow and Familiarity with DevOps practices, CI/CD pipelines and Azure DevOps.\nThe ideal candidate should have:\nKey Responsibilities:\nDesign, develop, and maintain ETL Notebook orchestration pipelines using PySpark and Microsoft Fabric.\nWorking with Apache Delta Lake tables, Change Data Feed (CDF), Lakehouses and custom libraries\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver efficient data solutions.\nMigrate and integrate data from legacy SQL Server environments into modern data platforms.\nOptimize data pipelines and workflows for scalability, efficiency, and reliability.\nProvide technical leadership and mentorship to junior developers and other team members.\nTroubleshoot and resolve complex data engineering issues related to performance, data quality, and system scalability.\nDebugging of code, breaking down to test components, identify issues and resolve\nDevelop, maintain, and enforce data engineering best practices, coding standards, and documentation.\nConduct code reviews and provide constructive feedback to improve team productivity and code quality.\nSupport data-driven decision-making processes by ensuring data integrity, availability, and consistency across different platforms.\nQualifications:\nBachelor s or Master s degree in Computer Science, Data Science, Engineering, or a related field.\n10+ years of experience in data engineering, with a strong focus on ETL development using PySpark or other Spark-based tools.\nProficiency in SQL with extensive experience in complex queries, performance tuning, and data modeling.\nExperience with Microsoft Fabric or similar cloud-based data integration platforms is a plus.\nStrong knowledge of data warehousing concepts, ETL frameworks, and big data processing.\nFamiliarity with other data processing technologies (e.g., Hadoop, Hive, Kafka) is an advantage.\nExperience working with both structured and unstructured data sources.\nExcellent problem-solving skills and the ability to troubleshoot complex data engineering issues.\nExperience with Azure Data Services, including Azure Data Factory, Azure Synapse, or similar tools.\nExperience of creating DAGs, implementing activities, and running Apache Airflow\nFamiliarity with DevOps practices, CI/CD pipelines and Azure DevOps.","['CI/CD', 'Senior Data Engineer ( Fabric )', 'Sql', 'Etl']",2025-06-10 19:37:16
Senior Data Engineer ( Fabric ),Aspire Systems India Private Limited,10-15 Years,,Chennai,Software,"We are looking for a Senior Data Engineer with extensive experience in developing ETL processes using PySpark Notebooks and Microsoft Fabric, and supporting existing legacy SQL Server environments. The ideal candidate will possess a strong background in Spark-based development, demonstrate a high proficiency in SQL, and be comfortable working independently, collaboratively within a team, or leading other developers when required, coupled with strong communication skills.\nRequirements:\nWe are looking for a Senior Data Engineer with extensive experience in developing ETL processes using PySpark Notebooks and Microsoft Fabric, and supporting existing legacy SQL Server environments. The ideal candidate will possess a strong background in Spark-based development, demonstrate a high proficiency in SQL, and be comfortable working independently, collaboratively within a team, or leading other developers when required, coupled with strong communication skills. The ideal candidate will possess Experience with Azure Data Services, including Azure Data Factory, Azure Synapse or similar tools,Experience of creating DAGs, implementing activities, and running Apache Airflow and Familiarity with DevOps practices, CI/CD pipelines and Azure DevOps.\nThe ideal candidate should have:\nKey Responsibilities:\nDesign, develop, and maintain ETL Notebook orchestration pipelines using PySpark and Microsoft Fabric.\nWorking with Apache Delta Lake tables, Change Data Feed (CDF), Lakehouses and custom libraries\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver efficient data solutions.\nMigrate and integrate data from legacy SQL Server environments into modern data platforms.\nOptimize data pipelines and workflows for scalability, efficiency, and reliability.\nProvide technical leadership and mentorship to junior developers and other team members.\nTroubleshoot and resolve complex data engineering issues related to performance, data quality, and system scalability.\nDebugging of code, breaking down to test components, identify issues and resolve\nDevelop, maintain, and enforce data engineering best practices, coding standards, and documentation.\nConduct code reviews and provide constructive feedback to improve team productivity and code quality.\nSupport data-driven decision-making processes by ensuring data integrity, availability, and consistency across different platforms.\nQualifications:\nBachelor s or Master s degree in Computer Science, Data Science, Engineering, or a related field.\n10+ years of experience in data engineering, with a strong focus on ETL development using PySpark or other Spark-based tools.\nProficiency in SQL with extensive experience in complex queries, performance tuning, and data modeling.\nExperience with Microsoft Fabric or similar cloud-based data integration platforms is a plus.\nStrong knowledge of data warehousing concepts, ETL frameworks, and big data processing.\nFamiliarity with other data processing technologies (e.g., Hadoop, Hive, Kafka) is an advantage.\nExperience working with both structured and unstructured data sources.\nExcellent problem-solving skills and the ability to troubleshoot complex data engineering issues.\nExperience with Azure Data Services, including Azure Data Factory, Azure Synapse, or similar tools.\nExperience of creating DAGs, implementing activities, and running Apache Airflow\nFamiliarity with DevOps practices, CI/CD pipelines and Azure DevOps.","['CI/CD', 'Senior Data Engineer ( Fabric )', 'Sql', 'Etl']",2025-06-10 19:37:17
Senior Data Engineer ( Fabric ),Aspire Systems India Private Limited,10-15 Years,,Chennai,Software,"We are looking for a Senior Data Engineer with extensive experience in developing ETL processes using PySpark Notebooks and Microsoft Fabric, and supporting existing legacy SQL Server environments. The ideal candidate will possess a strong background in Spark-based development, demonstrate a high proficiency in SQL, and be comfortable working independently, collaboratively within a team, or leading other developers when required, coupled with strong communication skills.\nRequirements:\nWe are looking for a Senior Data Engineer with extensive experience in developing ETL processes using PySpark Notebooks and Microsoft Fabric, and supporting existing legacy SQL Server environments. The ideal candidate will possess a strong background in Spark-based development, demonstrate a high proficiency in SQL, and be comfortable working independently, collaboratively within a team, or leading other developers when required, coupled with strong communication skills. The ideal candidate will possess Experience with Azure Data Services, including Azure Data Factory, Azure Synapse or similar tools,Experience of creating DAGs, implementing activities, and running Apache Airflow and Familiarity with DevOps practices, CI/CD pipelines and Azure DevOps.\nThe ideal candidate should have:\nKey Responsibilities:\nDesign, develop, and maintain ETL Notebook orchestration pipelines using PySpark and Microsoft Fabric.\nWorking with Apache Delta Lake tables, Change Data Feed (CDF), Lakehouses and custom libraries\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver efficient data solutions.\nMigrate and integrate data from legacy SQL Server environments into modern data platforms.\nOptimize data pipelines and workflows for scalability, efficiency, and reliability.\nProvide technical leadership and mentorship to junior developers and other team members.\nTroubleshoot and resolve complex data engineering issues related to performance, data quality, and system scalability.\nDebugging of code, breaking down to test components, identify issues and resolve\nDevelop, maintain, and enforce data engineering best practices, coding standards, and documentation.\nConduct code reviews and provide constructive feedback to improve team productivity and code quality.\nSupport data-driven decision-making processes by ensuring data integrity, availability, and consistency across different platforms.\nQualifications:\nBachelor s or Master s degree in Computer Science, Data Science, Engineering, or a related field.\n10+ years of experience in data engineering, with a strong focus on ETL development using PySpark or other Spark-based tools.\nProficiency in SQL with extensive experience in complex queries, performance tuning, and data modeling.\nExperience with Microsoft Fabric or similar cloud-based data integration platforms is a plus.\nStrong knowledge of data warehousing concepts, ETL frameworks, and big data processing.\nFamiliarity with other data processing technologies (e.g., Hadoop, Hive, Kafka) is an advantage.\nExperience working with both structured and unstructured data sources.\nExcellent problem-solving skills and the ability to troubleshoot complex data engineering issues.\nExperience with Azure Data Services, including Azure Data Factory, Azure Synapse, or similar tools.\nExperience of creating DAGs, implementing activities, and running Apache Airflow\nFamiliarity with DevOps practices, CI/CD pipelines and Azure DevOps.","['CI/CD', 'Senior Data Engineer ( Fabric )', 'Sql', 'Etl']",2025-06-10 19:37:18
Senior Data Engineer ( Fabric ),Aspire Systems India Private Limited,10-15 Years,,Chennai,Software,"We are looking for a Senior Data Engineer with extensive experience in developing ETL processes using PySpark Notebooks and Microsoft Fabric, and supporting existing legacy SQL Server environments. The ideal candidate will possess a strong background in Spark-based development, demonstrate a high proficiency in SQL, and be comfortable working independently, collaboratively within a team, or leading other developers when required, coupled with strong communication skills.\nRequirements:\nWe are looking for a Senior Data Engineer with extensive experience in developing ETL processes using PySpark Notebooks and Microsoft Fabric, and supporting existing legacy SQL Server environments. The ideal candidate will possess a strong background in Spark-based development, demonstrate a high proficiency in SQL, and be comfortable working independently, collaboratively within a team, or leading other developers when required, coupled with strong communication skills. The ideal candidate will possess Experience with Azure Data Services, including Azure Data Factory, Azure Synapse or similar tools,Experience of creating DAGs, implementing activities, and running Apache Airflow and Familiarity with DevOps practices, CI/CD pipelines and Azure DevOps.\nThe ideal candidate should have:\nKey Responsibilities:\nDesign, develop, and maintain ETL Notebook orchestration pipelines using PySpark and Microsoft Fabric.\nWorking with Apache Delta Lake tables, Change Data Feed (CDF), Lakehouses and custom libraries\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver efficient data solutions.\nMigrate and integrate data from legacy SQL Server environments into modern data platforms.\nOptimize data pipelines and workflows for scalability, efficiency, and reliability.\nProvide technical leadership and mentorship to junior developers and other team members.\nTroubleshoot and resolve complex data engineering issues related to performance, data quality, and system scalability.\nDebugging of code, breaking down to test components, identify issues and resolve\nDevelop, maintain, and enforce data engineering best practices, coding standards, and documentation.\nConduct code reviews and provide constructive feedback to improve team productivity and code quality.\nSupport data-driven decision-making processes by ensuring data integrity, availability, and consistency across different platforms.\nQualifications:\nBachelor s or Master s degree in Computer Science, Data Science, Engineering, or a related field.\n10+ years of experience in data engineering, with a strong focus on ETL development using PySpark or other Spark-based tools.\nProficiency in SQL with extensive experience in complex queries, performance tuning, and data modeling.\nExperience with Microsoft Fabric or similar cloud-based data integration platforms is a plus.\nStrong knowledge of data warehousing concepts, ETL frameworks, and big data processing.\nFamiliarity with other data processing technologies (e.g., Hadoop, Hive, Kafka) is an advantage.\nExperience working with both structured and unstructured data sources.\nExcellent problem-solving skills and the ability to troubleshoot complex data engineering issues.\nExperience with Azure Data Services, including Azure Data Factory, Azure Synapse, or similar tools.\nExperience of creating DAGs, implementing activities, and running Apache Airflow\nFamiliarity with DevOps practices, CI/CD pipelines and Azure DevOps.","['CI/CD', 'Senior Data Engineer ( Fabric )', 'Sql', 'Etl']",2025-06-10 19:37:19
Senior Data Engineer ( Fabric ),Aspire Systems India Private Limited,10-15 Years,,Chennai,Software,"We are looking for a Senior Data Engineer with extensive experience in developing ETL processes using PySpark Notebooks and Microsoft Fabric, and supporting existing legacy SQL Server environments. The ideal candidate will possess a strong background in Spark-based development, demonstrate a high proficiency in SQL, and be comfortable working independently, collaboratively within a team, or leading other developers when required, coupled with strong communication skills.\nRequirements:\nWe are looking for a Senior Data Engineer with extensive experience in developing ETL processes using PySpark Notebooks and Microsoft Fabric, and supporting existing legacy SQL Server environments. The ideal candidate will possess a strong background in Spark-based development, demonstrate a high proficiency in SQL, and be comfortable working independently, collaboratively within a team, or leading other developers when required, coupled with strong communication skills. The ideal candidate will possess Experience with Azure Data Services, including Azure Data Factory, Azure Synapse or similar tools,Experience of creating DAGs, implementing activities, and running Apache Airflow and Familiarity with DevOps practices, CI/CD pipelines and Azure DevOps.\nThe ideal candidate should have:\nKey Responsibilities:\nDesign, develop, and maintain ETL Notebook orchestration pipelines using PySpark and Microsoft Fabric.\nWorking with Apache Delta Lake tables, Change Data Feed (CDF), Lakehouses and custom libraries\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver efficient data solutions.\nMigrate and integrate data from legacy SQL Server environments into modern data platforms.\nOptimize data pipelines and workflows for scalability, efficiency, and reliability.\nProvide technical leadership and mentorship to junior developers and other team members.\nTroubleshoot and resolve complex data engineering issues related to performance, data quality, and system scalability.\nDebugging of code, breaking down to test components, identify issues and resolve\nDevelop, maintain, and enforce data engineering best practices, coding standards, and documentation.\nConduct code reviews and provide constructive feedback to improve team productivity and code quality.\nSupport data-driven decision-making processes by ensuring data integrity, availability, and consistency across different platforms.\nQualifications:\nBachelor s or Master s degree in Computer Science, Data Science, Engineering, or a related field.\n10+ years of experience in data engineering, with a strong focus on ETL development using PySpark or other Spark-based tools.\nProficiency in SQL with extensive experience in complex queries, performance tuning, and data modeling.\nExperience with Microsoft Fabric or similar cloud-based data integration platforms is a plus.\nStrong knowledge of data warehousing concepts, ETL frameworks, and big data processing.\nFamiliarity with other data processing technologies (e.g., Hadoop, Hive, Kafka) is an advantage.\nExperience working with both structured and unstructured data sources.\nExcellent problem-solving skills and the ability to troubleshoot complex data engineering issues.\nExperience with Azure Data Services, including Azure Data Factory, Azure Synapse, or similar tools.\nExperience of creating DAGs, implementing activities, and running Apache Airflow\nFamiliarity with DevOps practices, CI/CD pipelines and Azure DevOps.","['CI/CD', 'Senior Data Engineer ( Fabric )', 'Sql', 'Etl']",2025-06-10 19:37:20
Data Engineer-Data Platforms,IBM,5-7 Years,,Bengaluru,Information Technology,"Introduction\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.. Good working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala","['Hadoop Spark', 'Jenkins GIT', 'Scala', 'Python', 'Devops', 'Lambda']",2025-06-10 19:37:27
Data Engineer-Data Platforms,IBM,5-7 Years,,Bengaluru,Information Technology,"Introduction\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.. Good working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala","['Hadoop Spark', 'Jenkins GIT', 'Scala', 'Python', 'Devops', 'Lambda']",2025-06-10 19:37:30
Data Engineer-Data Platforms,IBM,5-7 Years,,Bengaluru,Information Technology,"Introduction\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.. Good working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala","['Hadoop Spark', 'Jenkins GIT', 'Scala', 'Python', 'Devops', 'Lambda']",2025-06-10 19:37:33
Data Engineer-Data Platforms,IBM,5-7 Years,,Bengaluru,Information Technology,"Introduction\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.. Good working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala","['Hadoop Spark', 'Jenkins GIT', 'Scala', 'Python', 'Devops', 'Lambda']",2025-06-10 19:37:34
Data Engineer-Data Platforms,IBM,5-7 Years,,Bengaluru,Information Technology,"Introduction\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.. Good working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala","['Hadoop Spark', 'Jenkins GIT', 'Scala', 'Python', 'Devops', 'Lambda']",2025-06-10 19:37:35
Data Engineer-Data Platforms,IBM,5-7 Years,,Bengaluru,Information Technology,"Introduction\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.. Good working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala","['Hadoop Spark', 'Jenkins GIT', 'Scala', 'Python', 'Devops', 'Lambda']",2025-06-10 19:37:35
Data Engineer-Data Platforms,IBM,5-7 Years,,Bengaluru,Information Technology,"Introduction\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.. Good working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala","['Hadoop Spark', 'Jenkins GIT', 'Scala', 'Python', 'Devops', 'Lambda']",2025-06-10 19:37:36
Data Engineer-Data Platforms,IBM,5-7 Years,,Bengaluru,Information Technology,"Introduction\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.. Good working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala","['Hadoop Spark', 'Jenkins GIT', 'Scala', 'Python', 'Devops', 'Lambda']",2025-06-10 19:37:37
Data Engineer-Data Platforms,IBM,5-7 Years,,Bengaluru,Information Technology,"Introduction\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.. Good working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala","['Hadoop Spark', 'Jenkins GIT', 'Scala', 'Python', 'Devops', 'Lambda']",2025-06-10 19:37:38
Data Engineer-Data Platforms,IBM,5-7 Years,,Bengaluru,Information Technology,"Introduction\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.. Good working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala","['Hadoop Spark', 'Jenkins GIT', 'Scala', 'Python', 'Devops', 'Lambda']",2025-06-10 19:37:39
Data Engineer-Data Platforms,IBM,5-7 Years,,Bengaluru,Information Technology,"Introduction\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.. Good working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala","['Hadoop Spark', 'Jenkins GIT', 'Scala', 'Python', 'Devops', 'Lambda']",2025-06-10 19:37:39
Data Engineer-Data Platforms,IBM,5-7 Years,,Bengaluru,Information Technology,"Introduction\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.. Good working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala","['Hadoop Spark', 'Jenkins GIT', 'Scala', 'Python', 'Devops', 'Lambda']",2025-06-10 19:37:40
Data Engineer-Data Platforms,IBM,5-7 Years,,Bengaluru,Information Technology,"Introduction\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.. Good working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala","['Hadoop Spark', 'Jenkins GIT', 'Scala', 'Python', 'Devops', 'Lambda']",2025-06-10 19:37:41
Data Engineer-Data Platforms,IBM,5-7 Years,,Bengaluru,Information Technology,"Introduction\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.. Good working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala","['Hadoop Spark', 'Jenkins GIT', 'Scala', 'Python', 'Devops', 'Lambda']",2025-06-10 19:37:42
Data Engineer-Data Platforms,IBM,5-7 Years,,Bengaluru,Information Technology,"Introduction\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.. Good working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala","['Hadoop Spark', 'Jenkins GIT', 'Scala', 'Python', 'Devops', 'Lambda']",2025-06-10 19:37:43
AWS Data Engineer,Tata Consultancy Services Limited,4-10 Years,,Bengaluru,Software,"Develop roadmap, architecture, implementation and process refinement related to digital assets using AWS, Terraform and Kubernetes delivery tools.\nDevelop and implement python/shell scripts, terraform templates and helm charts based on project requirements.\nCreate CI/CD pipelines to enable a fully automated SDLC.\nCreate and maintain platform/services automation documentation.","['Devops', 'Aws']",2025-06-10 19:37:50
AWS Data Engineer,Tata Consultancy Services Limited,4-10 Years,,Bengaluru,Software,"Develop roadmap, architecture, implementation and process refinement related to digital assets using AWS, Terraform and Kubernetes delivery tools.\nDevelop and implement python/shell scripts, terraform templates and helm charts based on project requirements.\nCreate CI/CD pipelines to enable a fully automated SDLC.\nCreate and maintain platform/services automation documentation.","['Devops', 'Aws']",2025-06-10 19:37:53
AWS Data Engineer,Tata Consultancy Services Limited,4-10 Years,,Bengaluru,Software,"Develop roadmap, architecture, implementation and process refinement related to digital assets using AWS, Terraform and Kubernetes delivery tools.\nDevelop and implement python/shell scripts, terraform templates and helm charts based on project requirements.\nCreate CI/CD pipelines to enable a fully automated SDLC.\nCreate and maintain platform/services automation documentation.","['Devops', 'Aws']",2025-06-10 19:37:56
AWS Data Engineer,Tata Consultancy Services Limited,4-10 Years,,Bengaluru,Software,"Develop roadmap, architecture, implementation and process refinement related to digital assets using AWS, Terraform and Kubernetes delivery tools.\nDevelop and implement python/shell scripts, terraform templates and helm charts based on project requirements.\nCreate CI/CD pipelines to enable a fully automated SDLC.\nCreate and maintain platform/services automation documentation.","['Devops', 'Aws']",2025-06-10 19:37:57
AWS Data Engineer,Tata Consultancy Services Limited,4-10 Years,,Bengaluru,Software,"Develop roadmap, architecture, implementation and process refinement related to digital assets using AWS, Terraform and Kubernetes delivery tools.\nDevelop and implement python/shell scripts, terraform templates and helm charts based on project requirements.\nCreate CI/CD pipelines to enable a fully automated SDLC.\nCreate and maintain platform/services automation documentation.","['Devops', 'Aws']",2025-06-10 19:37:58
AWS Data Engineer,Tata Consultancy Services Limited,4-10 Years,,Bengaluru,Software,"Develop roadmap, architecture, implementation and process refinement related to digital assets using AWS, Terraform and Kubernetes delivery tools.\nDevelop and implement python/shell scripts, terraform templates and helm charts based on project requirements.\nCreate CI/CD pipelines to enable a fully automated SDLC.\nCreate and maintain platform/services automation documentation.","['Devops', 'Aws']",2025-06-10 19:37:59
AWS Data Engineer,Tata Consultancy Services Limited,4-10 Years,,Bengaluru,Software,"Develop roadmap, architecture, implementation and process refinement related to digital assets using AWS, Terraform and Kubernetes delivery tools.\nDevelop and implement python/shell scripts, terraform templates and helm charts based on project requirements.\nCreate CI/CD pipelines to enable a fully automated SDLC.\nCreate and maintain platform/services automation documentation.","['Devops', 'Aws']",2025-06-10 19:37:59
AWS Data Engineer,Tata Consultancy Services Limited,4-10 Years,,Bengaluru,Software,"Develop roadmap, architecture, implementation and process refinement related to digital assets using AWS, Terraform and Kubernetes delivery tools.\nDevelop and implement python/shell scripts, terraform templates and helm charts based on project requirements.\nCreate CI/CD pipelines to enable a fully automated SDLC.\nCreate and maintain platform/services automation documentation.","['Devops', 'Aws']",2025-06-10 19:38:00
AWS Data Engineer,Tata Consultancy Services Limited,4-10 Years,,Bengaluru,Software,"Develop roadmap, architecture, implementation and process refinement related to digital assets using AWS, Terraform and Kubernetes delivery tools.\nDevelop and implement python/shell scripts, terraform templates and helm charts based on project requirements.\nCreate CI/CD pipelines to enable a fully automated SDLC.\nCreate and maintain platform/services automation documentation.","['Devops', 'Aws']",2025-06-10 19:38:01
AWS Data Engineer,Tata Consultancy Services Limited,4-10 Years,,Bengaluru,Software,"Develop roadmap, architecture, implementation and process refinement related to digital assets using AWS, Terraform and Kubernetes delivery tools.\nDevelop and implement python/shell scripts, terraform templates and helm charts based on project requirements.\nCreate CI/CD pipelines to enable a fully automated SDLC.\nCreate and maintain platform/services automation documentation.","['Devops', 'Aws']",2025-06-10 19:38:02
AWS Data Engineer,Tata Consultancy Services Limited,4-10 Years,,Bengaluru,Software,"Develop roadmap, architecture, implementation and process refinement related to digital assets using AWS, Terraform and Kubernetes delivery tools.\nDevelop and implement python/shell scripts, terraform templates and helm charts based on project requirements.\nCreate CI/CD pipelines to enable a fully automated SDLC.\nCreate and maintain platform/services automation documentation.","['Devops', 'Aws']",2025-06-10 19:38:02
AWS Data Engineer,Tata Consultancy Services Limited,4-10 Years,,Bengaluru,Software,"Develop roadmap, architecture, implementation and process refinement related to digital assets using AWS, Terraform and Kubernetes delivery tools.\nDevelop and implement python/shell scripts, terraform templates and helm charts based on project requirements.\nCreate CI/CD pipelines to enable a fully automated SDLC.\nCreate and maintain platform/services automation documentation.","['Devops', 'Aws']",2025-06-10 19:38:03
AWS Data Engineer,Tata Consultancy Services Limited,4-10 Years,,Bengaluru,Software,"Develop roadmap, architecture, implementation and process refinement related to digital assets using AWS, Terraform and Kubernetes delivery tools.\nDevelop and implement python/shell scripts, terraform templates and helm charts based on project requirements.\nCreate CI/CD pipelines to enable a fully automated SDLC.\nCreate and maintain platform/services automation documentation.","['Devops', 'Aws']",2025-06-10 19:38:04
AWS Data Engineer,Tata Consultancy Services Limited,4-10 Years,,Bengaluru,Software,"Develop roadmap, architecture, implementation and process refinement related to digital assets using AWS, Terraform and Kubernetes delivery tools.\nDevelop and implement python/shell scripts, terraform templates and helm charts based on project requirements.\nCreate CI/CD pipelines to enable a fully automated SDLC.\nCreate and maintain platform/services automation documentation.","['Devops', 'Aws']",2025-06-10 19:38:05
AWS Data Engineer,Tata Consultancy Services Limited,4-10 Years,,Bengaluru,Software,"Develop roadmap, architecture, implementation and process refinement related to digital assets using AWS, Terraform and Kubernetes delivery tools.\nDevelop and implement python/shell scripts, terraform templates and helm charts based on project requirements.\nCreate CI/CD pipelines to enable a fully automated SDLC.\nCreate and maintain platform/services automation documentation.","['Devops', 'Aws']",2025-06-10 19:38:06
Azure Data Engineer,Aspire Systems India Private Limited,4-9 Years,,"Bengaluru, Chennai",Software,"We are seeking an experienced Azure Data Engineer with over 4 to 13 years of proven expertise in Data lakes, Lake house, Synapse Analytic, Data bricks, Tsql, sql server, Synapse Db, Data warehouse and should have work exp in ETL, Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems with strong communication skills.\nRequirements:\nWe are seeking an experienced Azure Data Engineer with over 4 to 13 years of proven expertise in Data lakes, Lake house, Synapse Analytic, Data bricks, Tsql, sql server, Synapse Db, Data warehouse and should have work exp in ETL, Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems with strong communication skills.\nThe ideal candidate should have:\nKey Responsibilities:\nCreate Data lakes from scratch, configure existing systems and provide user support\nUnderstand different datasets and Storage elements to bring data\nHave good knowledge and work experience in ADF, Synapse Data pipelines\nHave good knowledge in python, Py spark and spark sql\nImplement Data security at DB and data movement layers\nShould have experience in ci/cd data pipelines\nWork with internal teams to design, develop and maintain software Qualifications & Key skills required:\nExpertise in Datalakes, Lakehouse, Synapse Analytics, Data bricks, Tsql, sql server, Synapse Db, Data warehouse\nHands-on experience in ETL, ELT, handling large volume of data and files.\nWorking knowledge in json, parquet, csv, xl, structured, unstructured data and other data sets\nExposure to any Source Control Management, like TFS/Git/SVN\nUnderstanding of non-functional requirements\nShould be proficient in Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems\nExperience in Azure Data Fabric, MS Purview, MDM tools is an added advantage\nA good team player and excellent communicator","['Azure Data Engineer', 'Dwh', 'OLAP', 'Oltp']",2025-06-10 19:38:15
Azure Data Engineer,Aspire Systems India Private Limited,4-9 Years,,"Bengaluru, Chennai",Software,"We are seeking an experienced Azure Data Engineer with over 4 to 13 years of proven expertise in Data lakes, Lake house, Synapse Analytic, Data bricks, Tsql, sql server, Synapse Db, Data warehouse and should have work exp in ETL, Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems with strong communication skills.\nRequirements:\nWe are seeking an experienced Azure Data Engineer with over 4 to 13 years of proven expertise in Data lakes, Lake house, Synapse Analytic, Data bricks, Tsql, sql server, Synapse Db, Data warehouse and should have work exp in ETL, Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems with strong communication skills.\nThe ideal candidate should have:\nKey Responsibilities:\nCreate Data lakes from scratch, configure existing systems and provide user support\nUnderstand different datasets and Storage elements to bring data\nHave good knowledge and work experience in ADF, Synapse Data pipelines\nHave good knowledge in python, Py spark and spark sql\nImplement Data security at DB and data movement layers\nShould have experience in ci/cd data pipelines\nWork with internal teams to design, develop and maintain software Qualifications & Key skills required:\nExpertise in Datalakes, Lakehouse, Synapse Analytics, Data bricks, Tsql, sql server, Synapse Db, Data warehouse\nHands-on experience in ETL, ELT, handling large volume of data and files.\nWorking knowledge in json, parquet, csv, xl, structured, unstructured data and other data sets\nExposure to any Source Control Management, like TFS/Git/SVN\nUnderstanding of non-functional requirements\nShould be proficient in Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems\nExperience in Azure Data Fabric, MS Purview, MDM tools is an added advantage\nA good team player and excellent communicator","['Azure Data Engineer', 'Dwh', 'OLAP', 'Oltp']",2025-06-10 19:38:18
Azure Data Engineer,Aspire Systems India Private Limited,4-9 Years,,"Bengaluru, Chennai",Software,"We are seeking an experienced Azure Data Engineer with over 4 to 13 years of proven expertise in Data lakes, Lake house, Synapse Analytic, Data bricks, Tsql, sql server, Synapse Db, Data warehouse and should have work exp in ETL, Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems with strong communication skills.\nRequirements:\nWe are seeking an experienced Azure Data Engineer with over 4 to 13 years of proven expertise in Data lakes, Lake house, Synapse Analytic, Data bricks, Tsql, sql server, Synapse Db, Data warehouse and should have work exp in ETL, Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems with strong communication skills.\nThe ideal candidate should have:\nKey Responsibilities:\nCreate Data lakes from scratch, configure existing systems and provide user support\nUnderstand different datasets and Storage elements to bring data\nHave good knowledge and work experience in ADF, Synapse Data pipelines\nHave good knowledge in python, Py spark and spark sql\nImplement Data security at DB and data movement layers\nShould have experience in ci/cd data pipelines\nWork with internal teams to design, develop and maintain software Qualifications & Key skills required:\nExpertise in Datalakes, Lakehouse, Synapse Analytics, Data bricks, Tsql, sql server, Synapse Db, Data warehouse\nHands-on experience in ETL, ELT, handling large volume of data and files.\nWorking knowledge in json, parquet, csv, xl, structured, unstructured data and other data sets\nExposure to any Source Control Management, like TFS/Git/SVN\nUnderstanding of non-functional requirements\nShould be proficient in Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems\nExperience in Azure Data Fabric, MS Purview, MDM tools is an added advantage\nA good team player and excellent communicator","['Azure Data Engineer', 'Dwh', 'OLAP', 'Oltp']",2025-06-10 19:38:21
Azure Data Engineer,Aspire Systems India Private Limited,4-9 Years,,"Bengaluru, Chennai",Software,"We are seeking an experienced Azure Data Engineer with over 4 to 13 years of proven expertise in Data lakes, Lake house, Synapse Analytic, Data bricks, Tsql, sql server, Synapse Db, Data warehouse and should have work exp in ETL, Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems with strong communication skills.\nRequirements:\nWe are seeking an experienced Azure Data Engineer with over 4 to 13 years of proven expertise in Data lakes, Lake house, Synapse Analytic, Data bricks, Tsql, sql server, Synapse Db, Data warehouse and should have work exp in ETL, Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems with strong communication skills.\nThe ideal candidate should have:\nKey Responsibilities:\nCreate Data lakes from scratch, configure existing systems and provide user support\nUnderstand different datasets and Storage elements to bring data\nHave good knowledge and work experience in ADF, Synapse Data pipelines\nHave good knowledge in python, Py spark and spark sql\nImplement Data security at DB and data movement layers\nShould have experience in ci/cd data pipelines\nWork with internal teams to design, develop and maintain software Qualifications & Key skills required:\nExpertise in Datalakes, Lakehouse, Synapse Analytics, Data bricks, Tsql, sql server, Synapse Db, Data warehouse\nHands-on experience in ETL, ELT, handling large volume of data and files.\nWorking knowledge in json, parquet, csv, xl, structured, unstructured data and other data sets\nExposure to any Source Control Management, like TFS/Git/SVN\nUnderstanding of non-functional requirements\nShould be proficient in Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems\nExperience in Azure Data Fabric, MS Purview, MDM tools is an added advantage\nA good team player and excellent communicator","['Azure Data Engineer', 'Dwh', 'OLAP', 'Oltp']",2025-06-10 19:38:22
Azure Data Engineer,Aspire Systems India Private Limited,4-9 Years,,"Bengaluru, Chennai",Software,"We are seeking an experienced Azure Data Engineer with over 4 to 13 years of proven expertise in Data lakes, Lake house, Synapse Analytic, Data bricks, Tsql, sql server, Synapse Db, Data warehouse and should have work exp in ETL, Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems with strong communication skills.\nRequirements:\nWe are seeking an experienced Azure Data Engineer with over 4 to 13 years of proven expertise in Data lakes, Lake house, Synapse Analytic, Data bricks, Tsql, sql server, Synapse Db, Data warehouse and should have work exp in ETL, Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems with strong communication skills.\nThe ideal candidate should have:\nKey Responsibilities:\nCreate Data lakes from scratch, configure existing systems and provide user support\nUnderstand different datasets and Storage elements to bring data\nHave good knowledge and work experience in ADF, Synapse Data pipelines\nHave good knowledge in python, Py spark and spark sql\nImplement Data security at DB and data movement layers\nShould have experience in ci/cd data pipelines\nWork with internal teams to design, develop and maintain software Qualifications & Key skills required:\nExpertise in Datalakes, Lakehouse, Synapse Analytics, Data bricks, Tsql, sql server, Synapse Db, Data warehouse\nHands-on experience in ETL, ELT, handling large volume of data and files.\nWorking knowledge in json, parquet, csv, xl, structured, unstructured data and other data sets\nExposure to any Source Control Management, like TFS/Git/SVN\nUnderstanding of non-functional requirements\nShould be proficient in Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems\nExperience in Azure Data Fabric, MS Purview, MDM tools is an added advantage\nA good team player and excellent communicator","['Azure Data Engineer', 'Dwh', 'OLAP', 'Oltp']",2025-06-10 19:38:23
Azure Data Engineer,Aspire Systems India Private Limited,4-9 Years,,"Bengaluru, Chennai",Software,"We are seeking an experienced Azure Data Engineer with over 4 to 13 years of proven expertise in Data lakes, Lake house, Synapse Analytic, Data bricks, Tsql, sql server, Synapse Db, Data warehouse and should have work exp in ETL, Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems with strong communication skills.\nRequirements:\nWe are seeking an experienced Azure Data Engineer with over 4 to 13 years of proven expertise in Data lakes, Lake house, Synapse Analytic, Data bricks, Tsql, sql server, Synapse Db, Data warehouse and should have work exp in ETL, Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems with strong communication skills.\nThe ideal candidate should have:\nKey Responsibilities:\nCreate Data lakes from scratch, configure existing systems and provide user support\nUnderstand different datasets and Storage elements to bring data\nHave good knowledge and work experience in ADF, Synapse Data pipelines\nHave good knowledge in python, Py spark and spark sql\nImplement Data security at DB and data movement layers\nShould have experience in ci/cd data pipelines\nWork with internal teams to design, develop and maintain software Qualifications & Key skills required:\nExpertise in Datalakes, Lakehouse, Synapse Analytics, Data bricks, Tsql, sql server, Synapse Db, Data warehouse\nHands-on experience in ETL, ELT, handling large volume of data and files.\nWorking knowledge in json, parquet, csv, xl, structured, unstructured data and other data sets\nExposure to any Source Control Management, like TFS/Git/SVN\nUnderstanding of non-functional requirements\nShould be proficient in Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems\nExperience in Azure Data Fabric, MS Purview, MDM tools is an added advantage\nA good team player and excellent communicator","['Azure Data Engineer', 'Dwh', 'OLAP', 'Oltp']",2025-06-10 19:38:23
Azure Data Engineer,Aspire Systems India Private Limited,4-9 Years,,"Bengaluru, Chennai",Software,"We are seeking an experienced Azure Data Engineer with over 4 to 13 years of proven expertise in Data lakes, Lake house, Synapse Analytic, Data bricks, Tsql, sql server, Synapse Db, Data warehouse and should have work exp in ETL, Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems with strong communication skills.\nRequirements:\nWe are seeking an experienced Azure Data Engineer with over 4 to 13 years of proven expertise in Data lakes, Lake house, Synapse Analytic, Data bricks, Tsql, sql server, Synapse Db, Data warehouse and should have work exp in ETL, Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems with strong communication skills.\nThe ideal candidate should have:\nKey Responsibilities:\nCreate Data lakes from scratch, configure existing systems and provide user support\nUnderstand different datasets and Storage elements to bring data\nHave good knowledge and work experience in ADF, Synapse Data pipelines\nHave good knowledge in python, Py spark and spark sql\nImplement Data security at DB and data movement layers\nShould have experience in ci/cd data pipelines\nWork with internal teams to design, develop and maintain software Qualifications & Key skills required:\nExpertise in Datalakes, Lakehouse, Synapse Analytics, Data bricks, Tsql, sql server, Synapse Db, Data warehouse\nHands-on experience in ETL, ELT, handling large volume of data and files.\nWorking knowledge in json, parquet, csv, xl, structured, unstructured data and other data sets\nExposure to any Source Control Management, like TFS/Git/SVN\nUnderstanding of non-functional requirements\nShould be proficient in Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems\nExperience in Azure Data Fabric, MS Purview, MDM tools is an added advantage\nA good team player and excellent communicator","['Azure Data Engineer', 'Dwh', 'OLAP', 'Oltp']",2025-06-10 19:38:24
Azure Data Engineer,Aspire Systems India Private Limited,4-9 Years,,"Bengaluru, Chennai",Software,"We are seeking an experienced Azure Data Engineer with over 4 to 13 years of proven expertise in Data lakes, Lake house, Synapse Analytic, Data bricks, Tsql, sql server, Synapse Db, Data warehouse and should have work exp in ETL, Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems with strong communication skills.\nRequirements:\nWe are seeking an experienced Azure Data Engineer with over 4 to 13 years of proven expertise in Data lakes, Lake house, Synapse Analytic, Data bricks, Tsql, sql server, Synapse Db, Data warehouse and should have work exp in ETL, Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems with strong communication skills.\nThe ideal candidate should have:\nKey Responsibilities:\nCreate Data lakes from scratch, configure existing systems and provide user support\nUnderstand different datasets and Storage elements to bring data\nHave good knowledge and work experience in ADF, Synapse Data pipelines\nHave good knowledge in python, Py spark and spark sql\nImplement Data security at DB and data movement layers\nShould have experience in ci/cd data pipelines\nWork with internal teams to design, develop and maintain software Qualifications & Key skills required:\nExpertise in Datalakes, Lakehouse, Synapse Analytics, Data bricks, Tsql, sql server, Synapse Db, Data warehouse\nHands-on experience in ETL, ELT, handling large volume of data and files.\nWorking knowledge in json, parquet, csv, xl, structured, unstructured data and other data sets\nExposure to any Source Control Management, like TFS/Git/SVN\nUnderstanding of non-functional requirements\nShould be proficient in Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems\nExperience in Azure Data Fabric, MS Purview, MDM tools is an added advantage\nA good team player and excellent communicator","['Azure Data Engineer', 'Dwh', 'OLAP', 'Oltp']",2025-06-10 19:38:25
Azure Data Engineer,Aspire Systems India Private Limited,4-9 Years,,"Bengaluru, Chennai",Software,"We are seeking an experienced Azure Data Engineer with over 4 to 13 years of proven expertise in Data lakes, Lake house, Synapse Analytic, Data bricks, Tsql, sql server, Synapse Db, Data warehouse and should have work exp in ETL, Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems with strong communication skills.\nRequirements:\nWe are seeking an experienced Azure Data Engineer with over 4 to 13 years of proven expertise in Data lakes, Lake house, Synapse Analytic, Data bricks, Tsql, sql server, Synapse Db, Data warehouse and should have work exp in ETL, Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems with strong communication skills.\nThe ideal candidate should have:\nKey Responsibilities:\nCreate Data lakes from scratch, configure existing systems and provide user support\nUnderstand different datasets and Storage elements to bring data\nHave good knowledge and work experience in ADF, Synapse Data pipelines\nHave good knowledge in python, Py spark and spark sql\nImplement Data security at DB and data movement layers\nShould have experience in ci/cd data pipelines\nWork with internal teams to design, develop and maintain software Qualifications & Key skills required:\nExpertise in Datalakes, Lakehouse, Synapse Analytics, Data bricks, Tsql, sql server, Synapse Db, Data warehouse\nHands-on experience in ETL, ELT, handling large volume of data and files.\nWorking knowledge in json, parquet, csv, xl, structured, unstructured data and other data sets\nExposure to any Source Control Management, like TFS/Git/SVN\nUnderstanding of non-functional requirements\nShould be proficient in Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems\nExperience in Azure Data Fabric, MS Purview, MDM tools is an added advantage\nA good team player and excellent communicator","['Azure Data Engineer', 'Dwh', 'OLAP', 'Oltp']",2025-06-10 19:38:26
Azure Data Engineer,Aspire Systems India Private Limited,4-9 Years,,"Bengaluru, Chennai",Software,"We are seeking an experienced Azure Data Engineer with over 4 to 13 years of proven expertise in Data lakes, Lake house, Synapse Analytic, Data bricks, Tsql, sql server, Synapse Db, Data warehouse and should have work exp in ETL, Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems with strong communication skills.\nRequirements:\nWe are seeking an experienced Azure Data Engineer with over 4 to 13 years of proven expertise in Data lakes, Lake house, Synapse Analytic, Data bricks, Tsql, sql server, Synapse Db, Data warehouse and should have work exp in ETL, Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems with strong communication skills.\nThe ideal candidate should have:\nKey Responsibilities:\nCreate Data lakes from scratch, configure existing systems and provide user support\nUnderstand different datasets and Storage elements to bring data\nHave good knowledge and work experience in ADF, Synapse Data pipelines\nHave good knowledge in python, Py spark and spark sql\nImplement Data security at DB and data movement layers\nShould have experience in ci/cd data pipelines\nWork with internal teams to design, develop and maintain software Qualifications & Key skills required:\nExpertise in Datalakes, Lakehouse, Synapse Analytics, Data bricks, Tsql, sql server, Synapse Db, Data warehouse\nHands-on experience in ETL, ELT, handling large volume of data and files.\nWorking knowledge in json, parquet, csv, xl, structured, unstructured data and other data sets\nExposure to any Source Control Management, like TFS/Git/SVN\nUnderstanding of non-functional requirements\nShould be proficient in Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems\nExperience in Azure Data Fabric, MS Purview, MDM tools is an added advantage\nA good team player and excellent communicator","['Azure Data Engineer', 'Dwh', 'OLAP', 'Oltp']",2025-06-10 19:38:27
Azure Data Engineer,Aspire Systems India Private Limited,4-9 Years,,"Bengaluru, Chennai",Software,"We are seeking an experienced Azure Data Engineer with over 4 to 13 years of proven expertise in Data lakes, Lake house, Synapse Analytic, Data bricks, Tsql, sql server, Synapse Db, Data warehouse and should have work exp in ETL, Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems with strong communication skills.\nRequirements:\nWe are seeking an experienced Azure Data Engineer with over 4 to 13 years of proven expertise in Data lakes, Lake house, Synapse Analytic, Data bricks, Tsql, sql server, Synapse Db, Data warehouse and should have work exp in ETL, Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems with strong communication skills.\nThe ideal candidate should have:\nKey Responsibilities:\nCreate Data lakes from scratch, configure existing systems and provide user support\nUnderstand different datasets and Storage elements to bring data\nHave good knowledge and work experience in ADF, Synapse Data pipelines\nHave good knowledge in python, Py spark and spark sql\nImplement Data security at DB and data movement layers\nShould have experience in ci/cd data pipelines\nWork with internal teams to design, develop and maintain software Qualifications & Key skills required:\nExpertise in Datalakes, Lakehouse, Synapse Analytics, Data bricks, Tsql, sql server, Synapse Db, Data warehouse\nHands-on experience in ETL, ELT, handling large volume of data and files.\nWorking knowledge in json, parquet, csv, xl, structured, unstructured data and other data sets\nExposure to any Source Control Management, like TFS/Git/SVN\nUnderstanding of non-functional requirements\nShould be proficient in Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems\nExperience in Azure Data Fabric, MS Purview, MDM tools is an added advantage\nA good team player and excellent communicator","['Azure Data Engineer', 'Dwh', 'OLAP', 'Oltp']",2025-06-10 19:38:27
Azure Data Engineer,Aspire Systems India Private Limited,4-9 Years,,"Bengaluru, Chennai",Software,"We are seeking an experienced Azure Data Engineer with over 4 to 13 years of proven expertise in Data lakes, Lake house, Synapse Analytic, Data bricks, Tsql, sql server, Synapse Db, Data warehouse and should have work exp in ETL, Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems with strong communication skills.\nRequirements:\nWe are seeking an experienced Azure Data Engineer with over 4 to 13 years of proven expertise in Data lakes, Lake house, Synapse Analytic, Data bricks, Tsql, sql server, Synapse Db, Data warehouse and should have work exp in ETL, Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems with strong communication skills.\nThe ideal candidate should have:\nKey Responsibilities:\nCreate Data lakes from scratch, configure existing systems and provide user support\nUnderstand different datasets and Storage elements to bring data\nHave good knowledge and work experience in ADF, Synapse Data pipelines\nHave good knowledge in python, Py spark and spark sql\nImplement Data security at DB and data movement layers\nShould have experience in ci/cd data pipelines\nWork with internal teams to design, develop and maintain software Qualifications & Key skills required:\nExpertise in Datalakes, Lakehouse, Synapse Analytics, Data bricks, Tsql, sql server, Synapse Db, Data warehouse\nHands-on experience in ETL, ELT, handling large volume of data and files.\nWorking knowledge in json, parquet, csv, xl, structured, unstructured data and other data sets\nExposure to any Source Control Management, like TFS/Git/SVN\nUnderstanding of non-functional requirements\nShould be proficient in Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems\nExperience in Azure Data Fabric, MS Purview, MDM tools is an added advantage\nA good team player and excellent communicator","['Azure Data Engineer', 'Dwh', 'OLAP', 'Oltp']",2025-06-10 19:38:28
Azure Data Engineer,Aspire Systems India Private Limited,4-9 Years,,"Bengaluru, Chennai",Software,"We are seeking an experienced Azure Data Engineer with over 4 to 13 years of proven expertise in Data lakes, Lake house, Synapse Analytic, Data bricks, Tsql, sql server, Synapse Db, Data warehouse and should have work exp in ETL, Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems with strong communication skills.\nRequirements:\nWe are seeking an experienced Azure Data Engineer with over 4 to 13 years of proven expertise in Data lakes, Lake house, Synapse Analytic, Data bricks, Tsql, sql server, Synapse Db, Data warehouse and should have work exp in ETL, Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems with strong communication skills.\nThe ideal candidate should have:\nKey Responsibilities:\nCreate Data lakes from scratch, configure existing systems and provide user support\nUnderstand different datasets and Storage elements to bring data\nHave good knowledge and work experience in ADF, Synapse Data pipelines\nHave good knowledge in python, Py spark and spark sql\nImplement Data security at DB and data movement layers\nShould have experience in ci/cd data pipelines\nWork with internal teams to design, develop and maintain software Qualifications & Key skills required:\nExpertise in Datalakes, Lakehouse, Synapse Analytics, Data bricks, Tsql, sql server, Synapse Db, Data warehouse\nHands-on experience in ETL, ELT, handling large volume of data and files.\nWorking knowledge in json, parquet, csv, xl, structured, unstructured data and other data sets\nExposure to any Source Control Management, like TFS/Git/SVN\nUnderstanding of non-functional requirements\nShould be proficient in Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems\nExperience in Azure Data Fabric, MS Purview, MDM tools is an added advantage\nA good team player and excellent communicator","['Azure Data Engineer', 'Dwh', 'OLAP', 'Oltp']",2025-06-10 19:38:29
Azure Data Engineer,Aspire Systems India Private Limited,4-9 Years,,"Bengaluru, Chennai",Software,"We are seeking an experienced Azure Data Engineer with over 4 to 13 years of proven expertise in Data lakes, Lake house, Synapse Analytic, Data bricks, Tsql, sql server, Synapse Db, Data warehouse and should have work exp in ETL, Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems with strong communication skills.\nRequirements:\nWe are seeking an experienced Azure Data Engineer with over 4 to 13 years of proven expertise in Data lakes, Lake house, Synapse Analytic, Data bricks, Tsql, sql server, Synapse Db, Data warehouse and should have work exp in ETL, Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems with strong communication skills.\nThe ideal candidate should have:\nKey Responsibilities:\nCreate Data lakes from scratch, configure existing systems and provide user support\nUnderstand different datasets and Storage elements to bring data\nHave good knowledge and work experience in ADF, Synapse Data pipelines\nHave good knowledge in python, Py spark and spark sql\nImplement Data security at DB and data movement layers\nShould have experience in ci/cd data pipelines\nWork with internal teams to design, develop and maintain software Qualifications & Key skills required:\nExpertise in Datalakes, Lakehouse, Synapse Analytics, Data bricks, Tsql, sql server, Synapse Db, Data warehouse\nHands-on experience in ETL, ELT, handling large volume of data and files.\nWorking knowledge in json, parquet, csv, xl, structured, unstructured data and other data sets\nExposure to any Source Control Management, like TFS/Git/SVN\nUnderstanding of non-functional requirements\nShould be proficient in Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems\nExperience in Azure Data Fabric, MS Purview, MDM tools is an added advantage\nA good team player and excellent communicator","['Azure Data Engineer', 'Dwh', 'OLAP', 'Oltp']",2025-06-10 19:38:30
Azure Data Engineer,Aspire Systems India Private Limited,4-9 Years,,"Bengaluru, Chennai",Software,"We are seeking an experienced Azure Data Engineer with over 4 to 13 years of proven expertise in Data lakes, Lake house, Synapse Analytic, Data bricks, Tsql, sql server, Synapse Db, Data warehouse and should have work exp in ETL, Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems with strong communication skills.\nRequirements:\nWe are seeking an experienced Azure Data Engineer with over 4 to 13 years of proven expertise in Data lakes, Lake house, Synapse Analytic, Data bricks, Tsql, sql server, Synapse Db, Data warehouse and should have work exp in ETL, Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems with strong communication skills.\nThe ideal candidate should have:\nKey Responsibilities:\nCreate Data lakes from scratch, configure existing systems and provide user support\nUnderstand different datasets and Storage elements to bring data\nHave good knowledge and work experience in ADF, Synapse Data pipelines\nHave good knowledge in python, Py spark and spark sql\nImplement Data security at DB and data movement layers\nShould have experience in ci/cd data pipelines\nWork with internal teams to design, develop and maintain software Qualifications & Key skills required:\nExpertise in Datalakes, Lakehouse, Synapse Analytics, Data bricks, Tsql, sql server, Synapse Db, Data warehouse\nHands-on experience in ETL, ELT, handling large volume of data and files.\nWorking knowledge in json, parquet, csv, xl, structured, unstructured data and other data sets\nExposure to any Source Control Management, like TFS/Git/SVN\nUnderstanding of non-functional requirements\nShould be proficient in Data catalogs, Meta data, DWH, mpp systems, OLTP, and OLAP systems\nExperience in Azure Data Fabric, MS Purview, MDM tools is an added advantage\nA good team player and excellent communicator","['Azure Data Engineer', 'Dwh', 'OLAP', 'Oltp']",2025-06-10 19:38:31
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,Software,"n this role, you'll work at one of our IBM Consulting Client Innovation Centers (Delivery Centers). These centers are where we deliver deep technical and industry expertise to a wide range of public and private sector clients globally. Our delivery centers offer clients locally based skills and technical expertise to drive innovation and adopt new technologies.\nYour Role and Responsibilities\nAs an Associate Software Developer at IBM, you will:\nHarness the power of data to unveil captivating stories and intricate patterns.\nContribute to data gathering, storage, and both batch and real-time processing.\nCollaborate closely with diverse teams, playing an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nYour responsibilities may include:\nImplementing and validating predictive models, as well as creating and maintaining statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques.\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants, and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or write programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nDevelop/Convert the database (Hadoop to GCP) of specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform.\nImplementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API.\nParticipate in the modernization roadmap journey.\nAnalyze discovery and analysis outcomes.\nLead discovery and analysis workshops/playbacks.\nIdentification of application dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare effort estimates, WBS, staffing plan, RACI, RAID, etc.\nLeads the team to adopt the right tools for various migration and modernization methods.\nPreferred Technical and Professional Experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions.\nAbility to communicate results to technical and non-technical audiences.","['cdc', 'data engineering', 'Hadoop', 'Gcp', 'Api Development', 'Predictive Modeling']",2025-06-10 19:38:40
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,Software,"n this role, you'll work at one of our IBM Consulting Client Innovation Centers (Delivery Centers). These centers are where we deliver deep technical and industry expertise to a wide range of public and private sector clients globally. Our delivery centers offer clients locally based skills and technical expertise to drive innovation and adopt new technologies.\nYour Role and Responsibilities\nAs an Associate Software Developer at IBM, you will:\nHarness the power of data to unveil captivating stories and intricate patterns.\nContribute to data gathering, storage, and both batch and real-time processing.\nCollaborate closely with diverse teams, playing an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nYour responsibilities may include:\nImplementing and validating predictive models, as well as creating and maintaining statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques.\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants, and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or write programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nDevelop/Convert the database (Hadoop to GCP) of specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform.\nImplementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API.\nParticipate in the modernization roadmap journey.\nAnalyze discovery and analysis outcomes.\nLead discovery and analysis workshops/playbacks.\nIdentification of application dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare effort estimates, WBS, staffing plan, RACI, RAID, etc.\nLeads the team to adopt the right tools for various migration and modernization methods.\nPreferred Technical and Professional Experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions.\nAbility to communicate results to technical and non-technical audiences.","['cdc', 'data engineering', 'Hadoop', 'Gcp', 'Api Development', 'Predictive Modeling']",2025-06-10 19:38:43
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,Software,"n this role, you'll work at one of our IBM Consulting Client Innovation Centers (Delivery Centers). These centers are where we deliver deep technical and industry expertise to a wide range of public and private sector clients globally. Our delivery centers offer clients locally based skills and technical expertise to drive innovation and adopt new technologies.\nYour Role and Responsibilities\nAs an Associate Software Developer at IBM, you will:\nHarness the power of data to unveil captivating stories and intricate patterns.\nContribute to data gathering, storage, and both batch and real-time processing.\nCollaborate closely with diverse teams, playing an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nYour responsibilities may include:\nImplementing and validating predictive models, as well as creating and maintaining statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques.\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants, and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or write programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nDevelop/Convert the database (Hadoop to GCP) of specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform.\nImplementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API.\nParticipate in the modernization roadmap journey.\nAnalyze discovery and analysis outcomes.\nLead discovery and analysis workshops/playbacks.\nIdentification of application dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare effort estimates, WBS, staffing plan, RACI, RAID, etc.\nLeads the team to adopt the right tools for various migration and modernization methods.\nPreferred Technical and Professional Experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions.\nAbility to communicate results to technical and non-technical audiences.","['cdc', 'data engineering', 'Hadoop', 'Gcp', 'Api Development', 'Predictive Modeling']",2025-06-10 19:38:46
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,Software,"n this role, you'll work at one of our IBM Consulting Client Innovation Centers (Delivery Centers). These centers are where we deliver deep technical and industry expertise to a wide range of public and private sector clients globally. Our delivery centers offer clients locally based skills and technical expertise to drive innovation and adopt new technologies.\nYour Role and Responsibilities\nAs an Associate Software Developer at IBM, you will:\nHarness the power of data to unveil captivating stories and intricate patterns.\nContribute to data gathering, storage, and both batch and real-time processing.\nCollaborate closely with diverse teams, playing an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nYour responsibilities may include:\nImplementing and validating predictive models, as well as creating and maintaining statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques.\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants, and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or write programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nDevelop/Convert the database (Hadoop to GCP) of specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform.\nImplementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API.\nParticipate in the modernization roadmap journey.\nAnalyze discovery and analysis outcomes.\nLead discovery and analysis workshops/playbacks.\nIdentification of application dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare effort estimates, WBS, staffing plan, RACI, RAID, etc.\nLeads the team to adopt the right tools for various migration and modernization methods.\nPreferred Technical and Professional Experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions.\nAbility to communicate results to technical and non-technical audiences.","['cdc', 'data engineering', 'Hadoop', 'Gcp', 'Api Development', 'Predictive Modeling']",2025-06-10 19:38:47
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,Software,"n this role, you'll work at one of our IBM Consulting Client Innovation Centers (Delivery Centers). These centers are where we deliver deep technical and industry expertise to a wide range of public and private sector clients globally. Our delivery centers offer clients locally based skills and technical expertise to drive innovation and adopt new technologies.\nYour Role and Responsibilities\nAs an Associate Software Developer at IBM, you will:\nHarness the power of data to unveil captivating stories and intricate patterns.\nContribute to data gathering, storage, and both batch and real-time processing.\nCollaborate closely with diverse teams, playing an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nYour responsibilities may include:\nImplementing and validating predictive models, as well as creating and maintaining statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques.\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants, and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or write programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nDevelop/Convert the database (Hadoop to GCP) of specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform.\nImplementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API.\nParticipate in the modernization roadmap journey.\nAnalyze discovery and analysis outcomes.\nLead discovery and analysis workshops/playbacks.\nIdentification of application dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare effort estimates, WBS, staffing plan, RACI, RAID, etc.\nLeads the team to adopt the right tools for various migration and modernization methods.\nPreferred Technical and Professional Experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions.\nAbility to communicate results to technical and non-technical audiences.","['cdc', 'data engineering', 'Hadoop', 'Gcp', 'Api Development', 'Predictive Modeling']",2025-06-10 19:38:48
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,Software,"n this role, you'll work at one of our IBM Consulting Client Innovation Centers (Delivery Centers). These centers are where we deliver deep technical and industry expertise to a wide range of public and private sector clients globally. Our delivery centers offer clients locally based skills and technical expertise to drive innovation and adopt new technologies.\nYour Role and Responsibilities\nAs an Associate Software Developer at IBM, you will:\nHarness the power of data to unveil captivating stories and intricate patterns.\nContribute to data gathering, storage, and both batch and real-time processing.\nCollaborate closely with diverse teams, playing an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nYour responsibilities may include:\nImplementing and validating predictive models, as well as creating and maintaining statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques.\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants, and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or write programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nDevelop/Convert the database (Hadoop to GCP) of specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform.\nImplementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API.\nParticipate in the modernization roadmap journey.\nAnalyze discovery and analysis outcomes.\nLead discovery and analysis workshops/playbacks.\nIdentification of application dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare effort estimates, WBS, staffing plan, RACI, RAID, etc.\nLeads the team to adopt the right tools for various migration and modernization methods.\nPreferred Technical and Professional Experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions.\nAbility to communicate results to technical and non-technical audiences.","['cdc', 'data engineering', 'Hadoop', 'Gcp', 'Api Development', 'Predictive Modeling']",2025-06-10 19:38:48
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,Software,"n this role, you'll work at one of our IBM Consulting Client Innovation Centers (Delivery Centers). These centers are where we deliver deep technical and industry expertise to a wide range of public and private sector clients globally. Our delivery centers offer clients locally based skills and technical expertise to drive innovation and adopt new technologies.\nYour Role and Responsibilities\nAs an Associate Software Developer at IBM, you will:\nHarness the power of data to unveil captivating stories and intricate patterns.\nContribute to data gathering, storage, and both batch and real-time processing.\nCollaborate closely with diverse teams, playing an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nYour responsibilities may include:\nImplementing and validating predictive models, as well as creating and maintaining statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques.\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants, and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or write programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nDevelop/Convert the database (Hadoop to GCP) of specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform.\nImplementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API.\nParticipate in the modernization roadmap journey.\nAnalyze discovery and analysis outcomes.\nLead discovery and analysis workshops/playbacks.\nIdentification of application dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare effort estimates, WBS, staffing plan, RACI, RAID, etc.\nLeads the team to adopt the right tools for various migration and modernization methods.\nPreferred Technical and Professional Experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions.\nAbility to communicate results to technical and non-technical audiences.","['cdc', 'data engineering', 'Hadoop', 'Gcp', 'Api Development', 'Predictive Modeling']",2025-06-10 19:38:49
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,Software,"n this role, you'll work at one of our IBM Consulting Client Innovation Centers (Delivery Centers). These centers are where we deliver deep technical and industry expertise to a wide range of public and private sector clients globally. Our delivery centers offer clients locally based skills and technical expertise to drive innovation and adopt new technologies.\nYour Role and Responsibilities\nAs an Associate Software Developer at IBM, you will:\nHarness the power of data to unveil captivating stories and intricate patterns.\nContribute to data gathering, storage, and both batch and real-time processing.\nCollaborate closely with diverse teams, playing an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nYour responsibilities may include:\nImplementing and validating predictive models, as well as creating and maintaining statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques.\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants, and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or write programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nDevelop/Convert the database (Hadoop to GCP) of specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform.\nImplementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API.\nParticipate in the modernization roadmap journey.\nAnalyze discovery and analysis outcomes.\nLead discovery and analysis workshops/playbacks.\nIdentification of application dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare effort estimates, WBS, staffing plan, RACI, RAID, etc.\nLeads the team to adopt the right tools for various migration and modernization methods.\nPreferred Technical and Professional Experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions.\nAbility to communicate results to technical and non-technical audiences.","['cdc', 'data engineering', 'Hadoop', 'Gcp', 'Api Development', 'Predictive Modeling']",2025-06-10 19:38:50
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,Software,"n this role, you'll work at one of our IBM Consulting Client Innovation Centers (Delivery Centers). These centers are where we deliver deep technical and industry expertise to a wide range of public and private sector clients globally. Our delivery centers offer clients locally based skills and technical expertise to drive innovation and adopt new technologies.\nYour Role and Responsibilities\nAs an Associate Software Developer at IBM, you will:\nHarness the power of data to unveil captivating stories and intricate patterns.\nContribute to data gathering, storage, and both batch and real-time processing.\nCollaborate closely with diverse teams, playing an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nYour responsibilities may include:\nImplementing and validating predictive models, as well as creating and maintaining statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques.\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants, and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or write programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nDevelop/Convert the database (Hadoop to GCP) of specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform.\nImplementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API.\nParticipate in the modernization roadmap journey.\nAnalyze discovery and analysis outcomes.\nLead discovery and analysis workshops/playbacks.\nIdentification of application dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare effort estimates, WBS, staffing plan, RACI, RAID, etc.\nLeads the team to adopt the right tools for various migration and modernization methods.\nPreferred Technical and Professional Experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions.\nAbility to communicate results to technical and non-technical audiences.","['cdc', 'data engineering', 'Hadoop', 'Gcp', 'Api Development', 'Predictive Modeling']",2025-06-10 19:38:51
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,Software,"n this role, you'll work at one of our IBM Consulting Client Innovation Centers (Delivery Centers). These centers are where we deliver deep technical and industry expertise to a wide range of public and private sector clients globally. Our delivery centers offer clients locally based skills and technical expertise to drive innovation and adopt new technologies.\nYour Role and Responsibilities\nAs an Associate Software Developer at IBM, you will:\nHarness the power of data to unveil captivating stories and intricate patterns.\nContribute to data gathering, storage, and both batch and real-time processing.\nCollaborate closely with diverse teams, playing an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nYour responsibilities may include:\nImplementing and validating predictive models, as well as creating and maintaining statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques.\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants, and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or write programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nDevelop/Convert the database (Hadoop to GCP) of specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform.\nImplementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API.\nParticipate in the modernization roadmap journey.\nAnalyze discovery and analysis outcomes.\nLead discovery and analysis workshops/playbacks.\nIdentification of application dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare effort estimates, WBS, staffing plan, RACI, RAID, etc.\nLeads the team to adopt the right tools for various migration and modernization methods.\nPreferred Technical and Professional Experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions.\nAbility to communicate results to technical and non-technical audiences.","['cdc', 'data engineering', 'Hadoop', 'Gcp', 'Api Development', 'Predictive Modeling']",2025-06-10 19:38:52
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,Software,"n this role, you'll work at one of our IBM Consulting Client Innovation Centers (Delivery Centers). These centers are where we deliver deep technical and industry expertise to a wide range of public and private sector clients globally. Our delivery centers offer clients locally based skills and technical expertise to drive innovation and adopt new technologies.\nYour Role and Responsibilities\nAs an Associate Software Developer at IBM, you will:\nHarness the power of data to unveil captivating stories and intricate patterns.\nContribute to data gathering, storage, and both batch and real-time processing.\nCollaborate closely with diverse teams, playing an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nYour responsibilities may include:\nImplementing and validating predictive models, as well as creating and maintaining statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques.\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants, and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or write programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nDevelop/Convert the database (Hadoop to GCP) of specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform.\nImplementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API.\nParticipate in the modernization roadmap journey.\nAnalyze discovery and analysis outcomes.\nLead discovery and analysis workshops/playbacks.\nIdentification of application dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare effort estimates, WBS, staffing plan, RACI, RAID, etc.\nLeads the team to adopt the right tools for various migration and modernization methods.\nPreferred Technical and Professional Experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions.\nAbility to communicate results to technical and non-technical audiences.","['cdc', 'data engineering', 'Hadoop', 'Gcp', 'Api Development', 'Predictive Modeling']",2025-06-10 19:38:53
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,Software,"n this role, you'll work at one of our IBM Consulting Client Innovation Centers (Delivery Centers). These centers are where we deliver deep technical and industry expertise to a wide range of public and private sector clients globally. Our delivery centers offer clients locally based skills and technical expertise to drive innovation and adopt new technologies.\nYour Role and Responsibilities\nAs an Associate Software Developer at IBM, you will:\nHarness the power of data to unveil captivating stories and intricate patterns.\nContribute to data gathering, storage, and both batch and real-time processing.\nCollaborate closely with diverse teams, playing an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nYour responsibilities may include:\nImplementing and validating predictive models, as well as creating and maintaining statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques.\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants, and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or write programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nDevelop/Convert the database (Hadoop to GCP) of specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform.\nImplementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API.\nParticipate in the modernization roadmap journey.\nAnalyze discovery and analysis outcomes.\nLead discovery and analysis workshops/playbacks.\nIdentification of application dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare effort estimates, WBS, staffing plan, RACI, RAID, etc.\nLeads the team to adopt the right tools for various migration and modernization methods.\nPreferred Technical and Professional Experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions.\nAbility to communicate results to technical and non-technical audiences.","['cdc', 'data engineering', 'Hadoop', 'Gcp', 'Api Development', 'Predictive Modeling']",2025-06-10 19:38:54
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,Software,"n this role, you'll work at one of our IBM Consulting Client Innovation Centers (Delivery Centers). These centers are where we deliver deep technical and industry expertise to a wide range of public and private sector clients globally. Our delivery centers offer clients locally based skills and technical expertise to drive innovation and adopt new technologies.\nYour Role and Responsibilities\nAs an Associate Software Developer at IBM, you will:\nHarness the power of data to unveil captivating stories and intricate patterns.\nContribute to data gathering, storage, and both batch and real-time processing.\nCollaborate closely with diverse teams, playing an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nYour responsibilities may include:\nImplementing and validating predictive models, as well as creating and maintaining statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques.\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants, and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or write programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nDevelop/Convert the database (Hadoop to GCP) of specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform.\nImplementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API.\nParticipate in the modernization roadmap journey.\nAnalyze discovery and analysis outcomes.\nLead discovery and analysis workshops/playbacks.\nIdentification of application dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare effort estimates, WBS, staffing plan, RACI, RAID, etc.\nLeads the team to adopt the right tools for various migration and modernization methods.\nPreferred Technical and Professional Experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions.\nAbility to communicate results to technical and non-technical audiences.","['cdc', 'data engineering', 'Hadoop', 'Gcp', 'Api Development', 'Predictive Modeling']",2025-06-10 19:38:54
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,Software,"n this role, you'll work at one of our IBM Consulting Client Innovation Centers (Delivery Centers). These centers are where we deliver deep technical and industry expertise to a wide range of public and private sector clients globally. Our delivery centers offer clients locally based skills and technical expertise to drive innovation and adopt new technologies.\nYour Role and Responsibilities\nAs an Associate Software Developer at IBM, you will:\nHarness the power of data to unveil captivating stories and intricate patterns.\nContribute to data gathering, storage, and both batch and real-time processing.\nCollaborate closely with diverse teams, playing an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nYour responsibilities may include:\nImplementing and validating predictive models, as well as creating and maintaining statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques.\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants, and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or write programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nDevelop/Convert the database (Hadoop to GCP) of specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform.\nImplementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API.\nParticipate in the modernization roadmap journey.\nAnalyze discovery and analysis outcomes.\nLead discovery and analysis workshops/playbacks.\nIdentification of application dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare effort estimates, WBS, staffing plan, RACI, RAID, etc.\nLeads the team to adopt the right tools for various migration and modernization methods.\nPreferred Technical and Professional Experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions.\nAbility to communicate results to technical and non-technical audiences.","['cdc', 'data engineering', 'Hadoop', 'Gcp', 'Api Development', 'Predictive Modeling']",2025-06-10 19:38:55
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,Software,"n this role, you'll work at one of our IBM Consulting Client Innovation Centers (Delivery Centers). These centers are where we deliver deep technical and industry expertise to a wide range of public and private sector clients globally. Our delivery centers offer clients locally based skills and technical expertise to drive innovation and adopt new technologies.\nYour Role and Responsibilities\nAs an Associate Software Developer at IBM, you will:\nHarness the power of data to unveil captivating stories and intricate patterns.\nContribute to data gathering, storage, and both batch and real-time processing.\nCollaborate closely with diverse teams, playing an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nYour responsibilities may include:\nImplementing and validating predictive models, as well as creating and maintaining statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques.\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants, and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or write programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nDevelop/Convert the database (Hadoop to GCP) of specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform.\nImplementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API.\nParticipate in the modernization roadmap journey.\nAnalyze discovery and analysis outcomes.\nLead discovery and analysis workshops/playbacks.\nIdentification of application dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare effort estimates, WBS, staffing plan, RACI, RAID, etc.\nLeads the team to adopt the right tools for various migration and modernization methods.\nPreferred Technical and Professional Experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions.\nAbility to communicate results to technical and non-technical audiences.","['cdc', 'data engineering', 'Hadoop', 'Gcp', 'Api Development', 'Predictive Modeling']",2025-06-10 19:38:56
Lead Consultant - Snowflake Data Engineer,Genpact,Fresher,,Hyderabad,IT/Computers - Hardware & Networking,"With a startup spirit and 80,000+ curious and courageous minds, we have the expertise to go deep with the world&rsquos biggest brands&mdashand we have fun doing it. Now, we&rsquore calling all you rule-breakers and risk-takers who see the world differently and are bold enough to reinvent it. Come, transform with us.\nInviting applications for the role of Lead Consultant - Snowflake Data Engineer!\nThe candidate will work as a Snowflake Architect with team of Quantitative Analyst and Modelers, as well as various other business and technology units. The individual will be part of an Agile team working closely with team members in India.\nResponsibilities\n. Will be responsible to plan, design, develop and maintain the data architecture, data models and standards for various Data Integration & Data warehouse projects in snowflake.\n. Ensure new features and subject areas are modelled to integrate with existing structures and provide a consistent view.\n. Develop and maintain documentation of the data architecture, data flow and data models of the data warehouse appropriate for various audiences.\n. Provide direction on adoption of Cloud technologies (Snowflake) and industry best practices in the field of data warehouse architecture and modelling.\n. providing technical leadership to large enterprise scale projects.\n. Will also be responsible for preparing estimates and defining technical solutions to proposals (RFPs).\n. This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project.\nQualifications\nMinimum qualifications\n. BE/B Tech/ MCA\n. Excellent written and verbal communication skills\n. Must have experience at least two end to end implementation of Snowflake cloud data warehouse and 3 end to end data warehouse implementations on-premise .\n. Expertise in Snowflake - data modelling, ELT using Snowflake SQL, implementing complex stored Procedures and standard DWH and ETL concepts.\n. Expertise in Snowflake advanced concepts like setting up resource monitors, RBAC controls, virtual warehouse sizing, query performance tuning,\n. Zero copy clone, time travel and understand how to use these features.\n. Expertise in deploying Snowflake features such as data sharing, events and lake-house patterns.\n. Hands-on experience with Snowflake utilities, SnowSQL, SnowPipe, Big Data model techniques using Python.\n. Experience in Data Migration from RDBMS to Snowflake cloud data warehouse.\n\nPreferred qualifications\n. Experience with data security and data access controls and design.\n. Experience with AWS or Azure data storage and management technologies such as S3 and ADLS.\n. Must have experience in DBT, Fivetran, GCP, Github\n. Deep understanding of relational as well as NoSQL data stores, methods and approaches (star and snowflake, dimensional modelling)\n. Build processes supporting data transformation, data structures, metadata, dependency and workload management\n. Proficiency in RDBMS, complex SQL, PL/SQL.\n. Unix Shell Scripting, performance tuning and troubleshoot (additional benfit)\n. Provide resolution to an extensive range of complicated data pipeline related problems, proactively and as issues surface\n. Must have expertise in AWS or Azure Platform as a Service (PAAS).\n. Certified Snowflake cloud data warehouse Architect (Desirable).\n. Should be able to troubleshoot problems across infrastructure, platform and application domains.\n. Must have experience of Agile development methodologies\n. Strong written communication skills. Is effective and persuasive in both written and oral communication Review the architectural/ technological solutions for ongoing projects and ensure right choice of solution.\n\n\nGenpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. Genpact is committed to creating a dynamic work environment that values diversity and inclusion, respect and integrity, customer focus, and innovation. For more information, visit www.genpact.com. Follow us on Twitter, Facebook, LinkedIn, and YouTube.",[],2025-06-10 19:39:04
Lead Consultant - Snowflake Data Engineer,Genpact,Fresher,,Hyderabad,IT/Computers - Hardware & Networking,"With a startup spirit and 80,000+ curious and courageous minds, we have the expertise to go deep with the world&rsquos biggest brands&mdashand we have fun doing it. Now, we&rsquore calling all you rule-breakers and risk-takers who see the world differently and are bold enough to reinvent it. Come, transform with us.\nInviting applications for the role of Lead Consultant - Snowflake Data Engineer!\nThe candidate will work as a Snowflake Architect with team of Quantitative Analyst and Modelers, as well as various other business and technology units. The individual will be part of an Agile team working closely with team members in India.\nResponsibilities\n. Will be responsible to plan, design, develop and maintain the data architecture, data models and standards for various Data Integration & Data warehouse projects in snowflake.\n. Ensure new features and subject areas are modelled to integrate with existing structures and provide a consistent view.\n. Develop and maintain documentation of the data architecture, data flow and data models of the data warehouse appropriate for various audiences.\n. Provide direction on adoption of Cloud technologies (Snowflake) and industry best practices in the field of data warehouse architecture and modelling.\n. providing technical leadership to large enterprise scale projects.\n. Will also be responsible for preparing estimates and defining technical solutions to proposals (RFPs).\n. This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project.\nQualifications\nMinimum qualifications\n. BE/B Tech/ MCA\n. Excellent written and verbal communication skills\n. Must have experience at least two end to end implementation of Snowflake cloud data warehouse and 3 end to end data warehouse implementations on-premise .\n. Expertise in Snowflake - data modelling, ELT using Snowflake SQL, implementing complex stored Procedures and standard DWH and ETL concepts.\n. Expertise in Snowflake advanced concepts like setting up resource monitors, RBAC controls, virtual warehouse sizing, query performance tuning,\n. Zero copy clone, time travel and understand how to use these features.\n. Expertise in deploying Snowflake features such as data sharing, events and lake-house patterns.\n. Hands-on experience with Snowflake utilities, SnowSQL, SnowPipe, Big Data model techniques using Python.\n. Experience in Data Migration from RDBMS to Snowflake cloud data warehouse.\n\nPreferred qualifications\n. Experience with data security and data access controls and design.\n. Experience with AWS or Azure data storage and management technologies such as S3 and ADLS.\n. Must have experience in DBT, Fivetran, GCP, Github\n. Deep understanding of relational as well as NoSQL data stores, methods and approaches (star and snowflake, dimensional modelling)\n. Build processes supporting data transformation, data structures, metadata, dependency and workload management\n. Proficiency in RDBMS, complex SQL, PL/SQL.\n. Unix Shell Scripting, performance tuning and troubleshoot (additional benfit)\n. Provide resolution to an extensive range of complicated data pipeline related problems, proactively and as issues surface\n. Must have expertise in AWS or Azure Platform as a Service (PAAS).\n. Certified Snowflake cloud data warehouse Architect (Desirable).\n. Should be able to troubleshoot problems across infrastructure, platform and application domains.\n. Must have experience of Agile development methodologies\n. Strong written communication skills. Is effective and persuasive in both written and oral communication Review the architectural/ technological solutions for ongoing projects and ensure right choice of solution.\n\n\nGenpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. Genpact is committed to creating a dynamic work environment that values diversity and inclusion, respect and integrity, customer focus, and innovation. For more information, visit www.genpact.com. Follow us on Twitter, Facebook, LinkedIn, and YouTube.",[],2025-06-10 19:39:07
Lead Consultant - Snowflake Data Engineer,Genpact,Fresher,,Hyderabad,IT/Computers - Hardware & Networking,"With a startup spirit and 80,000+ curious and courageous minds, we have the expertise to go deep with the world&rsquos biggest brands&mdashand we have fun doing it. Now, we&rsquore calling all you rule-breakers and risk-takers who see the world differently and are bold enough to reinvent it. Come, transform with us.\nInviting applications for the role of Lead Consultant - Snowflake Data Engineer!\nThe candidate will work as a Snowflake Architect with team of Quantitative Analyst and Modelers, as well as various other business and technology units. The individual will be part of an Agile team working closely with team members in India.\nResponsibilities\n. Will be responsible to plan, design, develop and maintain the data architecture, data models and standards for various Data Integration & Data warehouse projects in snowflake.\n. Ensure new features and subject areas are modelled to integrate with existing structures and provide a consistent view.\n. Develop and maintain documentation of the data architecture, data flow and data models of the data warehouse appropriate for various audiences.\n. Provide direction on adoption of Cloud technologies (Snowflake) and industry best practices in the field of data warehouse architecture and modelling.\n. providing technical leadership to large enterprise scale projects.\n. Will also be responsible for preparing estimates and defining technical solutions to proposals (RFPs).\n. This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project.\nQualifications\nMinimum qualifications\n. BE/B Tech/ MCA\n. Excellent written and verbal communication skills\n. Must have experience at least two end to end implementation of Snowflake cloud data warehouse and 3 end to end data warehouse implementations on-premise .\n. Expertise in Snowflake - data modelling, ELT using Snowflake SQL, implementing complex stored Procedures and standard DWH and ETL concepts.\n. Expertise in Snowflake advanced concepts like setting up resource monitors, RBAC controls, virtual warehouse sizing, query performance tuning,\n. Zero copy clone, time travel and understand how to use these features.\n. Expertise in deploying Snowflake features such as data sharing, events and lake-house patterns.\n. Hands-on experience with Snowflake utilities, SnowSQL, SnowPipe, Big Data model techniques using Python.\n. Experience in Data Migration from RDBMS to Snowflake cloud data warehouse.\n\nPreferred qualifications\n. Experience with data security and data access controls and design.\n. Experience with AWS or Azure data storage and management technologies such as S3 and ADLS.\n. Must have experience in DBT, Fivetran, GCP, Github\n. Deep understanding of relational as well as NoSQL data stores, methods and approaches (star and snowflake, dimensional modelling)\n. Build processes supporting data transformation, data structures, metadata, dependency and workload management\n. Proficiency in RDBMS, complex SQL, PL/SQL.\n. Unix Shell Scripting, performance tuning and troubleshoot (additional benfit)\n. Provide resolution to an extensive range of complicated data pipeline related problems, proactively and as issues surface\n. Must have expertise in AWS or Azure Platform as a Service (PAAS).\n. Certified Snowflake cloud data warehouse Architect (Desirable).\n. Should be able to troubleshoot problems across infrastructure, platform and application domains.\n. Must have experience of Agile development methodologies\n. Strong written communication skills. Is effective and persuasive in both written and oral communication Review the architectural/ technological solutions for ongoing projects and ensure right choice of solution.\n\n\nGenpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. Genpact is committed to creating a dynamic work environment that values diversity and inclusion, respect and integrity, customer focus, and innovation. For more information, visit www.genpact.com. Follow us on Twitter, Facebook, LinkedIn, and YouTube.",[],2025-06-10 19:39:09
Lead Consultant - Snowflake Data Engineer,Genpact,Fresher,,Hyderabad,IT/Computers - Hardware & Networking,"With a startup spirit and 80,000+ curious and courageous minds, we have the expertise to go deep with the world&rsquos biggest brands&mdashand we have fun doing it. Now, we&rsquore calling all you rule-breakers and risk-takers who see the world differently and are bold enough to reinvent it. Come, transform with us.\nInviting applications for the role of Lead Consultant - Snowflake Data Engineer!\nThe candidate will work as a Snowflake Architect with team of Quantitative Analyst and Modelers, as well as various other business and technology units. The individual will be part of an Agile team working closely with team members in India.\nResponsibilities\n. Will be responsible to plan, design, develop and maintain the data architecture, data models and standards for various Data Integration & Data warehouse projects in snowflake.\n. Ensure new features and subject areas are modelled to integrate with existing structures and provide a consistent view.\n. Develop and maintain documentation of the data architecture, data flow and data models of the data warehouse appropriate for various audiences.\n. Provide direction on adoption of Cloud technologies (Snowflake) and industry best practices in the field of data warehouse architecture and modelling.\n. providing technical leadership to large enterprise scale projects.\n. Will also be responsible for preparing estimates and defining technical solutions to proposals (RFPs).\n. This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project.\nQualifications\nMinimum qualifications\n. BE/B Tech/ MCA\n. Excellent written and verbal communication skills\n. Must have experience at least two end to end implementation of Snowflake cloud data warehouse and 3 end to end data warehouse implementations on-premise .\n. Expertise in Snowflake - data modelling, ELT using Snowflake SQL, implementing complex stored Procedures and standard DWH and ETL concepts.\n. Expertise in Snowflake advanced concepts like setting up resource monitors, RBAC controls, virtual warehouse sizing, query performance tuning,\n. Zero copy clone, time travel and understand how to use these features.\n. Expertise in deploying Snowflake features such as data sharing, events and lake-house patterns.\n. Hands-on experience with Snowflake utilities, SnowSQL, SnowPipe, Big Data model techniques using Python.\n. Experience in Data Migration from RDBMS to Snowflake cloud data warehouse.\n\nPreferred qualifications\n. Experience with data security and data access controls and design.\n. Experience with AWS or Azure data storage and management technologies such as S3 and ADLS.\n. Must have experience in DBT, Fivetran, GCP, Github\n. Deep understanding of relational as well as NoSQL data stores, methods and approaches (star and snowflake, dimensional modelling)\n. Build processes supporting data transformation, data structures, metadata, dependency and workload management\n. Proficiency in RDBMS, complex SQL, PL/SQL.\n. Unix Shell Scripting, performance tuning and troubleshoot (additional benfit)\n. Provide resolution to an extensive range of complicated data pipeline related problems, proactively and as issues surface\n. Must have expertise in AWS or Azure Platform as a Service (PAAS).\n. Certified Snowflake cloud data warehouse Architect (Desirable).\n. Should be able to troubleshoot problems across infrastructure, platform and application domains.\n. Must have experience of Agile development methodologies\n. Strong written communication skills. Is effective and persuasive in both written and oral communication Review the architectural/ technological solutions for ongoing projects and ensure right choice of solution.\n\n\nGenpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. Genpact is committed to creating a dynamic work environment that values diversity and inclusion, respect and integrity, customer focus, and innovation. For more information, visit www.genpact.com. Follow us on Twitter, Facebook, LinkedIn, and YouTube.",[],2025-06-10 19:39:11
Lead Consultant - Snowflake Data Engineer,Genpact,Fresher,,Hyderabad,IT/Computers - Hardware & Networking,"With a startup spirit and 80,000+ curious and courageous minds, we have the expertise to go deep with the world&rsquos biggest brands&mdashand we have fun doing it. Now, we&rsquore calling all you rule-breakers and risk-takers who see the world differently and are bold enough to reinvent it. Come, transform with us.\nInviting applications for the role of Lead Consultant - Snowflake Data Engineer!\nThe candidate will work as a Snowflake Architect with team of Quantitative Analyst and Modelers, as well as various other business and technology units. The individual will be part of an Agile team working closely with team members in India.\nResponsibilities\n. Will be responsible to plan, design, develop and maintain the data architecture, data models and standards for various Data Integration & Data warehouse projects in snowflake.\n. Ensure new features and subject areas are modelled to integrate with existing structures and provide a consistent view.\n. Develop and maintain documentation of the data architecture, data flow and data models of the data warehouse appropriate for various audiences.\n. Provide direction on adoption of Cloud technologies (Snowflake) and industry best practices in the field of data warehouse architecture and modelling.\n. providing technical leadership to large enterprise scale projects.\n. Will also be responsible for preparing estimates and defining technical solutions to proposals (RFPs).\n. This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project.\nQualifications\nMinimum qualifications\n. BE/B Tech/ MCA\n. Excellent written and verbal communication skills\n. Must have experience at least two end to end implementation of Snowflake cloud data warehouse and 3 end to end data warehouse implementations on-premise .\n. Expertise in Snowflake - data modelling, ELT using Snowflake SQL, implementing complex stored Procedures and standard DWH and ETL concepts.\n. Expertise in Snowflake advanced concepts like setting up resource monitors, RBAC controls, virtual warehouse sizing, query performance tuning,\n. Zero copy clone, time travel and understand how to use these features.\n. Expertise in deploying Snowflake features such as data sharing, events and lake-house patterns.\n. Hands-on experience with Snowflake utilities, SnowSQL, SnowPipe, Big Data model techniques using Python.\n. Experience in Data Migration from RDBMS to Snowflake cloud data warehouse.\n\nPreferred qualifications\n. Experience with data security and data access controls and design.\n. Experience with AWS or Azure data storage and management technologies such as S3 and ADLS.\n. Must have experience in DBT, Fivetran, GCP, Github\n. Deep understanding of relational as well as NoSQL data stores, methods and approaches (star and snowflake, dimensional modelling)\n. Build processes supporting data transformation, data structures, metadata, dependency and workload management\n. Proficiency in RDBMS, complex SQL, PL/SQL.\n. Unix Shell Scripting, performance tuning and troubleshoot (additional benfit)\n. Provide resolution to an extensive range of complicated data pipeline related problems, proactively and as issues surface\n. Must have expertise in AWS or Azure Platform as a Service (PAAS).\n. Certified Snowflake cloud data warehouse Architect (Desirable).\n. Should be able to troubleshoot problems across infrastructure, platform and application domains.\n. Must have experience of Agile development methodologies\n. Strong written communication skills. Is effective and persuasive in both written and oral communication Review the architectural/ technological solutions for ongoing projects and ensure right choice of solution.\n\n\nGenpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. Genpact is committed to creating a dynamic work environment that values diversity and inclusion, respect and integrity, customer focus, and innovation. For more information, visit www.genpact.com. Follow us on Twitter, Facebook, LinkedIn, and YouTube.",[],2025-06-10 19:39:12
Lead Consultant - Snowflake Data Engineer,Genpact,Fresher,,Hyderabad,IT/Computers - Hardware & Networking,"With a startup spirit and 80,000+ curious and courageous minds, we have the expertise to go deep with the world&rsquos biggest brands&mdashand we have fun doing it. Now, we&rsquore calling all you rule-breakers and risk-takers who see the world differently and are bold enough to reinvent it. Come, transform with us.\nInviting applications for the role of Lead Consultant - Snowflake Data Engineer!\nThe candidate will work as a Snowflake Architect with team of Quantitative Analyst and Modelers, as well as various other business and technology units. The individual will be part of an Agile team working closely with team members in India.\nResponsibilities\n. Will be responsible to plan, design, develop and maintain the data architecture, data models and standards for various Data Integration & Data warehouse projects in snowflake.\n. Ensure new features and subject areas are modelled to integrate with existing structures and provide a consistent view.\n. Develop and maintain documentation of the data architecture, data flow and data models of the data warehouse appropriate for various audiences.\n. Provide direction on adoption of Cloud technologies (Snowflake) and industry best practices in the field of data warehouse architecture and modelling.\n. providing technical leadership to large enterprise scale projects.\n. Will also be responsible for preparing estimates and defining technical solutions to proposals (RFPs).\n. This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project.\nQualifications\nMinimum qualifications\n. BE/B Tech/ MCA\n. Excellent written and verbal communication skills\n. Must have experience at least two end to end implementation of Snowflake cloud data warehouse and 3 end to end data warehouse implementations on-premise .\n. Expertise in Snowflake - data modelling, ELT using Snowflake SQL, implementing complex stored Procedures and standard DWH and ETL concepts.\n. Expertise in Snowflake advanced concepts like setting up resource monitors, RBAC controls, virtual warehouse sizing, query performance tuning,\n. Zero copy clone, time travel and understand how to use these features.\n. Expertise in deploying Snowflake features such as data sharing, events and lake-house patterns.\n. Hands-on experience with Snowflake utilities, SnowSQL, SnowPipe, Big Data model techniques using Python.\n. Experience in Data Migration from RDBMS to Snowflake cloud data warehouse.\n\nPreferred qualifications\n. Experience with data security and data access controls and design.\n. Experience with AWS or Azure data storage and management technologies such as S3 and ADLS.\n. Must have experience in DBT, Fivetran, GCP, Github\n. Deep understanding of relational as well as NoSQL data stores, methods and approaches (star and snowflake, dimensional modelling)\n. Build processes supporting data transformation, data structures, metadata, dependency and workload management\n. Proficiency in RDBMS, complex SQL, PL/SQL.\n. Unix Shell Scripting, performance tuning and troubleshoot (additional benfit)\n. Provide resolution to an extensive range of complicated data pipeline related problems, proactively and as issues surface\n. Must have expertise in AWS or Azure Platform as a Service (PAAS).\n. Certified Snowflake cloud data warehouse Architect (Desirable).\n. Should be able to troubleshoot problems across infrastructure, platform and application domains.\n. Must have experience of Agile development methodologies\n. Strong written communication skills. Is effective and persuasive in both written and oral communication Review the architectural/ technological solutions for ongoing projects and ensure right choice of solution.\n\n\nGenpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. Genpact is committed to creating a dynamic work environment that values diversity and inclusion, respect and integrity, customer focus, and innovation. For more information, visit www.genpact.com. Follow us on Twitter, Facebook, LinkedIn, and YouTube.",[],2025-06-10 19:39:12
Lead Consultant - Snowflake Data Engineer,Genpact,Fresher,,Hyderabad,IT/Computers - Hardware & Networking,"With a startup spirit and 80,000+ curious and courageous minds, we have the expertise to go deep with the world&rsquos biggest brands&mdashand we have fun doing it. Now, we&rsquore calling all you rule-breakers and risk-takers who see the world differently and are bold enough to reinvent it. Come, transform with us.\nInviting applications for the role of Lead Consultant - Snowflake Data Engineer!\nThe candidate will work as a Snowflake Architect with team of Quantitative Analyst and Modelers, as well as various other business and technology units. The individual will be part of an Agile team working closely with team members in India.\nResponsibilities\n. Will be responsible to plan, design, develop and maintain the data architecture, data models and standards for various Data Integration & Data warehouse projects in snowflake.\n. Ensure new features and subject areas are modelled to integrate with existing structures and provide a consistent view.\n. Develop and maintain documentation of the data architecture, data flow and data models of the data warehouse appropriate for various audiences.\n. Provide direction on adoption of Cloud technologies (Snowflake) and industry best practices in the field of data warehouse architecture and modelling.\n. providing technical leadership to large enterprise scale projects.\n. Will also be responsible for preparing estimates and defining technical solutions to proposals (RFPs).\n. This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project.\nQualifications\nMinimum qualifications\n. BE/B Tech/ MCA\n. Excellent written and verbal communication skills\n. Must have experience at least two end to end implementation of Snowflake cloud data warehouse and 3 end to end data warehouse implementations on-premise .\n. Expertise in Snowflake - data modelling, ELT using Snowflake SQL, implementing complex stored Procedures and standard DWH and ETL concepts.\n. Expertise in Snowflake advanced concepts like setting up resource monitors, RBAC controls, virtual warehouse sizing, query performance tuning,\n. Zero copy clone, time travel and understand how to use these features.\n. Expertise in deploying Snowflake features such as data sharing, events and lake-house patterns.\n. Hands-on experience with Snowflake utilities, SnowSQL, SnowPipe, Big Data model techniques using Python.\n. Experience in Data Migration from RDBMS to Snowflake cloud data warehouse.\n\nPreferred qualifications\n. Experience with data security and data access controls and design.\n. Experience with AWS or Azure data storage and management technologies such as S3 and ADLS.\n. Must have experience in DBT, Fivetran, GCP, Github\n. Deep understanding of relational as well as NoSQL data stores, methods and approaches (star and snowflake, dimensional modelling)\n. Build processes supporting data transformation, data structures, metadata, dependency and workload management\n. Proficiency in RDBMS, complex SQL, PL/SQL.\n. Unix Shell Scripting, performance tuning and troubleshoot (additional benfit)\n. Provide resolution to an extensive range of complicated data pipeline related problems, proactively and as issues surface\n. Must have expertise in AWS or Azure Platform as a Service (PAAS).\n. Certified Snowflake cloud data warehouse Architect (Desirable).\n. Should be able to troubleshoot problems across infrastructure, platform and application domains.\n. Must have experience of Agile development methodologies\n. Strong written communication skills. Is effective and persuasive in both written and oral communication Review the architectural/ technological solutions for ongoing projects and ensure right choice of solution.\n\n\nGenpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. Genpact is committed to creating a dynamic work environment that values diversity and inclusion, respect and integrity, customer focus, and innovation. For more information, visit www.genpact.com. Follow us on Twitter, Facebook, LinkedIn, and YouTube.",[],2025-06-10 19:39:13
Lead Consultant - Snowflake Data Engineer,Genpact,Fresher,,Hyderabad,IT/Computers - Hardware & Networking,"With a startup spirit and 80,000+ curious and courageous minds, we have the expertise to go deep with the world&rsquos biggest brands&mdashand we have fun doing it. Now, we&rsquore calling all you rule-breakers and risk-takers who see the world differently and are bold enough to reinvent it. Come, transform with us.\nInviting applications for the role of Lead Consultant - Snowflake Data Engineer!\nThe candidate will work as a Snowflake Architect with team of Quantitative Analyst and Modelers, as well as various other business and technology units. The individual will be part of an Agile team working closely with team members in India.\nResponsibilities\n. Will be responsible to plan, design, develop and maintain the data architecture, data models and standards for various Data Integration & Data warehouse projects in snowflake.\n. Ensure new features and subject areas are modelled to integrate with existing structures and provide a consistent view.\n. Develop and maintain documentation of the data architecture, data flow and data models of the data warehouse appropriate for various audiences.\n. Provide direction on adoption of Cloud technologies (Snowflake) and industry best practices in the field of data warehouse architecture and modelling.\n. providing technical leadership to large enterprise scale projects.\n. Will also be responsible for preparing estimates and defining technical solutions to proposals (RFPs).\n. This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project.\nQualifications\nMinimum qualifications\n. BE/B Tech/ MCA\n. Excellent written and verbal communication skills\n. Must have experience at least two end to end implementation of Snowflake cloud data warehouse and 3 end to end data warehouse implementations on-premise .\n. Expertise in Snowflake - data modelling, ELT using Snowflake SQL, implementing complex stored Procedures and standard DWH and ETL concepts.\n. Expertise in Snowflake advanced concepts like setting up resource monitors, RBAC controls, virtual warehouse sizing, query performance tuning,\n. Zero copy clone, time travel and understand how to use these features.\n. Expertise in deploying Snowflake features such as data sharing, events and lake-house patterns.\n. Hands-on experience with Snowflake utilities, SnowSQL, SnowPipe, Big Data model techniques using Python.\n. Experience in Data Migration from RDBMS to Snowflake cloud data warehouse.\n\nPreferred qualifications\n. Experience with data security and data access controls and design.\n. Experience with AWS or Azure data storage and management technologies such as S3 and ADLS.\n. Must have experience in DBT, Fivetran, GCP, Github\n. Deep understanding of relational as well as NoSQL data stores, methods and approaches (star and snowflake, dimensional modelling)\n. Build processes supporting data transformation, data structures, metadata, dependency and workload management\n. Proficiency in RDBMS, complex SQL, PL/SQL.\n. Unix Shell Scripting, performance tuning and troubleshoot (additional benfit)\n. Provide resolution to an extensive range of complicated data pipeline related problems, proactively and as issues surface\n. Must have expertise in AWS or Azure Platform as a Service (PAAS).\n. Certified Snowflake cloud data warehouse Architect (Desirable).\n. Should be able to troubleshoot problems across infrastructure, platform and application domains.\n. Must have experience of Agile development methodologies\n. Strong written communication skills. Is effective and persuasive in both written and oral communication Review the architectural/ technological solutions for ongoing projects and ensure right choice of solution.\n\n\nGenpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. Genpact is committed to creating a dynamic work environment that values diversity and inclusion, respect and integrity, customer focus, and innovation. For more information, visit www.genpact.com. Follow us on Twitter, Facebook, LinkedIn, and YouTube.",[],2025-06-10 19:39:14
Lead Consultant - Snowflake Data Engineer,Genpact,Fresher,,Hyderabad,IT/Computers - Hardware & Networking,"With a startup spirit and 80,000+ curious and courageous minds, we have the expertise to go deep with the world&rsquos biggest brands&mdashand we have fun doing it. Now, we&rsquore calling all you rule-breakers and risk-takers who see the world differently and are bold enough to reinvent it. Come, transform with us.\nInviting applications for the role of Lead Consultant - Snowflake Data Engineer!\nThe candidate will work as a Snowflake Architect with team of Quantitative Analyst and Modelers, as well as various other business and technology units. The individual will be part of an Agile team working closely with team members in India.\nResponsibilities\n. Will be responsible to plan, design, develop and maintain the data architecture, data models and standards for various Data Integration & Data warehouse projects in snowflake.\n. Ensure new features and subject areas are modelled to integrate with existing structures and provide a consistent view.\n. Develop and maintain documentation of the data architecture, data flow and data models of the data warehouse appropriate for various audiences.\n. Provide direction on adoption of Cloud technologies (Snowflake) and industry best practices in the field of data warehouse architecture and modelling.\n. providing technical leadership to large enterprise scale projects.\n. Will also be responsible for preparing estimates and defining technical solutions to proposals (RFPs).\n. This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project.\nQualifications\nMinimum qualifications\n. BE/B Tech/ MCA\n. Excellent written and verbal communication skills\n. Must have experience at least two end to end implementation of Snowflake cloud data warehouse and 3 end to end data warehouse implementations on-premise .\n. Expertise in Snowflake - data modelling, ELT using Snowflake SQL, implementing complex stored Procedures and standard DWH and ETL concepts.\n. Expertise in Snowflake advanced concepts like setting up resource monitors, RBAC controls, virtual warehouse sizing, query performance tuning,\n. Zero copy clone, time travel and understand how to use these features.\n. Expertise in deploying Snowflake features such as data sharing, events and lake-house patterns.\n. Hands-on experience with Snowflake utilities, SnowSQL, SnowPipe, Big Data model techniques using Python.\n. Experience in Data Migration from RDBMS to Snowflake cloud data warehouse.\n\nPreferred qualifications\n. Experience with data security and data access controls and design.\n. Experience with AWS or Azure data storage and management technologies such as S3 and ADLS.\n. Must have experience in DBT, Fivetran, GCP, Github\n. Deep understanding of relational as well as NoSQL data stores, methods and approaches (star and snowflake, dimensional modelling)\n. Build processes supporting data transformation, data structures, metadata, dependency and workload management\n. Proficiency in RDBMS, complex SQL, PL/SQL.\n. Unix Shell Scripting, performance tuning and troubleshoot (additional benfit)\n. Provide resolution to an extensive range of complicated data pipeline related problems, proactively and as issues surface\n. Must have expertise in AWS or Azure Platform as a Service (PAAS).\n. Certified Snowflake cloud data warehouse Architect (Desirable).\n. Should be able to troubleshoot problems across infrastructure, platform and application domains.\n. Must have experience of Agile development methodologies\n. Strong written communication skills. Is effective and persuasive in both written and oral communication Review the architectural/ technological solutions for ongoing projects and ensure right choice of solution.\n\n\nGenpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. Genpact is committed to creating a dynamic work environment that values diversity and inclusion, respect and integrity, customer focus, and innovation. For more information, visit www.genpact.com. Follow us on Twitter, Facebook, LinkedIn, and YouTube.",[],2025-06-10 19:39:15
Lead Consultant - Snowflake Data Engineer,Genpact,Fresher,,Hyderabad,IT/Computers - Hardware & Networking,"With a startup spirit and 80,000+ curious and courageous minds, we have the expertise to go deep with the world&rsquos biggest brands&mdashand we have fun doing it. Now, we&rsquore calling all you rule-breakers and risk-takers who see the world differently and are bold enough to reinvent it. Come, transform with us.\nInviting applications for the role of Lead Consultant - Snowflake Data Engineer!\nThe candidate will work as a Snowflake Architect with team of Quantitative Analyst and Modelers, as well as various other business and technology units. The individual will be part of an Agile team working closely with team members in India.\nResponsibilities\n. Will be responsible to plan, design, develop and maintain the data architecture, data models and standards for various Data Integration & Data warehouse projects in snowflake.\n. Ensure new features and subject areas are modelled to integrate with existing structures and provide a consistent view.\n. Develop and maintain documentation of the data architecture, data flow and data models of the data warehouse appropriate for various audiences.\n. Provide direction on adoption of Cloud technologies (Snowflake) and industry best practices in the field of data warehouse architecture and modelling.\n. providing technical leadership to large enterprise scale projects.\n. Will also be responsible for preparing estimates and defining technical solutions to proposals (RFPs).\n. This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project.\nQualifications\nMinimum qualifications\n. BE/B Tech/ MCA\n. Excellent written and verbal communication skills\n. Must have experience at least two end to end implementation of Snowflake cloud data warehouse and 3 end to end data warehouse implementations on-premise .\n. Expertise in Snowflake - data modelling, ELT using Snowflake SQL, implementing complex stored Procedures and standard DWH and ETL concepts.\n. Expertise in Snowflake advanced concepts like setting up resource monitors, RBAC controls, virtual warehouse sizing, query performance tuning,\n. Zero copy clone, time travel and understand how to use these features.\n. Expertise in deploying Snowflake features such as data sharing, events and lake-house patterns.\n. Hands-on experience with Snowflake utilities, SnowSQL, SnowPipe, Big Data model techniques using Python.\n. Experience in Data Migration from RDBMS to Snowflake cloud data warehouse.\n\nPreferred qualifications\n. Experience with data security and data access controls and design.\n. Experience with AWS or Azure data storage and management technologies such as S3 and ADLS.\n. Must have experience in DBT, Fivetran, GCP, Github\n. Deep understanding of relational as well as NoSQL data stores, methods and approaches (star and snowflake, dimensional modelling)\n. Build processes supporting data transformation, data structures, metadata, dependency and workload management\n. Proficiency in RDBMS, complex SQL, PL/SQL.\n. Unix Shell Scripting, performance tuning and troubleshoot (additional benfit)\n. Provide resolution to an extensive range of complicated data pipeline related problems, proactively and as issues surface\n. Must have expertise in AWS or Azure Platform as a Service (PAAS).\n. Certified Snowflake cloud data warehouse Architect (Desirable).\n. Should be able to troubleshoot problems across infrastructure, platform and application domains.\n. Must have experience of Agile development methodologies\n. Strong written communication skills. Is effective and persuasive in both written and oral communication Review the architectural/ technological solutions for ongoing projects and ensure right choice of solution.\n\n\nGenpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. Genpact is committed to creating a dynamic work environment that values diversity and inclusion, respect and integrity, customer focus, and innovation. For more information, visit www.genpact.com. Follow us on Twitter, Facebook, LinkedIn, and YouTube.",[],2025-06-10 19:39:15
Lead Consultant - Snowflake Data Engineer,Genpact,Fresher,,Hyderabad,IT/Computers - Hardware & Networking,"With a startup spirit and 80,000+ curious and courageous minds, we have the expertise to go deep with the world&rsquos biggest brands&mdashand we have fun doing it. Now, we&rsquore calling all you rule-breakers and risk-takers who see the world differently and are bold enough to reinvent it. Come, transform with us.\nInviting applications for the role of Lead Consultant - Snowflake Data Engineer!\nThe candidate will work as a Snowflake Architect with team of Quantitative Analyst and Modelers, as well as various other business and technology units. The individual will be part of an Agile team working closely with team members in India.\nResponsibilities\n. Will be responsible to plan, design, develop and maintain the data architecture, data models and standards for various Data Integration & Data warehouse projects in snowflake.\n. Ensure new features and subject areas are modelled to integrate with existing structures and provide a consistent view.\n. Develop and maintain documentation of the data architecture, data flow and data models of the data warehouse appropriate for various audiences.\n. Provide direction on adoption of Cloud technologies (Snowflake) and industry best practices in the field of data warehouse architecture and modelling.\n. providing technical leadership to large enterprise scale projects.\n. Will also be responsible for preparing estimates and defining technical solutions to proposals (RFPs).\n. This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project.\nQualifications\nMinimum qualifications\n. BE/B Tech/ MCA\n. Excellent written and verbal communication skills\n. Must have experience at least two end to end implementation of Snowflake cloud data warehouse and 3 end to end data warehouse implementations on-premise .\n. Expertise in Snowflake - data modelling, ELT using Snowflake SQL, implementing complex stored Procedures and standard DWH and ETL concepts.\n. Expertise in Snowflake advanced concepts like setting up resource monitors, RBAC controls, virtual warehouse sizing, query performance tuning,\n. Zero copy clone, time travel and understand how to use these features.\n. Expertise in deploying Snowflake features such as data sharing, events and lake-house patterns.\n. Hands-on experience with Snowflake utilities, SnowSQL, SnowPipe, Big Data model techniques using Python.\n. Experience in Data Migration from RDBMS to Snowflake cloud data warehouse.\n\nPreferred qualifications\n. Experience with data security and data access controls and design.\n. Experience with AWS or Azure data storage and management technologies such as S3 and ADLS.\n. Must have experience in DBT, Fivetran, GCP, Github\n. Deep understanding of relational as well as NoSQL data stores, methods and approaches (star and snowflake, dimensional modelling)\n. Build processes supporting data transformation, data structures, metadata, dependency and workload management\n. Proficiency in RDBMS, complex SQL, PL/SQL.\n. Unix Shell Scripting, performance tuning and troubleshoot (additional benfit)\n. Provide resolution to an extensive range of complicated data pipeline related problems, proactively and as issues surface\n. Must have expertise in AWS or Azure Platform as a Service (PAAS).\n. Certified Snowflake cloud data warehouse Architect (Desirable).\n. Should be able to troubleshoot problems across infrastructure, platform and application domains.\n. Must have experience of Agile development methodologies\n. Strong written communication skills. Is effective and persuasive in both written and oral communication Review the architectural/ technological solutions for ongoing projects and ensure right choice of solution.\n\n\nGenpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. Genpact is committed to creating a dynamic work environment that values diversity and inclusion, respect and integrity, customer focus, and innovation. For more information, visit www.genpact.com. Follow us on Twitter, Facebook, LinkedIn, and YouTube.",[],2025-06-10 19:39:16
Lead Consultant - Snowflake Data Engineer,Genpact,Fresher,,Hyderabad,IT/Computers - Hardware & Networking,"With a startup spirit and 80,000+ curious and courageous minds, we have the expertise to go deep with the world&rsquos biggest brands&mdashand we have fun doing it. Now, we&rsquore calling all you rule-breakers and risk-takers who see the world differently and are bold enough to reinvent it. Come, transform with us.\nInviting applications for the role of Lead Consultant - Snowflake Data Engineer!\nThe candidate will work as a Snowflake Architect with team of Quantitative Analyst and Modelers, as well as various other business and technology units. The individual will be part of an Agile team working closely with team members in India.\nResponsibilities\n. Will be responsible to plan, design, develop and maintain the data architecture, data models and standards for various Data Integration & Data warehouse projects in snowflake.\n. Ensure new features and subject areas are modelled to integrate with existing structures and provide a consistent view.\n. Develop and maintain documentation of the data architecture, data flow and data models of the data warehouse appropriate for various audiences.\n. Provide direction on adoption of Cloud technologies (Snowflake) and industry best practices in the field of data warehouse architecture and modelling.\n. providing technical leadership to large enterprise scale projects.\n. Will also be responsible for preparing estimates and defining technical solutions to proposals (RFPs).\n. This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project.\nQualifications\nMinimum qualifications\n. BE/B Tech/ MCA\n. Excellent written and verbal communication skills\n. Must have experience at least two end to end implementation of Snowflake cloud data warehouse and 3 end to end data warehouse implementations on-premise .\n. Expertise in Snowflake - data modelling, ELT using Snowflake SQL, implementing complex stored Procedures and standard DWH and ETL concepts.\n. Expertise in Snowflake advanced concepts like setting up resource monitors, RBAC controls, virtual warehouse sizing, query performance tuning,\n. Zero copy clone, time travel and understand how to use these features.\n. Expertise in deploying Snowflake features such as data sharing, events and lake-house patterns.\n. Hands-on experience with Snowflake utilities, SnowSQL, SnowPipe, Big Data model techniques using Python.\n. Experience in Data Migration from RDBMS to Snowflake cloud data warehouse.\n\nPreferred qualifications\n. Experience with data security and data access controls and design.\n. Experience with AWS or Azure data storage and management technologies such as S3 and ADLS.\n. Must have experience in DBT, Fivetran, GCP, Github\n. Deep understanding of relational as well as NoSQL data stores, methods and approaches (star and snowflake, dimensional modelling)\n. Build processes supporting data transformation, data structures, metadata, dependency and workload management\n. Proficiency in RDBMS, complex SQL, PL/SQL.\n. Unix Shell Scripting, performance tuning and troubleshoot (additional benfit)\n. Provide resolution to an extensive range of complicated data pipeline related problems, proactively and as issues surface\n. Must have expertise in AWS or Azure Platform as a Service (PAAS).\n. Certified Snowflake cloud data warehouse Architect (Desirable).\n. Should be able to troubleshoot problems across infrastructure, platform and application domains.\n. Must have experience of Agile development methodologies\n. Strong written communication skills. Is effective and persuasive in both written and oral communication Review the architectural/ technological solutions for ongoing projects and ensure right choice of solution.\n\n\nGenpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. Genpact is committed to creating a dynamic work environment that values diversity and inclusion, respect and integrity, customer focus, and innovation. For more information, visit www.genpact.com. Follow us on Twitter, Facebook, LinkedIn, and YouTube.",[],2025-06-10 19:39:17
Lead Consultant - Snowflake Data Engineer,Genpact,Fresher,,Hyderabad,IT/Computers - Hardware & Networking,"With a startup spirit and 80,000+ curious and courageous minds, we have the expertise to go deep with the world&rsquos biggest brands&mdashand we have fun doing it. Now, we&rsquore calling all you rule-breakers and risk-takers who see the world differently and are bold enough to reinvent it. Come, transform with us.\nInviting applications for the role of Lead Consultant - Snowflake Data Engineer!\nThe candidate will work as a Snowflake Architect with team of Quantitative Analyst and Modelers, as well as various other business and technology units. The individual will be part of an Agile team working closely with team members in India.\nResponsibilities\n. Will be responsible to plan, design, develop and maintain the data architecture, data models and standards for various Data Integration & Data warehouse projects in snowflake.\n. Ensure new features and subject areas are modelled to integrate with existing structures and provide a consistent view.\n. Develop and maintain documentation of the data architecture, data flow and data models of the data warehouse appropriate for various audiences.\n. Provide direction on adoption of Cloud technologies (Snowflake) and industry best practices in the field of data warehouse architecture and modelling.\n. providing technical leadership to large enterprise scale projects.\n. Will also be responsible for preparing estimates and defining technical solutions to proposals (RFPs).\n. This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project.\nQualifications\nMinimum qualifications\n. BE/B Tech/ MCA\n. Excellent written and verbal communication skills\n. Must have experience at least two end to end implementation of Snowflake cloud data warehouse and 3 end to end data warehouse implementations on-premise .\n. Expertise in Snowflake - data modelling, ELT using Snowflake SQL, implementing complex stored Procedures and standard DWH and ETL concepts.\n. Expertise in Snowflake advanced concepts like setting up resource monitors, RBAC controls, virtual warehouse sizing, query performance tuning,\n. Zero copy clone, time travel and understand how to use these features.\n. Expertise in deploying Snowflake features such as data sharing, events and lake-house patterns.\n. Hands-on experience with Snowflake utilities, SnowSQL, SnowPipe, Big Data model techniques using Python.\n. Experience in Data Migration from RDBMS to Snowflake cloud data warehouse.\n\nPreferred qualifications\n. Experience with data security and data access controls and design.\n. Experience with AWS or Azure data storage and management technologies such as S3 and ADLS.\n. Must have experience in DBT, Fivetran, GCP, Github\n. Deep understanding of relational as well as NoSQL data stores, methods and approaches (star and snowflake, dimensional modelling)\n. Build processes supporting data transformation, data structures, metadata, dependency and workload management\n. Proficiency in RDBMS, complex SQL, PL/SQL.\n. Unix Shell Scripting, performance tuning and troubleshoot (additional benfit)\n. Provide resolution to an extensive range of complicated data pipeline related problems, proactively and as issues surface\n. Must have expertise in AWS or Azure Platform as a Service (PAAS).\n. Certified Snowflake cloud data warehouse Architect (Desirable).\n. Should be able to troubleshoot problems across infrastructure, platform and application domains.\n. Must have experience of Agile development methodologies\n. Strong written communication skills. Is effective and persuasive in both written and oral communication Review the architectural/ technological solutions for ongoing projects and ensure right choice of solution.\n\n\nGenpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. Genpact is committed to creating a dynamic work environment that values diversity and inclusion, respect and integrity, customer focus, and innovation. For more information, visit www.genpact.com. Follow us on Twitter, Facebook, LinkedIn, and YouTube.",[],2025-06-10 19:39:18
Lead Consultant - Snowflake Data Engineer,Genpact,Fresher,,Hyderabad,IT/Computers - Hardware & Networking,"With a startup spirit and 80,000+ curious and courageous minds, we have the expertise to go deep with the world&rsquos biggest brands&mdashand we have fun doing it. Now, we&rsquore calling all you rule-breakers and risk-takers who see the world differently and are bold enough to reinvent it. Come, transform with us.\nInviting applications for the role of Lead Consultant - Snowflake Data Engineer!\nThe candidate will work as a Snowflake Architect with team of Quantitative Analyst and Modelers, as well as various other business and technology units. The individual will be part of an Agile team working closely with team members in India.\nResponsibilities\n. Will be responsible to plan, design, develop and maintain the data architecture, data models and standards for various Data Integration & Data warehouse projects in snowflake.\n. Ensure new features and subject areas are modelled to integrate with existing structures and provide a consistent view.\n. Develop and maintain documentation of the data architecture, data flow and data models of the data warehouse appropriate for various audiences.\n. Provide direction on adoption of Cloud technologies (Snowflake) and industry best practices in the field of data warehouse architecture and modelling.\n. providing technical leadership to large enterprise scale projects.\n. Will also be responsible for preparing estimates and defining technical solutions to proposals (RFPs).\n. This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project.\nQualifications\nMinimum qualifications\n. BE/B Tech/ MCA\n. Excellent written and verbal communication skills\n. Must have experience at least two end to end implementation of Snowflake cloud data warehouse and 3 end to end data warehouse implementations on-premise .\n. Expertise in Snowflake - data modelling, ELT using Snowflake SQL, implementing complex stored Procedures and standard DWH and ETL concepts.\n. Expertise in Snowflake advanced concepts like setting up resource monitors, RBAC controls, virtual warehouse sizing, query performance tuning,\n. Zero copy clone, time travel and understand how to use these features.\n. Expertise in deploying Snowflake features such as data sharing, events and lake-house patterns.\n. Hands-on experience with Snowflake utilities, SnowSQL, SnowPipe, Big Data model techniques using Python.\n. Experience in Data Migration from RDBMS to Snowflake cloud data warehouse.\n\nPreferred qualifications\n. Experience with data security and data access controls and design.\n. Experience with AWS or Azure data storage and management technologies such as S3 and ADLS.\n. Must have experience in DBT, Fivetran, GCP, Github\n. Deep understanding of relational as well as NoSQL data stores, methods and approaches (star and snowflake, dimensional modelling)\n. Build processes supporting data transformation, data structures, metadata, dependency and workload management\n. Proficiency in RDBMS, complex SQL, PL/SQL.\n. Unix Shell Scripting, performance tuning and troubleshoot (additional benfit)\n. Provide resolution to an extensive range of complicated data pipeline related problems, proactively and as issues surface\n. Must have expertise in AWS or Azure Platform as a Service (PAAS).\n. Certified Snowflake cloud data warehouse Architect (Desirable).\n. Should be able to troubleshoot problems across infrastructure, platform and application domains.\n. Must have experience of Agile development methodologies\n. Strong written communication skills. Is effective and persuasive in both written and oral communication Review the architectural/ technological solutions for ongoing projects and ensure right choice of solution.\n\n\nGenpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. Genpact is committed to creating a dynamic work environment that values diversity and inclusion, respect and integrity, customer focus, and innovation. For more information, visit www.genpact.com. Follow us on Twitter, Facebook, LinkedIn, and YouTube.",[],2025-06-10 19:39:19
Lead Consultant - Snowflake Data Engineer,Genpact,Fresher,,Hyderabad,IT/Computers - Hardware & Networking,"With a startup spirit and 80,000+ curious and courageous minds, we have the expertise to go deep with the world&rsquos biggest brands&mdashand we have fun doing it. Now, we&rsquore calling all you rule-breakers and risk-takers who see the world differently and are bold enough to reinvent it. Come, transform with us.\nInviting applications for the role of Lead Consultant - Snowflake Data Engineer!\nThe candidate will work as a Snowflake Architect with team of Quantitative Analyst and Modelers, as well as various other business and technology units. The individual will be part of an Agile team working closely with team members in India.\nResponsibilities\n. Will be responsible to plan, design, develop and maintain the data architecture, data models and standards for various Data Integration & Data warehouse projects in snowflake.\n. Ensure new features and subject areas are modelled to integrate with existing structures and provide a consistent view.\n. Develop and maintain documentation of the data architecture, data flow and data models of the data warehouse appropriate for various audiences.\n. Provide direction on adoption of Cloud technologies (Snowflake) and industry best practices in the field of data warehouse architecture and modelling.\n. providing technical leadership to large enterprise scale projects.\n. Will also be responsible for preparing estimates and defining technical solutions to proposals (RFPs).\n. This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project.\nQualifications\nMinimum qualifications\n. BE/B Tech/ MCA\n. Excellent written and verbal communication skills\n. Must have experience at least two end to end implementation of Snowflake cloud data warehouse and 3 end to end data warehouse implementations on-premise .\n. Expertise in Snowflake - data modelling, ELT using Snowflake SQL, implementing complex stored Procedures and standard DWH and ETL concepts.\n. Expertise in Snowflake advanced concepts like setting up resource monitors, RBAC controls, virtual warehouse sizing, query performance tuning,\n. Zero copy clone, time travel and understand how to use these features.\n. Expertise in deploying Snowflake features such as data sharing, events and lake-house patterns.\n. Hands-on experience with Snowflake utilities, SnowSQL, SnowPipe, Big Data model techniques using Python.\n. Experience in Data Migration from RDBMS to Snowflake cloud data warehouse.\n\nPreferred qualifications\n. Experience with data security and data access controls and design.\n. Experience with AWS or Azure data storage and management technologies such as S3 and ADLS.\n. Must have experience in DBT, Fivetran, GCP, Github\n. Deep understanding of relational as well as NoSQL data stores, methods and approaches (star and snowflake, dimensional modelling)\n. Build processes supporting data transformation, data structures, metadata, dependency and workload management\n. Proficiency in RDBMS, complex SQL, PL/SQL.\n. Unix Shell Scripting, performance tuning and troubleshoot (additional benfit)\n. Provide resolution to an extensive range of complicated data pipeline related problems, proactively and as issues surface\n. Must have expertise in AWS or Azure Platform as a Service (PAAS).\n. Certified Snowflake cloud data warehouse Architect (Desirable).\n. Should be able to troubleshoot problems across infrastructure, platform and application domains.\n. Must have experience of Agile development methodologies\n. Strong written communication skills. Is effective and persuasive in both written and oral communication Review the architectural/ technological solutions for ongoing projects and ensure right choice of solution.\n\n\nGenpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. Genpact is committed to creating a dynamic work environment that values diversity and inclusion, respect and integrity, customer focus, and innovation. For more information, visit www.genpact.com. Follow us on Twitter, Facebook, LinkedIn, and YouTube.",[],2025-06-10 19:39:20
Data Engineer (Streamlit Expertise),Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"Develop scalable data collection, storage, and distribution platform to house data from vendors, research providers, exchanges, PBs, and web-scraping. Make data available to systematic & fundamental PMs, and enterprise functions: Ops, Risk, Trading, and Compliance. Develop internal data products and analytics\nResponsibilities\nWeb scraping using scripts/APIs/Tools\nHelp build and maintain greenfield data platform running on Snowflake and AWS\nUnderstand the existing pipelines and enhance pipelines for the new requirements.\nOnboarding new data providers\nData migration projects\nSkills\nSQL\nsenior level\nPython\nsenior level\nStreamlit Expertise\nLinux\nContainerization (Docker, Kubernetes)\nGood communication skills\nAWS\nDev ops skills (K8s, Docker, Jenkins)\nNice to have\nMarket Data Projects/ Capital markets exp\nSnowflake is a big plus\nAirflow","['Streamlit', 'Jenkins', 'Linux', 'Docker', 'Sql', 'Python', 'Kubernetes', 'Aws']",2025-06-10 19:39:26
Data Engineer (Streamlit Expertise),Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"Develop scalable data collection, storage, and distribution platform to house data from vendors, research providers, exchanges, PBs, and web-scraping. Make data available to systematic & fundamental PMs, and enterprise functions: Ops, Risk, Trading, and Compliance. Develop internal data products and analytics\nResponsibilities\nWeb scraping using scripts/APIs/Tools\nHelp build and maintain greenfield data platform running on Snowflake and AWS\nUnderstand the existing pipelines and enhance pipelines for the new requirements.\nOnboarding new data providers\nData migration projects\nSkills\nSQL\nsenior level\nPython\nsenior level\nStreamlit Expertise\nLinux\nContainerization (Docker, Kubernetes)\nGood communication skills\nAWS\nDev ops skills (K8s, Docker, Jenkins)\nNice to have\nMarket Data Projects/ Capital markets exp\nSnowflake is a big plus\nAirflow","['Streamlit', 'Jenkins', 'Linux', 'Docker', 'Sql', 'Python', 'Kubernetes', 'Aws']",2025-06-10 19:39:26
Data Engineer (Streamlit Expertise),Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"Develop scalable data collection, storage, and distribution platform to house data from vendors, research providers, exchanges, PBs, and web-scraping. Make data available to systematic & fundamental PMs, and enterprise functions: Ops, Risk, Trading, and Compliance. Develop internal data products and analytics\nResponsibilities\nWeb scraping using scripts/APIs/Tools\nHelp build and maintain greenfield data platform running on Snowflake and AWS\nUnderstand the existing pipelines and enhance pipelines for the new requirements.\nOnboarding new data providers\nData migration projects\nSkills\nSQL\nsenior level\nPython\nsenior level\nStreamlit Expertise\nLinux\nContainerization (Docker, Kubernetes)\nGood communication skills\nAWS\nDev ops skills (K8s, Docker, Jenkins)\nNice to have\nMarket Data Projects/ Capital markets exp\nSnowflake is a big plus\nAirflow","['Streamlit', 'Jenkins', 'Linux', 'Docker', 'Sql', 'Python', 'Kubernetes', 'Aws']",2025-06-10 19:39:27
Data Engineer (Streamlit Expertise),Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"Develop scalable data collection, storage, and distribution platform to house data from vendors, research providers, exchanges, PBs, and web-scraping. Make data available to systematic & fundamental PMs, and enterprise functions: Ops, Risk, Trading, and Compliance. Develop internal data products and analytics\nResponsibilities\nWeb scraping using scripts/APIs/Tools\nHelp build and maintain greenfield data platform running on Snowflake and AWS\nUnderstand the existing pipelines and enhance pipelines for the new requirements.\nOnboarding new data providers\nData migration projects\nSkills\nSQL\nsenior level\nPython\nsenior level\nStreamlit Expertise\nLinux\nContainerization (Docker, Kubernetes)\nGood communication skills\nAWS\nDev ops skills (K8s, Docker, Jenkins)\nNice to have\nMarket Data Projects/ Capital markets exp\nSnowflake is a big plus\nAirflow","['Streamlit', 'Jenkins', 'Linux', 'Docker', 'Sql', 'Python', 'Kubernetes', 'Aws']",2025-06-10 19:39:28
Data Engineer (Streamlit Expertise),Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"Develop scalable data collection, storage, and distribution platform to house data from vendors, research providers, exchanges, PBs, and web-scraping. Make data available to systematic & fundamental PMs, and enterprise functions: Ops, Risk, Trading, and Compliance. Develop internal data products and analytics\nResponsibilities\nWeb scraping using scripts/APIs/Tools\nHelp build and maintain greenfield data platform running on Snowflake and AWS\nUnderstand the existing pipelines and enhance pipelines for the new requirements.\nOnboarding new data providers\nData migration projects\nSkills\nSQL\nsenior level\nPython\nsenior level\nStreamlit Expertise\nLinux\nContainerization (Docker, Kubernetes)\nGood communication skills\nAWS\nDev ops skills (K8s, Docker, Jenkins)\nNice to have\nMarket Data Projects/ Capital markets exp\nSnowflake is a big plus\nAirflow","['Streamlit', 'Jenkins', 'Linux', 'Docker', 'Sql', 'Python', 'Kubernetes', 'Aws']",2025-06-10 19:39:29
Data Engineer (Streamlit Expertise),Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"Develop scalable data collection, storage, and distribution platform to house data from vendors, research providers, exchanges, PBs, and web-scraping. Make data available to systematic & fundamental PMs, and enterprise functions: Ops, Risk, Trading, and Compliance. Develop internal data products and analytics\nResponsibilities\nWeb scraping using scripts/APIs/Tools\nHelp build and maintain greenfield data platform running on Snowflake and AWS\nUnderstand the existing pipelines and enhance pipelines for the new requirements.\nOnboarding new data providers\nData migration projects\nSkills\nSQL\nsenior level\nPython\nsenior level\nStreamlit Expertise\nLinux\nContainerization (Docker, Kubernetes)\nGood communication skills\nAWS\nDev ops skills (K8s, Docker, Jenkins)\nNice to have\nMarket Data Projects/ Capital markets exp\nSnowflake is a big plus\nAirflow","['Streamlit', 'Jenkins', 'Linux', 'Docker', 'Sql', 'Python', 'Kubernetes', 'Aws']",2025-06-10 19:39:30
Data Engineer (Streamlit Expertise),Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"Develop scalable data collection, storage, and distribution platform to house data from vendors, research providers, exchanges, PBs, and web-scraping. Make data available to systematic & fundamental PMs, and enterprise functions: Ops, Risk, Trading, and Compliance. Develop internal data products and analytics\nResponsibilities\nWeb scraping using scripts/APIs/Tools\nHelp build and maintain greenfield data platform running on Snowflake and AWS\nUnderstand the existing pipelines and enhance pipelines for the new requirements.\nOnboarding new data providers\nData migration projects\nSkills\nSQL\nsenior level\nPython\nsenior level\nStreamlit Expertise\nLinux\nContainerization (Docker, Kubernetes)\nGood communication skills\nAWS\nDev ops skills (K8s, Docker, Jenkins)\nNice to have\nMarket Data Projects/ Capital markets exp\nSnowflake is a big plus\nAirflow","['Streamlit', 'Jenkins', 'Linux', 'Docker', 'Sql', 'Python', 'Kubernetes', 'Aws']",2025-06-10 19:39:31
Data Engineer (Streamlit Expertise),Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"Develop scalable data collection, storage, and distribution platform to house data from vendors, research providers, exchanges, PBs, and web-scraping. Make data available to systematic & fundamental PMs, and enterprise functions: Ops, Risk, Trading, and Compliance. Develop internal data products and analytics\nResponsibilities\nWeb scraping using scripts/APIs/Tools\nHelp build and maintain greenfield data platform running on Snowflake and AWS\nUnderstand the existing pipelines and enhance pipelines for the new requirements.\nOnboarding new data providers\nData migration projects\nSkills\nSQL\nsenior level\nPython\nsenior level\nStreamlit Expertise\nLinux\nContainerization (Docker, Kubernetes)\nGood communication skills\nAWS\nDev ops skills (K8s, Docker, Jenkins)\nNice to have\nMarket Data Projects/ Capital markets exp\nSnowflake is a big plus\nAirflow","['Streamlit', 'Jenkins', 'Linux', 'Docker', 'Sql', 'Python', 'Kubernetes', 'Aws']",2025-06-10 19:39:32
Data Engineer (Streamlit Expertise),Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"Develop scalable data collection, storage, and distribution platform to house data from vendors, research providers, exchanges, PBs, and web-scraping. Make data available to systematic & fundamental PMs, and enterprise functions: Ops, Risk, Trading, and Compliance. Develop internal data products and analytics\nResponsibilities\nWeb scraping using scripts/APIs/Tools\nHelp build and maintain greenfield data platform running on Snowflake and AWS\nUnderstand the existing pipelines and enhance pipelines for the new requirements.\nOnboarding new data providers\nData migration projects\nSkills\nSQL\nsenior level\nPython\nsenior level\nStreamlit Expertise\nLinux\nContainerization (Docker, Kubernetes)\nGood communication skills\nAWS\nDev ops skills (K8s, Docker, Jenkins)\nNice to have\nMarket Data Projects/ Capital markets exp\nSnowflake is a big plus\nAirflow","['Streamlit', 'Jenkins', 'Linux', 'Docker', 'Sql', 'Python', 'Kubernetes', 'Aws']",2025-06-10 19:39:33
Data Engineer (Streamlit Expertise),Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"Develop scalable data collection, storage, and distribution platform to house data from vendors, research providers, exchanges, PBs, and web-scraping. Make data available to systematic & fundamental PMs, and enterprise functions: Ops, Risk, Trading, and Compliance. Develop internal data products and analytics\nResponsibilities\nWeb scraping using scripts/APIs/Tools\nHelp build and maintain greenfield data platform running on Snowflake and AWS\nUnderstand the existing pipelines and enhance pipelines for the new requirements.\nOnboarding new data providers\nData migration projects\nSkills\nSQL\nsenior level\nPython\nsenior level\nStreamlit Expertise\nLinux\nContainerization (Docker, Kubernetes)\nGood communication skills\nAWS\nDev ops skills (K8s, Docker, Jenkins)\nNice to have\nMarket Data Projects/ Capital markets exp\nSnowflake is a big plus\nAirflow","['Streamlit', 'Jenkins', 'Linux', 'Docker', 'Sql', 'Python', 'Kubernetes', 'Aws']",2025-06-10 19:39:34
Data Engineer (Streamlit Expertise),Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"Develop scalable data collection, storage, and distribution platform to house data from vendors, research providers, exchanges, PBs, and web-scraping. Make data available to systematic & fundamental PMs, and enterprise functions: Ops, Risk, Trading, and Compliance. Develop internal data products and analytics\nResponsibilities\nWeb scraping using scripts/APIs/Tools\nHelp build and maintain greenfield data platform running on Snowflake and AWS\nUnderstand the existing pipelines and enhance pipelines for the new requirements.\nOnboarding new data providers\nData migration projects\nSkills\nSQL\nsenior level\nPython\nsenior level\nStreamlit Expertise\nLinux\nContainerization (Docker, Kubernetes)\nGood communication skills\nAWS\nDev ops skills (K8s, Docker, Jenkins)\nNice to have\nMarket Data Projects/ Capital markets exp\nSnowflake is a big plus\nAirflow","['Streamlit', 'Jenkins', 'Linux', 'Docker', 'Sql', 'Python', 'Kubernetes', 'Aws']",2025-06-10 19:39:35
Data Engineer (Streamlit Expertise),Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"Develop scalable data collection, storage, and distribution platform to house data from vendors, research providers, exchanges, PBs, and web-scraping. Make data available to systematic & fundamental PMs, and enterprise functions: Ops, Risk, Trading, and Compliance. Develop internal data products and analytics\nResponsibilities\nWeb scraping using scripts/APIs/Tools\nHelp build and maintain greenfield data platform running on Snowflake and AWS\nUnderstand the existing pipelines and enhance pipelines for the new requirements.\nOnboarding new data providers\nData migration projects\nSkills\nSQL\nsenior level\nPython\nsenior level\nStreamlit Expertise\nLinux\nContainerization (Docker, Kubernetes)\nGood communication skills\nAWS\nDev ops skills (K8s, Docker, Jenkins)\nNice to have\nMarket Data Projects/ Capital markets exp\nSnowflake is a big plus\nAirflow","['Streamlit', 'Jenkins', 'Linux', 'Docker', 'Sql', 'Python', 'Kubernetes', 'Aws']",2025-06-10 19:39:36
Data Engineer (Streamlit Expertise),Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"Develop scalable data collection, storage, and distribution platform to house data from vendors, research providers, exchanges, PBs, and web-scraping. Make data available to systematic & fundamental PMs, and enterprise functions: Ops, Risk, Trading, and Compliance. Develop internal data products and analytics\nResponsibilities\nWeb scraping using scripts/APIs/Tools\nHelp build and maintain greenfield data platform running on Snowflake and AWS\nUnderstand the existing pipelines and enhance pipelines for the new requirements.\nOnboarding new data providers\nData migration projects\nSkills\nSQL\nsenior level\nPython\nsenior level\nStreamlit Expertise\nLinux\nContainerization (Docker, Kubernetes)\nGood communication skills\nAWS\nDev ops skills (K8s, Docker, Jenkins)\nNice to have\nMarket Data Projects/ Capital markets exp\nSnowflake is a big plus\nAirflow","['Streamlit', 'Jenkins', 'Linux', 'Docker', 'Sql', 'Python', 'Kubernetes', 'Aws']",2025-06-10 19:39:37
Data Engineer (Streamlit Expertise),Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"Develop scalable data collection, storage, and distribution platform to house data from vendors, research providers, exchanges, PBs, and web-scraping. Make data available to systematic & fundamental PMs, and enterprise functions: Ops, Risk, Trading, and Compliance. Develop internal data products and analytics\nResponsibilities\nWeb scraping using scripts/APIs/Tools\nHelp build and maintain greenfield data platform running on Snowflake and AWS\nUnderstand the existing pipelines and enhance pipelines for the new requirements.\nOnboarding new data providers\nData migration projects\nSkills\nSQL\nsenior level\nPython\nsenior level\nStreamlit Expertise\nLinux\nContainerization (Docker, Kubernetes)\nGood communication skills\nAWS\nDev ops skills (K8s, Docker, Jenkins)\nNice to have\nMarket Data Projects/ Capital markets exp\nSnowflake is a big plus\nAirflow","['Streamlit', 'Jenkins', 'Linux', 'Docker', 'Sql', 'Python', 'Kubernetes', 'Aws']",2025-06-10 19:39:38
Data Engineer (Streamlit Expertise),Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"Develop scalable data collection, storage, and distribution platform to house data from vendors, research providers, exchanges, PBs, and web-scraping. Make data available to systematic & fundamental PMs, and enterprise functions: Ops, Risk, Trading, and Compliance. Develop internal data products and analytics\nResponsibilities\nWeb scraping using scripts/APIs/Tools\nHelp build and maintain greenfield data platform running on Snowflake and AWS\nUnderstand the existing pipelines and enhance pipelines for the new requirements.\nOnboarding new data providers\nData migration projects\nSkills\nSQL\nsenior level\nPython\nsenior level\nStreamlit Expertise\nLinux\nContainerization (Docker, Kubernetes)\nGood communication skills\nAWS\nDev ops skills (K8s, Docker, Jenkins)\nNice to have\nMarket Data Projects/ Capital markets exp\nSnowflake is a big plus\nAirflow","['Streamlit', 'Jenkins', 'Linux', 'Docker', 'Sql', 'Python', 'Kubernetes', 'Aws']",2025-06-10 19:39:39
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:39:47
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:39:50
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:39:53
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:39:54
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:39:55
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:39:55
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:39:56
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:39:57
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:39:58
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:39:59
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:40:00
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:40:08
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:40:11
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:40:14
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:40:14
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:40:15
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:40:16
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:40:17
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:40:17
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:40:18
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:40:19
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:40:20
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:40:28
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:40:31
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:40:34
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:40:34
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:40:35
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:40:36
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:40:37
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:40:38
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:40:39
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:40:39
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:40:40
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:40:49
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:40:52
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:40:55
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:40:55
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:40:56
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:40:57
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:40:58
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:40:59
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:41:00
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:41:01
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:41:02
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:41:09
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:41:12
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:41:15
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:41:16
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:41:17
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:41:17
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:41:18
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:41:19
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:41:20
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:41:21
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:41:22
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:41:29
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:41:33
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:41:36
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:41:36
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:41:37
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:41:38
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:41:39
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:41:40
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:41:41
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:41:41
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:41:42
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:41:50
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:41:54
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:41:56
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:41:57
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:41:58
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:41:59
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:42:00
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:42:00
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:42:01
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:42:02
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:42:03
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:42:11
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:42:15
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:42:17
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:42:18
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:42:19
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:42:20
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:42:21
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:42:21
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:42:22
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:42:23
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:42:24
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:42:32
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:42:35
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:42:38
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:42:39
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:42:39
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:42:40
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:42:41
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:42:42
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:42:43
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:42:44
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:42:44
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:42:52
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:42:56
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:42:58
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:42:59
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:43:00
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:43:01
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:43:02
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:43:03
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:43:03
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:43:04
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:43:05
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:43:13
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:43:17
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:43:19
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:43:20
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:43:21
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:43:21
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:43:22
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:43:23
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:43:24
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:43:25
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:43:26
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:43:33
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:43:37
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:43:40
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:43:40
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:43:41
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:43:42
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:43:43
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:43:44
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:43:44
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:43:45
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:43:46
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:43:53
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:43:57
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:44:00
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:44:00
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:44:01
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:44:02
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:44:03
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:44:04
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:44:04
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:44:05
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:44:06
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:44:14
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:44:18
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:44:20
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:44:21
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:44:22
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:44:23
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:44:24
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:44:24
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:44:25
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:44:26
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:44:27
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:44:35
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:44:38
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:44:41
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:44:42
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:44:42
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:44:43
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:44:44
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:44:45
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:44:46
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:44:47
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:44:48
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:44:55
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:44:59
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:45:01
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:45:02
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:45:03
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:45:04
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:45:05
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:45:06
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:45:07
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:45:08
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:45:09
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:45:15
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:45:16
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:45:17
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:45:17
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:45:18
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:45:19
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:45:20
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:45:21
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:45:21
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:45:22
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:45:23
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:45:31
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:45:35
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:45:37
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:45:38
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:45:39
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:45:39
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:45:40
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:45:41
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:45:42
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:45:43
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:45:44
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:45:52
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:45:55
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:45:58
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:45:59
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:46:00
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:46:00
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:46:01
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:46:02
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:46:03
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:46:04
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:46:05
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:46:12
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:46:16
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:46:18
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:46:19
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:46:20
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:46:21
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:46:22
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:46:23
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:46:24
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:46:25
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:46:26
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:46:33
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:46:37
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:46:39
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:46:40
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:46:41
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:46:42
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:46:43
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:46:43
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:46:44
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:46:45
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:46:46
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:46:54
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:46:57
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:47:00
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:47:01
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:47:02
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:47:02
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:47:03
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:47:04
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:47:05
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:47:06
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:47:06
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:47:15
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:47:18
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:47:21
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:47:21
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:47:22
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:47:23
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:47:24
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:47:25
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:47:26
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:47:27
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:47:28
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:47:36
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:47:40
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:47:42
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:47:43
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:47:43
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:47:44
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:47:45
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:47:46
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:47:47
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:47:48
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:47:49
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:47:57
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:00
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:02
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:03
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:04
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:05
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:06
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:06
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:07
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:08
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:09
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:17
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:20
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:23
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:24
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:24
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:25
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:26
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:27
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:28
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:29
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:30
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:37
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:41
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:43
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:44
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:45
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:46
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:47
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:47
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:49
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:49
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:50
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:56
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:57
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:58
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:48:58
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:49:00
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:49:00
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:49:01
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:49:02
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:49:03
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:49:04
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:49:05
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:49:12
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:49:15
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:49:18
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:49:19
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:49:20
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:49:20
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:49:21
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:49:22
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:49:23
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:49:24
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:49:25
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:49:33
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:49:37
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:49:39
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:49:40
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:49:41
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:49:42
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:49:43
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:49:43
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:49:44
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:49:45
Data Platform Engineer,LTIMindtree Limited,5-8 Years,,Pune,Information Technology,Job description\nData Platform Engineer\nPrimary Skills\nApache NiFi Administration experience like building clusters\nNiFi cluster version upgrades\nStrong troubleshooting skills on linux\nGood knowledge of kafka administration\nConfiguration management tools like Jenkins Ansible\nSecondary Skills\nPython automation skills\nDatabase management like postgers\nAgile Practices,"['Troubleshooting', 'Database Management', 'Infrastructure Management', 'Linux', 'Configuration management', 'Agile', 'Jenkins', 'Automation', 'Apache', 'Python']",2025-06-10 19:49:46
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:49:54
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:49:57
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:50:00
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:50:01
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:50:01
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:50:02
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:50:03
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:50:04
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:50:05
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:50:06
Data & AI Engineer,Luxoft India Llp,2-6 Years,,Bengaluru,Login to check your skill match score,"Project Description:\nDXC - a Fortune 500 global IT services leader. At DXC Technology we deliver the mission-critical IT services that move the world. Every day we use the power of technology to build better futures for our customers, colleagues, environment, and communities across the globe.\nWe are flexible - we provide everything you need to comfortably work from home, but we also keep our offices open for collaboration, meetings, and building a strong team spirit. We tailor everyone's development path to their individual interests through training and additional certifications.\nWe are seeking an experienced SAP SuccessFactors Employee Central Payroll Consultant with specialized knowledge in payroll. The ideal candidate will have a deep understanding of payroll processes, compliance requirements, and the ability to implement and support SAP SuccessFactors Payroll solutions.\nResponsibilities:\nImplement and configure SAP SuccessFactors Employee Central solutions for clients.\nProvide expert advice on RBPs\nCollaborate with clients to gather requirements by organizing workshops, design and configure Employee Central solution that meet their needs.\nConduct system testing, troubleshooting, and provide ongoing support.\nTrain and support end-users to ensure successful adoption of the Employee Central system.\nStay current with SAP SuccessFactors updates and best practices.\nMandatory Skills Description:\nProven experience as an SAP SuccessFactors Employee Central Consultant.\nStrong understanding and configuration knowledge of SAP SuccessFactors Employee Central module.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and manage multiple projects simultaneously.\nStrong communication and interpersonal skills.\nNice-to-Have Skills Description:\nSAP SuccessFactors Employee Central certification.\nFamiliarity with SAP's activate project management methodology.","['Data Analysis', 'Tensorflow', 'Machine Learning', 'Data Visualization', 'Big Data', 'Python', 'Sql']",2025-06-10 19:50:06
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:50:14
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:50:17
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:50:20
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:50:21
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:50:22
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:50:23
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:50:23
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:50:24
Senior Data Ops Engineer,Luxoft India Llp,5-10 Years,,Remote,Login to check your skill match score,"We are looking for a Senior Data Engineer with expertise in SQL, Python, AWS, and containerization to build and maintain a scalable data platform. The role involves working with web scraping, data pipelines, and DevOps practices while collaborating with cross-functional teams to onboard new data providers and support data migration projects.\nResponsibilities:\nPerform web scraping using scripts, APIs, and tools.\nBuild and maintain a greenfield data platform on Snowflake and AWS.\nUnderstand existing data pipelines and enhance them for new business requirements.\nManage onboarding of new data providers and execute data migration projects.\nRequired Skills & Experience:\n10+ years of experience as a Data Engineer.\nProficiency in SQL and Python for data processing and automation.\nStrong experience working in Linux environments.\nHands-on experience with containerization technologies: Docker, Kubernetes.\nExpertise in AWS cloud services.\nStrong DevOps knowledge: Kubernetes, Docker, Jenkins.\nExcellent communication skills and ability to collaborate across teams.\nNice to Have:\nExperience in market data projects or capital markets.\nHands-on experience with Snowflake (big plus).\nKnowledge of Airflow for workflow orchestration.","['snowflake', 'Airflow', 'Jenkins', 'Sql', 'Python', 'Docker', 'Linux', 'Kubernetes']",2025-06-10 19:50:25
