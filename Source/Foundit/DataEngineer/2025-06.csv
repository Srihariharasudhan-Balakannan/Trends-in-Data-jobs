job_title,company_name,experience,salary,location,industry,job_description,skills,scraped_at
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:00:09
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:00:13
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:00:15
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:00:16
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:00:17
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:00:18
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:00:19
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:00:19
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:00:20
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:00:21
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:00:22
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:00:23
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:00:23
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:00:24
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:00:25
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:08:26
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:08:30
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:08:32
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:08:33
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:08:34
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:08:35
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:08:36
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:08:37
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:08:38
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:08:39
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:08:40
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:08:41
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:08:42
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:08:42
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:08:44
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:11:05
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:11:09
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:11:11
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:11:13
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:11:15
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:11:16
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:11:18
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:11:20
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:11:21
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:11:23
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:11:24
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:11:26
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:11:28
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:11:29
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:11:31
Data Engineer,Virtusa,5-8 Years,,Chennai,Information Technology,8 Years overall IT experience with minimum 5 years of work experience in below tech skills\nStrong experience in Python Scripting and PySpark for data processing\nProficiency in SQL dealing with big data over Informatica ETL\nProven experience in Data quality and data optimization of data lake in Iceberg format with strong understanding of architecture\nExperience in AWS Glue jobs\nExperience in AWS cloud platform and its data services S3 Redshift Lambda EMR Airflow Postgres SNS Event bridge\nExpertise in BASH Shell scripting\nStrong understanding of healthcare data systems and experience leading data engineering teams\nExperience in Agile environments\nExcellent problem solving skills and attention to detail\nEffective communication and collaboration skills\nResponsibilities\nLeads development of data pipelines and architectures that handle large scale data sets\nDesigns constructs and tests data architecture aligned with business requirements\nProvides technical leadership for data projects ensuring best practices and high quality data solutions\nCollaborates with product finance and other business units to ensure data pipelines meet business requirements\nWork with DBT Data Build Tool for transforming raw data into actionable insights\nOversees development of data solutions that enable predictive and prescriptive analytics\nEnsures the technical quality of solutions managing data as it moves across environments\nAligns data architecture to Healthfirst solution architecture,"['Iceberg', 'Python', 'Pyspark', 'Sql', 'AWS Glue', 'Informatica']",2025-06-12 01:11:40
Data Engineer,Virtusa,5-8 Years,,Chennai,Information Technology,8 Years overall IT experience with minimum 5 years of work experience in below tech skills\nStrong experience in Python Scripting and PySpark for data processing\nProficiency in SQL dealing with big data over Informatica ETL\nProven experience in Data quality and data optimization of data lake in Iceberg format with strong understanding of architecture\nExperience in AWS Glue jobs\nExperience in AWS cloud platform and its data services S3 Redshift Lambda EMR Airflow Postgres SNS Event bridge\nExpertise in BASH Shell scripting\nStrong understanding of healthcare data systems and experience leading data engineering teams\nExperience in Agile environments\nExcellent problem solving skills and attention to detail\nEffective communication and collaboration skills\nResponsibilities\nLeads development of data pipelines and architectures that handle large scale data sets\nDesigns constructs and tests data architecture aligned with business requirements\nProvides technical leadership for data projects ensuring best practices and high quality data solutions\nCollaborates with product finance and other business units to ensure data pipelines meet business requirements\nWork with DBT Data Build Tool for transforming raw data into actionable insights\nOversees development of data solutions that enable predictive and prescriptive analytics\nEnsures the technical quality of solutions managing data as it moves across environments\nAligns data architecture to Healthfirst solution architecture,"['Iceberg', 'Python', 'Pyspark', 'Sql', 'AWS Glue', 'Informatica']",2025-06-12 01:11:45
Data Engineer,Virtusa,5-8 Years,,Chennai,Information Technology,8 Years overall IT experience with minimum 5 years of work experience in below tech skills\nStrong experience in Python Scripting and PySpark for data processing\nProficiency in SQL dealing with big data over Informatica ETL\nProven experience in Data quality and data optimization of data lake in Iceberg format with strong understanding of architecture\nExperience in AWS Glue jobs\nExperience in AWS cloud platform and its data services S3 Redshift Lambda EMR Airflow Postgres SNS Event bridge\nExpertise in BASH Shell scripting\nStrong understanding of healthcare data systems and experience leading data engineering teams\nExperience in Agile environments\nExcellent problem solving skills and attention to detail\nEffective communication and collaboration skills\nResponsibilities\nLeads development of data pipelines and architectures that handle large scale data sets\nDesigns constructs and tests data architecture aligned with business requirements\nProvides technical leadership for data projects ensuring best practices and high quality data solutions\nCollaborates with product finance and other business units to ensure data pipelines meet business requirements\nWork with DBT Data Build Tool for transforming raw data into actionable insights\nOversees development of data solutions that enable predictive and prescriptive analytics\nEnsures the technical quality of solutions managing data as it moves across environments\nAligns data architecture to Healthfirst solution architecture,"['Iceberg', 'Python', 'Pyspark', 'Sql', 'AWS Glue', 'Informatica']",2025-06-12 01:11:47
Data Engineer,Virtusa,5-8 Years,,Chennai,Information Technology,8 Years overall IT experience with minimum 5 years of work experience in below tech skills\nStrong experience in Python Scripting and PySpark for data processing\nProficiency in SQL dealing with big data over Informatica ETL\nProven experience in Data quality and data optimization of data lake in Iceberg format with strong understanding of architecture\nExperience in AWS Glue jobs\nExperience in AWS cloud platform and its data services S3 Redshift Lambda EMR Airflow Postgres SNS Event bridge\nExpertise in BASH Shell scripting\nStrong understanding of healthcare data systems and experience leading data engineering teams\nExperience in Agile environments\nExcellent problem solving skills and attention to detail\nEffective communication and collaboration skills\nResponsibilities\nLeads development of data pipelines and architectures that handle large scale data sets\nDesigns constructs and tests data architecture aligned with business requirements\nProvides technical leadership for data projects ensuring best practices and high quality data solutions\nCollaborates with product finance and other business units to ensure data pipelines meet business requirements\nWork with DBT Data Build Tool for transforming raw data into actionable insights\nOversees development of data solutions that enable predictive and prescriptive analytics\nEnsures the technical quality of solutions managing data as it moves across environments\nAligns data architecture to Healthfirst solution architecture,"['Iceberg', 'Python', 'Pyspark', 'Sql', 'AWS Glue', 'Informatica']",2025-06-12 01:11:48
Data Engineer,Virtusa,5-8 Years,,Chennai,Information Technology,8 Years overall IT experience with minimum 5 years of work experience in below tech skills\nStrong experience in Python Scripting and PySpark for data processing\nProficiency in SQL dealing with big data over Informatica ETL\nProven experience in Data quality and data optimization of data lake in Iceberg format with strong understanding of architecture\nExperience in AWS Glue jobs\nExperience in AWS cloud platform and its data services S3 Redshift Lambda EMR Airflow Postgres SNS Event bridge\nExpertise in BASH Shell scripting\nStrong understanding of healthcare data systems and experience leading data engineering teams\nExperience in Agile environments\nExcellent problem solving skills and attention to detail\nEffective communication and collaboration skills\nResponsibilities\nLeads development of data pipelines and architectures that handle large scale data sets\nDesigns constructs and tests data architecture aligned with business requirements\nProvides technical leadership for data projects ensuring best practices and high quality data solutions\nCollaborates with product finance and other business units to ensure data pipelines meet business requirements\nWork with DBT Data Build Tool for transforming raw data into actionable insights\nOversees development of data solutions that enable predictive and prescriptive analytics\nEnsures the technical quality of solutions managing data as it moves across environments\nAligns data architecture to Healthfirst solution architecture,"['Iceberg', 'Python', 'Pyspark', 'Sql', 'AWS Glue', 'Informatica']",2025-06-12 01:11:50
Data Engineer,Virtusa,5-8 Years,,Chennai,Information Technology,8 Years overall IT experience with minimum 5 years of work experience in below tech skills\nStrong experience in Python Scripting and PySpark for data processing\nProficiency in SQL dealing with big data over Informatica ETL\nProven experience in Data quality and data optimization of data lake in Iceberg format with strong understanding of architecture\nExperience in AWS Glue jobs\nExperience in AWS cloud platform and its data services S3 Redshift Lambda EMR Airflow Postgres SNS Event bridge\nExpertise in BASH Shell scripting\nStrong understanding of healthcare data systems and experience leading data engineering teams\nExperience in Agile environments\nExcellent problem solving skills and attention to detail\nEffective communication and collaboration skills\nResponsibilities\nLeads development of data pipelines and architectures that handle large scale data sets\nDesigns constructs and tests data architecture aligned with business requirements\nProvides technical leadership for data projects ensuring best practices and high quality data solutions\nCollaborates with product finance and other business units to ensure data pipelines meet business requirements\nWork with DBT Data Build Tool for transforming raw data into actionable insights\nOversees development of data solutions that enable predictive and prescriptive analytics\nEnsures the technical quality of solutions managing data as it moves across environments\nAligns data architecture to Healthfirst solution architecture,"['Iceberg', 'Python', 'Pyspark', 'Sql', 'AWS Glue', 'Informatica']",2025-06-12 01:11:51
Data Engineer,Virtusa,5-8 Years,,Chennai,Information Technology,8 Years overall IT experience with minimum 5 years of work experience in below tech skills\nStrong experience in Python Scripting and PySpark for data processing\nProficiency in SQL dealing with big data over Informatica ETL\nProven experience in Data quality and data optimization of data lake in Iceberg format with strong understanding of architecture\nExperience in AWS Glue jobs\nExperience in AWS cloud platform and its data services S3 Redshift Lambda EMR Airflow Postgres SNS Event bridge\nExpertise in BASH Shell scripting\nStrong understanding of healthcare data systems and experience leading data engineering teams\nExperience in Agile environments\nExcellent problem solving skills and attention to detail\nEffective communication and collaboration skills\nResponsibilities\nLeads development of data pipelines and architectures that handle large scale data sets\nDesigns constructs and tests data architecture aligned with business requirements\nProvides technical leadership for data projects ensuring best practices and high quality data solutions\nCollaborates with product finance and other business units to ensure data pipelines meet business requirements\nWork with DBT Data Build Tool for transforming raw data into actionable insights\nOversees development of data solutions that enable predictive and prescriptive analytics\nEnsures the technical quality of solutions managing data as it moves across environments\nAligns data architecture to Healthfirst solution architecture,"['Iceberg', 'Python', 'Pyspark', 'Sql', 'AWS Glue', 'Informatica']",2025-06-12 01:11:53
Data Engineer,Virtusa,5-8 Years,,Chennai,Information Technology,8 Years overall IT experience with minimum 5 years of work experience in below tech skills\nStrong experience in Python Scripting and PySpark for data processing\nProficiency in SQL dealing with big data over Informatica ETL\nProven experience in Data quality and data optimization of data lake in Iceberg format with strong understanding of architecture\nExperience in AWS Glue jobs\nExperience in AWS cloud platform and its data services S3 Redshift Lambda EMR Airflow Postgres SNS Event bridge\nExpertise in BASH Shell scripting\nStrong understanding of healthcare data systems and experience leading data engineering teams\nExperience in Agile environments\nExcellent problem solving skills and attention to detail\nEffective communication and collaboration skills\nResponsibilities\nLeads development of data pipelines and architectures that handle large scale data sets\nDesigns constructs and tests data architecture aligned with business requirements\nProvides technical leadership for data projects ensuring best practices and high quality data solutions\nCollaborates with product finance and other business units to ensure data pipelines meet business requirements\nWork with DBT Data Build Tool for transforming raw data into actionable insights\nOversees development of data solutions that enable predictive and prescriptive analytics\nEnsures the technical quality of solutions managing data as it moves across environments\nAligns data architecture to Healthfirst solution architecture,"['Iceberg', 'Python', 'Pyspark', 'Sql', 'AWS Glue', 'Informatica']",2025-06-12 01:11:54
Data Engineer,Virtusa,5-8 Years,,Chennai,Information Technology,8 Years overall IT experience with minimum 5 years of work experience in below tech skills\nStrong experience in Python Scripting and PySpark for data processing\nProficiency in SQL dealing with big data over Informatica ETL\nProven experience in Data quality and data optimization of data lake in Iceberg format with strong understanding of architecture\nExperience in AWS Glue jobs\nExperience in AWS cloud platform and its data services S3 Redshift Lambda EMR Airflow Postgres SNS Event bridge\nExpertise in BASH Shell scripting\nStrong understanding of healthcare data systems and experience leading data engineering teams\nExperience in Agile environments\nExcellent problem solving skills and attention to detail\nEffective communication and collaboration skills\nResponsibilities\nLeads development of data pipelines and architectures that handle large scale data sets\nDesigns constructs and tests data architecture aligned with business requirements\nProvides technical leadership for data projects ensuring best practices and high quality data solutions\nCollaborates with product finance and other business units to ensure data pipelines meet business requirements\nWork with DBT Data Build Tool for transforming raw data into actionable insights\nOversees development of data solutions that enable predictive and prescriptive analytics\nEnsures the technical quality of solutions managing data as it moves across environments\nAligns data architecture to Healthfirst solution architecture,"['Iceberg', 'Python', 'Pyspark', 'Sql', 'AWS Glue', 'Informatica']",2025-06-12 01:11:56
Data Engineer,Virtusa,5-8 Years,,Chennai,Information Technology,8 Years overall IT experience with minimum 5 years of work experience in below tech skills\nStrong experience in Python Scripting and PySpark for data processing\nProficiency in SQL dealing with big data over Informatica ETL\nProven experience in Data quality and data optimization of data lake in Iceberg format with strong understanding of architecture\nExperience in AWS Glue jobs\nExperience in AWS cloud platform and its data services S3 Redshift Lambda EMR Airflow Postgres SNS Event bridge\nExpertise in BASH Shell scripting\nStrong understanding of healthcare data systems and experience leading data engineering teams\nExperience in Agile environments\nExcellent problem solving skills and attention to detail\nEffective communication and collaboration skills\nResponsibilities\nLeads development of data pipelines and architectures that handle large scale data sets\nDesigns constructs and tests data architecture aligned with business requirements\nProvides technical leadership for data projects ensuring best practices and high quality data solutions\nCollaborates with product finance and other business units to ensure data pipelines meet business requirements\nWork with DBT Data Build Tool for transforming raw data into actionable insights\nOversees development of data solutions that enable predictive and prescriptive analytics\nEnsures the technical quality of solutions managing data as it moves across environments\nAligns data architecture to Healthfirst solution architecture,"['Iceberg', 'Python', 'Pyspark', 'Sql', 'AWS Glue', 'Informatica']",2025-06-12 01:11:58
Data Engineer,Virtusa,5-8 Years,,Chennai,Information Technology,8 Years overall IT experience with minimum 5 years of work experience in below tech skills\nStrong experience in Python Scripting and PySpark for data processing\nProficiency in SQL dealing with big data over Informatica ETL\nProven experience in Data quality and data optimization of data lake in Iceberg format with strong understanding of architecture\nExperience in AWS Glue jobs\nExperience in AWS cloud platform and its data services S3 Redshift Lambda EMR Airflow Postgres SNS Event bridge\nExpertise in BASH Shell scripting\nStrong understanding of healthcare data systems and experience leading data engineering teams\nExperience in Agile environments\nExcellent problem solving skills and attention to detail\nEffective communication and collaboration skills\nResponsibilities\nLeads development of data pipelines and architectures that handle large scale data sets\nDesigns constructs and tests data architecture aligned with business requirements\nProvides technical leadership for data projects ensuring best practices and high quality data solutions\nCollaborates with product finance and other business units to ensure data pipelines meet business requirements\nWork with DBT Data Build Tool for transforming raw data into actionable insights\nOversees development of data solutions that enable predictive and prescriptive analytics\nEnsures the technical quality of solutions managing data as it moves across environments\nAligns data architecture to Healthfirst solution architecture,"['Iceberg', 'Python', 'Pyspark', 'Sql', 'AWS Glue', 'Informatica']",2025-06-12 01:11:59
Data Engineer,Virtusa,5-8 Years,,Chennai,Information Technology,8 Years overall IT experience with minimum 5 years of work experience in below tech skills\nStrong experience in Python Scripting and PySpark for data processing\nProficiency in SQL dealing with big data over Informatica ETL\nProven experience in Data quality and data optimization of data lake in Iceberg format with strong understanding of architecture\nExperience in AWS Glue jobs\nExperience in AWS cloud platform and its data services S3 Redshift Lambda EMR Airflow Postgres SNS Event bridge\nExpertise in BASH Shell scripting\nStrong understanding of healthcare data systems and experience leading data engineering teams\nExperience in Agile environments\nExcellent problem solving skills and attention to detail\nEffective communication and collaboration skills\nResponsibilities\nLeads development of data pipelines and architectures that handle large scale data sets\nDesigns constructs and tests data architecture aligned with business requirements\nProvides technical leadership for data projects ensuring best practices and high quality data solutions\nCollaborates with product finance and other business units to ensure data pipelines meet business requirements\nWork with DBT Data Build Tool for transforming raw data into actionable insights\nOversees development of data solutions that enable predictive and prescriptive analytics\nEnsures the technical quality of solutions managing data as it moves across environments\nAligns data architecture to Healthfirst solution architecture,"['Iceberg', 'Python', 'Pyspark', 'Sql', 'AWS Glue', 'Informatica']",2025-06-12 01:12:01
Data Engineer,Virtusa,5-8 Years,,Chennai,Information Technology,8 Years overall IT experience with minimum 5 years of work experience in below tech skills\nStrong experience in Python Scripting and PySpark for data processing\nProficiency in SQL dealing with big data over Informatica ETL\nProven experience in Data quality and data optimization of data lake in Iceberg format with strong understanding of architecture\nExperience in AWS Glue jobs\nExperience in AWS cloud platform and its data services S3 Redshift Lambda EMR Airflow Postgres SNS Event bridge\nExpertise in BASH Shell scripting\nStrong understanding of healthcare data systems and experience leading data engineering teams\nExperience in Agile environments\nExcellent problem solving skills and attention to detail\nEffective communication and collaboration skills\nResponsibilities\nLeads development of data pipelines and architectures that handle large scale data sets\nDesigns constructs and tests data architecture aligned with business requirements\nProvides technical leadership for data projects ensuring best practices and high quality data solutions\nCollaborates with product finance and other business units to ensure data pipelines meet business requirements\nWork with DBT Data Build Tool for transforming raw data into actionable insights\nOversees development of data solutions that enable predictive and prescriptive analytics\nEnsures the technical quality of solutions managing data as it moves across environments\nAligns data architecture to Healthfirst solution architecture,"['Iceberg', 'Python', 'Pyspark', 'Sql', 'AWS Glue', 'Informatica']",2025-06-12 01:12:02
Data Engineer,Virtusa,5-8 Years,,Chennai,Information Technology,8 Years overall IT experience with minimum 5 years of work experience in below tech skills\nStrong experience in Python Scripting and PySpark for data processing\nProficiency in SQL dealing with big data over Informatica ETL\nProven experience in Data quality and data optimization of data lake in Iceberg format with strong understanding of architecture\nExperience in AWS Glue jobs\nExperience in AWS cloud platform and its data services S3 Redshift Lambda EMR Airflow Postgres SNS Event bridge\nExpertise in BASH Shell scripting\nStrong understanding of healthcare data systems and experience leading data engineering teams\nExperience in Agile environments\nExcellent problem solving skills and attention to detail\nEffective communication and collaboration skills\nResponsibilities\nLeads development of data pipelines and architectures that handle large scale data sets\nDesigns constructs and tests data architecture aligned with business requirements\nProvides technical leadership for data projects ensuring best practices and high quality data solutions\nCollaborates with product finance and other business units to ensure data pipelines meet business requirements\nWork with DBT Data Build Tool for transforming raw data into actionable insights\nOversees development of data solutions that enable predictive and prescriptive analytics\nEnsures the technical quality of solutions managing data as it moves across environments\nAligns data architecture to Healthfirst solution architecture,"['Iceberg', 'Python', 'Pyspark', 'Sql', 'AWS Glue', 'Informatica']",2025-06-12 01:12:04
Data Engineer,Virtusa,5-8 Years,,Chennai,Information Technology,8 Years overall IT experience with minimum 5 years of work experience in below tech skills\nStrong experience in Python Scripting and PySpark for data processing\nProficiency in SQL dealing with big data over Informatica ETL\nProven experience in Data quality and data optimization of data lake in Iceberg format with strong understanding of architecture\nExperience in AWS Glue jobs\nExperience in AWS cloud platform and its data services S3 Redshift Lambda EMR Airflow Postgres SNS Event bridge\nExpertise in BASH Shell scripting\nStrong understanding of healthcare data systems and experience leading data engineering teams\nExperience in Agile environments\nExcellent problem solving skills and attention to detail\nEffective communication and collaboration skills\nResponsibilities\nLeads development of data pipelines and architectures that handle large scale data sets\nDesigns constructs and tests data architecture aligned with business requirements\nProvides technical leadership for data projects ensuring best practices and high quality data solutions\nCollaborates with product finance and other business units to ensure data pipelines meet business requirements\nWork with DBT Data Build Tool for transforming raw data into actionable insights\nOversees development of data solutions that enable predictive and prescriptive analytics\nEnsures the technical quality of solutions managing data as it moves across environments\nAligns data architecture to Healthfirst solution architecture,"['Iceberg', 'Python', 'Pyspark', 'Sql', 'AWS Glue', 'Informatica']",2025-06-12 01:12:06
GCP Data Engineer (Cloud Solutions and Data Processing),Synechron Technologies Private Limited,2-4 Years,,Pune,Software,"Software Requirements:\nStrong understanding of other technology platforms such as mobile, cloud, IoT, and blockchain.\nKnowledge of software development life cycle (SDLC) and Agile methodologies.\nOverall Responsibilities:\nWork closely with cross-functional teams to understand technology requirements and design solutions to meet business needs.\nDevelop technical specifications and detailed documentation for new features and enhancements.\nStay current with the latest technology trends and advancements and suggest ways to incorporate them into existing solutions.\nConduct code reviews to ensure the quality and maintainability of the codebase.\nParticipate in the resolution of technical issues and provide technical support to team members.\nCollaborate with the testing team to ensure software solutions are thoroughly tested and meet quality standards.\nSkills:\nStrong technical skills in mobile, cloud, IoT, or blockchain technologies.\nProficiency in one or more programming languages such as Java, Python, or Node.js.\nExcellent communication and interpersonal skills.\nAbility to work independently and as part of a team.\nAbility to take initiative, prioritize tasks, and manage time effectively.\nExperience:\nMinimum of 4+ years of experience in software development, with a focus on other technologies.\nExperience with software development methodologies and tools such as Agile, Scrum, Git, JIRA, and Confluence.\nExperience working with cross-functional teams and participating in code reviews.\nDay-to-Day Activities:\nParticipating in daily stand-up meetings and project planning sessions.\nCollaborating with cross-functional teams to understand business requirements and design solutions.\nWriting, testing, and deploying software solutions.\nParticipating in code reviews and providing feedback to other team members.\nStaying current with the latest technology trends and advancements.\nProviding technical support to team members and resolving technical issues.\nQualification:\nBachelors or Masters degree in Computer Science, Information Technology, or a related field.\nSoft Skills:\nExcellent written and verbal communication skills.\nAbility to work well in a team environment.\nStrong problem-solving and analytical skills.\nAbility to adapt to new technologies and changing requirements.\nGood time management and prioritization skills.","['GCP Data Engineer (Cloud Solutions and Data Processing)', 'Java', 'Node.js', 'Python', 'Sdlc']",2025-06-12 01:12:15
GCP Data Engineer (Cloud Solutions and Data Processing),Synechron Technologies Private Limited,2-4 Years,,Pune,Software,"Software Requirements:\nStrong understanding of other technology platforms such as mobile, cloud, IoT, and blockchain.\nKnowledge of software development life cycle (SDLC) and Agile methodologies.\nOverall Responsibilities:\nWork closely with cross-functional teams to understand technology requirements and design solutions to meet business needs.\nDevelop technical specifications and detailed documentation for new features and enhancements.\nStay current with the latest technology trends and advancements and suggest ways to incorporate them into existing solutions.\nConduct code reviews to ensure the quality and maintainability of the codebase.\nParticipate in the resolution of technical issues and provide technical support to team members.\nCollaborate with the testing team to ensure software solutions are thoroughly tested and meet quality standards.\nSkills:\nStrong technical skills in mobile, cloud, IoT, or blockchain technologies.\nProficiency in one or more programming languages such as Java, Python, or Node.js.\nExcellent communication and interpersonal skills.\nAbility to work independently and as part of a team.\nAbility to take initiative, prioritize tasks, and manage time effectively.\nExperience:\nMinimum of 4+ years of experience in software development, with a focus on other technologies.\nExperience with software development methodologies and tools such as Agile, Scrum, Git, JIRA, and Confluence.\nExperience working with cross-functional teams and participating in code reviews.\nDay-to-Day Activities:\nParticipating in daily stand-up meetings and project planning sessions.\nCollaborating with cross-functional teams to understand business requirements and design solutions.\nWriting, testing, and deploying software solutions.\nParticipating in code reviews and providing feedback to other team members.\nStaying current with the latest technology trends and advancements.\nProviding technical support to team members and resolving technical issues.\nQualification:\nBachelors or Masters degree in Computer Science, Information Technology, or a related field.\nSoft Skills:\nExcellent written and verbal communication skills.\nAbility to work well in a team environment.\nStrong problem-solving and analytical skills.\nAbility to adapt to new technologies and changing requirements.\nGood time management and prioritization skills.","['GCP Data Engineer (Cloud Solutions and Data Processing)', 'Java', 'Node.js', 'Python', 'Sdlc']",2025-06-12 01:12:18
GCP Data Engineer (Cloud Solutions and Data Processing),Synechron Technologies Private Limited,2-4 Years,,Pune,Software,"Software Requirements:\nStrong understanding of other technology platforms such as mobile, cloud, IoT, and blockchain.\nKnowledge of software development life cycle (SDLC) and Agile methodologies.\nOverall Responsibilities:\nWork closely with cross-functional teams to understand technology requirements and design solutions to meet business needs.\nDevelop technical specifications and detailed documentation for new features and enhancements.\nStay current with the latest technology trends and advancements and suggest ways to incorporate them into existing solutions.\nConduct code reviews to ensure the quality and maintainability of the codebase.\nParticipate in the resolution of technical issues and provide technical support to team members.\nCollaborate with the testing team to ensure software solutions are thoroughly tested and meet quality standards.\nSkills:\nStrong technical skills in mobile, cloud, IoT, or blockchain technologies.\nProficiency in one or more programming languages such as Java, Python, or Node.js.\nExcellent communication and interpersonal skills.\nAbility to work independently and as part of a team.\nAbility to take initiative, prioritize tasks, and manage time effectively.\nExperience:\nMinimum of 4+ years of experience in software development, with a focus on other technologies.\nExperience with software development methodologies and tools such as Agile, Scrum, Git, JIRA, and Confluence.\nExperience working with cross-functional teams and participating in code reviews.\nDay-to-Day Activities:\nParticipating in daily stand-up meetings and project planning sessions.\nCollaborating with cross-functional teams to understand business requirements and design solutions.\nWriting, testing, and deploying software solutions.\nParticipating in code reviews and providing feedback to other team members.\nStaying current with the latest technology trends and advancements.\nProviding technical support to team members and resolving technical issues.\nQualification:\nBachelors or Masters degree in Computer Science, Information Technology, or a related field.\nSoft Skills:\nExcellent written and verbal communication skills.\nAbility to work well in a team environment.\nStrong problem-solving and analytical skills.\nAbility to adapt to new technologies and changing requirements.\nGood time management and prioritization skills.","['GCP Data Engineer (Cloud Solutions and Data Processing)', 'Java', 'Node.js', 'Python', 'Sdlc']",2025-06-12 01:12:20
GCP Data Engineer (Cloud Solutions and Data Processing),Synechron Technologies Private Limited,2-4 Years,,Pune,Software,"Software Requirements:\nStrong understanding of other technology platforms such as mobile, cloud, IoT, and blockchain.\nKnowledge of software development life cycle (SDLC) and Agile methodologies.\nOverall Responsibilities:\nWork closely with cross-functional teams to understand technology requirements and design solutions to meet business needs.\nDevelop technical specifications and detailed documentation for new features and enhancements.\nStay current with the latest technology trends and advancements and suggest ways to incorporate them into existing solutions.\nConduct code reviews to ensure the quality and maintainability of the codebase.\nParticipate in the resolution of technical issues and provide technical support to team members.\nCollaborate with the testing team to ensure software solutions are thoroughly tested and meet quality standards.\nSkills:\nStrong technical skills in mobile, cloud, IoT, or blockchain technologies.\nProficiency in one or more programming languages such as Java, Python, or Node.js.\nExcellent communication and interpersonal skills.\nAbility to work independently and as part of a team.\nAbility to take initiative, prioritize tasks, and manage time effectively.\nExperience:\nMinimum of 4+ years of experience in software development, with a focus on other technologies.\nExperience with software development methodologies and tools such as Agile, Scrum, Git, JIRA, and Confluence.\nExperience working with cross-functional teams and participating in code reviews.\nDay-to-Day Activities:\nParticipating in daily stand-up meetings and project planning sessions.\nCollaborating with cross-functional teams to understand business requirements and design solutions.\nWriting, testing, and deploying software solutions.\nParticipating in code reviews and providing feedback to other team members.\nStaying current with the latest technology trends and advancements.\nProviding technical support to team members and resolving technical issues.\nQualification:\nBachelors or Masters degree in Computer Science, Information Technology, or a related field.\nSoft Skills:\nExcellent written and verbal communication skills.\nAbility to work well in a team environment.\nStrong problem-solving and analytical skills.\nAbility to adapt to new technologies and changing requirements.\nGood time management and prioritization skills.","['GCP Data Engineer (Cloud Solutions and Data Processing)', 'Java', 'Node.js', 'Python', 'Sdlc']",2025-06-12 01:12:22
GCP Data Engineer (Cloud Solutions and Data Processing),Synechron Technologies Private Limited,2-4 Years,,Pune,Software,"Software Requirements:\nStrong understanding of other technology platforms such as mobile, cloud, IoT, and blockchain.\nKnowledge of software development life cycle (SDLC) and Agile methodologies.\nOverall Responsibilities:\nWork closely with cross-functional teams to understand technology requirements and design solutions to meet business needs.\nDevelop technical specifications and detailed documentation for new features and enhancements.\nStay current with the latest technology trends and advancements and suggest ways to incorporate them into existing solutions.\nConduct code reviews to ensure the quality and maintainability of the codebase.\nParticipate in the resolution of technical issues and provide technical support to team members.\nCollaborate with the testing team to ensure software solutions are thoroughly tested and meet quality standards.\nSkills:\nStrong technical skills in mobile, cloud, IoT, or blockchain technologies.\nProficiency in one or more programming languages such as Java, Python, or Node.js.\nExcellent communication and interpersonal skills.\nAbility to work independently and as part of a team.\nAbility to take initiative, prioritize tasks, and manage time effectively.\nExperience:\nMinimum of 4+ years of experience in software development, with a focus on other technologies.\nExperience with software development methodologies and tools such as Agile, Scrum, Git, JIRA, and Confluence.\nExperience working with cross-functional teams and participating in code reviews.\nDay-to-Day Activities:\nParticipating in daily stand-up meetings and project planning sessions.\nCollaborating with cross-functional teams to understand business requirements and design solutions.\nWriting, testing, and deploying software solutions.\nParticipating in code reviews and providing feedback to other team members.\nStaying current with the latest technology trends and advancements.\nProviding technical support to team members and resolving technical issues.\nQualification:\nBachelors or Masters degree in Computer Science, Information Technology, or a related field.\nSoft Skills:\nExcellent written and verbal communication skills.\nAbility to work well in a team environment.\nStrong problem-solving and analytical skills.\nAbility to adapt to new technologies and changing requirements.\nGood time management and prioritization skills.","['GCP Data Engineer (Cloud Solutions and Data Processing)', 'Java', 'Node.js', 'Python', 'Sdlc']",2025-06-12 01:12:23
GCP Data Engineer (Cloud Solutions and Data Processing),Synechron Technologies Private Limited,2-4 Years,,Pune,Software,"Software Requirements:\nStrong understanding of other technology platforms such as mobile, cloud, IoT, and blockchain.\nKnowledge of software development life cycle (SDLC) and Agile methodologies.\nOverall Responsibilities:\nWork closely with cross-functional teams to understand technology requirements and design solutions to meet business needs.\nDevelop technical specifications and detailed documentation for new features and enhancements.\nStay current with the latest technology trends and advancements and suggest ways to incorporate them into existing solutions.\nConduct code reviews to ensure the quality and maintainability of the codebase.\nParticipate in the resolution of technical issues and provide technical support to team members.\nCollaborate with the testing team to ensure software solutions are thoroughly tested and meet quality standards.\nSkills:\nStrong technical skills in mobile, cloud, IoT, or blockchain technologies.\nProficiency in one or more programming languages such as Java, Python, or Node.js.\nExcellent communication and interpersonal skills.\nAbility to work independently and as part of a team.\nAbility to take initiative, prioritize tasks, and manage time effectively.\nExperience:\nMinimum of 4+ years of experience in software development, with a focus on other technologies.\nExperience with software development methodologies and tools such as Agile, Scrum, Git, JIRA, and Confluence.\nExperience working with cross-functional teams and participating in code reviews.\nDay-to-Day Activities:\nParticipating in daily stand-up meetings and project planning sessions.\nCollaborating with cross-functional teams to understand business requirements and design solutions.\nWriting, testing, and deploying software solutions.\nParticipating in code reviews and providing feedback to other team members.\nStaying current with the latest technology trends and advancements.\nProviding technical support to team members and resolving technical issues.\nQualification:\nBachelors or Masters degree in Computer Science, Information Technology, or a related field.\nSoft Skills:\nExcellent written and verbal communication skills.\nAbility to work well in a team environment.\nStrong problem-solving and analytical skills.\nAbility to adapt to new technologies and changing requirements.\nGood time management and prioritization skills.","['GCP Data Engineer (Cloud Solutions and Data Processing)', 'Java', 'Node.js', 'Python', 'Sdlc']",2025-06-12 01:12:25
GCP Data Engineer (Cloud Solutions and Data Processing),Synechron Technologies Private Limited,2-4 Years,,Pune,Software,"Software Requirements:\nStrong understanding of other technology platforms such as mobile, cloud, IoT, and blockchain.\nKnowledge of software development life cycle (SDLC) and Agile methodologies.\nOverall Responsibilities:\nWork closely with cross-functional teams to understand technology requirements and design solutions to meet business needs.\nDevelop technical specifications and detailed documentation for new features and enhancements.\nStay current with the latest technology trends and advancements and suggest ways to incorporate them into existing solutions.\nConduct code reviews to ensure the quality and maintainability of the codebase.\nParticipate in the resolution of technical issues and provide technical support to team members.\nCollaborate with the testing team to ensure software solutions are thoroughly tested and meet quality standards.\nSkills:\nStrong technical skills in mobile, cloud, IoT, or blockchain technologies.\nProficiency in one or more programming languages such as Java, Python, or Node.js.\nExcellent communication and interpersonal skills.\nAbility to work independently and as part of a team.\nAbility to take initiative, prioritize tasks, and manage time effectively.\nExperience:\nMinimum of 4+ years of experience in software development, with a focus on other technologies.\nExperience with software development methodologies and tools such as Agile, Scrum, Git, JIRA, and Confluence.\nExperience working with cross-functional teams and participating in code reviews.\nDay-to-Day Activities:\nParticipating in daily stand-up meetings and project planning sessions.\nCollaborating with cross-functional teams to understand business requirements and design solutions.\nWriting, testing, and deploying software solutions.\nParticipating in code reviews and providing feedback to other team members.\nStaying current with the latest technology trends and advancements.\nProviding technical support to team members and resolving technical issues.\nQualification:\nBachelors or Masters degree in Computer Science, Information Technology, or a related field.\nSoft Skills:\nExcellent written and verbal communication skills.\nAbility to work well in a team environment.\nStrong problem-solving and analytical skills.\nAbility to adapt to new technologies and changing requirements.\nGood time management and prioritization skills.","['GCP Data Engineer (Cloud Solutions and Data Processing)', 'Java', 'Node.js', 'Python', 'Sdlc']",2025-06-12 01:12:26
GCP Data Engineer (Cloud Solutions and Data Processing),Synechron Technologies Private Limited,2-4 Years,,Pune,Software,"Software Requirements:\nStrong understanding of other technology platforms such as mobile, cloud, IoT, and blockchain.\nKnowledge of software development life cycle (SDLC) and Agile methodologies.\nOverall Responsibilities:\nWork closely with cross-functional teams to understand technology requirements and design solutions to meet business needs.\nDevelop technical specifications and detailed documentation for new features and enhancements.\nStay current with the latest technology trends and advancements and suggest ways to incorporate them into existing solutions.\nConduct code reviews to ensure the quality and maintainability of the codebase.\nParticipate in the resolution of technical issues and provide technical support to team members.\nCollaborate with the testing team to ensure software solutions are thoroughly tested and meet quality standards.\nSkills:\nStrong technical skills in mobile, cloud, IoT, or blockchain technologies.\nProficiency in one or more programming languages such as Java, Python, or Node.js.\nExcellent communication and interpersonal skills.\nAbility to work independently and as part of a team.\nAbility to take initiative, prioritize tasks, and manage time effectively.\nExperience:\nMinimum of 4+ years of experience in software development, with a focus on other technologies.\nExperience with software development methodologies and tools such as Agile, Scrum, Git, JIRA, and Confluence.\nExperience working with cross-functional teams and participating in code reviews.\nDay-to-Day Activities:\nParticipating in daily stand-up meetings and project planning sessions.\nCollaborating with cross-functional teams to understand business requirements and design solutions.\nWriting, testing, and deploying software solutions.\nParticipating in code reviews and providing feedback to other team members.\nStaying current with the latest technology trends and advancements.\nProviding technical support to team members and resolving technical issues.\nQualification:\nBachelors or Masters degree in Computer Science, Information Technology, or a related field.\nSoft Skills:\nExcellent written and verbal communication skills.\nAbility to work well in a team environment.\nStrong problem-solving and analytical skills.\nAbility to adapt to new technologies and changing requirements.\nGood time management and prioritization skills.","['GCP Data Engineer (Cloud Solutions and Data Processing)', 'Java', 'Node.js', 'Python', 'Sdlc']",2025-06-12 01:12:28
GCP Data Engineer (Cloud Solutions and Data Processing),Synechron Technologies Private Limited,2-4 Years,,Pune,Software,"Software Requirements:\nStrong understanding of other technology platforms such as mobile, cloud, IoT, and blockchain.\nKnowledge of software development life cycle (SDLC) and Agile methodologies.\nOverall Responsibilities:\nWork closely with cross-functional teams to understand technology requirements and design solutions to meet business needs.\nDevelop technical specifications and detailed documentation for new features and enhancements.\nStay current with the latest technology trends and advancements and suggest ways to incorporate them into existing solutions.\nConduct code reviews to ensure the quality and maintainability of the codebase.\nParticipate in the resolution of technical issues and provide technical support to team members.\nCollaborate with the testing team to ensure software solutions are thoroughly tested and meet quality standards.\nSkills:\nStrong technical skills in mobile, cloud, IoT, or blockchain technologies.\nProficiency in one or more programming languages such as Java, Python, or Node.js.\nExcellent communication and interpersonal skills.\nAbility to work independently and as part of a team.\nAbility to take initiative, prioritize tasks, and manage time effectively.\nExperience:\nMinimum of 4+ years of experience in software development, with a focus on other technologies.\nExperience with software development methodologies and tools such as Agile, Scrum, Git, JIRA, and Confluence.\nExperience working with cross-functional teams and participating in code reviews.\nDay-to-Day Activities:\nParticipating in daily stand-up meetings and project planning sessions.\nCollaborating with cross-functional teams to understand business requirements and design solutions.\nWriting, testing, and deploying software solutions.\nParticipating in code reviews and providing feedback to other team members.\nStaying current with the latest technology trends and advancements.\nProviding technical support to team members and resolving technical issues.\nQualification:\nBachelors or Masters degree in Computer Science, Information Technology, or a related field.\nSoft Skills:\nExcellent written and verbal communication skills.\nAbility to work well in a team environment.\nStrong problem-solving and analytical skills.\nAbility to adapt to new technologies and changing requirements.\nGood time management and prioritization skills.","['GCP Data Engineer (Cloud Solutions and Data Processing)', 'Java', 'Node.js', 'Python', 'Sdlc']",2025-06-12 01:12:29
GCP Data Engineer (Cloud Solutions and Data Processing),Synechron Technologies Private Limited,2-4 Years,,Pune,Software,"Software Requirements:\nStrong understanding of other technology platforms such as mobile, cloud, IoT, and blockchain.\nKnowledge of software development life cycle (SDLC) and Agile methodologies.\nOverall Responsibilities:\nWork closely with cross-functional teams to understand technology requirements and design solutions to meet business needs.\nDevelop technical specifications and detailed documentation for new features and enhancements.\nStay current with the latest technology trends and advancements and suggest ways to incorporate them into existing solutions.\nConduct code reviews to ensure the quality and maintainability of the codebase.\nParticipate in the resolution of technical issues and provide technical support to team members.\nCollaborate with the testing team to ensure software solutions are thoroughly tested and meet quality standards.\nSkills:\nStrong technical skills in mobile, cloud, IoT, or blockchain technologies.\nProficiency in one or more programming languages such as Java, Python, or Node.js.\nExcellent communication and interpersonal skills.\nAbility to work independently and as part of a team.\nAbility to take initiative, prioritize tasks, and manage time effectively.\nExperience:\nMinimum of 4+ years of experience in software development, with a focus on other technologies.\nExperience with software development methodologies and tools such as Agile, Scrum, Git, JIRA, and Confluence.\nExperience working with cross-functional teams and participating in code reviews.\nDay-to-Day Activities:\nParticipating in daily stand-up meetings and project planning sessions.\nCollaborating with cross-functional teams to understand business requirements and design solutions.\nWriting, testing, and deploying software solutions.\nParticipating in code reviews and providing feedback to other team members.\nStaying current with the latest technology trends and advancements.\nProviding technical support to team members and resolving technical issues.\nQualification:\nBachelors or Masters degree in Computer Science, Information Technology, or a related field.\nSoft Skills:\nExcellent written and verbal communication skills.\nAbility to work well in a team environment.\nStrong problem-solving and analytical skills.\nAbility to adapt to new technologies and changing requirements.\nGood time management and prioritization skills.","['GCP Data Engineer (Cloud Solutions and Data Processing)', 'Java', 'Node.js', 'Python', 'Sdlc']",2025-06-12 01:12:31
GCP Data Engineer (Cloud Solutions and Data Processing),Synechron Technologies Private Limited,2-4 Years,,Pune,Software,"Software Requirements:\nStrong understanding of other technology platforms such as mobile, cloud, IoT, and blockchain.\nKnowledge of software development life cycle (SDLC) and Agile methodologies.\nOverall Responsibilities:\nWork closely with cross-functional teams to understand technology requirements and design solutions to meet business needs.\nDevelop technical specifications and detailed documentation for new features and enhancements.\nStay current with the latest technology trends and advancements and suggest ways to incorporate them into existing solutions.\nConduct code reviews to ensure the quality and maintainability of the codebase.\nParticipate in the resolution of technical issues and provide technical support to team members.\nCollaborate with the testing team to ensure software solutions are thoroughly tested and meet quality standards.\nSkills:\nStrong technical skills in mobile, cloud, IoT, or blockchain technologies.\nProficiency in one or more programming languages such as Java, Python, or Node.js.\nExcellent communication and interpersonal skills.\nAbility to work independently and as part of a team.\nAbility to take initiative, prioritize tasks, and manage time effectively.\nExperience:\nMinimum of 4+ years of experience in software development, with a focus on other technologies.\nExperience with software development methodologies and tools such as Agile, Scrum, Git, JIRA, and Confluence.\nExperience working with cross-functional teams and participating in code reviews.\nDay-to-Day Activities:\nParticipating in daily stand-up meetings and project planning sessions.\nCollaborating with cross-functional teams to understand business requirements and design solutions.\nWriting, testing, and deploying software solutions.\nParticipating in code reviews and providing feedback to other team members.\nStaying current with the latest technology trends and advancements.\nProviding technical support to team members and resolving technical issues.\nQualification:\nBachelors or Masters degree in Computer Science, Information Technology, or a related field.\nSoft Skills:\nExcellent written and verbal communication skills.\nAbility to work well in a team environment.\nStrong problem-solving and analytical skills.\nAbility to adapt to new technologies and changing requirements.\nGood time management and prioritization skills.","['GCP Data Engineer (Cloud Solutions and Data Processing)', 'Java', 'Node.js', 'Python', 'Sdlc']",2025-06-12 01:12:32
GCP Data Engineer (Cloud Solutions and Data Processing),Synechron Technologies Private Limited,2-4 Years,,Pune,Software,"Software Requirements:\nStrong understanding of other technology platforms such as mobile, cloud, IoT, and blockchain.\nKnowledge of software development life cycle (SDLC) and Agile methodologies.\nOverall Responsibilities:\nWork closely with cross-functional teams to understand technology requirements and design solutions to meet business needs.\nDevelop technical specifications and detailed documentation for new features and enhancements.\nStay current with the latest technology trends and advancements and suggest ways to incorporate them into existing solutions.\nConduct code reviews to ensure the quality and maintainability of the codebase.\nParticipate in the resolution of technical issues and provide technical support to team members.\nCollaborate with the testing team to ensure software solutions are thoroughly tested and meet quality standards.\nSkills:\nStrong technical skills in mobile, cloud, IoT, or blockchain technologies.\nProficiency in one or more programming languages such as Java, Python, or Node.js.\nExcellent communication and interpersonal skills.\nAbility to work independently and as part of a team.\nAbility to take initiative, prioritize tasks, and manage time effectively.\nExperience:\nMinimum of 4+ years of experience in software development, with a focus on other technologies.\nExperience with software development methodologies and tools such as Agile, Scrum, Git, JIRA, and Confluence.\nExperience working with cross-functional teams and participating in code reviews.\nDay-to-Day Activities:\nParticipating in daily stand-up meetings and project planning sessions.\nCollaborating with cross-functional teams to understand business requirements and design solutions.\nWriting, testing, and deploying software solutions.\nParticipating in code reviews and providing feedback to other team members.\nStaying current with the latest technology trends and advancements.\nProviding technical support to team members and resolving technical issues.\nQualification:\nBachelors or Masters degree in Computer Science, Information Technology, or a related field.\nSoft Skills:\nExcellent written and verbal communication skills.\nAbility to work well in a team environment.\nStrong problem-solving and analytical skills.\nAbility to adapt to new technologies and changing requirements.\nGood time management and prioritization skills.","['GCP Data Engineer (Cloud Solutions and Data Processing)', 'Java', 'Node.js', 'Python', 'Sdlc']",2025-06-12 01:12:34
GCP Data Engineer (Cloud Solutions and Data Processing),Synechron Technologies Private Limited,2-4 Years,,Pune,Software,"Software Requirements:\nStrong understanding of other technology platforms such as mobile, cloud, IoT, and blockchain.\nKnowledge of software development life cycle (SDLC) and Agile methodologies.\nOverall Responsibilities:\nWork closely with cross-functional teams to understand technology requirements and design solutions to meet business needs.\nDevelop technical specifications and detailed documentation for new features and enhancements.\nStay current with the latest technology trends and advancements and suggest ways to incorporate them into existing solutions.\nConduct code reviews to ensure the quality and maintainability of the codebase.\nParticipate in the resolution of technical issues and provide technical support to team members.\nCollaborate with the testing team to ensure software solutions are thoroughly tested and meet quality standards.\nSkills:\nStrong technical skills in mobile, cloud, IoT, or blockchain technologies.\nProficiency in one or more programming languages such as Java, Python, or Node.js.\nExcellent communication and interpersonal skills.\nAbility to work independently and as part of a team.\nAbility to take initiative, prioritize tasks, and manage time effectively.\nExperience:\nMinimum of 4+ years of experience in software development, with a focus on other technologies.\nExperience with software development methodologies and tools such as Agile, Scrum, Git, JIRA, and Confluence.\nExperience working with cross-functional teams and participating in code reviews.\nDay-to-Day Activities:\nParticipating in daily stand-up meetings and project planning sessions.\nCollaborating with cross-functional teams to understand business requirements and design solutions.\nWriting, testing, and deploying software solutions.\nParticipating in code reviews and providing feedback to other team members.\nStaying current with the latest technology trends and advancements.\nProviding technical support to team members and resolving technical issues.\nQualification:\nBachelors or Masters degree in Computer Science, Information Technology, or a related field.\nSoft Skills:\nExcellent written and verbal communication skills.\nAbility to work well in a team environment.\nStrong problem-solving and analytical skills.\nAbility to adapt to new technologies and changing requirements.\nGood time management and prioritization skills.","['GCP Data Engineer (Cloud Solutions and Data Processing)', 'Java', 'Node.js', 'Python', 'Sdlc']",2025-06-12 01:12:35
GCP Data Engineer (Cloud Solutions and Data Processing),Synechron Technologies Private Limited,2-4 Years,,Pune,Software,"Software Requirements:\nStrong understanding of other technology platforms such as mobile, cloud, IoT, and blockchain.\nKnowledge of software development life cycle (SDLC) and Agile methodologies.\nOverall Responsibilities:\nWork closely with cross-functional teams to understand technology requirements and design solutions to meet business needs.\nDevelop technical specifications and detailed documentation for new features and enhancements.\nStay current with the latest technology trends and advancements and suggest ways to incorporate them into existing solutions.\nConduct code reviews to ensure the quality and maintainability of the codebase.\nParticipate in the resolution of technical issues and provide technical support to team members.\nCollaborate with the testing team to ensure software solutions are thoroughly tested and meet quality standards.\nSkills:\nStrong technical skills in mobile, cloud, IoT, or blockchain technologies.\nProficiency in one or more programming languages such as Java, Python, or Node.js.\nExcellent communication and interpersonal skills.\nAbility to work independently and as part of a team.\nAbility to take initiative, prioritize tasks, and manage time effectively.\nExperience:\nMinimum of 4+ years of experience in software development, with a focus on other technologies.\nExperience with software development methodologies and tools such as Agile, Scrum, Git, JIRA, and Confluence.\nExperience working with cross-functional teams and participating in code reviews.\nDay-to-Day Activities:\nParticipating in daily stand-up meetings and project planning sessions.\nCollaborating with cross-functional teams to understand business requirements and design solutions.\nWriting, testing, and deploying software solutions.\nParticipating in code reviews and providing feedback to other team members.\nStaying current with the latest technology trends and advancements.\nProviding technical support to team members and resolving technical issues.\nQualification:\nBachelors or Masters degree in Computer Science, Information Technology, or a related field.\nSoft Skills:\nExcellent written and verbal communication skills.\nAbility to work well in a team environment.\nStrong problem-solving and analytical skills.\nAbility to adapt to new technologies and changing requirements.\nGood time management and prioritization skills.","['GCP Data Engineer (Cloud Solutions and Data Processing)', 'Java', 'Node.js', 'Python', 'Sdlc']",2025-06-12 01:12:37
GCP Data Engineer (Cloud Solutions and Data Processing),Synechron Technologies Private Limited,2-4 Years,,Pune,Software,"Software Requirements:\nStrong understanding of other technology platforms such as mobile, cloud, IoT, and blockchain.\nKnowledge of software development life cycle (SDLC) and Agile methodologies.\nOverall Responsibilities:\nWork closely with cross-functional teams to understand technology requirements and design solutions to meet business needs.\nDevelop technical specifications and detailed documentation for new features and enhancements.\nStay current with the latest technology trends and advancements and suggest ways to incorporate them into existing solutions.\nConduct code reviews to ensure the quality and maintainability of the codebase.\nParticipate in the resolution of technical issues and provide technical support to team members.\nCollaborate with the testing team to ensure software solutions are thoroughly tested and meet quality standards.\nSkills:\nStrong technical skills in mobile, cloud, IoT, or blockchain technologies.\nProficiency in one or more programming languages such as Java, Python, or Node.js.\nExcellent communication and interpersonal skills.\nAbility to work independently and as part of a team.\nAbility to take initiative, prioritize tasks, and manage time effectively.\nExperience:\nMinimum of 4+ years of experience in software development, with a focus on other technologies.\nExperience with software development methodologies and tools such as Agile, Scrum, Git, JIRA, and Confluence.\nExperience working with cross-functional teams and participating in code reviews.\nDay-to-Day Activities:\nParticipating in daily stand-up meetings and project planning sessions.\nCollaborating with cross-functional teams to understand business requirements and design solutions.\nWriting, testing, and deploying software solutions.\nParticipating in code reviews and providing feedback to other team members.\nStaying current with the latest technology trends and advancements.\nProviding technical support to team members and resolving technical issues.\nQualification:\nBachelors or Masters degree in Computer Science, Information Technology, or a related field.\nSoft Skills:\nExcellent written and verbal communication skills.\nAbility to work well in a team environment.\nStrong problem-solving and analytical skills.\nAbility to adapt to new technologies and changing requirements.\nGood time management and prioritization skills.","['GCP Data Engineer (Cloud Solutions and Data Processing)', 'Java', 'Node.js', 'Python', 'Sdlc']",2025-06-12 01:12:38
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,Software,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns.\nYou'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences","['Elasticsearch', 'Splunk', 'Hadoop', 'Gcp', 'Data Replication', 'Apis']",2025-06-12 01:12:49
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,Software,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns.\nYou'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences","['Elasticsearch', 'Splunk', 'Hadoop', 'Gcp', 'Data Replication', 'Apis']",2025-06-12 01:12:52
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,Software,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns.\nYou'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences","['Elasticsearch', 'Splunk', 'Hadoop', 'Gcp', 'Data Replication', 'Apis']",2025-06-12 01:12:55
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,Software,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns.\nYou'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences","['Elasticsearch', 'Splunk', 'Hadoop', 'Gcp', 'Data Replication', 'Apis']",2025-06-12 01:12:56
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,Software,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns.\nYou'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences","['Elasticsearch', 'Splunk', 'Hadoop', 'Gcp', 'Data Replication', 'Apis']",2025-06-12 01:12:58
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,Software,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns.\nYou'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences","['Elasticsearch', 'Splunk', 'Hadoop', 'Gcp', 'Data Replication', 'Apis']",2025-06-12 01:12:59
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,Software,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns.\nYou'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences","['Elasticsearch', 'Splunk', 'Hadoop', 'Gcp', 'Data Replication', 'Apis']",2025-06-12 01:13:01
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,Software,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns.\nYou'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences","['Elasticsearch', 'Splunk', 'Hadoop', 'Gcp', 'Data Replication', 'Apis']",2025-06-12 01:13:02
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,Software,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns.\nYou'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences","['Elasticsearch', 'Splunk', 'Hadoop', 'Gcp', 'Data Replication', 'Apis']",2025-06-12 01:13:04
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,Software,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns.\nYou'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences","['Elasticsearch', 'Splunk', 'Hadoop', 'Gcp', 'Data Replication', 'Apis']",2025-06-12 01:13:05
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,Software,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns.\nYou'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences","['Elasticsearch', 'Splunk', 'Hadoop', 'Gcp', 'Data Replication', 'Apis']",2025-06-12 01:13:07
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,Software,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns.\nYou'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences","['Elasticsearch', 'Splunk', 'Hadoop', 'Gcp', 'Data Replication', 'Apis']",2025-06-12 01:13:08
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,Software,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns.\nYou'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences","['Elasticsearch', 'Splunk', 'Hadoop', 'Gcp', 'Data Replication', 'Apis']",2025-06-12 01:13:10
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,Software,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns.\nYou'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences","['Elasticsearch', 'Splunk', 'Hadoop', 'Gcp', 'Data Replication', 'Apis']",2025-06-12 01:13:12
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,Software,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns.\nYou'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences","['Elasticsearch', 'Splunk', 'Hadoop', 'Gcp', 'Data Replication', 'Apis']",2025-06-12 01:13:13
AWS data engineer,Virtusa,6-12 Years,,Hyderabad,Information Technology,"Develop and implement efficient data pipelines using Apache Spark (PySpark preferred) to process and analyze large-scale data.\nDesign, build, and optimize complex SQL queries to extract, transform, and load (ETL) data from multiple sources.\nOrchestrate data workflows using Apache Airflow, ensuring smooth execution and error-free pipelines.\nDesign, implement, and maintain scalable and cost-effective data storage and processing solutions on AWS using S3, Glue, EMR, and Athena.\nLeverage AWS Lambda and Step Functions for serverless compute and task orchestration in data pipelines.\nWork with AWS databases like RDS and DynamoDB to ensure efficient data storage and retrieval.\nMonitor data processing and pipeline health using AWS CloudWatch and ensure smooth operation in production environments.\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.\nPerform performance tuning, optimize distributed data processing tasks, and handle scalability issues.\nProvide troubleshooting and support for data pipeline failures and ensure high availability and reliability.\nContribute to the setup and maintenance of CI/CD pipelines for automated deployment and testing of data workflows.\nRequired Skills & Experience:\nExperience: Minimum of 6+ years of hands-on experience in data engineering or big data development roles, with a focus on designing and building data pipelines and processing systems.\nTechnical Skills:\nStrong programming skills in Python with hands-on experience in Apache Spark (PySpark preferred).\nProficient in writing and optimizing complex SQL queries for data extraction, transformation, and loading.\nHands-on experience with Apache Airflow for orchestration of data workflows and pipeline management.\nIn-depth understanding and practical experience with AWS services:\nData Storage & Processing: S3, Glue, EMR, Athena\nCompute & Execution: Lambda, Step Functions\nDatabases: RDS, DynamoDB\nMonitoring: CloudWatch\nExperience with distributed data processing, parallel computing, and performance tuning techniques.\nStrong analytical and problem-solving skills to troubleshoot and optimize data workflows and pipelines.\nFamiliarity with CI/CD pipelines and DevOps practices for continuous integration and automated deployments is a plus.\nPreferred Qualifications:\nFamiliarity with other cloud platforms (Azure, Google Cloud) and services related to data engineering.\nExperience in handling unstructured and semi-structured data and working with data lakes.\nKnowledge of containerization technologies such as Docker or orchestration systems like Kubernetes.\nExperience with NoSQL databases or data warehouses like Redshift or BigQuery is a plus.\nQualifications:\nEducation: Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field.\nExperience: Minimum of 6+ years in a data engineering role with strong expertise in AWS and big data processing frameworks.","['Apache Spark (PySpark)', 'AWS (S3', 'Glue', 'Athena', 'Step Functions', 'Python', 'Sql', 'Apache Airflow', 'Emr']",2025-06-12 01:13:24
AWS data engineer,Virtusa,6-12 Years,,Hyderabad,Information Technology,"Develop and implement efficient data pipelines using Apache Spark (PySpark preferred) to process and analyze large-scale data.\nDesign, build, and optimize complex SQL queries to extract, transform, and load (ETL) data from multiple sources.\nOrchestrate data workflows using Apache Airflow, ensuring smooth execution and error-free pipelines.\nDesign, implement, and maintain scalable and cost-effective data storage and processing solutions on AWS using S3, Glue, EMR, and Athena.\nLeverage AWS Lambda and Step Functions for serverless compute and task orchestration in data pipelines.\nWork with AWS databases like RDS and DynamoDB to ensure efficient data storage and retrieval.\nMonitor data processing and pipeline health using AWS CloudWatch and ensure smooth operation in production environments.\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.\nPerform performance tuning, optimize distributed data processing tasks, and handle scalability issues.\nProvide troubleshooting and support for data pipeline failures and ensure high availability and reliability.\nContribute to the setup and maintenance of CI/CD pipelines for automated deployment and testing of data workflows.\nRequired Skills & Experience:\nExperience: Minimum of 6+ years of hands-on experience in data engineering or big data development roles, with a focus on designing and building data pipelines and processing systems.\nTechnical Skills:\nStrong programming skills in Python with hands-on experience in Apache Spark (PySpark preferred).\nProficient in writing and optimizing complex SQL queries for data extraction, transformation, and loading.\nHands-on experience with Apache Airflow for orchestration of data workflows and pipeline management.\nIn-depth understanding and practical experience with AWS services:\nData Storage & Processing: S3, Glue, EMR, Athena\nCompute & Execution: Lambda, Step Functions\nDatabases: RDS, DynamoDB\nMonitoring: CloudWatch\nExperience with distributed data processing, parallel computing, and performance tuning techniques.\nStrong analytical and problem-solving skills to troubleshoot and optimize data workflows and pipelines.\nFamiliarity with CI/CD pipelines and DevOps practices for continuous integration and automated deployments is a plus.\nPreferred Qualifications:\nFamiliarity with other cloud platforms (Azure, Google Cloud) and services related to data engineering.\nExperience in handling unstructured and semi-structured data and working with data lakes.\nKnowledge of containerization technologies such as Docker or orchestration systems like Kubernetes.\nExperience with NoSQL databases or data warehouses like Redshift or BigQuery is a plus.\nQualifications:\nEducation: Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field.\nExperience: Minimum of 6+ years in a data engineering role with strong expertise in AWS and big data processing frameworks.","['Apache Spark (PySpark)', 'AWS (S3', 'Glue', 'Athena', 'Step Functions', 'Python', 'Sql', 'Apache Airflow', 'Emr']",2025-06-12 01:13:28
AWS data engineer,Virtusa,6-12 Years,,Hyderabad,Information Technology,"Develop and implement efficient data pipelines using Apache Spark (PySpark preferred) to process and analyze large-scale data.\nDesign, build, and optimize complex SQL queries to extract, transform, and load (ETL) data from multiple sources.\nOrchestrate data workflows using Apache Airflow, ensuring smooth execution and error-free pipelines.\nDesign, implement, and maintain scalable and cost-effective data storage and processing solutions on AWS using S3, Glue, EMR, and Athena.\nLeverage AWS Lambda and Step Functions for serverless compute and task orchestration in data pipelines.\nWork with AWS databases like RDS and DynamoDB to ensure efficient data storage and retrieval.\nMonitor data processing and pipeline health using AWS CloudWatch and ensure smooth operation in production environments.\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.\nPerform performance tuning, optimize distributed data processing tasks, and handle scalability issues.\nProvide troubleshooting and support for data pipeline failures and ensure high availability and reliability.\nContribute to the setup and maintenance of CI/CD pipelines for automated deployment and testing of data workflows.\nRequired Skills & Experience:\nExperience: Minimum of 6+ years of hands-on experience in data engineering or big data development roles, with a focus on designing and building data pipelines and processing systems.\nTechnical Skills:\nStrong programming skills in Python with hands-on experience in Apache Spark (PySpark preferred).\nProficient in writing and optimizing complex SQL queries for data extraction, transformation, and loading.\nHands-on experience with Apache Airflow for orchestration of data workflows and pipeline management.\nIn-depth understanding and practical experience with AWS services:\nData Storage & Processing: S3, Glue, EMR, Athena\nCompute & Execution: Lambda, Step Functions\nDatabases: RDS, DynamoDB\nMonitoring: CloudWatch\nExperience with distributed data processing, parallel computing, and performance tuning techniques.\nStrong analytical and problem-solving skills to troubleshoot and optimize data workflows and pipelines.\nFamiliarity with CI/CD pipelines and DevOps practices for continuous integration and automated deployments is a plus.\nPreferred Qualifications:\nFamiliarity with other cloud platforms (Azure, Google Cloud) and services related to data engineering.\nExperience in handling unstructured and semi-structured data and working with data lakes.\nKnowledge of containerization technologies such as Docker or orchestration systems like Kubernetes.\nExperience with NoSQL databases or data warehouses like Redshift or BigQuery is a plus.\nQualifications:\nEducation: Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field.\nExperience: Minimum of 6+ years in a data engineering role with strong expertise in AWS and big data processing frameworks.","['Apache Spark (PySpark)', 'AWS (S3', 'Glue', 'Athena', 'Step Functions', 'Python', 'Sql', 'Apache Airflow', 'Emr']",2025-06-12 01:13:30
AWS data engineer,Virtusa,6-12 Years,,Hyderabad,Information Technology,"Develop and implement efficient data pipelines using Apache Spark (PySpark preferred) to process and analyze large-scale data.\nDesign, build, and optimize complex SQL queries to extract, transform, and load (ETL) data from multiple sources.\nOrchestrate data workflows using Apache Airflow, ensuring smooth execution and error-free pipelines.\nDesign, implement, and maintain scalable and cost-effective data storage and processing solutions on AWS using S3, Glue, EMR, and Athena.\nLeverage AWS Lambda and Step Functions for serverless compute and task orchestration in data pipelines.\nWork with AWS databases like RDS and DynamoDB to ensure efficient data storage and retrieval.\nMonitor data processing and pipeline health using AWS CloudWatch and ensure smooth operation in production environments.\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.\nPerform performance tuning, optimize distributed data processing tasks, and handle scalability issues.\nProvide troubleshooting and support for data pipeline failures and ensure high availability and reliability.\nContribute to the setup and maintenance of CI/CD pipelines for automated deployment and testing of data workflows.\nRequired Skills & Experience:\nExperience: Minimum of 6+ years of hands-on experience in data engineering or big data development roles, with a focus on designing and building data pipelines and processing systems.\nTechnical Skills:\nStrong programming skills in Python with hands-on experience in Apache Spark (PySpark preferred).\nProficient in writing and optimizing complex SQL queries for data extraction, transformation, and loading.\nHands-on experience with Apache Airflow for orchestration of data workflows and pipeline management.\nIn-depth understanding and practical experience with AWS services:\nData Storage & Processing: S3, Glue, EMR, Athena\nCompute & Execution: Lambda, Step Functions\nDatabases: RDS, DynamoDB\nMonitoring: CloudWatch\nExperience with distributed data processing, parallel computing, and performance tuning techniques.\nStrong analytical and problem-solving skills to troubleshoot and optimize data workflows and pipelines.\nFamiliarity with CI/CD pipelines and DevOps practices for continuous integration and automated deployments is a plus.\nPreferred Qualifications:\nFamiliarity with other cloud platforms (Azure, Google Cloud) and services related to data engineering.\nExperience in handling unstructured and semi-structured data and working with data lakes.\nKnowledge of containerization technologies such as Docker or orchestration systems like Kubernetes.\nExperience with NoSQL databases or data warehouses like Redshift or BigQuery is a plus.\nQualifications:\nEducation: Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field.\nExperience: Minimum of 6+ years in a data engineering role with strong expertise in AWS and big data processing frameworks.","['Apache Spark (PySpark)', 'AWS (S3', 'Glue', 'Athena', 'Step Functions', 'Python', 'Sql', 'Apache Airflow', 'Emr']",2025-06-12 01:13:32
AWS data engineer,Virtusa,6-12 Years,,Hyderabad,Information Technology,"Develop and implement efficient data pipelines using Apache Spark (PySpark preferred) to process and analyze large-scale data.\nDesign, build, and optimize complex SQL queries to extract, transform, and load (ETL) data from multiple sources.\nOrchestrate data workflows using Apache Airflow, ensuring smooth execution and error-free pipelines.\nDesign, implement, and maintain scalable and cost-effective data storage and processing solutions on AWS using S3, Glue, EMR, and Athena.\nLeverage AWS Lambda and Step Functions for serverless compute and task orchestration in data pipelines.\nWork with AWS databases like RDS and DynamoDB to ensure efficient data storage and retrieval.\nMonitor data processing and pipeline health using AWS CloudWatch and ensure smooth operation in production environments.\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.\nPerform performance tuning, optimize distributed data processing tasks, and handle scalability issues.\nProvide troubleshooting and support for data pipeline failures and ensure high availability and reliability.\nContribute to the setup and maintenance of CI/CD pipelines for automated deployment and testing of data workflows.\nRequired Skills & Experience:\nExperience: Minimum of 6+ years of hands-on experience in data engineering or big data development roles, with a focus on designing and building data pipelines and processing systems.\nTechnical Skills:\nStrong programming skills in Python with hands-on experience in Apache Spark (PySpark preferred).\nProficient in writing and optimizing complex SQL queries for data extraction, transformation, and loading.\nHands-on experience with Apache Airflow for orchestration of data workflows and pipeline management.\nIn-depth understanding and practical experience with AWS services:\nData Storage & Processing: S3, Glue, EMR, Athena\nCompute & Execution: Lambda, Step Functions\nDatabases: RDS, DynamoDB\nMonitoring: CloudWatch\nExperience with distributed data processing, parallel computing, and performance tuning techniques.\nStrong analytical and problem-solving skills to troubleshoot and optimize data workflows and pipelines.\nFamiliarity with CI/CD pipelines and DevOps practices for continuous integration and automated deployments is a plus.\nPreferred Qualifications:\nFamiliarity with other cloud platforms (Azure, Google Cloud) and services related to data engineering.\nExperience in handling unstructured and semi-structured data and working with data lakes.\nKnowledge of containerization technologies such as Docker or orchestration systems like Kubernetes.\nExperience with NoSQL databases or data warehouses like Redshift or BigQuery is a plus.\nQualifications:\nEducation: Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field.\nExperience: Minimum of 6+ years in a data engineering role with strong expertise in AWS and big data processing frameworks.","['Apache Spark (PySpark)', 'AWS (S3', 'Glue', 'Athena', 'Step Functions', 'Python', 'Sql', 'Apache Airflow', 'Emr']",2025-06-12 01:13:33
AWS data engineer,Virtusa,6-12 Years,,Hyderabad,Information Technology,"Develop and implement efficient data pipelines using Apache Spark (PySpark preferred) to process and analyze large-scale data.\nDesign, build, and optimize complex SQL queries to extract, transform, and load (ETL) data from multiple sources.\nOrchestrate data workflows using Apache Airflow, ensuring smooth execution and error-free pipelines.\nDesign, implement, and maintain scalable and cost-effective data storage and processing solutions on AWS using S3, Glue, EMR, and Athena.\nLeverage AWS Lambda and Step Functions for serverless compute and task orchestration in data pipelines.\nWork with AWS databases like RDS and DynamoDB to ensure efficient data storage and retrieval.\nMonitor data processing and pipeline health using AWS CloudWatch and ensure smooth operation in production environments.\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.\nPerform performance tuning, optimize distributed data processing tasks, and handle scalability issues.\nProvide troubleshooting and support for data pipeline failures and ensure high availability and reliability.\nContribute to the setup and maintenance of CI/CD pipelines for automated deployment and testing of data workflows.\nRequired Skills & Experience:\nExperience: Minimum of 6+ years of hands-on experience in data engineering or big data development roles, with a focus on designing and building data pipelines and processing systems.\nTechnical Skills:\nStrong programming skills in Python with hands-on experience in Apache Spark (PySpark preferred).\nProficient in writing and optimizing complex SQL queries for data extraction, transformation, and loading.\nHands-on experience with Apache Airflow for orchestration of data workflows and pipeline management.\nIn-depth understanding and practical experience with AWS services:\nData Storage & Processing: S3, Glue, EMR, Athena\nCompute & Execution: Lambda, Step Functions\nDatabases: RDS, DynamoDB\nMonitoring: CloudWatch\nExperience with distributed data processing, parallel computing, and performance tuning techniques.\nStrong analytical and problem-solving skills to troubleshoot and optimize data workflows and pipelines.\nFamiliarity with CI/CD pipelines and DevOps practices for continuous integration and automated deployments is a plus.\nPreferred Qualifications:\nFamiliarity with other cloud platforms (Azure, Google Cloud) and services related to data engineering.\nExperience in handling unstructured and semi-structured data and working with data lakes.\nKnowledge of containerization technologies such as Docker or orchestration systems like Kubernetes.\nExperience with NoSQL databases or data warehouses like Redshift or BigQuery is a plus.\nQualifications:\nEducation: Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field.\nExperience: Minimum of 6+ years in a data engineering role with strong expertise in AWS and big data processing frameworks.","['Apache Spark (PySpark)', 'AWS (S3', 'Glue', 'Athena', 'Step Functions', 'Python', 'Sql', 'Apache Airflow', 'Emr']",2025-06-12 01:13:35
AWS data engineer,Virtusa,6-12 Years,,Hyderabad,Information Technology,"Develop and implement efficient data pipelines using Apache Spark (PySpark preferred) to process and analyze large-scale data.\nDesign, build, and optimize complex SQL queries to extract, transform, and load (ETL) data from multiple sources.\nOrchestrate data workflows using Apache Airflow, ensuring smooth execution and error-free pipelines.\nDesign, implement, and maintain scalable and cost-effective data storage and processing solutions on AWS using S3, Glue, EMR, and Athena.\nLeverage AWS Lambda and Step Functions for serverless compute and task orchestration in data pipelines.\nWork with AWS databases like RDS and DynamoDB to ensure efficient data storage and retrieval.\nMonitor data processing and pipeline health using AWS CloudWatch and ensure smooth operation in production environments.\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.\nPerform performance tuning, optimize distributed data processing tasks, and handle scalability issues.\nProvide troubleshooting and support for data pipeline failures and ensure high availability and reliability.\nContribute to the setup and maintenance of CI/CD pipelines for automated deployment and testing of data workflows.\nRequired Skills & Experience:\nExperience: Minimum of 6+ years of hands-on experience in data engineering or big data development roles, with a focus on designing and building data pipelines and processing systems.\nTechnical Skills:\nStrong programming skills in Python with hands-on experience in Apache Spark (PySpark preferred).\nProficient in writing and optimizing complex SQL queries for data extraction, transformation, and loading.\nHands-on experience with Apache Airflow for orchestration of data workflows and pipeline management.\nIn-depth understanding and practical experience with AWS services:\nData Storage & Processing: S3, Glue, EMR, Athena\nCompute & Execution: Lambda, Step Functions\nDatabases: RDS, DynamoDB\nMonitoring: CloudWatch\nExperience with distributed data processing, parallel computing, and performance tuning techniques.\nStrong analytical and problem-solving skills to troubleshoot and optimize data workflows and pipelines.\nFamiliarity with CI/CD pipelines and DevOps practices for continuous integration and automated deployments is a plus.\nPreferred Qualifications:\nFamiliarity with other cloud platforms (Azure, Google Cloud) and services related to data engineering.\nExperience in handling unstructured and semi-structured data and working with data lakes.\nKnowledge of containerization technologies such as Docker or orchestration systems like Kubernetes.\nExperience with NoSQL databases or data warehouses like Redshift or BigQuery is a plus.\nQualifications:\nEducation: Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field.\nExperience: Minimum of 6+ years in a data engineering role with strong expertise in AWS and big data processing frameworks.","['Apache Spark (PySpark)', 'AWS (S3', 'Glue', 'Athena', 'Step Functions', 'Python', 'Sql', 'Apache Airflow', 'Emr']",2025-06-12 01:13:37
AWS data engineer,Virtusa,6-12 Years,,Hyderabad,Information Technology,"Develop and implement efficient data pipelines using Apache Spark (PySpark preferred) to process and analyze large-scale data.\nDesign, build, and optimize complex SQL queries to extract, transform, and load (ETL) data from multiple sources.\nOrchestrate data workflows using Apache Airflow, ensuring smooth execution and error-free pipelines.\nDesign, implement, and maintain scalable and cost-effective data storage and processing solutions on AWS using S3, Glue, EMR, and Athena.\nLeverage AWS Lambda and Step Functions for serverless compute and task orchestration in data pipelines.\nWork with AWS databases like RDS and DynamoDB to ensure efficient data storage and retrieval.\nMonitor data processing and pipeline health using AWS CloudWatch and ensure smooth operation in production environments.\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.\nPerform performance tuning, optimize distributed data processing tasks, and handle scalability issues.\nProvide troubleshooting and support for data pipeline failures and ensure high availability and reliability.\nContribute to the setup and maintenance of CI/CD pipelines for automated deployment and testing of data workflows.\nRequired Skills & Experience:\nExperience: Minimum of 6+ years of hands-on experience in data engineering or big data development roles, with a focus on designing and building data pipelines and processing systems.\nTechnical Skills:\nStrong programming skills in Python with hands-on experience in Apache Spark (PySpark preferred).\nProficient in writing and optimizing complex SQL queries for data extraction, transformation, and loading.\nHands-on experience with Apache Airflow for orchestration of data workflows and pipeline management.\nIn-depth understanding and practical experience with AWS services:\nData Storage & Processing: S3, Glue, EMR, Athena\nCompute & Execution: Lambda, Step Functions\nDatabases: RDS, DynamoDB\nMonitoring: CloudWatch\nExperience with distributed data processing, parallel computing, and performance tuning techniques.\nStrong analytical and problem-solving skills to troubleshoot and optimize data workflows and pipelines.\nFamiliarity with CI/CD pipelines and DevOps practices for continuous integration and automated deployments is a plus.\nPreferred Qualifications:\nFamiliarity with other cloud platforms (Azure, Google Cloud) and services related to data engineering.\nExperience in handling unstructured and semi-structured data and working with data lakes.\nKnowledge of containerization technologies such as Docker or orchestration systems like Kubernetes.\nExperience with NoSQL databases or data warehouses like Redshift or BigQuery is a plus.\nQualifications:\nEducation: Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field.\nExperience: Minimum of 6+ years in a data engineering role with strong expertise in AWS and big data processing frameworks.","['Apache Spark (PySpark)', 'AWS (S3', 'Glue', 'Athena', 'Step Functions', 'Python', 'Sql', 'Apache Airflow', 'Emr']",2025-06-12 01:13:39
AWS data engineer,Virtusa,6-12 Years,,Hyderabad,Information Technology,"Develop and implement efficient data pipelines using Apache Spark (PySpark preferred) to process and analyze large-scale data.\nDesign, build, and optimize complex SQL queries to extract, transform, and load (ETL) data from multiple sources.\nOrchestrate data workflows using Apache Airflow, ensuring smooth execution and error-free pipelines.\nDesign, implement, and maintain scalable and cost-effective data storage and processing solutions on AWS using S3, Glue, EMR, and Athena.\nLeverage AWS Lambda and Step Functions for serverless compute and task orchestration in data pipelines.\nWork with AWS databases like RDS and DynamoDB to ensure efficient data storage and retrieval.\nMonitor data processing and pipeline health using AWS CloudWatch and ensure smooth operation in production environments.\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.\nPerform performance tuning, optimize distributed data processing tasks, and handle scalability issues.\nProvide troubleshooting and support for data pipeline failures and ensure high availability and reliability.\nContribute to the setup and maintenance of CI/CD pipelines for automated deployment and testing of data workflows.\nRequired Skills & Experience:\nExperience: Minimum of 6+ years of hands-on experience in data engineering or big data development roles, with a focus on designing and building data pipelines and processing systems.\nTechnical Skills:\nStrong programming skills in Python with hands-on experience in Apache Spark (PySpark preferred).\nProficient in writing and optimizing complex SQL queries for data extraction, transformation, and loading.\nHands-on experience with Apache Airflow for orchestration of data workflows and pipeline management.\nIn-depth understanding and practical experience with AWS services:\nData Storage & Processing: S3, Glue, EMR, Athena\nCompute & Execution: Lambda, Step Functions\nDatabases: RDS, DynamoDB\nMonitoring: CloudWatch\nExperience with distributed data processing, parallel computing, and performance tuning techniques.\nStrong analytical and problem-solving skills to troubleshoot and optimize data workflows and pipelines.\nFamiliarity with CI/CD pipelines and DevOps practices for continuous integration and automated deployments is a plus.\nPreferred Qualifications:\nFamiliarity with other cloud platforms (Azure, Google Cloud) and services related to data engineering.\nExperience in handling unstructured and semi-structured data and working with data lakes.\nKnowledge of containerization technologies such as Docker or orchestration systems like Kubernetes.\nExperience with NoSQL databases or data warehouses like Redshift or BigQuery is a plus.\nQualifications:\nEducation: Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field.\nExperience: Minimum of 6+ years in a data engineering role with strong expertise in AWS and big data processing frameworks.","['Apache Spark (PySpark)', 'AWS (S3', 'Glue', 'Athena', 'Step Functions', 'Python', 'Sql', 'Apache Airflow', 'Emr']",2025-06-12 01:13:40
AWS data engineer,Virtusa,6-12 Years,,Hyderabad,Information Technology,"Develop and implement efficient data pipelines using Apache Spark (PySpark preferred) to process and analyze large-scale data.\nDesign, build, and optimize complex SQL queries to extract, transform, and load (ETL) data from multiple sources.\nOrchestrate data workflows using Apache Airflow, ensuring smooth execution and error-free pipelines.\nDesign, implement, and maintain scalable and cost-effective data storage and processing solutions on AWS using S3, Glue, EMR, and Athena.\nLeverage AWS Lambda and Step Functions for serverless compute and task orchestration in data pipelines.\nWork with AWS databases like RDS and DynamoDB to ensure efficient data storage and retrieval.\nMonitor data processing and pipeline health using AWS CloudWatch and ensure smooth operation in production environments.\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.\nPerform performance tuning, optimize distributed data processing tasks, and handle scalability issues.\nProvide troubleshooting and support for data pipeline failures and ensure high availability and reliability.\nContribute to the setup and maintenance of CI/CD pipelines for automated deployment and testing of data workflows.\nRequired Skills & Experience:\nExperience: Minimum of 6+ years of hands-on experience in data engineering or big data development roles, with a focus on designing and building data pipelines and processing systems.\nTechnical Skills:\nStrong programming skills in Python with hands-on experience in Apache Spark (PySpark preferred).\nProficient in writing and optimizing complex SQL queries for data extraction, transformation, and loading.\nHands-on experience with Apache Airflow for orchestration of data workflows and pipeline management.\nIn-depth understanding and practical experience with AWS services:\nData Storage & Processing: S3, Glue, EMR, Athena\nCompute & Execution: Lambda, Step Functions\nDatabases: RDS, DynamoDB\nMonitoring: CloudWatch\nExperience with distributed data processing, parallel computing, and performance tuning techniques.\nStrong analytical and problem-solving skills to troubleshoot and optimize data workflows and pipelines.\nFamiliarity with CI/CD pipelines and DevOps practices for continuous integration and automated deployments is a plus.\nPreferred Qualifications:\nFamiliarity with other cloud platforms (Azure, Google Cloud) and services related to data engineering.\nExperience in handling unstructured and semi-structured data and working with data lakes.\nKnowledge of containerization technologies such as Docker or orchestration systems like Kubernetes.\nExperience with NoSQL databases or data warehouses like Redshift or BigQuery is a plus.\nQualifications:\nEducation: Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field.\nExperience: Minimum of 6+ years in a data engineering role with strong expertise in AWS and big data processing frameworks.","['Apache Spark (PySpark)', 'AWS (S3', 'Glue', 'Athena', 'Step Functions', 'Python', 'Sql', 'Apache Airflow', 'Emr']",2025-06-12 01:13:42
AWS data engineer,Virtusa,6-12 Years,,Hyderabad,Information Technology,"Develop and implement efficient data pipelines using Apache Spark (PySpark preferred) to process and analyze large-scale data.\nDesign, build, and optimize complex SQL queries to extract, transform, and load (ETL) data from multiple sources.\nOrchestrate data workflows using Apache Airflow, ensuring smooth execution and error-free pipelines.\nDesign, implement, and maintain scalable and cost-effective data storage and processing solutions on AWS using S3, Glue, EMR, and Athena.\nLeverage AWS Lambda and Step Functions for serverless compute and task orchestration in data pipelines.\nWork with AWS databases like RDS and DynamoDB to ensure efficient data storage and retrieval.\nMonitor data processing and pipeline health using AWS CloudWatch and ensure smooth operation in production environments.\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.\nPerform performance tuning, optimize distributed data processing tasks, and handle scalability issues.\nProvide troubleshooting and support for data pipeline failures and ensure high availability and reliability.\nContribute to the setup and maintenance of CI/CD pipelines for automated deployment and testing of data workflows.\nRequired Skills & Experience:\nExperience: Minimum of 6+ years of hands-on experience in data engineering or big data development roles, with a focus on designing and building data pipelines and processing systems.\nTechnical Skills:\nStrong programming skills in Python with hands-on experience in Apache Spark (PySpark preferred).\nProficient in writing and optimizing complex SQL queries for data extraction, transformation, and loading.\nHands-on experience with Apache Airflow for orchestration of data workflows and pipeline management.\nIn-depth understanding and practical experience with AWS services:\nData Storage & Processing: S3, Glue, EMR, Athena\nCompute & Execution: Lambda, Step Functions\nDatabases: RDS, DynamoDB\nMonitoring: CloudWatch\nExperience with distributed data processing, parallel computing, and performance tuning techniques.\nStrong analytical and problem-solving skills to troubleshoot and optimize data workflows and pipelines.\nFamiliarity with CI/CD pipelines and DevOps practices for continuous integration and automated deployments is a plus.\nPreferred Qualifications:\nFamiliarity with other cloud platforms (Azure, Google Cloud) and services related to data engineering.\nExperience in handling unstructured and semi-structured data and working with data lakes.\nKnowledge of containerization technologies such as Docker or orchestration systems like Kubernetes.\nExperience with NoSQL databases or data warehouses like Redshift or BigQuery is a plus.\nQualifications:\nEducation: Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field.\nExperience: Minimum of 6+ years in a data engineering role with strong expertise in AWS and big data processing frameworks.","['Apache Spark (PySpark)', 'AWS (S3', 'Glue', 'Athena', 'Step Functions', 'Python', 'Sql', 'Apache Airflow', 'Emr']",2025-06-12 01:13:44
AWS data engineer,Virtusa,6-12 Years,,Hyderabad,Information Technology,"Develop and implement efficient data pipelines using Apache Spark (PySpark preferred) to process and analyze large-scale data.\nDesign, build, and optimize complex SQL queries to extract, transform, and load (ETL) data from multiple sources.\nOrchestrate data workflows using Apache Airflow, ensuring smooth execution and error-free pipelines.\nDesign, implement, and maintain scalable and cost-effective data storage and processing solutions on AWS using S3, Glue, EMR, and Athena.\nLeverage AWS Lambda and Step Functions for serverless compute and task orchestration in data pipelines.\nWork with AWS databases like RDS and DynamoDB to ensure efficient data storage and retrieval.\nMonitor data processing and pipeline health using AWS CloudWatch and ensure smooth operation in production environments.\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.\nPerform performance tuning, optimize distributed data processing tasks, and handle scalability issues.\nProvide troubleshooting and support for data pipeline failures and ensure high availability and reliability.\nContribute to the setup and maintenance of CI/CD pipelines for automated deployment and testing of data workflows.\nRequired Skills & Experience:\nExperience: Minimum of 6+ years of hands-on experience in data engineering or big data development roles, with a focus on designing and building data pipelines and processing systems.\nTechnical Skills:\nStrong programming skills in Python with hands-on experience in Apache Spark (PySpark preferred).\nProficient in writing and optimizing complex SQL queries for data extraction, transformation, and loading.\nHands-on experience with Apache Airflow for orchestration of data workflows and pipeline management.\nIn-depth understanding and practical experience with AWS services:\nData Storage & Processing: S3, Glue, EMR, Athena\nCompute & Execution: Lambda, Step Functions\nDatabases: RDS, DynamoDB\nMonitoring: CloudWatch\nExperience with distributed data processing, parallel computing, and performance tuning techniques.\nStrong analytical and problem-solving skills to troubleshoot and optimize data workflows and pipelines.\nFamiliarity with CI/CD pipelines and DevOps practices for continuous integration and automated deployments is a plus.\nPreferred Qualifications:\nFamiliarity with other cloud platforms (Azure, Google Cloud) and services related to data engineering.\nExperience in handling unstructured and semi-structured data and working with data lakes.\nKnowledge of containerization technologies such as Docker or orchestration systems like Kubernetes.\nExperience with NoSQL databases or data warehouses like Redshift or BigQuery is a plus.\nQualifications:\nEducation: Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field.\nExperience: Minimum of 6+ years in a data engineering role with strong expertise in AWS and big data processing frameworks.","['Apache Spark (PySpark)', 'AWS (S3', 'Glue', 'Athena', 'Step Functions', 'Python', 'Sql', 'Apache Airflow', 'Emr']",2025-06-12 01:13:46
AWS data engineer,Virtusa,6-12 Years,,Hyderabad,Information Technology,"Develop and implement efficient data pipelines using Apache Spark (PySpark preferred) to process and analyze large-scale data.\nDesign, build, and optimize complex SQL queries to extract, transform, and load (ETL) data from multiple sources.\nOrchestrate data workflows using Apache Airflow, ensuring smooth execution and error-free pipelines.\nDesign, implement, and maintain scalable and cost-effective data storage and processing solutions on AWS using S3, Glue, EMR, and Athena.\nLeverage AWS Lambda and Step Functions for serverless compute and task orchestration in data pipelines.\nWork with AWS databases like RDS and DynamoDB to ensure efficient data storage and retrieval.\nMonitor data processing and pipeline health using AWS CloudWatch and ensure smooth operation in production environments.\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.\nPerform performance tuning, optimize distributed data processing tasks, and handle scalability issues.\nProvide troubleshooting and support for data pipeline failures and ensure high availability and reliability.\nContribute to the setup and maintenance of CI/CD pipelines for automated deployment and testing of data workflows.\nRequired Skills & Experience:\nExperience: Minimum of 6+ years of hands-on experience in data engineering or big data development roles, with a focus on designing and building data pipelines and processing systems.\nTechnical Skills:\nStrong programming skills in Python with hands-on experience in Apache Spark (PySpark preferred).\nProficient in writing and optimizing complex SQL queries for data extraction, transformation, and loading.\nHands-on experience with Apache Airflow for orchestration of data workflows and pipeline management.\nIn-depth understanding and practical experience with AWS services:\nData Storage & Processing: S3, Glue, EMR, Athena\nCompute & Execution: Lambda, Step Functions\nDatabases: RDS, DynamoDB\nMonitoring: CloudWatch\nExperience with distributed data processing, parallel computing, and performance tuning techniques.\nStrong analytical and problem-solving skills to troubleshoot and optimize data workflows and pipelines.\nFamiliarity with CI/CD pipelines and DevOps practices for continuous integration and automated deployments is a plus.\nPreferred Qualifications:\nFamiliarity with other cloud platforms (Azure, Google Cloud) and services related to data engineering.\nExperience in handling unstructured and semi-structured data and working with data lakes.\nKnowledge of containerization technologies such as Docker or orchestration systems like Kubernetes.\nExperience with NoSQL databases or data warehouses like Redshift or BigQuery is a plus.\nQualifications:\nEducation: Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field.\nExperience: Minimum of 6+ years in a data engineering role with strong expertise in AWS and big data processing frameworks.","['Apache Spark (PySpark)', 'AWS (S3', 'Glue', 'Athena', 'Step Functions', 'Python', 'Sql', 'Apache Airflow', 'Emr']",2025-06-12 01:13:47
AWS data engineer,Virtusa,6-12 Years,,Hyderabad,Information Technology,"Develop and implement efficient data pipelines using Apache Spark (PySpark preferred) to process and analyze large-scale data.\nDesign, build, and optimize complex SQL queries to extract, transform, and load (ETL) data from multiple sources.\nOrchestrate data workflows using Apache Airflow, ensuring smooth execution and error-free pipelines.\nDesign, implement, and maintain scalable and cost-effective data storage and processing solutions on AWS using S3, Glue, EMR, and Athena.\nLeverage AWS Lambda and Step Functions for serverless compute and task orchestration in data pipelines.\nWork with AWS databases like RDS and DynamoDB to ensure efficient data storage and retrieval.\nMonitor data processing and pipeline health using AWS CloudWatch and ensure smooth operation in production environments.\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.\nPerform performance tuning, optimize distributed data processing tasks, and handle scalability issues.\nProvide troubleshooting and support for data pipeline failures and ensure high availability and reliability.\nContribute to the setup and maintenance of CI/CD pipelines for automated deployment and testing of data workflows.\nRequired Skills & Experience:\nExperience: Minimum of 6+ years of hands-on experience in data engineering or big data development roles, with a focus on designing and building data pipelines and processing systems.\nTechnical Skills:\nStrong programming skills in Python with hands-on experience in Apache Spark (PySpark preferred).\nProficient in writing and optimizing complex SQL queries for data extraction, transformation, and loading.\nHands-on experience with Apache Airflow for orchestration of data workflows and pipeline management.\nIn-depth understanding and practical experience with AWS services:\nData Storage & Processing: S3, Glue, EMR, Athena\nCompute & Execution: Lambda, Step Functions\nDatabases: RDS, DynamoDB\nMonitoring: CloudWatch\nExperience with distributed data processing, parallel computing, and performance tuning techniques.\nStrong analytical and problem-solving skills to troubleshoot and optimize data workflows and pipelines.\nFamiliarity with CI/CD pipelines and DevOps practices for continuous integration and automated deployments is a plus.\nPreferred Qualifications:\nFamiliarity with other cloud platforms (Azure, Google Cloud) and services related to data engineering.\nExperience in handling unstructured and semi-structured data and working with data lakes.\nKnowledge of containerization technologies such as Docker or orchestration systems like Kubernetes.\nExperience with NoSQL databases or data warehouses like Redshift or BigQuery is a plus.\nQualifications:\nEducation: Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field.\nExperience: Minimum of 6+ years in a data engineering role with strong expertise in AWS and big data processing frameworks.","['Apache Spark (PySpark)', 'AWS (S3', 'Glue', 'Athena', 'Step Functions', 'Python', 'Sql', 'Apache Airflow', 'Emr']",2025-06-12 01:13:49
AWS data engineer,Virtusa,6-12 Years,,Hyderabad,Information Technology,"Develop and implement efficient data pipelines using Apache Spark (PySpark preferred) to process and analyze large-scale data.\nDesign, build, and optimize complex SQL queries to extract, transform, and load (ETL) data from multiple sources.\nOrchestrate data workflows using Apache Airflow, ensuring smooth execution and error-free pipelines.\nDesign, implement, and maintain scalable and cost-effective data storage and processing solutions on AWS using S3, Glue, EMR, and Athena.\nLeverage AWS Lambda and Step Functions for serverless compute and task orchestration in data pipelines.\nWork with AWS databases like RDS and DynamoDB to ensure efficient data storage and retrieval.\nMonitor data processing and pipeline health using AWS CloudWatch and ensure smooth operation in production environments.\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.\nPerform performance tuning, optimize distributed data processing tasks, and handle scalability issues.\nProvide troubleshooting and support for data pipeline failures and ensure high availability and reliability.\nContribute to the setup and maintenance of CI/CD pipelines for automated deployment and testing of data workflows.\nRequired Skills & Experience:\nExperience: Minimum of 6+ years of hands-on experience in data engineering or big data development roles, with a focus on designing and building data pipelines and processing systems.\nTechnical Skills:\nStrong programming skills in Python with hands-on experience in Apache Spark (PySpark preferred).\nProficient in writing and optimizing complex SQL queries for data extraction, transformation, and loading.\nHands-on experience with Apache Airflow for orchestration of data workflows and pipeline management.\nIn-depth understanding and practical experience with AWS services:\nData Storage & Processing: S3, Glue, EMR, Athena\nCompute & Execution: Lambda, Step Functions\nDatabases: RDS, DynamoDB\nMonitoring: CloudWatch\nExperience with distributed data processing, parallel computing, and performance tuning techniques.\nStrong analytical and problem-solving skills to troubleshoot and optimize data workflows and pipelines.\nFamiliarity with CI/CD pipelines and DevOps practices for continuous integration and automated deployments is a plus.\nPreferred Qualifications:\nFamiliarity with other cloud platforms (Azure, Google Cloud) and services related to data engineering.\nExperience in handling unstructured and semi-structured data and working with data lakes.\nKnowledge of containerization technologies such as Docker or orchestration systems like Kubernetes.\nExperience with NoSQL databases or data warehouses like Redshift or BigQuery is a plus.\nQualifications:\nEducation: Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field.\nExperience: Minimum of 6+ years in a data engineering role with strong expertise in AWS and big data processing frameworks.","['Apache Spark (PySpark)', 'AWS (S3', 'Glue', 'Athena', 'Step Functions', 'Python', 'Sql', 'Apache Airflow', 'Emr']",2025-06-12 01:13:51
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,Information Technology,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.","['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 01:14:01
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,Information Technology,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.","['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 01:14:05
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,Information Technology,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.","['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 01:14:07
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,Information Technology,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.","['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 01:14:09
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,Information Technology,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.","['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 01:14:10
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,Information Technology,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.","['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 01:14:12
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,Information Technology,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.","['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 01:14:13
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,Information Technology,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.","['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 01:14:14
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,Information Technology,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.","['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 01:14:16
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,Information Technology,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.","['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 01:14:18
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,Information Technology,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.","['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 01:14:19
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,Information Technology,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.","['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 01:14:21
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,Information Technology,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.","['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 01:14:23
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,Information Technology,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.","['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 01:14:24
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,Information Technology,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.","['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 01:14:26
Technical Specialist – Data Engineer,Citiustech Healthcare Technology Private Limited,10-11 Years,,"Gurugram, Hyderabad, Chennai",Login to check your skill match score,"As a Technical Specialist Data Engineer, you will be a part of an Agile team to build healthcare applications and implement new features while adhering to the best coding development standards.\nResponsibilities: -\nDesign logical and physical data models using specified tools and methods.\nDefine entities and source system mappings.\nCollaborate with business teams to create source-to-target mappings.\nWork on HEDIS measures and Clinical EPIC data to analyze and interpret healthcare performance metrics.\nDevelop and optimize SQL queries for efficient data extraction, transformation, and analysis.\nDesign and implement data models to support healthcare analytics, reporting, and downstream applications.\nLeverage Databricks for scalable data processing, data pipelines, and analytics workflows.\nApply critical thinking and problem-solving skills to address complex data challenges and derive meaningful insights.\nDevelop Power BI dashboards and reports for effective healthcare data visualization and decision-making.\nCollaborate with clinical analysts, data engineers, and business stakeholders to understand requirements and deliver data-driven solutions.\nEnsure data quality, integrity, and compliance with HIPAA, NCQA, and other healthcare regulations and best practices.\nAutomate data validation and reconciliation processes to improve data consistency and accuracy.\nSupport performance tuning and optimization of SQL queries and data pipelines for efficient processing.\nEducational Qualifications: -\nEngineering Degree BE / BTech / BCS\nTechnical certification in multiple technologies\nCertification in Azure, Databricks, or healthcare analytics-related domains\nSkills: -\nMandatory Technical Skills: -\nExperience in developing and managing data pipelines in Databricks using PySpark/Spark SQL\nStrong data modeling skills to design efficient and scalable healthcare data structures\nPower BI for data visualization, dashboard creation, and report generation\nStrong understanding of healthcare data standards, including HL7, FHIR, and CCDA\nExperience in ETL processes and performance tuning for large datasets\nFamiliarity with Azure Data Services (ADLS, ADF, Synapse) for Cloud-based data management\nProficiency in data modeling tools such as Erwin\nStrong understanding of data management concepts\nKnowledge of the provider domain\nFamiliarity with data integration, and ETL tools and techniques\nExcellent problem-solving and analytical skills\nStrong communication and collaboration abilities\nKnowledge of healthcare and pharmacy data\nExtensive experience in data modeling and data profiling, with a strong understanding of data management concepts.\nHands-on experience with data modelling tools such as Erwin is highly desirable.\nMust have knowledge in healthcare. A good knowledge of the Provider workflows and Pharmacy data is essential.\nFamiliarity with data integration and ETL concepts, techniques, and tools.\nGood to Have Skills: -\nExperience in Python and PySpark for data transformation and analysis\nKnowledge of Confluent Kafka or event-driven architecture for real-time data processing\nHands-on experience in NoSQL databases like MongoDB or Snowflake for scalable data storage\nUnderstanding of data governance frameworks and compliance in healthcare analytics\nExposure to CI/CD pipelines and DevOps practices in Cloud environments\nFamiliarity with Machine Learning concepts for predictive healthcare analytics","['Spark', 'Python', 'Sql', 'Etl']",2025-06-12 01:14:35
Technical Specialist – Data Engineer,Citiustech Healthcare Technology Private Limited,10-11 Years,,"Gurugram, Hyderabad, Chennai",Login to check your skill match score,"As a Technical Specialist Data Engineer, you will be a part of an Agile team to build healthcare applications and implement new features while adhering to the best coding development standards.\nResponsibilities: -\nDesign logical and physical data models using specified tools and methods.\nDefine entities and source system mappings.\nCollaborate with business teams to create source-to-target mappings.\nWork on HEDIS measures and Clinical EPIC data to analyze and interpret healthcare performance metrics.\nDevelop and optimize SQL queries for efficient data extraction, transformation, and analysis.\nDesign and implement data models to support healthcare analytics, reporting, and downstream applications.\nLeverage Databricks for scalable data processing, data pipelines, and analytics workflows.\nApply critical thinking and problem-solving skills to address complex data challenges and derive meaningful insights.\nDevelop Power BI dashboards and reports for effective healthcare data visualization and decision-making.\nCollaborate with clinical analysts, data engineers, and business stakeholders to understand requirements and deliver data-driven solutions.\nEnsure data quality, integrity, and compliance with HIPAA, NCQA, and other healthcare regulations and best practices.\nAutomate data validation and reconciliation processes to improve data consistency and accuracy.\nSupport performance tuning and optimization of SQL queries and data pipelines for efficient processing.\nEducational Qualifications: -\nEngineering Degree BE / BTech / BCS\nTechnical certification in multiple technologies\nCertification in Azure, Databricks, or healthcare analytics-related domains\nSkills: -\nMandatory Technical Skills: -\nExperience in developing and managing data pipelines in Databricks using PySpark/Spark SQL\nStrong data modeling skills to design efficient and scalable healthcare data structures\nPower BI for data visualization, dashboard creation, and report generation\nStrong understanding of healthcare data standards, including HL7, FHIR, and CCDA\nExperience in ETL processes and performance tuning for large datasets\nFamiliarity with Azure Data Services (ADLS, ADF, Synapse) for Cloud-based data management\nProficiency in data modeling tools such as Erwin\nStrong understanding of data management concepts\nKnowledge of the provider domain\nFamiliarity with data integration, and ETL tools and techniques\nExcellent problem-solving and analytical skills\nStrong communication and collaboration abilities\nKnowledge of healthcare and pharmacy data\nExtensive experience in data modeling and data profiling, with a strong understanding of data management concepts.\nHands-on experience with data modelling tools such as Erwin is highly desirable.\nMust have knowledge in healthcare. A good knowledge of the Provider workflows and Pharmacy data is essential.\nFamiliarity with data integration and ETL concepts, techniques, and tools.\nGood to Have Skills: -\nExperience in Python and PySpark for data transformation and analysis\nKnowledge of Confluent Kafka or event-driven architecture for real-time data processing\nHands-on experience in NoSQL databases like MongoDB or Snowflake for scalable data storage\nUnderstanding of data governance frameworks and compliance in healthcare analytics\nExposure to CI/CD pipelines and DevOps practices in Cloud environments\nFamiliarity with Machine Learning concepts for predictive healthcare analytics","['Spark', 'Python', 'Sql', 'Etl']",2025-06-12 01:14:39
Technical Specialist – Data Engineer,Citiustech Healthcare Technology Private Limited,10-11 Years,,"Gurugram, Hyderabad, Chennai",Login to check your skill match score,"As a Technical Specialist Data Engineer, you will be a part of an Agile team to build healthcare applications and implement new features while adhering to the best coding development standards.\nResponsibilities: -\nDesign logical and physical data models using specified tools and methods.\nDefine entities and source system mappings.\nCollaborate with business teams to create source-to-target mappings.\nWork on HEDIS measures and Clinical EPIC data to analyze and interpret healthcare performance metrics.\nDevelop and optimize SQL queries for efficient data extraction, transformation, and analysis.\nDesign and implement data models to support healthcare analytics, reporting, and downstream applications.\nLeverage Databricks for scalable data processing, data pipelines, and analytics workflows.\nApply critical thinking and problem-solving skills to address complex data challenges and derive meaningful insights.\nDevelop Power BI dashboards and reports for effective healthcare data visualization and decision-making.\nCollaborate with clinical analysts, data engineers, and business stakeholders to understand requirements and deliver data-driven solutions.\nEnsure data quality, integrity, and compliance with HIPAA, NCQA, and other healthcare regulations and best practices.\nAutomate data validation and reconciliation processes to improve data consistency and accuracy.\nSupport performance tuning and optimization of SQL queries and data pipelines for efficient processing.\nEducational Qualifications: -\nEngineering Degree BE / BTech / BCS\nTechnical certification in multiple technologies\nCertification in Azure, Databricks, or healthcare analytics-related domains\nSkills: -\nMandatory Technical Skills: -\nExperience in developing and managing data pipelines in Databricks using PySpark/Spark SQL\nStrong data modeling skills to design efficient and scalable healthcare data structures\nPower BI for data visualization, dashboard creation, and report generation\nStrong understanding of healthcare data standards, including HL7, FHIR, and CCDA\nExperience in ETL processes and performance tuning for large datasets\nFamiliarity with Azure Data Services (ADLS, ADF, Synapse) for Cloud-based data management\nProficiency in data modeling tools such as Erwin\nStrong understanding of data management concepts\nKnowledge of the provider domain\nFamiliarity with data integration, and ETL tools and techniques\nExcellent problem-solving and analytical skills\nStrong communication and collaboration abilities\nKnowledge of healthcare and pharmacy data\nExtensive experience in data modeling and data profiling, with a strong understanding of data management concepts.\nHands-on experience with data modelling tools such as Erwin is highly desirable.\nMust have knowledge in healthcare. A good knowledge of the Provider workflows and Pharmacy data is essential.\nFamiliarity with data integration and ETL concepts, techniques, and tools.\nGood to Have Skills: -\nExperience in Python and PySpark for data transformation and analysis\nKnowledge of Confluent Kafka or event-driven architecture for real-time data processing\nHands-on experience in NoSQL databases like MongoDB or Snowflake for scalable data storage\nUnderstanding of data governance frameworks and compliance in healthcare analytics\nExposure to CI/CD pipelines and DevOps practices in Cloud environments\nFamiliarity with Machine Learning concepts for predictive healthcare analytics","['Spark', 'Python', 'Sql', 'Etl']",2025-06-12 01:14:41
Technical Specialist – Data Engineer,Citiustech Healthcare Technology Private Limited,10-11 Years,,"Gurugram, Hyderabad, Chennai",Login to check your skill match score,"As a Technical Specialist Data Engineer, you will be a part of an Agile team to build healthcare applications and implement new features while adhering to the best coding development standards.\nResponsibilities: -\nDesign logical and physical data models using specified tools and methods.\nDefine entities and source system mappings.\nCollaborate with business teams to create source-to-target mappings.\nWork on HEDIS measures and Clinical EPIC data to analyze and interpret healthcare performance metrics.\nDevelop and optimize SQL queries for efficient data extraction, transformation, and analysis.\nDesign and implement data models to support healthcare analytics, reporting, and downstream applications.\nLeverage Databricks for scalable data processing, data pipelines, and analytics workflows.\nApply critical thinking and problem-solving skills to address complex data challenges and derive meaningful insights.\nDevelop Power BI dashboards and reports for effective healthcare data visualization and decision-making.\nCollaborate with clinical analysts, data engineers, and business stakeholders to understand requirements and deliver data-driven solutions.\nEnsure data quality, integrity, and compliance with HIPAA, NCQA, and other healthcare regulations and best practices.\nAutomate data validation and reconciliation processes to improve data consistency and accuracy.\nSupport performance tuning and optimization of SQL queries and data pipelines for efficient processing.\nEducational Qualifications: -\nEngineering Degree BE / BTech / BCS\nTechnical certification in multiple technologies\nCertification in Azure, Databricks, or healthcare analytics-related domains\nSkills: -\nMandatory Technical Skills: -\nExperience in developing and managing data pipelines in Databricks using PySpark/Spark SQL\nStrong data modeling skills to design efficient and scalable healthcare data structures\nPower BI for data visualization, dashboard creation, and report generation\nStrong understanding of healthcare data standards, including HL7, FHIR, and CCDA\nExperience in ETL processes and performance tuning for large datasets\nFamiliarity with Azure Data Services (ADLS, ADF, Synapse) for Cloud-based data management\nProficiency in data modeling tools such as Erwin\nStrong understanding of data management concepts\nKnowledge of the provider domain\nFamiliarity with data integration, and ETL tools and techniques\nExcellent problem-solving and analytical skills\nStrong communication and collaboration abilities\nKnowledge of healthcare and pharmacy data\nExtensive experience in data modeling and data profiling, with a strong understanding of data management concepts.\nHands-on experience with data modelling tools such as Erwin is highly desirable.\nMust have knowledge in healthcare. A good knowledge of the Provider workflows and Pharmacy data is essential.\nFamiliarity with data integration and ETL concepts, techniques, and tools.\nGood to Have Skills: -\nExperience in Python and PySpark for data transformation and analysis\nKnowledge of Confluent Kafka or event-driven architecture for real-time data processing\nHands-on experience in NoSQL databases like MongoDB or Snowflake for scalable data storage\nUnderstanding of data governance frameworks and compliance in healthcare analytics\nExposure to CI/CD pipelines and DevOps practices in Cloud environments\nFamiliarity with Machine Learning concepts for predictive healthcare analytics","['Spark', 'Python', 'Sql', 'Etl']",2025-06-12 01:14:43
Technical Specialist – Data Engineer,Citiustech Healthcare Technology Private Limited,10-11 Years,,"Gurugram, Hyderabad, Chennai",Login to check your skill match score,"As a Technical Specialist Data Engineer, you will be a part of an Agile team to build healthcare applications and implement new features while adhering to the best coding development standards.\nResponsibilities: -\nDesign logical and physical data models using specified tools and methods.\nDefine entities and source system mappings.\nCollaborate with business teams to create source-to-target mappings.\nWork on HEDIS measures and Clinical EPIC data to analyze and interpret healthcare performance metrics.\nDevelop and optimize SQL queries for efficient data extraction, transformation, and analysis.\nDesign and implement data models to support healthcare analytics, reporting, and downstream applications.\nLeverage Databricks for scalable data processing, data pipelines, and analytics workflows.\nApply critical thinking and problem-solving skills to address complex data challenges and derive meaningful insights.\nDevelop Power BI dashboards and reports for effective healthcare data visualization and decision-making.\nCollaborate with clinical analysts, data engineers, and business stakeholders to understand requirements and deliver data-driven solutions.\nEnsure data quality, integrity, and compliance with HIPAA, NCQA, and other healthcare regulations and best practices.\nAutomate data validation and reconciliation processes to improve data consistency and accuracy.\nSupport performance tuning and optimization of SQL queries and data pipelines for efficient processing.\nEducational Qualifications: -\nEngineering Degree BE / BTech / BCS\nTechnical certification in multiple technologies\nCertification in Azure, Databricks, or healthcare analytics-related domains\nSkills: -\nMandatory Technical Skills: -\nExperience in developing and managing data pipelines in Databricks using PySpark/Spark SQL\nStrong data modeling skills to design efficient and scalable healthcare data structures\nPower BI for data visualization, dashboard creation, and report generation\nStrong understanding of healthcare data standards, including HL7, FHIR, and CCDA\nExperience in ETL processes and performance tuning for large datasets\nFamiliarity with Azure Data Services (ADLS, ADF, Synapse) for Cloud-based data management\nProficiency in data modeling tools such as Erwin\nStrong understanding of data management concepts\nKnowledge of the provider domain\nFamiliarity with data integration, and ETL tools and techniques\nExcellent problem-solving and analytical skills\nStrong communication and collaboration abilities\nKnowledge of healthcare and pharmacy data\nExtensive experience in data modeling and data profiling, with a strong understanding of data management concepts.\nHands-on experience with data modelling tools such as Erwin is highly desirable.\nMust have knowledge in healthcare. A good knowledge of the Provider workflows and Pharmacy data is essential.\nFamiliarity with data integration and ETL concepts, techniques, and tools.\nGood to Have Skills: -\nExperience in Python and PySpark for data transformation and analysis\nKnowledge of Confluent Kafka or event-driven architecture for real-time data processing\nHands-on experience in NoSQL databases like MongoDB or Snowflake for scalable data storage\nUnderstanding of data governance frameworks and compliance in healthcare analytics\nExposure to CI/CD pipelines and DevOps practices in Cloud environments\nFamiliarity with Machine Learning concepts for predictive healthcare analytics","['Spark', 'Python', 'Sql', 'Etl']",2025-06-12 01:14:44
Technical Specialist – Data Engineer,Citiustech Healthcare Technology Private Limited,10-11 Years,,"Gurugram, Hyderabad, Chennai",Login to check your skill match score,"As a Technical Specialist Data Engineer, you will be a part of an Agile team to build healthcare applications and implement new features while adhering to the best coding development standards.\nResponsibilities: -\nDesign logical and physical data models using specified tools and methods.\nDefine entities and source system mappings.\nCollaborate with business teams to create source-to-target mappings.\nWork on HEDIS measures and Clinical EPIC data to analyze and interpret healthcare performance metrics.\nDevelop and optimize SQL queries for efficient data extraction, transformation, and analysis.\nDesign and implement data models to support healthcare analytics, reporting, and downstream applications.\nLeverage Databricks for scalable data processing, data pipelines, and analytics workflows.\nApply critical thinking and problem-solving skills to address complex data challenges and derive meaningful insights.\nDevelop Power BI dashboards and reports for effective healthcare data visualization and decision-making.\nCollaborate with clinical analysts, data engineers, and business stakeholders to understand requirements and deliver data-driven solutions.\nEnsure data quality, integrity, and compliance with HIPAA, NCQA, and other healthcare regulations and best practices.\nAutomate data validation and reconciliation processes to improve data consistency and accuracy.\nSupport performance tuning and optimization of SQL queries and data pipelines for efficient processing.\nEducational Qualifications: -\nEngineering Degree BE / BTech / BCS\nTechnical certification in multiple technologies\nCertification in Azure, Databricks, or healthcare analytics-related domains\nSkills: -\nMandatory Technical Skills: -\nExperience in developing and managing data pipelines in Databricks using PySpark/Spark SQL\nStrong data modeling skills to design efficient and scalable healthcare data structures\nPower BI for data visualization, dashboard creation, and report generation\nStrong understanding of healthcare data standards, including HL7, FHIR, and CCDA\nExperience in ETL processes and performance tuning for large datasets\nFamiliarity with Azure Data Services (ADLS, ADF, Synapse) for Cloud-based data management\nProficiency in data modeling tools such as Erwin\nStrong understanding of data management concepts\nKnowledge of the provider domain\nFamiliarity with data integration, and ETL tools and techniques\nExcellent problem-solving and analytical skills\nStrong communication and collaboration abilities\nKnowledge of healthcare and pharmacy data\nExtensive experience in data modeling and data profiling, with a strong understanding of data management concepts.\nHands-on experience with data modelling tools such as Erwin is highly desirable.\nMust have knowledge in healthcare. A good knowledge of the Provider workflows and Pharmacy data is essential.\nFamiliarity with data integration and ETL concepts, techniques, and tools.\nGood to Have Skills: -\nExperience in Python and PySpark for data transformation and analysis\nKnowledge of Confluent Kafka or event-driven architecture for real-time data processing\nHands-on experience in NoSQL databases like MongoDB or Snowflake for scalable data storage\nUnderstanding of data governance frameworks and compliance in healthcare analytics\nExposure to CI/CD pipelines and DevOps practices in Cloud environments\nFamiliarity with Machine Learning concepts for predictive healthcare analytics","['Spark', 'Python', 'Sql', 'Etl']",2025-06-12 01:14:45
Technical Specialist – Data Engineer,Citiustech Healthcare Technology Private Limited,10-11 Years,,"Gurugram, Hyderabad, Chennai",Login to check your skill match score,"As a Technical Specialist Data Engineer, you will be a part of an Agile team to build healthcare applications and implement new features while adhering to the best coding development standards.\nResponsibilities: -\nDesign logical and physical data models using specified tools and methods.\nDefine entities and source system mappings.\nCollaborate with business teams to create source-to-target mappings.\nWork on HEDIS measures and Clinical EPIC data to analyze and interpret healthcare performance metrics.\nDevelop and optimize SQL queries for efficient data extraction, transformation, and analysis.\nDesign and implement data models to support healthcare analytics, reporting, and downstream applications.\nLeverage Databricks for scalable data processing, data pipelines, and analytics workflows.\nApply critical thinking and problem-solving skills to address complex data challenges and derive meaningful insights.\nDevelop Power BI dashboards and reports for effective healthcare data visualization and decision-making.\nCollaborate with clinical analysts, data engineers, and business stakeholders to understand requirements and deliver data-driven solutions.\nEnsure data quality, integrity, and compliance with HIPAA, NCQA, and other healthcare regulations and best practices.\nAutomate data validation and reconciliation processes to improve data consistency and accuracy.\nSupport performance tuning and optimization of SQL queries and data pipelines for efficient processing.\nEducational Qualifications: -\nEngineering Degree BE / BTech / BCS\nTechnical certification in multiple technologies\nCertification in Azure, Databricks, or healthcare analytics-related domains\nSkills: -\nMandatory Technical Skills: -\nExperience in developing and managing data pipelines in Databricks using PySpark/Spark SQL\nStrong data modeling skills to design efficient and scalable healthcare data structures\nPower BI for data visualization, dashboard creation, and report generation\nStrong understanding of healthcare data standards, including HL7, FHIR, and CCDA\nExperience in ETL processes and performance tuning for large datasets\nFamiliarity with Azure Data Services (ADLS, ADF, Synapse) for Cloud-based data management\nProficiency in data modeling tools such as Erwin\nStrong understanding of data management concepts\nKnowledge of the provider domain\nFamiliarity with data integration, and ETL tools and techniques\nExcellent problem-solving and analytical skills\nStrong communication and collaboration abilities\nKnowledge of healthcare and pharmacy data\nExtensive experience in data modeling and data profiling, with a strong understanding of data management concepts.\nHands-on experience with data modelling tools such as Erwin is highly desirable.\nMust have knowledge in healthcare. A good knowledge of the Provider workflows and Pharmacy data is essential.\nFamiliarity with data integration and ETL concepts, techniques, and tools.\nGood to Have Skills: -\nExperience in Python and PySpark for data transformation and analysis\nKnowledge of Confluent Kafka or event-driven architecture for real-time data processing\nHands-on experience in NoSQL databases like MongoDB or Snowflake for scalable data storage\nUnderstanding of data governance frameworks and compliance in healthcare analytics\nExposure to CI/CD pipelines and DevOps practices in Cloud environments\nFamiliarity with Machine Learning concepts for predictive healthcare analytics","['Spark', 'Python', 'Sql', 'Etl']",2025-06-12 01:14:47
Technical Specialist – Data Engineer,Citiustech Healthcare Technology Private Limited,10-11 Years,,"Gurugram, Hyderabad, Chennai",Login to check your skill match score,"As a Technical Specialist Data Engineer, you will be a part of an Agile team to build healthcare applications and implement new features while adhering to the best coding development standards.\nResponsibilities: -\nDesign logical and physical data models using specified tools and methods.\nDefine entities and source system mappings.\nCollaborate with business teams to create source-to-target mappings.\nWork on HEDIS measures and Clinical EPIC data to analyze and interpret healthcare performance metrics.\nDevelop and optimize SQL queries for efficient data extraction, transformation, and analysis.\nDesign and implement data models to support healthcare analytics, reporting, and downstream applications.\nLeverage Databricks for scalable data processing, data pipelines, and analytics workflows.\nApply critical thinking and problem-solving skills to address complex data challenges and derive meaningful insights.\nDevelop Power BI dashboards and reports for effective healthcare data visualization and decision-making.\nCollaborate with clinical analysts, data engineers, and business stakeholders to understand requirements and deliver data-driven solutions.\nEnsure data quality, integrity, and compliance with HIPAA, NCQA, and other healthcare regulations and best practices.\nAutomate data validation and reconciliation processes to improve data consistency and accuracy.\nSupport performance tuning and optimization of SQL queries and data pipelines for efficient processing.\nEducational Qualifications: -\nEngineering Degree BE / BTech / BCS\nTechnical certification in multiple technologies\nCertification in Azure, Databricks, or healthcare analytics-related domains\nSkills: -\nMandatory Technical Skills: -\nExperience in developing and managing data pipelines in Databricks using PySpark/Spark SQL\nStrong data modeling skills to design efficient and scalable healthcare data structures\nPower BI for data visualization, dashboard creation, and report generation\nStrong understanding of healthcare data standards, including HL7, FHIR, and CCDA\nExperience in ETL processes and performance tuning for large datasets\nFamiliarity with Azure Data Services (ADLS, ADF, Synapse) for Cloud-based data management\nProficiency in data modeling tools such as Erwin\nStrong understanding of data management concepts\nKnowledge of the provider domain\nFamiliarity with data integration, and ETL tools and techniques\nExcellent problem-solving and analytical skills\nStrong communication and collaboration abilities\nKnowledge of healthcare and pharmacy data\nExtensive experience in data modeling and data profiling, with a strong understanding of data management concepts.\nHands-on experience with data modelling tools such as Erwin is highly desirable.\nMust have knowledge in healthcare. A good knowledge of the Provider workflows and Pharmacy data is essential.\nFamiliarity with data integration and ETL concepts, techniques, and tools.\nGood to Have Skills: -\nExperience in Python and PySpark for data transformation and analysis\nKnowledge of Confluent Kafka or event-driven architecture for real-time data processing\nHands-on experience in NoSQL databases like MongoDB or Snowflake for scalable data storage\nUnderstanding of data governance frameworks and compliance in healthcare analytics\nExposure to CI/CD pipelines and DevOps practices in Cloud environments\nFamiliarity with Machine Learning concepts for predictive healthcare analytics","['Spark', 'Python', 'Sql', 'Etl']",2025-06-12 01:14:49
Technical Specialist – Data Engineer,Citiustech Healthcare Technology Private Limited,10-11 Years,,"Gurugram, Hyderabad, Chennai",Login to check your skill match score,"As a Technical Specialist Data Engineer, you will be a part of an Agile team to build healthcare applications and implement new features while adhering to the best coding development standards.\nResponsibilities: -\nDesign logical and physical data models using specified tools and methods.\nDefine entities and source system mappings.\nCollaborate with business teams to create source-to-target mappings.\nWork on HEDIS measures and Clinical EPIC data to analyze and interpret healthcare performance metrics.\nDevelop and optimize SQL queries for efficient data extraction, transformation, and analysis.\nDesign and implement data models to support healthcare analytics, reporting, and downstream applications.\nLeverage Databricks for scalable data processing, data pipelines, and analytics workflows.\nApply critical thinking and problem-solving skills to address complex data challenges and derive meaningful insights.\nDevelop Power BI dashboards and reports for effective healthcare data visualization and decision-making.\nCollaborate with clinical analysts, data engineers, and business stakeholders to understand requirements and deliver data-driven solutions.\nEnsure data quality, integrity, and compliance with HIPAA, NCQA, and other healthcare regulations and best practices.\nAutomate data validation and reconciliation processes to improve data consistency and accuracy.\nSupport performance tuning and optimization of SQL queries and data pipelines for efficient processing.\nEducational Qualifications: -\nEngineering Degree BE / BTech / BCS\nTechnical certification in multiple technologies\nCertification in Azure, Databricks, or healthcare analytics-related domains\nSkills: -\nMandatory Technical Skills: -\nExperience in developing and managing data pipelines in Databricks using PySpark/Spark SQL\nStrong data modeling skills to design efficient and scalable healthcare data structures\nPower BI for data visualization, dashboard creation, and report generation\nStrong understanding of healthcare data standards, including HL7, FHIR, and CCDA\nExperience in ETL processes and performance tuning for large datasets\nFamiliarity with Azure Data Services (ADLS, ADF, Synapse) for Cloud-based data management\nProficiency in data modeling tools such as Erwin\nStrong understanding of data management concepts\nKnowledge of the provider domain\nFamiliarity with data integration, and ETL tools and techniques\nExcellent problem-solving and analytical skills\nStrong communication and collaboration abilities\nKnowledge of healthcare and pharmacy data\nExtensive experience in data modeling and data profiling, with a strong understanding of data management concepts.\nHands-on experience with data modelling tools such as Erwin is highly desirable.\nMust have knowledge in healthcare. A good knowledge of the Provider workflows and Pharmacy data is essential.\nFamiliarity with data integration and ETL concepts, techniques, and tools.\nGood to Have Skills: -\nExperience in Python and PySpark for data transformation and analysis\nKnowledge of Confluent Kafka or event-driven architecture for real-time data processing\nHands-on experience in NoSQL databases like MongoDB or Snowflake for scalable data storage\nUnderstanding of data governance frameworks and compliance in healthcare analytics\nExposure to CI/CD pipelines and DevOps practices in Cloud environments\nFamiliarity with Machine Learning concepts for predictive healthcare analytics","['Spark', 'Python', 'Sql', 'Etl']",2025-06-12 01:14:50
Technical Specialist – Data Engineer,Citiustech Healthcare Technology Private Limited,10-11 Years,,"Gurugram, Hyderabad, Chennai",Login to check your skill match score,"As a Technical Specialist Data Engineer, you will be a part of an Agile team to build healthcare applications and implement new features while adhering to the best coding development standards.\nResponsibilities: -\nDesign logical and physical data models using specified tools and methods.\nDefine entities and source system mappings.\nCollaborate with business teams to create source-to-target mappings.\nWork on HEDIS measures and Clinical EPIC data to analyze and interpret healthcare performance metrics.\nDevelop and optimize SQL queries for efficient data extraction, transformation, and analysis.\nDesign and implement data models to support healthcare analytics, reporting, and downstream applications.\nLeverage Databricks for scalable data processing, data pipelines, and analytics workflows.\nApply critical thinking and problem-solving skills to address complex data challenges and derive meaningful insights.\nDevelop Power BI dashboards and reports for effective healthcare data visualization and decision-making.\nCollaborate with clinical analysts, data engineers, and business stakeholders to understand requirements and deliver data-driven solutions.\nEnsure data quality, integrity, and compliance with HIPAA, NCQA, and other healthcare regulations and best practices.\nAutomate data validation and reconciliation processes to improve data consistency and accuracy.\nSupport performance tuning and optimization of SQL queries and data pipelines for efficient processing.\nEducational Qualifications: -\nEngineering Degree BE / BTech / BCS\nTechnical certification in multiple technologies\nCertification in Azure, Databricks, or healthcare analytics-related domains\nSkills: -\nMandatory Technical Skills: -\nExperience in developing and managing data pipelines in Databricks using PySpark/Spark SQL\nStrong data modeling skills to design efficient and scalable healthcare data structures\nPower BI for data visualization, dashboard creation, and report generation\nStrong understanding of healthcare data standards, including HL7, FHIR, and CCDA\nExperience in ETL processes and performance tuning for large datasets\nFamiliarity with Azure Data Services (ADLS, ADF, Synapse) for Cloud-based data management\nProficiency in data modeling tools such as Erwin\nStrong understanding of data management concepts\nKnowledge of the provider domain\nFamiliarity with data integration, and ETL tools and techniques\nExcellent problem-solving and analytical skills\nStrong communication and collaboration abilities\nKnowledge of healthcare and pharmacy data\nExtensive experience in data modeling and data profiling, with a strong understanding of data management concepts.\nHands-on experience with data modelling tools such as Erwin is highly desirable.\nMust have knowledge in healthcare. A good knowledge of the Provider workflows and Pharmacy data is essential.\nFamiliarity with data integration and ETL concepts, techniques, and tools.\nGood to Have Skills: -\nExperience in Python and PySpark for data transformation and analysis\nKnowledge of Confluent Kafka or event-driven architecture for real-time data processing\nHands-on experience in NoSQL databases like MongoDB or Snowflake for scalable data storage\nUnderstanding of data governance frameworks and compliance in healthcare analytics\nExposure to CI/CD pipelines and DevOps practices in Cloud environments\nFamiliarity with Machine Learning concepts for predictive healthcare analytics","['Spark', 'Python', 'Sql', 'Etl']",2025-06-12 01:14:52
