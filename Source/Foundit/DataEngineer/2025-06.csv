job_title,company_name,experience,salary,location,industry,job_description,skills,scraped_at
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 06:45:11
Data Engineer,Virtusa,5-10 Years,,Chennai,Information Technology,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.","['AWS data services', 'dbt', 'snowflake', 'AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql', 'Python', 'Pyspark', 'Sql']",2025-06-12 06:45:17
Data Engineer,Virtusa,5-10 Years,,Chennai,Information Technology,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.","['AWS data services', 'dbt', 'snowflake', 'AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql', 'Python', 'Pyspark', 'Sql']",2025-06-12 06:45:18
Data Engineer,Virtusa,2-4 Years,,Hyderabad,Information Technology,"We are seeking a skilled Data Engineer to join our dynamic team in India. The ideal candidate will have experience in building and managing data pipelines, ensuring data quality, and optimizing our data infrastructure to support analytics and business intelligence initiatives.\nResponsibilities\nDesign, construct, install, and maintain large-scale processing systems and data pipelines.\nDevelop and optimize ETL processes to ensure smooth data integration and transformation.\nCollaborate with data scientists and analysts to understand data requirements and deliver solutions that meet their needs.\nImplement data quality checks and ensure data integrity throughout the data lifecycle.\nMonitor performance and troubleshoot data-related issues, enhancing system efficiency and reliability.\nSkills and Qualifications\nBachelor's degree in Computer Science, Engineering, or a related field.\n2-4 years of experience in data engineering or a similar role.\nProficiency in programming languages such as Python, Java, or Scala.\nStrong knowledge of SQL and experience with database management systems (e.g., MySQL, PostgreSQL, MongoDB).\nExperience with big data technologies such as Hadoop, Spark, or Kafka.\nFamiliarity with cloud platforms like AWS, Azure, or Google Cloud for data storage and processing.\nUnderstanding of data modeling concepts and data warehousing principles.\nAbility to work collaboratively in a team environment and communicate effectively with stakeholders.","['Sql', 'Apache Spark', 'Big Data', 'AWS', 'Data Modeling', 'Kafka']",2025-06-12 06:45:19
Data Engineer,Virtusa,2-4 Years,,Hyderabad,Information Technology,"Key Responsibilities:\nDesign, develop, and maintain ETL/ELT pipelines to move and transform data from various sources.\nBuild and optimize data warehouses, data lakes, and structured/unstructured storage solutions.\nEnsure data quality, integrity, and consistency across systems.\nCollaborate with data analysts and scientists to understand data needs and optimize performance.\nIntegrate APIs, third-party data sources, and internal systems to build robust datasets.\nMonitor and troubleshoot pipeline performance and resolve data-related issues.\nImplement data security, governance, and compliance best practices.\nRequired Skills & Qualifications:\nBachelor's degree in Computer Science, Engineering, Information Systems, or a related field.\n25 years of experience as a Data Engineer or in a related role.\nStrong programming skills in Python, SQL, and optionally Scala or Java.\nExperience with big data tools such as Spark, Hadoop, or Kafka.\nHands-on experience with cloud platforms (e.g., AWS, GCP, or Azure) and services like S3, Redshift, BigQuery, or Snowflake.\nProficiency with data modeling, schema design, and working with relational and NoSQL databases.\nFamiliarity with workflow orchestration tools like Airflow, Luigi, or Prefect.\nStrong understanding of data warehousing concepts and performance optimization.","['AWS', 'Gcp', 'Azure', 'Redshift', 'BigQuery']",2025-06-12 06:45:21
Data Engineer,Virtusa,2-4 Years,,Hyderabad,Information Technology,"We are seeking a skilled Data Engineer to join our dynamic team in India. The ideal candidate will have experience in building and managing data pipelines, ensuring data quality, and optimizing our data infrastructure to support analytics and business intelligence initiatives.\nResponsibilities\nDesign, construct, install, and maintain large-scale processing systems and data pipelines.\nDevelop and optimize ETL processes to ensure smooth data integration and transformation.\nCollaborate with data scientists and analysts to understand data requirements and deliver solutions that meet their needs.\nImplement data quality checks and ensure data integrity throughout the data lifecycle.\nMonitor performance and troubleshoot data-related issues, enhancing system efficiency and reliability.\nSkills and Qualifications\nBachelor's degree in Computer Science, Engineering, or a related field.\n2-4 years of experience in data engineering or a similar role.\nProficiency in programming languages such as Python, Java, or Scala.\nStrong knowledge of SQL and experience with database management systems (e.g., MySQL, PostgreSQL, MongoDB).\nExperience with big data technologies such as Hadoop, Spark, or Kafka.\nFamiliarity with cloud platforms like AWS, Azure, or Google Cloud for data storage and processing.\nUnderstanding of data modeling concepts and data warehousing principles.\nAbility to work collaboratively in a team environment and communicate effectively with stakeholders.","['Sql', 'Apache Spark', 'Big Data', 'AWS', 'Data Modeling', 'Kafka']",2025-06-12 06:45:22
Data Engineer,Virtusa,4-6 Years,,Bengaluru,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools.","['CI & CD', 'Cloudera Data platform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker']",2025-06-12 06:45:23
Data Engineer,Virtusa,6-8 Years,,Bengaluru,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 06:45:24
Data Engineer,Virtusa,6-10 Years,,Bengaluru,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark', 'Aws']",2025-06-12 06:45:26
Data Engineer,Virtusa,6-10 Years,,Bengaluru,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark', 'Aws']",2025-06-12 06:45:26
Data Engineer,Virtusa,6-12 Years,,Chennai,Information Technology,"Job Title: Senior Data Engineer\nLocation\nHyderabad, Telangana, India\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.","['Sql', 'Apache Spark', 'Big Data', 'AWS', 'Data Modeling', 'Kafka']",2025-06-12 06:45:28
Data Engineer,Virtusa,10-12 Years,,Bengaluru,Information Technology,"Requirements\n10+ years of strong experience with data transformation & ETL on large data sets\nExperience with designing customer centric datasets (i.e., CRM, Call Center, Marketing, Offline, Point of\nSale etc.)\n5+ years of Data Modeling experience (i.e., Relational, Dimensional, Columnar, Big Data)\n5+ years of complex SQL or NoSQL experience\nExperience in advanced Data Warehouse concepts\nExperience in industry ETL tools (i.e., Informatica, Unifi)\nExperience with Business Requirements definition and management, structured analysis, process\ndesign, use case documentation\nExperience with Reporting Technologies (i.e., Tableau, PowerBI)\nexperience in professional software development\nDemonstrate exceptional organizational skills and ability to multi-task simultaneous different customer\nprojects\nStrong verbal & written communication skills to interface with Sales team & lead customers to\nsuccessful outcome\nMust be self-managed, proactive and customer focused\nDegree in Computer Science, Information Systems, Data Science, or related field","['Python', 'Sql', 'Etl', 'BigQuery']",2025-06-12 06:45:29
Data Engineer,Virtusa,8-9 Years,,Bengaluru,Information Technology,"Proficiency in PySpark for distributed data processing and transformation.\nSolid experience with AWS Glue for ETL jobs and managing data workflows.\nHands-on experience with AWS Data Pipeline (DPL) for workflow orchestration.\nStrong experience with AWS services such as S3, Lambda, Redshift, RDS, and EC2.\nTechnical Skills:\nProficiency in Python and PySpark for data processing and transformation tasks.\nDeep understanding of ETL concepts and best practices.\nFamiliarity with AWS Glue (ETL jobs, Data Catalog, and Crawlers).\nExperience building and maintaining data pipelines with AWS Data Pipeline or similar orchestration tools.\nFamiliarity with AWS S3 for data storage and management, including file formats (CSV, Parquet, Avro).\nStrong knowledge of SQL for querying and manipulating relational and semi-structured data.\nExperience with Data Warehousing and Big Data technologies, specifically within AWS.\nAdditional Skills:\nExperience with AWS Lambda for serverless data processing and orchestration.\nUnderstanding of AWS Redshift for data warehousing and analytics.\nFamiliarity with Data Lakes, Amazon EMR, and Kinesis for streaming data processing.\nKnowledge of data governance practices, including data lineage and auditing.\nFamiliarity with CI/CD pipelines and Git for version control.\nExperience with Docker and containerization for building and deploying applications.\nDesign and Build Data Pipelines: Design, implement, and optimize data pipelines on AWS using PySpark, AWS Glue, and AWS Data Pipeline to automate data integration, transformation, and storage processes.\nETL Development: Develop and maintain Extract, Transform, and Load (ETL) processes using AWS Glue and PySpark to efficiently process large datasets.\nData Workflow Automation: Build and manage automated data workflows using AWS Data Pipeline, ensuring seamless scheduling, monitoring, and management of data jobs.\nData Integration: Work with different AWS data storage services (e.g., S3, Redshift, RDS) to ensure smooth integration and movement of data across platforms.\nOptimization and Scaling: Optimize and scale data pipelines for high performance and cost efficiency, utilizing AWS services like Lambda, S3, and EC2.","['CI & CD', 'Cloudera Data platform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Py Spark']",2025-06-12 06:45:30
Data Engineer,Virtusa,7-12 Years,,Chennai,Information Technology,"Job Title: Data Engineer\n\nExperience Level\n6+ years in Data Engineering\nKey Responsibilities\nAs a Data Engineer, you will:\nData Engineering: Apply expertise in Snowflake, DBT, and Python to develop and manage data solutions.\nCloud Integration: Utilize AWS Cloud services, specifically AWS S3 and Lambda, for data operations.\nSQL Development: Leverage strong SQL knowledge for data manipulation and querying.\nMandatory Skills & Experience\nTechnical Proficiency:\nSQL: Strong knowledge in SQL.\nData Warehousing & Transformation: Expertise in Snowflake and DBT (minimum 2+ years of experience).\nProgramming: Expertise in Python (minimum 2+ years of experience).\nCloud Platform: Knowledge in AWS Cloud (specifically AWS S3 & Lambda).\nETL/ELT Tools (Desirable): Knowledge of SnapLogic or Fivetran tools is an added advantage.\nExperience & Qualifications:\n6+ years of experience in data engineering.","['Snowflake design', 'ETL/ELT', 'Python', 'Sql', 'Data Warehousing', 'cloud platform']",2025-06-12 06:45:32
Data Engineer,Virtusa,5-8 Years,,Chennai,Information Technology,8 Years overall IT experience with minimum 5 years of work experience in below tech skills\nStrong experience in Python Scripting and PySpark for data processing\nProficiency in SQL dealing with big data over Informatica ETL\nProven experience in Data quality and data optimization of data lake in Iceberg format with strong understanding of architecture\nExperience in AWS Glue jobs\nExperience in AWS cloud platform and its data services S3 Redshift Lambda EMR Airflow Postgres SNS Event bridge\nExpertise in BASH Shell scripting\nStrong understanding of healthcare data systems and experience leading data engineering teams\nExperience in Agile environments\nExcellent problem solving skills and attention to detail\nEffective communication and collaboration skills\nResponsibilities\nLeads development of data pipelines and architectures that handle large scale data sets\nDesigns constructs and tests data architecture aligned with business requirements\nProvides technical leadership for data projects ensuring best practices and high quality data solutions\nCollaborates with product finance and other business units to ensure data pipelines meet business requirements\nWork with DBT Data Build Tool for transforming raw data into actionable insights\nOversees development of data solutions that enable predictive and prescriptive analytics\nEnsures the technical quality of solutions managing data as it moves across environments\nAligns data architecture to Healthfirst solution architecture,"['Iceberg', 'Python', 'Pyspark', 'Sql', 'AWS Glue', 'Informatica']",2025-06-12 06:45:33
Data Engineer,Virtusa,10-12 Years,,Bengaluru,Information Technology,"Requirements\n10+ years of strong experience with data transformation & ETL on large data sets\nExperience with designing customer centric datasets (i.e., CRM, Call Center, Marketing, Offline, Point of\nSale etc.)\n5+ years of Data Modeling experience (i.e., Relational, Dimensional, Columnar, Big Data)\n5+ years of complex SQL or NoSQL experience\nExperience in advanced Data Warehouse concepts\nExperience in industry ETL tools (i.e., Informatica, Unifi)\nExperience with Business Requirements definition and management, structured analysis, process\ndesign, use case documentation\nExperience with Reporting Technologies (i.e., Tableau, PowerBI)\nexperience in professional software development\nDemonstrate exceptional organizational skills and ability to multi-task simultaneous different customer\nprojects\nStrong verbal & written communication skills to interface with Sales team & lead customers to\nsuccessful outcome\nMust be self-managed, proactive and customer focused\nDegree in Computer Science, Information Systems, Data Science, or related field","['Python', 'Sql', 'Etl', 'BigQuery']",2025-06-12 06:45:43
Data Engineer,Virtusa,6-12 Years,,Chennai,Information Technology,"Job Title: Senior Data Engineer\nLocation\nHyderabad, Telangana, India\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.","['Sql', 'Apache Spark', 'Big Data', 'AWS', 'Data Modeling', 'Kafka']",2025-06-12 06:45:48
Data Engineer,Virtusa,6-12 Years,,Chennai,Information Technology,"Job Title: Senior Data Engineer\nLocation\nHyderabad, Telangana, India\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.","['Sql', 'Apache Spark', 'Big Data', 'AWS', 'Data Modeling', 'Kafka']",2025-06-12 06:45:49
AWS Data Engineer,Virtusa,6-8 Years,,Chennai,Information Technology,"Sr. AWS Data Engineer\nP3 C3 TSTS\nPrimary Skills\nExperience in data engineering, with a proven focus on data ingestion and extraction using Python/PySpark.\nExtensive AWS experience is mandatory, with proficiency in Glue, Lambda, SQS, SNS, AWS IAM, AWS Step Functions, S3, and RDS (Oracle, Aurora Postgres).\n4+ years of experience working with both relational and non-relational/NoSQL databases is required.\nStrong SQL experience is necessary, demonstrating the ability to write complex queries from scratch. Also, experience in Redshift is required along with other SQL DB experience.\nStrong scripting experience with the ability to build intricate data pipelines using AWS serverless architecture.\nUnderstanding of building an end-to-end Data pipeline.\nSecondary Skills\nStrong understanding of Kinesis, Kafka, CDK. Experience with Kafka and ECS is also required.\nStrong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling is required.\nExperience in Node Js and CDK.\nJD\nResponsibilities\nLead the architectural design and development of a scalable, reliable, and flexible metadata-driven data ingestion and extraction framework on AWS using Python/PySpark.\nDesign and implement a customizable data processing framework using Python/PySpark. This framework should be capable of handling diverse scenarios and evolving data processing requirements.\nImplement data pipeline for data ingestion, transformation, and extraction leveraging the AWS Cloud Services.\nSeamlessly integrate a variety of AWS services, including S3, Glue, Kafka, Lambda, SQL, SNS, Athena, EC2, RDS (Oracle, Postgres, MySQL), AWS Crawler to construct a highly scalable and reliable data ingestion and extraction pipeline.\nFacilitate configuration and extensibility of the framework to adapt to evolving data needs and processing scenarios.\nDevelop and maintain rigorous data quality checks and validation processes to safeguard the integrity of ingested data.\nImplement robust error handling, logging, monitoring, and alerting mechanisms to ensure the reliability of the entire data pipeline.\nQualifications\nMust Have\nOver 6 years of hands-on experience in data engineering, with a proven focus on data ingestion and extraction using Python/PySpark.\nExtensive AWS experience is mandatory, with proficiency in Glue, Lambda, SQS, SNS, AWS IAM, AWS Step Functions, S3, and RDS (Oracle, Aurora Postgres).\n4+ years of experience working with both relational and non-relational/NoSQL databases is required.\nStrong SQL experience is necessary, demonstrating the ability to write complex queries from scratch.\nStrong working experience in Redshift is required along with other SQL DB experience.\nStrong scripting experience with the ability to build intricate data pipelines using AWS serverless architecture.\nComplete understanding of building an end-to-end Data pipeline.\nNice to have\nStrong understanding of Kinesis, Kafka, CDK.\nA strong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling is required.\nExperience in Node Js and CDK.\nExperience with Kafka and ECS is also required.\nQualification\nAWS Data Engineer\nWorkmode: Hybrid\nWork location: PAN INDIA\nWork Timing: 2 PM to 11 PM\nPrimary Skill: Data Engineer\nExperience in data engineering, with a proven focus on data ingestion and extraction using Python/PySpark.\nExtensive AWS experience is mandatory, with proficiency in Glue, Lambda, SQS, SNS, AWS IAM, AWS Step Functions, S3, and RDS (Oracle, Aurora Postgres).\n4+ years of experience working with both relational and non-relational/NoSQL databases is required.\nStrong SQL experience is necessary, demonstrating the ability to write complex queries from scratch. Also, experience in Redshift is required along with other SQL DB experience.\nStrong scripting experience with the ability to build intricate data pipelines using AWS serverless architecture.\nUnderstanding of building an end-to-end Data pipeline.\nSecondary Skills\nStrong understanding of Kinesis, Kafka, CDK. Experience with Kafka and ECS is also required.\nStrong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling is required.\nExperience in Node Js and CDK.\nJD Responsibilities\nLead the architectural design and development of a scalable, reliable, and flexible metadata-driven data ingestion and extraction framework on AWS using Python/PySpark.\nDesign and implement a customizable data processing framework using Python/PySpark. This framework should be capable of handling diverse scenarios and evolving data processing requirements.\nImplement data pipeline for data ingestion, transformation, and extraction leveraging the AWS Cloud Services.\nSeamlessly integrate a variety of AWS services, including S3, Glue, Kafka, Lambda, SQL, SNS, Athena, EC2, RDS (Oracle, Postgres, MySQL), AWS Crawler to construct a highly scalable and reliable data ingestion and extraction pipeline.\nFacilitate configuration and extensibility of the framework to adapt to evolving data needs and processing scenarios.\nDevelop and maintain rigorous data quality checks and validation processes to safeguard the integrity of ingested data.\nImplement robust error handling, logging, monitoring, and alerting mechanisms to ensure the reliability of the entire data pipeline.\nQualifications\nMust Have\nOver 6 years of hands-on experience in data engineering, with a proven focus on data ingestion and extraction using Python/PySpark.\nExtensive AWS experience is mandatory, with proficiency in Glue, Lambda, SQS, SNS, AWS IAM, AWS Step Functions, S3, and RDS (Oracle, Aurora Postgres).\n4+ years of experience working with both relational and non-relational/NoSQL databases is required.\nStrong SQL experience is necessary, demonstrating the ability to write complex queries from scratch.\nStrong working experience in Redshift is required along with other SQL DB experience.\nStrong scripting experience with the ability to build intricate data pipelines using AWS serverless architecture.\nComplete understanding of building an end-to-end Data pipeline.\nNice to have\nStrong understanding of Kinesis, Kafka, CDK.\nA strong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling is required.\nExperience in Node Js and CDK.\nExperience with Kafka and ECS is also required.","['aws glue', 'Pyspark', 'Redshift', 'Sql', 'Lambda', 'Kafka']",2025-06-12 06:45:50
Data Engineer-Data Platforms-Azure,IBM,7-10 Years,,Bengaluru,Information Technology,"Your role and responsibilities\nGraduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.\n7+ Yrs total experience in Data Engineering projects & 4+ years of relevant experience on Azure technology services and Python\nAzure : Azure data factory, ADLS- Azure data lake store, Azure data bricks,\nMandatory Programming languages : Py-Spark, PL/SQL, Spark SQL\nDatabase : SQL DB\nExperience with Azure: ADLS, Databricks, Stream Analytics, SQL DW, COSMOS DB, Analysis Services, Azure Functions, Serverless Architecture, ARM Templates\nExperience with relational SQL and NoSQL databases, including Postgres and Cassandra.\nExperience with object-oriented/object function scripting languages: Python, SQL, Scala, Spark-SQL etc.\nData Warehousing experience with strong domain\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nIntuitive individual with an ability to manage change and proven time management\nProven interpersonal skills while contributing to team effort by accomplishing related results as needed\nUp-to-date technical knowledge by attending educational workshops, reviewing publications\nPreferred technical and professional experience\nExperience with Azure: ADLS, Databricks, Stream Analytics, SQL DW, COSMOS DB, Analysis Services, Azure Functions, Serverless Architecture, ARM Templates\nExperience with relational SQL and NoSQL databases, including Postgres and Cassandra.\nExperience with object-oriented/object function scripting languages: Python, SQL, Scala, Spark-SQL etc.","['Spark-SQL', 'Python', 'Sql', 'Scala', 'Nosql']",2025-06-12 06:45:51
Data Engineer,Virtusa,8-9 Years,,Bengaluru,Information Technology,"Proficiency in PySpark for distributed data processing and transformation.\nSolid experience with AWS Glue for ETL jobs and managing data workflows.\nHands-on experience with AWS Data Pipeline (DPL) for workflow orchestration.\nStrong experience with AWS services such as S3, Lambda, Redshift, RDS, and EC2.\nTechnical Skills:\nProficiency in Python and PySpark for data processing and transformation tasks.\nDeep understanding of ETL concepts and best practices.\nFamiliarity with AWS Glue (ETL jobs, Data Catalog, and Crawlers).\nExperience building and maintaining data pipelines with AWS Data Pipeline or similar orchestration tools.\nFamiliarity with AWS S3 for data storage and management, including file formats (CSV, Parquet, Avro).\nStrong knowledge of SQL for querying and manipulating relational and semi-structured data.\nExperience with Data Warehousing and Big Data technologies, specifically within AWS.\nAdditional Skills:\nExperience with AWS Lambda for serverless data processing and orchestration.\nUnderstanding of AWS Redshift for data warehousing and analytics.\nFamiliarity with Data Lakes, Amazon EMR, and Kinesis for streaming data processing.\nKnowledge of data governance practices, including data lineage and auditing.\nFamiliarity with CI/CD pipelines and Git for version control.\nExperience with Docker and containerization for building and deploying applications.\nDesign and Build Data Pipelines: Design, implement, and optimize data pipelines on AWS using PySpark, AWS Glue, and AWS Data Pipeline to automate data integration, transformation, and storage processes.\nETL Development: Develop and maintain Extract, Transform, and Load (ETL) processes using AWS Glue and PySpark to efficiently process large datasets.\nData Workflow Automation: Build and manage automated data workflows using AWS Data Pipeline, ensuring seamless scheduling, monitoring, and management of data jobs.\nData Integration: Work with different AWS data storage services (e.g., S3, Redshift, RDS) to ensure smooth integration and movement of data across platforms.\nOptimization and Scaling: Optimize and scale data pipelines for high performance and cost efficiency, utilizing AWS services like Lambda, S3, and EC2.","['CI & CD', 'Cloudera Data platform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker']",2025-06-12 06:45:54
Data Engineer,Virtusa,8-9 Years,,Bengaluru,Information Technology,"Proficiency in PySpark for distributed data processing and transformation.\nSolid experience with AWS Glue for ETL jobs and managing data workflows.\nHands-on experience with AWS Data Pipeline (DPL) for workflow orchestration.\nStrong experience with AWS services such as S3, Lambda, Redshift, RDS, and EC2.\nTechnical Skills:\nProficiency in Python and PySpark for data processing and transformation tasks.\nDeep understanding of ETL concepts and best practices.\nFamiliarity with AWS Glue (ETL jobs, Data Catalog, and Crawlers).\nExperience building and maintaining data pipelines with AWS Data Pipeline or similar orchestration tools.\nFamiliarity with AWS S3 for data storage and management, including file formats (CSV, Parquet, Avro).\nStrong knowledge of SQL for querying and manipulating relational and semi-structured data.\nExperience with Data Warehousing and Big Data technologies, specifically within AWS.\nAdditional Skills:\nExperience with AWS Lambda for serverless data processing and orchestration.\nUnderstanding of AWS Redshift for data warehousing and analytics.\nFamiliarity with Data Lakes, Amazon EMR, and Kinesis for streaming data processing.\nKnowledge of data governance practices, including data lineage and auditing.\nFamiliarity with CI/CD pipelines and Git for version control.\nExperience with Docker and containerization for building and deploying applications.\nDesign and Build Data Pipelines: Design, implement, and optimize data pipelines on AWS using PySpark, AWS Glue, and AWS Data Pipeline to automate data integration, transformation, and storage processes.\nETL Development: Develop and maintain Extract, Transform, and Load (ETL) processes using AWS Glue and PySpark to efficiently process large datasets.\nData Workflow Automation: Build and manage automated data workflows using AWS Data Pipeline, ensuring seamless scheduling, monitoring, and management of data jobs.\nData Integration: Work with different AWS data storage services (e.g., S3, Redshift, RDS) to ensure smooth integration and movement of data across platforms.\nOptimization and Scaling: Optimize and scale data pipelines for high performance and cost efficiency, utilizing AWS services like Lambda, S3, and EC2.","['CI & CD', 'Cloudera Data platform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Py Spark']",2025-06-12 06:45:55
Data Engineer,Virtusa,6-8 Years,,Bengaluru,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 06:45:56
Data Engineer,Virtusa,6-10 Years,,Bengaluru,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark', 'Aws']",2025-06-12 06:45:58
Data Engineer,Virtusa,6-10 Years,,Bengaluru,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark', 'Aws']",2025-06-12 06:45:59
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,Information Technology,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.","['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 06:46:00
AWS Data Engineer,Virtusa,6-8 Years,,Chennai,Information Technology,"Sr. AWS Data Engineer\nP3 C3 TSTS\nPrimary Skills\nExperience in data engineering, with a proven focus on data ingestion and extraction using Python/PySpark.\nExtensive AWS experience is mandatory, with proficiency in Glue, Lambda, SQS, SNS, AWS IAM, AWS Step Functions, S3, and RDS (Oracle, Aurora Postgres).\n4+ years of experience working with both relational and non-relational/NoSQL databases is required.\nStrong SQL experience is necessary, demonstrating the ability to write complex queries from scratch. Also, experience in Redshift is required along with other SQL DB experience.\nStrong scripting experience with the ability to build intricate data pipelines using AWS serverless architecture.\nUnderstanding of building an end-to-end Data pipeline.\nSecondary Skills\nStrong understanding of Kinesis, Kafka, CDK. Experience with Kafka and ECS is also required.\nStrong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling is required.\nExperience in Node Js and CDK.\nJD\nResponsibilities\nLead the architectural design and development of a scalable, reliable, and flexible metadata-driven data ingestion and extraction framework on AWS using Python/PySpark.\nDesign and implement a customizable data processing framework using Python/PySpark. This framework should be capable of handling diverse scenarios and evolving data processing requirements.\nImplement data pipeline for data ingestion, transformation, and extraction leveraging the AWS Cloud Services.\nSeamlessly integrate a variety of AWS services, including S3, Glue, Kafka, Lambda, SQL, SNS, Athena, EC2, RDS (Oracle, Postgres, MySQL), AWS Crawler to construct a highly scalable and reliable data ingestion and extraction pipeline.\nFacilitate configuration and extensibility of the framework to adapt to evolving data needs and processing scenarios.\nDevelop and maintain rigorous data quality checks and validation processes to safeguard the integrity of ingested data.\nImplement robust error handling, logging, monitoring, and alerting mechanisms to ensure the reliability of the entire data pipeline.\nQualifications\nMust Have\nOver 6 years of hands-on experience in data engineering, with a proven focus on data ingestion and extraction using Python/PySpark.\nExtensive AWS experience is mandatory, with proficiency in Glue, Lambda, SQS, SNS, AWS IAM, AWS Step Functions, S3, and RDS (Oracle, Aurora Postgres).\n4+ years of experience working with both relational and non-relational/NoSQL databases is required.\nStrong SQL experience is necessary, demonstrating the ability to write complex queries from scratch.\nStrong working experience in Redshift is required along with other SQL DB experience.\nStrong scripting experience with the ability to build intricate data pipelines using AWS serverless architecture.\nComplete understanding of building an end-to-end Data pipeline.\nNice to have\nStrong understanding of Kinesis, Kafka, CDK.\nA strong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling is required.\nExperience in Node Js and CDK.\nExperience with Kafka and ECS is also required.\nQualification\nAWS Data Engineer\nWorkmode: Hybrid\nWork location: PAN INDIA\nWork Timing: 2 PM to 11 PM\nPrimary Skill: Data Engineer\nExperience in data engineering, with a proven focus on data ingestion and extraction using Python/PySpark.\nExtensive AWS experience is mandatory, with proficiency in Glue, Lambda, SQS, SNS, AWS IAM, AWS Step Functions, S3, and RDS (Oracle, Aurora Postgres).\n4+ years of experience working with both relational and non-relational/NoSQL databases is required.\nStrong SQL experience is necessary, demonstrating the ability to write complex queries from scratch. Also, experience in Redshift is required along with other SQL DB experience.\nStrong scripting experience with the ability to build intricate data pipelines using AWS serverless architecture.\nUnderstanding of building an end-to-end Data pipeline.\nSecondary Skills\nStrong understanding of Kinesis, Kafka, CDK. Experience with Kafka and ECS is also required.\nStrong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling is required.\nExperience in Node Js and CDK.\nJD Responsibilities\nLead the architectural design and development of a scalable, reliable, and flexible metadata-driven data ingestion and extraction framework on AWS using Python/PySpark.\nDesign and implement a customizable data processing framework using Python/PySpark. This framework should be capable of handling diverse scenarios and evolving data processing requirements.\nImplement data pipeline for data ingestion, transformation, and extraction leveraging the AWS Cloud Services.\nSeamlessly integrate a variety of AWS services, including S3, Glue, Kafka, Lambda, SQL, SNS, Athena, EC2, RDS (Oracle, Postgres, MySQL), AWS Crawler to construct a highly scalable and reliable data ingestion and extraction pipeline.\nFacilitate configuration and extensibility of the framework to adapt to evolving data needs and processing scenarios.\nDevelop and maintain rigorous data quality checks and validation processes to safeguard the integrity of ingested data.\nImplement robust error handling, logging, monitoring, and alerting mechanisms to ensure the reliability of the entire data pipeline.\nQualifications\nMust Have\nOver 6 years of hands-on experience in data engineering, with a proven focus on data ingestion and extraction using Python/PySpark.\nExtensive AWS experience is mandatory, with proficiency in Glue, Lambda, SQS, SNS, AWS IAM, AWS Step Functions, S3, and RDS (Oracle, Aurora Postgres).\n4+ years of experience working with both relational and non-relational/NoSQL databases is required.\nStrong SQL experience is necessary, demonstrating the ability to write complex queries from scratch.\nStrong working experience in Redshift is required along with other SQL DB experience.\nStrong scripting experience with the ability to build intricate data pipelines using AWS serverless architecture.\nComplete understanding of building an end-to-end Data pipeline.\nNice to have\nStrong understanding of Kinesis, Kafka, CDK.\nA strong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling is required.\nExperience in Node Js and CDK.\nExperience with Kafka and ECS is also required.","['aws glue', 'Pyspark', 'Redshift', 'Sql', 'Lambda', 'Kafka']",2025-06-12 06:46:01
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,Information Technology,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.","['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 06:46:03
Data Engineer-Data Platforms,IBM,5-8 Years,,Navi Mumbai,Software,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions.\nYou will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.. Good working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala","['Big Data', 'Hadoop', 'Spark', 'Python', 'Scala', 'Pyspark']",2025-06-12 06:46:04
Data Engineer,Virtusa,8-12 Years,,Hyderabad,Information Technology,"Experience:\n8+ years of experience in data engineering, specifically in cloud environments like AWS.\nProficiency in PySpark for distributed data processing and transformation.\nSolid experience with AWS Glue for ETL jobs and managing data workflows.\nHands-on experience with AWS Data Pipeline (DPL) for workflow orchestration.\nStrong experience with AWS services such as S3, Lambda, Redshift, RDS, and EC2.\nTechnical Skills:\nProficiency in Python and PySpark for data processing and transformation tasks.\nDeep understanding of ETL concepts and best practices.\nFamiliarity with AWS Glue (ETL jobs, Data Catalog, and Crawlers).\nExperience building and maintaining data pipelines with AWS Data Pipeline or similar orchestration tools.\nFamiliarity with AWS S3 for data storage and management, including file formats (CSV, Parquet, Avro).\nStrong knowledge of SQL for querying and manipulating relational and semi-structured data.\nExperience with Data Warehousing and Big Data technologies, specifically within AWS.\nAdditional Skills:\nExperience with AWS Lambda for serverless data processing and orchestration.\nUnderstanding of AWS Redshift for data warehousing and analytics.\nFamiliarity with Data Lakes, Amazon EMR, and Kinesis for streaming data processing.\nKnowledge of data governance practices, including data lineage and auditing.\nFamiliarity with CI/CD pipelines and Git for version control.\nExperience with Docker and containerization for building and deploying applications.\nDesign and Build Data Pipelines: Design, implement, and optimize data pipelines on AWS using PySpark, AWS Glue, and AWS Data Pipeline to automate data integration, transformation, and storage processes.\nETL Development: Develop and maintain Extract, Transform, and Load (ETL) processes using AWS Glue and PySpark to efficiently process large datasets.\nData Workflow Automation: Build and manage automated data workflows using AWS Data Pipeline, ensuring seamless scheduling, monitoring, and management of data jobs.\nData Integration: Work with different AWS data storage services (e.g., S3, Redshift, RDS) to ensure smooth integration and movement of data across platforms.\nOptimization and Scaling: Optimize and scale data pipelines for high performance and cost efficiency, utilizing AWS services like Lambda, S3, and EC2.","['Snowflake design', 'ETL/ELT', 'Python', 'Sql', 'Data Warehousing', 'cloud platform']",2025-06-12 06:46:06
AWS Data Engineer,Virtusa,5-8 Years,,Chennai,Information Technology,"Job description\nWe are seeking a skilled and motivated Data Engineer to join our data engineering team. The ideal candidate will have hands-on experience with AWS storage solutions, particularly Redshift and S3, and a solid background in ETL development using SQL and Python. Proficiency in version control using GitHub is also essential.\nKey Responsibilities:\nDesign, develop, and maintain scalable ETL pipelines to support data integration and analytics initiatives.\nBuild and manage data models in Amazon Redshift, ensuring high performance and efficient storage.\nIngest and manage structured and unstructured data from various sources into AWS S3 and Redshift.\nOptimize SQL queries and Python scripts to improve data processing efficiency and reliability.\nCollaborate with data analysts, data scientists, and product teams to understand data needs and deliver robust solutions.\nMaintain code versioning and documentation using GitHub following best practices in code management and CI/CD.\nMonitor data workflows and troubleshoot issues to ensure data integrity and availability.\nRequired Skills & Qualifications:\n4 to 6 years of experience in data engineering or ETL development.\nStrong experience with AWS Redshift and Amazon S3.\nProficient in SQL for data manipulation and query optimization.\nStrong programming skills in Python for ETL scripting and automation.\nExperience with version control systems, particularly GitHub.\nFamiliarity with data warehousing concepts and cloud data architecture.\nAbility to work independently and in a collaborative team environment.\nStrong analytical and problem-solving skills.","['QE AI tools', 'Java', 'Python', 'Selenium', 'Bdd', 'Sql']",2025-06-12 06:46:13
