title,company,experience,salary,location,description,industry,skills,scraped_at
Data Engineer,Virtusa,6-12 Years,,Hyderabad,"Job Title: Senior Data Engineer\nLocation\nHyderabad, Telangana, India\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.",Information Technology,"['Sql', 'Apache Spark', 'Big Data', 'AWS', 'Data Modeling', 'Kafka']",2025-06-12 15:17:13
Data Engineer,Virtusa,6-12 Years,,Chennai,"Job Title: Senior Data Engineer\nLocation\nHyderabad, Telangana, India\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.",Information Technology,"['Sql', 'Apache Spark', 'Big Data', 'AWS', 'Data Modeling', 'Kafka']",2025-06-12 15:17:16
Data Engineer,Virtusa,6-12 Years,,Chennai,"Job Title: Senior Data Engineer\nLocation\nHyderabad, Telangana, India\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.",Information Technology,"['Sql', 'Apache Spark', 'Big Data', 'AWS', 'Data Modeling', 'Kafka']",2025-06-12 15:17:16
Data Engineer,Virtusa,2-4 Years,,Hyderabad,"Key Responsibilities:\nDesign, develop, and maintain ETL/ELT pipelines to move and transform data from various sources.\nBuild and optimize data warehouses, data lakes, and structured/unstructured storage solutions.\nEnsure data quality, integrity, and consistency across systems.\nCollaborate with data analysts and scientists to understand data needs and optimize performance.\nIntegrate APIs, third-party data sources, and internal systems to build robust datasets.\nMonitor and troubleshoot pipeline performance and resolve data-related issues.\nImplement data security, governance, and compliance best practices.\nRequired Skills & Qualifications:\nBachelor's degree in Computer Science, Engineering, Information Systems, or a related field.\n25 years of experience as a Data Engineer or in a related role.\nStrong programming skills in Python, SQL, and optionally Scala or Java.\nExperience with big data tools such as Spark, Hadoop, or Kafka.\nHands-on experience with cloud platforms (e.g., AWS, GCP, or Azure) and services like S3, Redshift, BigQuery, or Snowflake.\nProficiency with data modeling, schema design, and working with relational and NoSQL databases.\nFamiliarity with workflow orchestration tools like Airflow, Luigi, or Prefect.\nStrong understanding of data warehousing concepts and performance optimization.",Information Technology,"['AWS', 'Gcp', 'Azure', 'Redshift', 'BigQuery']",2025-06-12 15:17:17
Data Engineer,Virtusa,2-4 Years,,Hyderabad,"We are seeking a skilled Data Engineer to join our dynamic team in India. The ideal candidate will have experience in building and managing data pipelines, ensuring data quality, and optimizing our data infrastructure to support analytics and business intelligence initiatives.\nResponsibilities\nDesign, construct, install, and maintain large-scale processing systems and data pipelines.\nDevelop and optimize ETL processes to ensure smooth data integration and transformation.\nCollaborate with data scientists and analysts to understand data requirements and deliver solutions that meet their needs.\nImplement data quality checks and ensure data integrity throughout the data lifecycle.\nMonitor performance and troubleshoot data-related issues, enhancing system efficiency and reliability.\nSkills and Qualifications\nBachelor's degree in Computer Science, Engineering, or a related field.\n2-4 years of experience in data engineering or a similar role.\nProficiency in programming languages such as Python, Java, or Scala.\nStrong knowledge of SQL and experience with database management systems (e.g., MySQL, PostgreSQL, MongoDB).\nExperience with big data technologies such as Hadoop, Spark, or Kafka.\nFamiliarity with cloud platforms like AWS, Azure, or Google Cloud for data storage and processing.\nUnderstanding of data modeling concepts and data warehousing principles.\nAbility to work collaboratively in a team environment and communicate effectively with stakeholders.",Information Technology,"['Sql', 'Apache Spark', 'Big Data', 'AWS', 'Data Modeling', 'Kafka']",2025-06-12 15:17:18
Data Engineer,Virtusa,2-4 Years,,Hyderabad,"We are seeking a skilled Data Engineer to join our dynamic team in India. The ideal candidate will have experience in building and managing data pipelines, ensuring data quality, and optimizing our data infrastructure to support analytics and business intelligence initiatives.\nResponsibilities\nDesign, construct, install, and maintain large-scale processing systems and data pipelines.\nDevelop and optimize ETL processes to ensure smooth data integration and transformation.\nCollaborate with data scientists and analysts to understand data requirements and deliver solutions that meet their needs.\nImplement data quality checks and ensure data integrity throughout the data lifecycle.\nMonitor performance and troubleshoot data-related issues, enhancing system efficiency and reliability.\nSkills and Qualifications\nBachelor's degree in Computer Science, Engineering, or a related field.\n2-4 years of experience in data engineering or a similar role.\nProficiency in programming languages such as Python, Java, or Scala.\nStrong knowledge of SQL and experience with database management systems (e.g., MySQL, PostgreSQL, MongoDB).\nExperience with big data technologies such as Hadoop, Spark, or Kafka.\nFamiliarity with cloud platforms like AWS, Azure, or Google Cloud for data storage and processing.\nUnderstanding of data modeling concepts and data warehousing principles.\nAbility to work collaboratively in a team environment and communicate effectively with stakeholders.",Information Technology,"['Sql', 'Apache Spark', 'Big Data', 'AWS', 'Data Modeling', 'Kafka']",2025-06-12 15:17:20
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,"Key Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.",Information Technology,"['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 15:17:21
Data Engineer-Data Platforms,IBM,5-8 Years,,Navi Mumbai,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions.\nYou will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.. Good working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala",Software,"['Big Data', 'Hadoop', 'Spark', 'Python', 'Scala', 'Pyspark']",2025-06-12 15:17:22
Data Engineer-Data Platforms-Google,IBM,5-10 Years,,Gurugram,"Your role and responsibilities :\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusablemanner, developing predictive or prescriptive models, and evaluating modelling results\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience:\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Information Technology,"['Communication Skills', 'Data Replication mechanism', 'staffing plan', 'RACI', 'Data Engineer', 'Api']",2025-06-12 15:17:23
Data Engineer-Data Platforms-Google,IBM,5-10 Years,,Gurugram,"Your role and responsibilities :\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusablemanner, developing predictive or prescriptive models, and evaluating modelling results\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience:\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Information Technology,"['Communication Skills', 'Data Replication mechanism', 'staffing plan', 'RACI', 'Data Engineer', 'Api']",2025-06-12 15:17:24
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.",Information Technology,"['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 15:17:25
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.",Information Technology,"['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 15:17:26
Senior Data Engineer,Virtusa,8-10 Years,,Mumbai,"Sr Developer with special emphasis and experience of 8 to 10 years on Python and Pyspark along with hands on experience on AWS Data components like AWS Glue, Athena etc.,. Also have good knowledge on Data ware house tools to understand the existing system. Candidate should also have experience on Datalake, Teradata and Snowflake. Should be good at terraform.\n8-10 years of experience in designing and developing Python and Pyspark applications\nCreating or maintaining data lake solutions using Snowflake,taradata and other dataware house tools.\nShould have good knowledge and hands on experience on AWS Glue , Athena etc.,\nSound Knowledge on all Data lake concepts and able to work on data migration projects.\nProviding ongoing support and maintenance for applications, including troubleshooting and resolving issues.\nExpertise in practices like Agile, Peer reviews and CICD Pipelines.",Information Technology,"['Athena', 'snowflake', 'Python', 'Pyspark', 'AWS Glue', 'Terraform']",2025-06-12 15:17:28
AWS Data Engineer,Virtusa,6-8 Years,,Chennai,"Sr. AWS Data Engineer\nP3 C3 TSTS\nPrimary Skills\nExperience in data engineering, with a proven focus on data ingestion and extraction using Python/PySpark.\nExtensive AWS experience is mandatory, with proficiency in Glue, Lambda, SQS, SNS, AWS IAM, AWS Step Functions, S3, and RDS (Oracle, Aurora Postgres).\n4+ years of experience working with both relational and non-relational/NoSQL databases is required.\nStrong SQL experience is necessary, demonstrating the ability to write complex queries from scratch. Also, experience in Redshift is required along with other SQL DB experience.\nStrong scripting experience with the ability to build intricate data pipelines using AWS serverless architecture.\nUnderstanding of building an end-to-end Data pipeline.\nSecondary Skills\nStrong understanding of Kinesis, Kafka, CDK. Experience with Kafka and ECS is also required.\nStrong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling is required.\nExperience in Node Js and CDK.\nJD\nResponsibilities\nLead the architectural design and development of a scalable, reliable, and flexible metadata-driven data ingestion and extraction framework on AWS using Python/PySpark.\nDesign and implement a customizable data processing framework using Python/PySpark. This framework should be capable of handling diverse scenarios and evolving data processing requirements.\nImplement data pipeline for data ingestion, transformation, and extraction leveraging the AWS Cloud Services.\nSeamlessly integrate a variety of AWS services, including S3, Glue, Kafka, Lambda, SQL, SNS, Athena, EC2, RDS (Oracle, Postgres, MySQL), AWS Crawler to construct a highly scalable and reliable data ingestion and extraction pipeline.\nFacilitate configuration and extensibility of the framework to adapt to evolving data needs and processing scenarios.\nDevelop and maintain rigorous data quality checks and validation processes to safeguard the integrity of ingested data.\nImplement robust error handling, logging, monitoring, and alerting mechanisms to ensure the reliability of the entire data pipeline.\nQualifications\nMust Have\nOver 6 years of hands-on experience in data engineering, with a proven focus on data ingestion and extraction using Python/PySpark.\nExtensive AWS experience is mandatory, with proficiency in Glue, Lambda, SQS, SNS, AWS IAM, AWS Step Functions, S3, and RDS (Oracle, Aurora Postgres).\n4+ years of experience working with both relational and non-relational/NoSQL databases is required.\nStrong SQL experience is necessary, demonstrating the ability to write complex queries from scratch.\nStrong working experience in Redshift is required along with other SQL DB experience.\nStrong scripting experience with the ability to build intricate data pipelines using AWS serverless architecture.\nComplete understanding of building an end-to-end Data pipeline.\nNice to have\nStrong understanding of Kinesis, Kafka, CDK.\nA strong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling is required.\nExperience in Node Js and CDK.\nExperience with Kafka and ECS is also required.\nQualification\nAWS Data Engineer\nWorkmode: Hybrid\nWork location: PAN INDIA\nWork Timing: 2 PM to 11 PM\nPrimary Skill: Data Engineer\nExperience in data engineering, with a proven focus on data ingestion and extraction using Python/PySpark.\nExtensive AWS experience is mandatory, with proficiency in Glue, Lambda, SQS, SNS, AWS IAM, AWS Step Functions, S3, and RDS (Oracle, Aurora Postgres).\n4+ years of experience working with both relational and non-relational/NoSQL databases is required.\nStrong SQL experience is necessary, demonstrating the ability to write complex queries from scratch. Also, experience in Redshift is required along with other SQL DB experience.\nStrong scripting experience with the ability to build intricate data pipelines using AWS serverless architecture.\nUnderstanding of building an end-to-end Data pipeline.\nSecondary Skills\nStrong understanding of Kinesis, Kafka, CDK. Experience with Kafka and ECS is also required.\nStrong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling is required.\nExperience in Node Js and CDK.\nJD Responsibilities\nLead the architectural design and development of a scalable, reliable, and flexible metadata-driven data ingestion and extraction framework on AWS using Python/PySpark.\nDesign and implement a customizable data processing framework using Python/PySpark. This framework should be capable of handling diverse scenarios and evolving data processing requirements.\nImplement data pipeline for data ingestion, transformation, and extraction leveraging the AWS Cloud Services.\nSeamlessly integrate a variety of AWS services, including S3, Glue, Kafka, Lambda, SQL, SNS, Athena, EC2, RDS (Oracle, Postgres, MySQL), AWS Crawler to construct a highly scalable and reliable data ingestion and extraction pipeline.\nFacilitate configuration and extensibility of the framework to adapt to evolving data needs and processing scenarios.\nDevelop and maintain rigorous data quality checks and validation processes to safeguard the integrity of ingested data.\nImplement robust error handling, logging, monitoring, and alerting mechanisms to ensure the reliability of the entire data pipeline.\nQualifications\nMust Have\nOver 6 years of hands-on experience in data engineering, with a proven focus on data ingestion and extraction using Python/PySpark.\nExtensive AWS experience is mandatory, with proficiency in Glue, Lambda, SQS, SNS, AWS IAM, AWS Step Functions, S3, and RDS (Oracle, Aurora Postgres).\n4+ years of experience working with both relational and non-relational/NoSQL databases is required.\nStrong SQL experience is necessary, demonstrating the ability to write complex queries from scratch.\nStrong working experience in Redshift is required along with other SQL DB experience.\nStrong scripting experience with the ability to build intricate data pipelines using AWS serverless architecture.\nComplete understanding of building an end-to-end Data pipeline.\nNice to have\nStrong understanding of Kinesis, Kafka, CDK.\nA strong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling is required.\nExperience in Node Js and CDK.\nExperience with Kafka and ECS is also required.",Information Technology,"['aws glue', 'Pyspark', 'Redshift', 'Sql', 'Lambda', 'Kafka']",2025-06-12 15:17:29
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.",Information Technology,"['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 15:17:30
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.",Information Technology,"['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 15:17:39
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.",Information Technology,"['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 15:17:44
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.",Information Technology,"['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 15:17:46
Senior Data Engineer,Virtusa,8-10 Years,,Mumbai,"Sr Developer with special emphasis and experience of 8 to 10 years on Python and Pyspark along with hands on experience on AWS Data components like AWS Glue, Athena etc.,. Also have good knowledge on Data ware house tools to understand the existing system. Candidate should also have experience on Datalake, Teradata and Snowflake. Should be good at terraform.\n8-10 years of experience in designing and developing Python and Pyspark applications\nCreating or maintaining data lake solutions using Snowflake,taradata and other dataware house tools.\nShould have good knowledge and hands on experience on AWS Glue , Athena etc.,\nSound Knowledge on all Data lake concepts and able to work on data migration projects.\nProviding ongoing support and maintenance for applications, including troubleshooting and resolving issues.\nExpertise in practices like Agile, Peer reviews and CICD Pipelines.",Information Technology,"['Athena', 'snowflake', 'Python', 'Pyspark', 'AWS Glue', 'Terraform']",2025-06-12 15:17:48
Data Engineer-Data Platforms-Google,IBM,5-10 Years,,Gurugram,"Your role and responsibilities :\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusablemanner, developing predictive or prescriptive models, and evaluating modelling results\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience:\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Information Technology,"['Communication Skills', 'Data Replication mechanism', 'staffing plan', 'RACI', 'Data Engineer', 'Api']",2025-06-12 15:17:49
AWS Data Engineer,Virtusa,6-8 Years,,Chennai,"Sr. AWS Data Engineer\nP3 C3 TSTS\nPrimary Skills\nExperience in data engineering, with a proven focus on data ingestion and extraction using Python/PySpark.\nExtensive AWS experience is mandatory, with proficiency in Glue, Lambda, SQS, SNS, AWS IAM, AWS Step Functions, S3, and RDS (Oracle, Aurora Postgres).\n4+ years of experience working with both relational and non-relational/NoSQL databases is required.\nStrong SQL experience is necessary, demonstrating the ability to write complex queries from scratch. Also, experience in Redshift is required along with other SQL DB experience.\nStrong scripting experience with the ability to build intricate data pipelines using AWS serverless architecture.\nUnderstanding of building an end-to-end Data pipeline.\nSecondary Skills\nStrong understanding of Kinesis, Kafka, CDK. Experience with Kafka and ECS is also required.\nStrong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling is required.\nExperience in Node Js and CDK.\nJD\nResponsibilities\nLead the architectural design and development of a scalable, reliable, and flexible metadata-driven data ingestion and extraction framework on AWS using Python/PySpark.\nDesign and implement a customizable data processing framework using Python/PySpark. This framework should be capable of handling diverse scenarios and evolving data processing requirements.\nImplement data pipeline for data ingestion, transformation, and extraction leveraging the AWS Cloud Services.\nSeamlessly integrate a variety of AWS services, including S3, Glue, Kafka, Lambda, SQL, SNS, Athena, EC2, RDS (Oracle, Postgres, MySQL), AWS Crawler to construct a highly scalable and reliable data ingestion and extraction pipeline.\nFacilitate configuration and extensibility of the framework to adapt to evolving data needs and processing scenarios.\nDevelop and maintain rigorous data quality checks and validation processes to safeguard the integrity of ingested data.\nImplement robust error handling, logging, monitoring, and alerting mechanisms to ensure the reliability of the entire data pipeline.\nQualifications\nMust Have\nOver 6 years of hands-on experience in data engineering, with a proven focus on data ingestion and extraction using Python/PySpark.\nExtensive AWS experience is mandatory, with proficiency in Glue, Lambda, SQS, SNS, AWS IAM, AWS Step Functions, S3, and RDS (Oracle, Aurora Postgres).\n4+ years of experience working with both relational and non-relational/NoSQL databases is required.\nStrong SQL experience is necessary, demonstrating the ability to write complex queries from scratch.\nStrong working experience in Redshift is required along with other SQL DB experience.\nStrong scripting experience with the ability to build intricate data pipelines using AWS serverless architecture.\nComplete understanding of building an end-to-end Data pipeline.\nNice to have\nStrong understanding of Kinesis, Kafka, CDK.\nA strong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling is required.\nExperience in Node Js and CDK.\nExperience with Kafka and ECS is also required.\nQualification\nAWS Data Engineer\nWorkmode: Hybrid\nWork location: PAN INDIA\nWork Timing: 2 PM to 11 PM\nPrimary Skill: Data Engineer\nExperience in data engineering, with a proven focus on data ingestion and extraction using Python/PySpark.\nExtensive AWS experience is mandatory, with proficiency in Glue, Lambda, SQS, SNS, AWS IAM, AWS Step Functions, S3, and RDS (Oracle, Aurora Postgres).\n4+ years of experience working with both relational and non-relational/NoSQL databases is required.\nStrong SQL experience is necessary, demonstrating the ability to write complex queries from scratch. Also, experience in Redshift is required along with other SQL DB experience.\nStrong scripting experience with the ability to build intricate data pipelines using AWS serverless architecture.\nUnderstanding of building an end-to-end Data pipeline.\nSecondary Skills\nStrong understanding of Kinesis, Kafka, CDK. Experience with Kafka and ECS is also required.\nStrong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling is required.\nExperience in Node Js and CDK.\nJD Responsibilities\nLead the architectural design and development of a scalable, reliable, and flexible metadata-driven data ingestion and extraction framework on AWS using Python/PySpark.\nDesign and implement a customizable data processing framework using Python/PySpark. This framework should be capable of handling diverse scenarios and evolving data processing requirements.\nImplement data pipeline for data ingestion, transformation, and extraction leveraging the AWS Cloud Services.\nSeamlessly integrate a variety of AWS services, including S3, Glue, Kafka, Lambda, SQL, SNS, Athena, EC2, RDS (Oracle, Postgres, MySQL), AWS Crawler to construct a highly scalable and reliable data ingestion and extraction pipeline.\nFacilitate configuration and extensibility of the framework to adapt to evolving data needs and processing scenarios.\nDevelop and maintain rigorous data quality checks and validation processes to safeguard the integrity of ingested data.\nImplement robust error handling, logging, monitoring, and alerting mechanisms to ensure the reliability of the entire data pipeline.\nQualifications\nMust Have\nOver 6 years of hands-on experience in data engineering, with a proven focus on data ingestion and extraction using Python/PySpark.\nExtensive AWS experience is mandatory, with proficiency in Glue, Lambda, SQS, SNS, AWS IAM, AWS Step Functions, S3, and RDS (Oracle, Aurora Postgres).\n4+ years of experience working with both relational and non-relational/NoSQL databases is required.\nStrong SQL experience is necessary, demonstrating the ability to write complex queries from scratch.\nStrong working experience in Redshift is required along with other SQL DB experience.\nStrong scripting experience with the ability to build intricate data pipelines using AWS serverless architecture.\nComplete understanding of building an end-to-end Data pipeline.\nNice to have\nStrong understanding of Kinesis, Kafka, CDK.\nA strong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling is required.\nExperience in Node Js and CDK.\nExperience with Kafka and ECS is also required.",Information Technology,"['aws glue', 'Pyspark', 'Redshift', 'Sql', 'Lambda', 'Kafka']",2025-06-12 15:17:50
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.",Information Technology,"['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 15:17:51
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.",Information Technology,"['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 15:17:52
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,"Key Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.",Information Technology,"['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 15:17:54
Data Engineer-Data Platforms,IBM,5-8 Years,,Navi Mumbai,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions.\nYou will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.. Good working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala",Software,"['Big Data', 'Hadoop', 'Spark', 'Python', 'Scala', 'Pyspark']",2025-06-12 15:17:56
AWS Data Engineer,Virtusa,6-8 Years,,Chennai,"Sr. AWS Data Engineer\nP3 C3 TSTS\nPrimary Skills\nExperience in data engineering, with a proven focus on data ingestion and extraction using Python/PySpark.\nExtensive AWS experience is mandatory, with proficiency in Glue, Lambda, SQS, SNS, AWS IAM, AWS Step Functions, S3, and RDS (Oracle, Aurora Postgres).\n4+ years of experience working with both relational and non-relational/NoSQL databases is required.\nStrong SQL experience is necessary, demonstrating the ability to write complex queries from scratch. Also, experience in Redshift is required along with other SQL DB experience.\nStrong scripting experience with the ability to build intricate data pipelines using AWS serverless architecture.\nUnderstanding of building an end-to-end Data pipeline.\nSecondary Skills\nStrong understanding of Kinesis, Kafka, CDK. Experience with Kafka and ECS is also required.\nStrong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling is required.\nExperience in Node Js and CDK.\nJD\nResponsibilities\nLead the architectural design and development of a scalable, reliable, and flexible metadata-driven data ingestion and extraction framework on AWS using Python/PySpark.\nDesign and implement a customizable data processing framework using Python/PySpark. This framework should be capable of handling diverse scenarios and evolving data processing requirements.\nImplement data pipeline for data ingestion, transformation, and extraction leveraging the AWS Cloud Services.\nSeamlessly integrate a variety of AWS services, including S3, Glue, Kafka, Lambda, SQL, SNS, Athena, EC2, RDS (Oracle, Postgres, MySQL), AWS Crawler to construct a highly scalable and reliable data ingestion and extraction pipeline.\nFacilitate configuration and extensibility of the framework to adapt to evolving data needs and processing scenarios.\nDevelop and maintain rigorous data quality checks and validation processes to safeguard the integrity of ingested data.\nImplement robust error handling, logging, monitoring, and alerting mechanisms to ensure the reliability of the entire data pipeline.\nQualifications\nMust Have\nOver 6 years of hands-on experience in data engineering, with a proven focus on data ingestion and extraction using Python/PySpark.\nExtensive AWS experience is mandatory, with proficiency in Glue, Lambda, SQS, SNS, AWS IAM, AWS Step Functions, S3, and RDS (Oracle, Aurora Postgres).\n4+ years of experience working with both relational and non-relational/NoSQL databases is required.\nStrong SQL experience is necessary, demonstrating the ability to write complex queries from scratch.\nStrong working experience in Redshift is required along with other SQL DB experience.\nStrong scripting experience with the ability to build intricate data pipelines using AWS serverless architecture.\nComplete understanding of building an end-to-end Data pipeline.\nNice to have\nStrong understanding of Kinesis, Kafka, CDK.\nA strong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling is required.\nExperience in Node Js and CDK.\nExperience with Kafka and ECS is also required.\nQualification\nAWS Data Engineer\nWorkmode: Hybrid\nWork location: PAN INDIA\nWork Timing: 2 PM to 11 PM\nPrimary Skill: Data Engineer\nExperience in data engineering, with a proven focus on data ingestion and extraction using Python/PySpark.\nExtensive AWS experience is mandatory, with proficiency in Glue, Lambda, SQS, SNS, AWS IAM, AWS Step Functions, S3, and RDS (Oracle, Aurora Postgres).\n4+ years of experience working with both relational and non-relational/NoSQL databases is required.\nStrong SQL experience is necessary, demonstrating the ability to write complex queries from scratch. Also, experience in Redshift is required along with other SQL DB experience.\nStrong scripting experience with the ability to build intricate data pipelines using AWS serverless architecture.\nUnderstanding of building an end-to-end Data pipeline.\nSecondary Skills\nStrong understanding of Kinesis, Kafka, CDK. Experience with Kafka and ECS is also required.\nStrong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling is required.\nExperience in Node Js and CDK.\nJD Responsibilities\nLead the architectural design and development of a scalable, reliable, and flexible metadata-driven data ingestion and extraction framework on AWS using Python/PySpark.\nDesign and implement a customizable data processing framework using Python/PySpark. This framework should be capable of handling diverse scenarios and evolving data processing requirements.\nImplement data pipeline for data ingestion, transformation, and extraction leveraging the AWS Cloud Services.\nSeamlessly integrate a variety of AWS services, including S3, Glue, Kafka, Lambda, SQL, SNS, Athena, EC2, RDS (Oracle, Postgres, MySQL), AWS Crawler to construct a highly scalable and reliable data ingestion and extraction pipeline.\nFacilitate configuration and extensibility of the framework to adapt to evolving data needs and processing scenarios.\nDevelop and maintain rigorous data quality checks and validation processes to safeguard the integrity of ingested data.\nImplement robust error handling, logging, monitoring, and alerting mechanisms to ensure the reliability of the entire data pipeline.\nQualifications\nMust Have\nOver 6 years of hands-on experience in data engineering, with a proven focus on data ingestion and extraction using Python/PySpark.\nExtensive AWS experience is mandatory, with proficiency in Glue, Lambda, SQS, SNS, AWS IAM, AWS Step Functions, S3, and RDS (Oracle, Aurora Postgres).\n4+ years of experience working with both relational and non-relational/NoSQL databases is required.\nStrong SQL experience is necessary, demonstrating the ability to write complex queries from scratch.\nStrong working experience in Redshift is required along with other SQL DB experience.\nStrong scripting experience with the ability to build intricate data pipelines using AWS serverless architecture.\nComplete understanding of building an end-to-end Data pipeline.\nNice to have\nStrong understanding of Kinesis, Kafka, CDK.\nA strong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling is required.\nExperience in Node Js and CDK.\nExperience with Kafka and ECS is also required.",Information Technology,"['aws glue', 'Pyspark', 'Redshift', 'Sql', 'Lambda', 'Kafka']",2025-06-12 15:17:57
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.",Information Technology,"['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 15:17:58
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,"Key Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.",Information Technology,"['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 15:18:00
Data Engineer-Data Platforms,IBM,0-5 Years,,Bengaluru,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs a Big Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns.\nYou'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nAs a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions.\nYou will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nBig Data Developer, Hadoop, Hive, Spark, PySpark, Strong SQL.\nAbility to incorporate a variety of statistical and machine learning techniques. Basic understanding of Cloud (AWS,Azure, etc).\nAbility to use programming languages like Java, Python, Scala, etc., to build pipelines to extract and transform data from a repository to a data consumer\nAbility to use Extract, Transform, and Load (ETL) tools and/or data integration, or federation tools to prepare and transform data as needed.\nAbility to use leading edge tools such as Linux, SQL, Python, Spark, Hadoop and Java\nPreferred technical and professional experience\nBasic understanding or experience with predictive/prescriptive modeling skills\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions",Software,"['Hadoop Spark', 'Data Pipelines', 'Big Data', 'Pyspark', 'Sql Programming', 'Etl Tools']",2025-06-12 15:18:01
Data Engineer-Data Integration,IBM,0-5 Years,,Pune,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviour's.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nExpertise in designing and implementing scalable data warehouse solutions on Snowflake, including schema design, performance tuning, and query optimization.\nStrong experience in building data ingestion and transformation pipelines using Talend to process structured and unstructured data from various sources.\nProficiency in integrating data from cloud platforms into Snowflake using Talend and native Snowflake capabilities.\nHands-on experience with dimensional and relational data modelling techniques to support analytics and reporting requirements\nPreferred technical and professional experience\nUnderstanding of optimizing Snowflake workloads, including clustering keys, caching strategies, and query profiling.\nAbility to implement robust data validation, cleansing, and governance frameworks within ETL processes.\nProficiency in SQL and/or Shell scripting for custom transformations and automation tasks",Software,"['snowflake', 'Data Pipelines', 'Talend', 'Data Modeling', 'Sql', 'Etl']",2025-06-12 15:18:02
Data Engineer,Virtusa,4-6 Years,,Bengaluru,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools.",Information Technology,"['CI & CD', 'Cloudera Data platform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker']",2025-06-12 15:18:09
Data Engineer,Virtusa,5-8 Years,,Chennai,8 Years overall IT experience with minimum 5 years of work experience in below tech skills\nStrong experience in Python Scripting and PySpark for data processing\nProficiency in SQL dealing with big data over Informatica ETL\nProven experience in Data quality and data optimization of data lake in Iceberg format with strong understanding of architecture\nExperience in AWS Glue jobs\nExperience in AWS cloud platform and its data services S3 Redshift Lambda EMR Airflow Postgres SNS Event bridge\nExpertise in BASH Shell scripting\nStrong understanding of healthcare data systems and experience leading data engineering teams\nExperience in Agile environments\nExcellent problem solving skills and attention to detail\nEffective communication and collaboration skills\nResponsibilities\nLeads development of data pipelines and architectures that handle large scale data sets\nDesigns constructs and tests data architecture aligned with business requirements\nProvides technical leadership for data projects ensuring best practices and high quality data solutions\nCollaborates with product finance and other business units to ensure data pipelines meet business requirements\nWork with DBT Data Build Tool for transforming raw data into actionable insights\nOversees development of data solutions that enable predictive and prescriptive analytics\nEnsures the technical quality of solutions managing data as it moves across environments\nAligns data architecture to Healthfirst solution architecture,Information Technology,"['Iceberg', 'Python', 'Pyspark', 'Sql', 'AWS Glue', 'Informatica']",2025-06-12 15:18:11
Data Engineer,Virtusa,5-8 Years,,Chennai,8 Years overall IT experience with minimum 5 years of work experience in below tech skills\nStrong experience in Python Scripting and PySpark for data processing\nProficiency in SQL dealing with big data over Informatica ETL\nProven experience in Data quality and data optimization of data lake in Iceberg format with strong understanding of architecture\nExperience in AWS Glue jobs\nExperience in AWS cloud platform and its data services S3 Redshift Lambda EMR Airflow Postgres SNS Event bridge\nExpertise in BASH Shell scripting\nStrong understanding of healthcare data systems and experience leading data engineering teams\nExperience in Agile environments\nExcellent problem solving skills and attention to detail\nEffective communication and collaboration skills\nResponsibilities\nLeads development of data pipelines and architectures that handle large scale data sets\nDesigns constructs and tests data architecture aligned with business requirements\nProvides technical leadership for data projects ensuring best practices and high quality data solutions\nCollaborates with product finance and other business units to ensure data pipelines meet business requirements\nWork with DBT Data Build Tool for transforming raw data into actionable insights\nOversees development of data solutions that enable predictive and prescriptive analytics\nEnsures the technical quality of solutions managing data as it moves across environments\nAligns data architecture to Healthfirst solution architecture,Information Technology,"['Iceberg', 'Python', 'Pyspark', 'Sql', 'AWS Glue', 'Informatica']",2025-06-12 15:18:12
Data Engineer,Virtusa,8-12 Years,,Hyderabad,"Experience:\n8+ years of experience in data engineering, specifically in cloud environments like AWS.\nProficiency in PySpark for distributed data processing and transformation.\nSolid experience with AWS Glue for ETL jobs and managing data workflows.\nHands-on experience with AWS Data Pipeline (DPL) for workflow orchestration.\nStrong experience with AWS services such as S3, Lambda, Redshift, RDS, and EC2.\nTechnical Skills:\nProficiency in Python and PySpark for data processing and transformation tasks.\nDeep understanding of ETL concepts and best practices.\nFamiliarity with AWS Glue (ETL jobs, Data Catalog, and Crawlers).\nExperience building and maintaining data pipelines with AWS Data Pipeline or similar orchestration tools.\nFamiliarity with AWS S3 for data storage and management, including file formats (CSV, Parquet, Avro).\nStrong knowledge of SQL for querying and manipulating relational and semi-structured data.\nExperience with Data Warehousing and Big Data technologies, specifically within AWS.\nAdditional Skills:\nExperience with AWS Lambda for serverless data processing and orchestration.\nUnderstanding of AWS Redshift for data warehousing and analytics.\nFamiliarity with Data Lakes, Amazon EMR, and Kinesis for streaming data processing.\nKnowledge of data governance practices, including data lineage and auditing.\nFamiliarity with CI/CD pipelines and Git for version control.\nExperience with Docker and containerization for building and deploying applications.\nDesign and Build Data Pipelines: Design, implement, and optimize data pipelines on AWS using PySpark, AWS Glue, and AWS Data Pipeline to automate data integration, transformation, and storage processes.\nETL Development: Develop and maintain Extract, Transform, and Load (ETL) processes using AWS Glue and PySpark to efficiently process large datasets.\nData Workflow Automation: Build and manage automated data workflows using AWS Data Pipeline, ensuring seamless scheduling, monitoring, and management of data jobs.\nData Integration: Work with different AWS data storage services (e.g., S3, Redshift, RDS) to ensure smooth integration and movement of data across platforms.\nOptimization and Scaling: Optimize and scale data pipelines for high performance and cost efficiency, utilizing AWS services like Lambda, S3, and EC2.",Information Technology,"['Snowflake design', 'ETL/ELT', 'Python', 'Sql', 'Data Warehousing', 'cloud platform']",2025-06-12 15:18:14
Data Engineer,Accenture India,5-10 Years,,Jaipur,"Job Description: Data Engineer\nProject Role: Data Engineer\nProject Role Description: Design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform, and load) processes to migrate and deploy data across systems.\nMust-Have Skills: SAP HCM On Premise ABAP\nGood-to-Have Skills: SAP HCM Organizational Management\nExperience Required: Minimum 5 years\nEducational Qualification: 15 years full-time education\nJob Summary:\nAs a Data Engineer, your main responsibility will be to design, develop, and maintain data solutions for generating, collecting, and processing data. You'll work on creating data pipelines, ensuring data quality, and implementing ETL processes to move and deploy data across systems. Your role will be crucial in managing and analyzing data to support business decisions and provide data-driven insights.\nRoles & Responsibilities:\nSubject Matter Expert (SME): Expected to be an SME, collaborate and manage the team to deliver results. Responsible for making key team decisions.\nCollaboration & Teamwork: Work closely with multiple teams and contribute to important decisions and problem-solving efforts.\nData Solutions: Design, develop, and maintain data solutions that support data generation, collection, and processing.\nData Pipelines: Create and manage data pipelines to extract, transform, and load data (ETL processes) across systems.\nData Quality & Integrity: Ensure data quality and integrity throughout the data lifecycle.\nETL Processes: Implement and optimize ETL processes to efficiently migrate and deploy data across systems and platforms.\nProfessional & Technical Skills:\nMust-Have Skills:\nStrong proficiency in SAP HCM On Premise ABAP.\nExperience with data modeling and database design.\nFamiliarity with ETL tools and processes.\nExpertise in programming languages such as ABAP, SQL, and Python.\nGood-to-Have Skills:\nExperience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nAdditional Information:\nExperience Required: A minimum of 5 years of experience in SAP HCM On Premise ABAP.\nEducational Requirement: The candidate should have a minimum of 15 years of full-time education.","Consulting, Information Services","['Sql', 'Python', 'Abap']",2025-06-12 15:18:15
Data Engineer,Virtusa,10-12 Years,,Bengaluru,"Requirements\n10+ years of strong experience with data transformation & ETL on large data sets\nExperience with designing customer centric datasets (i.e., CRM, Call Center, Marketing, Offline, Point of\nSale etc.)\n5+ years of Data Modeling experience (i.e., Relational, Dimensional, Columnar, Big Data)\n5+ years of complex SQL or NoSQL experience\nExperience in advanced Data Warehouse concepts\nExperience in industry ETL tools (i.e., Informatica, Unifi)\nExperience with Business Requirements definition and management, structured analysis, process\ndesign, use case documentation\nExperience with Reporting Technologies (i.e., Tableau, PowerBI)\nexperience in professional software development\nDemonstrate exceptional organizational skills and ability to multi-task simultaneous different customer\nprojects\nStrong verbal & written communication skills to interface with Sales team & lead customers to\nsuccessful outcome\nMust be self-managed, proactive and customer focused\nDegree in Computer Science, Information Systems, Data Science, or related field",Information Technology,"['Python', 'Sql', 'Etl', 'BigQuery']",2025-06-12 15:18:16
Data Engineer,Virtusa,8-9 Years,,Bengaluru,"Proficiency in PySpark for distributed data processing and transformation.\nSolid experience with AWS Glue for ETL jobs and managing data workflows.\nHands-on experience with AWS Data Pipeline (DPL) for workflow orchestration.\nStrong experience with AWS services such as S3, Lambda, Redshift, RDS, and EC2.\nTechnical Skills:\nProficiency in Python and PySpark for data processing and transformation tasks.\nDeep understanding of ETL concepts and best practices.\nFamiliarity with AWS Glue (ETL jobs, Data Catalog, and Crawlers).\nExperience building and maintaining data pipelines with AWS Data Pipeline or similar orchestration tools.\nFamiliarity with AWS S3 for data storage and management, including file formats (CSV, Parquet, Avro).\nStrong knowledge of SQL for querying and manipulating relational and semi-structured data.\nExperience with Data Warehousing and Big Data technologies, specifically within AWS.\nAdditional Skills:\nExperience with AWS Lambda for serverless data processing and orchestration.\nUnderstanding of AWS Redshift for data warehousing and analytics.\nFamiliarity with Data Lakes, Amazon EMR, and Kinesis for streaming data processing.\nKnowledge of data governance practices, including data lineage and auditing.\nFamiliarity with CI/CD pipelines and Git for version control.\nExperience with Docker and containerization for building and deploying applications.\nDesign and Build Data Pipelines: Design, implement, and optimize data pipelines on AWS using PySpark, AWS Glue, and AWS Data Pipeline to automate data integration, transformation, and storage processes.\nETL Development: Develop and maintain Extract, Transform, and Load (ETL) processes using AWS Glue and PySpark to efficiently process large datasets.\nData Workflow Automation: Build and manage automated data workflows using AWS Data Pipeline, ensuring seamless scheduling, monitoring, and management of data jobs.\nData Integration: Work with different AWS data storage services (e.g., S3, Redshift, RDS) to ensure smooth integration and movement of data across platforms.\nOptimization and Scaling: Optimize and scale data pipelines for high performance and cost efficiency, utilizing AWS services like Lambda, S3, and EC2.",Information Technology,"['CI & CD', 'Cloudera Data platform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker']",2025-06-12 15:18:17
Data Engineer,Synechron Technologies Private Limited,6-8 Years,,Gurugram,"Job Profile : Data and analytics expert with 6-8 years of experience, proficient in big data technologies, data architecture, and customer-focused solutions.\nTechnology\nIn addition, you'll bring a selection of the following technical skills with an experience around 6-8 years.\nResponsibilities:\nDesign and implement data pipelines, data lakes, and visualization solutions to create business value.\nDevelop and maintain big data solutions using technologies like AWS Redshift, Kubernetes, Spark, Python, SQL, DBT, and Dataiku.\nApply best practices in data architecture, modeling, and metadata management.\nAnalyze and troubleshoot complex data issues, ensuring timely resolution..\nPrimary Skills:\n6-8 years of experience in data and analytics.\nProficiency in big data technologies: AWS Redshift, Kubernetes, Spark, Python, complex SQL, DBT, and Dataiku.\nStrong understanding of data architecture, modeling, and metadata management.\nExcellent data analytical and troubleshooting skills.\nStrong communication skills and a customer-focused approach\nGood to have:\nExperience with other data-related tools and platforms.\nKnowledge of emerging trends in data and analytics\nExperience: 6 to 8 years",Software Engineering,"['Aws Redshift', 'Spark', 'Kubernetes', 'Sql', 'Python']",2025-06-12 15:18:18
Data Engineer,Virtusa,7-12 Years,,Chennai,"Job Title: Data Engineer\n\nExperience Level\n6+ years in Data Engineering\nKey Responsibilities\nAs a Data Engineer, you will:\nData Engineering: Apply expertise in Snowflake, DBT, and Python to develop and manage data solutions.\nCloud Integration: Utilize AWS Cloud services, specifically AWS S3 and Lambda, for data operations.\nSQL Development: Leverage strong SQL knowledge for data manipulation and querying.\nMandatory Skills & Experience\nTechnical Proficiency:\nSQL: Strong knowledge in SQL.\nData Warehousing & Transformation: Expertise in Snowflake and DBT (minimum 2+ years of experience).\nProgramming: Expertise in Python (minimum 2+ years of experience).\nCloud Platform: Knowledge in AWS Cloud (specifically AWS S3 & Lambda).\nETL/ELT Tools (Desirable): Knowledge of SnapLogic or Fivetran tools is an added advantage.\nExperience & Qualifications:\n6+ years of experience in data engineering.",Information Technology,"['Snowflake design', 'ETL/ELT', 'Python', 'Sql', 'Data Warehousing', 'cloud platform']",2025-06-12 15:18:19
Data Engineer,Virtusa,7-12 Years,,Chennai,"Job Title: Data Engineer\n\nExperience Level\n6+ years in Data Engineering\nKey Responsibilities\nAs a Data Engineer, you will:\nData Engineering: Apply expertise in Snowflake, DBT, and Python to develop and manage data solutions.\nCloud Integration: Utilize AWS Cloud services, specifically AWS S3 and Lambda, for data operations.\nSQL Development: Leverage strong SQL knowledge for data manipulation and querying.\nMandatory Skills & Experience\nTechnical Proficiency:\nSQL: Strong knowledge in SQL.\nData Warehousing & Transformation: Expertise in Snowflake and DBT (minimum 2+ years of experience).\nProgramming: Expertise in Python (minimum 2+ years of experience).\nCloud Platform: Knowledge in AWS Cloud (specifically AWS S3 & Lambda).\nETL/ELT Tools (Desirable): Knowledge of SnapLogic or Fivetran tools is an added advantage.\nExperience & Qualifications:\n6+ years of experience in data engineering.",Information Technology,"['Snowflake design', 'ETL/ELT', 'Python', 'Sql', 'Data Warehousing', 'cloud platform']",2025-06-12 15:18:20
Data Engineer,Virtusa,5-8 Years,,Chennai,8 Years overall IT experience with minimum 5 years of work experience in below tech skills\nStrong experience in Python Scripting and PySpark for data processing\nProficiency in SQL dealing with big data over Informatica ETL\nProven experience in Data quality and data optimization of data lake in Iceberg format with strong understanding of architecture\nExperience in AWS Glue jobs\nExperience in AWS cloud platform and its data services S3 Redshift Lambda EMR Airflow Postgres SNS Event bridge\nExpertise in BASH Shell scripting\nStrong understanding of healthcare data systems and experience leading data engineering teams\nExperience in Agile environments\nExcellent problem solving skills and attention to detail\nEffective communication and collaboration skills\nResponsibilities\nLeads development of data pipelines and architectures that handle large scale data sets\nDesigns constructs and tests data architecture aligned with business requirements\nProvides technical leadership for data projects ensuring best practices and high quality data solutions\nCollaborates with product finance and other business units to ensure data pipelines meet business requirements\nWork with DBT Data Build Tool for transforming raw data into actionable insights\nOversees development of data solutions that enable predictive and prescriptive analytics\nEnsures the technical quality of solutions managing data as it moves across environments\nAligns data architecture to Healthfirst solution architecture,Information Technology,"['Iceberg', 'Python', 'Pyspark', 'Sql', 'AWS Glue', 'Informatica']",2025-06-12 15:18:21
Data Engineer,Virtusa,6-8 Years,,Bengaluru,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools",Information Technology,"['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 15:18:23
Data Engineer,Virtusa,6-10 Years,,Bengaluru,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools",Information Technology,"['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark', 'Aws']",2025-06-12 15:18:24
Data Engineer-Data Warehouse,IBM,5-8 Years,,Bengaluru,"IBM Consulting Overview In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour Role and Responsibilities As an Associate Software Developer at IBM, you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in selecting the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle challenges related to database integration and work with complex, unstructured data sets.\nPrimary Responsibilities:\nImplement and validate predictive models, as well as create and maintain statistical models with a focus on big data, incorporating various machine learning techniques.\nDesign and implement enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWork in an Agile, collaborative environment, partnering with scientists, engineers, consultants, and database administrators across disciplines to apply analytical rigor to predictive modeling challenges.\nDevelop teams or write programs to cleanse and integrate data efficiently and develop predictive or prescriptive models.\nEvaluate modeling results to ensure accuracy and effectiveness.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nStrong experience in SQL.\nStrong experience in DBT (Data Build Tool).\nStrong understanding of Data Warehousing concepts.\nStrong experience in AWS or other cloud platforms.\nRedshift experience is a plus.\nPreferred Technical and Professional Experience\nAbility to thrive in teamwork settings with excellent verbal and written communication skills.\nCapability to communicate with internal and external clients to understand business needs and provide analytical solutions.\nAbility to present results to both technical and non-technical audiences effectively.",Information Technology,"['dbt', 'Cloud Knowledge', 'Sql', 'Data Warehousing', 'AWS', 'Redshift']",2025-06-12 15:18:25
Data Engineer,Accenture India,5-10 Years,,Ahmedabad,"Project Role: Data Engineer\nProject Role Description:\nAs a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL (Extract, Transform, Load) processes to migrate and deploy data across systems.\nMust Have Skills:\nProficiency in Data Engineering\nGood to Have Skills:\nN/A\nMinimum Experience Required:\n5 years\nEducational Qualification:\n15 years of full-time education\nSummary:\nYou will play a crucial role in managing and optimizing data infrastructure to support the organization's data needs.\nRoles & Responsibilities:\nExpected to be a Subject Matter Expert (SME) and collaborate with the team.\nResponsible for making team decisions and engaging with multiple teams to contribute to key decisions.\nProvide solutions to problems for your immediate team and across multiple teams.\nDevelop and maintain data pipelines for efficient data processing.\nEnsure data quality and integrity throughout the data lifecycle.\nImplement ETL processes to migrate and deploy data across systems.\nOptimize data infrastructure to support the organization's data needs.\nProfessional & Technical Skills:\nStrong understanding of data generation, collection, and processing.\nExperience in designing and implementing data pipelines.\nKnowledge of ETL processes.\nFamiliarity with data quality assurance and data governance practices.\nAdditional Information:\nThe candidate should have a minimum of 5 years of experience in Data Engineering.\nThis position is based at our Ahmedabad office.\nA 15 years full-time education is required.","Consulting, Information Services","['Nosql', 'Sql', 'Etl']",2025-06-12 15:18:26
Data Engineer,Virtusa,7-12 Years,,Chennai,"Job Title: Data Engineer\n\nExperience Level\n6+ years in Data Engineering\nKey Responsibilities\nAs a Data Engineer, you will:\nData Engineering: Apply expertise in Snowflake, DBT, and Python to develop and manage data solutions.\nCloud Integration: Utilize AWS Cloud services, specifically AWS S3 and Lambda, for data operations.\nSQL Development: Leverage strong SQL knowledge for data manipulation and querying.\nMandatory Skills & Experience\nTechnical Proficiency:\nSQL: Strong knowledge in SQL.\nData Warehousing & Transformation: Expertise in Snowflake and DBT (minimum 2+ years of experience).\nProgramming: Expertise in Python (minimum 2+ years of experience).\nCloud Platform: Knowledge in AWS Cloud (specifically AWS S3 & Lambda).\nETL/ELT Tools (Desirable): Knowledge of SnapLogic or Fivetran tools is an added advantage.\nExperience & Qualifications:\n6+ years of experience in data engineering.",Information Technology,"['Snowflake design', 'ETL/ELT', 'Python', 'Sql', 'Data Warehousing', 'cloud platform']",2025-06-12 15:18:35
Data Engineer,IBM,3-10 Years,,"Navi Mumbai, Mumbai City, Mumbai","Role Overview:\nAs a Big Data Engineer, you'll design and build robust data pipelines on Cloudera using Spark (Scala/PySpark) for ingestion, transformation, and processing of high-volume data from banking systems.\nKey Responsibilities:\nBuild scalable batch and real-time ETL pipelines using Spark and Hive\nIntegrate structured and unstructured data sources\nPerform performance tuning and code optimization\nSupport orchestration and job scheduling (NiFi, Airflow)\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nExperience: 315 years\nProficiency in PySpark/Scala with Hive/Impala\nExperience with data partitioning, bucketing, and optimization\nFamiliarity with Kafka, Iceberg, NiFi is a must\nKnowledge of banking or financial datasets is a plus",Information Technology,"['Sql', 'Python', 'Hadoop', 'Etl', 'Data Warehousing', 'Nosql', 'Cloud Computing', 'Data Modeling']",2025-06-12 15:18:40
Data Engineer,IBM,3-10 Years,,"Navi Mumbai, Mumbai City, Mumbai","Role Overview:\nAs a Big Data Engineer, you'll design and build robust data pipelines on Cloudera using Spark (Scala/PySpark) for ingestion, transformation, and processing of high-volume data from banking systems.\nKey Responsibilities:\nBuild scalable batch and real-time ETL pipelines using Spark and Hive\nIntegrate structured and unstructured data sources\nPerform performance tuning and code optimization\nSupport orchestration and job scheduling (NiFi, Airflow)\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nExperience: 315 years\nProficiency in PySpark/Scala with Hive/Impala\nExperience with data partitioning, bucketing, and optimization\nFamiliarity with Kafka, Iceberg, NiFi is a must\nKnowledge of banking or financial datasets is a plus",Information Technology,"['Sql', 'Python', 'Hadoop', 'Etl', 'Data Warehousing', 'Nosql', 'Cloud Computing', 'Data Modeling']",2025-06-12 15:18:41
Data Engineer,Accenture India,0-2 Years,,Ahmedabad,"Project Role: Data Engineer\nProject Role Description:\nAs a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL (Extract, Transform, Load) processes to migrate and deploy data across systems.\nMust Have Skills:\nProficiency in Data Engineering\nGood to Have Skills:\nExperience with big data technologies (Hadoop, Spark)\nMinimum Experience Required:\n0-2 years\nEducational Qualification:\n15 years of full-time education\nSummary:\nIn this role, you will manage and optimize data infrastructure to support the organization's data needs, ensuring data integrity and quality throughout the processes.\nRoles & Responsibilities:\nBuild knowledge and support the team.\nParticipate in problem-solving discussions.\nDesign and develop data pipelines to extract, transform, and load data from various sources.\nEnsure data quality and integrity by implementing data validation and cleansing processes.\nCollaborate with cross-functional teams to understand data requirements and design efficient data solutions.\nOptimize and tune data pipelines for performance and scalability.\nTroubleshoot and resolve data-related issues and incidents.\nStay updated with the latest trends and technologies in data engineering and recommend improvements to existing processes.\nMentor and guide junior professionals in data engineering best practices.\nProfessional & Technical Skills:\nStrong understanding of data modeling and database design principles.\nExperience with ETL tools (Apache NiFi, Talend).\nFamiliarity with cloud platforms (AWS, Azure).\nKnowledge of data warehousing concepts and techniques.\nExperience with SQL and NoSQL databases.\nSolid understanding of data governance and security principles.\nAdditional Information:\nThe candidate should have a minimum of 0-2 years of experience in Data Engineering.\nThis position is based at our Ahmedabad office.\nA 15 years full-time education is required.","Consulting, Information Services","['Sql', 'Nosql', 'Etl']",2025-06-12 15:18:42
Data Engineer,Virtusa,6-8 Years,,Hyderabad,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools",Information Technology,"['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 15:18:44
Data Engineer,Virtusa,7-12 Years,,Chennai,"Job Title: Data Engineer\n\nExperience Level\n6+ years in Data Engineering\nKey Responsibilities\nAs a Data Engineer, you will:\nData Engineering: Apply expertise in Snowflake, DBT, and Python to develop and manage data solutions.\nCloud Integration: Utilize AWS Cloud services, specifically AWS S3 and Lambda, for data operations.\nSQL Development: Leverage strong SQL knowledge for data manipulation and querying.\nMandatory Skills & Experience\nTechnical Proficiency:\nSQL: Strong knowledge in SQL.\nData Warehousing & Transformation: Expertise in Snowflake and DBT (minimum 2+ years of experience).\nProgramming: Expertise in Python (minimum 2+ years of experience).\nCloud Platform: Knowledge in AWS Cloud (specifically AWS S3 & Lambda).\nETL/ELT Tools (Desirable): Knowledge of SnapLogic or Fivetran tools is an added advantage.\nExperience & Qualifications:\n6+ years of experience in data engineering.",Information Technology,"['Snowflake design', 'ETL/ELT', 'Python', 'Sql', 'Data Warehousing', 'cloud platform']",2025-06-12 15:18:45
Data Engineer,Virtusa,6-10 Years,,Bengaluru,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools",Information Technology,"['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark', 'Aws']",2025-06-12 15:18:47
Data Engineer,Virtusa,6-8 Years,,Bengaluru,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools",Information Technology,"['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 15:18:48
Data Engineer-Data Warehouse,IBM,2-6 Years,,Pune,"Required technical and professional expertise\nDesign and implement efficient database schemas and data models using Teradata.\nOptimize SQL queries and stored procedures for performance.\nPerform database administration tasks including installation, configuration, and maintenance of Teradata systems\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Information Technology,"['Linux', 'Sql', 'Python', 'Spark', 'Hadoop', 'Java']",2025-06-12 15:18:49
Data Engineer-Data Warehouse,IBM,2-6 Years,,Bengaluru,"Introduction\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nA career in IBM Consulting is rooted by long-term relationships and close collaboration with clients across the globe.\nYou'll work with visionaries across multiple industries to improve the hybrid cloud and AI journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio; including Software and Red Hat.\nCuriosity and a constant quest for knowledge serve as the foundation to success in IBM Consulting. In your role, you'll be encouraged to challenge the norm, investigate ideas outside of your role, and come up with creative solutions resulting in ground breaking impact for a wide network of clients. Our culture of evolution and empathy centers on long-term career growth and development opportunities in an environment that embraces your unique skills and experience.\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nGood Hands on experience in DBT is required.\nETL Datastage and snowflake preferred.\nAbility to use programming languages like Java, Python, Scala, etc., to build pipelines to extract and transform data from a repository to a data consumer\nAbility to use Extract, Transform, and Load (ETL) tools and/or data integration, or federation tools to prepare and transform data as needed.\nAbility to use leading edge tools such as Linux, SQL, Python, Spark, Hadoop and Java\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Information Technology,"['Linux', 'Sql', 'Python', 'Spark', 'Hadoop', 'Java']",2025-06-12 15:18:51
Data Engineer-Data Warehouse,IBM,2-6 Years,,Pune,"Introduction\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nA career in IBM Consulting is rooted by long-term relationships and close collaboration with clients across the globe.\nYou'll work with visionaries across multiple industries to improve the hybrid cloud and AI journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio; including Software and Red Hat.\nCuriosity and a constant quest for knowledge serve as the foundation to success in IBM Consulting. In your role, you'll be encouraged to challenge the norm, investigate ideas outside of your role, and come up with creative solutions resulting in ground breaking impact for a wide network of clients. Our culture of evolution and empathy centers on long-term career growth and development opportunities in an environment that embraces your unique skills and experience.\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nGood Hands on experience in DBT is required.\nETL Datastage and snowflake preferred.\nAbility to use programming languages like Java, Python, Scala, etc., to build pipelines to extract and transform data from a repository to a data consumer\nAbility to use Extract, Transform, and Load (ETL) tools and/or data integration, or federation tools to prepare and transform data as needed.\nAbility to use leading edge tools such as Linux, SQL, Python, Spark, Hadoop and Java\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Information Technology,"['Linux', 'Sql', 'Python', 'Spark', 'Hadoop', 'Java']",2025-06-12 15:18:52
Data Engineer-Data Warehouse,IBM,5-8 Years,,Bengaluru,"IBM Consulting Overview In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour Role and Responsibilities As an Associate Software Developer at IBM, you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in selecting the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle challenges related to database integration and work with complex, unstructured data sets.\nPrimary Responsibilities:\nImplement and validate predictive models, as well as create and maintain statistical models with a focus on big data, incorporating various machine learning techniques.\nDesign and implement enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWork in an Agile, collaborative environment, partnering with scientists, engineers, consultants, and database administrators across disciplines to apply analytical rigor to predictive modeling challenges.\nDevelop teams or write programs to cleanse and integrate data efficiently and develop predictive or prescriptive models.\nEvaluate modeling results to ensure accuracy and effectiveness.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nStrong experience in SQL.\nStrong experience in DBT (Data Build Tool).\nStrong understanding of Data Warehousing concepts.\nStrong experience in AWS or other cloud platforms.\nRedshift experience is a plus.\nPreferred Technical and Professional Experience\nAbility to thrive in teamwork settings with excellent verbal and written communication skills.\nCapability to communicate with internal and external clients to understand business needs and provide analytical solutions.\nAbility to present results to both technical and non-technical audiences effectively.",Information Technology,"['dbt', 'Cloud Knowledge', 'Sql', 'Data Warehousing', 'AWS', 'Redshift']",2025-06-12 15:18:53
Data Engineer-Data Warehouse,IBM,5-8 Years,,Cochin / Kochi / Ernakulam,"Introduction\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nResponsible to develop triggers, functions, stored procedures to support this effort\nAssist with impact analysis of changing upstream processes to Data Warehouse and Reporting systems.\nAssist with design, testing, support, and debugging of new and existing ETL and reporting processes.\nPerform data profiling and analysis using a variety of tools. Troubleshoot and support production processes. Create and maintain documentation\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nBE / B Tech in any stream, M.Sc. (Computer Science/IT) / M.C.A, with Minimum 5 plus years of experience\nMust Have : Snowflake, AWS, Complex SQL\nExperience with software architecture in cloud-based infrastructures\nExperience in ETL processes and data modelling techniques\nExperience in designing and managing large scale data warehouses\nPreferred technical and professional experience\nGood to have in addition to Must haves: DBT, Tableau, python, JavaScript\nDevelop complex SQL queries for data analytics and business intelligence.\nBackground working with data analytics, business intelligence, or related fields",Information Technology,"['dbt', 'Tableau', 'Python', 'Javascript', 'Data Analytics', 'Data Warehousing']",2025-06-12 15:18:55
Data Engineer-Data Warehouse,IBM,2-6 Years,,Cochin / Kochi / Ernakulam,"Introduction\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nA career in IBM Consulting is rooted by long-term relationships and close collaboration with clients across the globe.\nYou'll work with visionaries across multiple industries to improve the hybrid cloud and AI journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio; including Software and Red Hat.\nCuriosity and a constant quest for knowledge serve as the foundation to success in IBM Consulting. In your role, you'll be encouraged to challenge the norm, investigate ideas outside of your role, and come up with creative solutions resulting in ground breaking impact for a wide network of clients. Our culture of evolution and empathy centers on long-term career growth and development opportunities in an environment that embraces your unique skills and experience.\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nGood Hands on experience in DBT is required.\nETL Datastage and snowflake preferred.\nAbility to use programming languages like Java, Python, Scala, etc., to build pipelines to extract and transform data from a repository to a data consumer\nAbility to use Extract, Transform, and Load (ETL) tools and/or data integration, or federation tools to prepare and transform data as needed.\nAbility to use leading edge tools such as Linux, SQL, Python, Spark, Hadoop and Java\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Information Technology,"['Linux', 'Sql', 'Python', 'Spark', 'Hadoop', 'Java']",2025-06-12 15:18:56
Data Engineer-Data Warehouse,IBM,2-6 Years,,Pune,"Introduction\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nA career in IBM Consulting is rooted by long-term relationships and close collaboration with clients across the globe.\nYou'll work with visionaries across multiple industries to improve the hybrid cloud and AI journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio; including Software and Red Hat.\nCuriosity and a constant quest for knowledge serve as the foundation to success in IBM Consulting. In your role, you'll be encouraged to challenge the norm, investigate ideas outside of your role, and come up with creative solutions resulting in ground breaking impact for a wide network of clients. Our culture of evolution and empathy centers on long-term career growth and development opportunities in an environment that embraces your unique skills and experience.\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nGood Hands on experience in DBT is required.\nETL Datastage and snowflake preferred.\nAbility to use programming languages like Java, Python, Scala, etc., to build pipelines to extract and transform data from a repository to a data consumer\nAbility to use Extract, Transform, and Load (ETL) tools and/or data integration, or federation tools to prepare and transform data as needed.\nAbility to use leading edge tools such as Linux, SQL, Python, Spark, Hadoop and Java\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Information Technology,"['Linux', 'Sql', 'Python', 'Spark', 'Hadoop', 'Java']",2025-06-12 15:18:57
Data Engineer-Data Platforms,IBM,4-5 Years,,Mumbai,"Your role and responsibilities\nAs a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution.\nYour primary responsibilities include:\nLead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements.\nStrive for continuous improvements by testing the build solution and working under an agile framework.\nDiscover and implement the latest technologies trends to maximize and build creative solutions\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nExperience with Apache Spark (PySpark): In-depth knowledge of Spark's architecture, core APIs, and PySpark for distributed data processing.\nBig Data Technologies: Familiarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering Skills: Strong understanding of ETL pipelines, data modeling, and data warehousing concepts.\nStrong proficiency in Python: Expertise in Python programming with a focus on data processing and manipulation. Data Processing Frameworks: Knowledge of data processing libraries such as Pandas, NumPy.\nSQL Proficiency: Experience writing optimized SQL queries for large-scale data analysis and transformation.\nCloud Platforms: Experience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems\nPreferred technical and professional experience\nDefine, drive, and implement an architecture strategy and standards for end-to-end monitoring.\nPartner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering,\nGood to have detection and prevention tools for Company products and Platform and customer-facing",Information Technology,"['Sql', 'Python', 'Etl', 'Data Warehousing', 'Spark', 'Cloud Computing', 'Data Modeling']",2025-06-12 15:19:07
