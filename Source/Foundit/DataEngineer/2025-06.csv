title,company,experience,salary,location,description,industry,skills,scraped_at
Data Engineer,Virtusa,6-12 Years,,Hyderabad,"Job Title: Senior Data Engineer\nLocation\nHyderabad, Telangana, India\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.",Information Technology,"['Sql', 'Apache Spark', 'Big Data', 'AWS', 'Data Modeling', 'Kafka']",2025-06-12 15:17:13
Data Engineer,Virtusa,6-12 Years,,Chennai,"Job Title: Senior Data Engineer\nLocation\nHyderabad, Telangana, India\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.",Information Technology,"['Sql', 'Apache Spark', 'Big Data', 'AWS', 'Data Modeling', 'Kafka']",2025-06-12 15:17:16
Data Engineer,Virtusa,6-12 Years,,Chennai,"Job Title: Senior Data Engineer\nLocation\nHyderabad, Telangana, India\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.",Information Technology,"['Sql', 'Apache Spark', 'Big Data', 'AWS', 'Data Modeling', 'Kafka']",2025-06-12 15:17:16
Data Engineer,Virtusa,2-4 Years,,Hyderabad,"Key Responsibilities:\nDesign, develop, and maintain ETL/ELT pipelines to move and transform data from various sources.\nBuild and optimize data warehouses, data lakes, and structured/unstructured storage solutions.\nEnsure data quality, integrity, and consistency across systems.\nCollaborate with data analysts and scientists to understand data needs and optimize performance.\nIntegrate APIs, third-party data sources, and internal systems to build robust datasets.\nMonitor and troubleshoot pipeline performance and resolve data-related issues.\nImplement data security, governance, and compliance best practices.\nRequired Skills & Qualifications:\nBachelor's degree in Computer Science, Engineering, Information Systems, or a related field.\n25 years of experience as a Data Engineer or in a related role.\nStrong programming skills in Python, SQL, and optionally Scala or Java.\nExperience with big data tools such as Spark, Hadoop, or Kafka.\nHands-on experience with cloud platforms (e.g., AWS, GCP, or Azure) and services like S3, Redshift, BigQuery, or Snowflake.\nProficiency with data modeling, schema design, and working with relational and NoSQL databases.\nFamiliarity with workflow orchestration tools like Airflow, Luigi, or Prefect.\nStrong understanding of data warehousing concepts and performance optimization.",Information Technology,"['AWS', 'Gcp', 'Azure', 'Redshift', 'BigQuery']",2025-06-12 15:17:17
Data Engineer,Virtusa,2-4 Years,,Hyderabad,"We are seeking a skilled Data Engineer to join our dynamic team in India. The ideal candidate will have experience in building and managing data pipelines, ensuring data quality, and optimizing our data infrastructure to support analytics and business intelligence initiatives.\nResponsibilities\nDesign, construct, install, and maintain large-scale processing systems and data pipelines.\nDevelop and optimize ETL processes to ensure smooth data integration and transformation.\nCollaborate with data scientists and analysts to understand data requirements and deliver solutions that meet their needs.\nImplement data quality checks and ensure data integrity throughout the data lifecycle.\nMonitor performance and troubleshoot data-related issues, enhancing system efficiency and reliability.\nSkills and Qualifications\nBachelor's degree in Computer Science, Engineering, or a related field.\n2-4 years of experience in data engineering or a similar role.\nProficiency in programming languages such as Python, Java, or Scala.\nStrong knowledge of SQL and experience with database management systems (e.g., MySQL, PostgreSQL, MongoDB).\nExperience with big data technologies such as Hadoop, Spark, or Kafka.\nFamiliarity with cloud platforms like AWS, Azure, or Google Cloud for data storage and processing.\nUnderstanding of data modeling concepts and data warehousing principles.\nAbility to work collaboratively in a team environment and communicate effectively with stakeholders.",Information Technology,"['Sql', 'Apache Spark', 'Big Data', 'AWS', 'Data Modeling', 'Kafka']",2025-06-12 15:17:18
Data Engineer,Virtusa,2-4 Years,,Hyderabad,"We are seeking a skilled Data Engineer to join our dynamic team in India. The ideal candidate will have experience in building and managing data pipelines, ensuring data quality, and optimizing our data infrastructure to support analytics and business intelligence initiatives.\nResponsibilities\nDesign, construct, install, and maintain large-scale processing systems and data pipelines.\nDevelop and optimize ETL processes to ensure smooth data integration and transformation.\nCollaborate with data scientists and analysts to understand data requirements and deliver solutions that meet their needs.\nImplement data quality checks and ensure data integrity throughout the data lifecycle.\nMonitor performance and troubleshoot data-related issues, enhancing system efficiency and reliability.\nSkills and Qualifications\nBachelor's degree in Computer Science, Engineering, or a related field.\n2-4 years of experience in data engineering or a similar role.\nProficiency in programming languages such as Python, Java, or Scala.\nStrong knowledge of SQL and experience with database management systems (e.g., MySQL, PostgreSQL, MongoDB).\nExperience with big data technologies such as Hadoop, Spark, or Kafka.\nFamiliarity with cloud platforms like AWS, Azure, or Google Cloud for data storage and processing.\nUnderstanding of data modeling concepts and data warehousing principles.\nAbility to work collaboratively in a team environment and communicate effectively with stakeholders.",Information Technology,"['Sql', 'Apache Spark', 'Big Data', 'AWS', 'Data Modeling', 'Kafka']",2025-06-12 15:17:20
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,"Key Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.",Information Technology,"['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 15:17:21
Data Engineer-Data Platforms,IBM,5-8 Years,,Navi Mumbai,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions.\nYou will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.. Good working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala",Software,"['Big Data', 'Hadoop', 'Spark', 'Python', 'Scala', 'Pyspark']",2025-06-12 15:17:22
Data Engineer-Data Platforms-Google,IBM,5-10 Years,,Gurugram,"Your role and responsibilities :\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusablemanner, developing predictive or prescriptive models, and evaluating modelling results\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience:\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Information Technology,"['Communication Skills', 'Data Replication mechanism', 'staffing plan', 'RACI', 'Data Engineer', 'Api']",2025-06-12 15:17:23
Data Engineer-Data Platforms-Google,IBM,5-10 Years,,Gurugram,"Your role and responsibilities :\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusablemanner, developing predictive or prescriptive models, and evaluating modelling results\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience:\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Information Technology,"['Communication Skills', 'Data Replication mechanism', 'staffing plan', 'RACI', 'Data Engineer', 'Api']",2025-06-12 15:17:24
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.",Information Technology,"['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 15:17:25
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.",Information Technology,"['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 15:17:26
Senior Data Engineer,Virtusa,8-10 Years,,Mumbai,"Sr Developer with special emphasis and experience of 8 to 10 years on Python and Pyspark along with hands on experience on AWS Data components like AWS Glue, Athena etc.,. Also have good knowledge on Data ware house tools to understand the existing system. Candidate should also have experience on Datalake, Teradata and Snowflake. Should be good at terraform.\n8-10 years of experience in designing and developing Python and Pyspark applications\nCreating or maintaining data lake solutions using Snowflake,taradata and other dataware house tools.\nShould have good knowledge and hands on experience on AWS Glue , Athena etc.,\nSound Knowledge on all Data lake concepts and able to work on data migration projects.\nProviding ongoing support and maintenance for applications, including troubleshooting and resolving issues.\nExpertise in practices like Agile, Peer reviews and CICD Pipelines.",Information Technology,"['Athena', 'snowflake', 'Python', 'Pyspark', 'AWS Glue', 'Terraform']",2025-06-12 15:17:28
AWS Data Engineer,Virtusa,6-8 Years,,Chennai,"Sr. AWS Data Engineer\nP3 C3 TSTS\nPrimary Skills\nExperience in data engineering, with a proven focus on data ingestion and extraction using Python/PySpark.\nExtensive AWS experience is mandatory, with proficiency in Glue, Lambda, SQS, SNS, AWS IAM, AWS Step Functions, S3, and RDS (Oracle, Aurora Postgres).\n4+ years of experience working with both relational and non-relational/NoSQL databases is required.\nStrong SQL experience is necessary, demonstrating the ability to write complex queries from scratch. Also, experience in Redshift is required along with other SQL DB experience.\nStrong scripting experience with the ability to build intricate data pipelines using AWS serverless architecture.\nUnderstanding of building an end-to-end Data pipeline.\nSecondary Skills\nStrong understanding of Kinesis, Kafka, CDK. Experience with Kafka and ECS is also required.\nStrong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling is required.\nExperience in Node Js and CDK.\nJD\nResponsibilities\nLead the architectural design and development of a scalable, reliable, and flexible metadata-driven data ingestion and extraction framework on AWS using Python/PySpark.\nDesign and implement a customizable data processing framework using Python/PySpark. This framework should be capable of handling diverse scenarios and evolving data processing requirements.\nImplement data pipeline for data ingestion, transformation, and extraction leveraging the AWS Cloud Services.\nSeamlessly integrate a variety of AWS services, including S3, Glue, Kafka, Lambda, SQL, SNS, Athena, EC2, RDS (Oracle, Postgres, MySQL), AWS Crawler to construct a highly scalable and reliable data ingestion and extraction pipeline.\nFacilitate configuration and extensibility of the framework to adapt to evolving data needs and processing scenarios.\nDevelop and maintain rigorous data quality checks and validation processes to safeguard the integrity of ingested data.\nImplement robust error handling, logging, monitoring, and alerting mechanisms to ensure the reliability of the entire data pipeline.\nQualifications\nMust Have\nOver 6 years of hands-on experience in data engineering, with a proven focus on data ingestion and extraction using Python/PySpark.\nExtensive AWS experience is mandatory, with proficiency in Glue, Lambda, SQS, SNS, AWS IAM, AWS Step Functions, S3, and RDS (Oracle, Aurora Postgres).\n4+ years of experience working with both relational and non-relational/NoSQL databases is required.\nStrong SQL experience is necessary, demonstrating the ability to write complex queries from scratch.\nStrong working experience in Redshift is required along with other SQL DB experience.\nStrong scripting experience with the ability to build intricate data pipelines using AWS serverless architecture.\nComplete understanding of building an end-to-end Data pipeline.\nNice to have\nStrong understanding of Kinesis, Kafka, CDK.\nA strong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling is required.\nExperience in Node Js and CDK.\nExperience with Kafka and ECS is also required.\nQualification\nAWS Data Engineer\nWorkmode: Hybrid\nWork location: PAN INDIA\nWork Timing: 2 PM to 11 PM\nPrimary Skill: Data Engineer\nExperience in data engineering, with a proven focus on data ingestion and extraction using Python/PySpark.\nExtensive AWS experience is mandatory, with proficiency in Glue, Lambda, SQS, SNS, AWS IAM, AWS Step Functions, S3, and RDS (Oracle, Aurora Postgres).\n4+ years of experience working with both relational and non-relational/NoSQL databases is required.\nStrong SQL experience is necessary, demonstrating the ability to write complex queries from scratch. Also, experience in Redshift is required along with other SQL DB experience.\nStrong scripting experience with the ability to build intricate data pipelines using AWS serverless architecture.\nUnderstanding of building an end-to-end Data pipeline.\nSecondary Skills\nStrong understanding of Kinesis, Kafka, CDK. Experience with Kafka and ECS is also required.\nStrong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling is required.\nExperience in Node Js and CDK.\nJD Responsibilities\nLead the architectural design and development of a scalable, reliable, and flexible metadata-driven data ingestion and extraction framework on AWS using Python/PySpark.\nDesign and implement a customizable data processing framework using Python/PySpark. This framework should be capable of handling diverse scenarios and evolving data processing requirements.\nImplement data pipeline for data ingestion, transformation, and extraction leveraging the AWS Cloud Services.\nSeamlessly integrate a variety of AWS services, including S3, Glue, Kafka, Lambda, SQL, SNS, Athena, EC2, RDS (Oracle, Postgres, MySQL), AWS Crawler to construct a highly scalable and reliable data ingestion and extraction pipeline.\nFacilitate configuration and extensibility of the framework to adapt to evolving data needs and processing scenarios.\nDevelop and maintain rigorous data quality checks and validation processes to safeguard the integrity of ingested data.\nImplement robust error handling, logging, monitoring, and alerting mechanisms to ensure the reliability of the entire data pipeline.\nQualifications\nMust Have\nOver 6 years of hands-on experience in data engineering, with a proven focus on data ingestion and extraction using Python/PySpark.\nExtensive AWS experience is mandatory, with proficiency in Glue, Lambda, SQS, SNS, AWS IAM, AWS Step Functions, S3, and RDS (Oracle, Aurora Postgres).\n4+ years of experience working with both relational and non-relational/NoSQL databases is required.\nStrong SQL experience is necessary, demonstrating the ability to write complex queries from scratch.\nStrong working experience in Redshift is required along with other SQL DB experience.\nStrong scripting experience with the ability to build intricate data pipelines using AWS serverless architecture.\nComplete understanding of building an end-to-end Data pipeline.\nNice to have\nStrong understanding of Kinesis, Kafka, CDK.\nA strong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling is required.\nExperience in Node Js and CDK.\nExperience with Kafka and ECS is also required.",Information Technology,"['aws glue', 'Pyspark', 'Redshift', 'Sql', 'Lambda', 'Kafka']",2025-06-12 15:17:29
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.",Information Technology,"['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 15:17:30
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.",Information Technology,"['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 15:17:39
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.",Information Technology,"['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 15:17:44
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.",Information Technology,"['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 15:17:46
Senior Data Engineer,Virtusa,8-10 Years,,Mumbai,"Sr Developer with special emphasis and experience of 8 to 10 years on Python and Pyspark along with hands on experience on AWS Data components like AWS Glue, Athena etc.,. Also have good knowledge on Data ware house tools to understand the existing system. Candidate should also have experience on Datalake, Teradata and Snowflake. Should be good at terraform.\n8-10 years of experience in designing and developing Python and Pyspark applications\nCreating or maintaining data lake solutions using Snowflake,taradata and other dataware house tools.\nShould have good knowledge and hands on experience on AWS Glue , Athena etc.,\nSound Knowledge on all Data lake concepts and able to work on data migration projects.\nProviding ongoing support and maintenance for applications, including troubleshooting and resolving issues.\nExpertise in practices like Agile, Peer reviews and CICD Pipelines.",Information Technology,"['Athena', 'snowflake', 'Python', 'Pyspark', 'AWS Glue', 'Terraform']",2025-06-12 15:17:48
Data Engineer-Data Platforms-Google,IBM,5-10 Years,,Gurugram,"Your role and responsibilities :\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusablemanner, developing predictive or prescriptive models, and evaluating modelling results\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience:\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Information Technology,"['Communication Skills', 'Data Replication mechanism', 'staffing plan', 'RACI', 'Data Engineer', 'Api']",2025-06-12 15:17:49
AWS Data Engineer,Virtusa,6-8 Years,,Chennai,"Sr. AWS Data Engineer\nP3 C3 TSTS\nPrimary Skills\nExperience in data engineering, with a proven focus on data ingestion and extraction using Python/PySpark.\nExtensive AWS experience is mandatory, with proficiency in Glue, Lambda, SQS, SNS, AWS IAM, AWS Step Functions, S3, and RDS (Oracle, Aurora Postgres).\n4+ years of experience working with both relational and non-relational/NoSQL databases is required.\nStrong SQL experience is necessary, demonstrating the ability to write complex queries from scratch. Also, experience in Redshift is required along with other SQL DB experience.\nStrong scripting experience with the ability to build intricate data pipelines using AWS serverless architecture.\nUnderstanding of building an end-to-end Data pipeline.\nSecondary Skills\nStrong understanding of Kinesis, Kafka, CDK. Experience with Kafka and ECS is also required.\nStrong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling is required.\nExperience in Node Js and CDK.\nJD\nResponsibilities\nLead the architectural design and development of a scalable, reliable, and flexible metadata-driven data ingestion and extraction framework on AWS using Python/PySpark.\nDesign and implement a customizable data processing framework using Python/PySpark. This framework should be capable of handling diverse scenarios and evolving data processing requirements.\nImplement data pipeline for data ingestion, transformation, and extraction leveraging the AWS Cloud Services.\nSeamlessly integrate a variety of AWS services, including S3, Glue, Kafka, Lambda, SQL, SNS, Athena, EC2, RDS (Oracle, Postgres, MySQL), AWS Crawler to construct a highly scalable and reliable data ingestion and extraction pipeline.\nFacilitate configuration and extensibility of the framework to adapt to evolving data needs and processing scenarios.\nDevelop and maintain rigorous data quality checks and validation processes to safeguard the integrity of ingested data.\nImplement robust error handling, logging, monitoring, and alerting mechanisms to ensure the reliability of the entire data pipeline.\nQualifications\nMust Have\nOver 6 years of hands-on experience in data engineering, with a proven focus on data ingestion and extraction using Python/PySpark.\nExtensive AWS experience is mandatory, with proficiency in Glue, Lambda, SQS, SNS, AWS IAM, AWS Step Functions, S3, and RDS (Oracle, Aurora Postgres).\n4+ years of experience working with both relational and non-relational/NoSQL databases is required.\nStrong SQL experience is necessary, demonstrating the ability to write complex queries from scratch.\nStrong working experience in Redshift is required along with other SQL DB experience.\nStrong scripting experience with the ability to build intricate data pipelines using AWS serverless architecture.\nComplete understanding of building an end-to-end Data pipeline.\nNice to have\nStrong understanding of Kinesis, Kafka, CDK.\nA strong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling is required.\nExperience in Node Js and CDK.\nExperience with Kafka and ECS is also required.\nQualification\nAWS Data Engineer\nWorkmode: Hybrid\nWork location: PAN INDIA\nWork Timing: 2 PM to 11 PM\nPrimary Skill: Data Engineer\nExperience in data engineering, with a proven focus on data ingestion and extraction using Python/PySpark.\nExtensive AWS experience is mandatory, with proficiency in Glue, Lambda, SQS, SNS, AWS IAM, AWS Step Functions, S3, and RDS (Oracle, Aurora Postgres).\n4+ years of experience working with both relational and non-relational/NoSQL databases is required.\nStrong SQL experience is necessary, demonstrating the ability to write complex queries from scratch. Also, experience in Redshift is required along with other SQL DB experience.\nStrong scripting experience with the ability to build intricate data pipelines using AWS serverless architecture.\nUnderstanding of building an end-to-end Data pipeline.\nSecondary Skills\nStrong understanding of Kinesis, Kafka, CDK. Experience with Kafka and ECS is also required.\nStrong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling is required.\nExperience in Node Js and CDK.\nJD Responsibilities\nLead the architectural design and development of a scalable, reliable, and flexible metadata-driven data ingestion and extraction framework on AWS using Python/PySpark.\nDesign and implement a customizable data processing framework using Python/PySpark. This framework should be capable of handling diverse scenarios and evolving data processing requirements.\nImplement data pipeline for data ingestion, transformation, and extraction leveraging the AWS Cloud Services.\nSeamlessly integrate a variety of AWS services, including S3, Glue, Kafka, Lambda, SQL, SNS, Athena, EC2, RDS (Oracle, Postgres, MySQL), AWS Crawler to construct a highly scalable and reliable data ingestion and extraction pipeline.\nFacilitate configuration and extensibility of the framework to adapt to evolving data needs and processing scenarios.\nDevelop and maintain rigorous data quality checks and validation processes to safeguard the integrity of ingested data.\nImplement robust error handling, logging, monitoring, and alerting mechanisms to ensure the reliability of the entire data pipeline.\nQualifications\nMust Have\nOver 6 years of hands-on experience in data engineering, with a proven focus on data ingestion and extraction using Python/PySpark.\nExtensive AWS experience is mandatory, with proficiency in Glue, Lambda, SQS, SNS, AWS IAM, AWS Step Functions, S3, and RDS (Oracle, Aurora Postgres).\n4+ years of experience working with both relational and non-relational/NoSQL databases is required.\nStrong SQL experience is necessary, demonstrating the ability to write complex queries from scratch.\nStrong working experience in Redshift is required along with other SQL DB experience.\nStrong scripting experience with the ability to build intricate data pipelines using AWS serverless architecture.\nComplete understanding of building an end-to-end Data pipeline.\nNice to have\nStrong understanding of Kinesis, Kafka, CDK.\nA strong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling is required.\nExperience in Node Js and CDK.\nExperience with Kafka and ECS is also required.",Information Technology,"['aws glue', 'Pyspark', 'Redshift', 'Sql', 'Lambda', 'Kafka']",2025-06-12 15:17:50
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.",Information Technology,"['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 15:17:51
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.",Information Technology,"['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 15:17:52
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,"Key Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.",Information Technology,"['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 15:17:54
Data Engineer-Data Platforms,IBM,5-8 Years,,Navi Mumbai,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions.\nYou will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.. Good working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala",Software,"['Big Data', 'Hadoop', 'Spark', 'Python', 'Scala', 'Pyspark']",2025-06-12 15:17:56
AWS Data Engineer,Virtusa,6-8 Years,,Chennai,"Sr. AWS Data Engineer\nP3 C3 TSTS\nPrimary Skills\nExperience in data engineering, with a proven focus on data ingestion and extraction using Python/PySpark.\nExtensive AWS experience is mandatory, with proficiency in Glue, Lambda, SQS, SNS, AWS IAM, AWS Step Functions, S3, and RDS (Oracle, Aurora Postgres).\n4+ years of experience working with both relational and non-relational/NoSQL databases is required.\nStrong SQL experience is necessary, demonstrating the ability to write complex queries from scratch. Also, experience in Redshift is required along with other SQL DB experience.\nStrong scripting experience with the ability to build intricate data pipelines using AWS serverless architecture.\nUnderstanding of building an end-to-end Data pipeline.\nSecondary Skills\nStrong understanding of Kinesis, Kafka, CDK. Experience with Kafka and ECS is also required.\nStrong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling is required.\nExperience in Node Js and CDK.\nJD\nResponsibilities\nLead the architectural design and development of a scalable, reliable, and flexible metadata-driven data ingestion and extraction framework on AWS using Python/PySpark.\nDesign and implement a customizable data processing framework using Python/PySpark. This framework should be capable of handling diverse scenarios and evolving data processing requirements.\nImplement data pipeline for data ingestion, transformation, and extraction leveraging the AWS Cloud Services.\nSeamlessly integrate a variety of AWS services, including S3, Glue, Kafka, Lambda, SQL, SNS, Athena, EC2, RDS (Oracle, Postgres, MySQL), AWS Crawler to construct a highly scalable and reliable data ingestion and extraction pipeline.\nFacilitate configuration and extensibility of the framework to adapt to evolving data needs and processing scenarios.\nDevelop and maintain rigorous data quality checks and validation processes to safeguard the integrity of ingested data.\nImplement robust error handling, logging, monitoring, and alerting mechanisms to ensure the reliability of the entire data pipeline.\nQualifications\nMust Have\nOver 6 years of hands-on experience in data engineering, with a proven focus on data ingestion and extraction using Python/PySpark.\nExtensive AWS experience is mandatory, with proficiency in Glue, Lambda, SQS, SNS, AWS IAM, AWS Step Functions, S3, and RDS (Oracle, Aurora Postgres).\n4+ years of experience working with both relational and non-relational/NoSQL databases is required.\nStrong SQL experience is necessary, demonstrating the ability to write complex queries from scratch.\nStrong working experience in Redshift is required along with other SQL DB experience.\nStrong scripting experience with the ability to build intricate data pipelines using AWS serverless architecture.\nComplete understanding of building an end-to-end Data pipeline.\nNice to have\nStrong understanding of Kinesis, Kafka, CDK.\nA strong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling is required.\nExperience in Node Js and CDK.\nExperience with Kafka and ECS is also required.\nQualification\nAWS Data Engineer\nWorkmode: Hybrid\nWork location: PAN INDIA\nWork Timing: 2 PM to 11 PM\nPrimary Skill: Data Engineer\nExperience in data engineering, with a proven focus on data ingestion and extraction using Python/PySpark.\nExtensive AWS experience is mandatory, with proficiency in Glue, Lambda, SQS, SNS, AWS IAM, AWS Step Functions, S3, and RDS (Oracle, Aurora Postgres).\n4+ years of experience working with both relational and non-relational/NoSQL databases is required.\nStrong SQL experience is necessary, demonstrating the ability to write complex queries from scratch. Also, experience in Redshift is required along with other SQL DB experience.\nStrong scripting experience with the ability to build intricate data pipelines using AWS serverless architecture.\nUnderstanding of building an end-to-end Data pipeline.\nSecondary Skills\nStrong understanding of Kinesis, Kafka, CDK. Experience with Kafka and ECS is also required.\nStrong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling is required.\nExperience in Node Js and CDK.\nJD Responsibilities\nLead the architectural design and development of a scalable, reliable, and flexible metadata-driven data ingestion and extraction framework on AWS using Python/PySpark.\nDesign and implement a customizable data processing framework using Python/PySpark. This framework should be capable of handling diverse scenarios and evolving data processing requirements.\nImplement data pipeline for data ingestion, transformation, and extraction leveraging the AWS Cloud Services.\nSeamlessly integrate a variety of AWS services, including S3, Glue, Kafka, Lambda, SQL, SNS, Athena, EC2, RDS (Oracle, Postgres, MySQL), AWS Crawler to construct a highly scalable and reliable data ingestion and extraction pipeline.\nFacilitate configuration and extensibility of the framework to adapt to evolving data needs and processing scenarios.\nDevelop and maintain rigorous data quality checks and validation processes to safeguard the integrity of ingested data.\nImplement robust error handling, logging, monitoring, and alerting mechanisms to ensure the reliability of the entire data pipeline.\nQualifications\nMust Have\nOver 6 years of hands-on experience in data engineering, with a proven focus on data ingestion and extraction using Python/PySpark.\nExtensive AWS experience is mandatory, with proficiency in Glue, Lambda, SQS, SNS, AWS IAM, AWS Step Functions, S3, and RDS (Oracle, Aurora Postgres).\n4+ years of experience working with both relational and non-relational/NoSQL databases is required.\nStrong SQL experience is necessary, demonstrating the ability to write complex queries from scratch.\nStrong working experience in Redshift is required along with other SQL DB experience.\nStrong scripting experience with the ability to build intricate data pipelines using AWS serverless architecture.\nComplete understanding of building an end-to-end Data pipeline.\nNice to have\nStrong understanding of Kinesis, Kafka, CDK.\nA strong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling is required.\nExperience in Node Js and CDK.\nExperience with Kafka and ECS is also required.",Information Technology,"['aws glue', 'Pyspark', 'Redshift', 'Sql', 'Lambda', 'Kafka']",2025-06-12 15:17:57
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.",Information Technology,"['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 15:17:58
AWS Data Engineer,Virtusa,5-10 Years,,Chennai,"Key Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.",Information Technology,"['AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql']",2025-06-12 15:18:00
Data Engineer-Data Platforms,IBM,0-5 Years,,Bengaluru,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs a Big Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns.\nYou'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nAs a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions.\nYou will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nBig Data Developer, Hadoop, Hive, Spark, PySpark, Strong SQL.\nAbility to incorporate a variety of statistical and machine learning techniques. Basic understanding of Cloud (AWS,Azure, etc).\nAbility to use programming languages like Java, Python, Scala, etc., to build pipelines to extract and transform data from a repository to a data consumer\nAbility to use Extract, Transform, and Load (ETL) tools and/or data integration, or federation tools to prepare and transform data as needed.\nAbility to use leading edge tools such as Linux, SQL, Python, Spark, Hadoop and Java\nPreferred technical and professional experience\nBasic understanding or experience with predictive/prescriptive modeling skills\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions",Software,"['Hadoop Spark', 'Data Pipelines', 'Big Data', 'Pyspark', 'Sql Programming', 'Etl Tools']",2025-06-12 15:18:01
Data Engineer-Data Integration,IBM,0-5 Years,,Pune,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviour's.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nExpertise in designing and implementing scalable data warehouse solutions on Snowflake, including schema design, performance tuning, and query optimization.\nStrong experience in building data ingestion and transformation pipelines using Talend to process structured and unstructured data from various sources.\nProficiency in integrating data from cloud platforms into Snowflake using Talend and native Snowflake capabilities.\nHands-on experience with dimensional and relational data modelling techniques to support analytics and reporting requirements\nPreferred technical and professional experience\nUnderstanding of optimizing Snowflake workloads, including clustering keys, caching strategies, and query profiling.\nAbility to implement robust data validation, cleansing, and governance frameworks within ETL processes.\nProficiency in SQL and/or Shell scripting for custom transformations and automation tasks",Software,"['snowflake', 'Data Pipelines', 'Talend', 'Data Modeling', 'Sql', 'Etl']",2025-06-12 15:18:02
Data Engineer,Virtusa,4-6 Years,,Bengaluru,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools.",Information Technology,"['CI & CD', 'Cloudera Data platform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker']",2025-06-12 15:18:09
Data Engineer,Virtusa,5-8 Years,,Chennai,8 Years overall IT experience with minimum 5 years of work experience in below tech skills\nStrong experience in Python Scripting and PySpark for data processing\nProficiency in SQL dealing with big data over Informatica ETL\nProven experience in Data quality and data optimization of data lake in Iceberg format with strong understanding of architecture\nExperience in AWS Glue jobs\nExperience in AWS cloud platform and its data services S3 Redshift Lambda EMR Airflow Postgres SNS Event bridge\nExpertise in BASH Shell scripting\nStrong understanding of healthcare data systems and experience leading data engineering teams\nExperience in Agile environments\nExcellent problem solving skills and attention to detail\nEffective communication and collaboration skills\nResponsibilities\nLeads development of data pipelines and architectures that handle large scale data sets\nDesigns constructs and tests data architecture aligned with business requirements\nProvides technical leadership for data projects ensuring best practices and high quality data solutions\nCollaborates with product finance and other business units to ensure data pipelines meet business requirements\nWork with DBT Data Build Tool for transforming raw data into actionable insights\nOversees development of data solutions that enable predictive and prescriptive analytics\nEnsures the technical quality of solutions managing data as it moves across environments\nAligns data architecture to Healthfirst solution architecture,Information Technology,"['Iceberg', 'Python', 'Pyspark', 'Sql', 'AWS Glue', 'Informatica']",2025-06-12 15:18:11
Data Engineer,Virtusa,5-8 Years,,Chennai,8 Years overall IT experience with minimum 5 years of work experience in below tech skills\nStrong experience in Python Scripting and PySpark for data processing\nProficiency in SQL dealing with big data over Informatica ETL\nProven experience in Data quality and data optimization of data lake in Iceberg format with strong understanding of architecture\nExperience in AWS Glue jobs\nExperience in AWS cloud platform and its data services S3 Redshift Lambda EMR Airflow Postgres SNS Event bridge\nExpertise in BASH Shell scripting\nStrong understanding of healthcare data systems and experience leading data engineering teams\nExperience in Agile environments\nExcellent problem solving skills and attention to detail\nEffective communication and collaboration skills\nResponsibilities\nLeads development of data pipelines and architectures that handle large scale data sets\nDesigns constructs and tests data architecture aligned with business requirements\nProvides technical leadership for data projects ensuring best practices and high quality data solutions\nCollaborates with product finance and other business units to ensure data pipelines meet business requirements\nWork with DBT Data Build Tool for transforming raw data into actionable insights\nOversees development of data solutions that enable predictive and prescriptive analytics\nEnsures the technical quality of solutions managing data as it moves across environments\nAligns data architecture to Healthfirst solution architecture,Information Technology,"['Iceberg', 'Python', 'Pyspark', 'Sql', 'AWS Glue', 'Informatica']",2025-06-12 15:18:12
Data Engineer,Virtusa,8-12 Years,,Hyderabad,"Experience:\n8+ years of experience in data engineering, specifically in cloud environments like AWS.\nProficiency in PySpark for distributed data processing and transformation.\nSolid experience with AWS Glue for ETL jobs and managing data workflows.\nHands-on experience with AWS Data Pipeline (DPL) for workflow orchestration.\nStrong experience with AWS services such as S3, Lambda, Redshift, RDS, and EC2.\nTechnical Skills:\nProficiency in Python and PySpark for data processing and transformation tasks.\nDeep understanding of ETL concepts and best practices.\nFamiliarity with AWS Glue (ETL jobs, Data Catalog, and Crawlers).\nExperience building and maintaining data pipelines with AWS Data Pipeline or similar orchestration tools.\nFamiliarity with AWS S3 for data storage and management, including file formats (CSV, Parquet, Avro).\nStrong knowledge of SQL for querying and manipulating relational and semi-structured data.\nExperience with Data Warehousing and Big Data technologies, specifically within AWS.\nAdditional Skills:\nExperience with AWS Lambda for serverless data processing and orchestration.\nUnderstanding of AWS Redshift for data warehousing and analytics.\nFamiliarity with Data Lakes, Amazon EMR, and Kinesis for streaming data processing.\nKnowledge of data governance practices, including data lineage and auditing.\nFamiliarity with CI/CD pipelines and Git for version control.\nExperience with Docker and containerization for building and deploying applications.\nDesign and Build Data Pipelines: Design, implement, and optimize data pipelines on AWS using PySpark, AWS Glue, and AWS Data Pipeline to automate data integration, transformation, and storage processes.\nETL Development: Develop and maintain Extract, Transform, and Load (ETL) processes using AWS Glue and PySpark to efficiently process large datasets.\nData Workflow Automation: Build and manage automated data workflows using AWS Data Pipeline, ensuring seamless scheduling, monitoring, and management of data jobs.\nData Integration: Work with different AWS data storage services (e.g., S3, Redshift, RDS) to ensure smooth integration and movement of data across platforms.\nOptimization and Scaling: Optimize and scale data pipelines for high performance and cost efficiency, utilizing AWS services like Lambda, S3, and EC2.",Information Technology,"['Snowflake design', 'ETL/ELT', 'Python', 'Sql', 'Data Warehousing', 'cloud platform']",2025-06-12 15:18:14
Data Engineer,Accenture India,5-10 Years,,Jaipur,"Job Description: Data Engineer\nProject Role: Data Engineer\nProject Role Description: Design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform, and load) processes to migrate and deploy data across systems.\nMust-Have Skills: SAP HCM On Premise ABAP\nGood-to-Have Skills: SAP HCM Organizational Management\nExperience Required: Minimum 5 years\nEducational Qualification: 15 years full-time education\nJob Summary:\nAs a Data Engineer, your main responsibility will be to design, develop, and maintain data solutions for generating, collecting, and processing data. You'll work on creating data pipelines, ensuring data quality, and implementing ETL processes to move and deploy data across systems. Your role will be crucial in managing and analyzing data to support business decisions and provide data-driven insights.\nRoles & Responsibilities:\nSubject Matter Expert (SME): Expected to be an SME, collaborate and manage the team to deliver results. Responsible for making key team decisions.\nCollaboration & Teamwork: Work closely with multiple teams and contribute to important decisions and problem-solving efforts.\nData Solutions: Design, develop, and maintain data solutions that support data generation, collection, and processing.\nData Pipelines: Create and manage data pipelines to extract, transform, and load data (ETL processes) across systems.\nData Quality & Integrity: Ensure data quality and integrity throughout the data lifecycle.\nETL Processes: Implement and optimize ETL processes to efficiently migrate and deploy data across systems and platforms.\nProfessional & Technical Skills:\nMust-Have Skills:\nStrong proficiency in SAP HCM On Premise ABAP.\nExperience with data modeling and database design.\nFamiliarity with ETL tools and processes.\nExpertise in programming languages such as ABAP, SQL, and Python.\nGood-to-Have Skills:\nExperience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nAdditional Information:\nExperience Required: A minimum of 5 years of experience in SAP HCM On Premise ABAP.\nEducational Requirement: The candidate should have a minimum of 15 years of full-time education.","Consulting, Information Services","['Sql', 'Python', 'Abap']",2025-06-12 15:18:15
Data Engineer,Virtusa,10-12 Years,,Bengaluru,"Requirements\n10+ years of strong experience with data transformation & ETL on large data sets\nExperience with designing customer centric datasets (i.e., CRM, Call Center, Marketing, Offline, Point of\nSale etc.)\n5+ years of Data Modeling experience (i.e., Relational, Dimensional, Columnar, Big Data)\n5+ years of complex SQL or NoSQL experience\nExperience in advanced Data Warehouse concepts\nExperience in industry ETL tools (i.e., Informatica, Unifi)\nExperience with Business Requirements definition and management, structured analysis, process\ndesign, use case documentation\nExperience with Reporting Technologies (i.e., Tableau, PowerBI)\nexperience in professional software development\nDemonstrate exceptional organizational skills and ability to multi-task simultaneous different customer\nprojects\nStrong verbal & written communication skills to interface with Sales team & lead customers to\nsuccessful outcome\nMust be self-managed, proactive and customer focused\nDegree in Computer Science, Information Systems, Data Science, or related field",Information Technology,"['Python', 'Sql', 'Etl', 'BigQuery']",2025-06-12 15:18:16
Data Engineer,Virtusa,8-9 Years,,Bengaluru,"Proficiency in PySpark for distributed data processing and transformation.\nSolid experience with AWS Glue for ETL jobs and managing data workflows.\nHands-on experience with AWS Data Pipeline (DPL) for workflow orchestration.\nStrong experience with AWS services such as S3, Lambda, Redshift, RDS, and EC2.\nTechnical Skills:\nProficiency in Python and PySpark for data processing and transformation tasks.\nDeep understanding of ETL concepts and best practices.\nFamiliarity with AWS Glue (ETL jobs, Data Catalog, and Crawlers).\nExperience building and maintaining data pipelines with AWS Data Pipeline or similar orchestration tools.\nFamiliarity with AWS S3 for data storage and management, including file formats (CSV, Parquet, Avro).\nStrong knowledge of SQL for querying and manipulating relational and semi-structured data.\nExperience with Data Warehousing and Big Data technologies, specifically within AWS.\nAdditional Skills:\nExperience with AWS Lambda for serverless data processing and orchestration.\nUnderstanding of AWS Redshift for data warehousing and analytics.\nFamiliarity with Data Lakes, Amazon EMR, and Kinesis for streaming data processing.\nKnowledge of data governance practices, including data lineage and auditing.\nFamiliarity with CI/CD pipelines and Git for version control.\nExperience with Docker and containerization for building and deploying applications.\nDesign and Build Data Pipelines: Design, implement, and optimize data pipelines on AWS using PySpark, AWS Glue, and AWS Data Pipeline to automate data integration, transformation, and storage processes.\nETL Development: Develop and maintain Extract, Transform, and Load (ETL) processes using AWS Glue and PySpark to efficiently process large datasets.\nData Workflow Automation: Build and manage automated data workflows using AWS Data Pipeline, ensuring seamless scheduling, monitoring, and management of data jobs.\nData Integration: Work with different AWS data storage services (e.g., S3, Redshift, RDS) to ensure smooth integration and movement of data across platforms.\nOptimization and Scaling: Optimize and scale data pipelines for high performance and cost efficiency, utilizing AWS services like Lambda, S3, and EC2.",Information Technology,"['CI & CD', 'Cloudera Data platform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker']",2025-06-12 15:18:17
Data Engineer,Synechron Technologies Private Limited,6-8 Years,,Gurugram,"Job Profile : Data and analytics expert with 6-8 years of experience, proficient in big data technologies, data architecture, and customer-focused solutions.\nTechnology\nIn addition, you'll bring a selection of the following technical skills with an experience around 6-8 years.\nResponsibilities:\nDesign and implement data pipelines, data lakes, and visualization solutions to create business value.\nDevelop and maintain big data solutions using technologies like AWS Redshift, Kubernetes, Spark, Python, SQL, DBT, and Dataiku.\nApply best practices in data architecture, modeling, and metadata management.\nAnalyze and troubleshoot complex data issues, ensuring timely resolution..\nPrimary Skills:\n6-8 years of experience in data and analytics.\nProficiency in big data technologies: AWS Redshift, Kubernetes, Spark, Python, complex SQL, DBT, and Dataiku.\nStrong understanding of data architecture, modeling, and metadata management.\nExcellent data analytical and troubleshooting skills.\nStrong communication skills and a customer-focused approach\nGood to have:\nExperience with other data-related tools and platforms.\nKnowledge of emerging trends in data and analytics\nExperience: 6 to 8 years",Software Engineering,"['Aws Redshift', 'Spark', 'Kubernetes', 'Sql', 'Python']",2025-06-12 15:18:18
Data Engineer,Virtusa,7-12 Years,,Chennai,"Job Title: Data Engineer\n\nExperience Level\n6+ years in Data Engineering\nKey Responsibilities\nAs a Data Engineer, you will:\nData Engineering: Apply expertise in Snowflake, DBT, and Python to develop and manage data solutions.\nCloud Integration: Utilize AWS Cloud services, specifically AWS S3 and Lambda, for data operations.\nSQL Development: Leverage strong SQL knowledge for data manipulation and querying.\nMandatory Skills & Experience\nTechnical Proficiency:\nSQL: Strong knowledge in SQL.\nData Warehousing & Transformation: Expertise in Snowflake and DBT (minimum 2+ years of experience).\nProgramming: Expertise in Python (minimum 2+ years of experience).\nCloud Platform: Knowledge in AWS Cloud (specifically AWS S3 & Lambda).\nETL/ELT Tools (Desirable): Knowledge of SnapLogic or Fivetran tools is an added advantage.\nExperience & Qualifications:\n6+ years of experience in data engineering.",Information Technology,"['Snowflake design', 'ETL/ELT', 'Python', 'Sql', 'Data Warehousing', 'cloud platform']",2025-06-12 15:18:19
Data Engineer,Virtusa,7-12 Years,,Chennai,"Job Title: Data Engineer\n\nExperience Level\n6+ years in Data Engineering\nKey Responsibilities\nAs a Data Engineer, you will:\nData Engineering: Apply expertise in Snowflake, DBT, and Python to develop and manage data solutions.\nCloud Integration: Utilize AWS Cloud services, specifically AWS S3 and Lambda, for data operations.\nSQL Development: Leverage strong SQL knowledge for data manipulation and querying.\nMandatory Skills & Experience\nTechnical Proficiency:\nSQL: Strong knowledge in SQL.\nData Warehousing & Transformation: Expertise in Snowflake and DBT (minimum 2+ years of experience).\nProgramming: Expertise in Python (minimum 2+ years of experience).\nCloud Platform: Knowledge in AWS Cloud (specifically AWS S3 & Lambda).\nETL/ELT Tools (Desirable): Knowledge of SnapLogic or Fivetran tools is an added advantage.\nExperience & Qualifications:\n6+ years of experience in data engineering.",Information Technology,"['Snowflake design', 'ETL/ELT', 'Python', 'Sql', 'Data Warehousing', 'cloud platform']",2025-06-12 15:18:20
Data Engineer,Virtusa,5-8 Years,,Chennai,8 Years overall IT experience with minimum 5 years of work experience in below tech skills\nStrong experience in Python Scripting and PySpark for data processing\nProficiency in SQL dealing with big data over Informatica ETL\nProven experience in Data quality and data optimization of data lake in Iceberg format with strong understanding of architecture\nExperience in AWS Glue jobs\nExperience in AWS cloud platform and its data services S3 Redshift Lambda EMR Airflow Postgres SNS Event bridge\nExpertise in BASH Shell scripting\nStrong understanding of healthcare data systems and experience leading data engineering teams\nExperience in Agile environments\nExcellent problem solving skills and attention to detail\nEffective communication and collaboration skills\nResponsibilities\nLeads development of data pipelines and architectures that handle large scale data sets\nDesigns constructs and tests data architecture aligned with business requirements\nProvides technical leadership for data projects ensuring best practices and high quality data solutions\nCollaborates with product finance and other business units to ensure data pipelines meet business requirements\nWork with DBT Data Build Tool for transforming raw data into actionable insights\nOversees development of data solutions that enable predictive and prescriptive analytics\nEnsures the technical quality of solutions managing data as it moves across environments\nAligns data architecture to Healthfirst solution architecture,Information Technology,"['Iceberg', 'Python', 'Pyspark', 'Sql', 'AWS Glue', 'Informatica']",2025-06-12 15:18:21
Data Engineer,Virtusa,6-8 Years,,Bengaluru,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools",Information Technology,"['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 15:18:23
Data Engineer,Virtusa,6-10 Years,,Bengaluru,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools",Information Technology,"['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark', 'Aws']",2025-06-12 15:18:24
Data Engineer-Data Warehouse,IBM,5-8 Years,,Bengaluru,"IBM Consulting Overview In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour Role and Responsibilities As an Associate Software Developer at IBM, you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in selecting the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle challenges related to database integration and work with complex, unstructured data sets.\nPrimary Responsibilities:\nImplement and validate predictive models, as well as create and maintain statistical models with a focus on big data, incorporating various machine learning techniques.\nDesign and implement enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWork in an Agile, collaborative environment, partnering with scientists, engineers, consultants, and database administrators across disciplines to apply analytical rigor to predictive modeling challenges.\nDevelop teams or write programs to cleanse and integrate data efficiently and develop predictive or prescriptive models.\nEvaluate modeling results to ensure accuracy and effectiveness.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nStrong experience in SQL.\nStrong experience in DBT (Data Build Tool).\nStrong understanding of Data Warehousing concepts.\nStrong experience in AWS or other cloud platforms.\nRedshift experience is a plus.\nPreferred Technical and Professional Experience\nAbility to thrive in teamwork settings with excellent verbal and written communication skills.\nCapability to communicate with internal and external clients to understand business needs and provide analytical solutions.\nAbility to present results to both technical and non-technical audiences effectively.",Information Technology,"['dbt', 'Cloud Knowledge', 'Sql', 'Data Warehousing', 'AWS', 'Redshift']",2025-06-12 15:18:25
Data Engineer,Accenture India,5-10 Years,,Ahmedabad,"Project Role: Data Engineer\nProject Role Description:\nAs a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL (Extract, Transform, Load) processes to migrate and deploy data across systems.\nMust Have Skills:\nProficiency in Data Engineering\nGood to Have Skills:\nN/A\nMinimum Experience Required:\n5 years\nEducational Qualification:\n15 years of full-time education\nSummary:\nYou will play a crucial role in managing and optimizing data infrastructure to support the organization's data needs.\nRoles & Responsibilities:\nExpected to be a Subject Matter Expert (SME) and collaborate with the team.\nResponsible for making team decisions and engaging with multiple teams to contribute to key decisions.\nProvide solutions to problems for your immediate team and across multiple teams.\nDevelop and maintain data pipelines for efficient data processing.\nEnsure data quality and integrity throughout the data lifecycle.\nImplement ETL processes to migrate and deploy data across systems.\nOptimize data infrastructure to support the organization's data needs.\nProfessional & Technical Skills:\nStrong understanding of data generation, collection, and processing.\nExperience in designing and implementing data pipelines.\nKnowledge of ETL processes.\nFamiliarity with data quality assurance and data governance practices.\nAdditional Information:\nThe candidate should have a minimum of 5 years of experience in Data Engineering.\nThis position is based at our Ahmedabad office.\nA 15 years full-time education is required.","Consulting, Information Services","['Nosql', 'Sql', 'Etl']",2025-06-12 15:18:26
Data Engineer,Virtusa,7-12 Years,,Chennai,"Job Title: Data Engineer\n\nExperience Level\n6+ years in Data Engineering\nKey Responsibilities\nAs a Data Engineer, you will:\nData Engineering: Apply expertise in Snowflake, DBT, and Python to develop and manage data solutions.\nCloud Integration: Utilize AWS Cloud services, specifically AWS S3 and Lambda, for data operations.\nSQL Development: Leverage strong SQL knowledge for data manipulation and querying.\nMandatory Skills & Experience\nTechnical Proficiency:\nSQL: Strong knowledge in SQL.\nData Warehousing & Transformation: Expertise in Snowflake and DBT (minimum 2+ years of experience).\nProgramming: Expertise in Python (minimum 2+ years of experience).\nCloud Platform: Knowledge in AWS Cloud (specifically AWS S3 & Lambda).\nETL/ELT Tools (Desirable): Knowledge of SnapLogic or Fivetran tools is an added advantage.\nExperience & Qualifications:\n6+ years of experience in data engineering.",Information Technology,"['Snowflake design', 'ETL/ELT', 'Python', 'Sql', 'Data Warehousing', 'cloud platform']",2025-06-12 15:18:35
Data Engineer,IBM,3-10 Years,,"Navi Mumbai, Mumbai City, Mumbai","Role Overview:\nAs a Big Data Engineer, you'll design and build robust data pipelines on Cloudera using Spark (Scala/PySpark) for ingestion, transformation, and processing of high-volume data from banking systems.\nKey Responsibilities:\nBuild scalable batch and real-time ETL pipelines using Spark and Hive\nIntegrate structured and unstructured data sources\nPerform performance tuning and code optimization\nSupport orchestration and job scheduling (NiFi, Airflow)\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nExperience: 315 years\nProficiency in PySpark/Scala with Hive/Impala\nExperience with data partitioning, bucketing, and optimization\nFamiliarity with Kafka, Iceberg, NiFi is a must\nKnowledge of banking or financial datasets is a plus",Information Technology,"['Sql', 'Python', 'Hadoop', 'Etl', 'Data Warehousing', 'Nosql', 'Cloud Computing', 'Data Modeling']",2025-06-12 15:18:40
Data Engineer,IBM,3-10 Years,,"Navi Mumbai, Mumbai City, Mumbai","Role Overview:\nAs a Big Data Engineer, you'll design and build robust data pipelines on Cloudera using Spark (Scala/PySpark) for ingestion, transformation, and processing of high-volume data from banking systems.\nKey Responsibilities:\nBuild scalable batch and real-time ETL pipelines using Spark and Hive\nIntegrate structured and unstructured data sources\nPerform performance tuning and code optimization\nSupport orchestration and job scheduling (NiFi, Airflow)\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nExperience: 315 years\nProficiency in PySpark/Scala with Hive/Impala\nExperience with data partitioning, bucketing, and optimization\nFamiliarity with Kafka, Iceberg, NiFi is a must\nKnowledge of banking or financial datasets is a plus",Information Technology,"['Sql', 'Python', 'Hadoop', 'Etl', 'Data Warehousing', 'Nosql', 'Cloud Computing', 'Data Modeling']",2025-06-12 15:18:41
Data Engineer,Accenture India,0-2 Years,,Ahmedabad,"Project Role: Data Engineer\nProject Role Description:\nAs a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL (Extract, Transform, Load) processes to migrate and deploy data across systems.\nMust Have Skills:\nProficiency in Data Engineering\nGood to Have Skills:\nExperience with big data technologies (Hadoop, Spark)\nMinimum Experience Required:\n0-2 years\nEducational Qualification:\n15 years of full-time education\nSummary:\nIn this role, you will manage and optimize data infrastructure to support the organization's data needs, ensuring data integrity and quality throughout the processes.\nRoles & Responsibilities:\nBuild knowledge and support the team.\nParticipate in problem-solving discussions.\nDesign and develop data pipelines to extract, transform, and load data from various sources.\nEnsure data quality and integrity by implementing data validation and cleansing processes.\nCollaborate with cross-functional teams to understand data requirements and design efficient data solutions.\nOptimize and tune data pipelines for performance and scalability.\nTroubleshoot and resolve data-related issues and incidents.\nStay updated with the latest trends and technologies in data engineering and recommend improvements to existing processes.\nMentor and guide junior professionals in data engineering best practices.\nProfessional & Technical Skills:\nStrong understanding of data modeling and database design principles.\nExperience with ETL tools (Apache NiFi, Talend).\nFamiliarity with cloud platforms (AWS, Azure).\nKnowledge of data warehousing concepts and techniques.\nExperience with SQL and NoSQL databases.\nSolid understanding of data governance and security principles.\nAdditional Information:\nThe candidate should have a minimum of 0-2 years of experience in Data Engineering.\nThis position is based at our Ahmedabad office.\nA 15 years full-time education is required.","Consulting, Information Services","['Sql', 'Nosql', 'Etl']",2025-06-12 15:18:42
Data Engineer,Virtusa,6-8 Years,,Hyderabad,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools",Information Technology,"['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 15:18:44
Data Engineer,Virtusa,7-12 Years,,Chennai,"Job Title: Data Engineer\n\nExperience Level\n6+ years in Data Engineering\nKey Responsibilities\nAs a Data Engineer, you will:\nData Engineering: Apply expertise in Snowflake, DBT, and Python to develop and manage data solutions.\nCloud Integration: Utilize AWS Cloud services, specifically AWS S3 and Lambda, for data operations.\nSQL Development: Leverage strong SQL knowledge for data manipulation and querying.\nMandatory Skills & Experience\nTechnical Proficiency:\nSQL: Strong knowledge in SQL.\nData Warehousing & Transformation: Expertise in Snowflake and DBT (minimum 2+ years of experience).\nProgramming: Expertise in Python (minimum 2+ years of experience).\nCloud Platform: Knowledge in AWS Cloud (specifically AWS S3 & Lambda).\nETL/ELT Tools (Desirable): Knowledge of SnapLogic or Fivetran tools is an added advantage.\nExperience & Qualifications:\n6+ years of experience in data engineering.",Information Technology,"['Snowflake design', 'ETL/ELT', 'Python', 'Sql', 'Data Warehousing', 'cloud platform']",2025-06-12 15:18:45
Data Engineer,Virtusa,6-10 Years,,Bengaluru,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools",Information Technology,"['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark', 'Aws']",2025-06-12 15:18:47
Data Engineer,Virtusa,6-8 Years,,Bengaluru,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools",Information Technology,"['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 15:18:48
Data Engineer-Data Warehouse,IBM,2-6 Years,,Pune,"Required technical and professional expertise\nDesign and implement efficient database schemas and data models using Teradata.\nOptimize SQL queries and stored procedures for performance.\nPerform database administration tasks including installation, configuration, and maintenance of Teradata systems\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Information Technology,"['Linux', 'Sql', 'Python', 'Spark', 'Hadoop', 'Java']",2025-06-12 15:18:49
Data Engineer-Data Warehouse,IBM,2-6 Years,,Bengaluru,"Introduction\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nA career in IBM Consulting is rooted by long-term relationships and close collaboration with clients across the globe.\nYou'll work with visionaries across multiple industries to improve the hybrid cloud and AI journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio; including Software and Red Hat.\nCuriosity and a constant quest for knowledge serve as the foundation to success in IBM Consulting. In your role, you'll be encouraged to challenge the norm, investigate ideas outside of your role, and come up with creative solutions resulting in ground breaking impact for a wide network of clients. Our culture of evolution and empathy centers on long-term career growth and development opportunities in an environment that embraces your unique skills and experience.\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nGood Hands on experience in DBT is required.\nETL Datastage and snowflake preferred.\nAbility to use programming languages like Java, Python, Scala, etc., to build pipelines to extract and transform data from a repository to a data consumer\nAbility to use Extract, Transform, and Load (ETL) tools and/or data integration, or federation tools to prepare and transform data as needed.\nAbility to use leading edge tools such as Linux, SQL, Python, Spark, Hadoop and Java\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Information Technology,"['Linux', 'Sql', 'Python', 'Spark', 'Hadoop', 'Java']",2025-06-12 15:18:51
Data Engineer-Data Warehouse,IBM,2-6 Years,,Pune,"Introduction\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nA career in IBM Consulting is rooted by long-term relationships and close collaboration with clients across the globe.\nYou'll work with visionaries across multiple industries to improve the hybrid cloud and AI journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio; including Software and Red Hat.\nCuriosity and a constant quest for knowledge serve as the foundation to success in IBM Consulting. In your role, you'll be encouraged to challenge the norm, investigate ideas outside of your role, and come up with creative solutions resulting in ground breaking impact for a wide network of clients. Our culture of evolution and empathy centers on long-term career growth and development opportunities in an environment that embraces your unique skills and experience.\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nGood Hands on experience in DBT is required.\nETL Datastage and snowflake preferred.\nAbility to use programming languages like Java, Python, Scala, etc., to build pipelines to extract and transform data from a repository to a data consumer\nAbility to use Extract, Transform, and Load (ETL) tools and/or data integration, or federation tools to prepare and transform data as needed.\nAbility to use leading edge tools such as Linux, SQL, Python, Spark, Hadoop and Java\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Information Technology,"['Linux', 'Sql', 'Python', 'Spark', 'Hadoop', 'Java']",2025-06-12 15:18:52
Data Engineer-Data Warehouse,IBM,5-8 Years,,Bengaluru,"IBM Consulting Overview In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour Role and Responsibilities As an Associate Software Developer at IBM, you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in selecting the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle challenges related to database integration and work with complex, unstructured data sets.\nPrimary Responsibilities:\nImplement and validate predictive models, as well as create and maintain statistical models with a focus on big data, incorporating various machine learning techniques.\nDesign and implement enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWork in an Agile, collaborative environment, partnering with scientists, engineers, consultants, and database administrators across disciplines to apply analytical rigor to predictive modeling challenges.\nDevelop teams or write programs to cleanse and integrate data efficiently and develop predictive or prescriptive models.\nEvaluate modeling results to ensure accuracy and effectiveness.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nStrong experience in SQL.\nStrong experience in DBT (Data Build Tool).\nStrong understanding of Data Warehousing concepts.\nStrong experience in AWS or other cloud platforms.\nRedshift experience is a plus.\nPreferred Technical and Professional Experience\nAbility to thrive in teamwork settings with excellent verbal and written communication skills.\nCapability to communicate with internal and external clients to understand business needs and provide analytical solutions.\nAbility to present results to both technical and non-technical audiences effectively.",Information Technology,"['dbt', 'Cloud Knowledge', 'Sql', 'Data Warehousing', 'AWS', 'Redshift']",2025-06-12 15:18:53
Data Engineer-Data Warehouse,IBM,5-8 Years,,Cochin / Kochi / Ernakulam,"Introduction\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nResponsible to develop triggers, functions, stored procedures to support this effort\nAssist with impact analysis of changing upstream processes to Data Warehouse and Reporting systems.\nAssist with design, testing, support, and debugging of new and existing ETL and reporting processes.\nPerform data profiling and analysis using a variety of tools. Troubleshoot and support production processes. Create and maintain documentation\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nBE / B Tech in any stream, M.Sc. (Computer Science/IT) / M.C.A, with Minimum 5 plus years of experience\nMust Have : Snowflake, AWS, Complex SQL\nExperience with software architecture in cloud-based infrastructures\nExperience in ETL processes and data modelling techniques\nExperience in designing and managing large scale data warehouses\nPreferred technical and professional experience\nGood to have in addition to Must haves: DBT, Tableau, python, JavaScript\nDevelop complex SQL queries for data analytics and business intelligence.\nBackground working with data analytics, business intelligence, or related fields",Information Technology,"['dbt', 'Tableau', 'Python', 'Javascript', 'Data Analytics', 'Data Warehousing']",2025-06-12 15:18:55
Data Engineer-Data Warehouse,IBM,2-6 Years,,Cochin / Kochi / Ernakulam,"Introduction\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nA career in IBM Consulting is rooted by long-term relationships and close collaboration with clients across the globe.\nYou'll work with visionaries across multiple industries to improve the hybrid cloud and AI journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio; including Software and Red Hat.\nCuriosity and a constant quest for knowledge serve as the foundation to success in IBM Consulting. In your role, you'll be encouraged to challenge the norm, investigate ideas outside of your role, and come up with creative solutions resulting in ground breaking impact for a wide network of clients. Our culture of evolution and empathy centers on long-term career growth and development opportunities in an environment that embraces your unique skills and experience.\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nGood Hands on experience in DBT is required.\nETL Datastage and snowflake preferred.\nAbility to use programming languages like Java, Python, Scala, etc., to build pipelines to extract and transform data from a repository to a data consumer\nAbility to use Extract, Transform, and Load (ETL) tools and/or data integration, or federation tools to prepare and transform data as needed.\nAbility to use leading edge tools such as Linux, SQL, Python, Spark, Hadoop and Java\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Information Technology,"['Linux', 'Sql', 'Python', 'Spark', 'Hadoop', 'Java']",2025-06-12 15:18:56
Data Engineer-Data Warehouse,IBM,2-6 Years,,Pune,"Introduction\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nA career in IBM Consulting is rooted by long-term relationships and close collaboration with clients across the globe.\nYou'll work with visionaries across multiple industries to improve the hybrid cloud and AI journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio; including Software and Red Hat.\nCuriosity and a constant quest for knowledge serve as the foundation to success in IBM Consulting. In your role, you'll be encouraged to challenge the norm, investigate ideas outside of your role, and come up with creative solutions resulting in ground breaking impact for a wide network of clients. Our culture of evolution and empathy centers on long-term career growth and development opportunities in an environment that embraces your unique skills and experience.\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nGood Hands on experience in DBT is required.\nETL Datastage and snowflake preferred.\nAbility to use programming languages like Java, Python, Scala, etc., to build pipelines to extract and transform data from a repository to a data consumer\nAbility to use Extract, Transform, and Load (ETL) tools and/or data integration, or federation tools to prepare and transform data as needed.\nAbility to use leading edge tools such as Linux, SQL, Python, Spark, Hadoop and Java\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Information Technology,"['Linux', 'Sql', 'Python', 'Spark', 'Hadoop', 'Java']",2025-06-12 15:18:57
Data Engineer-Data Platforms,IBM,4-5 Years,,Mumbai,"Your role and responsibilities\nAs a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution.\nYour primary responsibilities include:\nLead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements.\nStrive for continuous improvements by testing the build solution and working under an agile framework.\nDiscover and implement the latest technologies trends to maximize and build creative solutions\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nExperience with Apache Spark (PySpark): In-depth knowledge of Spark's architecture, core APIs, and PySpark for distributed data processing.\nBig Data Technologies: Familiarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering Skills: Strong understanding of ETL pipelines, data modeling, and data warehousing concepts.\nStrong proficiency in Python: Expertise in Python programming with a focus on data processing and manipulation. Data Processing Frameworks: Knowledge of data processing libraries such as Pandas, NumPy.\nSQL Proficiency: Experience writing optimized SQL queries for large-scale data analysis and transformation.\nCloud Platforms: Experience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems\nPreferred technical and professional experience\nDefine, drive, and implement an architecture strategy and standards for end-to-end monitoring.\nPartner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering,\nGood to have detection and prevention tools for Company products and Platform and customer-facing",Information Technology,"['Sql', 'Python', 'Etl', 'Data Warehousing', 'Spark', 'Cloud Computing', 'Data Modeling']",2025-06-12 15:19:07
Data Engineer,Cybage Software Private Limited,6-11 Years,,Pune,"The DevOps Engineer supports the Cloud Platforms Manager by delivering and supporting Cloud Platform, Infrastructure and CICD services.This role involves designing, coding, test and, deploying infrastructure as code. The engineer oversees the full life cycle of Infrastructure as Code (IaC) activities related to systems and subscriptions for cloud environments, including computing, storage, network, connectivity, and security.\nRequired Skills and Qualifications:\nAt least 2-3 years experience working with Cloud technologies\nAt least 1 years experience working in DevOps and automation environments\nExperience in Financial Service (highly desirable).\nExperience in assisting organisation in developing key cloud adoption strategies.\nFamiliarity with Terraform\nAZ-204 Microsoft Certified: Azure Developer Associate highly desirable.\nResponsibilities\nDesign, code, test, deploy, and manage scalable and secure cloud infrastructure.\nImplement automation for cloud infrastructure provisioning and maintenance.\nMonitor and optimise cloud resource utilisation and costs.\nDevelop and maintain continuous integration and continuous deployment (CI/CD) pipelines.\nEnsure smooth and reliable deployment of applications and updates.\nProvide training and support to development teams on DevOps tools and processes.\nMaintain and improve infrastructure-as-code (IaC) practices using tools like Terraform or Bicep.\nProduce and maintain detailed technical documentation.\nPrioritise workload and manage numerous initiatives at once.\nPrioritise system and configuration changes, making informed decisions based on business, compliance, and risk requirements.\nEnable the delivery of system changes through co-ordination & adoption of Angle Finances IT Change Management framework.",Software,"['CI/CD', 'Data Engineer', 'Azure']",2025-06-14 06:34:49
Cybage is hiring For Data Engineer,Cybage Software Private Limited,12-15 Years,,Pune,"The Data Engineer will be responsible for designing, developing, and maintaining robust data pipelines and ETL processes to support data analytics and reporting initiatives. This role requires strong expertise in SQL Server 2017, Python, and Power BI along with handson experience in Master Data Management/Services (MDM) and API integrations. The successful candidate will collaborate with cross-functional teams to ensure high-quality, reliable data delivery and maintain excellent communication with both technical and non-technical stakeholders.\nKey Responsibilities\nData Pipeline & ETL Development:\nDesign, develop, and maintain ETL processes that extract, transform, and load data from various sources.\nEnsure data quality, consistency, and integrity across all systems.\nDatabase & MDM Management:\nManage and optimize SQL Server 2017 databases and Master Data Management/Services (MDM).\nImplement advanced SQL programming techniques to support complex data queries and transformations.\nIntegration & API Management:\nDevelop and maintain integrations using REST and/or SOAP APIs.\nCollaborate with development teams to support API-driven data exchange.\nData Analytics & Reporting:\nLeverage Power BI to build insightful dashboards and reports that support business decision-making.\nWork with business analysts and data scientists to understand analytical requirements.\nCollaboration & Communication:\nLiaise with stakeholders across different teams to gather requirements and communicate project updates.\nDocument processes, technical specifications, and system integrations.\nQuality & Continuous Improvement:\nMonitor and troubleshoot production issues to ensure minimal downtime.\nEvaluate and implement process improvements to enhance data processing efficiency.\nMandatory Skills\nCore Technologies:\nSQL Server 2017\nPython\nPower BI\nETL Processes\nMaster Data Management/Services (MDM)\nTechnical Expertise:\nAdvanced SQL programming\nData Analytics techniques\nIntegration via REST and/or SOAP APIs\nSoft Skills:\nExcellent communication and collaboration skills",Software,"['SOAP APIs', 'Data Engineer', 'Sql', 'Etl']",2025-06-14 06:34:54
Cybage is hiring For Data Engineer,Cybage Software Private Limited,12-15 Years,,Pune,"The Data Engineer will be responsible for designing, developing, and maintaining robust data pipelines and ETL processes to support data analytics and reporting initiatives. This role requires strong expertise in SQL Server 2017, Python, and Power BI along with handson experience in Master Data Management/Services (MDM) and API integrations. The successful candidate will collaborate with cross-functional teams to ensure high-quality, reliable data delivery and maintain excellent communication with both technical and non-technical stakeholders.\nKey Responsibilities\nData Pipeline & ETL Development:\nDesign, develop, and maintain ETL processes that extract, transform, and load data from various sources.\nEnsure data quality, consistency, and integrity across all systems.\nDatabase & MDM Management:\nManage and optimize SQL Server 2017 databases and Master Data Management/Services (MDM).\nImplement advanced SQL programming techniques to support complex data queries and transformations.\nIntegration & API Management:\nDevelop and maintain integrations using REST and/or SOAP APIs.\nCollaborate with development teams to support API-driven data exchange.\nData Analytics & Reporting:\nLeverage Power BI to build insightful dashboards and reports that support business decision-making.\nWork with business analysts and data scientists to understand analytical requirements.\nCollaboration & Communication:\nLiaise with stakeholders across different teams to gather requirements and communicate project updates.\nDocument processes, technical specifications, and system integrations.\nQuality & Continuous Improvement:\nMonitor and troubleshoot production issues to ensure minimal downtime.\nEvaluate and implement process improvements to enhance data processing efficiency.\nMandatory Skills\nCore Technologies:\nSQL Server 2017\nPython\nPower BI\nETL Processes\nMaster Data Management/Services (MDM)\nTechnical Expertise:\nAdvanced SQL programming\nData Analytics techniques\nIntegration via REST and/or SOAP APIs\nSoft Skills:\nExcellent communication and collaboration skills",Software,"['SOAP APIs', 'Data Engineer', 'Sql', 'Etl']",2025-06-14 06:34:55
Senior Associate - GCP Sr Data Engineer,Genpact,Fresher,,Hyderabad,"Inviting applications for the role ofSenior Associate - GCP Sr Data Engineer\n\n\nWe are seeking a highly accomplished and strategic Google Cloud Data Engineer with deep experience in data engineering, with a significant and demonstrable focus on the Google Cloud Platform (GCP).\n\nIn this leadership role, you will be instrumental in defining and driving our overall data strategy on GCP, architecting transformative data solutions, and providing expert guidance to engineering teams. You will be a thought leader in leveraging GCP's advanced data services to solve complex business challenges, optimize our data infrastructure at scale, and foster a culture of data excellence.\n\n\nResponsibilities\n\n.Define and champion the strategic direction for our data architecture and infrastructure on Google Cloud Platform, aligning with business objectives and future growth.\n\n.Architect and oversee the development of highly scalable, resilient, and cost-effective data platforms and pipelines on GCP, leveraging services like BigQuery, Dataflow, Cloud Composer, DataProc, and more.\n\n.Provide expert-level guidance and technical leadership to senior data engineers and development teams on best practices for data modeling, ETL/ELT processes, and data warehousing within GCP.\n\n.Drive the adoption of cutting-edge GCP data technologies and methodologies to enhance our data capabilities and efficiency.\n\n.Lead the design and implementation of comprehensive data governance frameworks, security protocols, and compliance measures within the Google Cloud environment.\n\n.Collaborate closely with executive leadership, product management, data science, and analytics teams to translate business vision into robust and scalable data solutions on GCP.\n\n.Identify and mitigate critical technical risks and challenges related to our data infrastructure and architecture on GCP.\n\n.Establish and enforce data quality standards, monitoring systems, and incident response processes within the GCP data landscape.\n\n.Mentor and develop senior data engineers, fostering their technical expertise and leadership skills within the Google Cloud context.\n\n.Evaluate and recommend new GCP services and third-party tools to optimize our data ecosystem.\n\n.Represent the data engineering team in strategic technical discussions and contribute to the overall technology roadmap.\n\n\n\nQualifications we seek in you!\n\nMinimum Qualifications / Skills\n\n.Bachelor's or Master's degree in Computer Science, Engineering, or a related field.\n\nexperience in data engineering roles, with a significant and deep focus on the Google Cloud Platform.\n\n.Expert-level knowledge of GCP's core data engineering services and best practices for building scalable and reliable solutions.\n\n.Proven ability to architect and implement complex data warehousing and data lake solutions on GCP (BigQuery, Cloud Storage).\n\n.Mastery of SQL and extensive experience with programming languages relevant to data engineering on GCP (e.g., Python, Scala, Java).\n\n.Deep understanding of data governance principles, security best practices within GCP (IAM, Security Command Center), and compliance frameworks (e.g., GDPR, HIPAA).\n\n.Exceptional problem-solving, strategic thinking, and analytical skills, with the ability to navigate complex technical and business challenges.\n\n.Outstanding communication, presentation, and influencing skills, with the ability to articulate complex technical visions to both technical and non-technical audiences, including executive leadership.\n\n.Proven track record of leading and mentoring high-performing data engineering teams within a cloud-first environment.\n\n\nPreferred Qualifications/ Skills\n\n.Google Cloud Certified Professional Data Engineer.\n\n.Extensive experience with infrastructure-as-code tools for GCP (e.g., Terraform, Deployment Manager).\n\n.Deep expertise in data streaming technologies on GCP (e.g., Dataflow, Pub/Sub, Apache Beam).\n\n.Proven experience in integrating machine learning workflows and MLOps on GCP (e.g., Vertex AI).\n\n.Significant contributions to open-source data projects or active participation in the GCP data engineering community.\n\n.Experience in defining and implementing data mesh or data fabric architectures on GCP.\n\n.Strong understanding of enterprise architecture principles and their application within the Google Cloud ecosystem.\n\n.Experience in [mention specific industry or domain relevant to your company].\n\n.Demonstrated ability to drive significant technical initiatives and influence organizational data strategy.\n\n\n\n.\n\n.\n\n.\n\n.\n\n.",IT/Computers - Hardware & Networking,[],2025-06-14 06:34:56
"Lead Consultant-Data Engineer, Azure+Python!",Genpact,Fresher,,Bengaluru,"Inviting applications for the role of Lead Consultant-Data Engineer, Azure+Python!\n\n\nResponsibilities\n\n.Design and deploy scalable, highly available, and fault-tolerant AWS data processes using AWS data services (Glue, Lambda, Step, Redshift)\n\n.Monitor and optimize the performance of cloud resources to ensure efficient utilization and cost-effectiveness.\n\n.Implement and maintain security measures to protect data and systems within the AWS environment, including IAM policies, security groups, and encryption mechanisms.\n\n.Migrate the application data from legacy databases to Cloud based solutions (Redshift, DynamoDB, etc) for high availability with low cost\n\n.Develop application programs using Big Data technologies like Apache Hadoop, Apache Spark, etc with appropriate cloud-based services like Amazon AWS, etc.\n\n.Build data pipelines by building ETL processes (Extract-Transform-Load)\n\n.Implement backup, disaster recovery, and business continuity strategies for cloud-based applications and data.\n\n.Responsible for analysing business and functional requirements which involves a review of existing system configurations and operating methodologies as well as understanding evolving business needs\n\n.Analyse requirements/User stories at the business meetings and strategize the impact of requirements on different platforms/applications, convert the business requirements into technical requirements\n\n.Participating in design reviews to provide input on functional requirements, product designs, schedules and/or potential problems\n\n.Understand current application infrastructure and suggest Cloud based solutions which reduces operational cost, requires minimal maintenance but provides high availability with improved security\n\n.Perform unit testing on the modified software to ensure that the new functionality is working as expected while existing functionalities continue to work in the same way\n\n.Coordinate with release management, other supporting teams to deploy changes in production environment\n\n\nQualifications we seek in you!\n\nMinimum Qualifications\n\n.Experience in designing, implementing data pipelines, build data applications, data migration on AWS\n\n.Strong experience of implementing data lake using AWS services like Glue, Lambda, Step, Redshift\n\n.Experience of Databricks will be added advantage\n\n.Strong experience in Python and SQL\n\n.Strong understanding of security principles and best practices for cloud-based environments.\n\n.Experience with monitoring tools and implementing proactive measures to ensure system availability and performance.\n\n.Excellent problem-solving skills and ability to troubleshoot complex issues in a distributed, cloud-based environment.\n\n.Strong communication and collaboration skills to work effectively with cross-functional teams.\n\n\nPreferred Qualifications/ Skills\n\n.Master&rsquos Degree-Computer Science, Electronics, Electrical.\n\n.AWS Data Engineering & Cloud certifications, Databricks certifications\n\n.Experience of working with Oracle ERP\n\n.Experience with multiple data integration technologies and cloud platforms\n\n.Knowledge of Change & Incident Management process\n\n\n\n.\n\n.\n\n.\n\n.\n\n.",IT/Computers - Hardware & Networking,[],2025-06-14 06:34:57
Manager_Lead_Data Engineer_Pune,Vodafone,8-10 Years,,Pune,"About VOIS:\nVOIS (Vodafone Intelligent Solutions) is a strategic arm of Vodafone Group Plc, creating value and enhancing quality and efficiency across 28 countries, and operating from 7 locations: Albania, Egypt, Hungary, India, Romania, Spain and the UK.\nOver 29,000 highly skilled individuals are dedicated to being Vodafone Group's partner of choice for talent, technology, and transformation. We deliver the best services across IT, Business Intelligence Services, Customer Operations, Business Operations, HR, Finance, Supply Chain, HR Operations, and many more.\nEstablished in 2006, VOIS has evolved into a global, multi-functional organisation, a Centre of Excellence for Intelligent Solutions focused on adding value and delivering business outcomes for Vodafone.\nAbout VOIS India:\nIn 2009, VOIS started operating in India and now has established global delivery centres in Pune, Bangalore and Ahmedabad. With more than 14,500 employees, VOIS India supports global markets and group functions of Vodafone, and delivers best-in-class customer experience through multi-functional services in the areas of Information Technology, Networks, Business Intelligence and Analytics, Digital Business Solutions (Robotics & AI), Commercial Operations (Consumer & Business), Intelligent Operations, Finance Operations, Supply Chain Operations and HR Operations and more.\nJob Description\nKey accountabilities and decision ownership\nPrepare LLD in accordance with HLD by Architect of the project.\nBuild and Operationalize data processing system as per Low level design.\nUnderstanding and experience of GCP Big Query, Airflow for optimized development approach\nKnowledge of data pipeline patterns used in cloud, ability to think outside the box to understand way around of product limitations.\nAct as SME in the team on GCP Data Engineering, impart knowledge to junior resources to enable them working alongside.\nDemonstrate zeal to pick up and learn new things within as well as outside projects and apply learning in day-to-day work.\nHandle small team / projects when required from build to closure.\nContribution to the organization in terms of process adoption, resource optimization, tools adoption to bring in efficiency and uplift quality.\nContribution to the Guilds with the technical expertise\nCore competencies, knowledge, and experience :\nEssential:\n1. Experience on GCP suite of products like big query,\n2. Prior experience working on MPP databases (Teradata, Postgre etc.)\n3. Knowledge of Apache Airflow or Cloud composer for scheduling.\nExperience:\n4. Overall experience of 8+ years\n5. Minimum 2-4 Years of relevant experience Cloud platforms (GCP/AWS/Azure)\nPlus:\n6. Google Cloud Certified Professional Data Engineer\nVOIS Equal Opportunity Employer Commitment\nIndia:\nVOIS is proud to be an Equal Employment Opportunity Employer. We celebrate differences and we welcome and value diverse people and insights. We believe that being authentically human and inclusive powers our employees growth and enables them to create a positive impact on themselves and society. We do not discriminate based on age, colour, gender (including pregnancy, childbirth, or related medical conditions), gender identity, gender expression, national origin, race, religion, sexual orientation, status as an individual with a disability, or other applicable legally protected characteristics.\nAs a result of living and breathing our commitment, our employees have helped us get certified as a Great Place to Work in India for four years running. We have been also highlighted among the Top 10 Best Workplaces for Millennials, Equity, and Inclusion,Top 50 Best Workplaces for Women,Top 25 Best Workplaces in IT & IT-BPMand10th Overall Best Workplaces in India by the Great Place to Work Institute in 2024. These achievements position us among a select group of trustworthy and high-performing companies which put their employees at the heart of everything they do.\nBy joining us, you are part of our commitment. We look forward to welcoming you into our family which represents a variety of cultures, backgrounds, perspectives, and skills!\nApply now, and we'll be in touch!",Telecom/ISP,"['Cloud Composer', 'GCP Big Query', 'Apache Airflow']",2025-06-14 06:34:58
Data Engineer,Xoriant Corporation,6-10 Years,,"Gurugram, Mumbai, Pune","Job description\nThis will be data engineer position. Detail JD as follow\nKey Responsibilities:\nData Infrastructure Development:Design, implement, and maintain scalable data pipelines and data warehousing solutions using SQL and Python.\nData Integration:Collaborate with cross-functional teams to integrate data from various sources (internal systems, APIs, third-party tools) into our data ecosystem.\nETL Processes:Develop and optimize ETL processes for the extraction, transformation, and loading of large data sets.\nAPI Integration:Develop and maintain APIs for data ingestion, ensuring seamless data flow between systems and platforms.\nDatabase Management:Design and manage relational and non-relational databases, ensuring high availability, security, and performance.\nData Quality Assurance:Implement data validation checks to ensure the integrity and accuracy of data across systems.\nAutomation & Optimization:Automate repetitive data tasks and optimize data workflows for better performance and scalability.\nCollaboration:Work closely with other teams to provide the data infrastructure support as needed\nRequired Skills & Qualifications:\nStrong proficiency in Pythonwith experience in libraries such as Pandas, NumPy, and SQLAlchemy.\nExtensive experience with SQLand relational database management systems (RDBMS), such as PostgreSQL, MySQL, or MS SQL Server.\nExperience with API integrationfor data ingestion and communication between different systems (RESTful APIs, JSON, OAuth).\nSolid understanding of data modelingand database design concepts.\nExperience with ETL frameworksand data integration techniques.\nFamiliarity with cloud platformssuch as AWS, GCP, or Azure is a plus.\nKnowledge of data warehousing conceptsand technologies like Redshift, Snowflake, or BigQuery is a plus.\nStrong problem-solving and debugging skillswith the ability to troubleshoot data issues across different systems.",Information Technology,"['Python', 'Sql', 'Etl']",2025-06-14 06:35:00
Data Engineer/Data Analyst,Navigators Software,3-8 Years,,"Navi Mumbai, Mumbai City, Mumbai","Data Pipeline Architecture and Development:\nDesign robust, scalable, and efficient data pipelines to gather data from diverse sources (databases, APIs, flat files, etc.)\nImplement ETL/ELT procedures, ensuring data is extracted, transformed, and loaded into appropriate storage systems for analysis\nChoose and use optimal data warehousing/lake solutions (e.g., Snowflake, Redshift, BigQuery) depending on needs\nData Quality and Integrity:\nThoroughly check data for inconsistencies, errors, and missing values, establishing cleaning processes\nImplement data validation rules and run regular checks to maintain data quality standards for the data science team\nMonitor pipelines for failures, promptly addressing data anomalies or breaks\nData Modeling and Optimization:\nCollaborate with data scientists to design data models according to their analysis requirements\nTransform raw data into usable, analysis-ready formats\nOptimize data structures and databases to improve query performance and efficiency for the data scientists workloads\nFeature Engineering:\nWork closely with data scientists to understand model requirements and engineer new features from the existing data",Information Technology,"['ETL/ELT', 'Data pipelines', 'Data warehousing (Snowflake/Redshift/BigQuery)', 'Feature engineering', 'Data quality & validation', 'Data Modeling']",2025-06-14 06:35:01
Engineer - Data Engineer,Astrazeneca,3-6 Years,,Chennai,"Architect of Solutions: Lead the design, development, and enhancement of scalable ETL pipelines and Data Products, as part of a Data Mesh inspired strategy.\nTechnical Expertise: Demonstrate your expertise in ELT solutions (DBT, Snowflake), Python and AWS ecosystems to deliver exceptional solutions.\nCollaborative Spirit: Work hand-in-hand with global and diverse Agile teams, from data to design, to overcome technical data challenges.\nInnovate & Inspire: Stay ahead of the curve by integrating the latest industry trends and innovations into your work such as GenAI.\nTools:\nETL Tools: Master tools like Python and explore preferred technologies such as DBT and Snowflake.\nDatabases: Utilize your SQL prowess across platforms like Snowflake, Postgres.\nAWS Cloud Expertise: Bring your AWS knowledge to life with services like Lambda, S3, Athena, Step Functions and more.\nEssential Skills/Experience\n3 to 6 years experience\nA proactive mindset and enthusiasm for Agile environments.\nStrong hands-on experience with cloud providers and services.\nExperience in performance tuning SQL and ETL pipelines.\nExtensive experience in troubleshooting data issues, analyzing end-to-end data pipelines and in working with users in resolving issues.\nMasterful debugging and testing skills to ensure excellence in execution.\nInspiring communication abilities that elevate team collaboration.\nExperience of structured, semi-structured (XML, JSON) and unstructured data handling including extraction and ingestion via web-scraping and FTP/SFTP.\nProduction experience delivering CI/CD pipelines (Github, Jenkins, DataOps.Live).\nExcellent Cloud Devops Engineer who can develop, test and maintain CICD Pipeline using Terraform, cloud formation.\nRemain up to date with latest technologies, like GenAI / AI platforms and FAIR scoring to improve outcomes",Pharmaceutical,"['Agile', 'Sql', 'Data Modelling', 'Power Bi', 'Cloud', 'Aws', 'Data Warehousing', 'Devops']",2025-06-14 06:35:03
Azure Data Engineer (Power Platform)- Priniciple,Systechcorp Private Limited,8-13 Years,,"Hyderabad, Chennai, Pune","Key Responsibilities:\nDesign, develop, and maintain data pipelines using Python, SQL, and Kedro\nImplement serverless solutions using AWS Lambda and Step Functions\nDevelop and manage data workflows in Azure and AWS cloud environments\nCreate integrations between data systems and Power Platform (Power Apps, Power Automate)\nDesign, develop, and maintain APIs for data exchange and integration\nImplement solutions to extract data from APIs and store in databases (Dataverse & PostgreSQL).\nOptimize data storage and retrieval processes for improved performance\nCollaborate with cross-functional teams to understand data requirements and provide solutions\nAPI Integration and data extraction from Sharepoint\nRequired Skills and Experience:\nExpert in Azure Data Engineer role, able to build solutions in Power Platform environment, AWS cloud and Azure cloud services.\nGood Knowledge on integration like Integration using API, XML with ADF, Logic App,\nAzure Functions, AWS Step functions, AWS Lambda Functions, etc\nStrong proficiency in Python, Pyspark (especially Kedro Framework) and SQL.\nHaving knowledge and experience in Scala, Java and R would be a good to have skill.\nExperience with Kedro framework for data engineering pipelines\nExpertise in AWS services, particularly Lambda and Step Functions\nProven experience with Power Platform (Power Apps, Power Automate) and Dataverse.\nStrong understanding of API development and integration\nExperience in database design and management\nExcellent problem-solving and communication skills",Software,"['XML with ADF', 'Logic App', 'Azure Data Engineer', 'Api', 'Python', 'Sql']",2025-06-14 06:35:04
Senior Consultant - Cloud Data Engineer,Astrazeneca,5-6 Years,,Chennai,"Lead the design, development, and maintenance of reliable, scalable data pipelines and ETL processes using tools such as SnapLogic, Snowflake, DBT, Fivetran, Informatica, and Python.\nWork closely with data scientists to understand model requirements and prepare the right data pipelines for training and deploying machine learning models.\nCollaborate with data scientists, analysts, and business teams to understand and optimize data requirements and workflows.\nApply Power BI, Spotfire, Domo, Qlik Sense to create actionable data visualizations and reports that drive business decisions.\nImplement standard methodologies for version control and automation using Git Actions, Liquibase, Flyway, and CI/CD tools.\nOptimize data storage, processing, and integration bringing to bear AWS Data Engineering tools (e.g., AWS Glue, Amazon Redshift, Amazon S3, Amazon Kinesis, AWS Lambda, Amazon EMR).\nTroubleshoot, debug, and resolve issues related to existing data pipelines and architectures.\nEnsure data security, privacy, and compliance with industry regulations and organizational policies.\nProvide mentorship to junior engineers, offering guidance on best practices and supporting technical growth within the team.\nEssential Skills/Experience\nSnapLogic:\nExpertise in SnapLogic for building, managing, and optimizing both batch and real-time data pipelines.\nProficiency in using SnapLogic Designer for designing, testing, and deploying data workflows.\nIn-depth experience with SnapLogic Snaps (e.g., REST, SOAP, SQL, AWS S3) and Ultra Pipelines for real-time data streaming and API management.\nAWS:\nStrong experience with AWS Data Engineering tools, including AWS Glue, Amazon Redshift, Amazon S3, AWS Lambda, Amazon Kinesis, AWS DMS, and Amazon EMR.\nExpertise in cloud data architectures, data migration strategies, and real-time data processing on AWS platforms.\nSnowflake:\nExtensive experience in Snowflake cloud data warehousing, including data modeling, query optimization, and managing ETL pipelines using DBT and Snowflake-native tools.\nFivetran:\nProficient in Fivetran for automating data integration from various sources to cloud-based data warehouses, optimizing connectors for data replication and transformation.\nReal-Time Messaging and Stream Processing:\nExperience with real-time data processing frameworks (e.g., Apache Kafka, Amazon Kinesis, RabbitMQ, Apache Pulsar).\nDesirable Skills/Experience\nExposure to other cloud platforms such as Azure or Google Cloud Platform (GCP).\nFamiliarity with data governance, data warehousing, and data lake architectures.",Pharmaceutical,"['snap logic', 'Azure', 'Google Cloud Platform', 'Data Warehousing', 'Apache Kafka', 'Data Modelling', 'Aws', 'Python', 'Informatica']",2025-06-14 06:35:06
Sr Specialist Development - Data Engineer,Now100 Solutions Private Limited,6-8 Years,,Gurugram,"Core Responsibility:\nProvide delivery that is efficient, functional, and maintainable.\nApply analysis skills to understand and assess the impact of functional and technical changes within the environment and provide working estimates accordingly.\nWork with leads to deliver projects. Be accountable for work and responsible for communicating effectively with Leads.\nIs flexible to support business needs.\nCommunicate effectively with project stakeholders\nLearn about our internal clients business, existing pipelines, the infrastructure that supports them and the design patterns used.\nEnsure SLF Information Security Policies are complied to in any solution.\nEligibility:\nA University degree in Computer Science, related technology degree/diploma, or equivalent experience\nExcellent Communication skills.\nStrong problem solving and analytical capabilities.\n6-8 years of experience in data engineering space.\nTechnical Skills:\nQliq Data Catalyst\nSqoophttps://sqoop.apache.org/\nSQL\nSQL BCPhttps://learn.microsoft.com/en-us/sql/tools/bcp-utilityview=sql-server-ver16\nUnderstanding of AWS (specifically AWS EMR with mapreduce)\nHADOOP\nLinux OS\nAutosys & Control M for scheduling (Autosys is replacing Control M)\nGIT & Jira\nETL Concepts\nCommitment to quality and best practices.",IT Management,"['AWS EMR', 'Autosys Scheduling', 'ETL Concepts', 'data engineering', 'Sql', 'Hadoop']",2025-06-14 06:35:08
Specialist Development _Data Engineer,Now100 Solutions Private Limited,6-8 Years,,Gurugram,"Core Responsibility:\nProvide delivery that is efficient, functional, and maintainable.\nApply analysis skills to understand and assess the impact of functional and technical changes within the environment and provide working estimates accordingly.\nWork with leads to deliver projects. Be accountable for work and responsible for communicating effectively with Leads.\nIs flexible to support business needs.\nCommunicate effectively with project stakeholders.\nLearn about our internal clients business, existing pipelines, the infrastructure that supports them and the design patterns used.\nEnsure SLF Information Security Policies are complied to in any solution.\nEligibility:\nA University degree in Computer Science, related technology degree/diploma, or equivalent experience\nExcellent Communication skills.\nStrong problem solving and analytical capabilities.\n6-8 years of experience in data engineering space.\nTechnical Skills:\nQliq Data Catalyst\nSqoophttps://sqoop.apache.org/\nSQL\nSQL BCPhttps://learn.microsoft.com/en-us/sql/tools/bcp-utilityview=sql-server-ver16\nUnderstanding of AWS (specifically AWS EMR with MapReduce)\nHADOOP\nLinux OS\nAutosys & Control M for scheduling (Autosys is replacing Control M)\nGIT & Jira\nETL Concepts\nCommitment to quality and best practices.",IT Management,"['AWS EMR', 'Autosys Scheduling', 'ETL Concepts', 'data engineering', 'Sql', 'Hadoop']",2025-06-14 06:35:10
Consultant - Data Engineer,Astrazeneca,3-8 Years,,Chennai,"Technical Expertise:\nDesign, develop, and implement scalable processes to extract, transform, and load data from various sources into data warehouses.\nDemonstrate expert understanding of the application development processes of AstraZeneca s implementation of the data product.\nDesign, manage, and fine-tune SQL queries and procedures to achieve optimal performance and scalability.\nWork on JIRAs and provide support on production issues and enhancements whenever required.\n2. Quality Engineering Standards:\nMonitor and optimize data pipelines, troubleshoot, and resolve any issues that may arise during data processing.\nMaintain quality standards in design, code, and data models, ensuring thorough reviews at each stage of the development life cycle.\nProvide detailed analysis and documentation of processes and flows where necessary.\n3. Collaboration:\nWork closely with data engineers to understand data sources, transformations, and dependencies thoroughly.\nCollaborate with cross-functional teams, including Product Owners, Business Analysts, and CriticalOps teams to ensure seamless data integration and reliability.\n4. Innovation and Process Improvement:\nDrive the adoption of new technologies and tools to improve data engineering processes and efficiency.\nRecommend and implement enhancements to improve reliability, efficiency, and quality of data processing pipelines.\nEssential Skills/Experience:\nEducation:\nBachelor s degree in Computer Science, Information Technology, or a related field.\nSkills and Experience:\nStrong experience with SQL, warehousing, and building ETL pipelines (Snaplogic, Informatica).\nProficient in working with column-level databases like Redshift, Cassandra, BigQuery.\nSolid understanding of data modeling, ETL processes, and data warehousing concepts.\nDeep SQL knowledge to write complex queries for data extraction, transformation, and reporting.\nExcellent communication with the ability to collaborate effectively with technical and non-technical stakeholders.\nKnowledge of database design principles, data modeling, and the ability to translate business requirements into database structures.\nStrong analytical skills with the ability to troubleshoot and deliver solutions in complex data environments.\nExperienced in Agile Development techniques and methodologies.\nDesirable Skills/Experience:\nExperience with Databricks/Snowflake.\nProficiency in scripting and programming languages such as Python.\nExperience with reporting tools like PowerBI.\nPrior significant experience working in Pharmaceutical or Healthcare industry IT environments.",Pharmaceutical,"['snowflake', 'Pharma', 'Healthcare', 'Databricks', 'Python', 'Power Bi', 'Sql', 'Data Modeling', 'Data Warehousing']",2025-06-14 06:35:11
Data Engineer AWS/ Power platform Engineer,Systechcorp Private Limited,3-7 Years,,Bengaluru,"Design and develop data pipelines for Generative AI projects by leveraging a combination of technologies, including Vector DB, Graph DB, Airflow, Spark, PySpark, Python, LangChain, AWS Functions, Redshift, and SSIS. This will involve the logical and efficient integration of these tools to create seamless, high-performance data flows that efficiently support the data requirements of our cutting-edge AI initiatives. Collaborate with data scientists, AI researchers, and other stakeholders to understand data requirements and translate them into effective data engineering solutions.\nUser will be managing movement, organization and quality assessments of large set of data to facilitate the creation of Knowledge base for RAG systems and model training\nDemonstrate familiarity with data integration services such as AWS Glue and Azure Data Factory, showcasing the ability to effectively utilize these platforms for seamless data ingestion, transformation, and orchestration across various sources and destinations.\nPossess proficiency in constructing data warehouses and data lakes, demonstrating a strong foundation in organizing and consolidating large volumes of structured and unstructured data for efficient storage, retrieval, and analysis.\nOptimize and maintain data pipelines to ensure high-performance, reliable, and scalable data processing.\nDevelop and implement data validation and quality assurance procedures to ensure the accuracy and consistency of the data used in Generative AI projects.\nMonitor and troubleshoot data pipeline performance, identify bottlenecks, and implement improvements as necessary.\nStay current with emerging trends and technologies in the fields of data engineering, Generative AI, and related areas to ensure the continued success of our projects.\nCollaborate with team members on documentation, knowledge sharing, and best practices for data engineering within a Generative AI context.\nEnsure data privacy and security compliance in accordance with industry standards and regulations.\nQualifications we seek in you:\nBachelors or Masters degree in Computer Science, Engineering, or a related field.\nStrong experience with data engineering technologies, including Vector DB, Graph DB, Airflow, Spark, PySpark, Python, langchain, AWS Functions, Redshift, and SSIS.\nStrong understanding of data warehousing concepts, ETL processes, and data modeling.\nStrong understanding of S3 and code-based scripting to move large volumes of data across application storage layers\nFamiliarity with Generative AI concepts and technologies, such as GPT-4, Transformers, and other natural language processing techniques.\nExcellent problem-solving, analytical, and critical thinking skills.\nStrong communication and collaboration skills, with the ability to work effectively with cross-functional teams.\nPreferred Qualifications/ skills\nKnowledge of cloud computing platforms, such as AWS, Azure, or Google Cloud Platform, is a plus.\nExperience with big data technologies, such as Hadoop, Hive, or Presto, is a plus.\nFamiliarity with machine learning frameworks, such as TensorFlow or PyTorch, is a plus.\nA continuous learning mindset and a passion for staying up-to-date with the latest advancements in data engineering and Generative AI.",Software,"['Airflow', 'Db', 'Spark', 'Pyspark', 'Python', 'AWS']",2025-06-14 06:35:13
Lead Consultant - Product Data Engineer,Astrazeneca,10-15 Years,,Chennai,"Accountabilities\nLead design, development, testing, deployment, and operations for data, analytics, and AI products supporting the US Oncology Business Unit.\nMaintain deep working knowledge of US commercial datasets (Sales, Patient, Digital, etc.), their data lineage, business rules, important metrics, usage, and operations.\nDevelop and maintain an in-depth understanding of data engineering and AI capabilities, staying ahead of with the latest advancements and industry trends.\nCollaborate with Global Capability, Business Engagement, Delivery, and Platform teams to drive the best customer experience and outcomes.\nEnsure products and solutions adhere to best practices and AZ architecture standards and strategy.\nImplement processes, tools, and automations to ensure developer productivity and quality.\nProvide technical expertise to peers and 3rd party development suppliers to ensure consistency with AZ IT and Commercial IT standards and processes.\nWork with platform delivery resources to validate business and functional requirements as part of product development responsibilities.\nEvaluate new technologies/application features and make recommendations for technology and business strategic roadmaps.\nDemonstrate expert understanding of AstraZeneca s application development processes for data products.\nMentor junior staff members through technical training sessions and project output reviews.\nEstablish partnerships with 3rd party suppliers for staff augmentation and project capacity, providing technical guidance for projects implemented through these suppliers.\nServe as an internal consultant for IT and business partners across regions and business areas (Commercial, Medical).\nContinuous Improvement and Innovation\nPartner with Platform Success Leads on continuous improvement opportunities within platform and product-level backlogs.\nIdentify new hardware/software technologies to fit specialized business needs and configurations.\nIdentify issues with current processes and provide feedback on proposed changes to enhance IT effectiveness.\nEssential Skills/Experience\nBachelor s degree or equivalent in Computer Engineering, Computer Science, or a related field\n10+ years of experience in data/software engineering roles\n5+ years of experience with building AWS cloud-based data pipelines\n5+ years of experience with Python and Spark\n3+ years of experience with Databricks, including Spark-based processing, data pipelines, and the platforms tools for analytics and machine learning\nCommitment to staying up-to-date with advancements in data engineering, AI, and Databricks features\nExperience in Continuous Integration and Deployment methods delivered as part of an agile team\nAbility to interpret and communicate technical information in business-friendly language\nStrong communication and interpersonal skills\nProven experience managing and coaching/mentoring junior engineers\nProven experience developing and managing relationships with 3rd party suppliers\nStrong written and verbal communication skills\nExperience working in a global organization with a complex geographical structure\nDesirable Skills/Experience\nDatabricks certification\nKnowledge of machine learning concepts, including model training, evaluation, deployment, and monitoring within Databricks MLflow or other MLOps platforms\nPrior experience working in Pharmaceutical or Healthcare industry IT environment\nExperience with IQVIA Claims, National and subnational sales data\nExperience working within a DevOps delivery model\nExperience working within a quality and compliance environment and application of policies, procedures, and guidelines",Pharmaceutical,"['Pharma', 'IT', 'Machine Learning', 'MLops', 'Devops', 'Aws', 'Cloud', 'Python', 'Spark']",2025-06-14 06:35:24
Principal Data Engineer,Amgen Technology Private Limited,6-11 Years,,Hyderabad,"Role Responsibilities:\nCollaborate with stakeholders to define and document business and system requirements\nDesign and optimize CI/CD pipelines and troubleshoot deployment issues\nTranslate AI/ML concepts into technical requirements and test plans\nMaintain backlog and user stories using tools like Jira\nKey Deliverables:\nBusiness process maps, user stories, and test cases\nFully functional and reliable CI/CD automation\nTechnical specifications for AI/ML solutions\nScalable and high-performing infrastructure setup",Pharmaceutical,"['CI/CD', 'Jenkins', 'Gitlab', 'Python', 'Sdlc']",2025-06-14 06:35:29
Principal Data Engineer,Amgen Technology Private Limited,6-11 Years,,Hyderabad,"Role Responsibilities:\nCollaborate with stakeholders to define and document business and system requirements\nDesign and optimize CI/CD pipelines and troubleshoot deployment issues\nTranslate AI/ML concepts into technical requirements and test plans\nMaintain backlog and user stories using tools like Jira\nKey Deliverables:\nBusiness process maps, user stories, and test cases\nFully functional and reliable CI/CD automation\nTechnical specifications for AI/ML solutions\nScalable and high-performing infrastructure setup",Pharmaceutical,"['CI/CD', 'Jenkins', 'Gitlab', 'Python', 'Sdlc']",2025-06-14 06:35:30
Data Engineer AWS/ Power platform Engineer,Systechcorp Private Limited,3-7 Years,,Bengaluru,"Design and develop data pipelines for Generative AI projects by leveraging a combination of technologies, including Vector DB, Graph DB, Airflow, Spark, PySpark, Python, LangChain, AWS Functions, Redshift, and SSIS. This will involve the logical and efficient integration of these tools to create seamless, high-performance data flows that efficiently support the data requirements of our cutting-edge AI initiatives. Collaborate with data scientists, AI researchers, and other stakeholders to understand data requirements and translate them into effective data engineering solutions.\nUser will be managing movement, organization and quality assessments of large set of data to facilitate the creation of Knowledge base for RAG systems and model training\nDemonstrate familiarity with data integration services such as AWS Glue and Azure Data Factory, showcasing the ability to effectively utilize these platforms for seamless data ingestion, transformation, and orchestration across various sources and destinations.\nPossess proficiency in constructing data warehouses and data lakes, demonstrating a strong foundation in organizing and consolidating large volumes of structured and unstructured data for efficient storage, retrieval, and analysis.\nOptimize and maintain data pipelines to ensure high-performance, reliable, and scalable data processing.\nDevelop and implement data validation and quality assurance procedures to ensure the accuracy and consistency of the data used in Generative AI projects.\nMonitor and troubleshoot data pipeline performance, identify bottlenecks, and implement improvements as necessary.\nStay current with emerging trends and technologies in the fields of data engineering, Generative AI, and related areas to ensure the continued success of our projects.\nCollaborate with team members on documentation, knowledge sharing, and best practices for data engineering within a Generative AI context.\nEnsure data privacy and security compliance in accordance with industry standards and regulations.\nQualifications we seek in you:\nBachelors or Masters degree in Computer Science, Engineering, or a related field.\nStrong experience with data engineering technologies, including Vector DB, Graph DB, Airflow, Spark, PySpark, Python, langchain, AWS Functions, Redshift, and SSIS.\nStrong understanding of data warehousing concepts, ETL processes, and data modeling.\nStrong understanding of S3 and code-based scripting to move large volumes of data across application storage layers\nFamiliarity with Generative AI concepts and technologies, such as GPT-4, Transformers, and other natural language processing techniques.\nExcellent problem-solving, analytical, and critical thinking skills.\nStrong communication and collaboration skills, with the ability to work effectively with cross-functional teams.\nPreferred Qualifications/ skills\nKnowledge of cloud computing platforms, such as AWS, Azure, or Google Cloud Platform, is a plus.\nExperience with big data technologies, such as Hadoop, Hive, or Presto, is a plus.\nFamiliarity with machine learning frameworks, such as TensorFlow or PyTorch, is a plus.\nA continuous learning mindset and a passion for staying up-to-date with the latest advancements in data engineering and Generative AI.",Software,"['Airflow', 'Db', 'Spark', 'Pyspark', 'Python', 'AWS']",2025-06-14 06:35:32
Data Engineer - AI,Quinnox Consultancy Services Limited,3-8 Years,,Bengaluru,"Responsibilities\nBuild, maintain, and optimize data pipelines and storage solutions to ensure data quality, accessibility, and performance for automation testing and AI applications.\nDevelop, implement, and maintain Python-based solutions for ETL processes, data transformation, and system integration.\nIntegrate data engineering workflows with AI and machine learning applications, ensuring compatibility and scalability.\nDesign and implement REST APIs and WebSocket connections for real-time data transfer and integration.\nCollaborate closely with AI and R&D teams to support new AI features, ensuring seamless integration with data systems.\nWork on distributed systems to ensure scalability and reliability for high-volume data processing tasks.\nImplement and optimize data solutions for structured and unstructured data, including SQL and NoSQL databases.\nParticipate in system design and architecture discussions to create high-performance, scalable, and maintainable systems.\nJob Description\nWe are looking for aData Engineerwho is passionate about building scalable, high-performance data solutions. The ideal candidate should have strong Python programming skills and hands-on experience in data engineering tasks, including data pipelines, distributed systems, and database management (SQL and NoSQL). You will work in a collaborative environment, focusing on implementing code day-in and day-out while ensuring compatibility with AI and automation testing workflows.\nSkills\n1.Python\n2.Problem Solving and Programming\n3.Data Engineering\n4.GenerativeAI\n5.REST APIs\n6.WebSockets\n7.UI and Integration\n8.Data Pipelines\n9.SQL & NoSQL Databases\n10.Distributed Systems\n11.High & Low-level System Design\n12.Data Warehouses and Lakes\n13.AIOps\n14.RPA\nWhat We are Looking For\nHands-on Python Expertise: Ability to write efficient, maintainable, and scalable code daily.\nImplementation-Oriented: A focus on delivering working code and solutions, not just theoretical designs.\nTeam Collaboration: A proactive team player who thrives in a collaborative environment.\nData Expertise: Solid knowledge of data transformation, storage, and real-time data integration techniques.\nThis role is a fantastic opportunity for individuals who enjoy working closely with data, building practical solutions, and pushing the boundaries of automation testing with AI.",Information Technology,"['Ai', 'Nosql', 'Integration', 'Rpa', 'Rest Api', 'Websocket', 'Python', 'Sql']",2025-06-14 06:35:34
AWS Data Engineer,Now100 Solutions Private Limited,5-7 Years,,Bengaluru,"We are seeking an experienced AWS Data Engineer to support the on-ground client team in delivering high-quality data engineering solutions. The ideal candidate will have hands-on experience with AWS data services and strong programming skills.\nKey Responsibilities:\nCollaborate with the client's onsite team to ensure timely completion of deliverables\nDesign, develop, and maintain data pipelines using AWS services such as Lambda, Glue, and Redshift\nWrite efficient, reusable, and optimized code in Python and SQL for data processing and transformation\nMonitor, troubleshoot, and optimize data workflows for performance and reliability\nImplement best practices in data engineering and cloud infrastructure management\nParticipate in code reviews, testing, and documentation activities\nRequired Skills:\nProven experience with AWS Data Engineering services: Lambda, Glue, Redshift\nStrong programming skills in Python and SQL\nExperience working in collaboration with onsite client teams to meet project goals\nGood problem-solving and communication skills\nKnowledge of data pipeline design, ETL/ELT processes, and cloud data management",IT Management,"['Data Pipelines', 'Python', 'Aws Lambda', 'AWS Glue', 'Redshift', 'Sql']",2025-06-14 06:35:35
Azure Data Engineer (Power Platform)- Priniciple,Systechcorp Private Limited,8-13 Years,,Bengaluru,"Key Responsibilities:\nDesign, develop, and maintain data pipelines using Python, SQL, and Kedro\nImplement serverless solutions using AWS Lambda and Step Functions\nDevelop and manage data workflows in Azure and AWS cloud environments\nCreate integrations between data systems and Power Platform (Power Apps, Power Automate)\nDesign, develop, and maintain APIs for data exchange and integration\nImplement solutions to extract data from APIs and store in databases (Dataverse & PostgreSQL).\nOptimize data storage and retrieval processes for improved performance\nCollaborate with cross-functional teams to understand data requirements and provide solutions\nAPI Integration and data extraction from Sharepoint\nRequired Skills and Experience:\nExpert in Azure Data Engineer role, able to build solutions in Power Platform environment, AWS cloud and Azure cloud services.\nGood Knowledge on integration like Integration using API, XML with ADF, Logic App,\nAzure Functions, AWS Step functions, AWS Lambda Functions, etc\nStrong proficiency in Python, Pyspark (especially Kedro Framework) and SQL.\nHaving knowledge and experience in Scala, Java and R would be a good to have skill.\nExperience with Kedro framework for data engineering pipelines\nExpertise in AWS services, particularly Lambda and Step Functions\nProven experience with Power Platform (Power Apps, Power Automate) and Dataverse.\nStrong understanding of API development and integration\nExperience in database design and management\nExcellent problem-solving and communication skills",Software,"['XML with ADF', 'Logic App', 'Azure Data Engineer', 'Api', 'Python', 'Sql']",2025-06-14 06:35:37
Data Engineer AWS/ Power platform Engineer,Systechcorp Private Limited,3-7 Years,,"Delhi, Kolkata, Mumbai","Design and develop data pipelines for Generative AI projects by leveraging a combination of technologies, including Vector DB, Graph DB, Airflow, Spark, PySpark, Python, LangChain, AWS Functions, Redshift, and SSIS. This will involve the logical and efficient integration of these tools to create seamless, high-performance data flows that efficiently support the data requirements of our cutting-edge AI initiatives. Collaborate with data scientists, AI researchers, and other stakeholders to understand data requirements and translate them into effective data engineering solutions.\nUser will be managing movement, organization and quality assessments of large set of data to facilitate the creation of Knowledge base for RAG systems and model training\nDemonstrate familiarity with data integration services such as AWS Glue and Azure Data Factory, showcasing the ability to effectively utilize these platforms for seamless data ingestion, transformation, and orchestration across various sources and destinations.\nPossess proficiency in constructing data warehouses and data lakes, demonstrating a strong foundation in organizing and consolidating large volumes of structured and unstructured data for efficient storage, retrieval, and analysis.\nOptimize and maintain data pipelines to ensure high-performance, reliable, and scalable data processing.\nDevelop and implement data validation and quality assurance procedures to ensure the accuracy and consistency of the data used in Generative AI projects.\nMonitor and troubleshoot data pipeline performance, identify bottlenecks, and implement improvements as necessary.\nStay current with emerging trends and technologies in the fields of data engineering, Generative AI, and related areas to ensure the continued success of our projects.\nCollaborate with team members on documentation, knowledge sharing, and best practices for data engineering within a Generative AI context.\nEnsure data privacy and security compliance in accordance with industry standards and regulations.\nQualifications we seek in you:\nBachelors or Masters degree in Computer Science, Engineering, or a related field.\nStrong experience with data engineering technologies, including Vector DB, Graph DB, Airflow, Spark, PySpark, Python, langchain, AWS Functions, Redshift, and SSIS.\nStrong understanding of data warehousing concepts, ETL processes, and data modeling.\nStrong understanding of S3 and code-based scripting to move large volumes of data across application storage layers\nFamiliarity with Generative AI concepts and technologies, such as GPT-4, Transformers, and other natural language processing techniques.\nExcellent problem-solving, analytical, and critical thinking skills.\nStrong communication and collaboration skills, with the ability to work effectively with cross-functional teams.\nPreferred Qualifications/ skills\nKnowledge of cloud computing platforms, such as AWS, Azure, or Google Cloud Platform, is a plus.\nExperience with big data technologies, such as Hadoop, Hive, or Presto, is a plus.\nFamiliarity with machine learning frameworks, such as TensorFlow or PyTorch, is a plus.\nA continuous learning mindset and a passion for staying up-to-date with the latest advancements in data engineering and Generative AI.",Software,"['Airflow', 'Db', 'Spark', 'Pyspark', 'Python', 'AWS']",2025-06-14 06:35:38
Data Engineer AWS/ Power platform Engineer,Systechcorp Private Limited,3-7 Years,,"Hyderabad, Chennai, Pune","Design and develop data pipelines for Generative AI projects by leveraging a combination of technologies, including Vector DB, Graph DB, Airflow, Spark, PySpark, Python, LangChain, AWS Functions, Redshift, and SSIS. This will involve the logical and efficient integration of these tools to create seamless, high-performance data flows that efficiently support the data requirements of our cutting-edge AI initiatives. Collaborate with data scientists, AI researchers, and other stakeholders to understand data requirements and translate them into effective data engineering solutions.\nUser will be managing movement, organization and quality assessments of large set of data to facilitate the creation of Knowledge base for RAG systems and model training\nDemonstrate familiarity with data integration services such as AWS Glue and Azure Data Factory, showcasing the ability to effectively utilize these platforms for seamless data ingestion, transformation, and orchestration across various sources and destinations.\nPossess proficiency in constructing data warehouses and data lakes, demonstrating a strong foundation in organizing and consolidating large volumes of structured and unstructured data for efficient storage, retrieval, and analysis.\nOptimize and maintain data pipelines to ensure high-performance, reliable, and scalable data processing.\nDevelop and implement data validation and quality assurance procedures to ensure the accuracy and consistency of the data used in Generative AI projects.\nMonitor and troubleshoot data pipeline performance, identify bottlenecks, and implement improvements as necessary.\nStay current with emerging trends and technologies in the fields of data engineering, Generative AI, and related areas to ensure the continued success of our projects.\nCollaborate with team members on documentation, knowledge sharing, and best practices for data engineering within a Generative AI context.\nEnsure data privacy and security compliance in accordance with industry standards and regulations.\nQualifications we seek in you:\nBachelors or Masters degree in Computer Science, Engineering, or a related field.\nStrong experience with data engineering technologies, including Vector DB, Graph DB, Airflow, Spark, PySpark, Python, langchain, AWS Functions, Redshift, and SSIS.\nStrong understanding of data warehousing concepts, ETL processes, and data modeling.\nStrong understanding of S3 and code-based scripting to move large volumes of data across application storage layers\nFamiliarity with Generative AI concepts and technologies, such as GPT-4, Transformers, and other natural language processing techniques.\nExcellent problem-solving, analytical, and critical thinking skills.\nStrong communication and collaboration skills, with the ability to work effectively with cross-functional teams.\nPreferred Qualifications/ skills\nKnowledge of cloud computing platforms, such as AWS, Azure, or Google Cloud Platform, is a plus.\nExperience with big data technologies, such as Hadoop, Hive, or Presto, is a plus.\nFamiliarity with machine learning frameworks, such as TensorFlow or PyTorch, is a plus.\nA continuous learning mindset and a passion for staying up-to-date with the latest advancements in data engineering and Generative AI.",Software,"['Airflow', 'Db', 'Spark', 'Pyspark', 'Python', 'AWS']",2025-06-14 06:35:39
Azure Data Engineer (Power Platform)- Priniciple,Systechcorp Private Limited,8-13 Years,,"Delhi, Kolkata, Mumbai","Key Responsibilities:\nDesign, develop, and maintain data pipelines using Python, SQL, and Kedro\nImplement serverless solutions using AWS Lambda and Step Functions\nDevelop and manage data workflows in Azure and AWS cloud environments\nCreate integrations between data systems and Power Platform (Power Apps, Power Automate)\nDesign, develop, and maintain APIs for data exchange and integration\nImplement solutions to extract data from APIs and store in databases (Dataverse & PostgreSQL).\nOptimize data storage and retrieval processes for improved performance\nCollaborate with cross-functional teams to understand data requirements and provide solutions\nAPI Integration and data extraction from Sharepoint\nRequired Skills and Experience:\nExpert in Azure Data Engineer role, able to build solutions in Power Platform environment, AWS cloud and Azure cloud services.\nGood Knowledge on integration like Integration using API, XML with ADF, Logic App,\nAzure Functions, AWS Step functions, AWS Lambda Functions, etc\nStrong proficiency in Python, Pyspark (especially Kedro Framework) and SQL.\nHaving knowledge and experience in Scala, Java and R would be a good to have skill.\nExperience with Kedro framework for data engineering pipelines\nExpertise in AWS services, particularly Lambda and Step Functions\nProven experience with Power Platform (Power Apps, Power Automate) and Dataverse.\nStrong understanding of API development and integration\nExperience in database design and management\nExcellent problem-solving and communication skills",Software,"['XML with ADF', 'Logic App', 'Azure Data Engineer', 'Api', 'Python', 'Sql']",2025-06-14 06:35:40
Azure Data Engineer,Coforge Solutions Private Limited,5-8 Years,INR 10 - 20 LPA,"Hyderabad, Bengaluru, Pune","We are Hiring For Azure Data Engineer at Coforge. We are seeking an experienced Data Engineer with expertise in Azure technologies to join our team.\nNote:- We are looking for an immediate joiner. This is an urgent project, So we are looking for someone who can join immediately.\nShare Your CV - [HIDDEN TEXT]\nWhatsApp Me For Any Queries: 9667427662 ( Gaurav Sinha )\nThe ideal candidate will have a strong background in designing, developing, and implementing large-scale data engineering solutions using Azure cloud services.\nKey Skills:\n- Azure Data Lake Storage\n- Azure Data Factory\n- Azure Functions\n- Event Hub\n- Azure Stream Analytics\n- Azure Databricks\n- Python\n- PySpark\n- SQL\nExperience Required:- 5 to 8 Years.\nJob Location:-Bangalore, Pune& Hyderabad\nJob Type: - Full-time\nJob Description:-\nWe are seeking an experienced Data Engineer with expertise in Azure technologies to join our team. The ideal candidate will have a strong background in designing, developing, and implementing large-scale data engineering solutions using Azure cloud services.\nKey Skills:\n- Azure Data Lake Storage\n- Azure Data Factory\n- Azure Functions\n- Event Hub\n- Azure Stream Analytics\n- Azure Databricks\n- Python\n- PySpark\n- SQL\nResponsibilities:\n- Design and develop data engineering solutions using Azure cloud services\n- Work with stakeholders to gather requirements and provide solutions\n- Develop and implement data pipelines using Azure Data Factory and Azure Databricks\n- Collaborate with team members to deliver projects\nRequirements:\n- 5-8 years of experience in data engineering and cloud development\n- Strong expertise in Azure technologies (Data Factory, Azure Functions, ADX, Event Hub, ASA)\n- Experience with data analytics using SQL\n- Experience with agile, DevOps, and product model\n- Bachelor's degree in Computer Science or related technical discipline",Software,['Azure'],2025-06-14 06:35:42
Data Engineer,Vivotex India Private Limited,5-8 Years,,"Hyderabad, Bengaluru, Chennai","Hi\nwe have opening for Data Engineer position with one of our client,\nExperience Level _ 5+ Years\nLocations - Hyderabad, Bangalore, Chennai, Pune, Noida and Gurgaon\nBachelor's or Master's degree in Computer Science, Engineering, Information Systems, or a related field.\n5+ years of experience as a Data Engineer or in a similar role.\nStrong hands-on experience with Snowflake, including performance tuning, role-based access control, and data sharing.\nProficiency in SQL, with the ability to write optimized and maintainable queries.\nDeep understanding of data warehousing concepts, dimensional modeling, and ETL/ELT processes",Login to check your skill match score,"['Snow Flake', 'Data Engineer', 'Sql', 'Data Warehousing']",2025-06-14 06:35:43
Data Engineer  Databricks & AWS,Acuity IT Solutions Private Limited,3-6 Years,,Bengaluru,"Data Engineer Databricks & AWS\nLocation: Bangalore\nExperience: 35+ years\nData Engineers- Databricks and AWS/Azure- Bangalore\nWe are seeking an experienced Data Engineer to design, build, and optimize data pipelines and solutions on the Databricks Lakehouse Platform. The ideal candidate will have hands-on experience with PySpark, Databricks Auto Loader, Delta Lake, Spark SQL, and Databricks Workflows, with a strong understanding of data modelling, performance tuning, and cloud-native data engineering practices.\nKey Responsibilities:\nDesign and implement scalable data ingestion and transformation pipelines using Databricks Auto Loader and Delta Lake.\nDevelop and optimize ETL/ELT workflows in PySpark and Spark SQL, ensuring performance and reliability.\nBuild modular, reusable Delta Live Tables (DLT) or production-grade Databricks workflows to manage batch and streaming data.\nArchitect and implement data lakehouse patterns on Databricks, using best practices for data versioning, quality, and lineage.\nDevelop and manage unit tests, data quality checks, and monitoring for critical pipelines.\nCollaborate with data modelers, analysts, and business stakeholders to deliver reliable, governed datasets.\nIntegrate with cloud storage solutions (e.g., ADLS, Amazon S3), and manage permissions and metadata with Unity Catalog (if applicable).\nSupport and maintain existing pipelines, troubleshoot performance issues, and provide optimization strategies.\nUse CI/CD tools (e.g., Azure DevOps, GitHub Actions) to manage deployment and version control of data pipelines.\nRequired Skills:\n35+ years of experience in data engineering with a strong focus on Apache Spark / PySpark.\nStrong experience with Databricks platform, including Auto Loader, Delta Tables, Workflows, and Spark SQL.\nExperience with Databricks Jobs or Delta Live Tables (DLT) for orchestration and pipeline automation.\nSolid understanding of data lakehouse architecture, including bronze-silver-gold data layers.\nHands-on experience with cloud platforms such as Azure or AWS, and cloud storage integrations (e.g., ADLS Gen2, S3).\nProficient in SQL and Python for data transformations and logic implementations.\nKnowledge of data modeling techniques such as Star Schema, Snowflake Schema, and Data Vault.\nFamiliarity with CI/CD tools, version control systems (Git), and DevOps best practices.\nExperience with data governance, security, and access control",Login to check your skill match score,"['Data Engineer', 'Databricks', 'Pyspark', 'Aws', 'Sql']",2025-06-14 06:35:44
Lead Data Engineer,Teizo Soft Private Limited,8-12 Years,,Hyderabad,"Designation: Lead Data Engineer\nExperience: 8+ Years\nContract: 6+ Months\nNotice period: Immediate to 15 Days\nInterested can share their profiles to [HIDDEN TEXT]\nMandatory Skills: Python, SQL, Snowflake (3+ years on each skill)\nRequired Skills:\nSenior developer with approximately 8 years of experience in Data engineering\nBackground in medium to large-scale client environments working on at least 3 or 4 projects\nStrong expertise in Data Engineering, ETL/ELT workflows\nSolid understanding of database concepts and data modeling\nProficient in SQL, PL/SQL, and Python\nSnowflake experience (3+ years) with base or advanced certification\nExcellent communication skills ( written and verbal)\nAbility to work independently and proactively\nMust be based in Hyderabad, working from Hyderabad office",Login to check your skill match score,"['snowflake', 'Python', 'Sql']",2025-06-14 06:35:45
Data Engineer -Azure Cloud Data and Analytics Platform.,Michael Page,8-10 Years,,"Gurugram, India","Product Based Leading MNC\nGrowth Opportunities\nJob Description\nAnalyse business requirements and supportcreate design for requirements\nBuild and deploy new/changes to data mappings, sessions, and workflows in Azure Cloud Platform - key focus area would be Azure Data Factory.\nDevelop performant code\nPerform ETL routines performance tuning, troubleshooting, support, and capacity estimation.\nConduct thorough testing of ETL code changes to ensure quality deliverables\nProvide day-to-day support and mentoring to end users who are interacting with the data\nProfile and understand large amounts of source data available, including structured and semi-structured/web activity data\nAnalyse defects and provide fixes\nProvide release notes for deployments\nSupport Release activities\nProblem solving attitude\nKeep up to date with new skills - Develop technology skills in other areas of Platform\nThe Successful Applicant\nSkills and Experience:\n8+ years of experienced in ETL tools, data projects\nRecent Azure experience - Strong knowledge of Azure Data Factory and Databricks\nPython knowledge would be highly desirable\nStrong knowledge of SQL\nStrong Analytical skills\nAzure DevOps knowledge\nExperience Logic Apps would be desirable\nExperience with Azure Functions would be a plus.\nDay to day decisions around coding and unit testing.\nWhat's on Offer\nCompetitve Compensation\n\nGrowth\n\nFlexible Working",Login to check your skill match score,"['Logic Apps', 'Azure Data Factory', 'Azure Functions', 'Etl Tools', 'Databricks', 'Python', 'Sql', 'Azure DevOps']",2025-06-14 06:35:47
Senior Data Engineer,TaskUs,5-10 Years,,"Chennai, India","About TaskUs:TaskUs is a provider of outsourced digital services and next-generation customer experience to fast-growing technology companies, helping its clients represent, protect and grow their brands. Leveraging a cloud-based infrastructure, TaskUs serves clients in the fastest-growing sectors, including social media, e-commerce, gaming, streaming media, food delivery, ride-sharing, HiTech, FinTech, and HealthTech.\nThe People First culture at TaskUs has enabled the company to expand its workforce to approximately 45,000 employees globally.Presently, we have a presence in twenty-three locations across twelve countries, which include the Philippines, India, and the United States.\nIt started with one ridiculously good idea to create a different breed of Business Processing Outsourcing (BPO)! We at TaskUs understand that achieving growth for our partners requires a culture of constant motion, exploring new technologies, being ready to handle any challenge at a moment's notice, and mastering consistency in an ever-changing world.\nWhat We Offer:At TaskUs, we prioritize our employees well-being by offering competitive industry salaries and comprehensive benefits packages. Our commitment to a People First culture is reflected in the various departments we have established, including Total Rewards, Wellness, HR, and Diversity. We take pride in our inclusive environment and positive impact on the community. Moreover, we actively encourage internal mobility and professional growth at all stages of an employee's career within TaskUs. Join our team today and experience firsthand our dedication to supporting People First.\nWhat you'll do\nDevelop, maintain, and enhance new data sources and tables, contributing to data engineering efforts to ensure comprehensive and efficient data architecture.\nServes as the liaison between Data Engineer team and the Airport operation teams, developing new data sources and overseeing enhancements to existing database being one of the main contact points for data requests, metadata, and statistical analysis\nMigrates all existing Hive Metastore tables to Unity Catalog, addressing access issues and ensuring smooth transition of jobs and tables.\nCollaborate with IT teams to validate package (gold level data) table outputs during the production deployment of developed notebooks\nDevelop and implement data quality alerting systems and Tableau alerting mechanisms for dashboards, setting up notifications for various thresholds.\nCreate and maintain standard reports and dashboards to provide insights into airport performance, helping guide stations to optimize operations and improve performance.\nAll you'll need for success\nPreferred Qualifications- Education & Prior Job Experience\nMaster's degree / UG\nMin 5 -10 years of experience\nDatabricks (Azur op)\nGood Communication\nExperience developing solutions on a Big Data platform utilizing tools such as Impala and Spark\nAdvanced knowledge/experience with Azure Databricks, PySpark, (Teradata)/Databricks SQL\nAdvanced knowledge/experience in Python along with associated development environments (e.g. JupyterHub, PyCharm, etc.)\nAdvanced knowledge/experience in building Tableau Dashboard / Clikview / PowerBi\nBasic idea on HTML and JavaScript\nImmediate Joiner\nSkills, Licenses & Certifications\nStrong project management skills\nProficient with MicrosoftOffice applications (MS Excel, Access and PowerPoint) advanced knowledge of Microsoft Excel\nAdvanced aptitude in problem-solving, including the ability to logically structure an appropriate analytical framework\nProficient in SharePoint, PowerApp and ability to use Graph API\nHow We Partner To Protect You: TaskUs will neither solicit money from you during your application process nor require any form of payment in order to proceed with your application. Kindly ensure that you are always in communication with only authorized recruiters of TaskUs.\n\nDEI:In TaskUs we believe that innovation and higher performance are brought by people from all walks of life. We welcome applicants of different backgrounds, demographics, and circumstances. Inclusive and equitable practices are our responsibility as a business. TaskUs is committed to providing equal access to\nWe invite you to explore all TaskUs career opportunities and apply through the provided URL.",IT/Computers - Software,"['Databricks SQL', 'Graph API', 'Teradata', 'Sharepoint', 'PowerApp', 'Pyspark', 'Tableau', 'Azure Databricks', 'Impala', 'HTML', 'Javascript', 'Powerbi', 'Spark', 'Databricks', 'Azure', 'Microsoft Excel', 'Python']",2025-06-14 06:35:58
Data Engineer -Azure Cloud Data and Analytics Platform.,Michael Page,8-10 Years,,"Gurugram, India","Product Based Leading MNC\nGrowth Opportunities\nJob Description\nAnalyse business requirements and supportcreate design for requirements\nBuild and deploy new/changes to data mappings, sessions, and workflows in Azure Cloud Platform - key focus area would be Azure Data Factory.\nDevelop performant code\nPerform ETL routines performance tuning, troubleshooting, support, and capacity estimation.\nConduct thorough testing of ETL code changes to ensure quality deliverables\nProvide day-to-day support and mentoring to end users who are interacting with the data\nProfile and understand large amounts of source data available, including structured and semi-structured/web activity data\nAnalyse defects and provide fixes\nProvide release notes for deployments\nSupport Release activities\nProblem solving attitude\nKeep up to date with new skills - Develop technology skills in other areas of Platform\nThe Successful Applicant\nSkills and Experience:\n8+ years of experienced in ETL tools, data projects\nRecent Azure experience - Strong knowledge of Azure Data Factory and Databricks\nPython knowledge would be highly desirable\nStrong knowledge of SQL\nStrong Analytical skills\nAzure DevOps knowledge\nExperience Logic Apps would be desirable\nExperience with Azure Functions would be a plus.\nDay to day decisions around coding and unit testing.\nWhat's on Offer\nCompetitve Compensation\n\nGrowth\n\nFlexible Working",Login to check your skill match score,"['Logic Apps', 'Azure Data Factory', 'Azure Functions', 'Etl Tools', 'Databricks', 'Python', 'Sql', 'Azure DevOps']",2025-06-14 06:36:03
Data Engineer -Azure Cloud Data and Analytics Platform.,Michael Page,8-10 Years,,"Gurugram, India","Product Based Leading MNC\nGrowth Opportunities\nJob Description\nAnalyse business requirements and supportcreate design for requirements\nBuild and deploy new/changes to data mappings, sessions, and workflows in Azure Cloud Platform - key focus area would be Azure Data Factory.\nDevelop performant code\nPerform ETL routines performance tuning, troubleshooting, support, and capacity estimation.\nConduct thorough testing of ETL code changes to ensure quality deliverables\nProvide day-to-day support and mentoring to end users who are interacting with the data\nProfile and understand large amounts of source data available, including structured and semi-structured/web activity data\nAnalyse defects and provide fixes\nProvide release notes for deployments\nSupport Release activities\nProblem solving attitude\nKeep up to date with new skills - Develop technology skills in other areas of Platform\nThe Successful Applicant\nSkills and Experience:\n8+ years of experienced in ETL tools, data projects\nRecent Azure experience - Strong knowledge of Azure Data Factory and Databricks\nPython knowledge would be highly desirable\nStrong knowledge of SQL\nStrong Analytical skills\nAzure DevOps knowledge\nExperience Logic Apps would be desirable\nExperience with Azure Functions would be a plus.\nDay to day decisions around coding and unit testing.\nWhat's on Offer\nCompetitve Compensation\n\nGrowth\n\nFlexible Working",Login to check your skill match score,"['Logic Apps', 'Azure Data Factory', 'Azure Functions', 'Etl Tools', 'Databricks', 'Python', 'Sql', 'Azure DevOps']",2025-06-14 06:36:05
Lead Consultant - Data Ops Engineer,Astrazeneca,4-7 Years,,Chennai,"Accountabilities\nStrategic Leadership: Evaluate current platforms and lead the design of future-ready solutions, embedding AI-driven efficiencies and proactive interventions.\nInnovation & Integration: Introduce and integrate AI technologies to enhance ways of working, driving cost-effectiveness and operational excellence.\nPlatform Maturity & Management: Ensure platforms are scalable and compliant, with robust automation and optimized technology stacks.\nLead Deliveries: Oversee and manage the delivery of projects, ensuring timely execution and alignment with strategic goals.\nThought Leadership: Champion data mesh and product-oriented work methodologies to continuously evolve our data landscapes.\nQuality and Compliance: Implement quality assurance processes, emphasizing data accuracy and security.\nCollaborative Leadership: Foster an environment that supports cross-functional collaboration and continuous improvement.\nEssential Skills/Experience\nExtensive experience with Snowflake, AI platforms, and cloud infrastructure.\nProven track record in thought leadership, platform strategy, and cross-disciplinary innovation.\nExpertise in AI/GenAI integration with a focus on practical business applications.\nStrong experience in DataOps, DevOps, and cloud environments such as AWS.\nExcellent stakeholder management and the ability to lead diverse teams toward innovative solutions.\nBackground in the pharmaceutical sector is a plus.",Pharmaceutical,"['snowflake', 'DataOps', 'Cloud Infrastructure', 'Devops', 'Aws']",2025-06-14 06:36:07
Engineering - Market Data Engineering - System Engineer - Analyst,Goldman Sachs,2-4 Years,,Bengaluru,"YOUR IMPACT\nHOW YOU WILL FULFILL YOUR POTENTIAL\nPerform pre-market checkout out critical applications and infrastructure.\nSupport both application functionality and data content; escalate data content issues to vendors and/or internal developers.\nAutomation, run-books and FAQs, for helpdesk.\nManage incident management process for Market Data issues.\nManage engagement with end users/vendors and helpdesk until problem is resolved.\nActing as role model and technical mentor for less experienced staff.\nSKILLS AND EXPERIENCE WE ARE LOOKING FOR\nCandidate should have relevant experience in the range of 2-4 years.\nDeep understanding of application configuration and troubleshooting, experience of supporting Windows environment essential.\nExperience of remote support (international), working with vendors for problem resolution.\nExperience with scripting languages such as Perl, Python, Powershell.\nAvailability: Be available to respond to emergency situations and perform regularly scheduled maintenance\nStrong inter-personal and communication skills; making presentations to customer or client audiences or professional peers, and working independently on problem solving.\nPreferred Qualifications\nApplication product support specialist for Refinitiv, Bloomberg, Datastream, Factset, etc\nDeep understanding of application configuration and troubleshooting, experience of supporting market data or trading desktop applications.\nUnderstanding of Linux environment.\nExperience with vendor data distribution systems like TREP, Elektron, Activ,, etc.\nFinancial Industry experience - Understanding of capital markets, experience with multiple asset classes.\nMarket Data Industry experience.",Financial Services,"['TREP', 'Elektron', 'Linux', 'Perl', 'Python']",2025-06-14 06:36:08
Lead Consultant - Data Governance Platform Engineer,Astrazeneca,5-8 Years,,Chennai,"Platform Management: Support and oversee the administration and maintenance of data platforms, ensuring they are reliable, scalable, and meet organizational performance requirements.\nSystem Integration: Manage the integration of data platforms with other enterprise systems, ensuring seamless data flow and interoperability.\nData Management: Oversee the configuration and management of data catalogs, ensuring that data assets are organized, accessible, and maintained according to best practices.\nData Quality & Observability: Implement and lead tools and processes to monitor and enhance data quality and observability, ensuring data integrity and availability.\nTechnical Expertise: Provide technical leadership and oversight for platform-related projects, guiding the selection and implementation of appropriate technologies.\nInnovation and Trends: Stay informed about the latest trends in data platform technologies and incorporate relevant advancements to enhance capabilities and efficiency.\nCollaboration and Alignment: Work closely with the Data Management and Governance Specialist to align platform capabilities with data governance policies and business objectives.\nVendor Management: Coordinate with vendors and service providers to ensure that platform needs are adequately met and that service level agreements are maintained.\nVision and Roadmap: Develop and execute a strategic vision and roadmap for the evolution of data platforms, incorporating feedback from stakeholders and aligning with company goals.\nProject Delivery: Lead the management of platform-related projects from inception to completion, ensuring objectives are met on time and within budget.\nEssential Skills/Experience\nProven experience in managing data platforms and related technologies.\nIdeally Data catalogue and Data Quality Platforms\nStrong understanding of data platform architecture and integration.\nDemonstrated experience with vendor management and negotiations.\nExcellent problem-solving and analytical skills.\nAbility to stay updated with technological advancements in data platforms.\nStrong organizational and leadership skills.",Pharmaceutical,"['Data Platform Management', 'Data Catalog Management', 'Data Quality Monitoring', 'vendor coordination', 'Strategic Planning.', 'System Integration', 'Technical Leadership']",2025-06-14 06:36:09
Senior Data Analytics & AI Engineer,Astrazeneca,8-12 Years,,Chennai,"Accountabilities:\nLead the design, development, and deployment of high-performance, scalable data warehouses and data pipelines.\nCollaborate closely with multi-functional teams to understand business requirements and translate them into technical solutions.\nOversee and optimize the use of Snowflake for data storage and analytics.\nDevelop and maintain SQL-based ETL processes.\nImplement data workflows and orchestrations using Airflow.\nApply DBT for data transformation and modeling tasks.\nMentor and guide junior data engineers, fostering a culture of learning and innovation within the team.\nConduct performance tuning and optimization for both ongoing and new data projects.\nConfirmed ability to handle large, complex data sets and develop data-centric solutions.\nStrong problem-solving skills and a keen analytical mentality.\nExcellent communication and leadership skills, with the ability to work effectively in a team-oriented environment.\n8-12 years of experience in data engineering roles, focusing on data warehousing, data integration, and data product development.\nEssential Skills/Experience:\nSnowflake\nSQL\nAirflow\nDBT\nDesirable Skills/Experience:\nSnaplogic\nPython\nAcademic Qualifications:\nBachelor s or Master s degree in Computer Science, Information Technology, or related field.",Pharmaceutical,"['snowflake', 'snap logic', 'Product Development', 'Database Technologies', 'Sql', 'Python', 'Data Warehousing']",2025-06-14 06:36:10
