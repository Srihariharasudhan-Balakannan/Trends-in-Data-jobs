job_title,company_name,experience,salary,location,industry,job_description,skills,scraped_at
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:35:54
Data Engineer,Virtusa,5-10 Years,,Chennai,Information Technology,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.","['AWS data services', 'dbt', 'snowflake', 'AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql', 'Python', 'Pyspark', 'Sql']",2025-06-12 01:35:56
Data Engineer,Virtusa,5-10 Years,,Chennai,Information Technology,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.","['AWS data services', 'dbt', 'snowflake', 'AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql', 'Python', 'Pyspark', 'Sql']",2025-06-12 01:35:57
Data Engineer,Virtusa,2-4 Years,,Hyderabad,Information Technology,"Key Responsibilities:\nDesign, develop, and maintain ETL/ELT pipelines to move and transform data from various sources.\nBuild and optimize data warehouses, data lakes, and structured/unstructured storage solutions.\nEnsure data quality, integrity, and consistency across systems.\nCollaborate with data analysts and scientists to understand data needs and optimize performance.\nIntegrate APIs, third-party data sources, and internal systems to build robust datasets.\nMonitor and troubleshoot pipeline performance and resolve data-related issues.\nImplement data security, governance, and compliance best practices.\nRequired Skills & Qualifications:\nBachelor's degree in Computer Science, Engineering, Information Systems, or a related field.\n25 years of experience as a Data Engineer or in a related role.\nStrong programming skills in Python, SQL, and optionally Scala or Java.\nExperience with big data tools such as Spark, Hadoop, or Kafka.\nHands-on experience with cloud platforms (e.g., AWS, GCP, or Azure) and services like S3, Redshift, BigQuery, or Snowflake.\nProficiency with data modeling, schema design, and working with relational and NoSQL databases.\nFamiliarity with workflow orchestration tools like Airflow, Luigi, or Prefect.\nStrong understanding of data warehousing concepts and performance optimization.","['AWS', 'Gcp', 'Azure', 'Redshift', 'BigQuery']",2025-06-12 01:35:59
Data Engineer,Virtusa,2-4 Years,,Hyderabad,Information Technology,"We are seeking a skilled Data Engineer to join our dynamic team in India. The ideal candidate will have experience in building and managing data pipelines, ensuring data quality, and optimizing our data infrastructure to support analytics and business intelligence initiatives.\nResponsibilities\nDesign, construct, install, and maintain large-scale processing systems and data pipelines.\nDevelop and optimize ETL processes to ensure smooth data integration and transformation.\nCollaborate with data scientists and analysts to understand data requirements and deliver solutions that meet their needs.\nImplement data quality checks and ensure data integrity throughout the data lifecycle.\nMonitor performance and troubleshoot data-related issues, enhancing system efficiency and reliability.\nSkills and Qualifications\nBachelor's degree in Computer Science, Engineering, or a related field.\n2-4 years of experience in data engineering or a similar role.\nProficiency in programming languages such as Python, Java, or Scala.\nStrong knowledge of SQL and experience with database management systems (e.g., MySQL, PostgreSQL, MongoDB).\nExperience with big data technologies such as Hadoop, Spark, or Kafka.\nFamiliarity with cloud platforms like AWS, Azure, or Google Cloud for data storage and processing.\nUnderstanding of data modeling concepts and data warehousing principles.\nAbility to work collaboratively in a team environment and communicate effectively with stakeholders.","['Sql', 'Apache Spark', 'Big Data', 'AWS', 'Data Modeling', 'Kafka']",2025-06-12 01:36:01
Data Engineer,Virtusa,6-12 Years,,Hyderabad,Information Technology,"Job Title: Senior Data Engineer\nLocation\nHyderabad, Telangana, India\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.","['Sql', 'Apache Spark', 'Big Data', 'AWS', 'Data Modeling', 'Kafka']",2025-06-12 01:36:02
Data Engineer,Virtusa,6-8 Years,,Bengaluru,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:36:03
Data Engineer,Virtusa,2-4 Years,,Hyderabad,Information Technology,"We are seeking a skilled Data Engineer to join our dynamic team in India. The ideal candidate will have experience in building and managing data pipelines, ensuring data quality, and optimizing our data infrastructure to support analytics and business intelligence initiatives.\nResponsibilities\nDesign, construct, install, and maintain large-scale processing systems and data pipelines.\nDevelop and optimize ETL processes to ensure smooth data integration and transformation.\nCollaborate with data scientists and analysts to understand data requirements and deliver solutions that meet their needs.\nImplement data quality checks and ensure data integrity throughout the data lifecycle.\nMonitor performance and troubleshoot data-related issues, enhancing system efficiency and reliability.\nSkills and Qualifications\nBachelor's degree in Computer Science, Engineering, or a related field.\n2-4 years of experience in data engineering or a similar role.\nProficiency in programming languages such as Python, Java, or Scala.\nStrong knowledge of SQL and experience with database management systems (e.g., MySQL, PostgreSQL, MongoDB).\nExperience with big data technologies such as Hadoop, Spark, or Kafka.\nFamiliarity with cloud platforms like AWS, Azure, or Google Cloud for data storage and processing.\nUnderstanding of data modeling concepts and data warehousing principles.\nAbility to work collaboratively in a team environment and communicate effectively with stakeholders.","['Sql', 'Apache Spark', 'Big Data', 'AWS', 'Data Modeling', 'Kafka']",2025-06-12 01:36:05
Data Engineer,Virtusa,6-10 Years,,Bengaluru,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark', 'Aws']",2025-06-12 01:36:07
Data Engineer,Virtusa,6-10 Years,,Bengaluru,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark', 'Aws']",2025-06-12 01:36:08
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:36:09
Data Engineer,Virtusa,8-12 Years,,Hyderabad,Information Technology,"Experience:\n8+ years of experience in data engineering, specifically in cloud environments like AWS.\nProficiency in PySpark for distributed data processing and transformation.\nSolid experience with AWS Glue for ETL jobs and managing data workflows.\nHands-on experience with AWS Data Pipeline (DPL) for workflow orchestration.\nStrong experience with AWS services such as S3, Lambda, Redshift, RDS, and EC2.\nTechnical Skills:\nProficiency in Python and PySpark for data processing and transformation tasks.\nDeep understanding of ETL concepts and best practices.\nFamiliarity with AWS Glue (ETL jobs, Data Catalog, and Crawlers).\nExperience building and maintaining data pipelines with AWS Data Pipeline or similar orchestration tools.\nFamiliarity with AWS S3 for data storage and management, including file formats (CSV, Parquet, Avro).\nStrong knowledge of SQL for querying and manipulating relational and semi-structured data.\nExperience with Data Warehousing and Big Data technologies, specifically within AWS.\nAdditional Skills:\nExperience with AWS Lambda for serverless data processing and orchestration.\nUnderstanding of AWS Redshift for data warehousing and analytics.\nFamiliarity with Data Lakes, Amazon EMR, and Kinesis for streaming data processing.\nKnowledge of data governance practices, including data lineage and auditing.\nFamiliarity with CI/CD pipelines and Git for version control.\nExperience with Docker and containerization for building and deploying applications.\nDesign and Build Data Pipelines: Design, implement, and optimize data pipelines on AWS using PySpark, AWS Glue, and AWS Data Pipeline to automate data integration, transformation, and storage processes.\nETL Development: Develop and maintain Extract, Transform, and Load (ETL) processes using AWS Glue and PySpark to efficiently process large datasets.\nData Workflow Automation: Build and manage automated data workflows using AWS Data Pipeline, ensuring seamless scheduling, monitoring, and management of data jobs.\nData Integration: Work with different AWS data storage services (e.g., S3, Redshift, RDS) to ensure smooth integration and movement of data across platforms.\nOptimization and Scaling: Optimize and scale data pipelines for high performance and cost efficiency, utilizing AWS services like Lambda, S3, and EC2.","['Snowflake design', 'ETL/ELT', 'Python', 'Sql', 'Data Warehousing', 'cloud platform']",2025-06-12 01:36:10
Data Engineer,Synechron Technologies Private Limited,6-8 Years,,Gurugram,Software Engineering,"Job Profile : Data and analytics expert with 6-8 years of experience, proficient in big data technologies, data architecture, and customer-focused solutions.\nTechnology\nIn addition, you'll bring a selection of the following technical skills with an experience around 6-8 years.\nResponsibilities:\nDesign and implement data pipelines, data lakes, and visualization solutions to create business value.\nDevelop and maintain big data solutions using technologies like AWS Redshift, Kubernetes, Spark, Python, SQL, DBT, and Dataiku.\nApply best practices in data architecture, modeling, and metadata management.\nAnalyze and troubleshoot complex data issues, ensuring timely resolution..\nPrimary Skills:\n6-8 years of experience in data and analytics.\nProficiency in big data technologies: AWS Redshift, Kubernetes, Spark, Python, complex SQL, DBT, and Dataiku.\nStrong understanding of data architecture, modeling, and metadata management.\nExcellent data analytical and troubleshooting skills.\nStrong communication skills and a customer-focused approach\nGood to have:\nExperience with other data-related tools and platforms.\nKnowledge of emerging trends in data and analytics\nExperience: 6 to 8 years","['Aws Redshift', 'Spark', 'Kubernetes', 'Sql', 'Python']",2025-06-12 01:36:12
Data Engineer,Virtusa,6-12 Years,,Chennai,Information Technology,"Job Title: Senior Data Engineer\nLocation\nHyderabad, Telangana, India\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.","['Sql', 'Apache Spark', 'Big Data', 'AWS', 'Data Modeling', 'Kafka']",2025-06-12 01:36:13
Data Engineer,Virtusa,10-12 Years,,Bengaluru,Information Technology,"Requirements\n10+ years of strong experience with data transformation & ETL on large data sets\nExperience with designing customer centric datasets (i.e., CRM, Call Center, Marketing, Offline, Point of\nSale etc.)\n5+ years of Data Modeling experience (i.e., Relational, Dimensional, Columnar, Big Data)\n5+ years of complex SQL or NoSQL experience\nExperience in advanced Data Warehouse concepts\nExperience in industry ETL tools (i.e., Informatica, Unifi)\nExperience with Business Requirements definition and management, structured analysis, process\ndesign, use case documentation\nExperience with Reporting Technologies (i.e., Tableau, PowerBI)\nexperience in professional software development\nDemonstrate exceptional organizational skills and ability to multi-task simultaneous different customer\nprojects\nStrong verbal & written communication skills to interface with Sales team & lead customers to\nsuccessful outcome\nMust be self-managed, proactive and customer focused\nDegree in Computer Science, Information Systems, Data Science, or related field","['Python', 'Sql', 'Etl', 'BigQuery']",2025-06-12 01:36:15
Data Engineer Tech Lead,Virtusa,5-7 Years,,Hyderabad,Information Technology,"Design, develop, and maintain ETL processes using Microsoft SSIS.\nWrite efficient and scalable T-SQL scripts for data manipulation and querying.\nDevelop reports and dashboards using SSRS to support business intelligence needs.\nPerform data modeling and create optimized database structures to support business analytics.\nConduct root cause analysis on internal/external data to resolve data issues and improve data quality.\nCollaborate directly with business users to gather requirements and deliver reliable data solutions.\nRequirements\n5+ years of experience in a data engineering or related role.\nStrong experience with Microsoft SQL Server, T-SQL, SSIS, and SSRS.\nBasic familiarity with C# is preferred.\nAbility to manage data transformation, metadata, and workflow dependencies.\nExcellent analytical, logical reasoning, and problem-solving skills.\nStrong communication skills and ability to work independently with business users.","['T-sql', 'Microsoft Ssis', 'microsoft ssrs', 'Microsoft Sql Server', 'Data Modeling', 'Etl Development']",2025-06-12 01:36:21
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:40:09
Data Engineer,Virtusa,5-10 Years,,Chennai,Information Technology,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.","['AWS data services', 'dbt', 'snowflake', 'AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql', 'Python', 'Pyspark', 'Sql']",2025-06-12 01:40:13
Data Engineer,Virtusa,5-10 Years,,Chennai,Information Technology,"Job Title: Senior Data Engineer\n\nKey Responsibilities\nAs a Senior Data Engineer, you will:\nData Pipeline Development: Design, build, and maintain scalable data pipelines using PySpark and Python.\nAWS Cloud Integration: Work with AWS cloud services (S3, Lambda, Glue, EMR, Redshift) for data ingestion, processing, and storage.\nETL Workflow Management: Implement and maintain ETL workflows using DBT and orchestration tools (e.g., Airflow).\nData Warehousing: Design and manage data models in Snowflake, ensuring performance and reliability.\nSQL Optimization: Utilize SQL for querying and optimizing datasets across different databases.\nData Integration: Integrate and manage data from MongoDB, Kafka, and other streaming or NoSQL sources.\nCollaboration & Support: Collaborate with data scientists, analysts, and other engineers to support advanced analytics and Machine Learning (ML) initiatives.\nData Quality & Governance: Ensure data quality, lineage, and governance through best practices and tools.\nMandatory Skills & Experience\nStrong programming skills in Python and PySpark.\nHands-on experience with AWS data services (S3, Lambda, Glue, EMR, Redshift).\nProficiency in SQL and experience with DBT for data transformation.\nExperience with Snowflake for data warehousing.\nKnowledge of MongoDB, Kafka, and data streaming concepts.\nGood understanding of data architecture, data modeling, and data governance.\nFamiliarity with large-scale data platforms.\nEssential Professional Skills\nExcellent problem-solving skills.\nAbility to work independently or as part of a team.\nExperience with CI/CD and DevOps practices in a data engineering environment (Plus).\nQualifications\nProven hands-on experience working with large-scale data platforms.\nStrong background in Python, PySpark, AWS, and modern data warehousing tools such as Snowflake and DBT.\nFamiliarity with NoSQL databases like MongoDB and real-time streaming platforms like Kafka.","['AWS data services', 'dbt', 'snowflake', 'AWS data services', 'dbt', 'snowflake', 'Python', 'Pyspark', 'Sql', 'Python', 'Pyspark', 'Sql']",2025-06-12 01:40:14
Data Engineer,Virtusa,2-4 Years,,Chennai,Information Technology,"We are seeking a skilled Data Engineer to join our dynamic team in India. The ideal candidate will have experience in building and managing data pipelines, ensuring data quality, and optimizing our data infrastructure to support analytics and business intelligence initiatives.\nResponsibilities\nDesign, construct, install, and maintain large-scale processing systems and data pipelines.\nDevelop and optimize ETL processes to ensure smooth data integration and transformation.\nCollaborate with data scientists and analysts to understand data requirements and deliver solutions that meet their needs.\nImplement data quality checks and ensure data integrity throughout the data lifecycle.\nMonitor performance and troubleshoot data-related issues, enhancing system efficiency and reliability.\nSkills and Qualifications\nBachelor's degree in Computer Science, Engineering, or a related field.\n2-4 years of experience in data engineering or a similar role.\nProficiency in programming languages such as Python, Java, or Scala.\nStrong knowledge of SQL and experience with database management systems (e.g., MySQL, PostgreSQL, MongoDB).\nExperience with big data technologies such as Hadoop, Spark, or Kafka.\nFamiliarity with cloud platforms like AWS, Azure, or Google Cloud for data storage and processing.\nUnderstanding of data modeling concepts and data warehousing principles.\nAbility to work collaboratively in a team environment and communicate effectively with stakeholders.","['Sql', 'Apache Spark', 'Big Data', 'AWS', 'Data Modeling', 'Kafka']",2025-06-12 01:40:15
Data Engineer,Virtusa,2-4 Years,,Hyderabad,Information Technology,"Key Responsibilities:\nDesign, develop, and maintain ETL/ELT pipelines to move and transform data from various sources.\nBuild and optimize data warehouses, data lakes, and structured/unstructured storage solutions.\nEnsure data quality, integrity, and consistency across systems.\nCollaborate with data analysts and scientists to understand data needs and optimize performance.\nIntegrate APIs, third-party data sources, and internal systems to build robust datasets.\nMonitor and troubleshoot pipeline performance and resolve data-related issues.\nImplement data security, governance, and compliance best practices.\nRequired Skills & Qualifications:\nBachelor's degree in Computer Science, Engineering, Information Systems, or a related field.\n25 years of experience as a Data Engineer or in a related role.\nStrong programming skills in Python, SQL, and optionally Scala or Java.\nExperience with big data tools such as Spark, Hadoop, or Kafka.\nHands-on experience with cloud platforms (e.g., AWS, GCP, or Azure) and services like S3, Redshift, BigQuery, or Snowflake.\nProficiency with data modeling, schema design, and working with relational and NoSQL databases.\nFamiliarity with workflow orchestration tools like Airflow, Luigi, or Prefect.\nStrong understanding of data warehousing concepts and performance optimization.","['AWS', 'Gcp', 'Azure', 'Redshift', 'BigQuery']",2025-06-12 01:40:17
Data Engineer,Virtusa,2-4 Years,,Hyderabad,Information Technology,"We are seeking a skilled Data Engineer to join our dynamic team in India. The ideal candidate will have experience in building and managing data pipelines, ensuring data quality, and optimizing our data infrastructure to support analytics and business intelligence initiatives.\nResponsibilities\nDesign, construct, install, and maintain large-scale processing systems and data pipelines.\nDevelop and optimize ETL processes to ensure smooth data integration and transformation.\nCollaborate with data scientists and analysts to understand data requirements and deliver solutions that meet their needs.\nImplement data quality checks and ensure data integrity throughout the data lifecycle.\nMonitor performance and troubleshoot data-related issues, enhancing system efficiency and reliability.\nSkills and Qualifications\nBachelor's degree in Computer Science, Engineering, or a related field.\n2-4 years of experience in data engineering or a similar role.\nProficiency in programming languages such as Python, Java, or Scala.\nStrong knowledge of SQL and experience with database management systems (e.g., MySQL, PostgreSQL, MongoDB).\nExperience with big data technologies such as Hadoop, Spark, or Kafka.\nFamiliarity with cloud platforms like AWS, Azure, or Google Cloud for data storage and processing.\nUnderstanding of data modeling concepts and data warehousing principles.\nAbility to work collaboratively in a team environment and communicate effectively with stakeholders.","['Sql', 'Apache Spark', 'Big Data', 'AWS', 'Data Modeling', 'Kafka']",2025-06-12 01:40:18
Data Engineer,Virtusa,6-8 Years,,Bengaluru,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:40:20
Data Engineer,Virtusa,2-4 Years,,Hyderabad,Information Technology,"We are seeking a skilled Data Engineer to join our dynamic team in India. The ideal candidate will have experience in building and managing data pipelines, ensuring data quality, and optimizing our data infrastructure to support analytics and business intelligence initiatives.\nResponsibilities\nDesign, construct, install, and maintain large-scale processing systems and data pipelines.\nDevelop and optimize ETL processes to ensure smooth data integration and transformation.\nCollaborate with data scientists and analysts to understand data requirements and deliver solutions that meet their needs.\nImplement data quality checks and ensure data integrity throughout the data lifecycle.\nMonitor performance and troubleshoot data-related issues, enhancing system efficiency and reliability.\nSkills and Qualifications\nBachelor's degree in Computer Science, Engineering, or a related field.\n2-4 years of experience in data engineering or a similar role.\nProficiency in programming languages such as Python, Java, or Scala.\nStrong knowledge of SQL and experience with database management systems (e.g., MySQL, PostgreSQL, MongoDB).\nExperience with big data technologies such as Hadoop, Spark, or Kafka.\nFamiliarity with cloud platforms like AWS, Azure, or Google Cloud for data storage and processing.\nUnderstanding of data modeling concepts and data warehousing principles.\nAbility to work collaboratively in a team environment and communicate effectively with stakeholders.","['Sql', 'Apache Spark', 'Big Data', 'AWS', 'Data Modeling', 'Kafka']",2025-06-12 01:40:21
Data Engineer,Virtusa,8-9 Years,,Bengaluru,Information Technology,"Proficiency in PySpark for distributed data processing and transformation.\nSolid experience with AWS Glue for ETL jobs and managing data workflows.\nHands-on experience with AWS Data Pipeline (DPL) for workflow orchestration.\nStrong experience with AWS services such as S3, Lambda, Redshift, RDS, and EC2.\nTechnical Skills:\nProficiency in Python and PySpark for data processing and transformation tasks.\nDeep understanding of ETL concepts and best practices.\nFamiliarity with AWS Glue (ETL jobs, Data Catalog, and Crawlers).\nExperience building and maintaining data pipelines with AWS Data Pipeline or similar orchestration tools.\nFamiliarity with AWS S3 for data storage and management, including file formats (CSV, Parquet, Avro).\nStrong knowledge of SQL for querying and manipulating relational and semi-structured data.\nExperience with Data Warehousing and Big Data technologies, specifically within AWS.\nAdditional Skills:\nExperience with AWS Lambda for serverless data processing and orchestration.\nUnderstanding of AWS Redshift for data warehousing and analytics.\nFamiliarity with Data Lakes, Amazon EMR, and Kinesis for streaming data processing.\nKnowledge of data governance practices, including data lineage and auditing.\nFamiliarity with CI/CD pipelines and Git for version control.\nExperience with Docker and containerization for building and deploying applications.\nDesign and Build Data Pipelines: Design, implement, and optimize data pipelines on AWS using PySpark, AWS Glue, and AWS Data Pipeline to automate data integration, transformation, and storage processes.\nETL Development: Develop and maintain Extract, Transform, and Load (ETL) processes using AWS Glue and PySpark to efficiently process large datasets.\nData Workflow Automation: Build and manage automated data workflows using AWS Data Pipeline, ensuring seamless scheduling, monitoring, and management of data jobs.\nData Integration: Work with different AWS data storage services (e.g., S3, Redshift, RDS) to ensure smooth integration and movement of data across platforms.\nOptimization and Scaling: Optimize and scale data pipelines for high performance and cost efficiency, utilizing AWS services like Lambda, S3, and EC2.","['CI & CD', 'Cloudera Data platform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker']",2025-06-12 01:40:22
Data Engineer,Virtusa,8-9 Years,,Bengaluru,Information Technology,"Proficiency in PySpark for distributed data processing and transformation.\nSolid experience with AWS Glue for ETL jobs and managing data workflows.\nHands-on experience with AWS Data Pipeline (DPL) for workflow orchestration.\nStrong experience with AWS services such as S3, Lambda, Redshift, RDS, and EC2.\nTechnical Skills:\nProficiency in Python and PySpark for data processing and transformation tasks.\nDeep understanding of ETL concepts and best practices.\nFamiliarity with AWS Glue (ETL jobs, Data Catalog, and Crawlers).\nExperience building and maintaining data pipelines with AWS Data Pipeline or similar orchestration tools.\nFamiliarity with AWS S3 for data storage and management, including file formats (CSV, Parquet, Avro).\nStrong knowledge of SQL for querying and manipulating relational and semi-structured data.\nExperience with Data Warehousing and Big Data technologies, specifically within AWS.\nAdditional Skills:\nExperience with AWS Lambda for serverless data processing and orchestration.\nUnderstanding of AWS Redshift for data warehousing and analytics.\nFamiliarity with Data Lakes, Amazon EMR, and Kinesis for streaming data processing.\nKnowledge of data governance practices, including data lineage and auditing.\nFamiliarity with CI/CD pipelines and Git for version control.\nExperience with Docker and containerization for building and deploying applications.\nDesign and Build Data Pipelines: Design, implement, and optimize data pipelines on AWS using PySpark, AWS Glue, and AWS Data Pipeline to automate data integration, transformation, and storage processes.\nETL Development: Develop and maintain Extract, Transform, and Load (ETL) processes using AWS Glue and PySpark to efficiently process large datasets.\nData Workflow Automation: Build and manage automated data workflows using AWS Data Pipeline, ensuring seamless scheduling, monitoring, and management of data jobs.\nData Integration: Work with different AWS data storage services (e.g., S3, Redshift, RDS) to ensure smooth integration and movement of data across platforms.\nOptimization and Scaling: Optimize and scale data pipelines for high performance and cost efficiency, utilizing AWS services like Lambda, S3, and EC2.","['CI & CD', 'Cloudera Data platform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker']",2025-06-12 01:40:23
Data Engineer,Virtusa,8-12 Years,,Hyderabad,Information Technology,"Experience:\n8+ years of experience in data engineering, specifically in cloud environments like AWS.\nProficiency in PySpark for distributed data processing and transformation.\nSolid experience with AWS Glue for ETL jobs and managing data workflows.\nHands-on experience with AWS Data Pipeline (DPL) for workflow orchestration.\nStrong experience with AWS services such as S3, Lambda, Redshift, RDS, and EC2.\nTechnical Skills:\nProficiency in Python and PySpark for data processing and transformation tasks.\nDeep understanding of ETL concepts and best practices.\nFamiliarity with AWS Glue (ETL jobs, Data Catalog, and Crawlers).\nExperience building and maintaining data pipelines with AWS Data Pipeline or similar orchestration tools.\nFamiliarity with AWS S3 for data storage and management, including file formats (CSV, Parquet, Avro).\nStrong knowledge of SQL for querying and manipulating relational and semi-structured data.\nExperience with Data Warehousing and Big Data technologies, specifically within AWS.\nAdditional Skills:\nExperience with AWS Lambda for serverless data processing and orchestration.\nUnderstanding of AWS Redshift for data warehousing and analytics.\nFamiliarity with Data Lakes, Amazon EMR, and Kinesis for streaming data processing.\nKnowledge of data governance practices, including data lineage and auditing.\nFamiliarity with CI/CD pipelines and Git for version control.\nExperience with Docker and containerization for building and deploying applications.\nDesign and Build Data Pipelines: Design, implement, and optimize data pipelines on AWS using PySpark, AWS Glue, and AWS Data Pipeline to automate data integration, transformation, and storage processes.\nETL Development: Develop and maintain Extract, Transform, and Load (ETL) processes using AWS Glue and PySpark to efficiently process large datasets.\nData Workflow Automation: Build and manage automated data workflows using AWS Data Pipeline, ensuring seamless scheduling, monitoring, and management of data jobs.\nData Integration: Work with different AWS data storage services (e.g., S3, Redshift, RDS) to ensure smooth integration and movement of data across platforms.\nOptimization and Scaling: Optimize and scale data pipelines for high performance and cost efficiency, utilizing AWS services like Lambda, S3, and EC2.","['Snowflake design', 'ETL/ELT', 'Python', 'Sql', 'Data Warehousing', 'cloud platform']",2025-06-12 01:40:24
Data Engineer,Virtusa,6-8 Years,,Hyderabad,Information Technology,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools","['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 01:40:26
Data Engineer,Synechron Technologies Private Limited,6-8 Years,,Gurugram,Software Engineering,"Job Profile : Data and analytics expert with 6-8 years of experience, proficient in big data technologies, data architecture, and customer-focused solutions.\nTechnology\nIn addition, you'll bring a selection of the following technical skills with an experience around 6-8 years.\nResponsibilities:\nDesign and implement data pipelines, data lakes, and visualization solutions to create business value.\nDevelop and maintain big data solutions using technologies like AWS Redshift, Kubernetes, Spark, Python, SQL, DBT, and Dataiku.\nApply best practices in data architecture, modeling, and metadata management.\nAnalyze and troubleshoot complex data issues, ensuring timely resolution..\nPrimary Skills:\n6-8 years of experience in data and analytics.\nProficiency in big data technologies: AWS Redshift, Kubernetes, Spark, Python, complex SQL, DBT, and Dataiku.\nStrong understanding of data architecture, modeling, and metadata management.\nExcellent data analytical and troubleshooting skills.\nStrong communication skills and a customer-focused approach\nGood to have:\nExperience with other data-related tools and platforms.\nKnowledge of emerging trends in data and analytics\nExperience: 6 to 8 years","['Aws Redshift', 'Spark', 'Kubernetes', 'Sql', 'Python']",2025-06-12 01:40:27
Data Engineer,Virtusa,5-8 Years,,Chennai,Information Technology,8 Years overall IT experience with minimum 5 years of work experience in below tech skills\nStrong experience in Python Scripting and PySpark for data processing\nProficiency in SQL dealing with big data over Informatica ETL\nProven experience in Data quality and data optimization of data lake in Iceberg format with strong understanding of architecture\nExperience in AWS Glue jobs\nExperience in AWS cloud platform and its data services S3 Redshift Lambda EMR Airflow Postgres SNS Event bridge\nExpertise in BASH Shell scripting\nStrong understanding of healthcare data systems and experience leading data engineering teams\nExperience in Agile environments\nExcellent problem solving skills and attention to detail\nEffective communication and collaboration skills\nResponsibilities\nLeads development of data pipelines and architectures that handle large scale data sets\nDesigns constructs and tests data architecture aligned with business requirements\nProvides technical leadership for data projects ensuring best practices and high quality data solutions\nCollaborates with product finance and other business units to ensure data pipelines meet business requirements\nWork with DBT Data Build Tool for transforming raw data into actionable insights\nOversees development of data solutions that enable predictive and prescriptive analytics\nEnsures the technical quality of solutions managing data as it moves across environments\nAligns data architecture to Healthfirst solution architecture,"['Iceberg', 'Python', 'Pyspark', 'Sql', 'AWS Glue', 'Informatica']",2025-06-12 01:40:28
Data Engineer,Virtusa,7-12 Years,,Chennai,Information Technology,"Job Title: Data Engineer\n\nExperience Level\n6+ years in Data Engineering\nKey Responsibilities\nAs a Data Engineer, you will:\nData Engineering: Apply expertise in Snowflake, DBT, and Python to develop and manage data solutions.\nCloud Integration: Utilize AWS Cloud services, specifically AWS S3 and Lambda, for data operations.\nSQL Development: Leverage strong SQL knowledge for data manipulation and querying.\nMandatory Skills & Experience\nTechnical Proficiency:\nSQL: Strong knowledge in SQL.\nData Warehousing & Transformation: Expertise in Snowflake and DBT (minimum 2+ years of experience).\nProgramming: Expertise in Python (minimum 2+ years of experience).\nCloud Platform: Knowledge in AWS Cloud (specifically AWS S3 & Lambda).\nETL/ELT Tools (Desirable): Knowledge of SnapLogic or Fivetran tools is an added advantage.\nExperience & Qualifications:\n6+ years of experience in data engineering.","['Snowflake design', 'ETL/ELT', 'Python', 'Sql', 'Data Warehousing', 'cloud platform']",2025-06-12 01:40:30
Data Engineer,Virtusa,7-12 Years,,Chennai,Information Technology,"Job Title: Data Engineer\n\nExperience Level\n6+ years in Data Engineering\nKey Responsibilities\nAs a Data Engineer, you will:\nData Engineering: Apply expertise in Snowflake, DBT, and Python to develop and manage data solutions.\nCloud Integration: Utilize AWS Cloud services, specifically AWS S3 and Lambda, for data operations.\nSQL Development: Leverage strong SQL knowledge for data manipulation and querying.\nMandatory Skills & Experience\nTechnical Proficiency:\nSQL: Strong knowledge in SQL.\nData Warehousing & Transformation: Expertise in Snowflake and DBT (minimum 2+ years of experience).\nProgramming: Expertise in Python (minimum 2+ years of experience).\nCloud Platform: Knowledge in AWS Cloud (specifically AWS S3 & Lambda).\nETL/ELT Tools (Desirable): Knowledge of SnapLogic or Fivetran tools is an added advantage.\nExperience & Qualifications:\n6+ years of experience in data engineering.","['Snowflake design', 'ETL/ELT', 'Python', 'Sql', 'Data Warehousing', 'cloud platform']",2025-06-12 01:40:38
