title,company,experience,salary,location,description,industry,skills,scraped_at
Data Architect,Virtusa,9-13 Years,,Chennai,"Bachelor's Degree or equivalent number of years of experience in a Computer Science or Data Management related field.\nExperience in leading and delivering enterprise data platform architectural thinking, and its practical application.\nExperience in the use of conceptual and logical data modelling technologies.\nExperience in defining and working with information and data regulatory governances.\nThe role holder will possess a blend of data/information architecture, analysis, and engineering skills.\nExperience in known industry IT architectural patterns and IT architecture ways of working/methodologies (e.g. FAIR data principles, Data Mesh).\nUnderstanding the appropriate data structure and technology based on business use case and completely familiar with data lifecycles.\nDesirable:\nExperience in a data architect role with practical examples of designing and providing data engineering/architectural blueprints that have been implemented.\nExperience of Information and Data Governance frameworks and their application in a commercial organization.\nUnderstands Data Platforms concepts and cloud-based containerization strategies for hybrid cloud environments.\nExperience in Agile data definition scrums.\nExperience in the use of tooling, e.g. metadata cataloguing tools, data modelling tools, EA tools.\nUnderstanding of or familiarity with,Data Mesh approaches (as distinct from Data Fabric or Data Platform).\nQualification\nDetailed Job Description forInformation Architect at PAN India:\nBachelor's Degree or equivalent number of years of experience in a Computer Science or Data Management related field.\nExperience in leading and delivering enterprise data platform architectural thinking, and its practical application.\nExperience in the use of conceptual and logical data modelling technologies.\nExperience in defining and working with information and data regulatory governances.\nThe role holder will possess a blend of data/information architecture, analysis, and engineering skills.\nExperience in known industry IT architectural patterns and IT architecture ways of working/methodologies (e.g. FAIR data principles, Data Mesh).\nUnderstanding the appropriate data structure and technology based on business use case and completely familiar with data lifecycles.",Consulting,"['FAIR data principles', 'Data Mesh', 'data lifecycles', 'data regulatory governances']",2025-06-12 15:31:30
Data Architect,Virtusa,9-13 Years,,Chennai,"Bachelor's Degree or equivalent number of years of experience in a Computer Science or Data Management related field.\nExperience in leading and delivering enterprise data platform architectural thinking, and its practical application.\nExperience in the use of conceptual and logical data modelling technologies.\nExperience in defining and working with information and data regulatory governances.\nThe role holder will possess a blend of data/information architecture, analysis, and engineering skills.\nExperience in known industry IT architectural patterns and IT architecture ways of working/methodologies (e.g. FAIR data principles, Data Mesh).\nUnderstanding the appropriate data structure and technology based on business use case and completely familiar with data lifecycles.\nDesirable:\nExperience in a data architect role with practical examples of designing and providing data engineering/architectural blueprints that have been implemented.\nExperience of Information and Data Governance frameworks and their application in a commercial organization.\nUnderstands Data Platforms concepts and cloud-based containerization strategies for hybrid cloud environments.\nExperience in Agile data definition scrums.\nExperience in the use of tooling, e.g. metadata cataloguing tools, data modelling tools, EA tools.\nUnderstanding of or familiarity with,Data Mesh approaches (as distinct from Data Fabric or Data Platform).\nQualification\nDetailed Job Description forInformation Architect at PAN India:\nBachelor's Degree or equivalent number of years of experience in a Computer Science or Data Management related field.\nExperience in leading and delivering enterprise data platform architectural thinking, and its practical application.\nExperience in the use of conceptual and logical data modelling technologies.\nExperience in defining and working with information and data regulatory governances.\nThe role holder will possess a blend of data/information architecture, analysis, and engineering skills.\nExperience in known industry IT architectural patterns and IT architecture ways of working/methodologies (e.g. FAIR data principles, Data Mesh).\nUnderstanding the appropriate data structure and technology based on business use case and completely familiar with data lifecycles.",Consulting,"['FAIR data principles', 'Data Mesh', 'data lifecycles', 'data regulatory governances']",2025-06-12 15:31:31
Data Architect,Virtusa,9-13 Years,,Chennai,"Bachelor's Degree or equivalent number of years of experience in a Computer Science or Data Management related field.\nExperience in leading and delivering enterprise data platform architectural thinking, and its practical application.\nExperience in the use of conceptual and logical data modelling technologies.\nExperience in defining and working with information and data regulatory governances.\nThe role holder will possess a blend of data/information architecture, analysis, and engineering skills.\nExperience in known industry IT architectural patterns and IT architecture ways of working/methodologies (e.g. FAIR data principles, Data Mesh).\nUnderstanding the appropriate data structure and technology based on business use case and completely familiar with data lifecycles.\nDesirable:\nExperience in a data architect role with practical examples of designing and providing data engineering/architectural blueprints that have been implemented.\nExperience of Information and Data Governance frameworks and their application in a commercial organization.\nUnderstands Data Platforms concepts and cloud-based containerization strategies for hybrid cloud environments.\nExperience in Agile data definition scrums.\nExperience in the use of tooling, e.g. metadata cataloguing tools, data modelling tools, EA tools.\nUnderstanding of or familiarity with,Data Mesh approaches (as distinct from Data Fabric or Data Platform).\nQualification\nDetailed Job Description forInformation Architect at PAN India:\nBachelor's Degree or equivalent number of years of experience in a Computer Science or Data Management related field.\nExperience in leading and delivering enterprise data platform architectural thinking, and its practical application.\nExperience in the use of conceptual and logical data modelling technologies.\nExperience in defining and working with information and data regulatory governances.\nThe role holder will possess a blend of data/information architecture, analysis, and engineering skills.\nExperience in known industry IT architectural patterns and IT architecture ways of working/methodologies (e.g. FAIR data principles, Data Mesh).\nUnderstanding the appropriate data structure and technology based on business use case and completely familiar with data lifecycles.",Consulting,"['FAIR data principles', 'Data Mesh', 'data lifecycles', 'data regulatory governances']",2025-06-12 15:31:36
Data Architect,Tata Consultancy Services Limited,4-8 Years,,"Thane, Kolkata, Mumbai","Job Description\nPosition Title- Data Architect / Solution Architect\nLocation: Pan india\nThis position description should represent your role and responsibilities at the time of appointment, however due to the dynamic nature of our business, your job title, key tasks and responsibilities are likely to evolve over time. The flexibility to adapt to any changes should be considered a key requirement of working at TPG Telecom.\nRole Purpose & Environment\nIn this role you will work hand-in-hand with various technology and business stakeholders to design and build TPGs modern data platform in the cloud and manage the legacy applications. You will provide strategic direction and leadership guidance driving architecture and implementation initiatives leveraging your knowledge and experience in the area. The role also extends into the consumption side of data and will allow you to deliver business intelligence capabilities (including Advanced Analytics) and strategies for information delivery and data exploration to support business objectives and requirements.\nWe are seeking someone with the passion for understanding and leveraging data, with the attitude and behaviour to deliver on commitments and take ownership of data products when required.\nKey Responsibilities\nDefine and design the overall data architecture, strategy, and data capabilities roadmap that are consistent with our technology direction.\nDefine and design the data platforms, tools and governing process\nCreate, maintain and communicate go-forward strategies for business intelligence capabilities and tools.\nResponsible and accountable for producing the data solution and data product architecture design ensuring that they are submitted and progress via the prescribed governance process through to approval (ARB) in a timely manner aligned with project prescribed timelines.\nDefine and review data solutions for re-usability, scalability, synergy opportunities and alignment to defined best practice and guidelines\nCreate and evolve data technology roadmap, to align with continuously evolving business needs.\nHelp defining and improving best practices, guidelines, and integration with other enterprise solutions.\nParticipates in planning, dependency identification, and management as well as estimation with Project Managers.\nLeads Work Breakdown identification and workshops utilising Architecture designs as input\nDemonstrated grasp of Architecture techniques and ability to work effectively with senior business stakeholders and initiative owners.\nAct as Technology advisors on data to business leaders and strategic leaders on technology direction.\nKey Experience, Skills, and Qualifications\nDomain Expertise\n7 years+ of professional experience in data architecture or data engineering role. Demonstrating a high degree of proficiency in designing and developing complex, high quality data solutions according to our architecture governance policies guidelines.\nStrong experience in developing and maintaining data warehouses (e.g., Redshift, Teradata)\nAble to work independently and develop the solution architecture according to the business requirements and compliance requirements.\nStrong Data warehouse development experience using different ETL tools (e.g. SAS DI, Glue, DBT)\nExperience with data streaming platforms (e.g., Kafka/Kinesis)\nFamiliarity with different operational orchestration platforms (e.g., Airflow, LSF scheduler etc)\nExperience with data catalogue and data governance tools.\nUnderstanding of CLDM, Star Schema, Data Mesh and Data Product concepts\nExposure to machine learning, reporting, data sharing, data intensive application-oriented use cases\nExtensive experience in consulting with business stakeholders and other user groups to deliver both strategic and tactical information management solution.\nExperience working within matrix structures, with demonstrated ability to broker outcomes effectively and collaboratively with colleagues and peers.\nExperience on different delivery methodologies (e.g., Waterfall, Agile)\nTelecommunication Industry experience\nBachelor's degree in computer science, computer programming or related field preferred\nIndividual Skills, Mindset & Behaviours\nStrong communication skills with ability to communicate complex technical concepts in a digestible way\nAbility to effortlessly switch gears from summary view for leadership to hands-on discussion with practitioners\nAssertive, with the confidence to be voice of authority what is best for team\nHigh-energy and passionate outlook to the role and can influence those around her/him\nAbility to build a sense of trust and rapport that creates a comfortable, respectful, and effective workplace",IT Infrastructure,"['Architect', 'Data Architect', 'Kafka', 'Etl']",2025-06-12 15:31:38
Data Architect,Tata Consultancy Services Limited,4-8 Years,,"Noida, Delhi NCR, Pune","Position Title- Data Architect / Solution Architect\nLocation: Pan india\nThis position description should represent your role and responsibilities at the time of appointment, however due to the dynamic nature of our business, your job title, key tasks and responsibilities are likely to evolve over time. The flexibility to adapt to any changes should be considered a key requirement of working at TPG Telecom.\nRole Purpose & Environment\nIn this role you will work hand-in-hand with various technology and business stakeholders to design and build TPGs modern data platform in the cloud and manage the legacy applications. You will provide strategic direction and leadership guidance driving architecture and implementation initiatives leveraging your knowledge and experience in the area. The role also extends into the consumption side of data and will allow you to deliver business intelligence capabilities (including Advanced Analytics) and strategies for information delivery and data exploration to support business objectives and requirements.\nWe are seeking someone with the passion for understanding and leveraging data, with the attitude and behaviour to deliver on commitments and take ownership of data products when required.\nKey Responsibilities\nDefine and design the overall data architecture, strategy, and data capabilities roadmap that are consistent with our technology direction.\nDefine and design the data platforms, tools and governing process\nCreate, maintain and communicate go-forward strategies for business intelligence capabilities and tools.\nResponsible and accountable for producing the data solution and data product architecture design ensuring that they are submitted and progress via the prescribed governance process through to approval (ARB) in a timely manner aligned with project prescribed timelines.\nDefine and review data solutions for re-usability, scalability, synergy opportunities and alignment to defined best practice and guidelines\nCreate and evolve data technology roadmap, to align with continuously evolving business needs.\nHelp defining and improving best practices, guidelines, and integration with other enterprise solutions.\nParticipates in planning, dependency identification, and management as well as estimation with Project Managers.\nLeads Work Breakdown identification and workshops utilising Architecture designs as input\nDemonstrated grasp of Architecture techniques and ability to work effectively with senior business stakeholders and initiative owners.\nAct as Technology advisors on data to business leaders and strategic leaders on technology direction.\nKey Experience, Skills, and Qualifications\nDomain Expertise\n7 years+ of professional experience in data architecture or data engineering role. Demonstrating a high degree of proficiency in designing and developing complex, high quality data solutions according to our architecture governance policies guidelines.\nStrong experience in developing and maintaining data warehouses (e.g., Redshift, Teradata)\nAble to work independently and develop the solution architecture according to the business requirements and compliance requirements.\nStrong Data warehouse development experience using different ETL tools (e.g. SAS DI, Glue, DBT)\nExperience with data streaming platforms (e.g., Kafka/Kinesis)\nFamiliarity with different operational orchestration platforms (e.g., Airflow, LSF scheduler etc)\nExperience with data catalogue and data governance tools.\nUnderstanding of CLDM, Star Schema, Data Mesh and Data Product concepts\nExposure to machine learning, reporting, data sharing, data intensive application-oriented use cases\nExtensive experience in consulting with business stakeholders and other user groups to deliver both strategic and tactical information management solution.\nExperience working within matrix structures, with demonstrated ability to broker outcomes effectively and collaboratively with colleagues and peers.\nExperience on different delivery methodologies (e.g., Waterfall, Agile)\nTelecommunication Industry experience\nBachelor's degree in computer science, computer programming or related field preferred\nIndividual Skills, Mindset & Behaviours\nStrong communication skills with ability to communicate complex technical concepts in a digestible way\nAbility to effortlessly switch gears from summary view for leadership to hands-on discussion with practitioners\nAssertive, with the confidence to be voice of authority what is best for team\nHigh-energy and passionate outlook to the role and can influence those around her/him\nAbility to build a sense of trust and rapport that creates a comfortable, respectful, and effective workplace",IT Infrastructure,"['Teradata', 'Kafka', 'Redshift', 'Etl']",2025-06-12 15:31:39
Data Architect,Tata Consultancy Services Limited,4-8 Years,,"Ahmedabad, Bengaluru, Chennai","Position Title- Data Architect / Solution Architect\nLocation: Pan india\nThis position description should represent your role and responsibilities at the time of appointment, however due to the dynamic nature of our business, your job title, key tasks and responsibilities are likely to evolve over time. The flexibility to adapt to any changes should be considered a key requirement of working at TPG Telecom.\nRole Purpose & Environment\nIn this role you will work hand-in-hand with various technology and business stakeholders to design and build TPGs modern data platform in the cloud and manage the legacy applications. You will provide strategic direction and leadership guidance driving architecture and implementation initiatives leveraging your knowledge and experience in the area. The role also extends into the consumption side of data and will allow you to deliver business intelligence capabilities (including Advanced Analytics) and strategies for information delivery and data exploration to support business objectives and requirements.\nWe are seeking someone with the passion for understanding and leveraging data, with the attitude and behaviour to deliver on commitments and take ownership of data products when required.\nKey Responsibilities\nDefine and design the overall data architecture, strategy, and data capabilities roadmap that are consistent with our technology direction.\nDefine and design the data platforms, tools and governing process\nCreate, maintain and communicate go-forward strategies for business intelligence capabilities and tools.\nResponsible and accountable for producing the data solution and data product architecture design ensuring that they are submitted and progress via the prescribed governance process through to approval (ARB) in a timely manner aligned with project prescribed timelines.\nDefine and review data solutions for re-usability, scalability, synergy opportunities and alignment to defined best practice and guidelines\nCreate and evolve data technology roadmap, to align with continuously evolving business needs.\nHelp defining and improving best practices, guidelines, and integration with other enterprise solutions.\nParticipates in planning, dependency identification, and management as well as estimation with Project Managers.\nLeads Work Breakdown identification and workshops utilising Architecture designs as input\nDemonstrated grasp of Architecture techniques and ability to work effectively with senior business stakeholders and initiative owners.\nAct as Technology advisors on data to business leaders and strategic leaders on technology direction.\nKey Experience, Skills, and Qualifications\nDomain Expertise\n7 years+ of professional experience in data architecture or data engineering role. Demonstrating a high degree of proficiency in designing and developing complex, high quality data solutions according to our architecture governance policies guidelines.\nStrong experience in developing and maintaining data warehouses (e.g., Redshift, Teradata)\nAble to work independently and develop the solution architecture according to the business requirements and compliance requirements.\nStrong Data warehouse development experience using different ETL tools (e.g. SAS DI, Glue, DBT)\nExperience with data streaming platforms (e.g., Kafka/Kinesis)\nFamiliarity with different operational orchestration platforms (e.g., Airflow, LSF scheduler etc)\nExperience with data catalogue and data governance tools.\nUnderstanding of CLDM, Star Schema, Data Mesh and Data Product concepts\nExposure to machine learning, reporting, data sharing, data intensive application-oriented use cases\nExtensive experience in consulting with business stakeholders and other user groups to deliver both strategic and tactical information management solution.\nExperience working within matrix structures, with demonstrated ability to broker outcomes effectively and collaboratively with colleagues and peers.\nExperience on different delivery methodologies (e.g., Waterfall, Agile)\nTelecommunication Industry experience\nBachelor's degree in computer science, computer programming or related field preferred\nIndividual Skills, Mindset & Behaviours\nStrong communication skills with ability to communicate complex technical concepts in a digestible way\nAbility to effortlessly switch gears from summary view for leadership to hands-on discussion with practitioners\nAssertive, with the confidence to be voice of authority what is best for team\nHigh-energy and passionate outlook to the role and can influence those around her/him\nAbility to build a sense of trust and rapport that creates a comfortable, respectful, and effective workplace",IT Infrastructure,"['Teradata', 'Kafka', 'Redshift', 'Etl']",2025-06-12 15:31:41
Data Architect,Tata Consultancy Services Limited,4-8 Years,,"Gurugram, Delhi, Hyderabad","Position Title- Data Architect / Solution Architect\nLocation: Pan india\nThis position description should represent your role and responsibilities at the time of appointment, however due to the dynamic nature of our business, your job title, key tasks and responsibilities are likely to evolve over time. The flexibility to adapt to any changes should be considered a key requirement of working at TPG Telecom.\nRole Purpose & Environment\nIn this role you will work hand-in-hand with various technology and business stakeholders to design and build TPGs modern data platform in the cloud and manage the legacy applications. You will provide strategic direction and leadership guidance driving architecture and implementation initiatives leveraging your knowledge and experience in the area. The role also extends into the consumption side of data and will allow you to deliver business intelligence capabilities (including Advanced Analytics) and strategies for information delivery and data exploration to support business objectives and requirements.\nWe are seeking someone with the passion for understanding and leveraging data, with the attitude and behaviour to deliver on commitments and take ownership of data products when required.\nKey Responsibilities\nDefine and design the overall data architecture, strategy, and data capabilities roadmap that are consistent with our technology direction.\nDefine and design the data platforms, tools and governing process\nCreate, maintain and communicate go-forward strategies for business intelligence capabilities and tools.\nResponsible and accountable for producing the data solution and data product architecture design ensuring that they are submitted and progress via the prescribed governance process through to approval (ARB) in a timely manner aligned with project prescribed timelines.\nDefine and review data solutions for re-usability, scalability, synergy opportunities and alignment to defined best practice and guidelines\nCreate and evolve data technology roadmap, to align with continuously evolving business needs.\nHelp defining and improving best practices, guidelines, and integration with other enterprise solutions.\nParticipates in planning, dependency identification, and management as well as estimation with Project Managers.\nLeads Work Breakdown identification and workshops utilising Architecture designs as input\nDemonstrated grasp of Architecture techniques and ability to work effectively with senior business stakeholders and initiative owners.\nAct as Technology advisors on data to business leaders and strategic leaders on technology direction.\nKey Experience, Skills, and Qualifications\nDomain Expertise\n7 years+ of professional experience in data architecture or data engineering role. Demonstrating a high degree of proficiency in designing and developing complex, high quality data solutions according to our architecture governance policies guidelines.\nStrong experience in developing and maintaining data warehouses (e.g., Redshift, Teradata)\nAble to work independently and develop the solution architecture according to the business requirements and compliance requirements.\nStrong Data warehouse development experience using different ETL tools (e.g. SAS DI, Glue, DBT)\nExperience with data streaming platforms (e.g., Kafka/Kinesis)\nFamiliarity with different operational orchestration platforms (e.g., Airflow, LSF scheduler etc)\nExperience with data catalogue and data governance tools.\nUnderstanding of CLDM, Star Schema, Data Mesh and Data Product concepts\nExposure to machine learning, reporting, data sharing, data intensive application-oriented use cases\nExtensive experience in consulting with business stakeholders and other user groups to deliver both strategic and tactical information management solution.\nExperience working within matrix structures, with demonstrated ability to broker outcomes effectively and collaboratively with colleagues and peers.\nExperience on different delivery methodologies (e.g., Waterfall, Agile)\nTelecommunication Industry experience\nBachelor's degree in computer science, computer programming or related field preferred\nIndividual Skills, Mindset & Behaviours\nStrong communication skills with ability to communicate complex technical concepts in a digestible way\nAbility to effortlessly switch gears from summary view for leadership to hands-on discussion with practitioners\nAssertive, with the confidence to be voice of authority what is best for team\nHigh-energy and passionate outlook to the role and can influence those around her/him\nAbility to build a sense of trust and rapport that creates a comfortable, respectful, and effective workplace",IT Infrastructure,"['Teradata', 'Kafka', 'Redshift', 'Etl']",2025-06-12 15:31:42
Data Architect,Tata Consultancy Services Limited,4-8 Years,,"Thane, Kolkata, Mumbai","Position Title- Data Architect / Solution Architect\nLocation: Pan india\nThis position description should represent your role and responsibilities at the time of appointment, however due to the dynamic nature of our business, your job title, key tasks and responsibilities are likely to evolve over time. The flexibility to adapt to any changes should be considered a key requirement of working at TPG Telecom.\nRole Purpose & Environment\nIn this role you will work hand-in-hand with various technology and business stakeholders to design and build TPGs modern data platform in the cloud and manage the legacy applications. You will provide strategic direction and leadership guidance driving architecture and implementation initiatives leveraging your knowledge and experience in the area. The role also extends into the consumption side of data and will allow you to deliver business intelligence capabilities (including Advanced Analytics) and strategies for information delivery and data exploration to support business objectives and requirements.\nWe are seeking someone with the passion for understanding and leveraging data, with the attitude and behaviour to deliver on commitments and take ownership of data products when required.\nKey Responsibilities\nDefine and design the overall data architecture, strategy, and data capabilities roadmap that are consistent with our technology direction.\nDefine and design the data platforms, tools and governing process\nCreate, maintain and communicate go-forward strategies for business intelligence capabilities and tools.\nResponsible and accountable for producing the data solution and data product architecture design ensuring that they are submitted and progress via the prescribed governance process through to approval (ARB) in a timely manner aligned with project prescribed timelines.\nDefine and review data solutions for re-usability, scalability, synergy opportunities and alignment to defined best practice and guidelines\nCreate and evolve data technology roadmap, to align with continuously evolving business needs.\nHelp defining and improving best practices, guidelines, and integration with other enterprise solutions.\nParticipates in planning, dependency identification, and management as well as estimation with Project Managers.\nLeads Work Breakdown identification and workshops utilising Architecture designs as input\nDemonstrated grasp of Architecture techniques and ability to work effectively with senior business stakeholders and initiative owners.\nAct as Technology advisors on data to business leaders and strategic leaders on technology direction.\nKey Experience, Skills, and Qualifications\nDomain Expertise\n7 years+ of professional experience in data architecture or data engineering role. Demonstrating a high degree of proficiency in designing and developing complex, high quality data solutions according to our architecture governance policies guidelines.\nStrong experience in developing and maintaining data warehouses (e.g., Redshift, Teradata)\nAble to work independently and develop the solution architecture according to the business requirements and compliance requirements.\nStrong Data warehouse development experience using different ETL tools (e.g. SAS DI, Glue, DBT)\nExperience with data streaming platforms (e.g., Kafka/Kinesis)\nFamiliarity with different operational orchestration platforms (e.g., Airflow, LSF scheduler etc)\nExperience with data catalogue and data governance tools.\nUnderstanding of CLDM, Star Schema, Data Mesh and Data Product concepts\nExposure to machine learning, reporting, data sharing, data intensive application-oriented use cases\nExtensive experience in consulting with business stakeholders and other user groups to deliver both strategic and tactical information management solution.\nExperience working within matrix structures, with demonstrated ability to broker outcomes effectively and collaboratively with colleagues and peers.\nExperience on different delivery methodologies (e.g., Waterfall, Agile)\nTelecommunication Industry experience\nBachelor's degree in computer science, computer programming or related field preferred\nIndividual Skills, Mindset & Behaviours\nStrong communication skills with ability to communicate complex technical concepts in a digestible way\nAbility to effortlessly switch gears from summary view for leadership to hands-on discussion with practitioners\nAssertive, with the confidence to be voice of authority what is best for team\nHigh-energy and passionate outlook to the role and can influence those around her/him\nAbility to build a sense of trust and rapport that creates a comfortable, respectful, and effective workplace",IT Infrastructure,"['Teradata', 'Kafka', 'Redshift', 'Etl']",2025-06-12 15:31:43
Data Architect,Tata Consultancy Services Limited,4-8 Years,,"Ahmedabad, Bengaluru, Chennai","Job Description\nPosition Title- Data Architect / Solution Architect\nLocation: Pan india\nThis position description should represent your role and responsibilities at the time of appointment, however due to the dynamic nature of our business, your job title, key tasks and responsibilities are likely to evolve over time. The flexibility to adapt to any changes should be considered a key requirement of working at TPG Telecom.\nRole Purpose & Environment\nIn this role you will work hand-in-hand with various technology and business stakeholders to design and build TPGs modern data platform in the cloud and manage the legacy applications. You will provide strategic direction and leadership guidance driving architecture and implementation initiatives leveraging your knowledge and experience in the area. The role also extends into the consumption side of data and will allow you to deliver business intelligence capabilities (including Advanced Analytics) and strategies for information delivery and data exploration to support business objectives and requirements.\nWe are seeking someone with the passion for understanding and leveraging data, with the attitude and behaviour to deliver on commitments and take ownership of data products when required.\nKey Responsibilities\nDefine and design the overall data architecture, strategy, and data capabilities roadmap that are consistent with our technology direction.\nDefine and design the data platforms, tools and governing process\nCreate, maintain and communicate go-forward strategies for business intelligence capabilities and tools.\nResponsible and accountable for producing the data solution and data product architecture design ensuring that they are submitted and progress via the prescribed governance process through to approval (ARB) in a timely manner aligned with project prescribed timelines.\nDefine and review data solutions for re-usability, scalability, synergy opportunities and alignment to defined best practice and guidelines\nCreate and evolve data technology roadmap, to align with continuously evolving business needs.\nHelp defining and improving best practices, guidelines, and integration with other enterprise solutions.\nParticipates in planning, dependency identification, and management as well as estimation with Project Managers.\nLeads Work Breakdown identification and workshops utilising Architecture designs as input\nDemonstrated grasp of Architecture techniques and ability to work effectively with senior business stakeholders and initiative owners.\nAct as Technology advisors on data to business leaders and strategic leaders on technology direction.\nKey Experience, Skills, and Qualifications\nDomain Expertise\n7 years+ of professional experience in data architecture or data engineering role. Demonstrating a high degree of proficiency in designing and developing complex, high quality data solutions according to our architecture governance policies guidelines.\nStrong experience in developing and maintaining data warehouses (e.g., Redshift, Teradata)\nAble to work independently and develop the solution architecture according to the business requirements and compliance requirements.\nStrong Data warehouse development experience using different ETL tools (e.g. SAS DI, Glue, DBT)\nExperience with data streaming platforms (e.g., Kafka/Kinesis)\nFamiliarity with different operational orchestration platforms (e.g., Airflow, LSF scheduler etc)\nExperience with data catalogue and data governance tools.\nUnderstanding of CLDM, Star Schema, Data Mesh and Data Product concepts\nExposure to machine learning, reporting, data sharing, data intensive application-oriented use cases\nExtensive experience in consulting with business stakeholders and other user groups to deliver both strategic and tactical information management solution.\nExperience working within matrix structures, with demonstrated ability to broker outcomes effectively and collaboratively with colleagues and peers.\nExperience on different delivery methodologies (e.g., Waterfall, Agile)\nTelecommunication Industry experience\nBachelor's degree in computer science, computer programming or related field preferred\nIndividual Skills, Mindset & Behaviours\nStrong communication skills with ability to communicate complex technical concepts in a digestible way\nAbility to effortlessly switch gears from summary view for leadership to hands-on discussion with practitioners\nAssertive, with the confidence to be voice of authority what is best for team\nHigh-energy and passionate outlook to the role and can influence those around her/him\nAbility to build a sense of trust and rapport that creates a comfortable, respectful, and effective workplace",IT Infrastructure,"['Architect', 'Data Architect', 'Kafka', 'Etl']",2025-06-12 15:31:44
Network Data Architect,Aspire Systems India Private Limited,12-16 Years,,Chennai,"Minimum 12+ years of relevant hands on experience in configuring and troubleshooting Network devices, Cisco routers, ACI, Switches, Firewalls and F5 Load Balancers.\nAbility to troubleshoot and upgrade Cisco routers, Switches, Firewalls and F5 Load Balancers.\nMust be skilled to perform router/switch network hardware/software upgrades.\nMust have hands on experience in Routing Protocol Development (BGP, OSPF, VPN and Static.), providing IP Addressing Strategy - NAT, Re-Addressing, WAN - IP Routed Network Integration Wireless LAN technologies.\nExperience with Cisco LAN routing, switching, Security, Voice, and Wireless LAN products.\nExperience in network protocols VPN, OSPF, BGP, TCP/IP.\nExperience in Cisco nexus and Datacenter support.\nExperience and hands on with Cisco Client/ Cisco Prime/ Public DNS services.\nExperience in providing Network Engineering services in support of the large enterprise customer data networks.\nConfigure, administer firewall infrastructure, working with Cisco and Palo alto.\nWork with customer technical teams to provide solutions for customers internetworking communications requirements.\nExperience in proposal presentation and participating in Pre-sales engagement is an added advantage.\nProvide valuable solutions for colleagues and stakeholders, both tactically and strategically and ensure compliance to process, procedure, and tools within the operations.\nExcellent written and verbal communications skills are essential.\nProven analytic skills and the ability to isolate and resolve complex issues.\nGood understanding of Migration Effort, Resources and Timelines estimation.\nDesign public cloud infrastructure and DevOps solutions, architectures and roadmaps.\nEvolve and build best practice materials for Infrastructure-as-Code and Configuration Management.\nEnable sales including knowledge transfer, architecture and design, as well as participate in the sales cycle as needed.\nParticipate in community and market activities including participation in trade shows.\nCreate, build and grow partnerships with various organizations relevant to the practice,\nEnable development teams to leverage cloud and cloud-native architectures with new and existing applications.",Software,"['Network Data Architect', 'Tcp/ip', 'BGP', 'OSPF']",2025-06-12 15:31:46
Data Architect,Xoom,2-4 Years,,Bengaluru,"Your day to day:\nDevelop and implement a long-term data strategy aligned with business objectives, focusing on event-driven architectures and real-time analytics.\nDemonstrate deep expertise in Kappa and Lambda architectures, data modeling, domain-driven design, CQRS, and stream processing frameworks.\nAddress complex data challenges, such as large-scale data integration, real-time analytics, and data governance.\nDesign and model data structures, schemas, and relationships to optimize performance and scalability.\nLead the transition of existing applications to event-driven architectures, leveraging event modeling, domain-driven design, and CQRS.\nCollaborate with development teams to ensure seamless integration of data models and architectures.\nMentor and guide junior data professionals, fostering a culture of innovation and excellence.\nStay updated on emerging technologies and evaluate their suitability for our organization.\nYour way to impact:-\nHybrid Data Architecture Expertise. Craft solutions that seamlessly integrate on-premises, cloud-based, and hybrid data storage options, ensuring optimal data management across the organization.\nCompliance & Governance. Ensure your data architectures and designs adhere to PayPals specific enterprise data architecture standards for each data store type.\nData Access Pattern & Structure Design. Design data structures and access patterns that meet both functional and non-functional requirements.\nFunctional. Align with the specific business needs of the project.\nNon-Functional. Prioritize security, availability, performance, scalability to create a robust and user-friendly data platform.\nAs a Customer Champion, youll collaborate with domain architects to identify opportunities for data platform improvements.\nData Security. Enhance data security posture by minimizing risks and vulnerabilities.\nFault Tolerance. Design data solutions to be highly available and resilient to failures.\nScalability. Ensure solutions can accommodate future growth.\nQuery Performance. Optimize data structures and access patterns for faster queries, ultimately improving the customer experience.\nDrive Improvement Implementation. Not only identify improvement opportunities but also actively work towards their implementation.\nWhat do you need to bring-\n10+ years of experience as a Data Architect or a similar role.\nProven track record of leading successful data modernization projects.\nDeep understanding of data modeling techniques.\nExpertise in domain-driven design principles and their application to data modeling.\nProficiency in Kappa and Lambda architectures, including their benefits, challenges, and best practices.\nStrong experience with stream processing frameworks (e.g., Apache Flink, Apache Spark Streaming).\nKnowledge of data lake and data warehouse concepts.\nExperience with cloud platforms (e.g., AWS, Azure, GCP) and their data services.\nCertifications related to data architecture, cloud platforms, or data analytics.\nExcellent communication and collaboration skills.",FinTech,"['Kappa and Lambda Architectures', 'Domain-Driven Design', 'CQRS', 'Data Modeling']",2025-06-12 15:31:47
Data Architect,Achnet,8-13 Years,,Bengaluru,"A Data Architect is a professional who is responsible for designing, building, and maintaining an organization's data architecture\nDesigning and implementing data models, data integration solutions, and data management systems that ensure data accuracy, consistency, and security\nDeveloping and maintaining data dictionaries, metadata, and data lineage documents to ensure data governance and compliance\nData Architect should have a strong technical background in data architecture and management, as well as excellent communication skills\nStrong problem-solving skills and the ability to think critically are also essential to identify and implement solutions to complex data issues",Career Planning,"['Business intelligence', 'Python', 'Data Management', 'Oracle', 'Data Warehousing', 'Data Architecture', 'SQL Server', 'Sql', 'PL/SQL', 'Data Quality', 'Tableau', 'Data Modeling']",2025-06-12 15:31:49
Data Architect,Achnet,8-15 Years,,Bengaluru,"A Data Architect is a professional who is responsible for designing, building, and maintaining an organization's data architecture\nDesigning and implementing data models, data integration solutions, and data management systems that ensure data accuracy, consistency, and security\nDeveloping and maintaining data dictionaries, metadata, and data lineage documents to ensure data governance and compliance\nData Architect should have a strong technical background in data architecture and management, as well as excellent communication skills\nStrong problem-solving skills and the ability to think critically are also essential to identify and implement solutions to complex data issues",Career Planning,"['Business intelligence', 'Python', 'Data Management', 'Oracle', 'Data Warehousing', 'Data Architecture', 'SQL Server', 'Sql', 'PL/SQL', 'Data Quality', 'Tableau', 'Data Modeling']",2025-06-12 15:31:50
Data Lake/Big Data Architect,Infosys Limited,10-12 Years,,"Delhi, India",Job Description:\nThe resource should mandatorily have minimum 10 Years of experience in solution planning and system architecture designing with at least 4 years of experience as a Lead Architect for DW BI or Big Data Systems\nThe resource should have demonstrated extensive experience in the use of various techniques to develop robust data warehouse business intelligence solutions which have significant weightage to statistical and advanced analytics\nKey Responsibilities:\nROLE AND RESPONSIBILITIES\nAs a Data Lake Big Data Architect lead the engagement efforts at different stages from problem definition to diagnosis to solution design development deployment in large government implementation programs\nCreate detailed design and architecture and process artifacts implement the solution and the deployment plan\nConnect with senior client business and IT stakeholders demonstrating thought leadership in domain process and technology\nREQUIRED SKILLS AND EXPERIENCE\nDomain process functional technical\nStrong hands on and in depth knowledge in Data Lakes Big Data modules\nStrong understanding of Data modelling concepts\nStrong understanding of the Data Warehousing Business Intelligence AI solutions good understanding of airlines industry\nThorough understanding of Agile methodologies\nGood understanding of business processes in the airlines domain or with government organizations\nExperience in leading and driving Business process workshops and Fit GAP analysis\nShould have working experience in a highly regulated environment\nShould be aware of release governance processes and have experience in working on any incident management tool\nPreferred Skills:\nFoundational->Development process generic->SaaS Development Process,IT/Computers - Software,"['Data Lakes', 'Business Intelligence', 'Data modelling concepts', 'Big Data modules', 'AI solutions', 'Data Warehousing', 'Agile Methodologies']",2025-06-12 15:31:52
Senior Data Architect,Celonis,3-5 Years,,Bengaluru,"The qualifications you need:\nYou have that rare combination a strong technical expertise and business acumen. You ll use this to build a system-agnostic data model for various business processes.\n3-5+ years of experience working in the data field as a Data Engineer, Data Analyst or similar.\nMust-have:\nExperience working with data from at least one of the following system types:\nStrong solution designing skills with solid understanding of business processes (supply chain, financial, CRM or IT-related processes) and data beneath the IT systems that run these processes.\nExperience with databases and data modeling, and hands-on experience with SQL.\nAbility to work independently and own a part of the team s goals\nVery good knowledge of spoken and written English\nAbility to communicate effectively and build a good rapport with team members",Software,"['Business Acumen', 'Supply Chain', 'Financial', 'CRM', 'Sql']",2025-06-12 15:31:53
Data Architect,Citiustech Healthcare Technology Private Limited,5-10 Years,,"Mumbai City, Bengaluru, Mumbai","We are looking for a data architect with below skillset\nHands on skills withImage data (preferably DICOM) parsing\nArchitect and implement data warehousing solutions using AWS (Redshift, S3, Lambda)\nDevelop and maintain ETL pipelines using Informatica PowerCenter or AWS Glue\nCollaborate with cross-functional teams to identify and prioritize data requirements\nAnalyze complex data sets using SQL, Python, and Java and or/Apex languageto inform business decisions\nImplement data visualization tools (Tableau, Power BI) for stakeholder reporting\nEnsure data integrity, security, and compliance with HIPAA, GDPR, and CCPA\nRequirements:> 5 years of experience in data management with focus in image metadata/bigdata\nStrong expertise in:\nPACS& RIS, SYNAPSE/XNAT, systems\nAWS (Redshift, S3, Lambda, Glue)\nData governance, quality, and security\nETL pipelines (Informatica PowerCenter or AWS Glue)\nData warehousing and visualization",Health Care,"['Data Warehousing', 'Tableau', 'Python', 'AWS']",2025-06-12 15:32:03
Celonis Data Architect,Harman Connected Services Corporation India Private Limited,12-15 Years,INR 2 - 3 LPA,Bengaluru,"A Celonis data architect is responsible for designing and implementing the data architecture required for effective utilization of the Celonis process mining tool within an organization. They work closely with various stakeholders, including data engineers, business analysts, and process owners, to define data requirements, ensure data integrity, and enable seamless data integration.\nThe role requires a strong understanding of data architecture principles, data modeling techniques, and relational and non-relational databases. Candidate should also be proficient in ETL processes and tools, as well as have experience in data integration and data governance. Additionally, candidate should have a good understanding of business processes, data visualization, and analytics to effectively implement process mining initiatives within the customer organization\nRole and Responsibilities\nOver all 12-15 years of IT experience with minimum 5+ years experience in Process Mining using Celonis\nStrong experience in Data Integration, Data Modeling, Data quality, Data pipeline management, Dashboard design\nExperience in delivering at least 1 to 2 large scale end-to-end Celonis process mining implementations as Data Architect\nExperience in leading team of data modelers, data engineers\nStrong experience in providing multiple solutions and reviewing the implementations in parallel\nExpertise in defining the governance, security, roles around Data pools and dashboards in Celonis\nExperience in implementing object centric process mining\nMajor accountabilities:\nCollaborating with business stakeholders to understand their data requirements and process mining goals.\nEngage with customers C-level their strategic objectives with the Celonis technical strategy\nDesigning and implementing data models, schemas, and structures that align with the organization's data governance policies and standards.\nIdentifying and documenting data sources, including databases, systems, and applications, that need to be integrated with Celonis.\nEnsuring proper data extraction, transformation, and loading (ETL) processes to populate the necessary data into Celonis.\nSupervising data engineers in the development and maintenance of data pipelines and workflows.\nAssessing data quality, consistency, and accuracy and taking proactive measures to resolve issues. Implementing data security measures, including access controls and data encryption, to protect sensitive information within Celonis.\nProviding technical support and guidance to business analysts and end-users in data preparation, analysis, and reporting using Celonis.\nStaying updated with the latest trends and advancements in data management and process mining technologies, including Celonis updates and new features.",Software,"['Celonis Data Architect', 'Etl']",2025-06-12 15:32:07
Celonis Data Architect,Harman Connected Services Corporation India Private Limited,12-15 Years,INR 2 - 3 LPA,Bengaluru,"A Celonis data architect is responsible for designing and implementing the data architecture required for effective utilization of the Celonis process mining tool within an organization. They work closely with various stakeholders, including data engineers, business analysts, and process owners, to define data requirements, ensure data integrity, and enable seamless data integration.\nThe role requires a strong understanding of data architecture principles, data modeling techniques, and relational and non-relational databases. Candidate should also be proficient in ETL processes and tools, as well as have experience in data integration and data governance. Additionally, candidate should have a good understanding of business processes, data visualization, and analytics to effectively implement process mining initiatives within the customer organization\nRole and Responsibilities\nOver all 12-15 years of IT experience with minimum 5+ years experience in Process Mining using Celonis\nStrong experience in Data Integration, Data Modeling, Data quality, Data pipeline management, Dashboard design\nExperience in delivering at least 1 to 2 large scale end-to-end Celonis process mining implementations as Data Architect\nExperience in leading team of data modelers, data engineers\nStrong experience in providing multiple solutions and reviewing the implementations in parallel\nExpertise in defining the governance, security, roles around Data pools and dashboards in Celonis\nExperience in implementing object centric process mining\nMajor accountabilities:\nCollaborating with business stakeholders to understand their data requirements and process mining goals.\nEngage with customers C-level their strategic objectives with the Celonis technical strategy\nDesigning and implementing data models, schemas, and structures that align with the organization's data governance policies and standards.\nIdentifying and documenting data sources, including databases, systems, and applications, that need to be integrated with Celonis.\nEnsuring proper data extraction, transformation, and loading (ETL) processes to populate the necessary data into Celonis.\nSupervising data engineers in the development and maintenance of data pipelines and workflows.\nAssessing data quality, consistency, and accuracy and taking proactive measures to resolve issues. Implementing data security measures, including access controls and data encryption, to protect sensitive information within Celonis.\nProviding technical support and guidance to business analysts and end-users in data preparation, analysis, and reporting using Celonis.\nStaying updated with the latest trends and advancements in data management and process mining technologies, including Celonis updates and new features.",Software,"['Celonis Data Architect', 'Etl']",2025-06-12 15:32:09
Data Modeler Architect,Virtusa,10-12 Years,,Hyderabad,"Design and develop conceptual, logical, and physical data models for enterprise and application-level databases.\nTranslate business requirements into well-structured data models that support analytics, reporting, and operational systems.\nDefine and maintain data standards, naming conventions, and metadata for consistency across systems.\nCollaborate with data architects, engineers, and analysts to implement models into databases and data warehouses/lakes.\nAnalyze existing data systems and provide recommendations for optimization, refactoring, and improvements.\nCreate entity relationship diagrams (ERDs) and data flow diagrams to document data structures and relationships.\nSupport data governance initiatives including data lineage, quality, and cataloging.\nReview and validate data models with business and technical stakeholders.\nProvide guidance on normalization, denormalization, and performance tuning of database designs.\nEnsure models comply with organizational data policies, security, and regulatory requirements.",Information Technology,"['Data Modelling for Analytical (OLAP)', 'ERDs', 'Sql', 'Data Integration', 'Redshift']",2025-06-12 15:32:11
Data Engineering Architect (ATC),Virtusa,8-12 Years,,Bengaluru,"Experience:\n8+ years of experience in data engineering, specifically in cloud environments like AWS.\nProficiency in PySpark for distributed data processing and transformation.\nSolid experience with AWS Glue for ETL jobs and managing data workflows.\nHands-on experience with AWS Data Pipeline (DPL) for workflow orchestration.\nStrong experience with AWS services such as S3, Lambda, Redshift, RDS, and EC2.\nTechnical Skills:\nProficiency in Python and PySpark for data processing and transformation tasks.\nDeep understanding of ETL concepts and best practices.\nFamiliarity with AWS Glue (ETL jobs, Data Catalog, and Crawlers).\nExperience building and maintaining data pipelines with AWS Data Pipeline or similar orchestration tools.\nFamiliarity with AWS S3 for data storage and management, including file formats (CSV, Parquet, Avro).\nStrong knowledge of SQL for querying and manipulating relational and semi-structured data.\nExperience with Data Warehousing and Big Data technologies, specifically within AWS.\nAdditional Skills:\nExperience with AWS Lambda for serverless data processing and orchestration.\nUnderstanding of AWS Redshift for data warehousing and analytics.\nFamiliarity with Data Lakes, Amazon EMR, and Kinesis for streaming data processing.\nKnowledge of data governance practices, including data lineage and auditing.\nFamiliarity with CI/CD pipelines and Git for version control.\nExperience with Docker and containerization for building and deploying applications.\nDesign and Build Data Pipelines: Design, implement, and optimize data pipelines on AWS using PySpark, AWS Glue, and AWS Data Pipeline to automate data integration, transformation, and storage processes.\nETL Development: Develop and maintain Extract, Transform, and Load (ETL) processes using AWS Glue and PySpark to efficiently process large datasets.\nData Workflow Automation: Build and manage automated data workflows using AWS Data Pipeline, ensuring seamless scheduling, monitoring, and management of data jobs.\nData Integration: Work with different AWS data storage services (e.g., S3, Redshift, RDS) to ensure smooth integration and movement of data across platforms.\nOptimization and Scaling: Optimize and scale data pipelines for high performance and cost efficiency, utilizing AWS services like Lambda, S3, and EC2.",Information Technology,"['Cloud Platforms', 'Data Architecture', 'Etl', 'Big Data']",2025-06-12 15:32:12
Data Warehouse Architect- Technology Group,Icici Bank Limited,8-12 Years,,Hyderabad,"Essential Services: Role & Location fungibility\nThe role descriptions give you an overview of the responsibilities; it is only directional and guiding in nature. At ICICI Bank, we believe in serving our customers beyond our role definition, product boundaries, and domain limitations through our philosophy of customer 360-degree. In essence, this captures our belief in serving the entire banking needs of our customers as One Bank, One Team. To achieve this, employees at ICICI Bank are expected to be role and location-fungible with the understanding that Banking is an essential service.\nAbout the Role:\nAs a Data Warehouse Architect, you will be responsible for managing and enhancing data warehouse that manages large volume of customer-life cycle data flowing in from various applications within guardrails of risk and compliance.You will be managing the day-to-day operations of data warehouse i.e. Vertica. In this role responsibility, you will manage a team of data warehouse engineers to develop data modelling, designing ETL data pipeline, issue management, upgrades, performance fine-tuning, migration, governance and security framework of the data warehouse. This role enables the Bank to maintain huge data sets in a structured manner that is amenable for data intelligence. The data warehouse supports numerous information systems used by various business groups to derive insights.\nAs a natural progression, the data warehouses will be gradually migrated to Data Lake enabling better analytical advantage. The role holder will also be responsible for guiding the team towards this migration.\nKey Responsibilities:\nData Pipeline Design:Responsible for designing and developing ETL data pipelines that can help in organising large volumes of data. Use of data warehousing technologies to ensure that the data warehouse is efficient, scalable, and secure.\nIssue Management: Responsible for ensuring that the data warehouse is running smoothly. Monitor system performance, diagnose and troubleshoot issues,and make necessary changes to optimize system performance.\nCollaboration:Collaborate with cross-functional teams to implement upgrades, migrations and continuous improvements.\nData Integration and Processing: Responsible for processing, cleaning, and integrating large data sets from various sources to ensure that the data is accurate, complete, and consistent.\nData Modelling: Responsible for designing and implementing data modelling solutions to ensure that the organization's data is properly structured and organized for analysis.\nKey Qualifications & Skills:\nEducation Qualification:B.E./B. Tech. in Computer Science, Information Technology or equivalent domain with 10 to 12 years of experience and at least 5 years or relevant work experience in Datawarehouse/mining/BI/MIS.\nExperience in Data Warehousing:Knowledge on ETL and data technologies and outline future vision in OLTP, OLAP (Oracle / MSSQL). Data Modelling, Data Analysis and Visualization experience (Analytical tools experience like Power BI / SAS / ClickView / Tableu etc). Good to have exposure to Azure Cloud Data platform services like COSMOS, Azure Data Lake, Azure Synapse, and Azure Data factory.\nSynergize with the Team:Regular interaction with business/product/functional teams to create mobility solutions.\nCertification:Azure certified DP 900, PL 300, DP 203 or any other Data platform/Data Analyst certifications.",Banking,"['Data Warehousing', 'Etl']",2025-06-12 15:32:14
Data Integration Architect,Alstom,10-12 Years,,Bengaluru,"Hands-on-experience architecting and delivering solutions related to enterprise integration, APIs, service-oriented architecture, and technology modernizations\n3-4 years hands-on experience with the design, and implementation of integrations in the area of Dell Boomi\nUnderstanding the Business requirements and Functional requirement Documents and Design a Technical Solution as per the needs\nPerson should be good with Master Data Management, Migration and Governance best practices\nExtensive data quality and data migration experience including proficiency in data warehousing, data analysis and conversion planning for data migration activities\nLead and build data migration objects as needed for conversions of data from different sources\nShould have architected integration solutions using Dell Boomi for cloud, hybrid and on-premise integration landscapes\nAbility to build and architect a high performing, highly available, highly scale Boomi Molecule Infrastructure\nIn depth understanding of enterprise integration patterns and prowess to apply them in the customers IT landscape\nAssists project teams during system design to promote the efficient re-use of IT assets Advises project team during system development to assure compliance with architectural principles, guidelines and standards\nAdept in building the Boomi processes with Error handling and email alerts logging best practices\nShould be proficient in using Enterprise level and Database connectors\nExtensive data quality and data migration experience including proficiency in data warehousing, data analysis and conversion planning for data migration activities\nExcellent understanding on REST with in-depth understanding on how Boomi processes can expose consume services using the different http methods, URI and Media type\nUnderstand Atom, Molecule, Atmosphere Configuration and Management, Platform Monitoring, Performance Optimization Suggestions, Platform Extension, User Permissions Control Skills.\nKnowledge on API governance and skills like caching, DB management and data warehousing\nShould have hands on experience in configuring AS2, https, SFTP involving different authentication methods\nThorough knowledge on process deployment, applying extensions, setting up schedules, Web Services user management process filtering and process reporting\nShould be expert with XML and JSON activities like creation, mapping and migrations\nPerson should have worked on integration on SAP, SuccessFactors, Sharepoint, cloud-based apps, Web applications and engineering application\nSupport and resolve issues related to data integration deliveries or platform\nProject Management\nPerson should deliver Data Integration projects using data integration platform\nManage partner deliveries by setting up governance of their deliveries\nUnderstand project priorities, timelines, budget, and deliverables and the need to proactively push yourself and others to achieve project goals\nManagerial:\nPerson is individual contributor and operationally managing small technical team\nQualifications & Skills:\n10+ years of experience in the area of enterprise integrations\nMinimum 3-4 years of experience with Dell boomi\nShould have working experience with database like sql server, Data warehousing\nHands on experience on REST, SOAP, XML, JSON, SFTP, EDI\nShould have worked on integration of multiple technologies like SAP, Web, cloud based apps.\nTECHNICAL COMPETENCIES & EXPERIENCE\nTechnical expertise in Delll Boomi for data integration is MUST.",Transportation,"['REST/SOAP/XML/JSON', 'Project management', 'Dell Boomi', 'Data Migration', 'Api Integration', 'Data Warehousing']",2025-06-12 15:32:15
Data Base Architect,Aspire Systems India Private Limited,7-11 Years,,"Hyderabad, Bengaluru, Cochin / Kochi / Ernakulam","Database Design and Architecture:Lead the design, implementation, and optimization of scalable and efficient database architectures to support business requirements.\nData Modeling and Management:Develop data models, database schemas, and data pipelines for efficient storage, retrieval, and management of data across platforms.\nAWS Database Services:Manage AWS databases, including Aurora (PostgreSQL/MySQL) and DynamoDB, optimizing for cost, performance, and scalability.\nPython Scripting:Use Python for data manipulation, automation of database tasks, and integration with other systems.\nDatabase Performance Optimization:Monitor and optimize database performance, troubleshoot issues, and ensure high availability and disaster recovery.\nDocumentation and Reporting:Maintain up-to-date documentation for database architectures, processes, and configurations. Generate reports on database performance, issues, and improvements.\nPrimary Skills (Essential):\nAt least five years of professional programming experience writing performant stored procedures/functions/SQL statements\nDatabase Management:Expertise in database administration, tuning, and troubleshooting.\nSQL Expertise:Strong SQL skills across various database systems.\nAWS Databases:Proficiency with Amazon Aurora (MySQL/PostgreSQL) and DynamoDB, including setup, optimization, and scaling.\nPython:Strong Python skills for data manipulation, automation, and database task management.\nData Modeling:Proficiency in designing logical and physical data models to support business processes.\nSecondary Skills (Highly Beneficial):\nSnowflake:Hands-on experience with Snowflake data warehousing, data modeling, and data ingestion processes.\nETL/ELT Tools:Familiarity with ETL/ELT processes and tools (e.g., Apache Airflow, AWS Glue) for data transformation and ingestion.\nData Governance and Compliance:Knowledge of data governance practices, including data security and regulatory compliance standards.",Software,"['Database Architect', 'MySQL', 'PostgreSQL', 'AWS']",2025-06-12 15:32:16
Data Engineering Architect (ATC),Virtusa,10-13 Years,,Hyderabad,"Proficiency in PySpark for distributed data processing and transformation.\nSolid experience with AWS Glue for ETL jobs and managing data workflows.\nHands-on experience with AWS Data Pipeline (DPL) for workflow orchestration.\nStrong experience with AWS services such as S3, Lambda, Redshift, RDS, and EC2.\nTechnical Skills:\nProficiency in Python and PySpark for data processing and transformation tasks.\nDeep understanding of ETL concepts and best practices.\nFamiliarity with AWS Glue (ETL jobs, Data Catalog, and Crawlers).\nExperience building and maintaining data pipelines with AWS Data Pipeline or similar orchestration tools.\nFamiliarity with AWS S3 for data storage and management, including file formats (CSV, Parquet, Avro).\nStrong knowledge of SQL for querying and manipulating relational and semi-structured data.\nExperience with Data Warehousing and Big Data technologies, specifically within AWS.\nAdditional Skills:\nExperience with AWS Lambda for serverless data processing and orchestration.\nUnderstanding of AWS Redshift for data warehousing and analytics.\nFamiliarity with Data Lakes, Amazon EMR, and Kinesis for streaming data processing.\nKnowledge of data governance practices, including data lineage and auditing.\nFamiliarity with CI/CD pipelines and Git for version control.\nExperience with Docker and containerization for building and deploying applications.\nDesign and Build Data Pipelines: Design, implement, and optimize data pipelines on AWS using PySpark, AWS Glue, and AWS Data Pipeline to automate data integration, transformation, and storage processes.\nETL Development: Develop and maintain Extract, Transform, and Load (ETL) processes using AWS Glue and PySpark to efficiently process large datasets.\nData Workflow Automation: Build and manage automated data workflows using AWS Data Pipeline, ensuring seamless scheduling, monitoring, and management of data jobs.\nData Integration: Work with different AWS data storage services (e.g., S3, Redshift, RDS) to ensure smooth integration and movement of data across platforms.\nOptimization and Scaling: Optimize and scale data pipelines for high performance and cost efficiency, utilizing AWS services like Lambda, S3, and EC2.",IT Management,"['Analytics - Kinesis', 'S3', 'Aws Lambda', 'Platform', 'Pyspark', 'Apache Kafka', 'Redshift', 'Python']",2025-06-12 15:32:18
Staff Data Engineer/Architect,Synopsys (India) Private Limited,5-10 Years,,Hyderabad,"What You'll Be Doing:\nBuilding the data engineering and analytics infrastructure for our new Enterprise Data Platform using Snowflake and Fivetran.\nLeading the development of data models, algorithms, data pipelines, and insights to enable data-driven decision-making.\nCollaborating with team members to shape the design and direction of the data platform.\nWorking end-to-end on data products, from problem understanding to developing data pipelines, dimensional data models, and visualizations.\nProviding support and advice to business users, including data preparation for predictive and prescriptive modeling.\nEnsuring consistency of processes and championing best practices in data management.\nEvaluating and recommending new data tools or processes.\nDesigning, developing, and deploying scalable AI/Generative AI and machine learning models as needed.\nProviding day-to-day production support to internal business unit customers, implementing enhancements and resolving defects.\nMaintaining awareness of emerging trends in AI, Generative AI, and machine learning to enhance existing systems and develop innovative solutions.\n\nThe Impact You Will Have:\nDriving the development of a cutting-edge data platform that supports enterprise-wide data initiatives.\nEnabling data-driven decision-making across the organization through robust data models and insights.\nEnhancing the efficiency and effectiveness of data management processes.\nSupporting business users in leveraging data for predictive and prescriptive analytics.\nInnovating and integrating advanced AI and machine learning solutions to solve complex business challenges.\nContributing to the overall success of Synopsys by ensuring high-quality data infrastructure and analytics capabilities.\n\nWhat You'll Need:\nBS with 5+ years of relevant experience or MS with 3+ years of relevant experience in Computer Sciences, Mathematics, Engineering, or MIS.\n5 years of experience in DW/BI development, reporting, and analytics roles, working with business and key stakeholders.\nAdvanced knowledge of Data Warehousing, SQL, ETL/ELT, dimensional modeling, and databases (e.g., mySQL, Postgres, HANA).\nHands-on experience with modern data stack tools, including cloud data warehouses (Snowflake), transformation tools (dbt), and cloud providers (Azure, AWS).\nExperience with data ingestion tools (e.g., Fivetran, HVR, Airbyte), CI/CD (GitLab, Kubernetes, Airflow), and data catalog tools (e.g., Datahub, Atlan) is a plus.\nProficiency in scripting languages like Python, Unix, SQL, Scala, and Java for data extraction and exploration.\nExperience with visualization tools like Tableau and PowerBI is a plus.\nKnowledge of machine learning frameworks and libraries (e.g., Pandas, NumPy, TensorFlow, PyTorch) and LLM models is a plus.\nUnderstanding of data governance, data integrity, and data quality best practices.\nExperience with agile development methodologies and change control processes.",Information Technology,"['Cloud Platforms', 'Data Modeling', 'Etl', 'Big Data']",2025-06-12 15:32:19
Data Solution Architect,BCForward,18-22 Years,,Bengaluru,"Data Solution Architect\nBachelor's degree with a preferred area of study in information technology, computer science, computer engineering or related fields\n8+ years of overall experience in big data, database and enterprise data architecture and delivery\n8+ years of programming proficiency in a subset of Python, Java, and Scala\n5+ years of hands-on experience building solutions on distributed processing frameworks such as Spark, Hadoop or Databricks\n5+ years of experience architecting, developing, releasing, and maintaining enterprise data lake platforms\n3+ years of experience implementing cloud-based systems. AWS and/or Databricks preferred\nNote: Looking for Immediate to 30-Days joiners at most.",IT Management,"['Databricks', 'AWS', 'Hadoop', 'Spark', 'Python']",2025-06-12 15:32:20
Solution Architect-Data Platforms,IBM,4-8 Years,,Bengaluru,"Experience in data architecture and engineering Proven expertise with Snowflake data platform Strong understanding of ETL/ELT processes and data integration\nExperience with data modeling and data warehousing concepts\nFamiliarity with performance tuning and optimization techniques\nExcellent problem-solving skills and attention to detail\nStrong communication and collaboration skills\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nCloud & Data Architecture: AWS ,Snowflake\nETL & Data Engineering: AWS Glue, Apache Spark, Step Functions\nBig Data & Analytics: Athena,Presto, Hadoop\nDatabase & Storage: SQL,Snow sql\nSecurity & Compliance: IAM, KMS, Data Masking\nPreferred technical and professional experience\nCloud Data Warehousing: Snowflake (Data Modeling, Query Optimization)\nData Transformation: DBT (Data Build Tool) for ELT pipeline management\nMetadata & Data Governance: Alation (Data Catalog, Lineage, Governance)",Information Technology,"['snowflake', 'dbt', 'AWS Glue', 'Data Modeling', 'Apache Spark', 'Sql']",2025-06-12 15:32:22
Solution Architect-Data Platforms,IBM,0-5 Years,,Navi Mumbai,"Role Overview\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour Role and Responsibilities\nData Strategy and Planning: Develop and implement data architecture strategies that align with organizational goals and objectives. Collaborate with business stakeholders to understand data requirements and translate them into actionable plans.\nData Modeling: Design and implement logical and physical data models to support business needs. Ensure data models are scalable, efficient, and comply with industry best practices.\nDatabase Design and Management: Oversee the design and management of databases, selecting appropriate database technologies based on requirements. Optimize database performance and ensure data integrity and security.\nData Integration: Define and implement data integration strategies to facilitate seamless flow of information across systems.\nResponsibilities:\nExperience in data architecture and engineering.\nProven expertise with Snowflake data platform.\nStrong understanding of ETL/ELT processes and data integration.\nExperience with data modeling and data warehousing concepts.\nFamiliarity with performance tuning and optimization techniques.\nExcellent problem-solving skills and attention to detail.\nStrong communication and collaboration skills.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nCloud & Data Architecture: AWS, Snowflake\nETL & Data Engineering: AWS Glue, Apache Spark, Step Functions\nBig Data & Analytics: Athena, Presto, Hadoop\nDatabase & Storage: SQL, Snow SQL\nSecurity & Compliance: IAM, KMS, Data Masking\nPreferred Technical and Professional Experience\nCloud Data Warehousing: Snowflake (Data Modeling, Query Optimization)\nData Transformation: DBT (Data Build Tool) for ELT pipeline management\nMetadata & Data Governance: Alation (Data Catalog, Lineage, Governance)",Software,"['snowflake', 'Athena', 'dbt', 'AWS', 'Spark', 'Sql', 'Etl', 'Data Modeling']",2025-06-12 15:32:23
"Assistant Vice President - Pre Sales solution architect - Data Engineering, cloud, AI/ML",Genpact,Fresher,,Bengaluru,"Ready to shape the future of work\n\n\n\nAt Genpact, we don&rsquot just adapt to change&mdashwe drive it. AI and digital innovation are redefining industries, and we&rsquore leading the charge. Genpact&rsquos , our industry-first accelerator, is an example of how we&rsquore scaling advanced technology solutions to help global enterprises work smarter, grow faster, and transform at scale. From large-scale models to , our breakthrough solutions tackle companies most complex challenges.\n\n\n\nIf you thrive in a fast-moving, tech-driven environment, love solving real-world problems, and want to be part of a team that&rsquos shaping the future, this is your moment.\n\n\n\nGenpact (NYSE: G) is anadvanced technology services and solutions company that deliverslastingvalue for leading enterprisesglobally.Through ourdeep business knowledge, operational excellence, and cutting-edge solutions - we help companies across industries get ahead and stay ahead.Powered by curiosity, courage, and innovation,our teamsimplementdata, technology, and AItocreate tomorrow, today.Get to know us atand on,,, and.\n\n\n\n\n\n\n\n\n\nInviting applications for the role of Assistant Vice President - Presales Solution Architect\n\n\n\nIn this role, you'll be part of Genpact's transformation under GenpactNext, as we lead the shift to Agentic AI Solutions&mdashdomain-specific, autonomous systems that redefine how we deliver value to clients. You'll help drive the adoption of innovations like the Genpact AP Suite in finance and accounting, with more Agentic AI products set to expand across service lines.\n\n\n\nIn this role, you will be responsible for being familiar with our products and offerings, understanding customer needs and requirements, creating a compelling story to solve the identified business challenges, running demos, and answering technical questions. You will play a crucial role in addressing technical challenges faced by our customers and proposing effective solutions. Your ability to analyze customer requirements and translate them into tailored solutions will be key to your success in this position.\n\n\n\n\n\n\nResponsibilities\n\n\n\n\n\nBe familiar with our products and offerings to effectively understand customer needs and pain points, align them with the most appropriate products we have available, and architect end-to-end software solutions to address the client&rsquos requirements, taking into account the existing client technology landscape.\n\n\n\n\n\n\n\nCreate a compelling story that effectively solves the identified issues faced by customers, using your technical knowledge together with our products and offerings knowledge.\n\n\n\n\n\n\n\nDeliver product demonstrations that showcase the functionality and value of our solutions, tailoring them to address specific customer pain points.\n\n\n\n\n\n\n\nCollaborate with cross-functional teams, including sales, product development, and customer success, to ensure seamless communication and deliver actionable solution strategies.\n\n\n\n\n\n\n\nSupport our effort to respond to clients' Request for Proposals (RFPs) and Request for Information (RFIs) by leveraging your technical expertise and understanding of our products and offerings. Prepare comprehensive and compelling responses that address the specific requirements outlined in the RFPs/RFIs, showcasing how our solutions can meet the client's needs.\n\n\n\n\n\n\n\nUtilize your technical expertise and understanding of our products and solutions to estimate the effort required for implementing and delivering customized solutions to clients.\n\n\n\n\n\n\n\nActively participate in the sales cycle, helping to close deals by demonstrating the value of the proposed solutions.\n\n\n\n\n\n\n\nFoster a culture of continuous innovation and experimentation to drive differentiation in the market.\n\n\n\n\n\n\n\nStay updated on industry trends and advancements to continuously enhance your technical knowledge and expertise.\n\n\n\n\n\n\n\n\n\n\nQualifications we seek in you!\n\n\n\nMinimum Qualifications\n\n\n\n\n\nShould have experience in complex data engineering, data management, Analytics and cloud migration/modernization initiatives.\n\n\n\n\n\n\n\nRelevant years in IT services with strong background in solutioning lead role.\n\n\n\n\n\n\n\nProven track record of creating and delivering new assets/accelerators products, or solutions that have contributed to revenue growth.\n\n\n\n\n\n\n\nStrong problem-solving skills and the ability to manage ambiguity.\n\n\n\n\n\n\n\nStrong understanding of software development lifecycle methodologies, solution architecture, and design principles.\n\n\n\n\n\n\n\nExcellent communication, presentation, and storytelling skills, with the ability to effectively convey complex technical concepts to both technical and non-technical stakeholders.\n\n\n\n\n\n\n\nAbility to work collaboratively in cross-functional teams, including sales, product management, and development teams.\n\n\n\n\n\n\n\nStrong problem-solving skills and the ability to think strategically to identify client needs and propose innovative solutions.\n\n\n\n\n\n\n\n\n\n\n\nPreferred Qualifications/ Skills\n\n\n\n\n\nExperience with AI/ML, Gen AI/Agentic AI solutions\n\n\n\n\n\n\n\nExperience in any one of cloud technologies (AWS/Azure/GCP)\n\n\n\n\n\n\n\n\n\n\n\nWhy join Genpact\n\n\n\n\n\nBe a transformation leader - Work at the cutting edge of AI, automation, and digital innovation\n\n\n\n\n\n\n\n\n\nMake an impact - Drive change for global enterprises and solve business challenges that matter\n\n\n\n\n\n\n\nAccelerate your career - Get hands-on experience, mentorship, and continuous learning opportunities\n\n\n\n\n\n\n\nWork with the best - Join 140,000+ bold thinkers and problem-solvers who push boundaries every day\n\n\n\n\n\n\n\nThrive in a values-driven culture - Our courage, curiosity, and incisiveness - built on a foundation of integrity and inclusion - allow your ideas to fuel progress\n\n\n\n\n\nCome join the tech shapers and growth makers at Genpact and take your career in the only direction that matters: Up.\n\n\n\nLet&rsquos build tomorrow together.\n\n\n\n\n\n\nGenpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. Genpact is committed to creating a dynamic work environment that values respect and integrity, customer focus, and innovation.\nFurthermore, please do note that Genpact does not charge fees to process job applications and applicants are not required to pay to participate in our hiring process in any other way. Examples of such scams include purchasing a 'starter kit,' paying to apply, or purchasing equipment or training.",IT/Computers - Hardware & Networking,[],2025-06-12 15:32:25
Data Center Solution Architect,SCHNEIDER ELECTRIC SINGAPORE PTE LTD,5-10 Years,,Bengaluru,"Key Responsibilities\nSolution Design & Engineering:\nLiaise with the Schneider offer team, customer project teams, consultants, and internal teams to fully understand project requirements, scope, and needs.\nEnsure solution compliance with local standards, regulations, codes, and safety requirements.\nCoordinate with electrical and mechanical engineers to create all necessary documentation, including capacity and design calculations, drawings, layouts, specifications, block schematics, solution value propositions, and compliance tables.\nDevelop solution designs that guarantee successful project implementation and high customer satisfaction.\nProvide engineering support during installation, offering guidance on processes, required tools, procedures, regulatory compliance, technical interpretations of do's and don'ts, and ensuring overall quality.\nProject Execution Support:\nAssist with vendor qualification and provide initial costing estimates.\nParticipate in customer meetings to communicate project status and address any roadblocks.\nEscalate roadblocks to internal stakeholders and provide regular reports on proposal status to the Project Manager and Solution Manager.\nEnsure adherence to customer project processes, methodologies, and procedures during the design phase.\nControl the schedule during the design phase to align with the project master plan.\nCustomer Care & Quality:\nDeliver services aimed at achieving the highest customer satisfaction during project execution.\nEscalate any design issues to the Project Manager and take responsibility for their resolution.\nIdentify needed improvements, document them, and communicate to the respective owners and stakeholders.\nPartner Management:\nCoordinate design finalization with third-party service providers during project execution.\nMonitor and communicate the performance of partners.\nQualifications\nEducation: Bachelor's Degree or equivalent in Electrical Engineering or Mechanical Engineering.\nExperience: At least 5 years of experience in the electrical or mechanical engineering industry, with a minimum of 2 years in designing complete systems for critical facilities such as data centers.\nLanguage: Good command of English.\nBusiness Understanding:Strong understanding of electrical distribution system design.\nUnderstanding of electrical system components such as HV, MV, LV distribution, switchgear, panel design standards, grounding, generator systems, etc.\nUnderstanding of mechanical design, including cooling, piping, and fire alarm systems.\nKnowledge of best practices and standards in electrical systems design, installation, and operation requirements.\nUnderstanding of engineering system documentation.\nProven skill in project engineering.\nAbility to meet deadlines and demonstrate effective time management skills.\nExcellent business ethics and integrity.\nDemonstrated flexibility in operational style to meet the requirements of a multi-cultural position",Electronics,"['Project Requirement Analysis', 'Local Standards Compliance', 'Regulatory Compliance', 'code compliance', 'Solution Design']",2025-06-12 15:32:26
Data and Analytics - Architect,SCHNEIDER ELECTRIC SINGAPORE PTE LTD,8-12 Years,,Bengaluru,"Key Responsibilities\nData Modeling & Expansion:\nDesign new data models or expand existing ones based on business feedback.\nCollaborate with global and regional data integration teams to acquire expanded data from source systems.\nDevelop and implement databases, data collection systems, data analytics, and other strategies to optimize statistical efficiency and quality.\nAcquire data from primary or secondary data sources and maintain databases.\nAnalyze and improve performance on existing data models, including query optimization, DAX optimization, and processing optimization (partitions).\nBuild and enhance SQL Server Analysis Services data models.\nData Quality & Governance:\nWork with the larger Growth Technology Systems team to develop and uphold data governance policies and procedures.\nEnsure standardized data naming, consistent data definitions, and monitor overall data quality for assigned data entities.\nFilter and clean data by reviewing reports and performance indicators to locate and correct code problems.\nData Analysis & Reporting:\nDevelop and implement data analyses and other strategies that optimize statistical efficiency and quality.\nInterpret data and analyze results using statistical techniques.\nDevelop or assist in the development of SSRS, Excel, Tableau, and Power BI reports.\nDocumentation & Compliance:Develop required process documentation and adhere to security compliance.\nEssential Functions\nDW solutioning\nUnderstanding business requirements, sourcing data, and designing data models.\nAssessing Data Quality, Data Integration, Migration & Modeling.\nHandling end-to-end development.\nFrequent client interaction.\nQualifications\nPrimary Skills\nGood business analytical skills.\nDeep understanding of dimensional modeling, OLTP, and OLAP database designs and implementation strategies.\nData Warehousing concepts, architecture, and schemas.\nFamiliarity with ETL tools such as Microsoft SSIS/DTS, Data Stage, Informatica Power Center, Informatica Cloud.\nExperience in providing solutions for BI & DW environments.\nStrong knowledge of and experience with reporting packages (e.g., Business Objects), databases (e.g., SQL), and programming (XML, Javascript, or ETL frameworks).\nKnowledge of statistics and experience using statistical packages for analyzing datasets (e.g., Excel, SPSS, SAS).\nStrong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.\nGood communication skills, both written and verbal.\nExperience in the Manufacturing sector.\nFamiliarity with Agile methodologies such as Scrum.\nPreferred Knowledge\nExposure to data visualization/BI.\nAWS",Electronics,"['Data Modeling', 'Databases', 'Data Analytics', 'Sql Server Analysis Services', 'Tableau', 'Power Bi']",2025-06-12 15:32:36
Architect (ATC)- Data Integration,Virtusa,6-10 Years,,Bengaluru,"Collaborate with business stakeholders, data architects, and IT teams to gather and understand data migration requirements.\nAnalyze legacy banking and insurance systems (e.g., core banking, policy admin, claims, CRM) to identify data structures and dependencies.\nWork with large-scale datasets and understand big data architectures (e.g., Hadoop, Spark, Hive) to support scalable data migration and transformation.\nPerform data profiling, cleansing, and transformation using SQL and ETL tools, with the ability to understand and write complex SQL queries and interpret the logic implemented in ETL workflows.\nDevelop and maintain data mapping documents and transformation logic specific to financial and insurance data (e.g., customer KYC, transactions, policies, claims).\nValidate migrated data against business rules, regulatory standards, and reconciliation reports.\nSupport UAT by preparing test cases and validating migrated data with business users.\nEnsure data privacy and security compliance throughout the migration process.",Information Technology,"['CRM', 'Core Banking', 'Spark', 'Hadoop Basics', 'Sql']",2025-06-12 15:32:41
Architect (ATC)- Data Integration,Virtusa,6-10 Years,,Bengaluru,"Collaborate with business stakeholders, data architects, and IT teams to gather and understand data migration requirements.\nAnalyze legacy banking and insurance systems (e.g., core banking, policy admin, claims, CRM) to identify data structures and dependencies.\nWork with large-scale datasets and understand big data architectures (e.g., Hadoop, Spark, Hive) to support scalable data migration and transformation.\nPerform data profiling, cleansing, and transformation using SQL and ETL tools, with the ability to understand and write complex SQL queries and interpret the logic implemented in ETL workflows.\nDevelop and maintain data mapping documents and transformation logic specific to financial and insurance data (e.g., customer KYC, transactions, policies, claims).\nValidate migrated data against business rules, regulatory standards, and reconciliation reports.\nSupport UAT by preparing test cases and validating migrated data with business users.\nEnsure data privacy and security compliance throughout the migration process.",Information Technology,"['CRM', 'Core Banking', 'Spark', 'Hadoop Basics', 'Sql']",2025-06-12 15:32:43
Data Modeler Team Lead/Architect,Paulwin George (Proprietor of Angel And Genie),7-10 Years,,Gurugram,"Job Description: Data Modeler Architect\nPosition: Data Modeler Architect\nExperience: 8+ years\nLocation: Gurugram\nDepartment: IT/Data Architecture\nReports to: Data Architecture Lead / Data Manager\nPosition Overview:\nWe are looking for a skilled and motivated Data Modeler Architect with 8 years of experience to join our\nIT team. The Data Modeler will be responsible for designing, developing, and maintaining data models\nthat support business requirements and data-driven decision-making. You will work closely with business\nanalysts, data architects, and other stakeholders to ensure the organization's data is structured\neffectively and efficiently. This position offers the opportunity to influence how the company handles\ndata at a large scale and play a pivotal role in the company's data strategy.\nKey Responsibilities:\nData Modeling & Design:\no Design and develop conceptual, logical, and physical data models in line with the\ncompany's data strategy and business objectives.\no Collaborate with business stakeholders and IT teamsto gather requirements and ensure\nmodels meet business needs.\no Create and maintain metadata and data dictionaries to ensure consistency across various\ndata sets.\no Review and optimize data models to improve system performance,scalability, and\nquality.\nData Integration & Management:\no Assist in integrating and transforming data from multiple sources into usable formats.\no Work on data governance and standardization initiatives to ensure data consistency,\nintegrity, and security.\no Collaborate with ETL developers to ensure proper data flow and transformation within\nthe models.\nCollaboration & Support:\no Work with other teams(e.g., data engineers, data analysts) to ensure alignment and\nproper implementation of data models across the organization.\no Provide guidance and support for datamigration, quality assurance, and reporting efforts.\no Act as a liaison between business users and technical teams, translating business\nrequirements into data specifications.\nPerformance Monitoring & Reporting:\no Monitor data model performance and proactively suggest optimizations to improve\nefficiency.\no Assist in the creation of dashboards and reports for business users by providing insights\ninto data structures and relationships.\nSkills & Qualifications:\nEducation:\no Bachelor's degree in Computer Science, Information Technology, Data Science, or a\nrelated field. Master's degree preferred.\nExperience:\no 5 years of experience in data modeling, database design, or a related role in an IT\ncompany.\no Hands-on experience with data modeling tools(e.g., Erwin, IBM Infosphere Data\nArchitect, Microsoft SQL Server Management Studio, or similar tools).\no Extensive experience in Snowflake\no Strong experience with database managementsystems such as SQL Server, Oracle,\nMySQL, or PostgreSQL.\nTechnical Skills:\no Expertise in designing relational, dimensional, and NoSQL data models.\no Proficiency in SQL and understanding of database query optimization techniques.\no Familiarity with ETL processes and data warehousing concepts.\no Experience with data governance practices and tools.\nSoft Skills:\no Strong analytical and problem-solving skills.\no Good communication and collaboration skillsto work with technical and non-technical\nteams.\no Ability to work in a fast-paced environment, prioritize tasks, and manage multiple\nprojects simultaneously.\no Detail-oriented with a focus on data quality and consistency.\nMust Skills:\nExperience of cloud-based data platforms such as AWS, Google Cloud, or Azure. (Must)\nExperience with big data tools(e.g., Hadoop, Spark) and platforms.\nExperience in Snowflake\nExperience with data visualization tools(e.g., Power BI, Tableau) is a plus.\nWhy Join Us:\nWork with industry-leading data solutions and innovative technologies to help clients\ntransform their businesses.\nCompetitive salary and performance-based incentives.\nCollaborative work environment with opportunitiesto engage in exciting, data-centric projects.\nExposure to clients across various industries, giving you a broad range of experience and\ngrowth opportunities.\nTravel opportunities to meet with clients and explore new markets.\nOngoing training and development to ensure you stay ahead of industry trends in the data space.","Information Technology, Human Resources","['Mysql', 'Postgresql', 'Microsoft Sql']",2025-06-12 15:32:44
Architect - Enterprise Data Operations,PepsiCo,3-5 Years,,"Gurugram, India","Overview\n\nThe resource will be an empowered member of a team of platform engineers who build environments with consistency maintaining the standards on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the platform engineering team, he/she will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics.\n\nResponsibilities\n\nActive contributor to code development and support existing projects.\nManage data pipelines from internal and external data sources to support day-to-day operations.\nUnderstand and adapt existing Infrastructure of platform engineering in the organization.\nResponsible for adopting best practices around systems integration, security, performance, and data management defined within the organization.\nCollaborate with team and learn to build data pipelines.\nSupport platform engineering teams and quickly respond to failures.\nCollaborate with the team and learn about building a modern and scalable platform.\nCreate documentation for learning and knowledge transfer.\nLearn and adapt automation skills/techniques in day-to-day activities.\nLearn well-defined coding/development life cycles within the organization\nKnowledge of data modeling, data warehousing, and ETL/ELT pipelines.\nKnowledge of data profiling and data quality tools is a plus.\nKnowledge in Statistical/Machine learning models is a plus.\nKnowledge in the retail or supply chain space is a plus\nFamiliarity with business intelligence tools (such as PowerBI) is a plus.\n\nQualifications\n\nBE/BTech in Computer Science, Math, Physics, or other technical fields.\n3+ years of overall technology experience includes some hands-on experience with software development and platform engineering work.\n3+ years of development experience in programming languages like Java, Python, PySpark, Scala, etc. Experience or knowledge in Data Modeling & SQL is a plus.\n2+ years of experience in any cloud technologies.\nWorking experience or knowledge of agile development, including DevOps and DataOps concepts.\nExperience with version control systems like GitHub and deployment & CI tools",Food Processing & Packaged Food,"['Data Quality Tools', 'DataOps', 'CI Tools', 'Statistical Machine Learning Models', 'Business Intelligence Tools', 'Sql', 'ELT', 'Cloud Technologies', 'Java', 'Data Profiling', 'Data Warehousing', 'Github', 'Devops', 'Pyspark', 'Etl', 'Powerbi', 'Python', 'Scala', 'Data Modeling', 'Agile Development']",2025-06-12 15:32:45
