title,company,experience,salary,location,description,industry,skills,scraped_at
Data Analyst,Virtusa,5-8 Years,,Pune,"Minimum 6 years of experience as DataAnalyst with at least 3+ Years experience in Data Migration initiatives.\nExperience in Migrating COTS/legacy systems including large volumes of data without compromising its accuracy and completeness\nTechnical expertise regarding data models, database design development, data mining and segmentation techniques\nExperience with ETL development both on premises and in the cloud\nStrong functional understanding of RDBMS DWH-BI conceptual knowledge",Information Technology,"['COTS', 'Data Modelling', 'Rdbms Concepts', 'Dwh', 'BI']",2025-06-12 15:19:22
Data Analyst,Virtusa,5-7 Years,,"Hyderabad, Pune","Minimum 6 years of experience as DataAnalyst with at least 3+ Years experience in Data Migration initiatives.\nExperience in Migrating COTS/legacy systems including large volumes of data without compromising its accuracy and completeness\nTechnical expertise regarding data models, database design development, data mining and segmentation techniques\nExperience with ETL development both on premises and in the cloud\nStrong functional understanding of RDBMS DWH-BI conceptual knowledge",Information Technology,"['COTS', 'Data Modelling', 'Rdbms Concepts', 'Dwh', 'BI']",2025-06-12 15:19:24
Data Analyst,Virtusa,5-7 Years,,"Hyderabad, Pune","Minimum 6 years of experience as DataAnalyst with at least 3+ Years experience in Data Migration initiatives.\nExperience in Migrating COTS/legacy systems including large volumes of data without compromising its accuracy and completeness\nTechnical expertise regarding data models, database design development, data mining and segmentation techniques\nExperience with ETL development both on premises and in the cloud\nStrong functional understanding of RDBMS DWH-BI conceptual knowledge",Information Technology,"['COTS', 'Data Modelling', 'Rdbms Concepts', 'Dwh', 'BI']",2025-06-12 15:19:26
Data Analyst,Virtusa,6-8 Years,,Hyderabad,"Minimum 6 years of experience as DataAnalyst with at least 3+ Years experience in Data Migration initiatives.\nExperience in Migrating COTS/legacy systems including large volumes of data without compromising its accuracy and completeness\nTechnical expertise regarding data models, database design development, data mining and segmentation techniques\nExperience with ETL development both on premises and in the cloud\nStrong functional understanding of RDBMS DWH-BI conceptual knowledge",Information Technology,"['COTS', 'Data Modelling', 'Rdbms Concepts', 'Dwh', 'BI']",2025-06-12 15:19:27
Data Analyst,Virtusa,7-8 Years,,Bengaluru,"Minimum 6 years of experience as DataAnalyst with at least 3+ Years experience in Data Migration initiatives.\nExperience in Migrating COTS/legacy systems including large volumes of data without compromising its accuracy and completeness\nTechnical expertise regarding data models, database design development, data mining and segmentation techniques\nExperience with ETL development both on premises and in the cloud\nStrong functional understanding of RDBMS DWH-BI conceptual knowledge",Information Technology,"['COTS', 'Data Modelling', 'Rdbms Concepts', 'Dwh', 'BI']",2025-06-12 15:19:28
Data Analyst,Virtusa,6-8 Years,,"Hyderabad, Bengaluru, Chennai","Minimum 6 years of experience as DataAnalyst with at least 3+ Years experience in Data Migration initiatives.\nExperience in Migrating COTS/legacy systems including large volumes of data without compromising its accuracy and completeness\nTechnical expertise regarding data models, database design development, data mining and segmentation techniques\nExperience with ETL development both on premises and in the cloud\nStrong functional understanding of RDBMS DWH-BI conceptual knowledge",Information Technology,"['COTS', 'Data Modelling', 'Rdbms Concepts', 'Dwh', 'BI']",2025-06-12 15:19:30
Data Analyst,Virtusa,2-4 Years,,Hyderabad,"Responsible for developing and creating BI and data analysis content to support decision-making and performance optimization.\nEnsure accurate reporting to stakeholders, improving transparency and data-driven insights.\nHold a bachelor's degree in Computer Science, IT, or a related field as a minimum requirement.\nExpertise in Azure Data Analytics, including Power BI and MS Fabric for data visualization and insights.\nExperience with Databricks for data processing, transformation, and analytics.\nProficiency in Azure DevOps for managing data workflows, automation, and collaboration.\nKnowledge of Confluence for documentation, project tracking, and team collaboration.\nStrong analytical and problem-solving skills to ensure effective data interpretation and recommendations.\nAbility to design, implement, and optimize BI solutions for business intelligence and reporting needs.\nCollaborate with cross-functional teams to ensure seamless integration of data analytics solutions into business operations.",Information Technology,"['Reporting', 'Business Analytics', 'Sql', 'Excel', 'Data Visualization']",2025-06-12 15:19:31
Data Analyst,Virtusa,5-8 Years,,Bengaluru,"Job Summary:\nWe are looking for a skilled Data Analyst to support a large-scale data migration initiative within the banking and insurance domain. The role involves analyzing, validating, and transforming data from legacy systems to modern platforms, ensuring regulatory compliance, data integrity, and business continuity.\nKey Responsibilities:\nCollaborate with business stakeholders, data architects, and IT teams to gather and understand data migration requirements.\nAnalyze legacy banking and insurance systems (e.g., core banking, policy admin, claims, CRM) to identify data structures and dependencies.\nWork with large-scale datasets and understand big data architectures (e.g., Hadoop, Spark, Hive) to support scalable data migration and transformation.\nPerform data profiling, cleansing, and transformation using SQL and ETL tools, with the ability to understand and write complex SQL queries and interpret the logic implemented in ETL workflows.\nDevelop and maintain data mapping documents and transformation logic specific to financial and insurance data (e.g., customer KYC, transactions, policies, claims).\nValidate migrated data against business rules, regulatory standards, and reconciliation reports.\nSupport UAT by preparing test cases and validating migrated data with business users.\nEnsure data privacy and security compliance throughout the migration process.\nDocument issues, risks, and resolutions related to data quality and migration.\nRequired Skills & Qualifications:\nBachelor's degree in Computer Science, Information Systems, Finance, or a related field.\n5+ years of experience in data analysis or data migration projects in banking or insurance.\nStrong SQL skills and experience with data profiling and cleansing.\nFamiliarity with ETL tools (e.g., Informatica, Talend, SSIS) and data visualization tools (e.g., Power BI, Tableau).\nExperience working with big data platforms (e.g., Hadoop, Spark, Hive) and handling large volumes of structured and unstructured data.\nUnderstanding of banking and insurance data domains (e.g., customer data, transactions, policies, claims, underwriting).\nKnowledge of regulatory and compliance requirements (e.g., AML, KYC, GDPR, IRDAI guidelines).\nExcellent analytical, documentation, and communication skills.\nPreferred Qualifications:\nExperience with core banking systems (e.g., Finacle, Flexcube) or insurance platforms\nExposure to cloud data platforms (e.g.,AWS, Azure, GCP).\nExperience working in Agile/Scrum environments.\nCertification in Business Analysis (e.g., CBAP, CCBA) or Data Analytics.",Information Technology,"['banking systems', 'Business Analysis', 'Data Analysis', 'Etl', 'cloud platform', 'Data Migration', 'Big Data Technologies', 'Data Visualization', 'Python']",2025-06-12 15:19:33
Data Analyst,Virtusa,6-10 Years,,Bengaluru,"We are looking for a skilled Data Analyst to support a large-scale data migration initiative within the banking and insurance domain. The role involves analyzing, validating, and transforming data from legacy systems to modern platforms, ensuring regulatory compliance, data integrity, and business continuity.\nKey Responsibilities:\nCollaborate with business stakeholders, data architects, and IT teams to gather and understand data migration requirements.\nAnalyze legacy banking and insurance systems (e.g., core banking, policy admin, claims, CRM) to identify data structures and dependencies.\nWork with large-scale datasets and understand big data architectures (e.g., Hadoop, Spark, Hive) to support scalable data migration and transformation.\nPerform data profiling, cleansing, and transformation using SQL and ETL tools, with the ability to understand and write complex SQL queries and interpret the logic implemented in ETL workflows.\nDevelop and maintain data mapping documents and transformation logic specific to financial and insurance data (e.g., customer KYC, transactions, policies, claims).\nValidate migrated data against business rules, regulatory standards, and reconciliation reports.\nSupport UAT by preparing test cases and validating migrated data with business users.\nEnsure data privacy and security compliance throughout the migration process.\nDocument issues, risks, and resolutions related to data quality and migration.\nRequired Skills & Qualifications:\nBachelor's degree in Computer Science, Information Systems, Finance, or a related field.\n5+ years of experience in data analysis or data migration projects in banking or insurance.\nStrong SQL skills and experience with data profiling and cleansing.\nFamiliarity with ETL tools (e.g., Informatica, Talend, SSIS) and data visualization tools (e.g., Power BI, Tableau).\nExperience working with big data platforms (e.g., Hadoop, Spark, Hive) and handling large volumes of structured and unstructured data.\nUnderstanding of banking and insurance data domains (e.g., customer data, transactions, policies, claims, underwriting).\nKnowledge of regulatory and compliance requirements (e.g., AML, KYC, GDPR, IRDAI guidelines).\nExcellent analytical, documentation, and communication skills.\nPreferred Qualifications:\nExperience with core banking systems (e.g., Finacle, Flexcube) or insurance platforms\nExposure to cloud data platforms (e.g.,AWS, Azure, GCP).\nExperience working in Agile/Scrum environments.\nCertification in Business Analysis (e.g., CBAP, CCBA) or Data Analytics.",Information Technology,"['banking systems', 'Business Analysis', 'Data Analysis', 'Etl', 'cloud platform', 'Data Migration', 'Big Data Technologies', 'Data Visualization', 'Python']",2025-06-12 15:19:34
Data Analyst / Technical Business Analyst,Virtusa,4-6 Years,,Bengaluru,"Key Responsibilities:\nCollaborate with business stakeholders, data architects, and IT teams to gather and understand data migration requirements.\nAnalyze legacy banking and insurance systems (e.g., core banking, policy admin, claims, CRM) to identify data structures and dependencies.\nWork with large-scale datasets and understand big data architectures (e.g., Hadoop, Spark, Hive) to support scalable data migration and transformation.\nPerform data profiling, cleansing, and transformation using SQL and ETL tools, with the ability to understand and write complex SQL queries and interpret the logic implemented in ETL workflows.\nDevelop and maintain data mapping documents and transformation logic specific to financial and insurance data (e.g., customer KYC, transactions, policies, claims).\nValidate migrated data against business rules, regulatory standards, and reconciliation reports.\nSupport UAT by preparing test cases and validating migrated data with business users.\nEnsure data privacy and security compliance throughout the migration process.\nDocument issues, risks, and resolutions related to data quality and migration.\nRequired Skills & Qualifications:\nBachelor's degree in Computer Science, Information Systems, Finance, or a related field.\n5+ years of experience in data analysis or data migration projects in banking or insurance.\nStrong SQL skills and experience with data profiling and cleansing.\nFamiliarity with ETL tools (e.g., Informatica, Talend, SSIS) and data visualization tools (e.g., Power BI, Tableau).\nExperience working with big data platforms (e.g., Hadoop, Spark, Hive) and handling large volumes of structured and unstructured data.\nUnderstanding of banking and insurance data domains (e.g., customer data, transactions, policies, claims, underwriting).\nKnowledge of regulatory and compliance requirements (e.g., AML, KYC, GDPR, IRDAI guidelines).\nExcellent analytical, documentation, and communication skills.\nPreferred Qualifications:\nExperience with core banking systems (e.g., Finacle, Flexcube) or insurance platforms\nExposure to cloud data platforms (e.g.,AWS, Azure, GCP).\nExperience working in Agile/Scrum environments.\nCertification in Business Analysis (e.g., CBAP, CCBA) or Data Analytics.",Information Technology,"['CCBA', 'Cbap', 'AWS', 'Flexcube', 'Aml']",2025-06-12 15:19:35
Data Analyst,Capgemini Technology Services India Limited,2-5 Years,,Hyderabad,"Job description\nData analysts import, inspect, clean, transform, validate, model, or interpret collections of data with regard to the business goals of the company. They ensure that the data sources and repositories provide consistent and reliable data. Data analysts use different algorithms and IT tools as demanded by the situation and the current data. They might prepare reports in the form of visualizations such as graphs, charts, and dashboards.\nJob Description - Grade Specific\nThe roles plays a critical role in leveraging data analysis to provide insights and recommendations to the stakeholders. Technical skills combined with consulting skills to support decision-making, drive business growth, and deliver value through data-driven insights.",Information Technology,"['Data Analysis', 'pivot table', 'Algorithms', 'Machine Learning', 'Tableau', 'data mining', 'Power Bi', 'Data Visualization', 'macros', 'Sql']",2025-06-12 15:19:37
Engineering Data Analyst,Alstom,3-5 Years,,Bengaluru,"Create and enforce Standard , Specific & Design parts for effective data management\nFormulate techniques for quality data collection to ensure adequacy, accuracy and legitimacy of data\nDevise and implement efficient and secure procedures for data handling and analysis with attention to all technical aspects\nSupport others in the daily use of data systems and ensure adherence to legal and company standards\nAssist with reports and data extraction when needed\nMonitor and analyze information and data systems and evaluate their performance to discover ways of enhancing them (new technologies, upgrades etc.)\nTroubleshoot data-related problems and authorize maintenance or modifications\nManage all incoming data files.\nContinually develop data management strategies.\nAnalyse & validate master data during rollouts. Raise incidents tickets and work closely with other IT operations teams to resolve MDM issues.\nBeing resilient and strive towards taking the team to next level by highlighting roadblocks to management\nCritical Challenges\nMtiers facing transformation challenges while business continuity must be maintained in Regions\nComplex end to end data flows with many cross-data dependencies",Transportation,"['Master data validation', 'Data quality management', 'ERP systems (MDM)', 'Issue resolution', 'Process Optimization', 'Data Governance']",2025-06-12 15:19:38
Cloud Security Compliance and Data Analyst,IBM,2-6 Years,,Bengaluru,"We are seeking a detail-oriented and analyticalSecurity Compliance and Data Analystto join our team. In this role, you will be responsible for ensuring that organizational data and systems adhere to security policies, regulatory standards, and best practices while providing insightful data analysis to support compliance and risk management efforts including internal readiness/assessments and external audits for IBM IaaS and Virtual Private Cloud.\nYou will work across our global teams and diverse stakeholders- security focals, executive program and delivery managers, regulatory and compliance certifications experts, product management, IT, and business teams to monitor, drive plans and deliver key security compliance metrics, analyze audit data, identify gaps, and contribute to the continuous improvement of IBM cloud security and compliance posture. Additionally, you will leverage your analytical skills to create dashboards, generate compliance reports, and support internal and external audits through data-driven insights.\nThe ideal candidate will be familiar with key regulatory and compliance frameworks, such as SOC2, PCI DSS, NIST standards, ISO 27K series, ISO 20000, GDPR, HITRUST, FEDRAMP or ITAR, and solid technical skills in data analysis, reporting, and produce data/reports for business operations that support the management for security compliance business decision making.\nYour role and responsibilities\nGenerate compliance reports from an existing dashboard or build requirements to create a new reporting dashboard\nProactively Monitor, track, and report on security compliance status across systems and processes.\nAnalyze large datasets to identify trends, anomalies, and compliance risks.\nSupport security audits, assessments, and certification efforts through data collection and analysis.\nPossess strong communication skill, collaborate with cross-functional matrix teams to drive root cause analysis, corrective actions and improvements based on data insights.\nMaintain and enhance compliance reporting dashboards and metrics for leadership visibility and decision making.\nRequired education\nBachelor's Degree\nRequired technical and professional expertise\nExperience working with security architects and technical security teams to define and implement security processes and procedures based on industry-standard best practices and compliance requirements. Defining the requirements and validating the procedures and audit testing methodology\nWorking with the Development teams to ensure automation of evidence collection and evidence management is always in line with compliance expectations, otherwise, identifies specific actions and owners to meet the expectations.\nAssisting team members in addressing highly complex security issues applicable to enterprise environment\nAbility to utilize project management principles to properly scope compliance work efforts by service lines, identify common areas of work, and create a measurable milestone plans across service lines to enable completion of compliance work items on time.\nAbility to manage multiple priority projects simultaneously under a short timeline\nExperience/familiar with enterprise risk management (ERM) framework, service delivery operations, software development lifecycle and be able to understand when to request and integrate risk items into compliance reporting.\nExperience with compliance programs such as FedRAMP/ FISMA, HIPAA, GDPR, SOC 2, PCI, NIST, ISO, ITAR, etc.\nConducting regular reviews on compliance progression of systems and hosting internal audit/assessment as required to maintain compliance certifications.\nAbility to translate and interpret regulatory compliance requirements into technical controls\nAbility to understand cloud enterprise business computing operations/requirements, and effectively communicate to service lines what is expected in order to consider a work item complete. Also, will possess good understanding of networking security including security systems such as firewalls, intrusion detection, vulnerability scanning, OS patching, health-checking\nDiagnosing the root cause of problems and propose solutions: Examples would be failed patches, tooling issues, false positives on system tests, authentication problems. Drive and track audit, security and compliance finding remediation to closure.\nExperience withenterprise configuration Managementdatabase (CMDB) orIT Asset inventory Management. UnderstandCMDB's structure, data quality, relationships between CIs (Configuration Items), and updates. Use the CMDB for risk, audit, and compliance analysis and reporting\nProficiency in SQL, Excel (advanced level: pivot tables, macros), and ServiceNow data analytics and visualization functionalities\nAbility to process large datasets, identify and handle missing data, data transformation, normalization, and data quality checks.\nAbility to perform data analysis to discover patterns and trends to mitigate security risks and drive business results\nWork with stakeholders to define key metrics and KPIs; develop dashboards and reports for business users.\nCollaborate with database engineers, data owners, security focal, product managers, and broader metrics teams to understand data needs.\nResults oriented with intense focus on achieving both short and long term goals. He/she should be able to drive and execute an agenda in a fast paced, dynamic environment.\nStrong project management skills with ability to design visual and appealing presentations\nStrong collaboration, problem-solving and critical-thinking abilities.\nExcellent communication skills ability to explain technical findings to non-technical audiences.\nGood time management, organizational skills, and ability to prioritize tasks.\nCuriosity and a continuous learning mindset.\nA highly organized with strong attention to detail, analytical and project management skills\nWork independently within a team focused organization.\nPreferred technical and professional experience\nExperience or familiar with cloud service models; IaaS preferred.\nProject management and consulting experience is a plus\nExperience with process automation is a plus\nExperience with Linux Shell, Perl or Python is a plus",Information Technology,"['Data Analysis', 'regulatory frameworks', 'risk management', 'Security Compliance', 'Sql', 'Servicenow']",2025-06-12 15:19:39
Data Analyst,Achnet,4-8 Years,,Noida,"Managing master data, including creation, updates, and deletion\nManaging users and user roles\nProvide quality assurance of imported data, working with quality assurance analysts if necessary\nCommissioning and decommissioning of data sets\nProcessing confidential data and information according to guidelines\nHelping develop reports and analysis\nManaging and designing the reporting environment, including data sources, security, and metadata\nSupporting the data warehouse in identifying and revising reporting requirements\nSupporting initiatives for data integrity and normalization\nAssessing tests and implementing new or upgraded software and assisting with strategic decisions on new systems\nGenerating reports from single or multiple systems\nTroubleshooting the reporting database environment and reports\nEvaluating changes and updates to source production systems\nTraining end-users on new reports and dashboards\nProviding technical expertise in data storage structures, data mining, and data cleansing",Career Planning,"['Administration', 'Analytical', 'Data Analyst', 'Talent Management', 'San', 'Data Modeling', 'MySQL', 'Power Bi', 'Sql', 'Python']",2025-06-12 15:19:41
Data Analyst,Systechcorp Private Limited,2-6 Years,,"Kolkata, Mumbai","Skillset :\nStrong analytical skills with the ability to interpret data and provide insights to the client.\nProficiency in MS Office (Excel, PowerPoint) and data visualization tools like Power BI or Tableau.\nExcellent communication, stakeholder management, and problem-solving abilities.\nExperience working with multiple store formats and client-specific merchandising strategies.\nResponsibilities:\nPlanogram & Space Optimization for the Client:\nDevelop and implement planograms tailored to the client s store formats and merchandising strategies.\nEnsure that planograms align with the client s business objectives and localized assortment requirements.\nLeverage space planning tools (JDA/Blue Yonder, Relex, etc.) to enhance product visibility and sales.\nOperations & Execution:\nEnsure accurate execution of the client s space planning and merchandising strategies.\nMaintain governance and control over store layout data to ensure consistency across locations.\nTrack and improve key performance metrics related to space utilization, planogram compliance, and operational efficiency.\nStakeholder Management & Client Coordination:\nAct as the primary point of contact for the client s teams regarding space planning and execution.\nWork closely with the client s store managers, marketing, and operations teams to implement and optimize merchandising initiatives.\nProvide regular updates and insights to the client, addressing concerns and ensuring alignment with their business goals.\nQualifications We Seek in You!\nMinimum Qualifications:\nBachelors degree in Retail Management, Operations, Architecture, Civil Engineering, or related fields.\nExperience in space planning, planogram development, and retail store layout optimization for clients.\nHands-on experience with space planning tools like JDA/Blue Yonder, Relex, Nexgen, or Apollo Retail.\nPreferred Qualifications/Skills:\nStrong analytical skills with the ability to interpret data and provide insights to the client.\nProficiency in MS Office (Excel, PowerPoint) and data visualization tools like Power BI or Tableau.\nExcellent communication, stakeholder management, and problem-solving abilities.\nExperience working with multiple store formats and client-specific merchandising strategies.",Software,"['Powerpoint', 'Operations', 'Relex', 'Ms Office', 'Excel', 'Jda']",2025-06-12 15:19:42
Data Analyst,Systechcorp Private Limited,2-6 Years,,"Kolkata, Mumbai","Skillset :\nStrong analytical skills with the ability to interpret data and provide insights to the client.\nProficiency in MS Office (Excel, PowerPoint) and data visualization tools like Power BI or Tableau.\nExcellent communication, stakeholder management, and problem-solving abilities.\nExperience working with multiple store formats and client-specific merchandising strategies.\nResponsibilities:\nPlanogram & Space Optimization for the Client:\nDevelop and implement planograms tailored to the client s store formats and merchandising strategies.\nEnsure that planograms align with the client s business objectives and localized assortment requirements.\nLeverage space planning tools (JDA/Blue Yonder, Relex, etc.) to enhance product visibility and sales.\nOperations & Execution:\nEnsure accurate execution of the client s space planning and merchandising strategies.\nMaintain governance and control over store layout data to ensure consistency across locations.\nTrack and improve key performance metrics related to space utilization, planogram compliance, and operational efficiency.\nStakeholder Management & Client Coordination:\nAct as the primary point of contact for the client s teams regarding space planning and execution.\nWork closely with the client s store managers, marketing, and operations teams to implement and optimize merchandising initiatives.\nProvide regular updates and insights to the client, addressing concerns and ensuring alignment with their business goals.\nQualifications We Seek in You!\nMinimum Qualifications:\nBachelors degree in Retail Management, Operations, Architecture, Civil Engineering, or related fields.\nExperience in space planning, planogram development, and retail store layout optimization for clients.\nHands-on experience with space planning tools like JDA/Blue Yonder, Relex, Nexgen, or Apollo Retail.\nPreferred Qualifications/Skills:\nStrong analytical skills with the ability to interpret data and provide insights to the client.\nProficiency in MS Office (Excel, PowerPoint) and data visualization tools like Power BI or Tableau.\nExcellent communication, stakeholder management, and problem-solving abilities.\nExperience working with multiple store formats and client-specific merchandising strategies.",Software,"['Powerpoint', 'Operations', 'Relex', 'Ms Office', 'Excel', 'Jda']",2025-06-12 15:19:51
Data Analyst,Turtlemint,0-3 Years,,Pune,"Using automated tools to extract data from primary and secondary sources\nRemoving corrupted data and fixing coding errors and related problems\nDeveloping and maintaining databases, data systems - reorganizing data in a readable format\nData sourcing through various tools, Data export and import in excel, pooling all the data from market source as per companies requirement.\nDatabase Presentation through dashboards and diagrams.\nMarket study and Analysis as per the Company requirement\nCompetitor's business strategy and rate analysis study.\nPerforming analysis to assess quality and meaning of data\nFilter Data by reviewing reports and performance indicators to identify and correct code problems\nUsing statistical tools to identify, analyse, and interpret patterns and trends in complex data sets that could be helpful for the diagnosis and prediction\nAssigning numerical value to essential business functions so that business performance can be assessed and compared over periods of time.\nAnalyzing local, national, and global trends that impact both the organization and the industry\nPreparing reports for the management stating trends, patterns, and predictions using relevant data\nPreparing final analysis reports for the management to understand the data-analysis steps, enabling them to take important decisions based on various facts and trends.\nSkills Qualifications:\nProven working experience in Data Analyst\nSolid knowledge of website analytics tools (e.g., Google Analytics, Web Trends)\nStrong analytical skills and data-driven thinking.",Insurance,"['Analytical Skills', 'Data Analysis', 'Data Analyst', 'Business Strategy', 'Google Analytics', 'Coding']",2025-06-12 15:19:53
Data Analyst,Turtlemint,0-3 Years,,Pune,"Using automated tools to extract data from primary and secondary sources\nRemoving corrupted data and fixing coding errors and related problems\nDeveloping and maintaining databases, data systems - reorganizing data in a readable format\nData sourcing through various tools, Data export and import in excel, pooling all the data from market source as per companies requirement.\nDatabase Presentation through dashboards and diagrams.\nMarket study and Analysis as per the Company requirement\nCompetitor's business strategy and rate analysis study.\nPerforming analysis to assess quality and meaning of data\nFilter Data by reviewing reports and performance indicators to identify and correct code problems\nUsing statistical tools to identify, analyse, and interpret patterns and trends in complex data sets that could be helpful for the diagnosis and prediction\nAssigning numerical value to essential business functions so that business performance can be assessed and compared over periods of time.\nAnalyzing local, national, and global trends that impact both the organization and the industry\nPreparing reports for the management stating trends, patterns, and predictions using relevant data\nPreparing final analysis reports for the management to understand the data-analysis steps, enabling them to take important decisions based on various facts and trends.\nSkills Qualifications:\nProven working experience in Data Analyst\nSolid knowledge of website analytics tools (e.g., Google Analytics, Web Trends)\nStrong analytical skills and data-driven thinking.",Insurance,"['Analytical Skills', 'Data Analysis', 'Data Analyst', 'Business Strategy', 'Google Analytics', 'Coding']",2025-06-12 15:19:55
Data Analyst,Systechcorp Private Limited,2-6 Years,,Bengaluru,"Responsibilities:\nPlanogram & Space Optimization for the Client:\nDevelop and implement planograms tailored to the client s store formats and merchandising strategies.\nEnsure that planograms align with the client s business objectives and localized assortment requirements.\nLeverage space planning tools (JDA/Blue Yonder, Relex, etc.) to enhance product visibility and sales.\nOperations & Execution:\nEnsure accurate execution of the client s space planning and merchandising strategies.\nMaintain governance and control over store layout data to ensure consistency across locations.\nTrack and improve key performance metrics related to space utilization, planogram compliance, and operational efficiency.\nStakeholder Management & Client Coordination:\nAct as the primary point of contact for the client s teams regarding space planning and execution.\nWork closely with the client s store managers, marketing, and operations teams to implement and optimize merchandising initiatives.\nProvide regular updates and insights to the client, addressing concerns and ensuring alignment with their business goals.\nQualifications We Seek in You!\nMinimum Qualifications:\nBachelors degree in Retail Management, Operations, Architecture, Civil Engineering, or related fields.\nExperience in space planning, planogram development, and retail store layout optimization for clients.\nHands-on experience with space planning tools like JDA/Blue Yonder, Relex, Nexgen, or Apollo Retail.\nPreferred Qualifications/Skills:\nStrong analytical skills with the ability to interpret data and provide insights to the client.\nProficiency in MS Office (Excel, PowerPoint) and data visualization tools like Power BI or Tableau.\nExcellent communication, stakeholder management, and problem-solving abilities.\nExperience working with multiple store formats and client-specific merchandising strategies.",Software,"['Blue Yonder', 'nexgen storage', 'Relex', 'Apollo Retail', 'Ms Office']",2025-06-12 15:19:56
Data Analyst,Systechcorp Private Limited,2-6 Years,,"Chennai, Bengaluru","Skillset :\nStrong analytical skills with the ability to interpret data and provide insights to the client.\nProficiency in MS Office (Excel, PowerPoint) and data visualization tools like Power BI or Tableau.\nExcellent communication, stakeholder management, and problem-solving abilities.\nExperience working with multiple store formats and client-specific merchandising strategies.\nResponsibilities:\nPlanogram & Space Optimization for the Client:\nDevelop and implement planograms tailored to the client s store formats and merchandising strategies.\nEnsure that planograms align with the client s business objectives and localized assortment requirements.\nLeverage space planning tools (JDA/Blue Yonder, Relex, etc.) to enhance product visibility and sales.\nOperations & Execution:\nEnsure accurate execution of the client s space planning and merchandising strategies.\nMaintain governance and control over store layout data to ensure consistency across locations.\nTrack and improve key performance metrics related to space utilization, planogram compliance, and operational efficiency.\nStakeholder Management & Client Coordination:\nAct as the primary point of contact for the client s teams regarding space planning and execution.\nWork closely with the client s store managers, marketing, and operations teams to implement and optimize merchandising initiatives.\nProvide regular updates and insights to the client, addressing concerns and ensuring alignment with their business goals.\nQualifications We Seek in You!\nMinimum Qualifications:\nBachelors degree in Retail Management, Operations, Architecture, Civil Engineering, or related fields.\nExperience in space planning, planogram development, and retail store layout optimization for clients.\nHands-on experience with space planning tools like JDA/Blue Yonder, Relex, Nexgen, or Apollo Retail.\nPreferred Qualifications/Skills:\nStrong analytical skills with the ability to interpret data and provide insights to the client.\nProficiency in MS Office (Excel, PowerPoint) and data visualization tools like Power BI or Tableau.\nExcellent communication, stakeholder management, and problem-solving abilities.\nExperience working with multiple store formats and client-specific merchandising strategies.",Software,"['Powerpoint', 'Operations', 'Relex', 'Ms Office', 'Excel', 'Jda']",2025-06-12 15:19:57
Data Analyst,Xoom,2-4 Years,,Bengaluru,"In your day to day role you will -\nIn this role you will have full ownership of a portfolio of merchants and is responsible for end-to-end management of loss and decline rates.\nCollaborate with different teams to develop strategies for fraud prevention, loss savings, and optimize transaction declines or improve customer friction.\nYou will work together with cross-functional teams to deliver solutions and providing Risk analytics on frustration trend/ KPIs monitoring or alerting for fraud events.\nThese solutions will adapt PayPal s advanced proprietary fraud prevention tools enabling business growth.\nWhat do you need to bring-\n2-4 years of relevant experience working with large-scale complex dataset.\nStrong analytical mindset, ability to decompose business requirements into an analytical plan, and execute the plan to answer those business questions\nExcellent communication skills, equally adept at working with engineers as well as business leaders\nWant to build new solutions and invent new approaches to big, ambiguous, critical problems\nStrong working knowledge ofExcel, SQL and Python/R\nTechnical Proficiency Exploratory Data Analysis and expertise in preparing a clean and structured data for model development.\nExperience in applying AI/ML techniques for business decisioning including supervised and unsupervised learning (e.g., regression, classification, clustering, decision trees, anomaly detection, etc.).\nKnowledge of model evaluation techniques such as Precision, Recall, ROC-AUC Curve, etc. along with basic statistical concepts.",FinTech,"['Data Analysis', 'AI/ML', 'Model Evaluation', 'Fraud Prevention', 'Sql', 'Python']",2025-06-12 15:19:59
Data Management - Data Analyst-IT I.,Systechcorp Private Limited,1-6 Years,,Bengaluru,"Job Description:\nWe re looking for a junior Data Engineer with at least 1 year of experience to join our growing HR Tech team. You ll work with cross-functional stakeholders including HR, Analytics, and Product teams to design, build, and maintain data pipelines and solutions that support strategic HR initiatives like workforce planning, employee engagement, talent acquisition, and diversity analytics.\nKey Responsibilities\nDesign and develop scalable data pipelines for HR analytics and reporting\nIntegrate data from Oracle HR systems and third-party sources\nOptimize SQL queries and ETL processes for performance and accuracy\nCollaborate with HR analysts and data scientists to deliver reliable datasets\nEnsure data quality, consistency, and security across platforms\nDocument workflows, data models, and technical processes\nRequired Qualifications\n1+ years of experience in data engineering, data analytics, or related field\nStrong SQL skills and familiarity with ETL tools like ODI\nExperience with Python, R, or similar scripting languages\nKnowledge of data warehousing concepts and relational databases (e.g., Oracle, MySQL)\nUnderstanding of HR data (e.g., employee lifecycle, organization hierarchy) is a plus\nBachelor s degree in Computer Science, Information Systems, Data Science, or related field\nPreferred Qualifications\nExperience working with Oracle Cloud Infrastructure (OCI)\nFamiliarity with Oracle HCM Cloud or other HRIS platforms\nInterest in using AI and machine learning to improve HR processes",Software,"['oracle hr', 'R', 'Hris', 'Etl', 'Python', 'Mysql']",2025-06-12 15:20:00
Sr. Data Analyst - Mpay,Mobile Premier League (MPL),7-9 Years,,Bengaluru,"Mobile Premier League is looking for Sr. Data Analyst - Mpay to join our dynamic team and embark on a rewarding career journey\nManaging master data, including creation, updates, and deletion.\nManaging users and user roles.\nProvide quality assurance of imported data, working with quality assurance analysts if necessary.\nCommissioning and decommissioning of data sets.\nProcessing confidential data and information according to guidelines.\nHelping develop reports and analysis.\nManaging and designing the reporting environment, including data sources, security, and metadata.\nSupporting the data warehouse in identifying and revising reporting requirements.\nSupporting initiatives for data integrity and normalization.\nAssessing tests and implementing new or upgraded software and assisting with strategic decisions on new systems.\nGenerating reports from single or multiple systems.\nTroubleshooting the reporting database environment and reports.\nEvaluating changes and updates to source production systems.\nTraining end-users on new reports and dashboards.\nProviding technical expertise in data storage structures, data mining, and data cleansing.",Publishing,"['ETL Processes', 'Business Intelligence', 'Sql', 'Python', 'Data Visualization', 'Machine Learning', 'Data Warehousing', 'Big Data']",2025-06-12 15:20:01
Data Management - Data Analyst-IT I.,Systechcorp Private Limited,1-6 Years,,"Delhi, Kolkata, Mumbai","Job Description:\nWe re looking for a junior Data Engineer with at least 1 year of experience to join our growing HR Tech team. You ll work with cross-functional stakeholders including HR, Analytics, and Product teams to design, build, and maintain data pipelines and solutions that support strategic HR initiatives like workforce planning, employee engagement, talent acquisition, and diversity analytics.\nKey Responsibilities\nDesign and develop scalable data pipelines for HR analytics and reporting\nIntegrate data from Oracle HR systems and third-party sources\nOptimize SQL queries and ETL processes for performance and accuracy\nCollaborate with HR analysts and data scientists to deliver reliable datasets\nEnsure data quality, consistency, and security across platforms\nDocument workflows, data models, and technical processes\nRequired Qualifications\n1+ years of experience in data engineering, data analytics, or related field\nStrong SQL skills and familiarity with ETL tools like ODI\nExperience with Python, R, or similar scripting languages\nKnowledge of data warehousing concepts and relational databases (e.g., Oracle, MySQL)\nUnderstanding of HR data (e.g., employee lifecycle, organization hierarchy) is a plus\nBachelor s degree in Computer Science, Information Systems, Data Science, or related field\nPreferred Qualifications\nExperience working with Oracle Cloud Infrastructure (OCI)\nFamiliarity with Oracle HCM Cloud or other HRIS platforms\nInterest in using AI and machine learning to improve HR processes",Software,"['oracle hr', 'R', 'Hris', 'Etl', 'Python', 'Mysql']",2025-06-12 15:20:03
Data Management - Data Analyst-IT I.,Systechcorp Private Limited,1-6 Years,,"Hyderabad, Chennai, Pune","Job Description:\nWe re looking for a junior Data Engineer with at least 1 year of experience to join our growing HR Tech team. You ll work with cross-functional stakeholders including HR, Analytics, and Product teams to design, build, and maintain data pipelines and solutions that support strategic HR initiatives like workforce planning, employee engagement, talent acquisition, and diversity analytics.\nKey Responsibilities\nDesign and develop scalable data pipelines for HR analytics and reporting\nIntegrate data from Oracle HR systems and third-party sources\nOptimize SQL queries and ETL processes for performance and accuracy\nCollaborate with HR analysts and data scientists to deliver reliable datasets\nEnsure data quality, consistency, and security across platforms\nDocument workflows, data models, and technical processes\nRequired Qualifications\n1+ years of experience in data engineering, data analytics, or related field\nStrong SQL skills and familiarity with ETL tools like ODI\nExperience with Python, R, or similar scripting languages\nKnowledge of data warehousing concepts and relational databases (e.g., Oracle, MySQL)\nUnderstanding of HR data (e.g., employee lifecycle, organization hierarchy) is a plus\nBachelor s degree in Computer Science, Information Systems, Data Science, or related field\nPreferred Qualifications\nExperience working with Oracle Cloud Infrastructure (OCI)\nFamiliarity with Oracle HCM Cloud or other HRIS platforms\nInterest in using AI and machine learning to improve HR processes",Software,"['oracle hr', 'R', 'Hris', 'Etl', 'Python', 'Mysql']",2025-06-12 15:20:04
Data Analyst,Systechcorp Private Limited,2-6 Years,,"Delhi, Kolkata, Mumbai","Responsibilities:\nPlanogram & Space Optimization for the Client:\nDevelop and implement planograms tailored to the client s store formats and merchandising strategies.\nEnsure that planograms align with the client s business objectives and localized assortment requirements.\nLeverage space planning tools (JDA/Blue Yonder, Relex, etc.) to enhance product visibility and sales.\nOperations & Execution:\nEnsure accurate execution of the client s space planning and merchandising strategies.\nMaintain governance and control over store layout data to ensure consistency across locations.\nTrack and improve key performance metrics related to space utilization, planogram compliance, and operational efficiency.\nStakeholder Management & Client Coordination:\nAct as the primary point of contact for the client s teams regarding space planning and execution.\nWork closely with the client s store managers, marketing, and operations teams to implement and optimize merchandising initiatives.\nProvide regular updates and insights to the client, addressing concerns and ensuring alignment with their business goals.\nQualifications We Seek in You!\nMinimum Qualifications:\nBachelors degree in Retail Management, Operations, Architecture, Civil Engineering, or related fields.\nExperience in space planning, planogram development, and retail store layout optimization for clients.\nHands-on experience with space planning tools like JDA/Blue Yonder, Relex, Nexgen, or Apollo Retail.\nPreferred Qualifications/Skills:\nStrong analytical skills with the ability to interpret data and provide insights to the client.\nProficiency in MS Office (Excel, PowerPoint) and data visualization tools like Power BI or Tableau.\nExcellent communication, stakeholder management, and problem-solving abilities.\nExperience working with multiple store formats and client-specific merchandising strategies.",Software,"['Blue Yonder', 'nexgen storage', 'Relex', 'Apollo Retail', 'Ms Office']",2025-06-12 15:20:05
Data Analyst,Systechcorp Private Limited,2-6 Years,,"Hyderabad, Chennai, Pune","Responsibilities:\nPlanogram & Space Optimization for the Client:\nDevelop and implement planograms tailored to the client s store formats and merchandising strategies.\nEnsure that planograms align with the client s business objectives and localized assortment requirements.\nLeverage space planning tools (JDA/Blue Yonder, Relex, etc.) to enhance product visibility and sales.\nOperations & Execution:\nEnsure accurate execution of the client s space planning and merchandising strategies.\nMaintain governance and control over store layout data to ensure consistency across locations.\nTrack and improve key performance metrics related to space utilization, planogram compliance, and operational efficiency.\nStakeholder Management & Client Coordination:\nAct as the primary point of contact for the client s teams regarding space planning and execution.\nWork closely with the client s store managers, marketing, and operations teams to implement and optimize merchandising initiatives.\nProvide regular updates and insights to the client, addressing concerns and ensuring alignment with their business goals.\nQualifications We Seek in You!\nMinimum Qualifications:\nBachelors degree in Retail Management, Operations, Architecture, Civil Engineering, or related fields.\nExperience in space planning, planogram development, and retail store layout optimization for clients.\nHands-on experience with space planning tools like JDA/Blue Yonder, Relex, Nexgen, or Apollo Retail.\nPreferred Qualifications/Skills:\nStrong analytical skills with the ability to interpret data and provide insights to the client.\nProficiency in MS Office (Excel, PowerPoint) and data visualization tools like Power BI or Tableau.\nExcellent communication, stakeholder management, and problem-solving abilities.\nExperience working with multiple store formats and client-specific merchandising strategies.",Software,"['Blue Yonder', 'nexgen storage', 'Relex', 'Apollo Retail', 'Ms Office']",2025-06-12 15:20:07
Sales Operations - Senior Data Analyst,Zenoti,4-5 Years,,Hyderabad,"Job description\nZenoti provides an all-in-one, cloud-based software solution for the beauty and wellness industry. Our solution allows users to seamlessly manage every aspect of the business in a comprehensive mobile solution: online appointment bookings, POS, CRM, employee management, inventory management, built-in marketing programs and more. Zenoti helps clients streamline their systems and reduce costs, while simultaneously improving customer retention and spending. Our platform is engineered for reliability and scale and harnesses the power of enterprise-level technology for businesses of all sizes\nZenoti powers more than 30,000 salons, spas, medspas and fitness studios in over 50 countries. This includes a vast portfolio of global brands, such as European Wax Center, Hand Stone, Massage Heights, Rush Hair Beauty, Sono Bello, Profile by Sanford, Hair Cuttery, CorePower Yoga and TONIGUY.\nOur recent accomplishments include surpassing a $1 billion unicorn valuation, being named Next Tech Titan by GeekWire, raising an $80 million investment from TPG, ranking as the 316th fastest-growing company in North America on Deloitte s 2020 Technology Fast 500 . We are also proud to be recognized as a Great Place to Work CertifiedTM for 2021-2022 as this reaffirms our commitment to empowering people to feel good and find their greatness. To learn more about Zenoti visit: https://www.zenoti.com\nWhats the opportunity\nAre you passionate about turning data into strategic insightsAs a Senior Data Analyst, your work as an individual contributor will play a pivotal role in empowering our sales organization with clear, actionable insights that drive strategic decisions and business growth.\nYou will work closely with the sales teams and senior leadership to:\nTrack, analyze, and interpret sales data to identify trends and provide clear, actionable recommendations to meet company objectives.\nTranslate complex data into clear and concise reports.\nDesign, build, and maintain key sales reports and dashboards, ensuring consistency and accuracy.\nMaintain accurate open pipeline and sales forecasts to support data-driven decisions.\nPartner with Sales Enablement to identify training opportunities.\nAs a trusted partner to sales leadership, you will be counted on to uncover key insights that drive positive change and find answers to the hard questions.\n\nThe ideal candidate is adept at translating complex data into actionable steps and clear insights, possesses strong analytical skills, and has a keen understanding of the SAAS industry.\nRequirements\nWhat will I be doing\nDevelop and maintain dashboards, reports, and visualizations to communicate key metrics.\nIdentify trends, patterns, and anomalies in sales data to inform strategic decision-making.\nConduct analyses to support sales initiatives, pricing strategies, and market expansion.\nAssist in the development and implementation of data-driven sales strategies.\nProvide monthly analysis and insights to internal stakeholders.\nOversee the sales funnel and ensure that its always accurate and up to date.\nCollaborate with multiple levels in sales and marketing as well as cross functional teams such as finance and services.\nPartner with Sales Enablement to identify training opportunities.\nWhat skills do I need\nEducation Experience\nBachelor s degree in Business, Statistics, Data Science, or a related field.\n4+ years of experience in sales data analysis, preferably in a SAAS or technology-focused company.\nProficient in Salesforce and PowerBI for data visualization (Quicksight experience is a plus).\nAdvanced proficiency in Excel and SQL for data manipulation and analysis.\nDeep understanding of Salesforce.\nAdditional tech stack experience, such as Outreach and Zuora, is beneficial.\nStrong business acumen with a deep understanding of how sales organizations operate and scale.\nProven expertise in tracking sales KPIs, analyzing trends, forecasting, and delivering insights to executive teams and leadership in a SaaS environment.\nAbility to adapt to evolving technologies and tools in the SAAS industry.\nCompetencies:\nExceptional analytical skills with a meticulous focus on data accuracy, presentation, and process-driven insights.\nAbility to analyze large datasets, extract meaningful insights, and translate findings into actionable recommendations.\nExcellent verbal and written communication, presentation, and persuasion skills.\nSkilled at translating complex data into clear, impactful insights for non-technical audiences\nExperience collaborating and presenting findings to the executive team, leadership, sales teams, and cross functional departments.\nWork Ethic\nTeam player with a proactive mindset, strong initiative, and a results-oriented approach.\nA strong sense of ownership and a proactive attitude, used to working independently without much oversight.\nFlexibility to adjust hours during critical business periods such as month-end, quarter-close, or year-end to ensure smooth operations.\nWhy Zenoti\nBe part of an innovative company that is revolutionizing the wellness and beauty industry.\nWork with a dynamic and diverse team that values collaboration, creativity, and growth.\nOpportunity to lead impactful projects and help shape the global success of Zenoti s platform.\nAttractive compensation.\nMedical coverage for yourself and your immediate family.\nAccess to regular yoga, meditation, breathwork, and stress management sessions. We also include your family in benefit awareness initiatives.\nRegular social activities, and opportunities to give back through social work and community initiatives.",Information Technology,"['Data Analysis', 'Crm', 'Sql']",2025-06-12 15:20:08
DATA ANALYST POWER BI INTERNSHIP,Maxgen Technologies Private Limited,Fresher,INR 1.5 - 2.5 LPA,"Ahmedabad, Surat, Anand","Maxgen Technologies Pvt ltd is an it company based in Ahmedabad Gujarat .Engineering and IT freshers will get data analyst internship and power bi program.\nWhy interns Choosing us\nUnique and professional Program.\nlive project in internship.\nhybrid mode internship.\ncollege project for interns.\ncontact us 9099039845 .\nvisit us www.maxgengentechnologies.com .\nAddress Abhijeet 3, 603, Netaji Rd, near Pantaloons, Mithakhali, Ellisbridge, Ahmedabad,",Information Technology,"['ETL Processes', 'Statistics', 'Data Analyst', 'Internship', 'Data Analysis', 'Data Modeling', 'Power Bi', 'Power Query', 'Sql', 'Data Visualization', 'Excel', 'Dax']",2025-06-12 15:20:10
Data Analyst,Freelancer Sreeraj M Gopalakrishnan,0-3 Years,INR 1 - 3 LPA,Cochin / Kochi / Ernakulam,"Job Title: Data Analyst\nLocation: Kaloor, Kochi\nCTC: 3 LPA\nJob Type: Full-time\nShift: UK Shift (12:30 PM 9:30 PM or 1:30 PM 10:30 PM)\nLanguage: English\nWe're hiring a Data Analyst to support UK-based sustainability and compliance projects (SECR, CCA, ESOS, DEC). Responsibilities include data validation, report preparation, audit support, and CRM (Salesforce) updates. Must be detail-oriented with strong Excel and communication skills.\nApply now to join a fast-growing team working on global environmental data!",Login to check your skill match score,"['Data Analyst', 'Excellent English', 'Advance Excel']",2025-06-12 15:20:11
Data Analyst,Robotics Technologies,4-8 Years,INR 8 - 18.5 LPA,"Navi Mumbai, Kolkata, Delhi NCR","Description\nWe are seeking a skilled Data Analyst to join our team in India. The ideal candidate will have 4-8 years of experience in data analysis, with a strong ability to analyze complex datasets and provide actionable insights that drive business decisions.\nResponsibilities\nAnalyze complex datasets to derive actionable insights and support business decision-making.\nDevelop and maintain dashboards and reports to track key performance indicators.\nCollaborate with cross-functional teams to understand data requirements and provide analytical support.\nUtilize statistical methods to identify trends, patterns, and anomalies in data.\nPresent findings and recommendations to stakeholders in a clear and concise manner.\nEnsure data accuracy and integrity by conducting regular data audits and validation.\nSkills and Qualifications\n4-8 years of experience in data analysis or related field.\nProficiency in data analysis tools such as SQL, Python, or R.\nExperience with data visualization tools like Tableau, Power BI, or similar.\nStrong knowledge of statistical methods and data modeling techniques.\nAbility to interpret complex data and communicate findings effectively.\nFamiliarity with database management systems and data warehousing concepts.\nExcellent problem-solving skills and attention to detail.\nStrong organizational skills and ability to manage multiple projects simultaneously.","Cloud Management, IT Infrastructure, Cyber Security, Virtualization","['ETL Processes', 'Excel Advanced', 'Dashboard Development', 'Sql', 'Python', 'Data Visualization', 'Statistical Analysis', 'Machine Learning', 'Data Warehousing', 'R Programming']",2025-06-12 15:20:23
Data Analyst,Robotics Technologies,5-10 Years,INR 7 - 20 LPA,"Ahmedabad, Bengaluru, Noida","We are seeking a skilled Data Analyst to join our team in India. The ideal candidate will have a strong background in data analysis and visualization, with the ability to work collaboratively across departments to drive data-driven decision-making.\nResponsibilities\nCollect, clean, and analyze data from various sources to support business decisions.\nDevelop and maintain dashboards and reports to visualize data insights.\nCollaborate with cross-functional teams to understand their data needs and provide analytical support.\nIdentify trends, patterns, and anomalies in complex data sets to drive strategic initiatives.\nPresent findings and recommendations to stakeholders in a clear and concise manner.\nSkills and Qualifications\n5-10 years of experience in a data analytics role.\nProficiency in data visualization tools such as Tableau, Power BI, or similar.\nStrong knowledge of SQL for data extraction and manipulation.\nExperience with statistical analysis and data modeling techniques.\nFamiliarity with programming languages such as Python or R for data analysis.\nExcellent problem-solving skills and attention to detail.\nStrong communication skills to effectively convey analytical findings.",Information Technology,"['ETL Processes', 'Excel Advanced', 'Sql', 'Python', 'Data Visualization', 'Machine Learning', 'Statistical Analysis', 'Data Mining', 'Tableau', 'R Programming']",2025-06-12 15:20:28
Data Analyst,Robotics Technologies,5-10 Years,INR 7 - 20 LPA,"Ahmedabad, Bengaluru, Noida","We are seeking a skilled Data Analyst to join our team in India. The ideal candidate will have a strong background in data analysis and visualization, with the ability to work collaboratively across departments to drive data-driven decision-making.\nResponsibilities\nCollect, clean, and analyze data from various sources to support business decisions.\nDevelop and maintain dashboards and reports to visualize data insights.\nCollaborate with cross-functional teams to understand their data needs and provide analytical support.\nIdentify trends, patterns, and anomalies in complex data sets to drive strategic initiatives.\nPresent findings and recommendations to stakeholders in a clear and concise manner.\nSkills and Qualifications\n5-10 years of experience in a data analytics role.\nProficiency in data visualization tools such as Tableau, Power BI, or similar.\nStrong knowledge of SQL for data extraction and manipulation.\nExperience with statistical analysis and data modeling techniques.\nFamiliarity with programming languages such as Python or R for data analysis.\nExcellent problem-solving skills and attention to detail.\nStrong communication skills to effectively convey analytical findings.",Information Technology,"['ETL Processes', 'Excel Advanced', 'Sql', 'Python', 'Data Visualization', 'Machine Learning', 'Statistical Analysis', 'Data Mining', 'Tableau', 'R Programming']",2025-06-12 15:20:30
Business Data Analyst,Orange Business Services,2-10 Years,,Gurugram,"We are seeking a skilled Business Analyst with expertise in AI, data automation, and Power BI with AI capabilities This role focuses on leveraging advanced analytics and AI-powered tools to transform raw data into actionable insights, driving business growth and operational efficiency\nMission\nThe mission of the Business Data Analyst role is to leverage data-driven insights for optimizing business processes and enhancing operational efficiency Key objectives include supporting 5strategic initiatives through advanced data analytics, fostering a culture of data-driven decision-making, and driving growth through actionable insights\nMain activities\nAnalyze and interpret complex datasets using machine learning libraries or AI-enabled analytics platforms to uncover trends, patterns, and actionable insights\nAutomate data processes, including extraction, transformation, and loading (ETL), using AI-driven automation tools to improve efficiency\nDevelop predictive models and perform advanced analytics to support forecasting, customer segmentation, and other business needs\nDesign, develop, and maintain interactive Power BI dashboards and reports enhanced with AI functionalities for business decision-making\nEnsure data accuracy and integrity by validating, cleaning, and preprocessing datasets\nCollaborate with cross-functional teams to identify business challenges and propose data-driven solutions\nStay updated with the latest advancements in AI, automation, and Power BI to continuously enhance analytical capabilities\nRequired Skills:\nProficiency in AI-powered SaaS platforms like Salesforce and tools for data analysis and automation\nExpertise in Power BI, including AI integration for advanced analytics and visualization (hands-on experience in using Gemini)\nStrong knowledge of data modeling techniques like Polyglot, SARIMA, Holt-Winters, clustering, forecasting, and algorithm selection (linear regression, decision tree, neural networks), as well as SQL, Python, R, and ETL processes\nExperience with AI-driven predictive modeling and machine learning techniques\nExcellent analytical, problem-solving, and communication skills\nAbility to translate complex data insights into actionable business strategies\nPreferred Skills:\nExperience with additional analytics platforms and consulting experience in business intelligence or data strategy would be beneficial\nFamiliarity with programming languages such as R or Python for data manipulation and application in real-world scenarios",IT Management,"['Data Analysis', 'Business intelligence', 'Data Modeling', 'Machine Learning', 'Sql', 'Python']",2025-06-12 15:20:31
Data Analyst,Orange Business Services,2-5 Years,,Gurugram,"Translate Business Problems:Translate a business problem into a computer and statistical problem\nData Management:Identify, evaluate, recover, and prepare the data Evaluate and guarantee the quality and relevance of the models used\nEnd-to-End Analysis:Conduct end-to-end analysis, communicating findings and developing recommendations for actions, optimization, and value creation\nImplement Computer Tools:Implement computer tools, techniques, and statistical methods to efficiently organize, synthesize, and translate data Extract customer knowledge or domains for business value creation\nReport Formatting:Develop clear, concise, and impactful reports to communicate results to stakeholders\nIndustrialization of Deliverables:Propose and industrialize the development of recurrent deliverables, ensuring efficiency and repeatability in analyses\nInterdisciplinary Collaboration:Collaborate with various s to understand their data needs and propose tailored solutions\nDevelopment of KPIs:Design and monitor key performance indicators to assess the effectiveness of strategic initiatives\nContinuous Improvement:Identify and propose improvements to data collection and analysis processes to optimize overall performance\nIntegration of Value:Integrate the value generated into our processes and the evolution of the information systems domain\nOptimize Data Use:Optimize the use of data in compliance with the legal and regulatory context\nData Evolution:Manage evolutions on data deposits and data enrichment for new levers of value creation\nCoordination of Exchanges:Facilitate discussions between operational managers and tool managers to ensure smooth implementation of tools\nManagement of Tooling Projects:Oversee the generalization and industrialization of tooling projects, ensuring alignment with operational needs\nRequired Skills:\nData Analysis Expertise:Proficiency in analysis tools (Excel, SQL, Python, R)\nVisualization Skills:Experience with visualization tools (Tableau, Power BI, Elastic ELK)\nCritical Analysis:Ability to interpret complex data and formulate relevant conclusions\nEffective Communication:Excellent written and oral communication skills, capable of conveying technical information to diverse audiences\nProactive Problem Solving:Ability to identify issues and develop data-driven solutions\nUnderstanding of ITIL Processes:Strong knowledge of ITIL processes and the ability to apply them effectively\nInterpersonal Skills:Excellent ability to interact with users and stakeholders\nProject Management:Proven experience in managing projects related to operational tooling\nUser-Oriented Approach:Ability to listen and incorporate user feedback to improve tools\nQualifications:\nDegree in statistics, computer science, or a related field\n2 to 5 years of experience as a Data Analyst or in a similar role\nExperience in the telecommunications and IT sector is a plus",IT Management,"['Data Analysis', 'Project management', 'Data Analyst', 'Data Management', 'Sql', 'Python']",2025-06-12 15:20:32
Data Management - Data Analyst-IT I.,Systechcorp Private Limited,1-6 Years,,"Delhi, Kolkata, Mumbai","Key Responsibilities\nDesign and develop scalable data pipelines for HR analytics and reporting\nIntegrate data from Oracle HR systems and third-party sources\nOptimize SQL queries and ETL processes for performance and accuracy\nCollaborate with HR analysts and data scientists to deliver reliable datasets\nEnsure data quality, consistency, and security across platforms\nDocument workflows, data models, and technical processes\nRequired Qualifications\n1+ years of experience in data engineering, data analytics, or related field\nStrong SQL skills and familiarity with ETL tools like ODI\nExperience with Python, R, or similar scripting languages\nKnowledge of data warehousing concepts and relational databases (e.g., Oracle, MySQL)\nUnderstanding of HR data (e.g., employee lifecycle, organization hierarchy) is a plus\nBachelor s degree in Computer Science, Information Systems, Data Science, or related field\nPreferred Qualifications\nExperience working with Oracle Cloud Infrastructure (OCI)\nFamiliarity with Oracle HCM Cloud or other HRIS platforms\nInterest in using AI and machine learning to improve HR processes",Software,"['Hr', 'Sql', 'Etl', 'Odi', 'python', 'oracle']",2025-06-12 15:20:34
Senior Data Analyst,GreyOrange,3-5 Years,,Gurugram,"Roles and Responsibilities :\nWe are seeking a highly motivated Senior Data Analyst to join our team. In this role, you will write complex SQL queries to power analytics dashboards and collaborate with cross-functional teams Product Managers, Solution Architects, and Data Engineers to deliver end-to-end analytics solutions. The ideal candidate is skilled in translating business requirements into technical insights, ensuring that data is readily available and consumable for reporting and decision-making.\nKey Responsibilities\nAnalytics Dashboarding\nWrite and optimize SQL queries to support real-time analytics dashboards and reporting solutions.\nConduct data validation and troubleshooting issues related to data accuracy and integrity.\nCross-Functional Collaboration\nWork closely with Product Managers and Solution Architects to identify customer analytics needs and define project scope.\nPartner with Data Engineers and other technical teams to ensure data is accessible, reliable, and aligned with business requirements.\nData Management Delivery\nCollaborate with stakeholders to gather, document, and prioritize analytics requirements.\nDefine data requirements for data pipelines, ensuring end-to-end delivery of analytics solutions.\nPerformance Optimization\nImplement best practices in query optimization to handle large, complex data sets.\nMonitor and enhance the performance of analytical processes and data models.\nData Quality Governance\nEstablish and maintain data quality metrics and governance standards.\nIdentify data gaps and work with engineering teams to implement corrective actions.\nReporting Insights\nDevelop and maintain dashboards, reports, and ad hoc analyses to inform business decisions.\nTranslate findings into actionable recommendations and present them to stakeholders at various levels.\nQualifications Skills:\nEducation: Bachelor s or master s degree in computer science, Information Systems, Statistics, or a related field.\n4-5 years of relevant experience.\nSQL Expertise: Advanced SQL skills with a proven track record of writing, optimizing, and troubleshooting complex queries. Knowledge of programming languages used for data manipulation.\nData Analysis Tools: Experience working with cloud data warehouses (e.g., BigQuery, Snowflake) and Data visualization tools (e.g., Power BI, Metabase etc).\nCollaboration: Strong communication and interpersonal skills for working with Product Managers, Solution Architects, and technical teams.\nBusiness Acumen: Translating business questions into technical requirements and analytical outputs.\nProblem-Solving: Demonstrated capacity to handle large data sets, uncover insights, and make data-driven recommendations.\nAgile Mindset: Familiarity with Agile methodologies, sprint planning, and iterative development cycles.",Software,"['Data Analyst', 'Advanced SQL skills', 'Data Analysis Tools', 'Problem-Solving skills', 'Agile Methodologies', 'Sprint Planning']",2025-06-12 15:20:35
Senior Data Analyst,GreyOrange,4-5 Years,,Gurugram,"Roles and Responsibilities :\nWe are seeking a highly motivated Senior Data Analyst to join our team. In this role, you will write complex SQL queries to power analytics dashboards and collaborate with cross-functional teams Product Managers, Solution Architects, and Data Engineers to deliver end-to-end analytics solutions. The ideal candidate is skilled in translating business requirements into technical insights, ensuring that data is readily available and consumable for reporting and decision-making.\nKey Responsibilities\nAnalytics Dashboarding\nWrite and optimize SQL queries to support real-time analytics dashboards and reporting solutions.\nConduct data validation and troubleshooting issues related to data accuracy and integrity.\nCross-Functional Collaboration\nWork closely with Product Managers and Solution Architects to identify customer analytics needs and define project scope.\nPartner with Data Engineers and other technical teams to ensure data is accessible, reliable, and aligned with business requirements.\nData Management Delivery\nCollaborate with stakeholders to gather, document, and prioritize analytics requirements.\nDefine data requirements for data pipelines, ensuring end-to-end delivery of analytics solutions.\nPerformance Optimization\nImplement best practices in query optimization to handle large, complex data sets.\nMonitor and enhance the performance of analytical processes and data models.\nData Quality Governance\nEstablish and maintain data quality metrics and governance standards.\nIdentify data gaps and work with engineering teams to implement corrective actions.\nReporting Insights\nDevelop and maintain dashboards, reports, and ad hoc analyses to inform business decisions.\nTranslate findings into actionable recommendations and present them to stakeholders at various levels.\nQualifications Skills:\nEducation: Bachelor s or master s degree in computer science, Information Systems, Statistics, or a related field.\n4-5 years of relevant experience.\nSQL Expertise: Advanced SQL skills with a proven track record of writing, optimizing, and troubleshooting complex queries. Knowledge of programming languages used for data manipulation.\nData Analysis Tools: Experience working with cloud data warehouses (e.g., BigQuery, Snowflake) and Data visualization tools (e.g., Power BI, Metabase etc).\nCollaboration: Strong communication and interpersonal skills for working with Product Managers, Solution Architects, and technical teams.\nBusiness Acumen: Translating business questions into technical requirements and analytical outputs.\nProblem-Solving: Demonstrated capacity to handle large data sets, uncover insights, and make data-driven recommendations.\nAgile Mindset: Familiarity with Agile methodologies, sprint planning, and iterative development cycles.",Software,"['Data Analyst', 'Advanced SQL skills', 'Data Analysis Tools', 'Problem-Solving skills', 'Agile Methodologies', 'Sprint Planning']",2025-06-12 15:20:36
Data Analyst,Turtlemint,0-3 Years,,Pune,"Using automated tools to extract data from primary and secondary sources\nRemoving corrupted data and fixing coding errors and related problems\nDeveloping and maintaining databases, data systems - reorganizing data in a readable format\nData sourcing through various tools, Data export and import in excel, pooling all the data from market source as per companies requirement.\nDatabase Presentation through dashboards and diagrams.\nMarket study and Analysis as per the Company requirement\nCompetitor's business strategy and rate analysis study.\nPerforming analysis to assess quality and meaning of data\nFilter Data by reviewing reports and performance indicators to identify and correct code problems\nUsing statistical tools to identify, analyse, and interpret patterns and trends in complex data sets that could be helpful for the diagnosis and prediction\nAssigning numerical value to essential business functions so that business performance can be assessed and compared over periods of time.\nAnalyzing local, national, and global trends that impact both the organization and the industry\nPreparing reports for the management stating trends, patterns, and predictions using relevant data\nPreparing final analysis reports for the management to understand the data-analysis steps, enabling them to take important decisions based on various facts and trends.\nSkills Qualifications:\nProven working experience in Data Analyst\nSolid knowledge of website analytics tools (e.g., Google Analytics, Web Trends)\nStrong analytical skills and data-driven thinking.",Insurance,"['Data Analysis', 'Business Strategy', 'Rate Analysis', 'Google Analytics', 'Coding', 'Database']",2025-06-12 15:20:38
Data Management - Data Analyst-IT I.,Systechcorp Private Limited,1-6 Years,,Bengaluru,"Key Responsibilities\nDesign and develop scalable data pipelines for HR analytics and reporting\nIntegrate data from Oracle HR systems and third-party sources\nOptimize SQL queries and ETL processes for performance and accuracy\nCollaborate with HR analysts and data scientists to deliver reliable datasets\nEnsure data quality, consistency, and security across platforms\nDocument workflows, data models, and technical processes\nRequired Qualifications\n1+ years of experience in data engineering, data analytics, or related field\nStrong SQL skills and familiarity with ETL tools like ODI\nExperience with Python, R, or similar scripting languages\nKnowledge of data warehousing concepts and relational databases (e.g., Oracle, MySQL)\nUnderstanding of HR data (e.g., employee lifecycle, organization hierarchy) is a plus\nBachelor s degree in Computer Science, Information Systems, Data Science, or related field\nPreferred Qualifications\nExperience working with Oracle Cloud Infrastructure (OCI)\nFamiliarity with Oracle HCM Cloud or other HRIS platforms\nInterest in using AI and machine learning to improve HR processes",Software,"['Hr', 'Sql', 'Etl', 'Odi', 'python', 'oracle']",2025-06-12 15:20:39
Data Analyst,Turtlemint,0-3 Years,,Bengaluru,"Using automated tools to extract data from primary and secondary sources\nRemoving corrupted data and fixing coding errors and related problems\nDeveloping and maintaining databases, data systems - reorganizing data in a readable format\nData sourcing through various tools, Data export and import in excel, pooling all the data from market source as per companies requirement.\nDatabase Presentation through dashboards and diagrams.\nMarket study and Analysis as per the Company requirement\nCompetitor's business strategy and rate analysis study.\nPerforming analysis to assess quality and meaning of data\nFilter Data by reviewing reports and performance indicators to identify and correct code problems\nUsing statistical tools to identify, analyse, and interpret patterns and trends in complex data sets that could be helpful for the diagnosis and prediction\nAssigning numerical value to essential business functions so that business performance can be assessed and compared over periods of time.\nAnalyzing local, national, and global trends that impact both the organization and the industry\nPreparing reports for the management stating trends, patterns, and predictions using relevant data\nPreparing final analysis reports for the management to understand the data-analysis steps, enabling them to take important decisions based on various facts and trends.\nSkills Qualifications:\nProven working experience in Data Analyst\nSolid knowledge of website analytics tools (e.g., Google Analytics, Web Trends)\nStrong analytical skills and data-driven thinking.",Insurance,"['Data Analysis', 'Business Strategy', 'Rate Analysis', 'Google Analytics', 'Coding', 'Database']",2025-06-12 15:20:40
Data Management - Data Analyst-IT I.,Systechcorp Private Limited,1-6 Years,,"Hyderabad, Chennai, Pune","Key Responsibilities\nDesign and develop scalable data pipelines for HR analytics and reporting\nIntegrate data from Oracle HR systems and third-party sources\nOptimize SQL queries and ETL processes for performance and accuracy\nCollaborate with HR analysts and data scientists to deliver reliable datasets\nEnsure data quality, consistency, and security across platforms\nDocument workflows, data models, and technical processes\nRequired Qualifications\n1+ years of experience in data engineering, data analytics, or related field\nStrong SQL skills and familiarity with ETL tools like ODI\nExperience with Python, R, or similar scripting languages\nKnowledge of data warehousing concepts and relational databases (e.g., Oracle, MySQL)\nUnderstanding of HR data (e.g., employee lifecycle, organization hierarchy) is a plus\nBachelor s degree in Computer Science, Information Systems, Data Science, or related field\nPreferred Qualifications\nExperience working with Oracle Cloud Infrastructure (OCI)\nFamiliarity with Oracle HCM Cloud or other HRIS platforms\nInterest in using AI and machine learning to improve HR processes",Software,"['Hr', 'Sql', 'Etl', 'Odi', 'python', 'oracle']",2025-06-12 15:20:42
Data Analyst,Harman Connected Services Corporation India Private Limited,9-12 Years,,Bengaluru,"Apply mathematical, problem-solving, and coding skills to manage big data, extracting valuable insights and making key recommendations.\nDesign and build tailor-made solutions from this data, aiding the Area Learning Leadership (ALL) in achieving their unique objectives and goals.\nWork closely with ALL nominated stakeholders to understand their business problems and objectives and design data modelling processes, create algorithms and build models to address.\nUndertake data collection, preprocessing and analysis.\nPresent information using data visualization techniques specifically Microsoft Power BI (PBI)\nSkills required.\nProven experience as a Data Scientist or Data Analyst\nExperience in data mining.\nConnect to and integrate data from multiple sources, including OLAP cubes.\nDesign, develop, and maintain Power BI dashboards and Power Apps.\nUnderstanding of machine-learning and operations research\nKnowledge of R, SQL and Python; familiarity with Scala, Java or C++ is an asset.\nExperience using business intelligence tools and data frameworks specifically require deep skills in Microsoft Power BI (PBI)\nAnalytical mind and business acumen\nProblem-solving aptitude\nSolid communication skills\nPrior experience working at Microsoft (preferred)\nBSc/BA in Computer Science, Engineering or relevant field; graduate degree in Data Science or other quantitative field is preferred.",Software,"['Data Analyst', 'Scala', 'Java']",2025-06-12 15:20:43
Senior Data Analyst - (Informatica ETL + DMX + Alteryx + AWS),Harman Connected Services Corporation India Private Limited,4-9 Years,,Pune,"HARMAN s engineers and designers are creative, purposeful and agile. As part of this team, you ll combine your technical expertise with innovative ideas to help drive cutting-edge solutions in the car, enterprise and connected ecosystem. Every day, you will push the boundaries of creative design, and HARMAN is committed to providing you with the opportunities, innovative technologies and resources to build a successful career.A Career at HARMANAs a technology leader that is rapidly on the move, HARMAN is filled with people who are focused on making life better. Innovation, inclusivity and teamwork are a part of our DNA. When you add that to the challenges we take on and solve together, you ll discover that at HARMAN you can grow, make a difference and be proud of the work you do everyday.HARMAN is proud to be an Equal Opportunity / Affirmative Action employer. All qualified applicants will receive consideration for employment without regard torace, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.",Software,"['Dmx', 'Etl', 'AWS']",2025-06-12 15:20:45
Assistant Manager / Deputy Manager – Data Analyst,Bosch India,4-6 Years,,Bengaluru,"Data collection and preprocessing: Gather, clean, and preprocess large datasets from various sources, ensuring data quality, consistency, and accuracy.\nML analysis and modeling: Apply ML techniques and algorithms to analyze data, build predictive models, and identify patterns and trends that can drive actionable insights.\nData visualization: Create compelling data visualizations, dashboards, and reports using visualization tools (such as Tableau, Power BI, or Python libraries) to effectively communicate data-driven insights to stakeholders.\nData management: Develop and implement data management strategies, including data cleansing, transformation, and integration, to ensure data integrity, accessibility, and availability.\nStatistical analysis: Conduct statistical analysis, hypothesis testing, and data mining techniques to uncover meaningful insights and trends in complex datasets.\nPerformance monitoring: Monitor and evaluate the performance of ML models, identify areas for improvement, and provide recommendations to optimize model accuracy and efficiency.\nData-driven decision-making: Collaborate with business stakeholders to define KPIs, develop data-driven strategies, and provide insights that contribute to informed decision-making.\nDocumentation and reporting: Document data analysis methodologies, findings, and insights in clear and concise reports, presentations, and documentation for both technical and non-technical audiences.\nContinuous learning: Stay updated with the latest trends and advancements in ML, data management, visualization techniques, and actively seek opportunities to enhance your skills and knowledge by keeping regular connects with universities on these topics.\nSeamlessly collaborate across the Sales & Marketing organization and other departments to develop and execute Analytical projects\nQualifications\nEducation: Bachelor's degree in Computer Science, Data Science, Statistics, Mathematics, or a related field. A master's degree is a plus\nExperience:4-6 yearsof experience of professional data science. 2-3 years (in case of a Masters degree holder)\nKnow-How: Extensive know-how of Python, (R is a plus) and in object-oriented programming languages with high code quality (Clean Code). Proven track record in machine learning, neural networks, pattern analysis, time series forecasting, data analysis as well as data-pipeline technologies (e.g., Kubernetes, Docker, NoSQL-Databases, Workflow-Engine). Strong experience with data visualization tools and libraries, such as Tableau, Power BI, Matplotlib, or Plotly. Solid understanding of statistics; knowledge of Cloud Technologies (ideally Microsoft Azure) and SQL are beneficial\nLanguages: Fluent in English, written and spoken. Good communication and presentation skills\nOrganizational skills, with the ability to handle multiple initiatives simultaneously\nAbility to work independently, manage teams, partners, Interns and be part of a team\nIf you are passionate about data analysis, ML, and data visualization and have the required experience and skills, we invite you to apply for this exciting opportunity. Join our team and make a significant impact by leveraging data to drive strategic decision-making and contribute to the organization's success.\nAdditional Information\nWill be part of a dynamic and passionate team who are responsible to shape the strategy of Bosch's Mobility Business in India.\nWe are an equal opportunity employer and are seeking diversity in all aspects\nOpportunity to work with Global marketing and strategy teams of Bosch","Industrial Manufacturing, Mechanical Design, Manufacturing","['Neural Network', 'python', 'Machine Learning']",2025-06-12 15:20:46
Data Analyst Engineer,KPIT Technologies Limited,3-4 Years,,Bengaluru,"Job Description: Engineer - Powertrain SW Development\nDepartment: RD/CEP - Powertrain Data Analytics\nJob Area: Engineer - Powertrain SW Development\nQualification: Bachelor's degree in Electronics, Electrical, Mechatronics, Embedded Systems, or Instrumentation Engineering\nExperience: 3 to 4 years\nWork Location: Bangalore (Only)\nMandatory Skills:\nDevelopment experience with:\no Python (Mandatory)\no R (M)\no SQL (M)\no Matlab (M)\nWork experience in advance statistical analysis/modeling/data mining/Machine Learning (M)\nStatistical modeling (Regression, Classification, Time-series forecasting) / Machine Learning / Text Mining / Optimization / Visualization (M)\nGerman Language and knowledge of German culture is an added advantage\nJob Responsibilities:\n1. Individual contributor role with 3-4 years of experience in software design and development using the above skills.\n2. Analyze data, develop solutions using available tools, and create, analyze, and maintain explanatory/predictive statistical models.\n3. Coordinate with different Subject Matter Experts (SMEs) to explore data extraction approaches and roadmap analysis milestones.\n4. Develop and maintain working relationships with key customer stakeholders.\n5. Ability to work with data, moving it from databases or APIs, through transformations, to models, and into human-readable forms (e.g., ROC curves, Excel charts, maps, d3 visualizations, Tableau, etc.).",Software,"['Mechatronics', 'Data Analyst Engineer', 'Data Analytics', 'Sql']",2025-06-12 15:20:58
Data Analyst - Sr. Software Engineer,KPIT Technologies Limited,3-7 Years,,Bengaluru,"Keywords:Statistical analysis, Data Visualization, Dashboards, Root Cause Analysis, EDA Exploratory Data analysis\nDescription:A data analyst who can fetch required data from field vehicle data tables and analyze as per the use cases by using various data visualization techniques\nResponsibilities:\nAnalysis of field car data of e motors\nUsage of different data pipelines and get required data\nData cleaning and plausibility checks\nStatistical analysis as per the requirement\nBuild and maintain dashboards\nCollaborating with Global stakeholders and Product owners\nSkills/Qualifications/Experience:\nBachler's or Master's degree in any STEM disciplines\nMicrosoft Azure\nStatistics\nPython\nTableau\nSQL\nOptional skills (good to have) Databricks, Power BI, Machine Learning, Automotive Powertrain domain knowledge\nSoft Skills: Excellent written and verbal communication skills, Flexible, Self-driven, Team Player",Software,"['Data Analyst - Sr. Software Engineer', 'Root Cause Analysis', 'Eda']",2025-06-12 15:21:02
Data Analyst - Sr. Software Engineer,KPIT Technologies Limited,3-7 Years,,Bengaluru,"Keywords:Statistical analysis, Data Visualization, Dashboards, Root Cause Analysis, EDA Exploratory Data analysis\nDescription:A data analyst who can fetch required data from field vehicle data tables and analyze as per the use cases by using various data visualization techniques\nResponsibilities:\nAnalysis of field car data of e motors\nUsage of different data pipelines and get required data\nData cleaning and plausibility checks\nStatistical analysis as per the requirement\nBuild and maintain dashboards\nCollaborating with Global stakeholders and Product owners\nSkills/Qualifications/Experience:\nBachler's or Master's degree in any STEM disciplines\nMicrosoft Azure\nStatistics\nPython\nTableau\nSQL\nOptional skills (good to have) Databricks, Power BI, Machine Learning, Automotive Powertrain domain knowledge\nSoft Skills: Excellent written and verbal communication skills, Flexible, Self-driven, Team Player",Software,"['Data Analyst - Sr. Software Engineer', 'Root Cause Analysis', 'Eda']",2025-06-12 15:21:03
DATA ANALYST - MEDICAL AFFAIRS,Lilly,7-9 Years,,"Bengaluru, India","Position Description:\nThe purpose of this role is to serve as a trusted partner with Business Unit Medical Affairs and Internal Global Medical Affairs Organization (GMAO) teams to lead creation of high-quality data reporting and visualization support that can drive better customer experience and business impact. This role will champion our self-service reporting strategy and would play a key role in helping us automate and create scalable frameworks for our reporting and analytics. We are looking for a hands-on person who can help expand our analytics & reporting capabilities and drive business-critical initiatives.\nKey Objectives/Deliverables:\nKnow Lilly TA business and our internal business partners.\nBuild and exhibit deep expertise on available data sets and supports data enabled decision making by developing data lakes, insights, reporting & visualization\nExecute and monitor operations tasks to ensure timely availability of data in a reporting / dashboard structure to the business.\nPerform thorough data validations to ensure data quality\nRespond to queries from internal stakeholders\nConsistently meet operations SLAs\nPerform incident resolution and root cause analysis to support data and reporting operations\nConsistent delivery of high quality, timely and insightful reports to enable stakeholders and senior leadership to take key decisions\nDevelop and publish regularly, different execution dashboard as per the business roadmap & requirements\nDescriptive analytics and visualization to provide data-based insights on planning, execution and outcomes\nDemonstrate deep understanding of information and material flows, processes, procedures, systems, and methods\nDemonstrate understanding of internal business partnersu2019 people, processes, and technology\nPartner and collaborate with other site-level teams to identify synergies and implementation of best practices\nTechnical Skills\nExpertise in writing and debugging efficient SQL queries.\nStrong experience in data visualization tools - Power BI or Tableau (Power BI preferred) u2013 Should be able to independently design and develop dashboards as per business requirement.\nAdvanced MS-office skills (MS-Excel and MS-PowerPoint)\nCoding: SQL mandatory and one of R, Python would be good to have\nAnalytical Skills\nExperience in business analytics\nData cleaning and preparation skill (database querying, descriptive statistics)\nProblem solving skills and lateral thinking ability and an eye for detail\nEducational Requirements:\nBacheloru2019s or Masteru2019s degree in sciences or quantitative discipline i.e. Finance, Econometrics, Statistics, Engineering or Computer Sciences\nAdditional Preferences:\nAt least 7-9 years of evolving experience in data management, pharma market intelligence, performance reporting/visualization and/or descriptive analytics for leadership, with demonstrated results in understanding, structuring, and making sense of unfamiliar and messy datasets\nExperience with project management software (e.g., Wrike, JIRA, Adobe Workfront) and proficiency in a variety of PC applications and multifunctional diagramming tools including Microsoft Project, Visio, Lucid Chart etc.\nStrong work ethic and personal motivation\nInterpersonal and communication skills with ability to work across time zones.\nStrong stakeholder management skills\nAbility to operate effectively in an international matrix environment.\nStrong team player who is dynamic and result oriented\nProven planning and organizational skills\nProven ability to manage multiple projects at a time with flexibility to adjust quickly and effectively to frequent change and altered priorities\nProduct Launch experience\nDemonstrated enthusiasm and the ability to work under pressure to meet deadlines\nLilly is dedicated to helping individuals with disabilities to actively engage in the workforce, ensuring equal opportunities when vying for positions. If you require accommodation to submit a resume for a position at Lilly, please complete the accommodation request form () for further assistance. Please note this is for individuals to request an accommodation as part of the application process and any other correspondence will not receive a response.\nLillyu00A0does not discriminate on the basis of age, race, color, religion, gender, sexual orientation, gender identity, gender expression, national origin, protected veteran status, disability or any other legally protected status.\n#WeAreLilly",Pharmaceutical,"['Lucid Chart', 'Visio', 'MS-PowerPoint', 'Microsoft Project', 'Performance Reporting', 'descriptive analytics', 'R', 'MS-Excel', 'Data Management', 'Power Bi', 'Tableau', 'Sql', 'Python', 'project management software']",2025-06-12 15:21:06
Senior Data Analyst,OnlineSales.ai,2-4 Years,,"Pune, India","About OnlineSales.ai\nBuilt byex-Amazon ad-tech experts,OnlineSales.aioffers a future-proof Retail Media Operating System - boosting Retailer's profitability by 7% of Sales! We are an Enterprise B2B SaaS startup, based out of Pune India. With OnlineSales.ai's platform, retailers activate and delight 10x more Brands by offering an omni-channel media buying experience, advanced targeting, analytics & 2x better ROAS. Tier 1 Retailers and Marketplaces globally are accelerating their Monetization strategy with OnlineSales.ai and are innovating ahead of the market by at least 2 years.\nAbout theRole\nWe are seeking a talented and motivated individual to join our team as a Senior Data Analyst who will be responsible for extracting insights from complex datasets to drive informed decision-making and enhance business performance. You will collaborate closely with cross-functional teams to identify key metrics, develop data-driven strategies, and provide actionable recommendations. Additional responsibilities may include managing daily regulatory reporting tasks and remediation activities, as well as process improvement.\nWhat will you do @OnlineSales\nData Analysis: Utilize advanced analytical techniques to explore large datasets, identify trends, patterns, and anomalies, and extract actionable insights.\nData Visualization: Create visually compelling dashboards and reports to communicate findings effectively to stakeholders, enabling them to make informed decisions.\nData Extraction: regular extraction of relevant data from internal databases using SQL queries. Design and optimize SQL queries to retrieve specific datasets required for performance analysis and reporting\nIssue Identification: Proactively identify performance-related issues by monitoring key performance indicators (KPIs), analyzing trends, and investigating anomalies reported by internal stakeholders or external clients.\nAddressing Client Exceptions and Issues: Responsively address performance-related exceptions and issues raised by clients, ensuring timely resolution and effective communication throughout the process. Collaborate with client-facing teams to understand client requirements, prioritize tasks, and deliver solutions that meet or exceed client expectations.\nRoot Cause Analysis: Dive deep into data to understand the root causes of performance issues, considering factors such as system architecture, infrastructure, code efficiency, and user behavior.\nHypothesis Testing: Apply hypothesis testing techniques to validate assumptions and identify statistically significant factors impacting performance.\nDocumentation and SOP Creation: Create clear and detailed Standard Operating Procedures (SOPs) outlining the process for diagnosing, troubleshooting, and resolving performance issues. Ensure that documentation is organized, easily accessible, and regularly updated to reflect changes in systems, processes, or configurations.\nCross-Functional Collaboration: Collaborate with teams across the organization, including business development, marketing, product development and operations, to understand their data needs and provide analytical support\n\nYou will be a great fit, if you have:\n2-4 years of relevant experience.\nBachelor's or Master's degree in Computer Science, Engineering, or a related technical field.\nProficiency in SQL for data extraction and manipulation from relational databases.\nFamiliarity with programming languages such as Python for Data Analysis and Data modeling is a plus.\nStrong analytical skills with the ability to interpret complex datasets and draw meaningful insights.\nStrong problem-solving abilities with a proactive approach to troubleshooting and issue resolution.\nAdvanced proficiency in Excel and adept data manipulation skills for efficient analysis and visualization of large datasets.\nEffective communication and interpersonal skills for collaboration with cross-functional teams and stakeholders.\nUnderstanding of E-Commerce as a domain.\nExcellent documentation skills with the ability to create clear and comprehensive reports and SOPs.\nAttention to detail and commitment to data accuracy and quality. Willingness to work for a startup.\nWhy Online Sales.ai\nStartup-y. We believe Startup is a mindset. It's about being scrappy, being nimble, solving tough problems with constrained resources, and more. It's about working hard and playing hard\nEnterprise SaaS. Opportunity to work with an Enterprise Product SaaS firm with aspirations of growing 10x across the globe\nAI-led Retail Tech. We are working to digitize & democratize one of the most exciting and growing verticals - Retail Tech leveraging data, machine learning, and automation (culmination of ad-tech, mar-tech, and analytics for Retail vertical)\nMeaningful work. This is not just a job. You can find a job anywhere. This is a place for the bold to get paid who make a real impact on business\nNo red tape. Say goodbye to pointless meetings or political hoops to jump through. We're scrappy, believe in autonomy, and empower our teams to do whatever it takes to do the unthinkable\nProblem Solving. We ignite the best in you. We exist not only to deliver meaningful innovation but to ignite and inspire the creative problem-solver in you\nQuirky & fun. Enjoy new skills and hobbies like being a quiz master, playing board games, trying your hands on percussion, playing Djembe, and spreading love within the org!",Internet/E-commerce,"['Root Cause Analysis', 'Data Analysis', 'Documentation and SOP Creation', 'Data Visualization', 'Hypothesis Testing', 'Excel', 'Python', 'Sql']",2025-06-12 15:21:08
Data Analyst,TalentBasket,5-12 Years,INR 12 - 20 LPA,Cochin / Kochi / Ernakulam,"Job Purpose\nWe are seeking an experienced and analytical Senior Data Analyst to join our Data & Analytics team. The ideal\ncandidate will have a strong background in data analysis, visualization, and stakeholder communication. You\nwill be responsible for turning data into actionable insights that help shape strategic and operational decisions\nacross the organization.\nJob Description / Duties & Responsibilities\nCollaborate with business stakeholders to understand data needs and translate them into analytical\nrequirements.\nAnalyze large datasets to uncover trends, patterns, and actionable insights.\nDesign and build dashboards and reports using Power BI.\nPerform ad-hoc analysis and develop data-driven narratives to support decision-making.\nEnsure data accuracy, consistency, and integrity through data validation and quality checks.\nBuild and maintain SQL queries, views, and data models for reporting purposes.\nCommunicate findings clearly through presentations, visualizations, and written summaries.\nPartner with data engineers and architects to improve data pipelines and architecture.\nContribute to the definition of KPIs, metrics, and data governance standards.\nJob Specification / Skills and Competencies\nBachelor's or Master's degree in Statistics, Mathematics, Computer Science,\nEconomics, or a related field.\n5+ years of experience in a data analyst or business intelligence role.\nAdvanced proficiency in SQL and experience working with relational databases (e.g.,\nSQL Server, Redshift, Snowflake).\nHands-on experience in Power BI.\nProficiency in Python, Excel and data storytelling.\nUnderstanding of data modelling, ETL concepts, and basic data architecture.\nStrong analytical thinking and problem-solving skills.\nExcellent communication and stakeholder management skills\nTo adhere to the Information Security Management policies and procedures.\nSoft Skills Required\nMust be a good team player with good communication skills\nMust have good presentation skills\nMust be a pro-active problem solver and a leader by self\nManage & nurture a team of data engineers",Login to check your skill match score,"['Data Analyst', 'AWS Athena', 'Power Bi', 'Python', 'Sql']",2025-06-12 15:21:09
Data Analyst,Robotics Technologies,5-10 Years,INR 16 - 32 LPA,"Ahmedabad, Bengaluru, Chennai","Description\nWe are seeking a skilled Data Analyst with 5-10 years of experience to join our team in India. The ideal candidate will have a strong background in data analysis and will be responsible for transforming data into actionable insights that drive business strategies.\nResponsibilities\nCollect, process, and analyze data to help inform business decisions.\nCreate and maintain dashboards and reports for stakeholders.\nIdentify trends and patterns in data sets to provide actionable insights.\nCollaborate with cross-functional teams to understand data needs and deliver solutions.\nEnsure data accuracy and integrity by performing regular audits and validations.\nSkills and Qualifications\nBachelor's degree in Mathematics, Statistics, Computer Science, or related field.\nProficiency in data analysis tools such as Excel, SQL, and Python/R.\nExperience with data visualization tools such as Tableau or Power BI.\nStrong analytical and problem-solving skills.\nAbility to communicate complex data findings to non-technical stakeholders.","Sales Automation, Information Technology, IT Management, GovTech, Information Services, Video Conferencing","['ETL Processes', 'Dashboard Development', 'Sql', 'Python', 'Data Visualization', 'Machine Learning', 'Statistical Analysis', 'Data Cleaning', 'Big Data', 'Predictive Modeling']",2025-06-12 15:21:11
Data Analyst,Robotics Technologies,5-10 Years,INR 7 - 20 LPA,"Bengaluru, Chennai, Noida","We are seeking a skilled Data Analyst to join our team in India. The ideal candidate will have 5-10 years of experience in data analysis, with a strong background in statistical analysis and data visualization. You will be responsible for transforming data into actionable insights that drive business performance.\nResponsibilities\nCollect, process, and analyze large datasets to derive actionable insights.\nDevelop and maintain dashboards and reports to track key performance indicators (KPIs).\nCollaborate with cross-functional teams to understand data needs and provide analytical support.\nIdentify trends and patterns in data to inform business decisions and strategy.\nCommunicate findings and insights effectively to stakeholders through presentations and visualizations.\nSkills and Qualifications\nBachelor's degree in Data Science, Statistics, Mathematics, Computer Science, or a related field.\n5-10 years of experience in data analysis or a similar role.\nProficiency in SQL for data extraction and manipulation.\nStrong knowledge of statistical analysis and data visualization tools (e.g., Tableau, Power BI, or similar).\nExperience with programming languages such as Python or R for data analysis.\nExcellent problem-solving skills and attention to detail.\nStrong communication skills, both verbal and written, to convey complex data insights clearly.",Information Technology,"['ETL Processes', 'Sql', 'Python', 'Data Visualization', 'Statistical Analysis', 'Data Warehousing', 'Machine Learning', 'Data Mining', 'R Programming', 'Big Data']",2025-06-12 15:21:12
DATA ANALYST INTERNSHIP AT AHMEDABAD,Maxgen Technologies Private Limited,Fresher,,"Ahmedabad, Mehsana, Gandhinagar","Maxgen Technologies Pvt ltd offering program in ahmedabad. Diploma and Engineering wil get data analyst internship and power bi program with live project experience .\nBenefit to jpin us\nAdvance learning program\nexperienced staff .\nattractive locations .\nskills development services .\nlive projects internships.\ncall at 9099039845\nvisit us www.maxgentechnologies.com\naddress :Abhijeet 3, 603, Netaji Rd, near Pantaloons, Mithakhali, Ellisbridge, Ahmedabad,",Information Technology,"['Data Analysis', 'Data Analyst', 'Internship', 'Power Bi']",2025-06-12 15:21:14
Hiring for Data Analyst Role,Robotics Technologies,3-10 Years,INR 5 - 15 LPA,"Ahmedabad, Bengaluru, Chennai","Description\nWe are seeking a Data Analyst with 3-10 years of experience to join our team in India. The ideal candidate will be responsible for analyzing data, creating insightful reports, and collaborating with various teams to enhance data-driven decision-making.\nResponsibilities\nAnalyze large datasets to identify trends and patterns.\nCreate and maintain dashboards and reports to monitor key metrics.\nCollaborate with cross-functional teams to understand data needs and provide actionable insights.\nAssist in the design and implementation of data collection systems and other strategies that optimize statistical efficiency and data quality.\nPerform data validation and cleaning to ensure accuracy and reliability of data.\nSkills and Qualifications\nProficiency in SQL for data manipulation and querying.\nExperience with data visualization tools such as Tableau, Power BI, or similar.\nStrong analytical skills with the ability to interpret complex data sets.\nKnowledge of statistical analysis and data modeling techniques.\nFamiliarity with programming languages like Python or R for data analysis.\nExcellent problem-solving skills and attention to detail.\nStrong communication skills to convey findings effectively.",IT Management,"['ETL Processes', 'Dashboard Development', 'Sql', 'Python', 'Data Visualization', 'Statistical Analysis', 'Machine Learning', 'Excel', 'Data Mining', 'Big Data']",2025-06-12 15:21:16
Data Analyst,Robotics Technologies,3-7 Years,INR 6 - 16 LPA,"Hyderabad, Chennai, Bengaluru","Description\nWe are seeking a skilled Data Analyst to join our team in India. The ideal candidate will have a strong background in data analysis and visualization, with a proven ability to transform data into actionable insights that drive business growth.\nResponsibilities\nCollect, process, and analyze large datasets to derive actionable insights.\nDevelop and maintain dashboards and reports to track key performance indicators.\nCollaborate with cross-functional teams to understand data needs and provide analytical support.\nIdentify trends and patterns in data to inform business decisions.\nPresent findings and recommendations to stakeholders in a clear and concise manner.\nSkills and Qualifications\nBachelor's degree in Data Science, Statistics, Mathematics, Computer Science, or related field.\n3-7 years of experience as a Data Analyst or in a similar role.\nProficiency in SQL for database querying and data manipulation.\nExperience with data visualization tools such as Tableau, Power BI, or similar.\nStrong analytical skills with the ability to work with large datasets and derive meaningful insights.\nKnowledge of programming languages such as Python or R for data analysis.\nFamiliarity with statistical analysis and machine learning concepts.\nExcellent communication skills to present complex information in an understandable format.","Information Technology, Virtualization, Data Integration, IT Infrastructure","['ETL Processes', 'Excel Advanced', 'Sql', 'Python', 'Data Visualization', 'Statistical Analysis', 'Machine Learning', 'Data Warehousing', 'Power Bi', 'R Programming']",2025-06-12 15:21:17
Data Analyst,Robotics Technologies,5-10 Years,INR 7 - 20 LPA,"Chennai, Bengaluru, Pune","We are seeking an experienced Data Analyst to join our dynamic team in India. The ideal candidate will be responsible for analyzing complex data sets, generating insights, and supporting data-driven decision-making across the organization.\nResponsibilities\nCollect, analyze, and interpret complex data sets to identify trends and insights.\nCreate and maintain dashboards and reports to communicate findings to stakeholders.\nCollaborate with cross-functional teams to understand data requirements and provide analytical support.\nPerform data validation and cleansing to ensure accuracy and reliability of data.\nDevelop and implement data models to support business decision-making.\nSkills and Qualifications\nBachelor's degree in Data Science, Statistics, Mathematics, Computer Science, or a related field.\n5-10 years of experience in data analysis or a related field.\nProficiency in SQL for data extraction and manipulation.\nStrong knowledge of data visualization tools such as Tableau, Power BI, or similar.\nExperience with programming languages such as Python or R for data analysis and modeling.\nFamiliarity with statistical analysis and machine learning techniques.\nExcellent problem-solving skills and attention to detail.\nStrong communication skills to present findings to non-technical stakeholders.",Information Technology,"['ETL Processes', 'Excel Expertise', 'Business Intelligence', 'Sql', 'Python', 'Data Visualization', 'Statistical Analysis', 'Machine Learning', 'Data Mining', 'Database Management']",2025-06-12 15:21:19
Data Analyst,Robotics Technologies,4-8 Years,INR 12 - 22.5 LPA,"Thane, Mumbai City, Noida","Description\nWe are seeking a skilled Data Analyst with 4-8 years of experience to join our team in India. The ideal candidate will have a strong background in data analysis and visualization, and will be responsible for transforming data into actionable insights to drive business decisions.\nResponsibilities\nCollecting, processing, and analyzing large datasets to identify trends and insights.\nDeveloping and maintaining dashboards and reports to communicate findings to stakeholders.\nCollaborating with cross-functional teams to understand data requirements and deliver actionable insights.\nConducting statistical analysis and modeling to support decision-making processes.\nEnsuring data quality and integrity through validation and cleansing processes.\nSkills and Qualifications\nProficiency in data analysis tools such as SQL, Python, or R.\nStrong experience with data visualization tools like Tableau or Power BI.\nSolid understanding of statistical analysis and methodologies.\nExcellent problem-solving skills and attention to detail.\nAbility to communicate complex data insights clearly to non-technical stakeholders.\nBachelor's degree in Data Science, Statistics, Mathematics, or a related field.",Information Technology,"['ETL Processes', 'Excel Advanced', 'Dashboard Creation', 'Sql', 'Python', 'Data Visualization', 'Statistical Analysis', 'Machine Learning', 'Data Mining', 'Big Data Technologies']",2025-06-12 15:21:21
Data Analyst,Robotics Technologies,4-8 Years,INR 8 - 20.5 LPA,"Gurugram, Delhi, Kolkata","Description\nWe are seeking a Data Analyst with 4-8 years of experience to join our team in India. The ideal candidate will be responsible for analyzing data to help drive business decisions, creating reports, and collaborating with various departments to ensure data-driven strategies.\nResponsibilities\nCollect, process, and analyze data to support business decisions.\nDevelop and maintain dashboards and reports to visualize key metrics.\nIdentify trends and patterns in data sets to inform strategy.\nCollaborate with cross-functional teams to understand data needs and deliver insights.\nEnsure data quality and integrity through regular audits and validation processes.\nPresent findings and recommendations to stakeholders in a clear and concise manner.\nSkills and Qualifications\nBachelor's degree in Data Science, Statistics, Computer Science, or a related field.\nProficiency in SQL for data extraction and manipulation.\nExperience with data visualization tools such as Tableau, Power BI, or similar.\nStrong analytical skills with the ability to interpret complex data sets.\nFamiliarity with statistical analysis tools and techniques.\nKnowledge of programming languages such as Python or R is an advantage.\nExcellent communication skills to convey data insights effectively.","Cloud Computing, IaaS, Virtualization, Machine Learning, Android, Data Storage","['ETL Processes', 'Excel Modeling', 'Business Intelligence', 'Dashboard Development', 'Sql', 'Python', 'Data Visualization', 'Statistical Analysis', 'Machine Learning', 'Data Mining']",2025-06-12 15:21:23
Data Analyst,Robotics Technologies,3-7 Years,INR 15.5 - 31 LPA,"Ahmedabad, Bengaluru, Chennai","Description\nWe are seeking a skilled Data Analyst to join our team in India. The ideal candidate will have a strong background in data analysis and the ability to translate data into meaningful insights that drive business decisions.\nResponsibilities\nAnalyze and interpret complex datasets to provide actionable insights.\nCreate and maintain dashboards and reports to track key performance indicators (KPIs).\nCollaborate with cross-functional teams to identify opportunities for process improvements.\nPerform data cleaning and validation to ensure data accuracy and integrity.\nPresent findings and recommendations to stakeholders in a clear and concise manner.\nSkills and Qualifications\nBachelor's degree in Data Science, Statistics, Computer Science, or a related field.\n3-7 years of experience in data analysis or a related field.\nProficiency in SQL for data extraction and manipulation.\nExperience with data visualization tools such as Tableau, Power BI, or similar.\nStrong analytical and problem-solving skills with attention to detail.\nKnowledge of programming languages such as Python or R for data analysis.\nAbility to work independently and manage multiple projects simultaneously.","Document Management, E-Signature, Data Integration, Social CRM, Video Conferencing","['ETL Processes', 'Excel Advanced', 'R', 'Sql', 'Python', 'Data Visualization', 'Statistical Analysis', 'Machine Learning', 'Data Warehousing', 'Tableau']",2025-06-12 15:21:24
Data Analyst,Robotics Technologies,5-10 Years,INR 7 - 20 LPA,"Chennai, Bengaluru, Pune","We are seeking an experienced Data Analyst to join our dynamic team in India. The ideal candidate will be responsible for analyzing complex data sets, generating insights, and supporting data-driven decision-making across the organization.\nResponsibilities\nCollect, analyze, and interpret complex data sets to identify trends and insights.\nCreate and maintain dashboards and reports to communicate findings to stakeholders.\nCollaborate with cross-functional teams to understand data requirements and provide analytical support.\nPerform data validation and cleansing to ensure accuracy and reliability of data.\nDevelop and implement data models to support business decision-making.\nSkills and Qualifications\nBachelor's degree in Data Science, Statistics, Mathematics, Computer Science, or a related field.\n5-10 years of experience in data analysis or a related field.\nProficiency in SQL for data extraction and manipulation.\nStrong knowledge of data visualization tools such as Tableau, Power BI, or similar.\nExperience with programming languages such as Python or R for data analysis and modeling.\nFamiliarity with statistical analysis and machine learning techniques.\nExcellent problem-solving skills and attention to detail.\nStrong communication skills to present findings to non-technical stakeholders.",Information Technology,"['ETL Processes', 'Excel Expertise', 'Business Intelligence', 'Sql', 'Python', 'Data Visualization', 'Statistical Analysis', 'Machine Learning', 'Data Mining', 'Database Management']",2025-06-12 15:21:36
Data Analyst,Robotics Technologies,4-8 Years,INR 8 - 20.5 LPA,"Gurugram, Delhi, Kolkata","Description\nWe are seeking a Data Analyst with 4-8 years of experience to join our team in India. The ideal candidate will be responsible for analyzing data to help drive business decisions, creating reports, and collaborating with various departments to ensure data-driven strategies.\nResponsibilities\nCollect, process, and analyze data to support business decisions.\nDevelop and maintain dashboards and reports to visualize key metrics.\nIdentify trends and patterns in data sets to inform strategy.\nCollaborate with cross-functional teams to understand data needs and deliver insights.\nEnsure data quality and integrity through regular audits and validation processes.\nPresent findings and recommendations to stakeholders in a clear and concise manner.\nSkills and Qualifications\nBachelor's degree in Data Science, Statistics, Computer Science, or a related field.\nProficiency in SQL for data extraction and manipulation.\nExperience with data visualization tools such as Tableau, Power BI, or similar.\nStrong analytical skills with the ability to interpret complex data sets.\nFamiliarity with statistical analysis tools and techniques.\nKnowledge of programming languages such as Python or R is an advantage.\nExcellent communication skills to convey data insights effectively.","Cloud Computing, IaaS, Virtualization, Machine Learning, Android, Data Storage","['ETL Processes', 'Excel Modeling', 'Business Intelligence', 'Dashboard Development', 'Sql', 'Python', 'Data Visualization', 'Statistical Analysis', 'Machine Learning', 'Data Mining']",2025-06-12 15:21:41
Data Analyst,Robotics Technologies,4-8 Years,INR 8 - 20.5 LPA,"Gurugram, Delhi, Kolkata","Description\nWe are seeking a Data Analyst with 4-8 years of experience to join our team in India. The ideal candidate will be responsible for analyzing data to help drive business decisions, creating reports, and collaborating with various departments to ensure data-driven strategies.\nResponsibilities\nCollect, process, and analyze data to support business decisions.\nDevelop and maintain dashboards and reports to visualize key metrics.\nIdentify trends and patterns in data sets to inform strategy.\nCollaborate with cross-functional teams to understand data needs and deliver insights.\nEnsure data quality and integrity through regular audits and validation processes.\nPresent findings and recommendations to stakeholders in a clear and concise manner.\nSkills and Qualifications\nBachelor's degree in Data Science, Statistics, Computer Science, or a related field.\nProficiency in SQL for data extraction and manipulation.\nExperience with data visualization tools such as Tableau, Power BI, or similar.\nStrong analytical skills with the ability to interpret complex data sets.\nFamiliarity with statistical analysis tools and techniques.\nKnowledge of programming languages such as Python or R is an advantage.\nExcellent communication skills to convey data insights effectively.","Cloud Computing, IaaS, Virtualization, Machine Learning, Android, Data Storage","['ETL Processes', 'Excel Modeling', 'Business Intelligence', 'Dashboard Development', 'Sql', 'Python', 'Data Visualization', 'Statistical Analysis', 'Machine Learning', 'Data Mining']",2025-06-12 15:21:42
Data Analyst,Robotics Technologies,5-10 Years,INR 7 - 20 LPA,"Ahmedabad, Bengaluru, Chennai","We are seeking a skilled Data Analyst to join our team in India. The ideal candidate will have extensive experience in data analysis, with the ability to collect, process, and analyze large sets of data to help drive informed business decisions.\nResponsibilities\nCollect, analyze, and interpret large datasets to derive actionable insights.\nDevelop and maintain dashboards and reports to track key performance indicators (KPIs).\nCollaborate with cross-functional teams to understand their data needs and provide analytical support.\nIdentify trends and patterns in data to inform strategic business decisions.\nEnsure data accuracy and integrity by conducting regular data quality checks.\nSkills and Qualifications\nBachelor's degree in Data Science, Statistics, Computer Science, or a related field.\n5-10 years of experience in data analysis or a related role.\nProficiency in data visualization tools such as Tableau, Power BI, or similar.\nStrong knowledge of SQL for database querying and management.\nExperience with programming languages such as Python or R for data manipulation and analysis.\nFamiliarity with statistical analysis techniques and methodologies.\nExcellent problem-solving skills and attention to detail.\nStrong communication skills to present findings to stakeholders.",Information Technology,"['ETL Processes', 'Excel Advanced', 'Dashboarding Tools', 'Sql', 'Python', 'Data Visualization', 'Statistical Analysis', 'Machine Learning', 'Data Warehousing', 'R Programming']",2025-06-12 15:21:44
Data Analyst Role,Robotics Technologies,5-10 Years,INR 4 - 18 LPA,"Hyderabad, Bengaluru, Chennai","Description\nWe are seeking an experienced Data Analyst to join our team in India. The ideal candidate will have 5-10 years of experience in data analysis and will be responsible for collecting, analyzing, and interpreting data to help drive business decisions.\nResponsibilities\nCollecting and interpreting data from various sources\nDeveloping and maintaining databases and data systems\nAnalyzing data to identify trends and patterns\nCreating reports and visualizations to present findings\nCollaborating with cross-functional teams to understand data needs\nProviding insights and recommendations based on data analysis\nSkills and Qualifications\nBachelor's degree in Data Science, Statistics, Computer Science, or related field\nProficiency in SQL and database management\nExperience with data visualization tools such as Tableau or Power BI\nStrong analytical skills and attention to detail\nKnowledge of statistical analysis techniques\nFamiliarity with programming languages such as Python or R\nExcellent communication skills to present findings clearly\nProblem-solving skills and the ability to work independently",Recruiting,"['ETL Processes', 'Sql', 'Python', 'Data Visualization', 'Statistical Analysis', 'Machine Learning', 'Data Cleaning', 'Big Data', 'Excel', 'Dashboarding']",2025-06-12 15:21:45
DATA ANALYST INTERNSHIP PUNE,Maxgen Technologies Private Limited,Fresher,INR 2 - 4.5 LPA,"Jalna, Ratnagiri, Pune","Maxgen Technologies Pvt ltd offering internships in Pune .the interns will get live project internships in data analyst and power bi program.\nWhat we are offering\nprofessional services to interns.\ninspiring work culture.\nadvance technology while internship.\nonline or offline options.\nsupportive management .\ncall us 9099039845,9607007009,9975064558.\nvisit us www.maxgentechnologies.com .\nAddress 509 ,5th floor pride icon near atithi veg restaurant kharadi pune.",Information Technology,"['Data Analyst', 'Internship', 'Power Bi', 'Data Analytics']",2025-06-12 15:21:46
DATA ANALYST INTERNSHIP AT PUNE,Maxgen Technologies Private Limited,Fresher,,"Aurangabad, Dhule, Pune","Maxgen Technologies Pvt ltd offering internship in ahmedabad. candidates will get live project data analyst internship with power bi program on job placement process..\nWhy we are perfect\nQuality work services\nCollege project Program.\njob placement after program.\nprofessional working environment .\ncall us at 9099039845 .\nvisit us www.amxgentechnologies.com .\nAddress 303 shopper s plaza 4, opp. bsnl bhavan, cg road, ahmedabad, gujarat.",Information Technology,"['R', 'Statistics', 'Internship', 'Data Cleaning', 'Power Bi', 'Data Visualization', 'Tableau', 'Excel', 'Sql', 'Python', 'Etl']",2025-06-12 15:21:47
Assistant Manager- Data Analyst,Genpact,1-3 Years,,"Gurugram, Gurugram","Genpact (NYSE: G) is a global professional services and solutions firm delivering outcomes that shape the future. Our 125,000+ people across 30+ countries are driven by our innate curiosity, entrepreneurial agility, and desire to create lasting value for clients. Powered by our purpose - the relentless pursuit of a world that works better for people - we serve and transform leading enterprises, including the Fortune Global 500, with our deep business and industry knowledge, digital operations services, and expertise in data, technology, and AI.\n\n\n\nInviting applications for the role of Assistant Manager-Data Analyst\n\nWe are looking for a candidate with good analytical capability and a knowhow of general-purpose languages like R and Python would be an added advantage. The role requires strong logical reasoning skills and business intelligence. The ability to communicate effectively is essential.\n\n\nResponsibilities\n\n\n.Understand business needs and objectives to develop self-sustaining data visualization automations\n\n.Perform data analytics on large data sets to identify trends, make forecasts and generate insights\n\n.Extract, modify and standardize data from across multiple sources ensuring accuracy and timeliness of delivery\n\n.Work with Genpact teams based in multiple locations to collaborate and provide support\n\n.Understand business needs and objectives to develop self-sustaining data visualization automations\n\n.Perform data analytics on large data sets to identify trends, make forecasts and generate insights\n\n.Extract, modify and standardize data from across multiple sources ensuring accuracy and timeliness of delivery\n\n.Work with Genpact teams based in multiple locations to collaborate and provide support\n\n\nQualifications we seek in you!\n\nMinimum qualifications\n\n\n.Prior exposure to Data Analytics - should have handled automations and visualization solutions\n\n.Good command of Advanced Excel familiarity with business intelligence tools (e.g., R and Python)\n\n.Excellent communication skills\n\n.Analytical skills and strong organizational abilities\n\n.Good written and verbal English language skills\n\n.Bachelor&rsquos or master&rsquos degree\n\n\nPreferred Qualifications\n\n.At least 1-2 large scale enterprise level Power-BI Implementation\n\n.Building up Strong COE Approach\n\n.Integration with R, Python will be an added advantage\n\n.Solid grasp of tool functions, limitations, and hidden features\n\n.Certified Power-BI developer. Migration and Upgrade experience. Power BI Cluster Management and Performance tuning is added advantage.\n\n\n\n\nGenpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. Genpact is committed to creating a dynamic work environment that values respect and integrity, customer focus, and innovation. Get to know us at and on , , , and .\n\nFurthermore, please do note that Genpact does not charge fees to process job applications and applicants are not required to pay to participate in our hiring process in any other way. Examples of such scams include purchasing a 'starter kit,' paying to apply, or purchasing equipment or training.",IT/Computers - Hardware & Networking,"['R', 'Data Analytics', 'Power Bi', 'Python', 'Advanced Excel', 'Data Visualization']",2025-06-12 15:21:49
Associate Systems and Data Analyst,Boeing,5-7 Years,,"Bengaluru, India","Job Description\nJob Description\nOverview\nAs a leading global aerospace company, Boeing develops, manufactures and services commercial airplanes, defense products and space systems for customers in more than 150 countries. As a top U.S. exporter, the company leverages the talents of a global supplier base to advance economic opportunity, sustainability and community impact. Boeing's team is committed to innovating for the future, leading with sustainability, and cultivating a culture based on the company's core values of safety, quality and integrity.\n\nTechnology for today and tomorrow\nThe Boeing India Engineering & Technology Center (BIETC) is a 5500+ engineering workforce that contributes to global aerospace growth. Our engineers deliver cutting-edge R&D, innovation, and high-quality engineering work in global markets, and leverage new-age technologies such as AI/ML, IIoT, Cloud, Model-Based Engineering, and Additive Manufacturing, shaping the future of aerospace.\n\nPeople-driven culture\nAt Boeing, we believe creativity and innovation thrives when every employee is trusted, empowered, and has the flexibility to choose, grow, learn, and explore. We offer variable arrangements depending upon business and customer needs, and professional pursuits that offer greater flexibility in the way our people work. We also believe that collaboration, frequent team engagements, and face-to-face meetings bring together different perspectives and thoughts - enabling every voice to be heard and every perspective to be respected. No matter where or how our teammates work, we are committed to positively shaping people's careers and being thoughtful about employee wellbeing.With us, you can create and contribute to what matters most in your career, community, country, and world. Join us in powering the progress of global aerospace.\n\nPosition Summary\nBoeing India IT Supply Chain Systems Organization is currently looking for an Associate Software Developer - Oracle EBS to join them in their team in Bangalore, India.\n\nThis role will be based out of Bengaluru, India.\nPosition Responsibilities:\nUnderstands and develops software solutions to meet end user requirements. Ensures that application integrates with overall system architecture, utilizing standard IT lifecycle methodologies and tools..\nDevelops programs, data and process models, plans interface and writes packages for use in interfaces and conversions.\nThis position supports initiatives of Boeing India organization related to engineering excellence, employee development, customer engagement etc.\n\nEmployer will not sponsor applicants for employment visa status.\nBasic Qualifications (Required Skills/Experience):\nMinimum of 5+ years of experience developing solutions in Oracle EBS\nGood knowledge in Oracle EBS modules Inventory and Purchasing\nStrong experience in developing SQL and PL/SQL objects and RICE components.\nKnowledge of data models and web services available in Oracle EBS\nStrong technical skills to develop interface API.\nGood experience in Report builder and Forms Builder and Personalization\nGood experience in developing Web ADI and XML publisher reports.\nExperience in application and SQL performance monitoring and troubleshooting.\nExpertise in Oracle Forms, Reports, Workflow, SQL loader, Oracle Applications API's, Database concepts, SQL, PL/SQL, Unix Shell Scripts\nGood Knowledge of Oracle Apps Standards, Table Structure, Architecture and Databases.\nExpertise in creating Interfaces through Standard API's and Interface Tables and data loading.\nMust have a strong experience and knowledge of Oracle e-Business Suite SCM modules\nMust have strong technical experience on PL/SQL, XML Publisher reports, Java, Oracle Applications Framework (OAF)\nMust be able to work independently with business users and external users and responsible for design, development, Testing support, production deployment and production support.\nExcellent organization and communication skills\n\nPreferred Qualifications (Desired Skills/Experience) :\nDevelop and maintain relationships / partnerships with customers, team members, peers, and partners to develop collaborative plans and complete projects.\nDemonstrate strong written, verbal and interpersonal communication skills. Be fluent in written and spoken English and have high degree of proficiency with MS Office tools to prepare comprehensive reports, presentations, proposals, and Statements of Work.\n\nTypical Education & Experience:\nEducation/experience typically acquired through advanced education (e.g. Bachelor) and typically 5-8 years related work experience or Master's Degree with 4+ years of experience with an equivalent combination of education and experience\n\nRelocation:\nThis position does offer relocation within INDIA.\n\nApplications for this position will be accepted until Jun. 14, 2025\n\nExport Control Requirements: This is not an Export Control position.\nRelocation\nThis position offers relocation based on candidate eligibility.\nVisa Sponsorship\nEmployer will not sponsor applicants for employment visa status.\nShift\nShift 1 - Morning (India)\n\nEqual Opportunity Employer:\nWe are an equal opportunity employer. We do not accept unlawful discrimination in our recruitment or employment practices on any grounds including but not limited to race, color, ethnicity, religion, national origin, gender, sexual orientation, gender identity, age, physical or mental disability, genetic factors, military and veteran status, or other characteristics covered by applicable law.\nWe have teams in more than 65 countries, and each person plays a role in helping us become one of the world's most innovative, diverse and inclusive companies. We are proud members of the and welcome applications from candidates with disabilities. Applicants are encouraged to share with our recruitment team any accommodations required during the recruitment process. Accommodations may include but are not limited to: conducting interviews in accessible locations that accommodate mobility needs, encouraging candidates to bring and use any existing assistive technology such as screen readers and offering flexible interview formats such as virtual or phone interviews.",Banking/Accounting/Financial Services,"['Architecture', 'Oracle Apps Standards', 'Troubleshooting', 'Table Structure', 'data models', 'Forms Builder', 'Interface API', 'Java', 'Unix Shell Scripts', 'Rice Components', 'Database Concepts', 'Scm Modules', 'Pl Sql', 'Sql Loader', 'Sql', 'Report Builder', 'Web Services', 'Workflow', 'Web Adi', 'Xml Publisher Reports', 'Oracle Forms', 'Oracle Ebs']",2025-06-12 15:21:51
Petrophysics Data Analyst-G&G Ops Specialist,BCForward,5-10 Years,,Bengaluru,"The Geomechanics and G&G Ops Specialist will provide basic modeling support across various disciplines and functions to enable safe operations & reducing environmental risk, enhance field productivity & ensure reserves, and safeguard the system integrity & reliability\nThe position supports the construction, setup and execution of well and reservoir geomechanical models that are integral to upstream activities\nThe role brings together working subsurface knowledge and expertise in a growing technical area with significant impact and overlap with Shale and Tight, Carbon Sequestration, Deepwater and Heavy Oil assets\nThe role requires close collaboration with subject matter experts and chapter managers in Technical Center to maintain workflow alignment\nCreate, enhance, update, and maintain documents related to Well operations within the clients requirement management system (RMS)\nTrack and manage the workflow of documents through the entire revision process\nThe ideal candidate will be capable of becoming an expert in the RMS interface, improving upon it, troubleshooting it, and training others in its use\nThis role will be accountable for the development and maintenance of all facility Operating Procedures, Work Instructions and Process & Equipment Manuals\nThe Tech Writer will generate materials with input from SMEs and ensure that all documents are developed for their intended audience\nThis role will be required to research engineering documentation, diagrams, corporate standards, and other regulations and standards to ensure sufficient compliance\nThe Tech Writer will also be responsible for ensuring that impact from MOCs, HAZOPs and other processes are adequately captured and reflected in all affected documents\n:Required qualifications: Bachelors degree in applicable STEM discipline5+ years relevant industry experience with well log data and G&G applications Knowledge of petrophysics, well log data mnemonics and standards\nKnowledge of log data storage in databases and tools/systems, especially Techlog",IT Management,"['MOC', 'HAZOPs', 'G&G', 'geomechanical', 'techlog']",2025-06-12 15:21:52
Master Data Analyst - Material & Production,Amgen Technology Private Limited,3-7 Years,,Hyderabad,Role Responsibilities:\nMaintain and validate master data for accuracy\nOptimize data management processes\nAnalyze data and resolve discrepancies\nCollaborate with teams to support data integrity\nKey Deliverables:\nAccurate master data across domains\nImproved data workflows\nTimely issue resolution\nClear communication of data insights,Pharmaceutical,"['Sap Ecc', 'Oracle', 'Ms Office', 'Data Governance', 'Informatica']",2025-06-12 15:21:53
Data Analyst - FTE,BCForward,10-15 Years,,Bengaluru,"Job Title: Data Analyst FTE\nExperience: 1015 Years\nWork Mode: Hybrid (Bengaluru)\nLocation: Bengaluru, India\nNotice Period: Immediate to 30 Days\nJob Description:\nWe are seeking a highly skilled and experienced Data Analyst with 1015 years of experience, preferably in the healthcare domain, to join our team in a hybrid work model based out of Bengaluru. The ideal candidate will have strong hands-on expertise in QNXT schema, healthcare data domains, SQL, and experience working across data mapping, conversion, interface, and reporting.\nKey Responsibilities:\nWork with large-scale healthcare data including member, eligibility, TPL, provider, claims (medical & pharmacy), etc.\nAnalyze and understand QNXT schema to support data extraction, transformation, and integration tasks.\nDesign and develop robust SQL queries to support reporting, data validation, and transformation needs.\nInterpret data and analyze results using statistical techniques and provide ongoing reports and insights.\nPerform data mapping and data lineage activities to support migration, integration, or interface projects.\nContribute to data conversion, interface development, and reporting projects within healthcare operations.\nCollaborate with stakeholders to understand business requirements and translate them into analytical solutions.\nDocument data flows, data dictionaries, and transformation logic.\nCommunicate effectively with technical and non-technical stakeholders through both verbal and written channels.\nRequired Skills & Qualifications:\n1015 years of experience in data analysis, preferably in the healthcare payer domain.\nHands-on experience with QNXT platform and schema is a must.\nStrong domain knowledge in healthcare operations: member, eligibility, TPL, provider, claims processing, and pharmacy claims.\nExpert-level SQL skills ability to write complex queries, perform data profiling, and troubleshoot large datasets.\nExperience with data mapping in the context of ETL/data conversion or migration projects.\nPrior experience working on data conversions, interface development, and reporting projects.\nStrong analytical and problem-solving skills.\nExcellent verbal and written communication skills.\nAbility to work independently and as part of a collaborative team in a hybrid environment.\nPreferred Qualifications:\nFamiliarity with reporting tools such as Power BI, Tableau, or SSRS.\nExperience working with cloud platforms (Azure, AWS) is a plus.\nKnowledge of HIPAA regulations and data privacy standards in healthcare.\nTo Apply:\nPlease share your updated resume along with your availability for joining.",IT Management,"['Data Mapping', 'Interface Development', 'Analytical Skills', 'Qnxt', 'Sql', 'Healthcare Domain']",2025-06-12 15:21:55
Data Analyst - FTE,BCForward,10-15 Years,,Bengaluru,"QNXT/Schema knowledge\nHealthcare/business knowledge in member, eligibility, TPL, claims processing, provider, pharmacy claims\nGood/Excellent SQL skills\nGood Oral/Written communication skills\nData Mapping experience\nConversion/Interface/Reporting Experience\nData Collection & Management\nGather, clean, and validate data from various internal and external sources.\nEnsure data integrity and consistency across different datasets.\nDevelop and maintain data pipelines and databases to manage incoming data efficiently.\nData Analysis & Interpretation\nAnalyze data sets using statistical techniques and tools (Excel, SQL, Python, R, etc.).\nIdentify trends, patterns, and insights from large datasets.\nInterpret data to provide actionable recommendations and business solutions.\nReporting & Visualization\nPrepare detailed reports with key metrics, analysis, and trends.\nCreate data visualizations (using tools like Tableau, Power BI, or others) to simplify complex data and provide stakeholders with clear insights.\nDevelop dashboards and regular performance reports for management.\nCollaboration & Communication\nCollaborate with cross-functional teams (marketing, finance, operations) to understand their data needs.\nPresent findings to stakeholders in a clear, concise manner, translating technical data into business-friendly insights.\nSupport decision-making by providing data-driven reports and analysis.\nProcess Improvement\nSuggest improvements to existing data collection, processing, and reporting practices.\nParticipate in the design and optimization of automated reporting systems.\nWork on optimizing data models and processes for better efficiency and insights.",IT Management,"['TPL', 'FTE', 'Data Analyst', 'Qnxt', 'Sql']",2025-06-12 15:21:57
Petrophysics Data Analyst-G&G Ops Specialist 5+ Yrs,BCForward,5-10 Years,,Bengaluru,"About the Role:\nThe Geomechanics and G&G Ops Specialist will provide basic modeling support across various disciplines and functions to enable safe operations & reducing environmental risk, enhance field productivity & ensure reserves, and safeguard the system integrity & reliability\nThe position supports the construction, setup and execution of well and reservoir geomechanical models that are integral to upstream activities\nThe role brings together working subsurface knowledge and expertise in a growing technical area with significant impact and overlap with Shale and Tight, Carbon Sequestration, Deepwater and Heavy Oil assets\nThe role requires close collaboration with subject matter experts and chapter managers in Technical Center to maintain workflow alignment\nCreate, enhance, update, and maintain documents related to Well operations within the clients requirement management system (RMS)\nTrack and manage the workflow of documents through the entire revision process\nThe ideal candidate will be capable of becoming an expert in the RMS interface, improving upon it, troubleshooting it, and training others in its use\nThis role will be accountable for the development and maintenance of all facility Operating Procedures, Work Instructions and Process & Equipment Manuals\nThe Tech Writer will generate materials with input from SMEs and ensure that all documents are developed for their intended audience\nThis role will be required to research engineering documentation, diagrams, corporate standards, and other regulations and standards to ensure sufficient compliance\nThe Tech Writer will also be responsible for ensuring that impact from MOCs, HAZOPs and other processes are adequately captured and reflected in all affected documents\nRequired qualifications:\nBachelors degree in applicable STEM discipline5+ years relevant industry experience with well log data and G&G applications Knowledge of petrophysics, well log data mnemonics and standards\nKnowledge of log data storage in databases and tools/systems, especially Techlog",IT Management,"['geomechanics', 'Petrophysics', 'Data mnemonics', 'G&G Ops', 'log data storage', 'Databases']",2025-06-12 15:21:58
Data Analyst - FTE,BCForward,10-15 Years,,Bengaluru,"1) QNXT/Schema knowledge\n2) Healthcare/business knowledge in member, eligibility, TPL, claims processing, provider, pharmacy claims\n3) Good/Excellent SQL skills\n4) Good Oral/Written communication skills\n5) Data Mapping experience\n6) Conversion/Interface/Reporting Experience\nPlease share your Updated Resume.\nNote: Looking for Immediate to 30-Days joiners at most.",IT Management,"['tpl', 'Data Mapping', 'Reporting', 'Sql', 'Qnxt']",2025-06-12 15:22:00
Data Analyst,Robotics Technologies,5-15 Years,INR 5.5 - 15.5 LPA,"Gurugram, Kolkata, Mumbai","Description\nWe are seeking a Data Analyst with 5-15 years of experience to join our dynamic team. The ideal candidate will be responsible for analyzing data to support business operations and strategy, developing reports, and providing insights to stakeholders.\nResponsibilities\nCollecting, processing, and analyzing data to help drive business decisions.\nDeveloping and maintaining dashboards and reports to visualize data insights.\nCollaborating with cross-functional teams to understand their data needs and provide analytical support.\nIdentifying trends and patterns in data to inform strategy and operational improvements.\nPresenting findings and recommendations to stakeholders in a clear and actionable manner.\nSkills and Qualifications\nBachelor's degree in Data Science, Statistics, Computer Science, or a related field.\nProficiency in SQL for data extraction and manipulation.\nExperience with data visualization tools such as Tableau, Power BI, or similar.\nStrong analytical skills with the ability to interpret complex datasets.\nFamiliarity with programming languages such as Python or R for data analysis.\nExcellent communication skills to convey findings to non-technical audiences.\nExperience with statistical analysis and modeling techniques.","Management Information Systems, Management Consulting, Outsourcing","['ETL Processes', 'Excel Advanced', 'Business Intelligence', 'Sql', 'Python', 'Data Visualization', 'Statistical Analysis', 'Machine Learning', 'Data Warehousing', 'Data Mining']",2025-06-12 15:22:08
Hiring for Data Analyst Role,Robotics Technologies,3-10 Years,INR 5 - 15 LPA,"Ahmedabad, Bengaluru, Chennai","Description\nWe are seeking a Data Analyst with 3-10 years of experience to join our team in India. The ideal candidate will have a strong analytical background and be skilled in data manipulation and visualization. You will play a crucial role in helping the organization make data-driven decisions.\nResponsibilities\nCollect, process, and analyze data to provide actionable insights.\nDevelop and maintain dashboards and reports to track key performance indicators.\nCollaborate with cross-functional teams to understand data needs and deliver analytical solutions.\nIdentify trends, patterns, and anomalies in data sets to support decision-making.\nCommunicate findings and recommendations to stakeholders in a clear and effective manner.\nSkills and Qualifications\nBachelor's or Master's degree in Data Science, Statistics, Mathematics, or a related field.\nProficiency in SQL for data manipulation and extraction.\nExperience with data visualization tools such as Tableau, Power BI, or similar.\nStrong knowledge of statistical analysis and modeling techniques.\nFamiliarity with programming languages such as Python or R for data analysis.\nExcellent problem-solving skills and attention to detail.\nStrong communication skills to present complex data insights to non-technical stakeholders.",IT Management,"['ETL Processes', 'Sql', 'Python', 'Data Visualization', 'Statistical Analysis', 'Machine Learning', 'Data Mining', 'Excel', 'Dashboarding', 'Data Warehousing']",2025-06-12 15:22:09
Hiring for Data Analyst Role,Robotics Technologies,3-10 Years,INR 5 - 15 LPA,"Ahmedabad, Bengaluru, Chennai","Description\nWe are seeking a Data Analyst with 3-10 years of experience to join our team in India. The ideal candidate will have a strong analytical background and be skilled in data manipulation and visualization. You will play a crucial role in helping the organization make data-driven decisions.\nResponsibilities\nCollect, process, and analyze data to provide actionable insights.\nDevelop and maintain dashboards and reports to track key performance indicators.\nCollaborate with cross-functional teams to understand data needs and deliver analytical solutions.\nIdentify trends, patterns, and anomalies in data sets to support decision-making.\nCommunicate findings and recommendations to stakeholders in a clear and effective manner.\nSkills and Qualifications\nBachelor's or Master's degree in Data Science, Statistics, Mathematics, or a related field.\nProficiency in SQL for data manipulation and extraction.\nExperience with data visualization tools such as Tableau, Power BI, or similar.\nStrong knowledge of statistical analysis and modeling techniques.\nFamiliarity with programming languages such as Python or R for data analysis.\nExcellent problem-solving skills and attention to detail.\nStrong communication skills to present complex data insights to non-technical stakeholders.",IT Management,"['ETL Processes', 'Sql', 'Python', 'Data Visualization', 'Statistical Analysis', 'Machine Learning', 'Data Mining', 'Excel', 'Dashboarding', 'Data Warehousing']",2025-06-12 15:22:10
Hiring for Data Analyst Role,Robotics Technologies,3-10 Years,INR 5.5 - 12 LPA,"Ahmedabad, Bengaluru, Chennai","Description\nWe are seeking a skilled Data Analyst to join our team in India. The ideal candidate will have 3-10 years of experience in data analysis and will be responsible for turning data into actionable insights that drive business decisions.\nResponsibilities\nCollect, analyze, and interpret complex data sets.\nDevelop and maintain dashboards and reports to track key performance indicators.\nCollaborate with cross-functional teams to identify data needs and provide insights.\nUtilize statistical methods to analyze data trends and patterns.\nPresent findings and recommendations to stakeholders in a clear and concise manner.\nSkills and Qualifications\nProficiency in data analysis tools such as SQL, R, or Python.\nExperience with data visualization tools like Tableau or Power BI.\nStrong understanding of statistical analysis and methodologies.\nExcellent problem-solving skills and attention to detail.\nAbility to communicate complex data insights effectively to non-technical audiences.",IT Management,"['ETL Processes', 'Business Intelligence', 'Sql', 'Python', 'Excel', 'Data Visualization', 'Statistical Analysis', 'Machine Learning', 'Data Cleaning', 'Data Warehousing']",2025-06-12 15:22:12
Data Analyst,Robotics Technologies,5-10 Years,INR 7 - 20 LPA,"Bengaluru, Chennai, Pune","We are seeking an experienced Data Analyst to join our team in India. The ideal candidate will have a strong background in data analysis and will be responsible for transforming data into actionable insights that drive business performance.\nResponsibilities\nCollecting, analyzing, and interpreting large datasets to inform business decisions.\nCreating visualizations and reports to communicate data insights to stakeholders.\nDeveloping and maintaining databases and data systems to optimize data collection and analysis processes.\nCollaborating with cross-functional teams to understand data needs and provide analytical support.\nIdentifying trends, patterns, and anomalies in data to drive strategic initiatives.\nSkills and Qualifications\nBachelor's degree in Data Science, Statistics, Mathematics, Computer Science, or a related field.\n5-10 years of experience in data analysis or a related field.\nProficiency in data analysis tools and languages such as SQL, Python, R, or similar.\nStrong experience with data visualization tools (e.g., Tableau, Power BI, or similar).\nSolid understanding of statistical analysis and modeling techniques.\nExcellent problem-solving skills and attention to detail.\nAbility to communicate complex data insights clearly to non-technical stakeholders.\nExperience with data warehousing and ETL processes is a plus.",Information Technology,"['ETL Processes', 'Excel Advanced', 'Sql', 'Python', 'Data Visualization', 'Statistical Analysis', 'Machine Learning', 'Data Warehousing', 'R Programming', 'Big Data']",2025-06-12 15:22:13
Data Analyst Role,Robotics Technologies,5-10 Years,INR 4 - 20 LPA,"Hyderabad, Bengaluru, Chennai","Description\nWe are seeking an experienced Data Analyst to join our team in India. The ideal candidate will have 5-10 years of experience in data analysis and be able to transform complex data into actionable insights to support our business strategies.\nResponsibilities\nCollect, clean, and analyze data from various sources to inform business decisions.\nDevelop and maintain dashboards and reports to track key performance indicators (KPIs).\nPerform statistical analysis to identify trends and patterns in data.\nCollaborate with cross-functional teams to understand their data needs and provide insights.\nPresent findings and recommendations to stakeholders in a clear and concise manner.\nSkills and Qualifications\nBachelor's degree in Data Science, Statistics, Mathematics, or a related field.\nProficiency in data analysis tools such as SQL, Excel, and Python or R.\nExperience with data visualization tools like Tableau or Power BI.\nStrong analytical and problem-solving skills.\nExcellent communication skills, both written and verbal.\nAbility to work independently and as part of a team.\nFamiliarity with machine learning concepts and techniques is a plus.",Recruiting,"['ETL Processes', 'R', 'Sql', 'Python', 'Data Visualization', 'Statistical Analysis', 'Machine Learning', 'Data Warehousing', 'Excel', 'Big Data']",2025-06-12 15:22:14
Data Analyst Role,Robotics Technologies,5-10 Years,INR 4 - 20 LPA,"Hyderabad, Bengaluru, Chennai","Description\nWe are seeking a highly skilled Data Analyst with 5-10 years of experience to join our team in India. The ideal candidate will have a strong analytical mindset and a passion for turning data into actionable insights.\nResponsibilities\nCollecting, analyzing, and interpreting large datasets to inform business decisions.\nDeveloping and maintaining dashboards and reports to visualize key performance indicators.\nCollaborating with cross-functional teams to understand data needs and provide actionable insights.\nPerforming statistical analysis to identify trends and patterns in data.\nEnsuring data integrity and accuracy through regular audits and validation processes.\nSkills and Qualifications\nProficiency in data analysis tools such as SQL, Python, or R.\nExperience with data visualization tools like Tableau or Power BI.\nStrong knowledge of statistical analysis techniques and methodologies.\nFamiliarity with database management and data warehousing concepts.\nExcellent problem-solving skills and attention to detail.\nAbility to communicate complex data findings in a clear and concise manner.",Recruiting,"['ETL Processes', 'Excel Advanced', 'Sql', 'Python', 'Data Visualization', 'Machine Learning', 'Statistical Analysis', 'Data Mining', 'Database Management', 'Bi Tools']",2025-06-12 15:22:16
Data Analyst,Right Human Skills And Resources Private Limited,Fresher,INR 1.5 - 3 LPA,Cochin / Kochi / Ernakulam,"Description\nWe are looking for a Data Analyst to join our team in India. This role is ideal for freshers/entry-level candidates who are eager to learn and grow in the field of data analysis. The Data Analyst will be responsible for collecting and interpreting data to help inform business decisions.\nResponsibilities\nCollecting, processing, and analyzing data to identify trends and insights.\nCreating visualizations and reports to communicate findings to stakeholders.\nCollaborating with cross-functional teams to understand data needs and requirements.\nAssisting in the development and implementation of data management strategies.\nPerforming data quality checks and ensuring data integrity.\nSkills and Qualifications\nProficiency in Excel and data visualization tools like Tableau or Power BI.\nBasic knowledge of SQL for data querying.\nFamiliarity with statistical analysis and data modeling techniques.\nStrong analytical and problem-solving skills.\nAbility to communicate complex data insights in a clear and concise manner.\nUnderstanding of programming languages such as Python or R is a plus.",Login to check your skill match score,"['Sql', 'Python', 'Excel', 'Data Visualization', 'Statistical Analysis', 'Data Cleaning', 'Tableau', 'R Programming', 'Power Bi', 'Machine Learning']",2025-06-12 15:22:17
DATA ANALYST POWER BI INTERNSHIP,Maxgen Technologies Private Limited,Fresher,INR 2 - 4.5 LPA,"Pune, Jalna, Ratnagiri","Maxgen Technologies Pvt ltd offering internships in Pune .Diploma and engineering interns will get live project internships in data analyst and power bi program.\nWhat we are offering\nprofessional services to interns.\ninspiring work culture.\nadvance technology while internship.\nonline or offline options.\nsupportive management .\ncall us 9099039845,9607007009,9975064558.\nvisit us www.maxgentechnologies.com .\nAddress 509 ,5th floor pride icon near atithi veg restaurant kharadi pune.",Information Technology,"['Data Analyst', 'Internship', 'Power Bi', 'Data Analytics']",2025-06-12 15:22:18
DATA ANALYST POWER BI INTERNSHIP,Maxgen Technologies Private Limited,Fresher,INR 2 - 4.5 LPA,"Pune, Jalna, Ratnagiri","Maxgen Technologies Pvt ltd offering internships in Pune .Diploma and engineering interns will get live project internships in data analyst and power bi program.\nWhat we are offering\nprofessional services to interns.\ninspiring work culture.\nadvance technology while internship.\nonline or offline options.\nsupportive management .\ncall us 9099039845,9607007009,9975064558.\nvisit us www.maxgentechnologies.com .\nAddress 509 ,5th floor pride icon near atithi veg restaurant kharadi pune.",Information Technology,"['Data Analyst', 'Internship', 'Power Bi', 'Data Analytics']",2025-06-12 15:22:19
Global Banking & Markets - Franchise Data Engineering - Analyst,Goldman Sachs,1-4 Years,,Bengaluru,"The Analyst/Associate Role (1-4 years of experience) involves dynamically collaborating across business and engineering teams, translating\nbusiness problems into detailed data specifications and then designing, building and deploying scalable relational data models which would serve\nas the source for business user's/consumer's analytical use cases. The role requires end-to-end skills in data engineering, ETL, data modeling,\ndistributed databases, math/logic and a good grasp SDLC best practices along with Data Governance aspect. In the course of building this data\nsolution, the engineer will benefit from and be required to learn financial data engineering as it is performed at a top tier financial firm.\nSkills & Experience We Are Looking For\nAcademic Qualifications: A Bachelors or Masters degree in a computational field (Computer Science, Applied Mathematics, Engineering, or in a\nrelated quantitative discipline)\n1-4 years of relevant work experience in a global team-oriented environment\nStrong object-oriented design and hands on experience in one of programming languages (such as Java, Python, C++) using Object Oriented\ndesign techniques and best practices.\nDeep understanding of multidimensionality of data, data curation and data quality, such as traceability, security, performance latency and\ncorrectness across supply and demand processes\nIn-depth knowledge of relational and columnar SQL databases, including database design\nExpertise in data warehousing concepts (e.g. star schema, entitlement implementations, SQL modeling, milestoning, indexing, partitioning)\nExcellent communications skills and the ability to work with subject matter experts to extract critical business concepts and gather business\nrequirements\nIndependent thinker, willing to engage, challenge or learn\nAbility to stay commercially focused and to always push for quantifiable commercial impact\nStrong work ethic, a sense of ownership and urgency\nStrong analytical and problem solving skills\nAbility to collaborate effectively across global teams and communicate complex ideas in a simple manner\nPreferred Qualifications\nIndustry Experience in Data engineering.\nExposure to cloud databases (such as Snowflake, Single Store).\nExposure to cloud infrastructure (AWS, Azure, or GCP) and infrastructure as code (Terraform).\nExperience with programming for extract transform load (ETL) operations and data analysis.",Login to check your skill match score,"['snowflake', 'Java', 'Python', 'Sql', 'Data Modeling', 'Etl', 'data engineering']",2025-06-12 15:22:20
Enterprise Data Operations Analyst,PepsiCo,4-6 Years,,"Gurugram, India","Overview\n\nAs a member of the data engineering team, you will be the key technical expert developing and overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be an empowered member of a team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premise data sources as well as cloud and remote systems and help grow DevOps and DataOps culture.\n\nResponsibilities\n\nActive contributor to code development in projects and services.\nCollaborate with a cross-functional team of application developers, operations engineers, architects to understand complex product requirements and translate them into automated solutions that you build.\nCollaborate with colleagues to support and improve architecture, systems, processes, standards and tools.\nHelp technical discussions to ensure solutions are designed for successful deployment, security, and high availability in the cloud\nDesign, implement, and maintain server, storage, network, and security infrastructure as code.\nBuild reusable pipelines for application deployments.\nWrite and maintain code for automating the creation of scalable/resilient systems/infrastructure with a focus on immutability and containers.\nDevelop, implement, and test automated data backup and recovery, and disaster recovery procedures across multiple regions.\nWrite and maintain clear, concise documentation, runbooks and operational standards including infrastructure diagrams.\nAssist development teams in the creation and understanding of automated application configurations.\nEnsure all solutions are properly monitored and instrumented.\nTroubleshoot and resolve complex issues in development, test and production environments.\nDesign and deploy scalable, highly available, and fault-tolerant CICD pipelines using Azure DevOps.\nContinuously identify, adopt, & refine best practices.\n\nQualifications\n\nBachelor's Degree in Cyber Security, Information Technology, Computer Science or related field or related practical experience.\n4 or 4+ years of experience in Software and/or Infrastructure, with a desired 3+ years in a relevant cloud, Kubernetes, automation development, and/or orchestration positions.\n2+ years of hands-on experience on Azure leveraging number PAAS services offered by the platform.\nRequires excellent problem solving and analytic skills to effectively address the needs of customers, including experience handling problem escalations and notifications.\nExperience working in GCP, AWS, PCF, Azure, or other cloud-based technologies.\nExperience with Terraform, Ansible, Salt or similar automation tools are a benefit as we drive towards Infrastructure as Code (IaC).\nExperience with SCM and DevOps tool suites examples include Git, Sonar, Jenkins, Artifactory, HashiCorp Packer etc.\nExperience with containers, docker, Kubernetes, serverless functions.\nProgramming / Scripting background with knowledge of Python, PowerShell, Groovy.\nHands-on experience with Azure services (Proficiency with Azure DevOps, ARM Templates, Azure Policy, Azure CLI, Azure Rest API).\nExperience provisioning, operating, monitoring, troubleshooting and maintaining systems running in the cloud.\nMulti-year experience in application development and configuration automation.\nUnderstanding of application, server, and network security.\nUnderstanding of immutable infrastructure and infrastructure as code concepts.\nWorking knowledge of Agile/Scrum, experience leading continuous integration and continuous delivery concepts and frameworks.\nCloud Certifications (Azure Solutions Architect, DevOps Engineer, or other cloud professional certifications) is a plus.",Food Processing & Packaged Food,"['Azure Rest API', 'Azure CLI', 'Azure Policy', 'Serverless Functions', 'HashiCorp Packer', 'Salt', 'Git', 'Sonar', 'Groovy', 'Artifactory', 'Ansible', 'PowerShell', 'Automation', 'Kubernetes', 'Python', 'Azure', 'Terraform', 'Docker', 'ARM Templates', 'Jenkins', 'Azure DevOps']",2025-06-12 15:22:22
Growth Data Scientist/ Analyst,Crypto.com,3-6 Years,,United States of America (USA),"We are seeking a dynamic Growth Data Scientist/Analyst to join our Growth team. The successful candidate will be instrumental in leveraging data to drive strategic decisions, optimize growth initiatives, and enhance user acquisition strategies.\nResponsibilities\nDevelop and maintain acquisition and engagement (e.g. influencer/affiliate, organic, VIP, , paid acquisition etc.) dashboards to monitor user growth and campaign effectiveness.\nAssist in building predictive models to guide strategic decisions on acquisition and engagement, encompassing time series, predictive analytics, and recommender systems.\nAnalyze data to derive actionable insights that drive business decisions and performance improvements.\nAutomate repetitive tasks and data processes to enhance team efficiency.\nCollaborate with cross-functional teams to support data-driven decisions and project implementations.\nEngage in continuous learning to stay ahead of industry trends and leverage new tools and techniques in data analysis.\nRequirements\nBachelor's degree in a quantitative field such as Statistics, Computer Science, Engineering, or related fields.\nUp to 3 years of experience in data analysis or a related field. Experience in the crypto industry is a plus.\nFamiliarity with User Acquisition (UA) and/or Customer Relationship Management (CRM) concepts is advantageous.\nProficiency in SQL and familiarity with data visualization tools like Tableau.\nExperience with statistical software (e.g., R, Python) and libraries for managing, manipulating, and analyzing data.\nStrong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.\nAdept at queries, report writing, and presenting findings.\nKnowledge of acquisition campaign platforms and tools such as Appsflyer, SensorTower, or similar platforms.\nExcellent verbal and written communication skills.",Financial Services,"['Data Analysis', 'Sql', 'Tableau', 'Python', 'Predictive Modeling', 'Automation']",2025-06-12 15:22:23
Business Analyst (Data Management),Aspire Systems India Private Limited,10-14 Years,,Chennai,"We are seeking an experienced Senior Business Analyst with over 10 years of proven expertise in data management and analysis, coupled with strong communication skills.\nRequirements:\nWe are seeking an experienced Senior Business Analyst with over 10 years of proven expertise in data management and analysis, coupled with strong communication skills. The ideal candidate will possess advanced knowledge in data mapping, business rules, data cleansing techniques, and SQL proficiency. As a Senior Business Analyst specializing in data management, you will play a crucial role in ensuring the accuracy, integrity, and usability of our organization's data assets.\nThe ideal candidate should have:\nKey Responsibilities:\nData Analysis:Utilize advanced data analysis techniques to assess the quality, completeness, and consistency of organizational data, identifying trends, anomalies, and opportunities for improvement.\nData Mapping:Collaborate with stakeholders to understand data requirements and document data mappings between various systems, ensuring seamless integration and interoperability.\nBusiness Rules Definition:Define and document business rules governing data validation, transformation, and enrichment processes to ensure compliance with regulatory requirements and business standards.\nData Cleansing:Develop and implement data cleansing strategies and procedures to address data quality issues, including duplicate records, missing values, and inconsistencies, thereby enhancing the reliability and accuracy of our data.\nSQL Expertise:Leverage SQL skills to query, manipulate, and analyze large datasets stored in relational databases, performing data validation, transformation, and aggregation tasks as needed. Stakeholder Collaboration: Work closely with cross-functional teams, including IT, business users, and data stewards, to understand data requirements, gather feedback, and ensure alignment with business objectives.\nDocumentation and Communication:Create comprehensive documentation, including data dictionaries, data lineage diagrams, and process workflows, to facilitate understanding and collaboration among stakeholders.\nData Governance:Support the development and implementation of data governance policies, standards, and procedures to ensure the security, privacy, and compliance of organizational data assets.\nContinuous Improvement:Stay informed about industry trends, best practices, and emerging technologies in data management and analysis, and proactively identify opportunities for process optimization and skill enhancement.\nQualifications:\nBachelor's degree in Computer Science, Information Systems, Business Administration, or a related field; advanced degree preferred.\nMinimum of 10 years of experience as a Business Analyst, with a focus on data management, data analysis, and SQL.\nProficiency in SQL and experience working with relational databases (e.g., Oracle, SQL Server, PostgreSQL).\nStrong understanding of data management concepts and techniques, including data modeling, data mapping, and data quality management.\nExcellent communication.",Software,"['Business Analyst', 'Data Management']",2025-06-12 15:22:24
Data Acquisition & Reporting Analyst,IBM,1-3 Years,,Bengaluru,"A career in IBM Consulting embraces long-term relationships and close collaboration with clients across the globe. In this role, you will work for IBM BPO, part of Consulting that, accelerates digital transformation using agile methodologies, process mining, and AI-powered workflows.\nYou'll work with visionaries across multiple industries to improve the hybrid cloud and AI journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio, including IBM Software and Red Hat.\nCuriosity and a constant quest for knowledge serve as the foundation to success in IBM Consulting. In your role, you'll be supported by mentors and coaches who will encourage you to challenge the norm, investigate ideas outside of your role, and come up with creative solutions resulting in groundbreaking impact for a wide network of clients. Our culture of evolution and empathy centers on long-term career growth and learning opportunities in an environment that embraces your unique skills and experience.\nYour role and responsibilities\nPerform data extraction from NIQ, Circana, Spins, and other retail and CPG sources to gather raw data for further analysis.\nClean, organize, and validate data to ensure accuracy and completeness.\nUtilize Microsoft Excel, VBA, and SQL for data manipulation, analysis, and reporting.\nCollaborate with cross-functional teams to understand data requirements and deliver actionable insights.\nGenerate and publish excel reports for further uses\nApply knowledge of retail and CPG industries to enhance data analysis and reporting.\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nBachelor's degree in Computer Application, Science, or relevant education.\n1-3 years of experience in data acquisition & analysis or a related field.\nProficiency in Microsoft Excel, Microsoft Office, VBA, SQL, and experience with NIQ, Circana, and Spins.\nPreferred technical and professional experience\nStrong analytical skills and attention to detail.\nExcellent communication and teamwork abilities.\nAbility to manage multiple tasks and meet deadlines.\nExperience in retail and CPG industries is preferred.",Information Technology,"['niq', 'circana', 'spins', 'Excel', 'Vba', 'Sql']",2025-06-12 15:22:26
Data Acquisition & Reporting Analyst,IBM,1-3 Years,,Bengaluru,"A career in IBM Consulting embraces long-term relationships and close collaboration with clients across the globe. In this role, you will work for IBM BPO, part of Consulting that, accelerates digital transformation using agile methodologies, process mining, and AI-powered workflows.\nYou'll work with visionaries across multiple industries to improve the hybrid cloud and AI journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio, including IBM Software and Red Hat.\nCuriosity and a constant quest for knowledge serve as the foundation to success in IBM Consulting. In your role, you'll be supported by mentors and coaches who will encourage you to challenge the norm, investigate ideas outside of your role, and come up with creative solutions resulting in groundbreaking impact for a wide network of clients. Our culture of evolution and empathy centers on long-term career growth and learning opportunities in an environment that embraces your unique skills and experience.\nYour role and responsibilities\nPerform data extraction from NIQ, Circana, Spins, and other retail and CPG sources to gather raw data for further analysis.\nClean, organize, and validate data to ensure accuracy and completeness.\nUtilize Microsoft Excel, VBA, and SQL for data manipulation, analysis, and reporting.\nCollaborate with cross-functional teams to understand data requirements and deliver actionable insights.\nGenerate and publish excel reports for further uses\nApply knowledge of retail and CPG industries to enhance data analysis and reporting.\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nBachelor's degree in Computer Application, Science, or relevant education.\n13 years of experience in data acquisition & analysis or a related field.\nProficiency in Microsoft Excel, Microsoft Office, VBA, SQL, and experience with NIQ, Circana, and Spins.\nPreferred technical and professional experience\nStrong analytical skills and attention to detail.\nExcellent communication and teamwork abilities.\nAbility to manage multiple tasks and meet deadlines.\nExperience in retail and CPG industries is preferred.",Information Technology,"['niq', 'circana', 'spins', 'Excel', 'Vba', 'Sql']",2025-06-12 15:22:35
Data Acquisition & Reporting Analyst,IBM,1-3 Years,,Hyderabad,"A career in IBM Consulting embraces long-term relationships and close collaboration with clients across the globe. In this role, you will work for IBM BPO, part of Consulting that, accelerates digital transformation using agile methodologies, process mining, and AI-powered workflows.\nYou'll work with visionaries across multiple industries to improve the hybrid cloud and AI journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio, including IBM Software and Red Hat.\nCuriosity and a constant quest for knowledge serve as the foundation to success in IBM Consulting. In your role, you'll be supported by mentors and coaches who will encourage you to challenge the norm, investigate ideas outside of your role, and come up with creative solutions resulting in groundbreaking impact for a wide network of clients. Our culture of evolution and empathy centers on long-term career growth and learning opportunities in an environment that embraces your unique skills and experience.\nYour role and responsibilities\nPerform data extraction from NIQ, Circana, Spins, and other retail and CPG sources to gather raw data for further analysis.\nClean, organize, and validate data to ensure accuracy and completeness.\nUtilize Microsoft Excel, VBA, and SQL for data manipulation, analysis, and reporting.\nCollaborate with cross-functional teams to understand data requirements and deliver actionable insights.\nGenerate and publish excel reports for further uses\nApply knowledge of retail and CPG industries to enhance data analysis and reporting.\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nBachelor's degree in Computer Application, Science, or relevant education.\n13 years of experience in data acquisition & analysis or a related field.\nProficiency in Microsoft Excel, Microsoft Office, VBA, SQL, and experience with NIQ, Circana, and Spins.\nPreferred technical and professional experience\nStrong analytical skills and attention to detail.\nExcellent communication and teamwork abilities.\nAbility to manage multiple tasks and meet deadlines.\nExperience in retail and CPG industries is preferred.",Information Technology,"['niq', 'circana', 'spins', 'Excel', 'Vba', 'Sql']",2025-06-12 15:22:40
Data Acquisition & Reporting Analyst,IBM,1-3 Years,,Hyderabad,"A career in IBM Consulting embraces long-term relationships and close collaboration with clients across the globe. In this role, you will work for IBM BPO, part of Consulting that, accelerates digital transformation using agile methodologies, process mining, and AI-powered workflows.\nYou'll work with visionaries across multiple industries to improve the hybrid cloud and AI journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio, including IBM Software and Red Hat.\nCuriosity and a constant quest for knowledge serve as the foundation to success in IBM Consulting. In your role, you'll be supported by mentors and coaches who will encourage you to challenge the norm, investigate ideas outside of your role, and come up with creative solutions resulting in groundbreaking impact for a wide network of clients. Our culture of evolution and empathy centers on long-term career growth and learning opportunities in an environment that embraces your unique skills and experience.\nYour role and responsibilities\nPerform data extraction from NIQ, Circana, Spins, and other retail and CPG sources to gather raw data for further analysis.\nClean, organize, and validate data to ensure accuracy and completeness.\nUtilize Microsoft Excel, VBA, and SQL for data manipulation, analysis, and reporting.\nCollaborate with cross-functional teams to understand data requirements and deliver actionable insights.\nGenerate and publish excel reports for further uses\nApply knowledge of retail and CPG industries to enhance data analysis and reporting.\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nBachelor's degree in Computer Application, Science, or relevant education.\n13 years of experience in data acquisition & analysis or a related field.\nProficiency in Microsoft Excel, Microsoft Office, VBA, SQL, and experience with NIQ, Circana, and Spins.\nPreferred technical and professional experience\nStrong analytical skills and attention to detail.\nExcellent communication and teamwork abilities.\nAbility to manage multiple tasks and meet deadlines.\nExperience in retail and CPG industries is preferred.",Information Technology,"['niq', 'circana', 'spins', 'Excel', 'Vba', 'Sql']",2025-06-12 15:22:42
"Data Engineering, Investments, Analyst",Primetrace Technologies,2-4 Years,,Mumbai,"What We re Looking For\n2-4 years of experience in data engineering, data extraction, web scraping, or unstructured data processing.\nStrong proficiency in Python, Pandas/NumPy, Regex Text Processing, Shell scripting/Bash.\nFamiliarity with web scraping tools or Beautiful Soup and data governance.\nKnowledge of frontend/ backend development (React, APIs, Python Flask or FastAPI, databases, cloud technologies) is a plus.\nSomeone capable of working with unstructured or alternative data sources\nCompetence in deploying solutions on Google Cloud Platform (GCP), particularly BigQuery and Cloud Functions along with experience with Snowflake for data modeling and performance tuning\nExperience working in a fast-paced environment with evolving priorities\nEffective communication and an ability to collaborate across technical and non-technical teams.\nData product lifecycle management from acquisition to QC and delivery is a plus.\nStrong problem-solving skills with attention to detail and a proactive approach",Information Technology,"['Regex Text Processing', 'Beautiful Soup', 'snowflake', 'Investments', 'Python', 'Bash', 'Gcp', 'data engineering']",2025-06-12 15:22:44
HIH - Data Science Lead Analyst - Evernorth,Manipalcigna Health Insurance,5-8 Years,,Hyderabad,"Role Summary\nAs a member of the Data Science Center of Expertise (DSCOE), the DS Lead Analyst is responsible for leading and enabling Data Science within Cigna Group with demonstrable aptitude in Data Science (i) Technical Skills (ii) Leadership (iii) Scope & Impact (iv) Influence. Please see Qualifications section below for more details.\nThe role will support the development and maintenance of machine learning models, with a focus on ensuring that models meet Cigna s requirements for governance and legal compliance. The role will require collaboration with other data scientists and involve work across many lines of business.\nKey Responsibilities:\nAnalyze model performance of new models with specific regards to requirements for legal compliance and governance standards around accuracy and bias;\nPerform periodic analyses of performance of existing models to ensure continued compliance with internal and external standards for accuracy and bias;\nConduct research (i.e. literature review) to understand when bias may be biologically or medically justifiable, and to what degree, for example: finding evidence from literature that heart disease is more prevalent among older populations\nUsing machine learning development tools to mitigate model bias when this is determined to be necessary\nCollaborating with data scientists, business stakeholders, and governance/compliance teams to ensure models meet compliance and governance standards\nQualifications:\nBachelors or Masters/PhD (preferred) in statistics or computer science or equivalent field with 5-8 years of relevant experience\nStrong proficiency in ML, statistics, python or R, SQL, version control (e.g., Git), health care data (e.g., claims, EHR)\nAbility to promote best coding practices, championing a culture of documentation/logging\nThorough understanding of ML lifecycle, including necessary tradeoffs and associated risks Leadership in Data Science\nCan own a project end-to-end e.g., scoping, business value estimation, ideation, dev, prod, timeline\nCollaborates and guides junior team members in completion of projects and career development\nWorks cross functionally with technical (e.g., Data Science, Data Engineering) and business (e.g., clinical, marketing, pricing, business analysts) to implement solutions with measurable value Scope and Impact\nIndependently delivers clear and well-developed presentations for both technical and business audiences\nCreates data science specific project goals associated with project deliverables\nArticulates timeline changes, rationale, and goals to meet deadlines moving forward\nValues diversity, growth mindset, and improving health outcomes of our customers Level of Influence\nCommunicate with stakeholders to identify opportunities and possible solutions based on business need\nDraft project charter, timeline, and features/stories\nInfluence matrix-partner leadership",Health Insurance,"['Statistics', 'ML lifecycle', 'Ml', 'python', 'Data Science', 'data engineering']",2025-06-12 15:22:45
Data Measurement & Reporting Analyst,Manipalcigna Health Insurance,3-5 Years,,Bengaluru,"Roles and Responsibilities:\nSolution Capability:\nDesign and devise quick solutions for small to complex business challenges, using Qlik.\nComfortable building sustainable yet tactical solutions in spite of lack of enterprise solutions with while working with technology for end state solutions\nMaintain existing home grown tools.\nBusiness:\nKnowledge/ prior experience of Health Insurance preferred with demonstrated experience in driving /MIS and improvements while working directly with internal and external customers\nAbility to develop quick solutions and support existing in house tool sets\nAbility to communicate clearly to stakeholders and manage expectations both for customers and team members\nCross Functional:\nKnowledge/ prior experience of data visualization, Health Insurance.\nSenior leadership communication skills.\nAdaptability, multitasking without impacting quality, ability to handle pressure, changing priorities.\nTools:\nExpert in QlikView/ Qlik sense - 3 to 5 years\nExpert in advanced Excel Macro / Access and other MS office suites - 2 to 5 years.\nExpert in SharePoint or other collaboration tools - 2 to 3 years.\nExpert in SQL: 3 to 5 years\nEducation:\nBachelors with 3 to 5 years of work experience or Masters with total 2 years of work experience.",Health Insurance,"['QlikView/ Qlik', 'Sharepoint', 'Excel Macro', 'Ms Office', 'Sql']",2025-06-12 15:22:46
Data Science Assoc Analyst,PepsiCo,Fresher,,"Hyderabad, India","Overview\n\nData Science Team works in developing Machine Learning (ML) and Artificial Intelligence (AI) projects. Specific scope of this role is to develop ML solution in support of ML/AI projects using big analytics toolsets in a CI/CD environment. Analytics toolsets may include DS tools/Spark/Databricks, and other technologies offered by Microsoft Azure or open-source toolsets. This role will also help automate the end-to-end cycle with Azure Pipelines.\nYou will be part of a collaborative interdisciplinary team around data, where you will be responsible of our continuous delivery of statistical/ML models. You will work closely with process owners, product owners and final business users. This will provide you the correct visibility and understanding of criticality of your developments.\n\nResponsibilities\n\nDelivery of key Advanced Analytics/Data Science projects within time and budget, particularly around DevOps/MLOps and Machine Learning models in scope\nActive contributor to code & development in projects and services\nPartner with data engineers to ensure data access for discovery and proper data is prepared for model consumption.\nPartner with ML engineers working on industrialization.\nCommunicate with business stakeholders in the process of service design, training and knowledge transfer.\nSupport large-scale experimentation and build data-driven models.\nRefine requirements into modelling problems.\nInfluence product teams through data-based recommendations.\nResearch in state-of-the-art methodologies.\nCreate documentation for learnings and knowledge transfer.\nCreate reusable packages or libraries.\nEnsure on time and on budget delivery which satisfies project requirements, while adhering to enterprise architecture standards\nLeverage big data technologies to help process data and build scaled data pipelines (batch to real time)\nImplement end-to-end ML lifecycle with Azure Databricks and Azure Pipelines\nAutomate ML models deployments\n\nQualifications\n\nBE/B.Tech in Computer Science, Maths, technical fields.\nOverall 2-4 years of experience working as a Data Scientist.\n2+ years experience building solutions in the commercial or in the supply chain space.\n2+ years working in a team to deliver production level analytic solutions. Fluent in git (version control). Understanding of Jenkins, Docker are a plus.\nFluent in SQL syntaxis.\n2+ years experience in Statistical/ML techniques to solve supervised (regression, classification) and unsupervised problems.\n2+ years experience in developing business problem related statistical/ML modeling with industry tools with primary focus on Python or Pyspark development.\nData Science - Hands on experience and strong knowledge of building machine learning models - supervised and unsupervised models. Knowledge of Time series/Demand Forecast models is a plus\nProgramming Skills - Hands-on experience in statistical programming languages like Python, Pyspark and database query languages like SQL\nStatistics - Good applied statistical skills, including knowledge of statistical tests, distributions, regression, maximum likelihood estimators\nCloud (Azure) - Experience in Databricks and ADF is desirable\nFamiliarity with Spark, Hive, Pig is an added advantage\nBusiness storytelling and communicating data insights in business consumable format. Fluent in one Visualization tool.\nStrong communications and organizational skills with the ability to deal with ambiguity while juggling multiple priorities\nExperience with Agile methodology for team work and analytics product creation.\nExperience in Reinforcement Learning is a plus.\nExperience in Simulation and Optimization problems in any space is a plus.\nExperience with Bayesian methods is a plus.\nExperience with Causal inference is a plus.\nExperience with NLP is a plus.\nExperience with Responsible AI is a plus.\nExperience with distributed machine learning is a plus\nExperience in DevOps, hands-on experience with one or more cloud service providers AWS, GCP, Azure(preferred)\nModel deployment experience is a plus\nExperience with version control systems like GitHub and CI/CD tools\nExperience in Exploratory data Analysis\nKnowledge of ML Ops / DevOps and deploying ML models is preferred\nExperience using MLFlow, Kubeflow etc. will be preferred\nExperience executing and contributing to ML OPS automation infrastructure is good to have\nExceptional analytical and problem-solving skills\nStakeholder engagement-BU, Vendors.\nExperience building statistical models in the Retail or Supply chain space is a plus",Food Processing & Packaged Food,[],2025-06-12 15:22:47
Data Analytics Support Analyst (Contractual),Trafigura,Fresher,,"Mumbai, India","Main Purpose:\nWe are recruiting an expert application support engineer to scale up the global support capability for our data nad analytics platform used by our research and trading teams. The candidate will work closely with our data engineers, data scientists, external data vendors, and various trading teams to rapidly resolve data and analytics application issues related to data quality, data integration, model pipelines, and analtics applications.\nKnowledge Skills and Abilities, Key Responsibilities:\nKnowledge, Skills and Abilities\n- Python, SQL\n- Familiarity with data engineering\n- Experience with AWS data and analytics services or similar cloud vendor services\n- Strong problem solving and interpersonal skills\n- Ablity to organise and prioritise work effectively\nKey Responsibilities\n- Incident and user management for data and analytics platform\n- Development and maintenance of Data Quliaty framework (including anomaly detection)\n- Implemenation of Python & SQL hotfixes and working with data engineers on more complex issues\n- Diagnostic tools implementation and automation of operational processes\nKey Relationships\n- Work closely with data scientists, data engineers, and platform engineers in a highly commercial environment\n- Support research analysts and traders with issue resolution\nCompetencies\n- Excellent problem solving skills\n- Ability to communicate effectively with a diverse set of customers across business lines and technology\nKey Relationships and Department Overview:\n- Report to Head of DSE Engineering Mumbai, who reports to Global Head of Cloud and Data Engineering",Oil/Gas/Petroleum,[],2025-06-12 15:22:49
Business Analyst - Master Data Management,Genpact,Fresher,,Jodhpur,"Genpact (NYSE: G) is a global professional services and solutions firm delivering outcomes that shape the future. Our 125,000+ people across 30+ countries are driven by our innate curiosity, entrepreneurial agility, and desire to create lasting value for clients. Powered by our purpose - the relentless pursuit of a world that works better for people - we serve and transform leading enterprises, including the Fortune Global 500, with our deep business and industry knowledge, digital operations services, and expertise in data, technology, and AI.\n\nInviting applications for the role of Business Analyst, Master Data Management!\n\nIn this role, you will be responsible to perform day-to-day operations while maintaining SLA. Solving queries related to Vendor & Customer master data domain. This role will be reporting to the manager of MDM. The resource will be working with a cross functional team and will maintain, analyse, execute requests with utmost quality with eye for details with good experience in MDM- vendor & customer master. We are looking for some who should be open to work in any shift as per the business requirement.\n\nResponsibilities\n\n. Manage vendor master and customer master data and ensure smooth delivery of day-to-day work which is assigned (add / update / extend / deactivate).\n. Highlight and take appropriate help from the Lead on SOP deviations or issues\n. Take independent ownership of performance against all the assigned Master data processes and key categories such as hierarchies.\n. Accountable for the quality and accuracy\n. Collaborate with internal & external audit team for any challenges. Prepare relevant documents or evidence & supporting material to address audit finding and recommendations.\n. Ensure timely and accurate update of all master data records. Responsible for delivering all the tasks on-time, every-time. Ensure that the schedule is reviewed as circumstances change. (timeliness)\n. Be responsible for maintaining the SLA of each period and the delivery performance\n. Raise incident tickets to the relevant helpdesks when there is an issue\n. Participate in idea generation and improvement activities\n. Good knowledge of governing policies around vendor & customer data as per the area of work\n. Update /maintain the departmental procedures (SOP&rsquos) tools, and metrics for the master data team\n. Ensure all Data Quality KPIs (Key Performance Indicators) are met\n. Identify gaps/missing information and coordinate with client stakeholders to fill those gaps\n. Identify continuous improvement opportunities.\n\nQualifications we seek in you!\n\nMinimum Qualifications\n\n. Any Graduate / Bachelor&rsquos degree (B. Com preferred) postgraduate (M.Com preferred)\n. Great at communication, collaboration (verbal and written)\n. Understanding of any ERP or relevant experience\n. Hands on experience on MDM platforms (SAP, Reltio, Oracle, JDE, etc) would be added advantage.\n\nPreferred qualifications\n\n. Good customer handling skills\n. Good understanding of Master Data Management activities across service lines and platforms.\n. Basic understanding of Source-to-pay (S2P), Order-to-cash (O2C) and Record-to-report (R2R).\n. Advanced use of MS Office (Excel, PowerPoint, Word)\n. Ability to define & solve problems, Collect Data, Establish facts\n. High Energy Level\n. Work with business units and process experts to resolve master data issues\n\n\nGenpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. Genpact is committed to creating a dynamic work environment that values respect and integrity, customer focus, and innovation. For more information, visit www.genpact.com. Follow us on Twitter, Facebook, LinkedIn, and YouTube.\nFurthermore, please do note that Genpact does not charge fees to process job applications and applicants are not required to pay to participate in our hiring process in any other way. Examples of such fraud include purchasing a 'starter kit,' paying to apply, or purchasing equipment or training.",IT/Computers - Hardware & Networking,[],2025-06-12 15:22:50
Senior Analyst - Master Data Management,Flex,3-6 Years,,Chennai,"Job Summary\nTo support our extraordinary teams who build great products and contribute to our growth, we're looking to add a Senior Analyst GBS Master Data Management in Chennai, India.\nThe Specialist GBS Master Data Management performs analysis, provide reports, information to create and maintain vendor master records and updates vendor setup. Additionally, work with suppliers and buyer groups to resolve Pricing.\nWhat a typical day looks like:\nCreate and review efficient procedures for all records, including invoice/purchase orders and relevant documentation.\nPerform general reporting and research activities for score carding of suppliers and keep system updated reflecting current acceptance.\nEnter, update, and maintain information on various business systems and/or various departments (e.g: Warehouse, Planning, Operations).\nMaintains logistic records such as inbound and outbound shipments, accurate import compliance for inbound international shipments or other logistics information.\nImplement and maintain record-keeping systems to comply with all customs.\nMaintain and update part number database with most up to date part details/master data collected internally and from suppliers.\nCollect and maintain part details and identify mismatches and incompliances.\nPrepare liability reports for excess and obsolete management, commodity risk positions, logistics performance and key focus areas for site or corporate requirements.\nCalculate cost savings & avoidance and provide reports and metrics.\nPull reports from databases, dashboards, enterprise resource planning system or other relevant systems.\nPreparation of recommendation to management based on the research of data.\nThe experience we're looking to add to our team:\nTypically requires a Bachelor's degree in related field or equivalent experience.\nTypically requires 3 years of experience in materials or related field.\nA background in manufacturing is desired.\nWhat you'll receive for the great work you provide:\nHealth Insurance\nPaid Time Off",Manufacturing,"['Master Data Management', 'performs analysis', 'provide reports', 'Maintains logistic records', 'maintain record-keeping systems']",2025-06-12 15:22:51
Qlik,Virtusa,3-6 Years,,Gurugram,"WE need Reporting developer to make changes in Existing dashboard.\nSkills:\nGood experinece and Handson in Qlik, QlikSense and Qlikview\nGood experinece and Handson in Data Modelling\nExperience in data analysis and data visualization tools.\nStrong SQL skills, ability to perform complex data analysis and data manipulations.",Information Technology,"['Data Analysis', 'Qlik', 'Qlik Sense', 'Qlikview', 'Data Modeling', 'Sql']",2025-06-12 15:22:52
AIML,Virtusa,7-10 Years,,Chennai,"Analyze application logs to identify data discrepancies, trends, and issues.\nPresent structured analytical insights and findings to business stakeholders.\nPerform Root Cause Analysis (RCA) by collaborating with cross-functional teams.\nAutomate manual processes through custom scripting or lightweight development.\nBuild and deploy minor Python-based enhancements.\nConduct Exploratory Data Analysis (EDA) and feature engineering to enhance existing models.\nBegin contributing to AI/ML model development, especially model drifts and enhancements.\nTranslate business requirements into technical specifications with cross-functional collaboration.\nMentor junior team members and define best practices for analytics and ML projects.\nCreate and maintain technical documentation and visual presentations.\nQualifications:\n710 years of experience in data analysis, particularly using SQL (any RDBMS).\nMinimum 2 years of hands-on experience in US Healthcare domain, preferably in Claims & Provider data.\nMinimum 1 year of experience in Python or Java development and interest in advancing Python skills.\nExperience or exposure to Cloud technologies like GCP, Cloud Run, and Vertex AI is a plus.\nFamiliarity with AI/ML techniques and platforms.\nWillingness to operate in Run & Operate model including analysis, support, and minor development.",Information Technology,"['Data Analysis', 'US Healthcare (Claims/Provider)', 'Cloud (GCP/Vertex AI)', 'AI/ML', 'Sql', 'Python']",2025-06-12 15:22:54
Data Engineer Tech Lead,Virtusa,5-7 Years,,Hyderabad,"Design, develop, and maintain ETL processes using Microsoft SSIS.\nWrite efficient and scalable T-SQL scripts for data manipulation and querying.\nDevelop reports and dashboards using SSRS to support business intelligence needs.\nPerform data modeling and create optimized database structures to support business analytics.\nConduct root cause analysis on internal/external data to resolve data issues and improve data quality.\nCollaborate directly with business users to gather requirements and deliver reliable data solutions.\nRequirements\n5+ years of experience in a data engineering or related role.\nStrong experience with Microsoft SQL Server, T-SQL, SSIS, and SSRS.\nBasic familiarity with C# is preferred.\nAbility to manage data transformation, metadata, and workflow dependencies.\nExcellent analytical, logical reasoning, and problem-solving skills.\nStrong communication skills and ability to work independently with business users.",Information Technology,"['T-sql', 'Microsoft Ssis', 'microsoft ssrs', 'Microsoft Sql Server', 'Data Modeling', 'Etl Development']",2025-06-12 15:22:55
Tableau - Developer,Virtusa,5-10 Years,,Chennai,Extensive experience of Data Integration in Banking domain.\nExtensive experience in ETL tools and Oracle DB\nVery strong communication and intrapersonal skills\nShould have:\nCollaboration\nTeamwork\ntime management\ndata analysis\ncreativity\nresearcher\nproblem solving\nExperience in Agile Methodology,Consulting,"['Data Analysis', 'Tableau', 'Agile Methodology']",2025-06-12 15:22:56
Data Scientist-MLOps/LLMOps,IBM,3-4 Years,,Mumbai,"Your role and responsibilities\nRole Overview:\nHiring an ML Engineer with experience in Cloudera ML to support end-to-end model development, deployment, and monitoring on the CDP platform.\nKey Responsibilities:\nDevelop and deploy models using CML workspaces\nBuild CI/CD pipelines for ML lifecycle\nIntegrate with governance and monitoring tools\nEnable secure model serving via REST APIs\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nSkills Required:\nExperience in Cloudera ML, Spark MLlib, or scikit-learn\nML pipeline automation (MLflow, Airflow, or equivalent)\nModel governance, lineage, and versioning\nAPI exposure for real-time inference",Information Technology,"['Cloudera Spark', 'Html', 'Ms Sql', 'aws', 'Kafka', 'DataBricks']",2025-06-12 15:22:58
Data Scientist-MLOps/LLMOps,IBM,3-4 Years,,Mumbai,"Your role and responsibilities\nRole Overview:\nHiring an ML Engineer with experience in Cloudera ML to support end-to-end model development, deployment, and monitoring on the CDP platform.\nKey Responsibilities:\nDevelop and deploy models using CML workspaces\nBuild CI/CD pipelines for ML lifecycle\nIntegrate with governance and monitoring tools\nEnable secure model serving via REST APIs\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nSkills Required:\nExperience in Cloudera ML, Spark MLlib, or scikit-learn\nML pipeline automation (MLflow, Airflow, or equivalent)\nModel governance, lineage, and versioning\nAPI exposure for real-time inference",Information Technology,"['Cloudera Spark', 'Html', 'Ms Sql', 'aws', 'Kafka', 'DataBricks']",2025-06-12 15:23:09
Data Engineer-Data Platforms-,IBM,3-4 Years,,"Navi Mumbai, Mumbai","Your role and responsibilities\nAs Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in the development of data solutions using Spark Framework with Python or Scala on Hadoop and AWS Cloud Data Platform\nResponsibilities:\nExperienced in building data pipelines to Ingest, process, and transform data from files, streams and databases. Process the data with Spark, Python, PySpark, Scala, and Hive, Hbase or other NoSQL databases on Cloud Data Platforms (AWS) or HDFS\nExperienced in develop efficient software code for multiple use cases leveraging Spark Framework / using Python or Scala and Big Data technologies for various use cases built on the platform\nExperience in developing streaming pipelines\nExperience to work with Hadoop / AWS eco system components to implement scalable solutions to meet the ever-increasing data volumes, using big data/cloud technologies Apache Spark, Kafka, any Cloud computing etc\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nMinimum 4+ years of experience in Big Data technologies with extensive data engineering experience in Spark / Python or Scala.\nMinimum 3 years of experience on Cloud Data Platforms on AWS;\nExperience in AWS EMR / AWS Glue / DataBricks, AWS RedShift, DynamoDB\nGood to excellent SQL skills\nExposure to streaming solutions and message brokers like Kafka technologies.\nPreferred technical and professional experience\nCertification in AWS and Data Bricks or Cloudera Spark Certified developers.",Information Technology,"['Cloudera Spark', 'Html', 'Ms Sql', 'aws', 'Kafka', 'DataBricks']",2025-06-12 15:23:13
Data Engineer-Data Platforms-,IBM,3-4 Years,,"Navi Mumbai, Mumbai","Your role and responsibilities\nAs Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in the development of data solutions using Spark Framework with Python or Scala on Hadoop and AWS Cloud Data Platform\nResponsibilities:\nExperienced in building data pipelines to Ingest, process, and transform data from files, streams and databases. Process the data with Spark, Python, PySpark, Scala, and Hive, Hbase or other NoSQL databases on Cloud Data Platforms (AWS) or HDFS\nExperienced in develop efficient software code for multiple use cases leveraging Spark Framework / using Python or Scala and Big Data technologies for various use cases built on the platform\nExperience in developing streaming pipelines\nExperience to work with Hadoop / AWS eco system components to implement scalable solutions to meet the ever-increasing data volumes, using big data/cloud technologies Apache Spark, Kafka, any Cloud computing etc\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nMinimum 4+ years of experience in Big Data technologies with extensive data engineering experience in Spark / Python or Scala.\nMinimum 3 years of experience on Cloud Data Platforms on AWS;\nExperience in AWS EMR / AWS Glue / DataBricks, AWS RedShift, DynamoDB\nGood to excellent SQL skills\nExposure to streaming solutions and message brokers like Kafka technologies.\nPreferred technical and professional experience\nCertification in AWS and Data Bricks or Cloudera Spark Certified developers.",Information Technology,"['Cloudera Spark', 'Html', 'Ms Sql', 'aws', 'Kafka', 'DataBricks']",2025-06-12 15:23:14
SAP Analytics Cloud,Virtusa,2-5 Years,,Chennai,"Collaborating with stakeholders to understand their business process requirements and objectives, translating requirements into SAP Analytics Cloud (SAC) solutions. Extracting, transforming, and loading data necessary for data modelling purposes. Validating and assuring data quality and accuracy, performing data cleansing and enrichment, and building data models and stories. Creating comprehensive reports and dashboards to help business stakeholders track their key performance indicators (KPIs) and drive insights from their data. Augmenting solution experiences and visualisations using low/no-code development.\nEssential requirements:\nA degree in Computer Science, Business Informatics or a comparable degree. At least 2 years experience working on SAP Analytics Cloud (SAC) solutions as a Data Analyst and/or Data Engineer. Experience in building data pipelines, preparing and integrating data sources for comprehensive analysis and reporting. Experience in building efficient data models, understanding of relational data modelling and denormalization technique. Fluency in SQL for building database transformations and extractions. Fluency with the SAC ecosystem and visualization capabilities. Past experience with SAP data around Finance and/or Operations processes will be appreciated. Certification in one or more of the following will be appreciated: SAC Data Analyst, Data Engineer, Low-Code/No-Code Developer. Desirable but not required: experience in building advanced visualisations or experiences using low/no-code, JavaScript and/or R. Excellent communication skills to be able to work independently with stakeholders. Energetic, organised and self-motivated. Fluent in business English.",Consulting,"['ETL Processes', 'DAX Queries', 'SAP Analytics', 'Data Modeling', 'Sql', 'Cloud Computing', 'Data Visualization', 'Sap Bw']",2025-06-12 15:23:15
Data Engineer-Data Platforms-AWS,IBM,5-7 Years,,Bengaluru,"Your role and responsibilities\nAs a senior SAP Consultant, you will serve as a client-facing practitioner working collaboratively with clients to deliver high-quality solutions and be a trusted business advisor with deep understanding of SAP Accelerate delivery methodology or equivalent and associated work products.\nYou will work on projects that assist clients in integrating strategy, process, technology, and information to enhance effectiveness, reduce costs, and improve profit and shareholder value. There are opportunities for you to acquire new skills, work across different disciplines, take on new challenges, and develop a comprehensive understanding of various industries.\nYour primary responsibilities include:\nStrategic SAP Solution Focus: Working across technical design, development, and implementation of SAP solutions for simplicity, amplification, and maintainability that meet client needs.\nComprehensive Solution Delivery: Involvement in strategy development and solution implementation, leveraging your knowledge of SAP and working with the latest technologies.\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nTotal 5 - 7+ years of experience in Data Management (DW, DL, Data Platform, Lakehouse) and Data Engineering skills\nMinimum 4+ years of experience in Big Data technologies with extensive data engineering experience in Spark / Python or Scala.\nMinimum 3 years of experience on Cloud Data Platforms on AWS; Exposure to streaming solutions and message brokers like Kafka technologies.\nExperience in AWS EMR / AWS Glue / DataBricks, AWS RedShift, DynamoDB\nGood to excellent SQL skills\nPreferred technical and professional experience\nCertification in AWS and Data Bricks or Cloudera Spark Certified developers",Information Technology,"['Data Engineer', 'Business Intelligence Applications', 'Python', 'Aws Ec2']",2025-06-12 15:23:16
AIML,Virtusa,7-10 Years,,Chennai,"Analyze application logs to identify data discrepancies, trends, and issues.\nPresent structured analytical insights and findings to business stakeholders.\nPerform Root Cause Analysis (RCA) by collaborating with cross-functional teams.\nAutomate manual processes through custom scripting or lightweight development.\nBuild and deploy minor Python-based enhancements.\nConduct Exploratory Data Analysis (EDA) and feature engineering to enhance existing models.\nBegin contributing to AI/ML model development, especially model drifts and enhancements.\nTranslate business requirements into technical specifications with cross-functional collaboration.\nMentor junior team members and define best practices for analytics and ML projects.\nCreate and maintain technical documentation and visual presentations.\nQualifications:\n710 years of experience in data analysis, particularly using SQL (any RDBMS).\nMinimum 2 years of hands-on experience in US Healthcare domain, preferably in Claims & Provider data.\nMinimum 1 year of experience in Python or Java development and interest in advancing Python skills.\nExperience or exposure to Cloud technologies like GCP, Cloud Run, and Vertex AI is a plus.\nFamiliarity with AI/ML techniques and platforms.\nWillingness to operate in Run & Operate model including analysis, support, and minor development.",Information Technology,"['Data Analysis', 'US Healthcare (Claims/Provider)', 'Cloud (GCP/Vertex AI)', 'AI/ML', 'Sql', 'Python']",2025-06-12 15:23:18
Qlik,Virtusa,3-6 Years,,Gurugram,"WE need Reporting developer to make changes in Existing dashboard.\nSkills:\nGood experinece and Handson in Qlik, QlikSense and Qlikview\nGood experinece and Handson in Data Modelling\nExperience in data analysis and data visualization tools.\nStrong SQL skills, ability to perform complex data analysis and data manipulations.",Information Technology,"['Data Analysis', 'Qlik', 'Qlik Sense', 'Qlikview', 'Data Modeling', 'Sql']",2025-06-12 15:23:19
Data Engineer-Data Integration,IBM,0-5 Years,,Pune,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviour's.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nExpertise in designing and implementing scalable data warehouse solutions on Snowflake, including schema design, performance tuning, and query optimization.\nStrong experience in building data ingestion and transformation pipelines using Talend to process structured and unstructured data from various sources.\nProficiency in integrating data from cloud platforms into Snowflake using Talend and native Snowflake capabilities.\nHands-on experience with dimensional and relational data modelling techniques to support analytics and reporting requirements\nPreferred technical and professional experience\nUnderstanding of optimizing Snowflake workloads, including clustering keys, caching strategies, and query profiling.\nAbility to implement robust data validation, cleansing, and governance frameworks within ETL processes.\nProficiency in SQL and/or Shell scripting for custom transformations and automation tasks",Software,"['snowflake', 'Data Pipelines', 'Talend', 'Data Modeling', 'Sql', 'Etl']",2025-06-12 15:23:20
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Gurugram,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns.\nYou'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Software,"['Elasticsearch', 'Splunk', 'Hadoop', 'Gcp', 'Data Replication', 'Apis']",2025-06-12 15:23:21
Data Engineer-Data Platforms,IBM,0-5 Years,,Bengaluru,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs a Big Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns.\nYou'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nAs a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions.\nYou will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nBig Data Developer, Hadoop, Hive, Spark, PySpark, Strong SQL.\nAbility to incorporate a variety of statistical and machine learning techniques. Basic understanding of Cloud (AWS,Azure, etc).\nAbility to use programming languages like Java, Python, Scala, etc., to build pipelines to extract and transform data from a repository to a data consumer\nAbility to use Extract, Transform, and Load (ETL) tools and/or data integration, or federation tools to prepare and transform data as needed.\nAbility to use leading edge tools such as Linux, SQL, Python, Spark, Hadoop and Java\nPreferred technical and professional experience\nBasic understanding or experience with predictive/prescriptive modeling skills\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions",Software,"['Hadoop Spark', 'Data Pipelines', 'Big Data', 'Pyspark', 'Sql Programming', 'Etl Tools']",2025-06-12 15:23:23
Data Engineer-Business Intelligence,IBM,2-5 Years,,Pune,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nProvide expertise in analysis, requirements gathering, design, coordination, customization, testing and support of reports, in client's environment\nDevelop and maintain a strong working relationship with business and technical members of the team\nRelentless focus on quality and continuous improvement\nPerform root cause analysis of reports issues\nDevelopment / evolutionary maintenance of the environment, performance, capability and availability.\nAssisting in defining technical requirements and developing solutions\nEffective content and source-code management, troubleshooting and debugging\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nTableau Desktop Specialist, SQL -Strong understanding of SQL for Querying database Good to have - Python ; Snowflake, Statistics, ETL experience.\nExtensive knowledge on using creating impactful visualization using Tableau.\nMust have thorough understanding of SQL & advance SQL (Joining & Relationships).\nMust have experience in working with different databases and how to blend & create relationships in Tableau.\nMust have extensive knowledge to creating Custom SQL to pull desired data from databases.Troubleshooting capabilities to debug Data controls\nPreferred technical and professional experience\nTroubleshooting capabilities to debug Data controls Capable of converting business requirements into workable model.\nGood communication skills, willingness to learn new technologies, Team Player, Self-Motivated, Positive Attitude.\nMust have thorough understanding of SQL & advance SQL (Joining & Relationships)",Information Technology,"['snowflake', 'Data Engineer', 'Business Intelligence Applications', 'Sqlit', 'Etl Design', 'Python']",2025-06-12 15:23:24
Data Engineer-Data Platforms-Google,IBM,6-10 Years,,Gurugram,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers) located in Hyderabad, Telangana, India. Our delivery centers provide deep technical and industry expertise to a wide range of public and private sector clients globally, offering locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour Role and Responsibilities\n6+ years of industry work experience.\nExperience extracting data from a variety of sources, and a desire to expand those skills.\nWorked on Google Looker tool.\nWorked on Big Query and GCP technologies.\nStrong SQL and Spark knowledge.\nExcellent Data Analysis skills. Must be comfortable with querying and analyzing large amounts of data on Hadoop HDFS using Hive and Spark.\nKnowledge of Financial Accounting is a bonus.\nWork independently with cross-functional teams and drive towards resolution.\nExperience with Object-oriented programming using Python and its design patterns.\nExperience handling Unix systems, for optimal usage to host enterprise web applications.\nGCP certifications preferred.\nPayments Industry Background good to have.\nCandidate who has been part of a Google Cloud Migration is an ideal Fit.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\n3-5 years of experience.\nIntuitive individual with an ability to manage change and proven time management.\nProven interpersonal skills while contributing to team effort by accomplishing related results as needed.\nUp-to-date technical knowledge by attending educational workshops, reviewing publications.\nPreferred Technical and Professional Experience\n6+ years of industry work experience.\nExperience extracting data from a variety of sources, and a desire to expand those skills.\nWorked on Google Looker tool.",Software,"['Looker', 'Gcp', 'BigQuery', 'Spark', 'Sql', 'Python']",2025-06-12 15:23:25
Data Engineer-Data Platforms,IBM,4-5 Years,,Mumbai,"Your role and responsibilities\nAs a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution.\nYour primary responsibilities include:\nLead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements.\nStrive for continuous improvements by testing the build solution and working under an agile framework.\nDiscover and implement the latest technologies trends to maximize and build creative solutions\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nExperience with Apache Spark (PySpark): In-depth knowledge of Spark's architecture, core APIs, and PySpark for distributed data processing.\nBig Data Technologies: Familiarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering Skills: Strong understanding of ETL pipelines, data modeling, and data warehousing concepts.\nStrong proficiency in Python: Expertise in Python programming with a focus on data processing and manipulation. Data Processing Frameworks: Knowledge of data processing libraries such as Pandas, NumPy.\nSQL Proficiency: Experience writing optimized SQL queries for large-scale data analysis and transformation.\nCloud Platforms: Experience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems\nPreferred technical and professional experience\nDefine, drive, and implement an architecture strategy and standards for end-to-end monitoring.\nPartner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering,\nGood to have detection and prevention tools for Company products and Platform and customer-facing",Information Technology,"['Sql', 'Python', 'Etl', 'Data Warehousing', 'Spark', 'Cloud Computing', 'Data Modeling']",2025-06-12 15:23:27
Sr. Developer AI Center of Excellence,IBM,4-8 Years,,Bengaluru,"Your Role & Responsibilities:\nLooking to make a significant impact This is your chance to become a key part of a dynamic team of talented professionals, leading the development and deployment of innovative, industry-leading, cloud-based AI services.\nWe are seeking an experienced AI & Cloud Software Engineer to join us. This role designing, developing, and deploying AI-based services. You will be instrumental in problem-solving, automating wide ranges of tasks, and interfacing with other teams andsolve complex problems.\nYour role and responsibilities\nResponsibilities:\nDevelop AI capabilities in IBM Cloud based applications\nDesign and be an avid coder who can get his hands dirty and be involved in the coding to the deepest level.\nWork in an agile environment of continuous deliverable.\nYou'll have access to all the technical training courses you need to become the expert you want to be.\nDefine all aspects of development from appropriate technology and workflow to coding standards\nCollaborate with other professionals to determine functional and non-functional requirements\nParticipate in technical reviews of requirements, specifications, designs, code and other artifacts.\nLearn new skills and adopt new practices readily in order to develop innovative and cutting-edge software products that maintain Company's technical leadership position.\nRequired education\nBachelor's Degree\nRequired technical and professional expertise\nRequired Skills\n57 years of hands-on full stack development experience\nExperience with AI/ML frameworks (TensorFlow, PyTorch, scikit-learn, LLMs, GenAI)\nStrong backend skills (Java, Python, Node.js), REST APIs, Kafka, GitHub\nSolid cloud experience (IBM Cloud, AWS, or Azure), Kubernetes, Docker, CI/CD\nExperience with databases like Cassandra, PostgreSQL, Redis, etc.\nExposure to microservices architecture and modern API design\nGood understanding of web tech: HTTP, JSON, JavaScript, HTML\nPassionate about constant, continuous learning and applying new technologies as well as mentoring others.\nKeen troubleshooting skills and strong verbal/written communication skills.\nPreferred technical and professional experience\nPreferred Skills\nMessaging tools (Kafka, RabbitMQ)\nOS knowledge (Linux distros), networking basics (TCP/IP, HTTP)\nExperience in SaaS, automation (Selenium/Puppeteer), and CI/CD pipelines\nOwnership mindset and passion for continuous learning\nWorking across global teams and collaborating across teams and organization boundaries\nFinding innovative ways to solve complex problems with cutting-edge technologies.",Information Technology,"['Ai', 'python', 'Java', 'Kubernetes', 'Tensorflow', 'Microservices']",2025-06-12 15:23:28
Space Planning Analyst,IBM,4-6 Years,,Bengaluru,"A career in IBM Consulting embraces long-term relationships and close collaboration with clients across the globe.\nIn this role, you will work for IBM BPO, part of Consulting that accelerates digital transformation using agile methodologies, process mining, and AI-powered workflows.\nYou'll work with visionaries across multiple industries to improve the hybrid cloud and AI journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio, including IBM Software and Red Hat.\nCuriosity and a constant quest for knowledge serve as the foundation to success in IBM Consulting. In your role, you'll be supported by mentors and coaches who will encourage you to challenge the norm, investigate ideas outside of your role, and come up with creative solutions resulting in groundbreaking impact for a wide network of clients. Our culture of evolution and empathy centers on long-term career growth and learning opportunities in an environment that embraces your unique skills and experience.\nYour role and responsibilities\nDevelop and deliver insightful, data-driven reports and dashboards that evaluate the effectiveness of space planning strategies across categories and store formats.\nAnalyze planogram performance and shelf space allocation to support category growth, maximize ROI, and improve in-store execution.\nLeverage tools like JDA/Blue Yonder, NIQ, Spins, and Circana to extract and interpret data, delivering actionable insights for merchandising and sales leadership.\nCollaborate with cross-functional teams to refine space planning initiatives based on data trends, market intelligence, and performance metrics.\nMonitor key KPIs, SLAs, and customer metrics to ensure alignment with business goals and continuous process improvement.\nVisualize complex data in a clear, engaging manner using Microsoft Power BI, Excel, and PowerPoint for both strategic and operational presentations.\nEnsure accuracy and consistency in planogram data and reporting outputs, supporting both internal stakeholders and external retail partners.\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nBachelor's or Master's degree in Computer Science, Statistics, Business Analytics, or a related field.\n46 years of experience in space planning analysis, data analytics, or retail reporting roles.\nHands-on experience with space planning tools (e.g., JDA/Blue Yonder, Apollo Retail), CPG reporting platforms (NIQ, Spins, Circana), and BI tools (Power BI, Excel).\nIn-depth understanding of CPG & retail merchandising strategies, planogram design, and space-to-sales analysis.\nPreferred technical and professional experience\nStrong analytical mindset with a high level of accuracy and attention to detail.\nExcellent communication, storytelling, and stakeholder engagement skills.\nProven ability to handle multiple projects simultaneously in a fast-paced retail or CPG environment.",Information Technology,"['Space Planning (JDA/Blue Yonder)', 'CPG Reporting (NIQ/Spins/Circana)', 'Planogram Analysis', 'Retail/CPG Domain Knowledge', 'Data Analytics', 'Power Bi']",2025-06-12 15:23:30
Data Engineer-Data Integration,IBM,4-7 Years,,Pune,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the client's needs\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our client's business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nDesign, develop, and maintain Ab Initio graphs for extracting, transforming, and loading (ETL) data from diverse sources to various target systems.\nImplement data quality and validation processes within Ab Initio. Data Modelling and Analysis.\nCollaborate with data architects and business analysts to understand data requirements and translate them into effective ETL processes.\nAnalyse and model data to ensure optimal ETL design and performance.\nAb Initio Components. . Utilize Ab Initio components such as Transform Functions, Rollup, Join, Normalize, and others to build scalable and efficient data integration solutions. Implement best practices for reusable Ab Initio components\nPreferred technical and professional experience\nOptimize Ab Initio graphs for performance, ensuring efficient data processing and minimal resource utilization. Conduct performance tuning and troubleshooting as needed. Collaboration. .\nWork closely with cross-functional teams, including data analysts, database administrators, and quality assurance, to ensure seamless integration of ETL processes.\nParticipate in design reviews and provide technical expertise to enhance overall solution quality Documentation",Information Technology,"['Ab Initio ETL development', 'data modeling and analysis', 'data pipeline design', 'data integration using Ab Initio components', 'collaboration with cross-functional teams.', 'Performance Tuning']",2025-06-12 15:23:40
Data Engineer-Data Platforms-AWS,IBM,3-4 Years,,Cochin / Kochi / Ernakulam,"Your role and responsibilities\nAs Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in the development of data solutions using Spark Framework with Python or Scala on Hadoop and AWS Cloud Data Platform\nResponsibilities:\nExperienced in building data pipelines to Ingest, process, and transform data from files, streams and databases. Process the data with Spark, Python, PySpark, Scala, and Hive, Hbase or other NoSQL databases on Cloud Data Platforms (AWS) or HDFS\nExperienced in develop efficient software code for multiple use cases leveraging Spark Framework / using Python or Scala and Big Data technologies for various use cases built on the platform\nExperience in developing streaming pipelines\nExperience to work with Hadoop / AWS eco system components to implement scalable solutions to meet the ever-increasing data volumes, using big data/cloud technologies Apache Spark, Kafka, any Cloud computing etc\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nMinimum 4+ years of experience in Big Data technologies with extensive data engineering experience in Spark / Python or Scala.\nMinimum 3 years of experience on Cloud Data Platforms on AWS;\nExperience in AWS EMR / AWS Glue / DataBricks, AWS RedShift, DynamoDB\nGood to excellent SQL skills\nExposure to streaming solutions and message brokers like Kafka technologies.\nPreferred technical and professional experience\nCertification in AWS and Data Bricks or Cloudera Spark Certified developers.",Information Technology,"['Data Platforms', 'Sql', 'Python', 'Data Warehousing', 'AWS', 'Big Data', 'Data Engineer']",2025-06-12 15:23:44
Data Engineer-Data Platforms-AWS,IBM,3-4 Years,,Cochin / Kochi / Ernakulam,"Your role and responsibilities\nAs Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in the development of data solutions using Spark Framework with Python or Scala on Hadoop and AWS Cloud Data Platform\nResponsibilities:\nExperienced in building data pipelines to Ingest, process, and transform data from files, streams and databases. Process the data with Spark, Python, PySpark, Scala, and Hive, Hbase or other NoSQL databases on Cloud Data Platforms (AWS) or HDFS\nExperienced in develop efficient software code for multiple use cases leveraging Spark Framework / using Python or Scala and Big Data technologies for various use cases built on the platform\nExperience in developing streaming pipelines\nExperience to work with Hadoop / AWS eco system components to implement scalable solutions to meet the ever-increasing data volumes, using big data/cloud technologies Apache Spark, Kafka, any Cloud computing etc\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nMinimum 4+ years of experience in Big Data technologies with extensive data engineering experience in Spark / Python or Scala.\nMinimum 3 years of experience on Cloud Data Platforms on AWS;\nExperience in AWS EMR / AWS Glue / DataBricks, AWS RedShift, DynamoDB\nGood to excellent SQL skills\nExposure to streaming solutions and message brokers like Kafka technologies.\nPreferred technical and professional experience\nCertification in AWS and Data Bricks or Cloudera Spark Certified developers.",Information Technology,"['Data Platforms', 'Sql', 'Python', 'Data Warehousing', 'AWS', 'Big Data', 'Data Engineer']",2025-06-12 15:23:45
Data Engineer-Data Platforms-Google,IBM,3-6 Years,,Pune,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviour's.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Information Technology,"['Data migration from Hadoop to GCP', 'data replication mechanisms', 'database modernization', 'stakeholder communication.', 'Api Development', 'Predictive Modeling']",2025-06-12 15:23:47
Data Scientist-MLOps/LLMOps,IBM,4-6 Years,,Mumbai,"Your role and responsibilities\nRole Overview:\nHiring an ML Engineer with experience in Cloudera ML to support end-to-end model development, deployment, and monitoring on the CDP platform.\nKey Responsibilities:\nDevelop and deploy models using CML workspaces\nBuild CI/CD pipelines for ML lifecycle\nIntegrate with governance and monitoring tools\nEnable secure model serving via REST APIs\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nSkills Required:\nExperience in Cloudera ML, Spark MLlib, or scikit-learn\nML pipeline automation (MLflow, Airflow, or equivalent)\nModel governance, lineage, and versioning\nAPI exposure for real-time inference",Information Technology,"['Model Deployment', 'Python', 'Machine Learning', 'Docker', 'Kubernetes', 'Terraform', 'data engineering']",2025-06-12 15:23:48
Data Scientist-Artificial Intelligence,IBM,3-6 Years,,Gurugram,"Required technical and professional expertise\nProof of Concept (POC) Development: Develop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions.\nHelp in showcasing the ability of Gen AI code assistant to refactor/rewrite and document code from one language to another\nDocument solution architectures, design decisions, implementation details, and lessons learned.\nStay up to date with the latest trends and advancements in AI, foundation models, and large language models.\nEvaluate emerging technologies, tools, and frameworks to assess their potential impact on solution design and implementation\nPreferred technical and professional experience\nExperience and working knowledge in COBOL & JAVA would be preferred\nHaving experience in Code generation, code matching & code translation leveraging LLM capabilities would be a Big plus\nDemonstrate a growth mindset to understand clients business processes and challenges",Information Technology,"['Hugging Face', 'Foundation Models', 'Large Language Models', 'Python', 'Tensorflow', 'Pytorch', 'Java', 'Cobol']",2025-06-12 15:23:49
Data Engineer,Virtusa,8-12 Years,,Hyderabad,"Experience:\n8+ years of experience in data engineering, specifically in cloud environments like AWS.\nProficiency in PySpark for distributed data processing and transformation.\nSolid experience with AWS Glue for ETL jobs and managing data workflows.\nHands-on experience with AWS Data Pipeline (DPL) for workflow orchestration.\nStrong experience with AWS services such as S3, Lambda, Redshift, RDS, and EC2.\nTechnical Skills:\nProficiency in Python and PySpark for data processing and transformation tasks.\nDeep understanding of ETL concepts and best practices.\nFamiliarity with AWS Glue (ETL jobs, Data Catalog, and Crawlers).\nExperience building and maintaining data pipelines with AWS Data Pipeline or similar orchestration tools.\nFamiliarity with AWS S3 for data storage and management, including file formats (CSV, Parquet, Avro).\nStrong knowledge of SQL for querying and manipulating relational and semi-structured data.\nExperience with Data Warehousing and Big Data technologies, specifically within AWS.\nAdditional Skills:\nExperience with AWS Lambda for serverless data processing and orchestration.\nUnderstanding of AWS Redshift for data warehousing and analytics.\nFamiliarity with Data Lakes, Amazon EMR, and Kinesis for streaming data processing.\nKnowledge of data governance practices, including data lineage and auditing.\nFamiliarity with CI/CD pipelines and Git for version control.\nExperience with Docker and containerization for building and deploying applications.\nDesign and Build Data Pipelines: Design, implement, and optimize data pipelines on AWS using PySpark, AWS Glue, and AWS Data Pipeline to automate data integration, transformation, and storage processes.\nETL Development: Develop and maintain Extract, Transform, and Load (ETL) processes using AWS Glue and PySpark to efficiently process large datasets.\nData Workflow Automation: Build and manage automated data workflows using AWS Data Pipeline, ensuring seamless scheduling, monitoring, and management of data jobs.\nData Integration: Work with different AWS data storage services (e.g., S3, Redshift, RDS) to ensure smooth integration and movement of data across platforms.\nOptimization and Scaling: Optimize and scale data pipelines for high performance and cost efficiency, utilizing AWS services like Lambda, S3, and EC2.",Information Technology,"['Snowflake design', 'ETL/ELT', 'Python', 'Sql', 'Data Warehousing', 'cloud platform']",2025-06-12 15:23:50
Data Engineer,Virtusa,10-12 Years,,Bengaluru,"Requirements\n10+ years of strong experience with data transformation & ETL on large data sets\nExperience with designing customer centric datasets (i.e., CRM, Call Center, Marketing, Offline, Point of\nSale etc.)\n5+ years of Data Modeling experience (i.e., Relational, Dimensional, Columnar, Big Data)\n5+ years of complex SQL or NoSQL experience\nExperience in advanced Data Warehouse concepts\nExperience in industry ETL tools (i.e., Informatica, Unifi)\nExperience with Business Requirements definition and management, structured analysis, process\ndesign, use case documentation\nExperience with Reporting Technologies (i.e., Tableau, PowerBI)\nexperience in professional software development\nDemonstrate exceptional organizational skills and ability to multi-task simultaneous different customer\nprojects\nStrong verbal & written communication skills to interface with Sales team & lead customers to\nsuccessful outcome\nMust be self-managed, proactive and customer focused\nDegree in Computer Science, Information Systems, Data Science, or related field",Information Technology,"['Python', 'Sql', 'Etl', 'BigQuery']",2025-06-12 15:23:52
Data Scientist-Artificial Intelligence,IBM,3-6 Years,,Bengaluru,"Required technical and professional expertise\nProof of Concept (POC) Development: Develop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions.\nHelp in showcasing the ability of Gen AI code assistant to refactor/rewrite and document code from one language to another\nDocument solution architectures, design decisions, implementation details, and lessons learned.\nStay up to date with the latest trends and advancements in AI, foundation models, and large language models.\nEvaluate emerging technologies, tools, and frameworks to assess their potential impact on solution design and implementation\nPreferred technical and professional experience\nExperience and working knowledge in COBOL & JAVA would be preferred\nHaving experience in Code generation, code matching & code translation leveraging LLM capabilities would be a Big plus\nDemonstrate a growth mindset to understand clients business processes and challenges",Information Technology,"['Hugging Face', 'Foundation Models', 'Large Language Models', 'Python', 'Tensorflow', 'Pytorch', 'Java', 'Cobol']",2025-06-12 15:23:53
Data Scientist-Artificial Intelligence,IBM,3-6 Years,,Bengaluru,"Required technical and professional expertise\nProof of Concept (POC) Development: Develop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions.\nHelp in showcasing the ability of Gen AI code assistant to refactor/rewrite and document code from one language to another\nDocument solution architectures, design decisions, implementation details, and lessons learned.\nStay up to date with the latest trends and advancements in AI, foundation models, and large language models.\nEvaluate emerging technologies, tools, and frameworks to assess their potential impact on solution design and implementation\nPreferred technical and professional experience\nExperience and working knowledge in COBOL & JAVA would be preferred\nHaving experience in Code generation, code matching & code translation leveraging LLM capabilities would be a Big plus\nDemonstrate a growth mindset to understand clients business processes and challenges",Information Technology,"['Hugging Face', 'Foundation Models', 'Large Language Models', 'Python', 'Tensorflow', 'Pytorch', 'Java', 'Cobol']",2025-06-12 15:23:54
Data Engineer,Virtusa,5-8 Years,,Chennai,8 Years overall IT experience with minimum 5 years of work experience in below tech skills\nStrong experience in Python Scripting and PySpark for data processing\nProficiency in SQL dealing with big data over Informatica ETL\nProven experience in Data quality and data optimization of data lake in Iceberg format with strong understanding of architecture\nExperience in AWS Glue jobs\nExperience in AWS cloud platform and its data services S3 Redshift Lambda EMR Airflow Postgres SNS Event bridge\nExpertise in BASH Shell scripting\nStrong understanding of healthcare data systems and experience leading data engineering teams\nExperience in Agile environments\nExcellent problem solving skills and attention to detail\nEffective communication and collaboration skills\nResponsibilities\nLeads development of data pipelines and architectures that handle large scale data sets\nDesigns constructs and tests data architecture aligned with business requirements\nProvides technical leadership for data projects ensuring best practices and high quality data solutions\nCollaborates with product finance and other business units to ensure data pipelines meet business requirements\nWork with DBT Data Build Tool for transforming raw data into actionable insights\nOversees development of data solutions that enable predictive and prescriptive analytics\nEnsures the technical quality of solutions managing data as it moves across environments\nAligns data architecture to Healthfirst solution architecture,Information Technology,"['Iceberg', 'Python', 'Pyspark', 'Sql', 'AWS Glue', 'Informatica']",2025-06-12 15:23:55
PROCESS DELIVERY SPECIALIST-RISK & COMPLIANCE,IBM,1-4 Years,,Bengaluru,"Your role and responsibilities\nAnalysts engage in risk, compliance, and financial crimes projects to serve Promontory clients within the banking and finance industry helping them meet and exceed regulatory expectations. Analysts are expected to positively contribute to Promontory's success in a variety of areas, including BSA, anti-money laundering surveillance and reporting assistance, OFAC sanctions compliance, and other areas relevant to today's heightened regulatory climate.\nAnalysts must possess excellent writing, research, analytical, and critical thinking skills, and other applicable experience that leads to success in the role, such as strong work ethic and natural curiosity. Analysts must be able to judiciously analyze, assess, and write clearly and concisely. The nature of this work requires individuals to be flexible, learn new skills, work within time constraints, and meet uncompromising quality requirements and production expectations while working closely with others in a dynamic team environment.\nApply logic and strong reasoning skills to conduct research for case analysis.\nUse sound decision-making skills to make recommendations based on research results.\nCompose comprehensive supporting narratives\nInterpret and apply project policies and procedures to direct work.\nMaintain high work product quality as outlined by each project specifications.\nEnsure work adheres to defined engagement policies and procedures\nManage work efficiently to meet production goals and project deliverables.\nContribute to developing individual and project goals and execute on tactical strategies for goal attainment.\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\n1+ year prior AML experience REQUIRED.\nCompliance experience at a financial institution is STRONGLY PREFERRED.\nRetail banking experience is PREFERRED.\nThe ability to work independently, take initiative, and able to adapt to change.\nAbility to quickly understand and maintain current know3ledge of banking regulations, concepts, and issues.\nQuickly learn new applications and client systems to conduct research.\nExercise sound judgment and observe the highest degree of confidentiality.\nAdept at multi-tasking and meeting deadlines in high-pressure environment.\nResults oriented team player with strong initiative and flexibility.\nStrong analytical and problem-solving abilities.\nSuperior writing skills with the ability to convey ideas clearly and succinctly.\nExemplary customer service towards both internal and external parties.\nExceptional research and processing skills with the ability to analyze large data sets, decipher higher risk attributes (transactional, geographical, product, customer type, etc.), and disposition appropriately.\nDemonstrate competency with computers, including proficiency with all Microsoft Office products, and should have prior experience with a variety of computer database and testing systems.\nPreferred technical and professional experience\nAn undergraduate degree in Business Administration, accounting, finance, or other related discipline; or equivalent combination of education and experience that is required for the specific job level.",Information Technology,"['Compliance', 'Retail Banking', 'Risk Analysis', 'Transaction Monitoring', 'Aml', 'Microsoft Office']",2025-06-12 15:23:57
Data Scientist-Artificial Intelligence,IBM,10-15 Years,,Hyderabad,"At IBM, work is more than a job - it's a calling: To build. To design. To code. To consult. To think along with clients and sell. To make markets. To invent. To collaborate. Not just to do something better, but to attempt things you've never thought possible. Are you ready to lead in this new era of technology and solve some of the world's most challenging problems If so, let's talk.\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nWe're seeking a results-driven and collaborative Software Development Manager to lead the design and development of IBM Consulting Advantage Platform. As a management leader, you'll collaborate with peers and stakeholders to ensure business continuity. You'll also be responsible for building and leading an impactful team of Developers & QA engineers, focusing on software developments, productivity improvements and fostering a culture of continuous learning and improvement.\nIn this role, you will be responsible for:\nLead a team of engineers to meet release dates along with committed deliverables on-time and with quality\nBalance priorities and work assignments across team members following agile processes to meet delivery schedules\nInterface with product management and offering managers to understand customer requirements and business prioritization\nDrive development activities, monitor progress, collaborate to align dependencies, remove blockers for team members and manage risks\nDevelop and implement effective strategies for software development, testing, and deployment\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\n10+ years of professional experience; 5+ years as team lead/manager\nExcellent organizational skills including attention to details, time management, and multi-tasking skills\nHands-on experience Experienced building Microservices & REST APIs using Java, and other related technologies\nExperience with Front End Development programming languages and design Frameworks\nStrong project management, organizational, problem-solving, communication, and collaboration skills\nPreferred technical and professional experience\nHands-on experience with SpringBoot, ReactJS, NodeJS etc\nExperience in working on a production SaaS application with SOC2 certification\nKnowledge of Containerisation technologies such as Kubernetes & Docker, and CI/CD pipelines such as Tekton, ArgoCD etc.",Information Technology,"['Team Leadership', 'Microservices Development', 'SpringBoot ReactJS NodeJS', 'Project Management.', 'Rest API Development', 'Front End Development', 'Data Science', 'Artificial Intelligence']",2025-06-12 15:23:59
Data Engineer-Data Platforms-Azure,IBM,3-6 Years,,Bengaluru,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nWe are seeking a skilled Azure Data Engineer with 5+ years of experience\nIncluding 3+ years of hands-on experience with ADF/Databricks\nThe ideal candidate Data bricks, Data Lake, Python programming skills.\nThe candidate will also have experience for deploying to data bricks.\nFamiliarity with Azure Data Factory\nPreferred technical and professional experience\nGood communication skills.\n3+ years of experience with ADF/DB/DataLake.\nAbility to communicate results to technical and non-technical audiences",Software,"['Azure', 'Databricks', 'Data Lake', 'Python', 'Adf', 'Elasticsearch']",2025-06-12 15:24:00
PROCESS DELIVERY SPECIALIST-RISK & COMPLIANCE,IBM,1-4 Years,,Bengaluru,"Your role and responsibilities\nAnalysts engage in risk, compliance, and financial crimes projects to serve Promontory clients within the banking and finance industry helping them meet and exceed regulatory expectations. Analysts are expected to positively contribute to Promontory's success in a variety of areas, including BSA, anti-money laundering surveillance and reporting assistance, OFAC sanctions compliance, and other areas relevant to today's heightened regulatory climate.\nAnalysts must possess excellent writing, research, analytical, and critical thinking skills, and other applicable experience that leads to success in the role, such as strong work ethic and natural curiosity. Analysts must be able to judiciously analyze, assess, and write clearly and concisely. The nature of this work requires individuals to be flexible, learn new skills, work within time constraints, and meet uncompromising quality requirements and production expectations while working closely with others in a dynamic team environment.\nApply logic and strong reasoning skills to conduct research for case analysis.\nUse sound decision-making skills to make recommendations based on research results.\nCompose comprehensive supporting narratives\nInterpret and apply project policies and procedures to direct work.\nMaintain high work product quality as outlined by each project specifications.\nEnsure work adheres to defined engagement policies and procedures\nManage work efficiently to meet production goals and project deliverables.\nContribute to developing individual and project goals and execute on tactical strategies for goal attainment.\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\n1+ year prior AML experience REQUIRED.\nCompliance experience at a financial institution is STRONGLY PREFERRED.\nRetail banking experience is PREFERRED.\nThe ability to work independently, take initiative, and able to adapt to change.\nAbility to quickly understand and maintain current know3ledge of banking regulations, concepts, and issues.\nQuickly learn new applications and client systems to conduct research.\nExercise sound judgment and observe the highest degree of confidentiality.\nAdept at multi-tasking and meeting deadlines in high-pressure environment.\nResults oriented team player with strong initiative and flexibility.\nStrong analytical and problem-solving abilities.\nSuperior writing skills with the ability to convey ideas clearly and succinctly.\nExemplary customer service towards both internal and external parties.\nExceptional research and processing skills with the ability to analyze large data sets, decipher higher risk attributes (transactional, geographical, product, customer type, etc.), and disposition appropriately.\nDemonstrate competency with computers, including proficiency with all Microsoft Office products, and should have prior experience with a variety of computer database and testing systems.\nPreferred technical and professional experience\nAn undergraduate degree in Business Administration, accounting, finance, or other related discipline; or equivalent combination of education and experience that is required for the specific job level.",Information Technology,"['Compliance', 'Retail Banking', 'Risk Analysis', 'Transaction Monitoring', 'Aml', 'Microsoft Office']",2025-06-12 15:24:01
PROCESS DELIVERY SPECIALIST-RISK & COMPLIANCE,IBM,1-4 Years,,Bengaluru,"Your role and responsibilities\nAnalysts engage in risk, compliance, and financial crimes projects to serve Promontory clients within the banking and finance industry helping them meet and exceed regulatory expectations. Analysts are expected to positively contribute to Promontory's success in a variety of areas, including BSA, anti-money laundering surveillance and reporting assistance, OFAC sanctions compliance, and other areas relevant to today's heightened regulatory climate.\nAnalysts must possess excellent writing, research, analytical, and critical thinking skills, and other applicable experience that leads to success in the role, such as strong work ethic and natural curiosity. Analysts must be able to judiciously analyze, assess, and write clearly and concisely. The nature of this work requires individuals to be flexible, learn new skills, work within time constraints, and meet uncompromising quality requirements and production expectations while working closely with others in a dynamic team environment.\nApply logic and strong reasoning skills to conduct research for case analysis.\nUse sound decision-making skills to make recommendations based on research results.\nCompose comprehensive supporting narratives\nInterpret and apply project policies and procedures to direct work.\nMaintain high work product quality as outlined by each project specifications.\nEnsure work adheres to defined engagement policies and procedures\nManage work efficiently to meet production goals and project deliverables.\nContribute to developing individual and project goals and execute on tactical strategies for goal attainment.\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\n1+ year prior AML experience REQUIRED.\nCompliance experience at a financial institution is STRONGLY PREFERRED.\nRetail banking experience is PREFERRED.\nThe ability to work independently, take initiative, and able to adapt to change.\nAbility to quickly understand and maintain current know3ledge of banking regulations, concepts, and issues.\nQuickly learn new applications and client systems to conduct research.\nExercise sound judgment and observe the highest degree of confidentiality.\nAdept at multi-tasking and meeting deadlines in high-pressure environment.\nResults oriented team player with strong initiative and flexibility.\nStrong analytical and problem-solving abilities.\nSuperior writing skills with the ability to convey ideas clearly and succinctly.\nExemplary customer service towards both internal and external parties.\nExceptional research and processing skills with the ability to analyze large data sets, decipher higher risk attributes (transactional, geographical, product, customer type, etc.), and disposition appropriately.\nDemonstrate competency with computers, including proficiency with all Microsoft Office products, and should have prior experience with a variety of computer database and testing systems.\nPreferred technical and professional experience\nAn undergraduate degree in Business Administration, accounting, finance, or other related discipline; or equivalent combination of education and experience that is required for the specific job level.",Information Technology,"['Compliance', 'Retail Banking', 'Risk Analysis', 'Transaction Monitoring', 'Aml', 'Microsoft Office']",2025-06-12 15:24:13
Data Engineer-Data Platforms-Azure,IBM,3-6 Years,,Hyderabad,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nWe are seeking a skilled Azure Data Engineer with 5+ years of experience\nIncluding 3+ years of hands-on experience with ADF/Databricks\nThe ideal candidate Data bricks, Data Lake, Python programming skills.\nThe candidate will also have experience for deploying to data bricks.\nFamiliarity with Azure Data Factory\nPreferred technical and professional experience\nGood communication skills.\n3+ years of experience with ADF/DB/DataLake.\nAbility to communicate results to technical and non-technical audiences",Software,"['Azure', 'Databricks', 'Data Lake', 'Python', 'Adf', 'Elasticsearch']",2025-06-12 15:24:17
Data Engineer-Data Platforms-Azure,IBM,3-6 Years,,Hyderabad,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nWe are seeking a skilled Azure Data Engineer with 5+ years of experience\nIncluding 3+ years of hands-on experience with ADF/Databricks\nThe ideal candidate Data bricks, Data Lake, Python programming skills.\nThe candidate will also have experience for deploying to data bricks.\nFamiliarity with Azure Data Factory\nPreferred technical and professional experience\nGood communication skills.\n3+ years of experience with ADF/DB/DataLake.\nAbility to communicate results to technical and non-technical audiences",Software,"['Azure', 'Databricks', 'Data Lake', 'Python', 'Adf', 'Elasticsearch']",2025-06-12 15:24:18
Datacenter Technician,IBM,2-5 Years,,Hyderabad,"If you are someone who:\nWants to understand what it takes to build a scalable, secure, and reliable service.\nDesires to deepen your technical expertise in all aspects of Site Reliability Engineering, including security, monitoring, automation, development, infrastructure, self-healing, and troubleshooting.\nIs a go-getter with an ownership mindset.\n...then we might be the right team for you!\nYour Role and Responsibilities\nAs a Site Reliability Engineer, you will be responsible for:\nApplying a logical, methodical, and analytical approach to isolate and solve technical problems.\nCommunicating and collaborating effectively with other technicians, departments, and customers in technical support situations.\nDemonstrating and applying extensive knowledge of the company's products.\nExercising limited discretion in deviating from standard practice to solve problems within your area of experience.\nResearching problems and recommending solutions.\nAssisting in the provision of on-job training.\nProfessionally processing and resolving asset request cases to support proper accounting for site inventory.\nMaintaining site inventory with zero discrepancies through strict adherence to asset management procedures.\nManaging inbound and outbound fulfillment to enable the achievement of business deliverables.\nEnsuring that any asset defects, damages, discrepancies, or deviations are escalated to management for timely support and resolution.\nWorking in shift rotations that may include day, evening, overnight, and/or weekends and holidays.\nWorking in various IBM Cloud locations in Chennai.\nRequired Education\nBachelor's Degree\nRequired Technical and Professional Expertise\n2+ Years of experience including:\nPhysical server hardware experience (assembling servers including motherboards, RAM, hard drives, RAID controllers, network cards, etc.). OS experience is a plus, with an emphasis on physical server hardware exposure.\nScheduling and performing hardware maintenances for IBM Cloud customers involving upgrades, downgrades, support requests, etc. This includes physical hardware upgrades/downgrades referring to server hardware like RAM, hard drives, processors, and network cards.\nTroubleshooting and resolving problems with basic physical network cable/device connections at the server/switch/stack for network devices in the Data Center physically.\n100% onsite experience working in a physical Data Center (no remote support).\n24/7 Operations with 100% onsite support (no remote or on-call support). This involves a rotating shift schedule with no specific shifts.\nResponding to UIP-related events around physical infrastructure.\nCoordinating with internal departments to resolve outage events related to faulty links, optics, or failed networking devices.\nPossessing physical infrastructure server knowledge along with a basic understanding of network devices (like network physical cabling, optics, and interconnectivities) to extend onsite support for remote network teams and Network ISP personnel.\nPerforming outage events under the supervision and guidance of Site Management.\nPreferred Technical and Professional Experience\nMay be directed to perform other duties consistent with training and skill levels required for this position.\nUnderstanding site capacity utilization and providing assistance with Management on capacity planning.",Software,"['SRE', 'Hardware', 'Troubleshooting', 'Inventory', 'Networking', 'Data Center']",2025-06-12 15:24:19
Market Analyst,Alstom,2-5 Years,,Bengaluru,"Supporting market price activities and competitive intelligence\nDeveloping and enriching analytical price models and databases\nProviding sales teams with up-to-date market price information\nCollaborating with solution platform directors to increase price competitiveness\nPerforming ad-hoc analysis on price-related topics for top management\nImproving data analytic tools and automation possibilities\nDegree in Economics, Business, Engineering, or related field\nExperience or understanding of market analysis and pricing strategies\nKnowledge of the rail or transportation industry (desirable but not essential)\nFamiliarity with data analytics and tools such as MS Excel\nA certification in data analysis or related field (advantageous)\nStrong analytical and problem-solving skills\nEffective communication and presentation abilities",Transportation,"['Data Analyst', 'Market Analyst', 'Pricing Analysis', 'Market Intelligence', 'competitive strategy', 'Business Analysis', 'Excel']",2025-06-12 15:24:21
Business Analyst,Virtusa,8-12 Years,,Bengaluru,"Qualification\nBachelors degree in Computer Science, Information Systems, Business Administration, or a related field.\n8-12 years of experience as a Business Analyst, preferably in a technical environment.\nStrong understanding of system integration, APIs, databases, and basic programming concepts.\nProficient in tools such as JIRA, Confluence, Microsoft Excel, Visio, Lucidchart, or equivalent.\nExperience with Agile/Scrum methodologies.\nExcellent problem-solving skills and attention to detail.\nStrong verbal and written communication skills.\nGather, document, and analyze business requirements and translate them into functional and technical specifications\nCollaborate with product owners, business stakeholders, and engineering teams to ensure alignment of solutions with business goals.\nWork with QA teams to define test cases and ensure quality delivery through user acceptance testing (UAT).\nPerform gap analysis, impact analysis, and risk assessments for proposed changes and enhancements.\nAct as a liaison between business units and technical teams throughout the software development life cycle (SDLC).",Information Technology,"['Documentation', 'SDLC Repos', 'User Story Documentation', 'Requirement Analysis', 'Agile']",2025-06-12 15:24:22
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Gurugram,"n this role, you'll work at one of our IBM Consulting Client Innovation Centers (Delivery Centers). These centers are where we deliver deep technical and industry expertise to a wide range of public and private sector clients globally. Our delivery centers offer clients locally based skills and technical expertise to drive innovation and adopt new technologies.\nYour Role and Responsibilities\nAs an Associate Software Developer at IBM, you will:\nHarness the power of data to unveil captivating stories and intricate patterns.\nContribute to data gathering, storage, and both batch and real-time processing.\nCollaborate closely with diverse teams, playing an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nYour responsibilities may include:\nImplementing and validating predictive models, as well as creating and maintaining statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques.\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants, and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or write programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nDevelop/Convert the database (Hadoop to GCP) of specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform.\nImplementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API.\nParticipate in the modernization roadmap journey.\nAnalyze discovery and analysis outcomes.\nLead discovery and analysis workshops/playbacks.\nIdentification of application dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare effort estimates, WBS, staffing plan, RACI, RAID, etc.\nLeads the team to adopt the right tools for various migration and modernization methods.\nPreferred Technical and Professional Experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions.\nAbility to communicate results to technical and non-technical audiences.",Software,"['cdc', 'data engineering', 'Hadoop', 'Gcp', 'Api Development', 'Predictive Modeling']",2025-06-12 15:24:23
Data Engineer-Data Modelling,IBM,0-5 Years,,Pune,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nTranslates business needs into a data model, providing expertise on data modeling tools and techniques for designing data models for applications and related systems.\nSkills include logical and physical data modeling, and knowledge of ERWin, MDM, and/or ETL.\nData modeling is a process used to define and analyze data requirements needed to support the business processes within the scope of corresponding information systems in organizations\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nTranslates business needs into a data model, providing expertise on data modeling tools and techniques for designing data models for applications and related systems.\nSkills include logical and physical data modeling, and knowledge of ERWin, MDM, and/or ETL.\nData modeling is a process used to define and analyze data requirements needed to support the business processes within the scope of corresponding information systems in organizations.\nTherefore, the process of data modeling involves professional data modelers working closely with business stakeholders, as well as potential users of the information system.\nThere are three different types of data models produced while progressing from requirements to the actual database to be used for the information system.\nPreferred technical and professional experience\nTranslates business needs into a data model, providing expertise on data modeling tools and techniques for designing data models for applications and related systems.\nSkills include logical and physical data modeling, and knowledge of ERWin, MDM, and/or ETL.\nData modeling is a process used to define and analyze data requirements needed to support the business processes within the scope of corresponding information systems in organizations.",Software,"['MDM', 'Logical modeling', 'physical modeling', 'Data Modeling', 'Erwin', 'Etl']",2025-06-12 15:24:24
Data Engineer-Data Integration,IBM,0-5 Years,,Pune,"As a Data Engineer at IBM, you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing. Collaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nYour Role and Responsibilities\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintaining statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques.\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants, and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nExpertise in designing and implementing scalable data warehouse solutions on Snowflake, including schema design, performance tuning, and query optimization.\nStrong experience in building data ingestion and transformation pipelines using Talend to process structured and unstructured data from various sources.\nProficiency in integrating data from cloud platforms into Snowflake using Talend and native Snowflake capabilities.\nHands-on experience with dimensional and relational data modeling techniques to support analytics and reporting requirements.\nPreferred Technical and Professional Experience\nUnderstanding of optimizing Snowflake workloads, including clustering keys, caching strategies, and query profiling.\nAbility to implement robust data validation, cleansing, and governance frameworks within ETL processes.\nProficiency in SQL and/or Shell scripting for custom transformations and automation tasks.",Software,"['snowflake', 'ETL Pipelines', 'Talend', 'Sql', 'Data Modeling', 'data engineering']",2025-06-12 15:24:26
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Gurugram,"As an Associate Software Developer at IBM, you'll harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured datasets. You'll work in one of our IBM Consulting Client Innovation Centers, delivering deep technical and industry expertise to a wide range of public and private sector clients.\nYour Role and Responsibilities\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintaining statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques.\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWorking in an Agile, collaborative environment, partnering with other scientists, engineers, consultants, and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuilding teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nDevelop/Convert the database (Hadoop to GCP) of specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform.\nImplement a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API.\nParticipate in the modernization roadmap journey.\nAnalyze discovery and analysis outcomes.\nLead discovery and analysis workshops/playbacks.\nIdentify application dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare effort estimates, WBS, staffing plan, RACI, RAID etc.\nLead the team to adopt the right tools for various migration and modernization methods.\nPreferred Technical and Professional Experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions.\nAbility to communicate results to technical and non-technical audiences.",Software,"['cdc', 'Gcp', 'data engineering', 'Hadoop', 'Api', 'Elasticsearch']",2025-06-12 15:24:27
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Noida,"n this role, you'll work at one of our IBM Consulting Client Innovation Centers (Delivery Centers). These centers are where we deliver deep technical and industry expertise to a wide range of public and private sector clients globally. Our delivery centers offer clients locally based skills and technical expertise to drive innovation and adopt new technologies.\nYour Role and Responsibilities\nAs an Associate Software Developer at IBM, you will:\nHarness the power of data to unveil captivating stories and intricate patterns.\nContribute to data gathering, storage, and both batch and real-time processing.\nCollaborate closely with diverse teams, playing an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nYour responsibilities may include:\nImplementing and validating predictive models, as well as creating and maintaining statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques.\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants, and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or write programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nDevelop/Convert the database (Hadoop to GCP) of specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform.\nImplementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API.\nParticipate in the modernization roadmap journey.\nAnalyze discovery and analysis outcomes.\nLead discovery and analysis workshops/playbacks.\nIdentification of application dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare effort estimates, WBS, staffing plan, RACI, RAID, etc.\nLeads the team to adopt the right tools for various migration and modernization methods.\nPreferred Technical and Professional Experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions.\nAbility to communicate results to technical and non-technical audiences.",Software,"['cdc', 'data engineering', 'Hadoop', 'Gcp', 'Api Development', 'Predictive Modeling']",2025-06-12 15:24:29
Cloud & AI Engineer,IBM,6-10 Years,,Bengaluru,"Your role and responsibilities\nBack end development with GoLang\nExpertise in Kubernetes/OpenShift, Cloud service providers.\nKnowledge of Generative AI, and ability to integrate AI to applications.\nAbility to pick up new areas based on business requirements\nExcellent communication skills\nRequired education\nBachelor's Degree\nRequired technical and professional expertise\n6+ years of overall experience in backend development. Excellent understanding of system design and best practices.\n6+ years of application development with GoLang development.\nGood level of expertise in Kubernetes or OpenShift, use of Docker/Podman and Cloud service providers.\nGood level of knowledge of CNI, container native storage.\nExpertise in Version Control - Git\nExperience using cloud technologies (AWS/GCP/Azure/IBM Cloud)\nExperience with Ansible and Shell scripting\nProficient in Linux administration.\nExperience of IaC (Terraform)\nExpertise in Version Control - Git\nDesign functional DevOps application lifecycle. Good understanding of CICD pipelines such as Jenkins. Should have hands-on in writing and debugging Jenkinsfile\nExperience using build tools such as Maven, Gradle, Make, Ant\nKnowledge of AI - Pytorch, TensorFlow, Scikit,Generative AI, and ability to integrate AI functionalities to applications.\nABOUT BUSINESS UNIT\nIBM Systems helps IT leaders think differently about their infrastructure. IBM servers and storage are no longer inanimate - they can understand, reason, and learn so our clients can innovate while avoiding IT issues. Our systems power the world's most important industries and our clients are the architects of the future.Join us to help build our leading-edge technology portfoliodesigned for cognitive business and optimized for cloud computing.",Information Technology,"['Generative AI', 'Golang', 'Kubernetes', 'Terraform', 'Jenkins', 'Cloud Technologies']",2025-06-12 15:24:30
BI-MSTR,Virtusa,4-6 Years,,Bengaluru,"We are seeking a skilled and detail oriented MicroStrategy Developer to design, develop, and maintain BI solutions using the MicroStrategy platform. The ideal candidate will have strong experience in creating dashboards/dossiers, reports, and schema objects, and be able to translate business requirements into actionable insights.\nKey Responsibilities:\nDevelop and maintain MicroStrategy reports, dashboards/dossiers, visualizations, and documents.\nDesign schema objects such as attributes, facts, hierarchies, and transformations & application objects such as metrics, filters, prompts so on.\nCollaborate with business analysts, data architects, and stakeholders to understand reporting requirements.\nNeed to create reports or datasets based on custom SQL queries in MicroStrategy.\nWrite SQL to perform E2E data validation and analyse performance.\nOptimize and troubleshoot existing reports and dashboards for performance and usability.\nEnsure data integrity and accuracy across BI deliverables.\nPerform unit testing and support user acceptance testing (UAT).\nProvide technical support and troubleshooting for MicroStrategy applications.\nRequired Qualifications:\nStrong understanding of data warehousing concepts and dimensional modelling.\nProficiency in SQL and experience working with relational databases (e.g., Oracle, SQL Server, Teradata).\nExperience in creating dashboards/dossiers, scorecards, and custom visualizations.\nExperience working on Big data environment as source for data for Reporting.\nFamiliarity with MicroStrategy Architect, Developer, and Web.\nAbility to translate business needs into technical solutions.\nStrong analytical and problem-solving skills.",Information Technology,"['Data Analysis', 'Business Intelligence', 'Micro Strategy', 'Sql', 'Tableau', 'Data Visualization', 'Uat']",2025-06-12 15:24:31
BI-MSTR,Virtusa,4-6 Years,,Bengaluru,"We are seeking a skilled and detail oriented MicroStrategy Developer to design, develop, and maintain BI solutions using the MicroStrategy platform. The ideal candidate will have strong experience in creating dashboards/dossiers, reports, and schema objects, and be able to translate business requirements into actionable insights.\nKey Responsibilities:\nDevelop and maintain MicroStrategy reports, dashboards/dossiers, visualizations, and documents.\nDesign schema objects such as attributes, facts, hierarchies, and transformations & application objects such as metrics, filters, prompts so on.\nCollaborate with business analysts, data architects, and stakeholders to understand reporting requirements.\nNeed to create reports or datasets based on custom SQL queries in MicroStrategy.\nWrite SQL to perform E2E data validation and analyse performance.\nOptimize and troubleshoot existing reports and dashboards for performance and usability.\nEnsure data integrity and accuracy across BI deliverables.\nPerform unit testing and support user acceptance testing (UAT).\nProvide technical support and troubleshooting for MicroStrategy applications.\nRequired Qualifications:\nStrong understanding of data warehousing concepts and dimensional modelling.\nProficiency in SQL and experience working with relational databases (e.g., Oracle, SQL Server, Teradata).\nExperience in creating dashboards/dossiers, scorecards, and custom visualizations.\nExperience working on Big data environment as source for data for Reporting.\nFamiliarity with MicroStrategy Architect, Developer, and Web.\nAbility to translate business needs into technical solutions.\nStrong analytical and problem-solving skills.",Information Technology,"['Data Analysis', 'Reporting', 'Sql', 'Tableau']",2025-06-12 15:24:33
BI-MSTR,Virtusa,5-8 Years,,Bengaluru,"We are seeking a skilled and detail oriented MicroStrategy Developer to design, develop, and maintain BI solutions using the MicroStrategy platform. The ideal candidate will have strong experience in creating dashboards/dossiers, reports, and schema objects, and be able to translate business requirements into actionable insights.\nKey Responsibilities:\nDevelop and maintain MicroStrategy reports, dashboards/dossiers, visualizations, and documents.\nDesign schema objects such as attributes, facts, hierarchies, and transformations & application objects such as metrics, filters, prompts so on.\nCollaborate with business analysts, data architects, and stakeholders to understand reporting requirements.\nNeed to create reports or datasets based on custom SQL queries in MicroStrategy.\nWrite SQL to perform E2E data validation and analyse performance.\nOptimize and troubleshoot existing reports and dashboards for performance and usability.\nEnsure data integrity and accuracy across BI deliverables.\nPerform unit testing and support user acceptance testing (UAT).\nProvide technical support and troubleshooting for MicroStrategy applications.\nRequired Qualifications:\nStrong understanding of data warehousing concepts and dimensional modelling.\nProficiency in SQL and experience working with relational databases (e.g., Oracle, SQL Server, Teradata).\nExperience in creating dashboards/dossiers, scorecards, and custom visualizations.\nExperience working on Big data environment as source for data for Reporting.\nFamiliarity with MicroStrategy Architect, Developer, and Web.\nAbility to translate business needs into technical solutions.\nStrong analytical and problem-solving skills.",Information Technology,"['Data Analysis', 'Reporting', 'Sql', 'Tableau']",2025-06-12 15:24:34
Data Engineer-Data Platforms-Azure,IBM,3-6 Years,,Bengaluru,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nWe are seeking a skilled Azure Data Engineer with 5+ years of experience\nIncluding 3+ years of hands-on experience with ADF/Databricks\nThe ideal candidate Data bricks, Data Lake, Python programming skills.\nThe candidate will also have experience for deploying to data bricks.\nFamiliarity with Azure Data Factory\nPreferred technical and professional experience\nGood communication skills.\n3+ years of experience with ADF/DB/DataLake.\nAbility to communicate results to technical and non-technical audiences",Software,"['Azure', 'Databricks', 'Data Lake', 'Python', 'Adf', 'Elasticsearch']",2025-06-12 15:24:45
PROCESS DELIVERY SPECIALIST-RISK & COMPLIANCE,IBM,1-4 Years,,Bengaluru,"Your role and responsibilities\nAnalysts engage in risk, compliance, and financial crimes projects to serve Promontory clients within the banking and finance industry helping them meet and exceed regulatory expectations. Analysts are expected to positively contribute to Promontory's success in a variety of areas, including BSA, anti-money laundering surveillance and reporting assistance, OFAC sanctions compliance, and other areas relevant to today's heightened regulatory climate.\nAnalysts must possess excellent writing, research, analytical, and critical thinking skills, and other applicable experience that leads to success in the role, such as strong work ethic and natural curiosity. Analysts must be able to judiciously analyze, assess, and write clearly and concisely. The nature of this work requires individuals to be flexible, learn new skills, work within time constraints, and meet uncompromising quality requirements and production expectations while working closely with others in a dynamic team environment.\nApply logic and strong reasoning skills to conduct research for case analysis.\nUse sound decision-making skills to make recommendations based on research results.\nCompose comprehensive supporting narratives\nInterpret and apply project policies and procedures to direct work.\nMaintain high work product quality as outlined by each project specifications.\nEnsure work adheres to defined engagement policies and procedures\nManage work efficiently to meet production goals and project deliverables.\nContribute to developing individual and project goals and execute on tactical strategies for goal attainment.\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\n1+ year prior AML experience REQUIRED.\nCompliance experience at a financial institution is STRONGLY PREFERRED.\nRetail banking experience is PREFERRED.\nThe ability to work independently, take initiative, and able to adapt to change.\nAbility to quickly understand and maintain current know3ledge of banking regulations, concepts, and issues.\nQuickly learn new applications and client systems to conduct research.\nExercise sound judgment and observe the highest degree of confidentiality.\nAdept at multi-tasking and meeting deadlines in high-pressure environment.\nResults oriented team player with strong initiative and flexibility.\nStrong analytical and problem-solving abilities.\nSuperior writing skills with the ability to convey ideas clearly and succinctly.\nExemplary customer service towards both internal and external parties.\nExceptional research and processing skills with the ability to analyze large data sets, decipher higher risk attributes (transactional, geographical, product, customer type, etc.), and disposition appropriately.\nDemonstrate competency with computers, including proficiency with all Microsoft Office products, and should have prior experience with a variety of computer database and testing systems.\nPreferred technical and professional experience\nAn undergraduate degree in Business Administration, accounting, finance, or other related discipline; or equivalent combination of education and experience that is required for the specific job level.",Information Technology,"['Compliance', 'Retail Banking', 'Risk Analysis', 'Transaction Monitoring', 'Aml', 'Microsoft Office']",2025-06-12 15:24:50
PROCESS DELIVERY SPECIALIST-RISK & COMPLIANCE,IBM,1-4 Years,,Bengaluru,"Your role and responsibilities\nAnalysts engage in risk, compliance, and financial crimes projects to serve Promontory clients within the banking and finance industry helping them meet and exceed regulatory expectations. Analysts are expected to positively contribute to Promontory's success in a variety of areas, including BSA, anti-money laundering surveillance and reporting assistance, OFAC sanctions compliance, and other areas relevant to today's heightened regulatory climate.\nAnalysts must possess excellent writing, research, analytical, and critical thinking skills, and other applicable experience that leads to success in the role, such as strong work ethic and natural curiosity. Analysts must be able to judiciously analyze, assess, and write clearly and concisely. The nature of this work requires individuals to be flexible, learn new skills, work within time constraints, and meet uncompromising quality requirements and production expectations while working closely with others in a dynamic team environment.\nApply logic and strong reasoning skills to conduct research for case analysis.\nUse sound decision-making skills to make recommendations based on research results.\nCompose comprehensive supporting narratives\nInterpret and apply project policies and procedures to direct work.\nMaintain high work product quality as outlined by each project specifications.\nEnsure work adheres to defined engagement policies and procedures\nManage work efficiently to meet production goals and project deliverables.\nContribute to developing individual and project goals and execute on tactical strategies for goal attainment.\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\n1+ year prior AML experience REQUIRED.\nCompliance experience at a financial institution is STRONGLY PREFERRED.\nRetail banking experience is PREFERRED.\nThe ability to work independently, take initiative, and able to adapt to change.\nAbility to quickly understand and maintain current know3ledge of banking regulations, concepts, and issues.\nQuickly learn new applications and client systems to conduct research.\nExercise sound judgment and observe the highest degree of confidentiality.\nAdept at multi-tasking and meeting deadlines in high-pressure environment.\nResults oriented team player with strong initiative and flexibility.\nStrong analytical and problem-solving abilities.\nSuperior writing skills with the ability to convey ideas clearly and succinctly.\nExemplary customer service towards both internal and external parties.\nExceptional research and processing skills with the ability to analyze large data sets, decipher higher risk attributes (transactional, geographical, product, customer type, etc.), and disposition appropriately.\nDemonstrate competency with computers, including proficiency with all Microsoft Office products, and should have prior experience with a variety of computer database and testing systems.\nPreferred technical and professional experience\nAn undergraduate degree in Business Administration, accounting, finance, or other related discipline; or equivalent combination of education and experience that is required for the specific job level.",Information Technology,"['Compliance', 'Retail Banking', 'Risk Analysis', 'Transaction Monitoring', 'Aml', 'Microsoft Office']",2025-06-12 15:24:51
Data Consultant-Data Governance,IBM,3-6 Years,,Hyderabad,"In this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviour's.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nExperience in the integration efforts between Alation and Manta, ensuring seamless data flow and compatibility.\nCollaborate with cross-functional teams to gather requirements and design solutions that leverage both Alation and Manta platforms effectively..\nDevelop and maintain data governance processes and standards within Alation, leveraging Manta's data lineage capabilities..\nAnalyze data lineage and metadata to provide insights into data quality, compliance, and usage patterns\nPreferred technical and professional experience\nLead the evaluation and implementation of new features and updates for both Alation and Manta platforms\nEnsuring alignment with organizational goals and objectives.\nDrive continuous improvement initiatives to enhance the efficiency and effectiveness of data management processes, leveraging Alati",Information Technology,"['Alation', 'Manta', 'Data Governance', 'Data Lineage', 'Predictive Modeling', 'Elasticsearch']",2025-06-12 15:24:52
BO Developer,Virtusa,5-10 Years,,Chennai,"Solid technical background and experience in reporting, Business Intelligence applications\nUnderstand and implement solution in support of physical data models necessary to support business intelligence reporting initiatives\nDesign, develop, test, and support SQL stored procedures, functions\nCommunicate directly with other BI team members to confirm requirements and clarify business rules\nDefine and design the universe, BI Dashboards and reports\nDevelop, Support and improve existing BO systems including both Universe and Reports\nResponsible for documenting modifications of pre-existing development new development for peer reference and knowledge transfer\nAt least basic Linux/Weblogic skills\nBO Java SDK beneficial but not essential (in case he needs to migrate the workflow tool)\nStrong exp in Developing Business Objects reports and dashboard solutions\nDesign, develop, test, and support SQL stored procedures, functions\nDefine and design the universe, BI Dashboards and reports",Information Technology,"['Business Objects', 'BI Dashboards', 'SQL Stored Procedures', 'Universe Design', 'Linux', 'Weblogic']",2025-06-12 15:24:54
Data Engineer-Data Platforms-Google,IBM,2-5 Years,,Gurugram,"Your role and responsibilities\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Information Technology,"['Sql', 'Python', 'Etl', 'BigQuery', 'Data Modeling', 'Data Warehousing', 'Cloud Storage']",2025-06-12 15:24:55
Data Engineer,IBM,3-10 Years,,"Navi Mumbai, Mumbai City, Mumbai","Role Overview:\nAs a Big Data Engineer, you'll design and build robust data pipelines on Cloudera using Spark (Scala/PySpark) for ingestion, transformation, and processing of high-volume data from banking systems.\nKey Responsibilities:\nBuild scalable batch and real-time ETL pipelines using Spark and Hive\nIntegrate structured and unstructured data sources\nPerform performance tuning and code optimization\nSupport orchestration and job scheduling (NiFi, Airflow)\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nExperience: 315 years\nProficiency in PySpark/Scala with Hive/Impala\nExperience with data partitioning, bucketing, and optimization\nFamiliarity with Kafka, Iceberg, NiFi is a must\nKnowledge of banking or financial datasets is a plus",Information Technology,"['Sql', 'Python', 'Hadoop', 'Etl', 'Data Warehousing', 'Nosql', 'Cloud Computing', 'Data Modeling']",2025-06-12 15:24:56
BI-MSTR,Virtusa,4-6 Years,,Bengaluru,"We are seeking a skilled and detail oriented MicroStrategy Developer to design, develop, and maintain BI solutions using the MicroStrategy platform. The ideal candidate will have strong experience in creating dashboards/dossiers, reports, and schema objects, and be able to translate business requirements into actionable insights.\nKey Responsibilities:\nDevelop and maintain MicroStrategy reports, dashboards/dossiers, visualizations, and documents.\nDesign schema objects such as attributes, facts, hierarchies, and transformations & application objects such as metrics, filters, prompts so on.\nCollaborate with business analysts, data architects, and stakeholders to understand reporting requirements.\nNeed to create reports or datasets based on custom SQL queries in MicroStrategy.\nWrite SQL to perform E2E data validation and analyse performance.\nOptimize and troubleshoot existing reports and dashboards for performance and usability.\nEnsure data integrity and accuracy across BI deliverables.\nPerform unit testing and support user acceptance testing (UAT).\nProvide technical support and troubleshooting for MicroStrategy applications.\nRequired Qualifications:\nStrong understanding of data warehousing concepts and dimensional modelling.\nProficiency in SQL and experience working with relational databases (e.g., Oracle, SQL Server, Teradata).\nExperience in creating dashboards/dossiers, scorecards, and custom visualizations.\nExperience working on Big data environment as source for data for Reporting.\nFamiliarity with MicroStrategy Architect, Developer, and Web.\nAbility to translate business needs into technical solutions.\nStrong analytical and problem-solving skills.",Information Technology,"['Data Analysis', 'Business Intelligence', 'Micro Strategy', 'Sql', 'Tableau', 'Data Visualization', 'Uat']",2025-06-12 15:24:57
BI-MSTR,Virtusa,4-6 Years,,Bengaluru,"We are seeking a skilled and detail oriented MicroStrategy Developer to design, develop, and maintain BI solutions using the MicroStrategy platform. The ideal candidate will have strong experience in creating dashboards/dossiers, reports, and schema objects, and be able to translate business requirements into actionable insights.\nKey Responsibilities:\nDevelop and maintain MicroStrategy reports, dashboards/dossiers, visualizations, and documents.\nDesign schema objects such as attributes, facts, hierarchies, and transformations & application objects such as metrics, filters, prompts so on.\nCollaborate with business analysts, data architects, and stakeholders to understand reporting requirements.\nNeed to create reports or datasets based on custom SQL queries in MicroStrategy.\nWrite SQL to perform E2E data validation and analyse performance.\nOptimize and troubleshoot existing reports and dashboards for performance and usability.\nEnsure data integrity and accuracy across BI deliverables.\nPerform unit testing and support user acceptance testing (UAT).\nProvide technical support and troubleshooting for MicroStrategy applications.\nRequired Qualifications:\nStrong understanding of data warehousing concepts and dimensional modelling.\nProficiency in SQL and experience working with relational databases (e.g., Oracle, SQL Server, Teradata).\nExperience in creating dashboards/dossiers, scorecards, and custom visualizations.\nExperience working on Big data environment as source for data for Reporting.\nFamiliarity with MicroStrategy Architect, Developer, and Web.\nAbility to translate business needs into technical solutions.\nStrong analytical and problem-solving skills.",Information Technology,"['Data Analysis', 'Reporting', 'Sql', 'Tableau']",2025-06-12 15:24:58
BI-MSTR,Virtusa,4-6 Years,,Bengaluru,"We are seeking a skilled and detail oriented MicroStrategy Developer to design, develop, and maintain BI solutions using the MicroStrategy platform. The ideal candidate will have strong experience in creating dashboards/dossiers, reports, and schema objects, and be able to translate business requirements into actionable insights.\nKey Responsibilities:\nDevelop and maintain MicroStrategy reports, dashboards/dossiers, visualizations, and documents.\nDesign schema objects such as attributes, facts, hierarchies, and transformations & application objects such as metrics, filters, prompts so on.\nCollaborate with business analysts, data architects, and stakeholders to understand reporting requirements.\nNeed to create reports or datasets based on custom SQL queries in MicroStrategy.\nWrite SQL to perform E2E data validation and analyse performance.\nOptimize and troubleshoot existing reports and dashboards for performance and usability.\nEnsure data integrity and accuracy across BI deliverables.\nPerform unit testing and support user acceptance testing (UAT).\nProvide technical support and troubleshooting for MicroStrategy applications.\nRequired Qualifications:\nStrong understanding of data warehousing concepts and dimensional modelling.\nProficiency in SQL and experience working with relational databases (e.g., Oracle, SQL Server, Teradata).\nExperience in creating dashboards/dossiers, scorecards, and custom visualizations.\nExperience working on Big data environment as source for data for Reporting.\nFamiliarity with MicroStrategy Architect, Developer, and Web.\nAbility to translate business needs into technical solutions.\nStrong analytical and problem-solving skills.",Information Technology,"['Data Analysis', 'Reporting', 'Sql', 'Tableau']",2025-06-12 15:24:59
Cloud & AI Engineer,IBM,6-10 Years,,Bengaluru,"Your role and responsibilities\nBack end development with GoLang\nExpertise in Kubernetes/OpenShift, Cloud service providers.\nKnowledge of Generative AI, and ability to integrate AI to applications.\nAbility to pick up new areas based on business requirements\nExcellent communication skills\nRequired education\nBachelor's Degree\nRequired technical and professional expertise\n6+ years of overall experience in backend development. Excellent understanding of system design and best practices.\n6+ years of application development with GoLang development.\nGood level of expertise in Kubernetes or OpenShift, use of Docker/Podman and Cloud service providers.\nGood level of knowledge of CNI, container native storage.\nExpertise in Version Control - Git\nExperience using cloud technologies (AWS/GCP/Azure/IBM Cloud)\nExperience with Ansible and Shell scripting\nProficient in Linux administration.\nExperience of IaC (Terraform)\nExpertise in Version Control - Git\nDesign functional DevOps application lifecycle. Good understanding of CICD pipelines such as Jenkins. Should have hands-on in writing and debugging Jenkinsfile\nExperience using build tools such as Maven, Gradle, Make, Ant\nKnowledge of AI - Pytorch, TensorFlow, Scikit,Generative AI, and ability to integrate AI functionalities to applications.\nABOUT BUSINESS UNIT\nIBM Systems helps IT leaders think differently about their infrastructure. IBM servers and storage are no longer inanimate - they can understand, reason, and learn so our clients can innovate while avoiding IT issues. Our systems power the world's most important industries and our clients are the architects of the future.Join us to help build our leading-edge technology portfoliodesigned for cognitive business and optimized for cloud computing.",Information Technology,"['Generative AI', 'Golang', 'Kubernetes', 'Terraform', 'Jenkins', 'Cloud Technologies']",2025-06-12 15:25:00
Data Engineer,Virtusa,6-8 Years,,Hyderabad,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools",Information Technology,"['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 15:25:02
Data Engineer-Business Intelligence,IBM,3-5 Years,,Pune,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nThe IBM Client Innovation Center (CIC) is an innovative and exciting part of IBM, working as IBM Global Business Services technical delivery partner. We continually innovate for our customers, employees, and shareholders. Our technical skills range from Application Development, Testing Services, Technical Support through to Cloud and Artificial Intelligence. Your work will power IBM and its clients globally, collaborating and integrating code and processes into enterprise systems. You will have access to the latest education, tools and technology, and a limitless career path with the world's technology leader. Come to IBM and make a global impact!\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nProvide expertise in analysis, requirements gathering, design, coordination, customization, testing and support of reports, in client's environment\nDevelop and maintain a strong working relationship with business and technical members of the team\nRelentless focus on quality and continuous improvement\nPerform root cause analysis of reports issues\nDevelopment / evolutionary maintenance of the environment, performance, capability and availability\nAssisting in defining technical requirements and developing solutions\nEffective content and source-code management, troubleshooting and debugging\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nTableau Desktop Specialist, SQL - Strong understanding of SQL for Querying database\nGood to have - Python; Snowflake, Statistics, ETL experience\nExtensive knowledge on using and creating impactful visualization using Tableau\nMust have thorough understanding of SQL & advanced SQL (Joining & Relationships)\nMust have experience in working with different databases and how to blend & create relationships in Tableau\nMust have extensive knowledge in creating Custom SQL to pull desired data from databases\nTroubleshooting capabilities to debug data controls\nPreferred technical and professional experience\nTroubleshooting capabilities to debug data controls\nCapable of converting business requirements into workable model\nGood communication skills, willingness to learn new technologies, Team Player, Self-Motivated, Positive Attitude\nMust have thorough understanding of SQL & advanced SQL (Joining & Relationships)",Information Technology,"['SQL (Advanced)', 'Custom SQL', 'ETL Concepts', 'Data Troubleshooting', 'Tableau', 'Data Visualization']",2025-06-12 15:25:03
Data Engineer-Business Intelligence,IBM,5-7 Years,,Pune,"Required technical and professional expertise\n5+ years of experience with BI tools, with expertise and/or certification in at least one major BI platform Tableau preferred.\nAdvanced knowledge of SQL, including the ability to write complex stored procedures, views, and functions.\nProven capability in data storytelling and visualization, delivering actionable insights through compelling presentations.\nExcellent communication skills, with the ability to convey complex analytical findings to non-technical stakeholders in a clear, concise, and meaningful way.\n5.Identifying and analyzing industry trends, geographic variations, competitor strategies, and emerging customer behavior\nPreferred technical and professional experience\nTroubleshooting capabilities to debug Data controls Capable of converting business requirements into workable model.\nGood communication skills, willingness to learn new technologies, Team Player, Self-Motivated, Positive Attitude.\nMust have thorough understanding of SQL & advance SQL (Joining & Relationships)",Information Technology,"['snowflake', 'Tableau', 'Data Visualization', 'Sql', 'Python', 'Etl']",2025-06-12 15:25:04
Data Engineer-Business Intelligence,IBM,3-6 Years,,Pune,"our role and responsibilities\nProvide expertise in analysis, requirements gathering, design, coordination, customization, testing and support of reports, in client's environment\nDevelop and maintain a strong working relationship with business and technical members of the team\nRelentless focus on quality and continuous improvement\nPerform root cause analysis of reports issues\nDevelopment / evolutionary maintenance of the environment, performance, capability and availability.\nAssisting in defining technical requirements and developing solutions\nEffective content and source-code management, troubleshooting and debugging.\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nCognos Developer & Admin Required. Education: The resource should be full time MCA/M. Tech/B. Tech/B.E. and should preferably have relevant certifications\nExperience: The resource should have a minimum of 3 years of experience of working in the in BI DW projects in areas pertaining to reporting and visualization using cognos.\nThe resources shall have worked in at least two projects where they were involved in developing reporting/ visualization\nHe shall have good understanding of UNIX.\nShould be well conversant in English and should have excellent writing, MIS, communication, time management and multi-tasking skill\nPreferred technical and professional experience\nExperience with various cloud and integration platforms (e.g. AWS, Google, Azure)\nAgile mindset ability to process changes of priorities and requests, ownership, critical thinking\nExperience with an ETL/Data Integration tool (eg. IBM InfoSphere DataStage, Azure Data Factory, Informatica PowerCenter)\nABOUT BUSINESS UNIT\nIBM Consulting is IBM's consulting and global professional services business, with market leading capabilities in business and technology transformation. With deep expertise in many industries, we offer strategy, experience, technology, and operations services to many of the most innovative and valuable companies in the world. Our people are focused on accelerating our clients businesses through the power of collaboration. We believe in the power of technology responsibly used to help people, partners and the planet.",Information Technology,"['Reporting', 'cloud platforms', 'Cognos', 'UNIX', 'Data Visualization', 'Etl']",2025-06-12 15:25:06
Data Engineer-Data Platforms,IBM,4-5 Years,,Bengaluru,"Required technical and professional expertise\n4-7 years of experience in big data engineering, data integration, and distributed computing.\nStrong skills in Apache Spark, PySpark, Kafka, SQL, and Cloudera Data Platform (CDP).\nProficiency in Python or Scala for data processing.\nExperience with data pipeline orchestration tools (Apache Airflow, Stonebranch UDM).\nUnderstanding of data security, encryption, and compliance frameworks\nPreferred technical and professional experience\nExperience in banking or financial services data platforms.\nExposure to Denodo for data virtualization and DGraph for graph-based insights.\nFamiliarity with cloud data platforms (AWS, Azure, GCP).\nCertifications in Cloudera Data Engineering, IBM Data Engineering, or AWS Data Analytics",Information Technology,"['Sql', 'Python', 'Etl', 'Data Warehousing', 'Spark', 'Cloud Computing', 'Data Modeling']",2025-06-12 15:25:14
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Gurugram,"As an Associate Software Developer at IBM, you'll harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured datasets. You'll work in one of our IBM Consulting Client Innovation Centers, delivering deep technical and industry expertise to a wide range of public and private sector clients.\nYour Role and Responsibilities\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintaining statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques.\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWorking in an Agile, collaborative environment, partnering with other scientists, engineers, consultants, and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuilding teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nDevelop/Convert the database (Hadoop to GCP) of specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform.\nImplement a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API.\nParticipate in the modernization roadmap journey.\nAnalyze discovery and analysis outcomes.\nLead discovery and analysis workshops/playbacks.\nIdentify application dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare effort estimates, WBS, staffing plan, RACI, RAID etc.\nLead the team to adopt the right tools for various migration and modernization methods.\nPreferred Technical and Professional Experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions.\nAbility to communicate results to technical and non-technical audiences.",Software,"['cdc', 'Gcp', 'data engineering', 'Hadoop', 'Api', 'Elasticsearch']",2025-06-12 15:25:17
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Gurugram,"As an Associate Software Developer at IBM, you'll harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured datasets. You'll work in one of our IBM Consulting Client Innovation Centers, delivering deep technical and industry expertise to a wide range of public and private sector clients.\nYour Role and Responsibilities\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintaining statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques.\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWorking in an Agile, collaborative environment, partnering with other scientists, engineers, consultants, and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuilding teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nDevelop/Convert the database (Hadoop to GCP) of specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform.\nImplement a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API.\nParticipate in the modernization roadmap journey.\nAnalyze discovery and analysis outcomes.\nLead discovery and analysis workshops/playbacks.\nIdentify application dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare effort estimates, WBS, staffing plan, RACI, RAID etc.\nLead the team to adopt the right tools for various migration and modernization methods.\nPreferred Technical and Professional Experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions.\nAbility to communicate results to technical and non-technical audiences.",Software,"['cdc', 'Gcp', 'data engineering', 'Hadoop', 'Api', 'Elasticsearch']",2025-06-12 15:25:18
Business Analyst,Virtusa,8-12 Years,,Bengaluru,"Qualification\nBachelors degree in Computer Science, Information Systems, Business Administration, or a related field.\n8-12 years of experience as a Business Analyst, preferably in a technical environment.\nStrong understanding of system integration, APIs, databases, and basic programming concepts.\nProficient in tools such as JIRA, Confluence, Microsoft Excel, Visio, Lucidchart, or equivalent.\nExperience with Agile/Scrum methodologies.\nExcellent problem-solving skills and attention to detail.\nStrong verbal and written communication skills.\nGather, document, and analyze business requirements and translate them into functional and technical specifications\nCollaborate with product owners, business stakeholders, and engineering teams to ensure alignment of solutions with business goals.\nWork with QA teams to define test cases and ensure quality delivery through user acceptance testing (UAT).\nPerform gap analysis, impact analysis, and risk assessments for proposed changes and enhancements.\nAct as a liaison between business units and technical teams throughout the software development life cycle (SDLC).",Information Technology,"['Documentation', 'SDLC Repos', 'User Story Documentation', 'Requirement Analysis', 'Agile']",2025-06-12 15:25:19
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Gurugram,"n this role, you'll work at one of our IBM Consulting Client Innovation Centers (Delivery Centers). These centers are where we deliver deep technical and industry expertise to a wide range of public and private sector clients globally. Our delivery centers offer clients locally based skills and technical expertise to drive innovation and adopt new technologies.\nYour Role and Responsibilities\nAs an Associate Software Developer at IBM, you will:\nHarness the power of data to unveil captivating stories and intricate patterns.\nContribute to data gathering, storage, and both batch and real-time processing.\nCollaborate closely with diverse teams, playing an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nYour responsibilities may include:\nImplementing and validating predictive models, as well as creating and maintaining statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques.\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants, and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or write programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nDevelop/Convert the database (Hadoop to GCP) of specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform.\nImplementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API.\nParticipate in the modernization roadmap journey.\nAnalyze discovery and analysis outcomes.\nLead discovery and analysis workshops/playbacks.\nIdentification of application dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare effort estimates, WBS, staffing plan, RACI, RAID, etc.\nLeads the team to adopt the right tools for various migration and modernization methods.\nPreferred Technical and Professional Experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions.\nAbility to communicate results to technical and non-technical audiences.",Software,"['cdc', 'data engineering', 'Hadoop', 'Gcp', 'Api Development', 'Predictive Modeling']",2025-06-12 15:25:20
Data Engineer-Business Intelligence,IBM,3-5 Years,,Pune,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nThe IBM Client Innovation Center (CIC) is an innovative and exciting part of IBM, working as IBM Global Business Services technical delivery partner. We continually innovate for our customers, employees, and shareholders. Our technical skills range from Application Development, Testing Services, Technical Support through to Cloud and Artificial Intelligence. Your work will power IBM and its clients globally, collaborating and integrating code and processes into enterprise systems. You will have access to the latest education, tools and technology, and a limitless career path with the world's technology leader. Come to IBM and make a global impact!\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nProvide expertise in analysis, requirements gathering, design, coordination, customization, testing and support of reports, in client's environment\nDevelop and maintain a strong working relationship with business and technical members of the team\nRelentless focus on quality and continuous improvement\nPerform root cause analysis of reports issues\nDevelopment / evolutionary maintenance of the environment, performance, capability and availability\nAssisting in defining technical requirements and developing solutions\nEffective content and source-code management, troubleshooting and debugging\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nTableau Desktop Specialist, SQL - Strong understanding of SQL for Querying database\nGood to have - Python; Snowflake, Statistics, ETL experience\nExtensive knowledge on using and creating impactful visualization using Tableau\nMust have thorough understanding of SQL & advanced SQL (Joining & Relationships)\nMust have experience in working with different databases and how to blend & create relationships in Tableau\nMust have extensive knowledge in creating Custom SQL to pull desired data from databases\nTroubleshooting capabilities to debug data controls\nPreferred technical and professional experience\nTroubleshooting capabilities to debug data controls\nCapable of converting business requirements into workable model\nGood communication skills, willingness to learn new technologies, Team Player, Self-Motivated, Positive Attitude\nMust have thorough understanding of SQL & advanced SQL (Joining & Relationships)",Information Technology,"['SQL (Advanced)', 'Custom SQL', 'ETL Concepts', 'Data Troubleshooting', 'Tableau', 'Data Visualization']",2025-06-12 15:25:21
Data Engineer-Business Intelligence,IBM,5-7 Years,,Pune,"Required technical and professional expertise\n5+ years of experience with BI tools, with expertise and/or certification in at least one major BI platform Tableau preferred.\nAdvanced knowledge of SQL, including the ability to write complex stored procedures, views, and functions.\nProven capability in data storytelling and visualization, delivering actionable insights through compelling presentations.\nExcellent communication skills, with the ability to convey complex analytical findings to non-technical stakeholders in a clear, concise, and meaningful way.\n5.Identifying and analyzing industry trends, geographic variations, competitor strategies, and emerging customer behavior\nPreferred technical and professional experience\nTroubleshooting capabilities to debug Data controls Capable of converting business requirements into workable model.\nGood communication skills, willingness to learn new technologies, Team Player, Self-Motivated, Positive Attitude.\nMust have thorough understanding of SQL & advance SQL (Joining & Relationships)",Information Technology,"['snowflake', 'Tableau', 'Data Visualization', 'Sql', 'Python', 'Etl']",2025-06-12 15:25:22
Data Engineer-Business Intelligence,IBM,3-6 Years,,Pune,"our role and responsibilities\nProvide expertise in analysis, requirements gathering, design, coordination, customization, testing and support of reports, in client's environment\nDevelop and maintain a strong working relationship with business and technical members of the team\nRelentless focus on quality and continuous improvement\nPerform root cause analysis of reports issues\nDevelopment / evolutionary maintenance of the environment, performance, capability and availability.\nAssisting in defining technical requirements and developing solutions\nEffective content and source-code management, troubleshooting and debugging.\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nCognos Developer & Admin Required. Education: The resource should be full time MCA/M. Tech/B. Tech/B.E. and should preferably have relevant certifications\nExperience: The resource should have a minimum of 3 years of experience of working in the in BI DW projects in areas pertaining to reporting and visualization using cognos.\nThe resources shall have worked in at least two projects where they were involved in developing reporting/ visualization\nHe shall have good understanding of UNIX.\nShould be well conversant in English and should have excellent writing, MIS, communication, time management and multi-tasking skill\nPreferred technical and professional experience\nExperience with various cloud and integration platforms (e.g. AWS, Google, Azure)\nAgile mindset ability to process changes of priorities and requests, ownership, critical thinking\nExperience with an ETL/Data Integration tool (eg. IBM InfoSphere DataStage, Azure Data Factory, Informatica PowerCenter)\nABOUT BUSINESS UNIT\nIBM Consulting is IBM's consulting and global professional services business, with market leading capabilities in business and technology transformation. With deep expertise in many industries, we offer strategy, experience, technology, and operations services to many of the most innovative and valuable companies in the world. Our people are focused on accelerating our clients businesses through the power of collaboration. We believe in the power of technology responsibly used to help people, partners and the planet.",Information Technology,"['Reporting', 'cloud platforms', 'Cognos', 'UNIX', 'Data Visualization', 'Etl']",2025-06-12 15:25:23
Data Engineer,Virtusa,6-8 Years,,Hyderabad,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools",Information Technology,"['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 15:25:25
Data Engineer-Data Modelling,IBM,0-5 Years,,Pune,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nTranslates business needs into a data model, providing expertise on data modeling tools and techniques for designing data models for applications and related systems.\nSkills include logical and physical data modeling, and knowledge of ERWin, MDM, and/or ETL.\nData modeling is a process used to define and analyze data requirements needed to support the business processes within the scope of corresponding information systems in organizations\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nTranslates business needs into a data model, providing expertise on data modeling tools and techniques for designing data models for applications and related systems.\nSkills include logical and physical data modeling, and knowledge of ERWin, MDM, and/or ETL.\nData modeling is a process used to define and analyze data requirements needed to support the business processes within the scope of corresponding information systems in organizations.\nTherefore, the process of data modeling involves professional data modelers working closely with business stakeholders, as well as potential users of the information system.\nThere are three different types of data models produced while progressing from requirements to the actual database to be used for the information system.\nPreferred technical and professional experience\nTranslates business needs into a data model, providing expertise on data modeling tools and techniques for designing data models for applications and related systems.\nSkills include logical and physical data modeling, and knowledge of ERWin, MDM, and/or ETL.\nData modeling is a process used to define and analyze data requirements needed to support the business processes within the scope of corresponding information systems in organizations.",Software,"['MDM', 'Logical modeling', 'physical modeling', 'Data Modeling', 'Erwin', 'Etl']",2025-06-12 15:25:26
Data Engineer-Data Integration,IBM,0-5 Years,,Pune,"As a Data Engineer at IBM, you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing. Collaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nYour Role and Responsibilities\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintaining statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques.\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants, and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nExpertise in designing and implementing scalable data warehouse solutions on Snowflake, including schema design, performance tuning, and query optimization.\nStrong experience in building data ingestion and transformation pipelines using Talend to process structured and unstructured data from various sources.\nProficiency in integrating data from cloud platforms into Snowflake using Talend and native Snowflake capabilities.\nHands-on experience with dimensional and relational data modeling techniques to support analytics and reporting requirements.\nPreferred Technical and Professional Experience\nUnderstanding of optimizing Snowflake workloads, including clustering keys, caching strategies, and query profiling.\nAbility to implement robust data validation, cleansing, and governance frameworks within ETL processes.\nProficiency in SQL and/or Shell scripting for custom transformations and automation tasks.",Software,"['snowflake', 'ETL Pipelines', 'Talend', 'Sql', 'Data Modeling', 'data engineering']",2025-06-12 15:25:27
BI-MSTR,Virtusa,4-6 Years,,Bengaluru,"We are seeking a skilled and detail oriented MicroStrategy Developer to design, develop, and maintain BI solutions using the MicroStrategy platform. The ideal candidate will have strong experience in creating dashboards/dossiers, reports, and schema objects, and be able to translate business requirements into actionable insights.\nKey Responsibilities:\nDevelop and maintain MicroStrategy reports, dashboards/dossiers, visualizations, and documents.\nDesign schema objects such as attributes, facts, hierarchies, and transformations & application objects such as metrics, filters, prompts so on.\nCollaborate with business analysts, data architects, and stakeholders to understand reporting requirements.\nNeed to create reports or datasets based on custom SQL queries in MicroStrategy.\nWrite SQL to perform E2E data validation and analyse performance.\nOptimize and troubleshoot existing reports and dashboards for performance and usability.\nEnsure data integrity and accuracy across BI deliverables.\nPerform unit testing and support user acceptance testing (UAT).\nProvide technical support and troubleshooting for MicroStrategy applications.\nRequired Qualifications:\nStrong understanding of data warehousing concepts and dimensional modelling.\nProficiency in SQL and experience working with relational databases (e.g., Oracle, SQL Server, Teradata).\nExperience in creating dashboards/dossiers, scorecards, and custom visualizations.\nExperience working on Big data environment as source for data for Reporting.\nFamiliarity with MicroStrategy Architect, Developer, and Web.\nAbility to translate business needs into technical solutions.\nStrong analytical and problem-solving skills.",Information Technology,"['Data Analysis', 'Business Intelligence', 'Micro Strategy', 'Sql', 'Tableau', 'Data Visualization', 'Uat']",2025-06-12 15:25:28
BI-MSTR,Virtusa,4-6 Years,,Bengaluru,"We are seeking a skilled and detail oriented MicroStrategy Developer to design, develop, and maintain BI solutions using the MicroStrategy platform. The ideal candidate will have strong experience in creating dashboards/dossiers, reports, and schema objects, and be able to translate business requirements into actionable insights.\nKey Responsibilities:\nDevelop and maintain MicroStrategy reports, dashboards/dossiers, visualizations, and documents.\nDesign schema objects such as attributes, facts, hierarchies, and transformations & application objects such as metrics, filters, prompts so on.\nCollaborate with business analysts, data architects, and stakeholders to understand reporting requirements.\nNeed to create reports or datasets based on custom SQL queries in MicroStrategy.\nWrite SQL to perform E2E data validation and analyse performance.\nOptimize and troubleshoot existing reports and dashboards for performance and usability.\nEnsure data integrity and accuracy across BI deliverables.\nPerform unit testing and support user acceptance testing (UAT).\nProvide technical support and troubleshooting for MicroStrategy applications.\nRequired Qualifications:\nStrong understanding of data warehousing concepts and dimensional modelling.\nProficiency in SQL and experience working with relational databases (e.g., Oracle, SQL Server, Teradata).\nExperience in creating dashboards/dossiers, scorecards, and custom visualizations.\nExperience working on Big data environment as source for data for Reporting.\nFamiliarity with MicroStrategy Architect, Developer, and Web.\nAbility to translate business needs into technical solutions.\nStrong analytical and problem-solving skills.",Information Technology,"['Data Analysis', 'Reporting', 'Sql', 'Tableau']",2025-06-12 15:25:30
BI-MSTR,Virtusa,5-8 Years,,Bengaluru,"We are seeking a skilled and detail oriented MicroStrategy Developer to design, develop, and maintain BI solutions using the MicroStrategy platform. The ideal candidate will have strong experience in creating dashboards/dossiers, reports, and schema objects, and be able to translate business requirements into actionable insights.\nKey Responsibilities:\nDevelop and maintain MicroStrategy reports, dashboards/dossiers, visualizations, and documents.\nDesign schema objects such as attributes, facts, hierarchies, and transformations & application objects such as metrics, filters, prompts so on.\nCollaborate with business analysts, data architects, and stakeholders to understand reporting requirements.\nNeed to create reports or datasets based on custom SQL queries in MicroStrategy.\nWrite SQL to perform E2E data validation and analyse performance.\nOptimize and troubleshoot existing reports and dashboards for performance and usability.\nEnsure data integrity and accuracy across BI deliverables.\nPerform unit testing and support user acceptance testing (UAT).\nProvide technical support and troubleshooting for MicroStrategy applications.\nRequired Qualifications:\nStrong understanding of data warehousing concepts and dimensional modelling.\nProficiency in SQL and experience working with relational databases (e.g., Oracle, SQL Server, Teradata).\nExperience in creating dashboards/dossiers, scorecards, and custom visualizations.\nExperience working on Big data environment as source for data for Reporting.\nFamiliarity with MicroStrategy Architect, Developer, and Web.\nAbility to translate business needs into technical solutions.\nStrong analytical and problem-solving skills.",Information Technology,"['Data Analysis', 'Reporting', 'Sql', 'Tableau']",2025-06-12 15:25:31
Cloud & AI Engineer,IBM,6-10 Years,,Bengaluru,"Your role and responsibilities\nBack end development with GoLang\nExpertise in Kubernetes/OpenShift, Cloud service providers.\nKnowledge of Generative AI, and ability to integrate AI to applications.\nAbility to pick up new areas based on business requirements\nExcellent communication skills\nRequired education\nBachelor's Degree\nRequired technical and professional expertise\n6+ years of overall experience in backend development. Excellent understanding of system design and best practices.\n6+ years of application development with GoLang development.\nGood level of expertise in Kubernetes or OpenShift, use of Docker/Podman and Cloud service providers.\nGood level of knowledge of CNI, container native storage.\nExpertise in Version Control - Git\nExperience using cloud technologies (AWS/GCP/Azure/IBM Cloud)\nExperience with Ansible and Shell scripting\nProficient in Linux administration.\nExperience of IaC (Terraform)\nExpertise in Version Control - Git\nDesign functional DevOps application lifecycle. Good understanding of CICD pipelines such as Jenkins. Should have hands-on in writing and debugging Jenkinsfile\nExperience using build tools such as Maven, Gradle, Make, Ant\nKnowledge of AI - Pytorch, TensorFlow, Scikit,Generative AI, and ability to integrate AI functionalities to applications.\nABOUT BUSINESS UNIT\nIBM Systems helps IT leaders think differently about their infrastructure. IBM servers and storage are no longer inanimate - they can understand, reason, and learn so our clients can innovate while avoiding IT issues. Our systems power the world's most important industries and our clients are the architects of the future.Join us to help build our leading-edge technology portfoliodesigned for cognitive business and optimized for cloud computing.",Information Technology,"['Generative AI', 'Golang', 'Kubernetes', 'Terraform', 'Jenkins', 'Cloud Technologies']",2025-06-12 15:25:33
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Noida,"n this role, you'll work at one of our IBM Consulting Client Innovation Centers (Delivery Centers). These centers are where we deliver deep technical and industry expertise to a wide range of public and private sector clients globally. Our delivery centers offer clients locally based skills and technical expertise to drive innovation and adopt new technologies.\nYour Role and Responsibilities\nAs an Associate Software Developer at IBM, you will:\nHarness the power of data to unveil captivating stories and intricate patterns.\nContribute to data gathering, storage, and both batch and real-time processing.\nCollaborate closely with diverse teams, playing an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nYour responsibilities may include:\nImplementing and validating predictive models, as well as creating and maintaining statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques.\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants, and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or write programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nDevelop/Convert the database (Hadoop to GCP) of specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform.\nImplementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API.\nParticipate in the modernization roadmap journey.\nAnalyze discovery and analysis outcomes.\nLead discovery and analysis workshops/playbacks.\nIdentification of application dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare effort estimates, WBS, staffing plan, RACI, RAID, etc.\nLeads the team to adopt the right tools for various migration and modernization methods.\nPreferred Technical and Professional Experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions.\nAbility to communicate results to technical and non-technical audiences.",Software,"['cdc', 'data engineering', 'Hadoop', 'Gcp', 'Api Development', 'Predictive Modeling']",2025-06-12 15:25:42
Data Engineer-Data Platforms-Azure,IBM,3-6 Years,,Bengaluru,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nEstablish and implement best practices for DBT workflows, ensuring efficiency, reliability, and maintainability.\nCollaborate with data analysts, engineers, and business teams to align data transformations with business needs.\nMonitor and troubleshoot data pipelines to ensure accuracy and performance.\nWork with Azure-based cloud technologies to support data storage, transformation, and processing\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nStrong MS SQL, Azure Databricks experience\nImplement and manage data models in DBT, data transformation and alignment with business requirements.\nIngest raw, unstructured data into structured datasets to cloud object store.\nUtilize DBT to convert raw, unstructured data into structured datasets, enabling efficient analysis and reporting.\nWrite and optimize SQL queries within DBT to enhance data transformation processes and improve overall performance\nPreferred technical and professional experience\nEstablish best DBT processes to improve performance, scalability, and reliability.\nDesign, develop, and maintain scalable data models and transformations using DBT in conjunction with Databricks\nProven interpersonal skills while contributing to team effort by accomplishing related results as required",Information Technology,"['dbt', 'Data pipelines', 'Structured datasets', 'Raw data ingestion', 'Data Storage', 'Data Processing', 'MS SQL', 'Azure Databricks', 'Data Modeling', 'Data Transformation', 'Sql Queries', 'Azure']",2025-06-12 15:25:48
Data Engineer-Data Platforms-Azure,IBM,3-6 Years,,Bengaluru,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nEstablish and implement best practices for DBT workflows, ensuring efficiency, reliability, and maintainability.\nCollaborate with data analysts, engineers, and business teams to align data transformations with business needs.\nMonitor and troubleshoot data pipelines to ensure accuracy and performance.\nWork with Azure-based cloud technologies to support data storage, transformation, and processing\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nStrong MS SQL, Azure Databricks experience\nImplement and manage data models in DBT, data transformation and alignment with business requirements.\nIngest raw, unstructured data into structured datasets to cloud object store.\nUtilize DBT to convert raw, unstructured data into structured datasets, enabling efficient analysis and reporting.\nWrite and optimize SQL queries within DBT to enhance data transformation processes and improve overall performance\nPreferred technical and professional experience\nEstablish best DBT processes to improve performance, scalability, and reliability.\nDesign, develop, and maintain scalable data models and transformations using DBT in conjunction with Databricks\nProven interpersonal skills while contributing to team effort by accomplishing related results as required",Information Technology,"['dbt', 'Data pipelines', 'Structured datasets', 'Raw data ingestion', 'Data Storage', 'Data Processing', 'MS SQL', 'Azure Databricks', 'Data Modeling', 'Data Transformation', 'Sql Queries', 'Azure']",2025-06-12 15:25:49
Data Engineer-Data Platforms,IBM,4-5 Years,,Bengaluru,"Required technical and professional expertise\n4-7 years of experience in big data engineering, data integration, and distributed computing.\nStrong skills in Apache Spark, PySpark, Kafka, SQL, and Cloudera Data Platform (CDP).\nProficiency in Python or Scala for data processing.\nExperience with data pipeline orchestration tools (Apache Airflow, Stonebranch UDM).\nUnderstanding of data security, encryption, and compliance frameworks\nPreferred technical and professional experience\nExperience in banking or financial services data platforms.\nExposure to Denodo for data virtualization and DGraph for graph-based insights.\nFamiliarity with cloud data platforms (AWS, Azure, GCP).\nCertifications in Cloudera Data Engineering, IBM Data Engineering, or AWS Data Analytics",Information Technology,"['Sql', 'Python', 'Etl', 'Data Warehousing', 'Spark', 'Cloud Computing', 'Data Modeling']",2025-06-12 15:25:51
Data Insights & Visualization Practition,Accenture India,0-2 Years,,Ahmedabad,"Project Role: Data Insights & Visualization Practitioner\nProject Role Description:\nAs a Data Insights & Visualization Practitioner, you will create interactive interfaces that enable users to understand, interpret, and communicate complex data and insights. Your typical day will involve wrangling, analyzing, and preparing data to ensure the delivery of relevant, consistent, timely, and actionable insights. You will leverage modern business intelligence, storytelling, and web-based visualization tools to create interactive dashboards, reports, and emerging VIS/BI artifacts. Additionally, you will use and customize (Gen)AI and AI-powered VIS/BI capabilities to enable a dialog with data.\nMust Have Skills:\nProficiency in Java Full Stack Development\nGood to Have Skills:\nN/A\nMinimum Experience Required:\n0-2 years\nEducational Qualification:\n15 years of full-time education\nSummary:\nYou will play a crucial role in delivering actionable insights and visualizations that drive decision-making.\nRoles & Responsibilities:\nExpected to build knowledge and support the team.\nParticipate in problem-solving discussions.\nCreate interactive interfaces that enable users to understand complex data.\nWrangle, analyze, and prepare data for timely and actionable insights.\nLeverage modern business intelligence tools to create dashboards and reports.\nUse and customize (Gen)AI and AI-powered VIS/BI capabilities.\nCollaborate with cross-functional teams to gather requirements and understand business needs.\nDevelop and maintain data visualization solutions that adhere to best practices.\nProfessional & Technical Skills:\nStrong understanding of statistical analysis and machine learning algorithms.\nExperience with data visualization tools such as Tableau or Power BI.\nHands-on experience implementing machine learning algorithms like linear regression, decision trees, and clustering.\nSolid grasp of data munging techniques to ensure data quality and integrity.\nAdditional Information:\nThe candidate should have a minimum of 0-2 years of experience in Java Full Stack Development.\nThis position is based at our Ahmedabad office.\nA 15 years full-time education is required.","Consulting, Information Services","['(Gen)AI', 'VIS/BI', 'Data Visualization']",2025-06-12 15:25:52
Data Scientist,IBM,2-6 Years,,Bengaluru,"IBM Systems helps IT leaders think differently about their infrastructure. IBM servers and storage are no longer inanimate - they can understand, reason, and learn so our clients can innovate while avoiding IT issues. Our systems power the world's most important industries and our clients are the architects of the future. Join us to help build our leading-edge technology portfoliodesigned for cognitive business and optimized for cloud computing.\nRobust background in traditional AI methodologies, encompassing both machine learning and deep learning frameworks.\nProficiency in Python, C++, and relevant ML libraries (e.g., TensorFlow, PyTorch) to develop production-grade quality products is essential.\nSkilled in integrating, cleansing, and shaping data, with expertise in various databases including open-source databases like MongoDB, CouchDB, CockroachDB.\nProficient in developing optimal data pipeline architectures for AI applications, ensuring adherence to client's SLAs.\nFamiliarity with Linux platform and experience in Linux app development is desirable.\nExperienced in DevOps, skilled in Git, CI/CD pipelines (Jenkins, Travis CI, GitLab CI), and containerization (Docker, Kubernetes).\nExperience in Generative AI would be a huge plus.\nAI compiler/runtime skills would be a huge plus.\nOpen-source Contribution is a huge plus. Experience in contributing to open-source AI projects or utilizing open-source AI frameworks is beneficial.\nStrong problem-solving and analytical skills, with experience in optimizing AI algorithms for performance and scalability.\nYour role and responsibilities\nUtilize expertise in AI/ML and Data Science to develop and deploy AI models in production environments, ensuring scalability, reliability, and efficiency.\nImplement and optimize machine learning algorithms, neural networks, and statistical modelling techniques to solve complex problems.\nHands-on experience in developing and deploying large language models (LLMs) in production environments, with a good understanding of distributed systems, microservice architecture, and REST APIs.\nCollaborate with cross-functional teams to integrate MLOps pipelines with CI/CD tools for continuous integration and deployment.\nStay updated with the latest advancements in AI/ML technologies and contribute to the development and improvement of AI frameworks and libraries.\nEnsure compliance with industry best practices and standards in AI engineering, maintaining high standards of code quality, performance, and security.\nRequired education\nBachelor's Degree\nPreferred education\nBachelor's Degree\nRequired technical and professional expertise\nProficiency in Python, C++.\nExperience with relevant ML libraries (e.g., TensorFlow, PyTorch) for developing production-grade quality products.\nExpertise in various databases including open-source databases like MongoDB, CouchDB, CockroachDB.\nSkills in Git, CI/CD pipelines (Jenkins, Travis CI, GitLab CI), and containerization (Docker, Kubernetes).\nExperience in contributing to open-source AI projects or utilizing open-source AI frameworks.\nExperience in optimizing AI algorithms for performance and scalability.\nPreferred technical and professional experience\nProven ability to implement and optimize machine learning algorithms, neural networks, and statistical modelling techniques to solve complex problems effectively.\nProficiency in distributed systems, microservice architecture, and REST APIs.\nExperience in collaborating with cross-functional teams to integrate MLOps pipelines with CI/CD tools for continuous integration and deployment, ensuring seamless integration of AI/ML models into production workflows.\nExperience in using container orchestration platforms such as Kubernetes to deploy and manage machine learning models in production environments, ensuring efficient scalability and management of AI infrastructure.\nProven ability to contribute to the development and improvement of AI frameworks and libraries.\nStrong communication skills with the ability to communicate technical concepts effectively to non-technical stakeholders.",Information Technology,"['Python', 'C++', 'Tensorflow', 'Pytorch', 'Kubernetes', 'Git']",2025-06-12 15:25:53
Power Bi Developer,Virtusa,8-12 Years,,Bengaluru,"Key Responsibilities:\nDesign and develop scalable dashboards and reports using Power BI.\nLeverage QlikView/Qlik Sense experience to support legacy systems and contribute to migration strategies (if applicable).\nCollaborate with stakeholders to gather requirements, translate business needs into technical specifications.\nDevelop DAX queries and optimize data models for performance and usability.\nCreate data visualizations and interactive dashboards that communicate trends, patterns, and insights.\nIntegrate data from various sources such as SQL Server, Excel, SharePoint, and cloud platforms.\nImplement row-level security and data governance standards in Power BI.\nMaintain documentation for BI solutions, including data flows, report logic, and troubleshooting steps.\nSupport the BI team in migrating reports from Qlik to Power BI where necessary.\nRequired Skills and Experience:\n4+ years of experience in Power BI report/dashboard development.\n1-2 years of experience working with QlikView/Qlik Sense.\nStrong proficiency in DAX, Power Query (M), and data modeling.\nSolid understanding of ETL processes, data warehousing, and relational databases (SQL).\nAbility to work with large datasets and perform complex data analysis.\nFamiliarity with Power BI Service (publishing, workspaces, security roles).\nGood communication and stakeholder management skills.\nExperience in migrating dashboards from Qlik to Power BI.",Information Technology,"['Power Bi', 'Dashboard Design', 'Qlik', 'Dax', 'Sql', 'Etl', 'Data Visualization', 'Data Modeling']",2025-06-12 15:25:55
Data Scientist-Artificial Intelligence,IBM,5-7 Years,,Hyderabad,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour Role and Responsibilities\nWork with broader team to build, analyze, and improve the AI solutions.\nYou will also work with our software developers in consuming different enterprise applications.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nResource should have 5-7 years of experience.\nSound knowledge of Python and should know how to use the ML related services.\nProficient in Python with focus on Data Analytics Packages.\nStrategy: Analyze large, complex data sets and provide actionable insights to inform business decisions.\nStrategy: Design and implement data models that help in identifying patterns and trends.\nCollaboration: Work with data engineers to optimize and maintain data pipelines.\nPerform quantitative analyses that translate data into actionable insights and provide analytical, data-driven decision-making.\nIdentify and recommend process improvements to enhance the efficiency of the data platform.\nDevelop and maintain data models, algorithms, and statistical models.\nPreferred Technical and Professional Experience\nExperience with conversation analytics.\nExperience with cloud technologies.\nExperience with data exploration tools.",Software,"['Conversation Analytics', 'Python', 'Data Analytics', 'Machine Learning', 'Data Modeling', 'Cloud Technologies', 'Data Science']",2025-06-12 15:25:56
Data Engineer-Data Platforms-AWS,IBM,0-5 Years,,Bengaluru,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers). These centers are where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour Role and Responsibilities\nAs a Data Engineer, you will develop, maintain, evaluate, and test big data solutions.\nYou will be involved in the development of data solutions using Spark Framework with Python or Scala on Hadoop and AWS Cloud Data Platform.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nAWS Data Vault 2.0 development mechanism for agile data ingestion, storage, and scaling.\nDatabricks for complex queries on transformation, aggregation, business logic implementation.\nAWS Redshift and Redshift Spectrum, for complex queries on transformation, aggregation, business logic implementation.\nDWH Concept on star schema, Materialized view concept.\nStrong SQL and data manipulation/transformation skills.\nPreferred Technical and Professional Experience\nRobust and Scalable Cloud Infrastructure.\nEnd-to-End Data Engineering Pipeline.\nVersatile Programming Capabilities.",Software,"['AWS Data Vault', 'Sql', 'Databricks', 'Aws Redshift', 'Star Schema', 'Data Pipeline']",2025-06-12 15:25:57
Deputy Manager Data Engineer - Analytics,IBM,6-9 Years,,Bengaluru,"Your role and responsibilities\nDevelop, test and support future-ready data solutions for customers across industry verticals.\nDevelop, test, and support end-to-end batch and near real-time data flows/pipelines.\nDemonstrate understanding of data architectures, modern data platforms, big data, analytics, cloud platforms, data governance and information management and associated technologies.\nCommunicates risks and ensures understanding of these risks.\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nGraduate with a minimum of 6+ years of related experience required.\nExperience in modelling and business system designs.\nGood hands-on experience on DataStage, Cloud-based ETL Services.\nHave great expertise in writing TSQL code.\nWell-versed with data warehouse schemas and OLAP techniques.\nPreferred technical and professional experience\nAbility to manage and make decisions about competing priorities and resources.\nAbility to delegate where appropriate.\nMust be a strong team player/leader.\nAbility to lead Data transformation projects with multiple junior data engineers.\nStrong oral written and interpersonal skills for interacting throughout all levels of the organization.\nAbility to communicate complex business problems and technical solutions.",Information Technology,"['Cloud-based ETL Services', 'Business Systems Design', 'DataStage', 'Tsql', 'Data Warehousing', 'OLAP', 'Data Modeling', 'Data Architecture']",2025-06-12 15:25:58
Data Scientist-Artificial Intelligence,IBM,2-6 Years,,Bengaluru,"Your role and responsibilities\nAn AI Data Scientist at IBM is not just a job title - it's a mindset. You'll leverage the watsonx,AWS Sagemaker,Azure Open AI platform to co-create AI value with clients, focusing on technology patterns to enhance repeatability and delight clients.\nWe are seeking an experienced and innovative AI Data Scientist to be specialized in foundation models and large language models. In this role, you will be responsible for architecting and delivering AI solutions using cutting-edge technologies, with a strong focus on foundation models and large language models. You will work closely with customers, product managers, and development teams to understand business requirements and design custom AI solutions that address complex challenges. Experience with tools like Github Copilot, Amazon Code Whisperer etc. is desirable.\nSuccess is our passion, and your accomplishments will reflect this, driving your career forward, propelling your team to success, and helping our clients to thrive.\nDay-to-Day Duties:\nProof of Concept (POC) Development: Develop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions. Collaborate with development teams to implement and iterate on POCs, ensuring alignment with customer requirements and expectations.\nHelp in showcasing the ability of Gen AI code assistant to refactor/rewrite and document code from one language to another, particularly COBOL to JAVA through rapid prototypes/ PoC\nDocumentation and Knowledge Sharing: Document solution architectures, design decisions, implementation details, and lessons learned. Create technical documentation, white papers, and best practice guides. Contribute to internal knowledge sharing initiatives and mentor new team members.\nIndustry Trends and Innovation: Stay up to date with the latest trends and advancements in AI, foundation models, and large language models. Evaluate emerging technologies, tools, and frameworks to assess their potential impact on solution design and implementation\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nStrong programming skills, with proficiency in Python and experience with AI frameworks such as TensorFlow, PyTorch, Keras or Hugging Face. Understanding in the usage of libraries such as SciKit Learn, Pandas, Matplotlib, etc. Familiarity with cloud platforms (e.g. Kubernetes, AWS, Azure, GCP) and related services is a plus.\nExperience and working knowledge in COBOL & JAVA would be preferred\nHaving experience in Code generation, code matching & code translation leveraging LLM capabilities would be a Big plus (e.g. Amazon Code Whisperer, Github Copilot etc.)Soft Skills: Excellent interpersonal and communication skills. Engage with stakeholders for analysis and implementation. Commitment to continuous learning and staying updated with advancements in the field of AI.\nGrowth mindset: Demonstrate a growth mindset to understand clients business processes and challenges.\nExperience in python and pyspark will be added advantage\nPreferred technical and professional experience\nExperience: Proven experience in designing and delivering AI solutions, with a focus on foundation models, large language models, exposure to open source, or similar technologies. Experience in natural language processing (NLP) and text analytics is highly desirable. Understanding of machine learning and deep learning algorithms.\nStrong track record in scientific publications or open-source communities\nExperience in full AI project lifecycle, from research and prototyping to deployment in production environments",Information Technology,"['Hugging Face', 'Foundation Models', 'Large Language Models', 'Python', 'Tensorflow', 'Pytorch']",2025-06-12 15:25:59
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns.\nYou'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Software,"['predictive models', 'data engineering', 'Machine Learning', 'Elasticsearch', 'Splunk', 'Database Migration']",2025-06-12 15:26:01
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns.\nYou'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Software,"['Elasticsearch', 'Splunk', 'Hadoop', 'Gcp', 'Data Replication', 'Apis']",2025-06-12 15:26:02
Data Engineer,Virtusa,6-8 Years,,Bengaluru,"Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nWorking with modern data orchestration tools\nQualification\nSeeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools.\nData Engineer to work on cloud-based data pipelines and analytics platforms\nPySpark and AWS, with proficiency in designing Data Lakes\nworking with modern data orchestration tools",Information Technology,"['CI & CD', 'Cloudera Data platform', 'Terraform', 'Jenkins', 'Devops', 'Cloudformation', 'Docker', 'Pyspark']",2025-06-12 15:26:03
Data Engineer-Business Intelligence,IBM,0-5 Years,,Pune,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nProvide expertise in analysis, requirements gathering, design, coordination, customization, testing and support of reports, in client's environment\nDevelop and maintain a strong working relationship with business and technical members of the team\nRelentless focus on quality and continuous improvement\nPerform root cause analysis of reports issues\nDevelopment / evolutionary maintenance of the environment, performance, capability and availability.\nAssisting in defining technical requirements and developing solutions\nEffective content and source-code management, troubleshooting and debugging\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nTableau Desktop Specialist, SQL -Strong understanding of SQL for Querying database Good to have - Python ; Snowflake, Statistics, ETL experience.\nExtensive knowledge on using creating impactful visualization using Tableau.\nMust have thorough understanding of SQL & advance SQL (Joining & Relationships).\nMust have experience in working with different databases and how to blend & create relationships in Tableau.\nMust have extensive knowledge to creating Custom SQL to pull desired data from databases.Troubleshooting capabilities to debug Data controls\nPreferred technical and professional experience\nTroubleshooting capabilities to debug Data controls Capable of converting business requirements into workable model.\nGood communication skills, willingness to learn new technologies, Team Player, Self-Motivated, Positive Attitude.\nMust have thorough understanding of SQL & advance SQL",Information Technology,"['snowflake', 'Tableau', 'Data Visualization', 'Sql', 'Python', 'Etl']",2025-06-12 15:26:05
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,"Your role and responsibilities\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Software,"['cdc', 'data engineering', 'Hadoop', 'Gcp', 'Api Development', 'Predictive Modeling']",2025-06-12 15:26:14
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,"n this role, you'll work at one of our IBM Consulting Client Innovation Centers (Delivery Centers). These centers are where we deliver deep technical and industry expertise to a wide range of public and private sector clients globally. Our delivery centers offer clients locally based skills and technical expertise to drive innovation and adopt new technologies.\nYour Role and Responsibilities\nAs an Associate Software Developer at IBM, you will:\nHarness the power of data to unveil captivating stories and intricate patterns.\nContribute to data gathering, storage, and both batch and real-time processing.\nCollaborate closely with diverse teams, playing an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nYour responsibilities may include:\nImplementing and validating predictive models, as well as creating and maintaining statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques.\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants, and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or write programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nDevelop/Convert the database (Hadoop to GCP) of specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform.\nImplementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API.\nParticipate in the modernization roadmap journey.\nAnalyze discovery and analysis outcomes.\nLead discovery and analysis workshops/playbacks.\nIdentification of application dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare effort estimates, WBS, staffing plan, RACI, RAID, etc.\nLeads the team to adopt the right tools for various migration and modernization methods.\nPreferred Technical and Professional Experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions.\nAbility to communicate results to technical and non-technical audiences.",Software,"['cdc', 'data engineering', 'Hadoop', 'Gcp', 'Api Development', 'Predictive Modeling']",2025-06-12 15:26:19
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,"n this role, you'll work at one of our IBM Consulting Client Innovation Centers (Delivery Centers). These centers are where we deliver deep technical and industry expertise to a wide range of public and private sector clients globally. Our delivery centers offer clients locally based skills and technical expertise to drive innovation and adopt new technologies.\nYour Role and Responsibilities\nAs an Associate Software Developer at IBM, you will:\nHarness the power of data to unveil captivating stories and intricate patterns.\nContribute to data gathering, storage, and both batch and real-time processing.\nCollaborate closely with diverse teams, playing an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nYour responsibilities may include:\nImplementing and validating predictive models, as well as creating and maintaining statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques.\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants, and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or write programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nDevelop/Convert the database (Hadoop to GCP) of specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform.\nImplementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API.\nParticipate in the modernization roadmap journey.\nAnalyze discovery and analysis outcomes.\nLead discovery and analysis workshops/playbacks.\nIdentification of application dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare effort estimates, WBS, staffing plan, RACI, RAID, etc.\nLeads the team to adopt the right tools for various migration and modernization methods.\nPreferred Technical and Professional Experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions.\nAbility to communicate results to technical and non-technical audiences.",Software,"['cdc', 'data engineering', 'Hadoop', 'Gcp', 'Api Development', 'Predictive Modeling']",2025-06-12 15:26:20
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns.\nYou'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise speach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Software,"['Elasticsearch', 'Splunk', 'Hadoop', 'Gcp', 'Data Replication', 'Apis']",2025-06-12 15:26:21
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns.\nYou'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Software,"['data engineering', 'Hadoop', 'Gcp', 'Data Replication', 'Api Development', 'Agile']",2025-06-12 15:26:22
Data Engineer-Business Intelligence,IBM,0-5 Years,,Pune,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nThe IBM Client Innovation Center (CIC) is an innovative and exciting part of IBM, working as IBM Global Business Services technical delivery partner. We continually innovate for our customers, employees, and shareholders. Our technical skills range from Application Development, Testing Services, Technical Support through to Cloud and Artificial Intelligence. Your work will power IBM and its clients globally, collaborating and integrating code and processes into enterprise systems. You will have access to the latest education, tools and technology, and a limitless career path with the world's technology leader. Come to IBM and make a global impact!\nIn this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nProvide expertise in analysis, requirements gathering, design, coordination, customization, testing and support of reports, in client's environment\nDevelop and maintain a strong working relationship with business and technical members of the team\nRelentless focus on quality and continuous improvement\nPerform root cause analysis of reports issues\nDevelopment / evolutionary maintenance of the environment, performance, capability and availability\nAssisting in defining technical requirements and developing solutions\nEffective content and source-code management, troubleshooting and debugging\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nTableau Desktop Specialist, SQL - Strong understanding of SQL for Querying database\nGood to have - Python; Snowflake, Statistics, ETL experience\nExtensive knowledge on using and creating impactful visualization using Tableau\nMust have thorough understanding of SQL & advanced SQL (Joining & Relationships)\nMust have experience in working with different databases and how to blend & create relationships in Tableau\nMust have extensive knowledge in creating Custom SQL to pull desired data from databases\nTroubleshooting capabilities to debug data controls\nPreferred technical and professional experience\nTroubleshooting capabilities to debug data controls\nCapable of converting business requirements into workable model\nGood communication skills, willingness to learn new technologies, Team Player, Self-Motivated, Positive Attitude\nMust have thorough understanding of SQL & advanced SQL (Joining & Relationships)",Information Technology,"['SQL (Advanced)', 'Custom SQL', 'ETL Concepts', 'Data Troubleshooting', 'Tableau', 'Data Visualization']",2025-06-12 15:26:24
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,"n this role, you'll work at one of our IBM Consulting Client Innovation Centers (Delivery Centers). These centers are where we deliver deep technical and industry expertise to a wide range of public and private sector clients globally. Our delivery centers offer clients locally based skills and technical expertise to drive innovation and adopt new technologies.\nYour Role and Responsibilities\nAs an Associate Software Developer at IBM, you will:\nHarness the power of data to unveil captivating stories and intricate patterns.\nContribute to data gathering, storage, and both batch and real-time processing.\nCollaborate closely with diverse teams, playing an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nYour responsibilities may include:\nImplementing and validating predictive models, as well as creating and maintaining statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques.\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants, and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or write programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nDevelop/Convert the database (Hadoop to GCP) of specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform.\nImplementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API.\nParticipate in the modernization roadmap journey.\nAnalyze discovery and analysis outcomes.\nLead discovery and analysis workshops/playbacks.\nIdentification of application dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare effort estimates, WBS, staffing plan, RACI, RAID, etc.\nLeads the team to adopt the right tools for various migration and modernization methods.\nPreferred Technical and Professional Experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions.\nAbility to communicate results to technical and non-technical audiences.",Software,"['cdc', 'data engineering', 'Hadoop', 'Gcp', 'Api Development', 'Predictive Modeling']",2025-06-12 15:26:25
Data Scientist-MLOps/LLMOps,IBM,0-5 Years,,Mumbai,"Curiosity and a constant quest for knowledge serve as the foundation to success in IBM Consulting. In your role, you'll be encouraged to challenge the norm, investigate ideas outside of your role, and come up with creative solutions resulting in ground breaking impact for a wide network of clients. Our culture of evolution and empathy centers on long-term career growth and development opportunities in an environment that embraces your unique skills and experience.\nYour role and responsibilities\nRole Overview:\nHiring an ML Engineer with experience in Cloudera ML to support end-to-end model development, deployment, and monitoring on the CDP platform.\nKey Responsibilities:\nDevelop and deploy models using CML workspaces.\nBuild CI/CD pipelines for ML lifecycle.\nIntegrate with governance and monitoring tools.\nEnable secure model serving via REST APIs.\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nSkills Required:\nExperience in Cloudera ML, Spark MLlib, or scikit-learn.\nML pipeline automation (MLflow, Airflow, or equivalent).\nModel governance, lineage, and versioning.\nAPI exposure for real-time inference.",Software,"['Cloudera ML', 'Spark MLlib', 'scikit-learn', 'MLflow', 'Airflow', 'Api Integration', 'Data Science']",2025-06-12 15:26:26
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns.\nYou'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Information Technology,"['Data Gathering', 'Data Storage', 'Data Processing', 'Predictive models', 'Database conversion', 'Data Replication']",2025-06-12 15:26:27
Data Engineer-Business Intelligence,IBM,0-5 Years,,Pune,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nProvide expertise in analysis, requirements gathering, design, coordination, customization, testing and support of reports, in client's environment\nDevelop and maintain a strong working relationship with business and technical members of the team\nRelentless focus on quality and continuous improvement\nPerform root cause analysis of reports issues\nDevelopment / evolutionary maintenance of the environment, performance, capability and availability.\nAssisting in defining technical requirements and developing solutions\nEffective content and source-code management, troubleshooting and debugging\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nTableau Desktop Specialist, SQL -Strong understanding of SQL for Querying database Good to have - Python ; Snowflake, Statistics, ETL experience.\nExtensive knowledge on using creating impactful visualization using Tableau.\nMust have thorough understanding of SQL & advance SQL (Joining & Relationships).\nMust have experience in working with different databases and how to blend & create relationships in Tableau.\nMust have extensive knowledge to creating Custom SQL to pull desired data from databases.Troubleshooting capabilities to debug Data controls\nPreferred technical and professional experience\nTroubleshooting capabilities to debug Data controls Capable of converting business requirements into workable model.\nGood communication skills, willingness to learn new technologies, Team Player, Self-Motivated, Positive Attitude.\nMust have thorough understanding of SQL & advance SQL",Information Technology,"['snowflake', 'Tableau', 'Data Visualization', 'Sql', 'Python', 'Etl']",2025-06-12 15:26:28
Data Engineer-Data Integration,IBM,0-5 Years,,Pune,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviour's.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nExpertise in designing and implementing scalable data warehouse solutions on Snowflake, including schema design, performance tuning, and query optimization.\nStrong experience in building data ingestion and transformation pipelines using Talend to process structured and unstructured data from various sources.\nProficiency in integrating data from cloud platforms into Snowflake using Talend and native Snowflake capabilities.\nHands-on experience with dimensional and relational data modelling techniques to support analytics and reporting requirements\nPreferred technical and professional experience\nUnderstanding of optimizing Snowflake workloads, including clustering keys, caching strategies, and query profiling.\nAbility to implement robust data validation, cleansing, and governance frameworks within ETL processes.\nProficiency in SQL and/or Shell scripting for custom transformations and automation tasks",Software,"['snowflake', 'Data Pipelines', 'Talend', 'Data Modeling', 'Sql', 'Etl']",2025-06-12 15:26:30
Data Engineer-Data Integration,IBM,0-5 Years,,Pune,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns.\nYou'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results.\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nVery good experience on Continuous Flow Graph tool used for point based development\nDesign, develop, and maintain ETL processes using Ab Initio tools.\nWrite, test, and deploy Ab Initio graphs, scripts, and other necessary components.\nTroubleshoot and resolve data processing issues and improve performance\nData Integration:\nExtract, transform, and load data from various sources into data warehouses, operational data stores, or other target systems.\nWork with different data formats, including structured, semi-structured, andunstructured data.\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Software,"['ETL Processes', 'Predictive Models', 'Elasticsearch', 'Splunk', 'Ab Initio', 'Data Integration']",2025-06-12 15:26:31
Data Engineer-Data Integration,IBM,0-5 Years,,Pune,"As a Data Engineer at IBM, you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing. Collaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nYour Role and Responsibilities\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintaining statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques.\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants, and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nExpertise in designing and implementing scalable data warehouse solutions on Snowflake, including schema design, performance tuning, and query optimization.\nStrong experience in building data ingestion and transformation pipelines using Talend to process structured and unstructured data from various sources.\nProficiency in integrating data from cloud platforms into Snowflake using Talend and native Snowflake capabilities.\nHands-on experience with dimensional and relational data modeling techniques to support analytics and reporting requirements.\nPreferred Technical and Professional Experience\nUnderstanding of optimizing Snowflake workloads, including clustering keys, caching strategies, and query profiling.\nAbility to implement robust data validation, cleansing, and governance frameworks within ETL processes.\nProficiency in SQL and/or Shell scripting for custom transformations and automation tasks.",Software,"['snowflake', 'ETL Pipelines', 'Talend', 'Sql', 'Data Modeling', 'data engineering']",2025-06-12 15:26:32
Data Engineer-Data Platforms,IBM,0-5 Years,,Navi Mumbai,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution.\nYour primary responsibilities include:\nLead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements.\nStrive for continuous improvements by testing the build solution and working under an agile framework.\nDiscover and implement the latest technologies trends to maximize and build creative solutions\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nExperience with Apache Spark (PySpark): In-depth knowledge of Spark's architecture, core APIs, and PySpark for distributed data processing.\nBig Data Technologies: Familiarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering Skills: Strong understanding of ETL pipelines, data modeling, and data warehousing concepts.\nStrong proficiency in Python: Expertise in Python programming with a focus on data processing and manipulation. Data Processing Frameworks: Knowledge of data processing libraries such as Pandas, NumPy.\nSQL Proficiency: Experience writing optimized SQL queries for large-scale data analysis and transformation.\nCloud Platforms: Experience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems\nPreferred technical and professional experience\nDefine, drive, and implement an architecture strategy and standards for end-to-end monitoring.\nPartner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering,\nGood to have detection and prevention tools for Company products and Platform and customer-facing",Software,"['Cloud Platforms', 'Sql', 'Python', 'Apache Spark', 'Pyspark', 'Hadoop']",2025-06-12 15:26:34
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Bengaluru,"Your role and responsibilities\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Software,"['cdc', 'data engineering', 'Hadoop', 'Gcp', 'Api Development', 'Predictive Modeling']",2025-06-12 15:26:35
Data Engineer-Data Platforms-Azure,IBM,0-5 Years,,Pune,"As a Data Engineer specializing in DBT, you'll be joining one of IBM Consulting's Client Innovation Centers here in Hyderabad. In this role, you'll contribute your deep technical and industry expertise to a variety of public and private sector clients, driving innovation and adopting new technologies.\nYour Responsibilities\nEstablish and implement best practices for DBT workflows, ensuring efficiency, reliability, and maintainability.\nCollaborate with data analysts, engineers, and business teams to align data transformations with business needs.\nMonitor and troubleshoot data pipelines to ensure accuracy and performance.\nWork with Azure-based cloud technologies to support data storage, transformation, and processing.\nRequired Qualifications\nEducation: Bachelor's Degree\nTechnical & Professional Expertise:\nStrong MS SQL and Azure Databricks experience.\nAbility to implement and manage data models in DBT, focusing on data transformation and alignment with business requirements.\nExperience ingesting raw, unstructured data into structured datasets within a cloud object store.\nProficiency in utilizing DBT to convert raw, unstructured data into structured datasets, enabling efficient analysis and reporting.\nSkilled in writing and optimizing SQL queries within DBT to enhance data transformation processes and improve overall performance.\nPreferred Qualifications\nEducation: Master's Degree\nTechnical & Professional Expertise:\nAbility to establish best DBT processes to improve performance, scalability, and reliability.\nExperience in designing, developing, and maintaining scalable data models and transformations using DBT in conjunction with Databricks.\nProven interpersonal skills, contributing to team efforts and achieving results as required.",Software,"['dbt', 'SQL Optimization', 'Azure Databricks', 'MS SQL', 'Data Modeling', 'Cloud Storage']",2025-06-12 15:26:44
Data Engineer-Data Platforms,IBM,0-5 Years,,Navi Mumbai,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nYour role and responsibilities\nAs a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution.\nYour primary responsibilities include:\nLead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements.\nStrive for continuous improvements by testing the build solution and working under an agile framework.\nDiscover and implement the latest technologies trends to maximize and build creative solutions\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nExperience with Apache Spark (PySpark): In-depth knowledge of Spark's architecture, core APIs, and PySpark for distributed data processing.\nBig Data Technologies: Familiarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering Skills: Strong understanding of ETL pipelines, data modeling, and data warehousing concepts.\nStrong proficiency in Python: Expertise in Python programming with a focus on data processing and manipulation. Data Processing Frameworks: Knowledge of data processing libraries such as Pandas, NumPy.\nSQL Proficiency: Experience writing optimized SQL queries for large-scale data analysis and transformation.\nCloud Platforms: Experience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems\nPreferred technical and professional experience\nDefine, drive, and implement an architecture strategy and standards for end-to-end monitoring.\nPartner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering,\nGood to have detection and prevention tools for Company products and Platform and customer-facing",Software,"['Cloud Platforms', 'Sql', 'Python', 'Apache Spark', 'Pyspark', 'Hadoop']",2025-06-12 15:26:49
Data Engineer-Data Platforms,IBM,0-5 Years,,Navi Mumbai,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology\nYour role and responsibilities\nAs a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution.\nYour primary responsibilities include:\nLead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements.\nStrive for continuous improvements by testing the build solution and working under an agile framework.\nDiscover and implement the latest technologies trends to maximize and build creative solutions\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nExperience with Apache Spark (PySpark): In-depth knowledge of Spark's architecture, core APIs, and PySpark for distributed data processing.\nBig Data Technologies: Familiarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering Skills: Strong understanding of ETL pipelines, data modeling, and data warehousing concepts.\nStrong proficiency in Python: Expertise in Python programming with a focus on data processing and manipulation. Data Processing Frameworks: Knowledge of data processing libraries such as Pandas, NumPy.\nSQL Proficiency: Experience writing optimized SQL queries for large-scale data analysis and transformation.\nCloud Platforms: Experience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems\nPreferred technical and professional experience\nDefine, drive, and implement an architecture strategy and standards for end-to-end monitoring.\nPartner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering,\nGood to have detection and prevention tools for Company products and Platform and customer-facing",Software,"['Cloud Platforms', 'Sql', 'Python', 'Apache Spark', 'Pyspark', 'Hadoop']",2025-06-12 15:26:51
Data Scientist-MLOps/LLMOps,IBM,0-5 Years,,Mumbai,"Curiosity and a constant quest for knowledge serve as the foundation to success in IBM Consulting. In your role, you'll be encouraged to challenge the norm, investigate ideas outside of your role, and come up with creative solutions resulting in ground breaking impact for a wide network of clients. Our culture of evolution and empathy centers on long-term career growth and development opportunities in an environment that embraces your unique skills and experience.\nYour role and responsibilities\nRole Overview:\nHiring an ML Engineer with experience in Cloudera ML to support end-to-end model development, deployment, and monitoring on the CDP platform.\nKey Responsibilities:\nDevelop and deploy models using CML workspaces.\nBuild CI/CD pipelines for ML lifecycle.\nIntegrate with governance and monitoring tools.\nEnable secure model serving via REST APIs.\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nSkills Required:\nExperience in Cloudera ML, Spark MLlib, or scikit-learn.\nML pipeline automation (MLflow, Airflow, or equivalent).\nModel governance, lineage, and versioning.\nAPI exposure for real-time inference.",Software,"['Cloudera ML', 'Spark MLlib', 'scikit-learn', 'MLflow', 'Airflow', 'Api Integration', 'Data Science']",2025-06-12 15:26:52
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns.\nYou'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Software,"['Elasticsearch', 'Splunk', 'Hadoop', 'Gcp', 'Data Replication', 'Apis']",2025-06-12 15:26:53
Data Engineer-Business Intelligence,IBM,5-7 Years,,Hyderabad,"Required technical and professional expertise\n5+ years of experience with BI tools, with expertise and/or certification in at least one major BI platform Tableau preferred.\nAdvanced knowledge of SQL, including the ability to write complex stored procedures, views, and functions.\nProven capability in data storytelling and visualization, delivering actionable insights through compelling presentations.\nExcellent communication skills, with the ability to convey complex analytical findings to non-technical stakeholders in a clear, concise, and meaningful way.\n5.Identifying and analyzing industry trends, geographic variations, competitor strategies, and emerging customer behavior\nPreferred technical and professional experience\nTroubleshooting capabilities to debug Data controls Capable of converting business requirements into workable model.\nGood communication skills, willingness to learn new technologies, Team Player, Self-Motivated, Positive Attitude.\nMust have thorough understanding of SQL & advance SQL (Joining & Relationships)",Information Technology,"['snowflake', 'Tableau', 'Data Visualization', 'Sql', 'Python', 'Etl']",2025-06-12 15:26:54
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,"Your role and responsibilities\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Software,"['cdc', 'data engineering', 'Hadoop', 'Gcp', 'Api Development', 'Predictive Modeling']",2025-06-12 15:26:55
Data Scientist-Advanced Analytics,IBM,0-5 Years,,Bengaluru,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nProof of Concept (POC) Development: Develop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions.\nHelp in showcasing the ability of Gen AI code assistant to refactor/rewrite and document code from one language to another\nDocument solution architectures, design decisions, implementation details, and lessons learned.\nStay up to date with the latest trends and advancements in AI, foundation models, and large language models.\nEvaluate emerging technologies, tools, and frameworks to assess their potential impact on solution design and implementation\nPreferred technical and professional experience\nExperience and working knowledge in COBOL & JAVA would be preferred\nHaving experience in Code generation, code matching & code translation leveraging LLM capabilities would be a Big plus\nDemonstrate a growth mindset to understand clients business processes and challenges",Software,"['Data Gathering', 'Data Storage', 'Data Processing', 'Predictive models', 'Statistical models', 'Elasticsearch']",2025-06-12 15:26:57
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,"n this role, you'll work at one of our IBM Consulting Client Innovation Centers (Delivery Centers). These centers are where we deliver deep technical and industry expertise to a wide range of public and private sector clients globally. Our delivery centers offer clients locally based skills and technical expertise to drive innovation and adopt new technologies.\nYour Role and Responsibilities\nAs an Associate Software Developer at IBM, you will:\nHarness the power of data to unveil captivating stories and intricate patterns.\nContribute to data gathering, storage, and both batch and real-time processing.\nCollaborate closely with diverse teams, playing an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nYour responsibilities may include:\nImplementing and validating predictive models, as well as creating and maintaining statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques.\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements.\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants, and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or write programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results.\nRequired Education\nBachelor's Degree\nPreferred Education\nMaster's Degree\nRequired Technical and Professional Expertise\nDevelop/Convert the database (Hadoop to GCP) of specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform.\nImplementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API.\nParticipate in the modernization roadmap journey.\nAnalyze discovery and analysis outcomes.\nLead discovery and analysis workshops/playbacks.\nIdentification of application dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare effort estimates, WBS, staffing plan, RACI, RAID, etc.\nLeads the team to adopt the right tools for various migration and modernization methods.\nPreferred Technical and Professional Experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions.\nAbility to communicate results to technical and non-technical audiences.",Software,"['cdc', 'data engineering', 'Hadoop', 'Gcp', 'Api Development', 'Predictive Modeling']",2025-06-12 15:26:58
Data Engineer-Data Platforms-Google,IBM,0-5 Years,,Hyderabad,"In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.\nYour role and responsibilities\nAs an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns.\nYou'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis.\nAs a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\nRequired education\nBachelor's Degree\nPreferred education\nMaster's Degree\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Software,"['data engineering', 'Elasticsearch', 'Splunk', 'Predictive Modeling', 'Hadoop', 'Gcp']",2025-06-12 15:26:59
"Quant Data Specialist , Aladdin Financial Engineering - Associate",Primetrace Technologies,3-6 Years,,Mumbai,"Experience\nExperience on Scala\nKnowledge of ETL, data curation and analytical jobs using distributed computing framework with Spark\nKnowledge and Experience of working with large enterprisedatabaseslike Snowflake, CassandraCloud manged services likeDataproc, Databricks\nKnowledge of financial instruments like Corporate Bonds, Derivatives etc\nKnowledge of regression methodologies\nAptitude for design and building tools forDataGovernance\nPythonknowledge is a plus\nQualifications\nBachelors/masters in computer sciencewitha majorin Math,Econ,or related field\n3-6years of relevant experience",Information Technology,"['Scala experience', 'Spark distributed computing', 'Snowflake and Cassandra databases', 'financial instruments knowledge', 'data governance tools design', 'regression methodologies expertis']",2025-06-12 15:27:00
"Manager, Data Science 3",Xoom,5-10 Years,,Bengaluru,"Your day to day -\nHigh proficiency in technical / data-mining skills and working knowledge of dealing with large volumes of data using tools such as SQL, Python in GCP BigQuery/Hadoop environment.\nAbility to identify trends, patterns, and anomalies in large datasets and strong critical thinking to derive actionable insights from data.\nExpertise to perform storytelling with data and insights, drive key business decision-making\nExpertise in A/B testing, hypothesis testing, and statistical significance in supporting initiative and program focus.\nExperience with business segmentation, customer experience and churn analytics, business growth evaluation.\nDesign, build and maintain dashboards and data visualization through tools like Tableau, Power BI, and Looker.\nExpertise in predictive modelling, regression, and time-series analysis\nAbility to translated technical findings into business recommendations and working with business partners to implement.\nAttention to details and ensuring work outcome accuracy and reliability\nWhat do you need to bring-\nMaster s degree in related field and 10+ years related professional work experience (Data Science, Computer Science, Statistics, Business Analytics, Operations Research, etc.)\n5+ year of experience as people manager\nUnderstanding of Fraud/Credit risk in the financial industry, experience with Payment risk preferred\nGood time and program management, can handle multiple projects or action items at the same time with challenging deadlines.\nWell-developed research and analytical skills with good problem solving and lateral thinking ability\nWorking knowledge of SQL, Python, or other programming languages. Experience with GCP Big Query, Hadoop environment\nExcellent verbal and written communication skills with the ability to articulate concepts and ideas to diverse audiences.",FinTech,"['Sql', 'Python', 'BigQuery', 'Hadoop', 'Tableau', 'Predictive Modeling']",2025-06-12 15:27:02
Database Reliability Engineer,Xoom,5-8 Years,,Chennai,"In your day to day role you will\nYou would manage and maintain a comprehensive list of open incidents and follow up with relevant teams to facilitate driving the incident to closure. Create comprehensive corrective actions for recurring problems. You would be driving all current incidents on the bridge and continue driving on offline channels to conclusion. Create Dashboards/reports that clearly depicts the major metrics and displays the health of the Database infrastructure availability and performance.\nWhat do you need to bring-\nOverall 5-8 years proven track record in incident management and problem-solving.\n3-4 years experience in database technologies and distributed systems.\nA sound knowledge on ITSM/ITIL framework and proven expertise in Change/incident/Problem management techniques\nIt would be preferred if you have a good technical understanding of databases Since you would be closely working with the TPDS Database operations team.\nHaving knowledge in automation/dashboards would be an added advantage and would tremendously help with management reporting.\nStrong analytical skills for complex root cause analysis\nGood communication skills, particularly during high-pressure situations with ability to articulate complex technical issues clearly to various stakeholders\nGood experience with monitoring tools and observability platforms\nAbility to influence and drive change across multiple teams\nExperience with dashboard/reporting and simple automation",FinTech,"['Root Cause Analysis', 'Incident Management', 'Itil', 'Databases', 'Dashboards', 'Automation']",2025-06-12 15:27:04
Data Scientist,Xoom,20-30 Years,,Bengaluru,"Your day to day\nIn this role you will have full ownership of a portfolio of merchants and is responsible for end-to-end management of loss and decline rates.\nCollaborate with different teams to develop strategies for fraud prevention, loss savings, and optimize transaction declines or improve customer friction.\nYou will work together with cross-functional teams to deliver solutions and providing Risk analytics on frustration trend/ KPIs monitoring or alerting for fraud events.\nThese solutions will adapt PayPal s advanced proprietary fraud prevention tools enabling business growth.\nWhat do you need to bring\n3-6 years of relevant experience working with large-scale complex dataset.\nStrong analytical mindset, ability to decompose business requirements into an analytical plan, and execute the plan to answer those business questions\nExcellent communication skills, equally adept at working with engineers as well as business leaders\nWant to build new solutions and invent new approaches to big, ambiguous, critical problems Strong working knowledge ofExcel, SQL and Python/R\nTechnical Proficiency Exploratory Data Analysis and expertise in preparing a clean and structured data for model development.\nExperience in applying AI/ML techniques for business decisioning including supervised and unsupervised learning (e.g., regression, classification, clustering, decision trees, anomaly detection, etc.).\nKnowledge of model evaluation techniques such as Precision, Recall, ROC-AUC Curve, etc. along with basic statistical concepts",FinTech,"['Python/R', 'AI/ML Techniques', 'Model Evaluation', 'Excel', 'Sql', 'Exploratory Data Analysis']",2025-06-12 15:27:05
Growth Analyst - Share Market,PhonePe,3-8 Years,,Bengaluru,Role Responsibilities:\nMonitor business health metrics for the stockbroking category regularly.\nTrack industry trends and share insights that can drive category growth.\nCollaborate with the Analytics team to formulate growth interventions.\nImplement strategies to drive growth across various assets within the stockbroking business.\nKey Deliverables:\nRegular reports and insights on business health and industry trends.\nData-driven recommendations for growth interventions.\nSuccessfully implemented initiatives leading to business growth.\nEffective collaboration and communication with cross-functional teams.,Mobile Payments,"['Data Analysis', 'Growth Analysis', 'Market Research', 'Excel', 'Sql']",2025-06-12 15:27:07
"Manager, Data Science 3",Xoom,5-10 Years,,Bengaluru,"Your day to day -\nHigh proficiency in technical / data-mining skills and working knowledge of dealing with large volumes of data using tools such as SQL, Python in GCP BigQuery/Hadoop environment.\nAbility to identify trends, patterns, and anomalies in large datasets and strong critical thinking to derive actionable insights from data.\nExpertise to perform storytelling with data and insights, drive key business decision-making\nExpertise in A/B testing, hypothesis testing, and statistical significance in supporting initiative and program focus.\nExperience with business segmentation, customer experience and churn analytics, business growth evaluation.\nDesign, build and maintain dashboards and data visualization through tools like Tableau, Power BI, and Looker.\nExpertise in predictive modelling, regression, and time-series analysis\nAbility to translated technical findings into business recommendations and working with business partners to implement.\nAttention to details and ensuring work outcome accuracy and reliability\nWhat do you need to bring-\nMaster s degree in related field and 10+ years related professional work experience (Data Science, Computer Science, Statistics, Business Analytics, Operations Research, etc.)\n5+ year of experience as people manager\nUnderstanding of Fraud/Credit risk in the financial industry, experience with Payment risk preferred\nGood time and program management, can handle multiple projects or action items at the same time with challenging deadlines.\nWell-developed research and analytical skills with good problem solving and lateral thinking ability\nWorking knowledge of SQL, Python, or other programming languages. Experience with GCP Big Query, Hadoop environment\nExcellent verbal and written communication skills with the ability to articulate concepts and ideas to diverse audiences.",FinTech,"['Sql', 'Python', 'BigQuery', 'Hadoop', 'Tableau', 'Predictive Modeling']",2025-06-12 15:27:16
SAP Enterprise Analytics Engineer,Astrazeneca,3-7 Years,,Bengaluru,"The Axial programme needs the best talent to work in it. Whether it s the technical skills, business understanding or change leadership, we want to ensure we have the strongest team deployed throughout. We are aiming to deliver a world class change programme that leaves all employees with a fuller understanding of their role in the end-to-end nature of our global company. This programme will provide AZ with a competitive edge, to the benefit of our employees, customers and patients.\nJoin Axial as an Engineer and help build cutting-edge pipelines and visualizations on our new SAP analytics stack using Datasphere and SAP Analytics Cloud. Be part of a self-sufficient, agile team creating high-impact, reusable data products and dashboards that drive business decisions. With opportunities to work across data pipelines, modeling, testing, and more, this is your chance to shape the future of enterprise analytics while advancing your skills with world-class tools!\nWe are looking for engineers who want to build pipelines and visualisations on the new SAP analytics stack (Datasphere and SAP Analytics Cloud).\nWe are building out teams to face key finance and operations process areas. And, more importantly for this advert, we are building out an enterprise team to build and run the cross-cutting data products and dashboards. Data products like dimensions for business partner, material master, cost centre, profit centre, location and geography; or fact tables for complex multi-process reporting and exports to other data hubs. This team will be self-sufficient, so you will get experience of everything - data pipelines, data analysis, data modelling, testing, incident management. The team will work in a transparent and agile way, meaning there will be a published roadmap of analytics products and a constant flow of new work from across the Axial program and AZ. You will be part of a team that creates the most re-used analytics artefacts on the stack.\nThis means you need to have a passion for well-engineered analytics. You are likely to have experience of some (but not all) of the following: ETL tooling; data modelling; SAP analytics tooling; dashboard creation; analytics best practices; data warehousing; data modelling; analytics testing; SAP data.\nWe will provide training on SAP Datasphere and SAP Analytics cloud.\nDesirable experience:\nETL tooling / data pipeline tools,\nData modelling,\nSAP data and associated business processes,\nSAP Datasphere,\nSAP Analytics Cloud,\nDashboard creation,\nAnalytics best practices,\nData warehousing,\nAnalytics testing.",Pharmaceutical,"['Analytics testing', 'Etl', 'Data Modelling', 'Sap', 'SAP Analytics', 'Dashboards', 'Data Warehousing']",2025-06-12 15:27:21
SAP Enterprise Analytics Engineer,Astrazeneca,3-7 Years,,Bengaluru,"The Axial programme needs the best talent to work in it. Whether it s the technical skills, business understanding or change leadership, we want to ensure we have the strongest team deployed throughout. We are aiming to deliver a world class change programme that leaves all employees with a fuller understanding of their role in the end-to-end nature of our global company. This programme will provide AZ with a competitive edge, to the benefit of our employees, customers and patients.\nJoin Axial as an Engineer and help build cutting-edge pipelines and visualizations on our new SAP analytics stack using Datasphere and SAP Analytics Cloud. Be part of a self-sufficient, agile team creating high-impact, reusable data products and dashboards that drive business decisions. With opportunities to work across data pipelines, modeling, testing, and more, this is your chance to shape the future of enterprise analytics while advancing your skills with world-class tools!\nWe are looking for engineers who want to build pipelines and visualisations on the new SAP analytics stack (Datasphere and SAP Analytics Cloud).\nWe are building out teams to face key finance and operations process areas. And, more importantly for this advert, we are building out an enterprise team to build and run the cross-cutting data products and dashboards. Data products like dimensions for business partner, material master, cost centre, profit centre, location and geography; or fact tables for complex multi-process reporting and exports to other data hubs. This team will be self-sufficient, so you will get experience of everything - data pipelines, data analysis, data modelling, testing, incident management. The team will work in a transparent and agile way, meaning there will be a published roadmap of analytics products and a constant flow of new work from across the Axial program and AZ. You will be part of a team that creates the most re-used analytics artefacts on the stack.\nThis means you need to have a passion for well-engineered analytics. You are likely to have experience of some (but not all) of the following: ETL tooling; data modelling; SAP analytics tooling; dashboard creation; analytics best practices; data warehousing; data modelling; analytics testing; SAP data.\nWe will provide training on SAP Datasphere and SAP Analytics cloud.\nDesirable experience:\nETL tooling / data pipeline tools,\nData modelling,\nSAP data and associated business processes,\nSAP Datasphere,\nSAP Analytics Cloud,\nDashboard creation,\nAnalytics best practices,\nData warehousing,\nAnalytics testing.",Pharmaceutical,"['Analytics testing', 'Etl', 'Data Modelling', 'Sap', 'SAP Analytics', 'Dashboards', 'Data Warehousing']",2025-06-12 15:27:22
Associate Consultant -Data Analytics +Forecasting,Spectral Consultants,5-7 Years,,"Gurugram, Pune","Develop and apply advanced statistical models that help clients understand dynamic business issues.\nLeverage analytic techniques to use data to guide client andteam decision-making.\nDesign custom analyses in R, Tableau, SAS, Visual Basic and Excel to investigate and inform client needs.\nSynthesize and communicate results to clients andteams through oral and written presentations.\nDevelop client relationships and serve as key point of contact on aspects of projects.\nProvide client andteams project status updates.\nCreate project deliverables and implement solutions.\nAdvance problem-solving skills and improvecapabilities.\nGuide and mentor Associates on teams.\nWhat You'll Bring\nBachelor's or master's degree required in any discipline with strong record of academic success in quantitative and analytic coursework such as operations research, applied mathematics, management science, data science, statistics, econometrics or engineering.\n5 to 7 years of relevant post-collegiate job experience. A PhD may substitute in lieu of work experience.\nHigh motivation, good work ethic, maturity and personal initiative.\nStrong oral and written communication skills.\nEmpathy, adaptability and emotional intelligence.\nClose attention to detail, with a quality-focused mindset.\nSelf-discipline for planning and organizing tasks.\nAptitude for, and enjoyment of, working in teams.","Consulting, Business Intelligence","['Visualization', 'Client Relationship Management', 'Statistical Modeling', 'Data Analysis', 'Team Leadership', 'Problem-solving', 'Decision Making', 'Mentorship']",2025-06-12 15:27:23
Data Analytics Mgr,Amgen Technology Private Limited,4-6 Years,,Hyderabad,"In this vital role you will report to the Organizational Planning Analytics & Insights Procurement & Sourcing Lead, you will support Amgen's Tech & Workforce Strategy by applying business analytics and change leadership skills to drive insights that impact resource allocation and sourcing strategy.\nYour responsibilities include dashboard development, ad-hoc reporting, business partnering & engagement, and financial baselining.\nThis role supports organizational change and enables the development of an integrated approach to global sourcing and financial planning.\nReporting to the Organizational Planning Analytics & Insights Procurement & Sourcing Lead, you will support Amgen's Tech & Workforce Strategy by applying business analytics and change management skills to drive insights that impact resource allocation and sourcing strategy.\nYour responsibilities include dashboard development, ad-hoc reporting, business partnering & engagement, and financial baselining.\nThis role supports change management and enables the development of an integrated approach to global sourcing and financial planning.\nRoles & Responsibilities:\nAddressing business challenges through process evaluation and insight generation.\nDevelop insights with a strong focus on Tableau and Power BI dashboard creation, as well as PowerPoint presentations.\nGuide data analysts and data engineers on standard methodologies for building data pipelines to support dashboards and other business objectives.\nConduct ad hoc analyses of FP&A and sourcing/procurement data.\nAddressing business challenges through process evaluation and insight generation.\nDevelop insights with a strong focus on Tableau and Power BI dashboard creation, as well as PowerPoint presentations.\nGuide data analysts and data engineers on standard methodologies for building data pipelines to support dashboards and other business objectives.\nConduct ad hoc analyses of FP&A and sourcing/procurement data.\nWhat we expect of you:\nWe are all different, yet we all use our unique contributions to serve patients.\nBasic Qualifications:\nMaster's degree and 4 to 6 years of applicable experience in business analysis (finance analysis, data analysis, sourcing analysis, or similar experience) OR\nBachelor's degree and 6 to 8 years of applicable experience in business analysis (finance analysis, data analysis, sourcing analysis, or similar experience) OR\nDiploma and 10 to 12 years of applicable experience in business analysis (finance analysis, data analysis, sourcing analysis, or similar experience)\nPreferred Qualifications:\nMaster's degree in data science, business, statistics, data mining, applied mathematics, business analytics, engineering, computer science, or a related field\n4 years of relevant experience in data science, data analytics, consulting, and/or financial planning & analysis\nA keen eye for design, with the ability to craft engaging PowerPoint decks and develop compelling Power BI and Tableau dashboards\nProven expertise in statistical/mathematical modeling and working with structured/unstructured data\nExperience with procurement, sourcing, and/or financial planning data\nSkilled in automating data workflows using tools like Tableau, Python, R, Alteryx, and PowerApps\nKnowledge of global finance systems, Procurement, and sourcing operations\nExperience with data analysis, budgeting, forecasting, and strategic planning in the Bio-Pharmaceutical or biotech industry\nGrowing in a start-up environment, building a data-driven transformation capability\nUnderstanding of the Bio-Pharmaceutical and biotech industry trends and operations\nProven ability to engage with cross-functional business leaders to align data strategies with corporate objectives, redefining complex data insights into actionable strategies\nFlexible work models, including remote work arrangements, where possible\nAs we work to develop treatments that deal with others, we also work to care for your professional and personal growth and well-being. From our competitive benefits to our collaborative culture, we'll support your journey every step of the way.",Biotechnology,"['Data Analysis', 'Sourcing', 'Financial Planning', 'Tableau', 'Power Bi', 'python']",2025-06-12 15:27:24
Growth Analyst - Share Market,PhonePe,3-8 Years,,Pune,Role Responsibilities:\nMonitor business health metrics for the stockbroking category regularly.\nTrack industry trends and share insights that can drive category growth.\nCollaborate with the Analytics team to formulate growth interventions.\nImplement strategies to drive growth across various assets within the stockbroking business.\nKey Deliverables:\nRegular reports and insights on business health and industry trends.\nData-driven recommendations for growth interventions.\nSuccessfully implemented initiatives leading to business growth.\nEffective collaboration and communication with cross-functional teams.,Mobile Payments,"['Data Analysis', 'Growth Analysis', 'Market Research', 'Excel', 'Sql']",2025-06-12 15:27:25
Senior Staff AI Engineer – AI Optimization & MLOps,Synopsys (India) Private Limited,8-10 Years,,Hyderabad,"What You'll Be Doing:\nDesign, fine-tune, and optimize LLMs, retrieval-augmented generation (RAG), and reinforcement learning models for IT automation.\nImprove model accuracy, latency, and efficiency, ensuring optimal performance for IT service workflows.\nExperiment with cutting-edge AI techniques, including multi-agent architectures, prompt tuning, and continual learning.\nImplement MLOps best practices, ensuring scalable, automated, and reliable model deployment.\nDevelop AI monitoring, logging, and observability pipelines to track model performance in production.\nOptimize GPU/TPU utilization and cloud-based AI model serving for efficiency and cost-effectiveness.\nDevelop tools to measure model drift, inference latency, and operational efficiency.\nImplement automated retraining pipelines to ensure AI models remain effective over time.\nWork closely with cloud teams to optimize AI model execution across hybrid cloud environments.\nStay ahead of emerging AI technologies, evaluating new frameworks, techniques, and research for real-world application.\nCollaborate to refine AI system architectures and capabilities, while also ensuring models are effectively embedded into IT automation workflows\n\nThe Impact You Will Have:\nEnhance the efficiency and reliability of AI-powered IT automation solutions.\nDrive continuous improvement and innovation in AI model development and deployment.\nEnsure scalable and cost-effective AI model serving in cloud and hybrid environments.\nImprove real-time AI processing with minimal downtime and high performance.\nOptimize AI systems for performance, security, and cost in IT automation applications.\nContribute to the advancement of Synopsys AI capabilities and technologies.\n\nWhat You'll Need:\n8+ years of experience in AI/ML engineering, with a focus on model optimization and deployment.\nStrong expertise in AI frameworks (LangGraph, OpenAI, Hugging Face, TensorFlow/PyTorch).\nExperience implementing MLOps pipelines, CI/CD for AI models, and cloud-based AI deployment.\nDeep understanding of AI performance tuning, inference optimization, and cost-efficient deployment.\nStrong programming skills in Python, AI model APIs, and cloud-based AI services.\nFamiliarity with IT automation and self-healing systems is a plus.\n\nWho You Are:\nInnovative and forward-thinking, constantly seeking to improve and optimize AI models.\nCollaborative and communicative, working effectively with cross-functional teams.\nDetail-oriented and meticulous, ensuring high standards in AI model performance and deployment.\nAdaptable and resilient, thriving in dynamic and fast-paced environments.\nPassionate about AI and its applications in IT automation and beyond.",Information Technology,"['Model Optimization', 'Deployment', 'MLops', 'Deep Learning']",2025-06-12 15:27:27
Senior Staff AI Engineer – Agentic IT,Synopsys (India) Private Limited,6-10 Years,,Hyderabad,"What You'll Be Doing:\nAnalyze and decompose IT workflows, identifying automation opportunities using agentic AI.\nImplement autonomous AI-driven workflows for incident management, self-healing systems, and intelligent service operations.\nFine-tune AI models and develop prompt engineering strategies for optimized performance.\nDevelop and maintain robust, scalable AI-powered IT workflows.\nEnsure AI-driven automation is fault-tolerant, efficient, and aligned with enterprise IT best practices.\nWork closely with IT teams to integrate AI workflows into existing operational frameworks.\nPartner with MLOps engineers to deploy, monitor, and optimize AI-driven workflows in production.\nContinuously improve deployed models through feedback loops and real-world performance tuning.\nUse Python to develop automation scripts, AI workflow integrations, and API-driven IT solutions.\nWork with Kubernetes, Terraform, and CI/CD pipelines to ensure automated deployment and lifecycle management.\n\nThe Impact You Will Have:\nTransform IT workflows into intelligent, agentic AI-driven processes.\nEnhance IT service efficiency through AI-driven incident resolution, self-healing systems, and workflow automation.\nOptimize IT operations by predicting, preventing, and resolving IT issues proactively.\nEnsure scalable AI deployment using Python, Kubernetes, and CI/CD for production-ready AI workflows.\nReduce costs and improve reliability by minimizing downtime and optimizing resource usage with AI-powered automation.\nEnable self-healing IT systems and predictive issue resolution.\n\nWhat You'll Need:\nStrong expertise in Python for scripting, API development, and IT automation.\nExperience in agent-based AI frameworks and autonomous workflow orchestration.\nFamiliarity with ITSM platforms such as ServiceNow, BMC, or Jira.\nHands-on experience with Kubernetes, Docker, and cloud automation tools.\nExperience working closely with MLOps teams to deploy and scale AI-driven IT automation.\n\nWho You Are:\nA proactive problem solver with a keen eye for detail.\nAn effective communicator who can collaborate with cross-functional teams.\nAdaptable and able to thrive in a fast-paced, dynamic environment.\nInnovative and driven by a passion for continuous improvement.\nA strategic thinker with the ability to translate complex technical concepts into practical solutions.",Information Technology,"['Autonomous Agents', 'LLMs', 'Tool Integration', 'Workflow Automation']",2025-06-12 15:27:28
Data Scientist 3,Xoom,4-6 Years,,"Bengaluru, Chennai","Provide analytical insights into problems, emerging trends and changes to portfolio\nDrive projects end to end from ideation to implementation in a time sensitive and efficient manner\nIdentify new opportunities and continuous improvement to contribute to the team s KPIs and targets\nCase rating Evaluate each lead and case for risk levels to determine the urgency, importance and need for manual reviews with a goal of Increasing case automation using various risk factors and exposure on a given account, additional profile or transactional information, adoption of Document Intelligence solution etc.\nEnsure that we balance automation goals with loss.\nPartner with GADS and front end strategy team for agent decision and trend-based feedback model to improve automation quality.\nWork closely with business partners and stakeholders to determine how to design analysis and measurement approaches that will significantly improve our ability to understand and address emerging business issues\nEffectively managing relevant stakeholders through the lifecycle of a project by effectively understanding the various needs, priorities and perspectives in order to drive for a solution\nUnderstand new products, features and trends and their impact to back-office reviews.\nIdentifying present or future gaps in the team s existing reporting and tools suite\nProviding regular updates to leadership, product and other stakeholders that will simplify and clarify complex concepts and the results of analyses effectively with emphasis on the actionable outcomes and impact to business\nWhat do you need to bring\nSolid technical / data-mining skills and ability to work with large volumes of data; extract and manipulate large datasets using common tools such as SQL, SAS, Hadoop, or other programming/scripting languages (Python, R, etc.) to translate data into business decisions/results\nProven experience in using data and analytics to drive innovation in business processes to reduce manual efforts\nBe data-driven and outcome-focused\nMust have good business judgment with demonstrated ability to think creatively and strategically\nMust be an intuitive, organized analytical thinker, with the ability to perform detailed analysis\nTakes personal ownership; Self-starter; Ability to drive projects with minimal guidance and focus on high impact work\nProven experience in working with multiple teams and stakeholders to deliver business outcomes.\nProven experience in engaging with senior leadership teams to set up roadmap and to showcase project outputs\nDegree in Statistics, Mathematics, Computer Science, Economics, Engineering or related field with 10+ years experience. Work experience in risk management, data science in the payments space or banking/finance services industry would be particularly desirable.\nProficiency using SQL and querying relational databases\nExperience in at least one statistical programming language (SAS, R, Python)\nExperience in predictive modelling techniques such as Regression, Classification, Time series forecasting, NLP, etc.\nKnowledge of latest developments in AI and how they can be used for enterprise level solutions\nHands on experience with project/program management or working with PMO teams to deliver projects\nTeam management skills to mentor team members to get best results\nDrive a culture of innovation in the team and proactively come up with ideas",FinTech,"['R', 'Ai', 'Sql', 'SAS', 'Python', 'Predictive Modeling']",2025-06-12 15:27:29
Director-Commercial IT & Digital,Astrazeneca,9-12 Years,,Mumbai,"Key focus areas currently are:\nOmnichannel and digital marketing for patients and HCPs\nE-commerce and consumerization\nInternal innovation\nExternal innovation with partners and start-up ecosystems\nAutomation and simplification, leveraging artificial intelligence\nAnalytics, insights, and data governance\nSupport and implementation of business systems for office and field operations Role Overview: The Digital Solutions Delivery Director is responsible for empowering the field force, sale team and supporting internal (non-sales) functions in driving efficiency and productivity at scale. This role demands a collaborative and proactive approach to working with internal and external stakeholders to understand Indias business priorities and challenges and to deliver effective solutions.\nSuccess in this role requires the following skills:\nComprehensive Business Analysis skill\nProject and program management Page 2 of 3\nStrong written and verbal communication\nCollaboration across functions and geographies\nStakeholder management\nSolution design\nRisk management\nChange Management\nAssertive while maintaining calm and composed demeanour\nA continuous learning mindset to adapt to the evolving technology landscape\nCore Accountabilities:\nEnable and empower the field force, sales team and supporting functions digitally, driving significant efficiency and productivity.\nEngage with stakeholders to understand business priorities, proactively identifying digitalization opportunities.\nLead projects and programs with precision, managing risks and dependencies, ensuring timely delivery, budget adherence, and alignment with project/program objectives.\nEnsure that necessary internal/global governance and compliance standards are met and adhered to for every digital projects/programs/initiative\nWork closely with global cross-functional teams to understand global innovation, innovative pilots successfully deployed elsewhere in the globe, and bring these innovations to India\nCommunicate project progress, updates, and risks to stakeholders, including executives and team members\nFoster a collaborative team environment, promoting effective teamwork and knowledge sharing.\nUse the bi-annual Pulse survey to evaluate and improve business and IT satisfaction\nEngage with tech start-ups through AstraZeneca s ACatalyst Network for potential collaborations in India. Essential Skills/Experience\nRelevant technical degree or equivalent (BTech/MTech with MBA) with strong academic performance.\nUnderstanding of the pharmaceutical/healthcare domain.\nProven leadership in global organizations with complex, diverse environments. Expertise in relationship-building with senior internal customers and suppliers. Technical proficiency in: o Salesforce CRM (Health Cloud, Einstein)\no Market Tech\no Digital Health\no Power BI\no Application of AI and GenAI\no Understanding of Workday, Coupa, SAP Concur Page 3 of 3\nStrong communication, facilitation, and relationship-building skills.\nExperience influencing solution directions and adopting standards using architecture strategies.\nProven ability to inspire change and challenge conventional ideas.\nExperience managing relationships with business teams and third-party suppliers.\nStrategic, big-picture thinking with an understanding of long-term implications.\nCritical thinking skills, questioning assumptions, and evaluating evidence objectively.\nOpenness to change, adaptable to new insights and circumstances, supporting innovation.\nAwareness of marketplace developments and their potential impact on AZ.\nDesirable Skills/Experience\nCurrent leadership role in the Pharma/Healthcare industry",Pharmaceutical,"['Business Analysis', 'Healthcare', 'Pharma', 'Automation', 'Sap']",2025-06-12 15:27:30
Customer Excellence Data Analysts,Achnet,8-13 Years,,Bengaluru,"Data Analysis and Manipulation: Proficiency in tools like Microsoft Excel (including advanced functions like pivot tables, VLOOKUP, and macros)\nStrong skills in querying and managing data using SQL\nData Visualization: Creating dashboards and visualizations using tools like Tableau, Power BI, or Looker\nUsing Python (Matplotlib, Seaborn) or R (ggplot2) for customized visualizations\nProgramming Languages: Python (libraries like Pandas, NumPy) or R for statistical analysis and automation\nDatabase Management: Experience working with relational databases (MySQL, PostgreSQL, SQL Server) and NoSQL databases (MongoDB)\nStatistical Software: Familiarity with tools like SAS, SPSS, or Stata for advanced statistical modeling\nDescriptive Statistics: Understanding measures like mean, median, standard deviation, and variance\nInferential Statistics: Conducting hypothesis testing, confidence intervals, and regression analysis\nProbability Theory: Applying probability concepts in predictive modeling and risk assessment\nAttention to Detail: Ensuring accuracy in data analysis and reporting\nCollaboration: Working with cross-functional teams (eg, IT, marketing, finance) to gather data and share insights\nTime Management: Prioritizing tasks and meeting deadlines for data projects\nData Storytelling: Presenting data findings in a clear and compelling way for non-technical stakeholders\nReport Writing: Creating detailed and concise reports summarizing analysis and insights\nPresentation Skills: Effectively communicating results and recommendations through visuals and narratives\nSKILLS AND EXPERIENCE\nAbility to understand data structures, data tables and find connections between data structures to build analysis\nHighly skilled at consolidating and manipulating large amounts of data and providing visualization to understand trends and patterns\nIdentifies key linkages in data that bring insights into process breaking points or process opportunities\nExcellent communication skills, written, verbal and strong presentation skills\nAbility to motivate and lead teams by influence\nAble to translate customer feedback into meaningful solutions, is a self-motivated multitasker with high attention to detail\nQUALIFICATIONS\nMust Have Skills\nDATA VISUALIZATION\nPOWER BI\nPYTHON\nGGPLOT2\nMYSQL\nVLOOKUP\nPIVOT\nGood To Have Skills\nSAS\nSPSS\nMinimum Education Level",Career Planning,"['Data Analysis', 'San', 'Automation', 'SAS', 'Postgresql', 'MySQL', 'Data structures', 'Macros', 'Sql', 'Python']",2025-06-12 15:27:32
Customer Excellence Data Analysts,Achnet,8-13 Years,,Bengaluru,"Data Analysis and Manipulation: Proficiency in tools like Microsoft Excel (including advanced functions like pivot tables, VLOOKUP, and macros)\nStrong skills in querying and managing data using SQL\nData Visualization: Creating dashboards and visualizations using tools like Tableau, Power BI, or Looker\nUsing Python (Matplotlib, Seaborn) or R (ggplot2) for customized visualizations\nProgramming Languages: Python (libraries like Pandas, NumPy) or R for statistical analysis and automation\nDatabase Management: Experience working with relational databases (MySQL, PostgreSQL, SQL Server) and NoSQL databases (MongoDB)\nStatistical Software: Familiarity with tools like SAS, SPSS, or Stata for advanced statistical modeling\nDescriptive Statistics: Understanding measures like mean, median, standard deviation, and variance\nInferential Statistics: Conducting hypothesis testing, confidence intervals, and regression analysis\nProbability Theory: Applying probability concepts in predictive modeling and risk assessment\nAttention to Detail: Ensuring accuracy in data analysis and reporting\nCollaboration: Working with cross-functional teams (eg, IT, marketing, finance) to gather data and share insights\nTime Management: Prioritizing tasks and meeting deadlines for data projects\nData Storytelling: Presenting data findings in a clear and compelling way for non-technical stakeholders\nReport Writing: Creating detailed and concise reports summarizing analysis and insights\nPresentation Skills: Effectively communicating results and recommendations through visuals and narratives\nSKILLS AND EXPERIENCE\nAbility to understand data structures, data tables and find connections between data structures to build analysis\nHighly skilled at consolidating and manipulating large amounts of data and providing visualization to understand trends and patterns\nIdentifies key linkages in data that bring insights into process breaking points or process opportunities\nExcellent communication skills, written, verbal and strong presentation skills\nAbility to motivate and lead teams by influence\nAble to translate customer feedback into meaningful solutions, is a self-motivated multitasker with high attention to detail\nQUALIFICATIONS\nMust Have Skills\nDATA VISUALIZATION\nPOWER BI\nPYTHON\nGGPLOT2\nMYSQL\nVLOOKUP\nPIVOT\nGood To Have Skills\nSAS\nSPSS\nMinimum Education Level",Career Planning,"['Data Analysis', 'San', 'Automation', 'SAS', 'Postgresql', 'MySQL', 'Data structures', 'Macros', 'Sql', 'Python']",2025-06-12 15:27:33
"MX, Business Intelligence, Associate Manager",PhonePe,2-7 Years,,Bengaluru,"Key deliverables:\nAnalyze CX data to identify performance trends and root causes.\nTranslate business issues into analytical problem statements.\nDeliver insight-driven narratives to inform CX strategy decisions.\nCollaborate with teams to design dashboards and streamline data consumption.\nRole responsibilities:\nMonitor CX metrics and evaluate the impact of interventions.\nWork with cross-functional teams to solve inefficiencies.\nPresent analytical insights to stakeholders with data-backed stories.\nBuild and maintain queries, dashboards, and visualizations.",Mobile Payments,"['Data Analysis', 'Business Operations', 'Project Management', 'Sql', 'Excel']",2025-06-12 15:27:35
Data Engineer,Xoom,3-5 Years,,Chennai,"Design and develop scalable backend systems.\nOptimizesystem performance and reliability.\nMentor junior engineers.\nWhatdoyou need to bring\nBachelors degree in Computer Scienceor related field.\n3-5years of backend development experience.\nProficiencyin at least one backend language (Java, Python, Ruby on Rails)\nAdvancedproficiencyin backend development with either Java EE frameworks, including experience with Spring MVC,orHibernate.\nExperience designing and implementing RESTful services, focusing on scalability and reliability, using Java.\nProven ability to mentor junior engineers and contribute to code reviews and design discussions.\nExperience with cloud platforms (AWS, GCP, Azure)\nExperience with databases (SQL, NoSQL)\nStrong understanding of database design, including SQL and NoSQL databases, and experience with ORM tools.\nPreferred Qualifications\nExperience with large-scale, high-performance systems.\nKnowledge of the payment processing industry and relevant regulations.\nExperience with cloud platforms (AWS, GCP, Azure).\nContributions to open-source projects.",FinTech,"['SQL/NoSQL', 'AWS/GCP/Azure', 'Java', 'Spring MVC', 'Rest Apis', 'Backend Development']",2025-06-12 15:27:36
Assistant Manager - Data Stewardship/Manager - Data Stewardship,Astrazeneca,3-7 Years,,Chennai,"Essential Skills/Experience\nStrong analytical and problem-solving skills\nPeople management experience\nStrong knowledge on databases and database concepts, SQL, analytical tools\nFamiliarity with MS Office Tools - Word, Excel, PowerPoint\nDocument processes and adhere to standard operating procedures and processes\nBasic project management skills and be detail oriented\nCapable of managing multiple projects/requests simultaneously\nWork in teams, Collaborate and ensure Compliance\nBasic to limited exposure to issue/ticket management and/or tracking tools (ex: JIRA, ServiceNow)\nDesirable Skills/Experience\nExposure to MDM tools and systems - Reltio, Veeva\nGood experience in Process transitions, Building teams & Process Maturity model\nFamiliarity with tools like PowerBI, Snowflake, Python\nIndustry Data knowledge - OneKey, Veeva Open Data, Definitive Health Care data, Payer Data etc.",Pharmaceutical,"['Data Analysis', 'Pharma', 'Healthcare', 'Sql', 'Python', 'Power Bi']",2025-06-12 15:27:37
Data Analytics Mgr,Amgen Technology Private Limited,4-6 Years,,Hyderabad,"In this vital role you will report to the Organizational Planning Analytics & Insights Procurement & Sourcing Lead, you will support Amgen's Tech & Workforce Strategy by applying business analytics and change leadership skills to drive insights that impact resource allocation and sourcing strategy.\nYour responsibilities include dashboard development, ad-hoc reporting, business partnering & engagement, and financial baselining.\nThis role supports organizational change and enables the development of an integrated approach to global sourcing and financial planning.\nReporting to the Organizational Planning Analytics & Insights Procurement & Sourcing Lead, you will support Amgen's Tech & Workforce Strategy by applying business analytics and change management skills to drive insights that impact resource allocation and sourcing strategy.\nYour responsibilities include dashboard development, ad-hoc reporting, business partnering & engagement, and financial baselining.\nThis role supports change management and enables the development of an integrated approach to global sourcing and financial planning.\nRoles & Responsibilities:\nAddressing business challenges through process evaluation and insight generation.\nDevelop insights with a strong focus on Tableau and Power BI dashboard creation, as well as PowerPoint presentations.\nGuide data analysts and data engineers on standard methodologies for building data pipelines to support dashboards and other business objectives.\nConduct ad hoc analyses of FP&A and sourcing/procurement data.\nAddressing business challenges through process evaluation and insight generation.\nDevelop insights with a strong focus on Tableau and Power BI dashboard creation, as well as PowerPoint presentations.\nGuide data analysts and data engineers on standard methodologies for building data pipelines to support dashboards and other business objectives.\nConduct ad hoc analyses of FP&A and sourcing/procurement data.\nWhat we expect of you:\nWe are all different, yet we all use our unique contributions to serve patients.\nBasic Qualifications:\nMaster's degree and 4 to 6 years of applicable experience in business analysis (finance analysis, data analysis, sourcing analysis, or similar experience) OR\nBachelor's degree and 6 to 8 years of applicable experience in business analysis (finance analysis, data analysis, sourcing analysis, or similar experience) OR\nDiploma and 10 to 12 years of applicable experience in business analysis (finance analysis, data analysis, sourcing analysis, or similar experience)\nPreferred Qualifications:\nMaster's degree in data science, business, statistics, data mining, applied mathematics, business analytics, engineering, computer science, or a related field\n4 years of relevant experience in data science, data analytics, consulting, and/or financial planning & analysis\nA keen eye for design, with the ability to craft engaging PowerPoint decks and develop compelling Power BI and Tableau dashboards\nProven expertise in statistical/mathematical modeling and working with structured/unstructured data\nExperience with procurement, sourcing, and/or financial planning data\nSkilled in automating data workflows using tools like Tableau, Python, R, Alteryx, and PowerApps\nKnowledge of global finance systems, Procurement, and sourcing operations\nExperience with data analysis, budgeting, forecasting, and strategic planning in the Bio-Pharmaceutical or biotech industry\nGrowing in a start-up environment, building a data-driven transformation capability\nUnderstanding of the Bio-Pharmaceutical and biotech industry trends and operations\nProven ability to engage with cross-functional business leaders to align data strategies with corporate objectives, redefining complex data insights into actionable strategies\nFlexible work models, including remote work arrangements, where possible\nAs we work to develop treatments that deal with others, we also work to care for your professional and personal growth and well-being. From our competitive benefits to our collaborative culture, we'll support your journey every step of the way.",Biotechnology,"['Data Analysis', 'Sourcing', 'Financial Planning', 'Tableau', 'Power Bi', 'python']",2025-06-12 15:27:47
EDC IT Platform Engineer,Astrazeneca,1-2 Years,,Chennai,"Facilitate key discussions with various stakeholders as requested by system owner/platform lead and provide SME technical support on system testingof the Core EDC Product,and anydata integrations (in and out)of EDC.\nPerform impact analysis for any change that happens on the EDCsystem\nWork with business and perform risk assessment during change implementations/systemupgradesor updates tointegrations\nFollow andparticipatein validation lifecycle for upgrades, changes, & new integrations\nCo-develop validation packagesin accordance withstandard operating procedures (SOPs).\nProvide support of code review and solutioningfor downstream customer requirements, including data extraction methods (SQL, Webservices, APIs)\nActivelyparticipatein business meetings and bringnew ideasto thetable\nResponsibleto solveday-to-day incidents, service requests andworktowards incident-reduction and automation. Also, update Service Now (ticket management tool) for all events with respect to the incidents, service requests,changesand problems.\nConstantly thrive to learn and upskill according to the organization s goals and demands.\nCreateGxpValidation Test cases inJiraXrayfor system/upgradetesting\nPerform validationtest executions in Jira Xray,for bothsystem testingand regression testing.\nAssistin creating or create UAT test cases that cover the essential business scenarios/use casesfor the business.\nFacilitateprocess requirementsgathering:capturing user stories, requirement analysis, data analysis, programming, database management, and systems integration.\nMandatory Skills\nExtensive experience of usingMedidataRAVE UI(Classic or Rave EDC)to perform the following actions:\nStudy build, edit checks programming, deployment to production andmigration\nUnderstanding aboutdifferent rolesand permissions in Rave\nUnderstanding of core configuration and clinical views\nUnderstanding of what data extraction capabilities there are\nHands on Experience of Inbound Rave Web Services (egIVRS or discrepancy inputs)\nExperience of setting up Rave safety gateway (RSG) and SAE notifications\nExperience of setting up SAS extracts\nExperience on setting up Target Source Data Verification (TSDV)\nHands on experience of being involved in URL upgrade(s) (Medidata release updates)\nKnowledge on system validation process / Application Life Cycle Management\nMust havestrong communicationskills and interpersonalskills\nResponsible to work on technical tasks to keep the Rave URL(s) andit sintegrations in a maintained state\nAdhere to AZ core ITIL guidelines and perform tasks as incident, change, problem etc.\nWorking experience in clinicalstandard anddatacollection, clinical data conversion or clinical data programming/testing.\nUnderstandingof CDASH, SDTM,ADaMCDISC controlled TerminologyandFDA/ICH guidelines for clinical studydata\nEffective in stakeholder management, translating technical concepts, and working within team environments.\nSkilled in project management basics and creating clear technical documentation.\nPreferred Skills\nHands on Experience of custom function,SASprogramming\nKnowledge on Coder/ Coder+setup&Lab setup\nHands on Experience of outbound Rave Web Services (ODM Adapter, Biostats Adapter)\nHands on Experience of Rave backend data model,sqlscript writing, stored procedures\nHands-on knowledge in agile management tools like Service NOW\nHands-on experience in performing system testing including authoring and executing testcases\nPassion for learning, innovating, and delivering valuable software to people.\nAny other programming languages/technical skillse.g: Python, JAVA, etc.\nKnowledge of other Medidata products and/or Cloud\nEDCtesting(Rave/Inform/Veeva)experience.\nEducational Requirements\nBachelor or master sdegree in computer science or life science related discipline with IT/CRO/Pharma experience with total of8+ years of experience",Pharmaceutical,"['Medidata Rave EDC', 'Clinical Data Programming (SAS)', 'Clinical Data', 'rave web Services', 'EDC Testing', 'Data Analysis', 'Sdtm', 'Sql', 'Python', 'Java', 'Sas Programming']",2025-06-12 15:27:52
EDC IT Platform Engineer,Astrazeneca,1-2 Years,,Chennai,"Facilitate key discussions with various stakeholders as requested by system owner/platform lead and provide SME technical support on system testingof the Core EDC Product,and anydata integrations (in and out)of EDC.\nPerform impact analysis for any change that happens on the EDCsystem\nWork with business and perform risk assessment during change implementations/systemupgradesor updates tointegrations\nFollow andparticipatein validation lifecycle for upgrades, changes, & new integrations\nCo-develop validation packagesin accordance withstandard operating procedures (SOPs).\nProvide support of code review and solutioningfor downstream customer requirements, including data extraction methods (SQL, Webservices, APIs)\nActivelyparticipatein business meetings and bringnew ideasto thetable\nResponsibleto solveday-to-day incidents, service requests andworktowards incident-reduction and automation. Also, update Service Now (ticket management tool) for all events with respect to the incidents, service requests,changesand problems.\nConstantly thrive to learn and upskill according to the organization s goals and demands.\nCreateGxpValidation Test cases inJiraXrayfor system/upgradetesting\nPerform validationtest executions in Jira Xray,for bothsystem testingand regression testing.\nAssistin creating or create UAT test cases that cover the essential business scenarios/use casesfor the business.\nFacilitateprocess requirementsgathering:capturing user stories, requirement analysis, data analysis, programming, database management, and systems integration.\nMandatory Skills\nExtensive experience of usingMedidataRAVE UI(Classic or Rave EDC)to perform the following actions:\nStudy build, edit checks programming, deployment to production andmigration\nUnderstanding aboutdifferent rolesand permissions in Rave\nUnderstanding of core configuration and clinical views\nUnderstanding of what data extraction capabilities there are\nHands on Experience of Inbound Rave Web Services (egIVRS or discrepancy inputs)\nExperience of setting up Rave safety gateway (RSG) and SAE notifications\nExperience of setting up SAS extracts\nExperience on setting up Target Source Data Verification (TSDV)\nHands on experience of being involved in URL upgrade(s) (Medidata release updates)\nKnowledge on system validation process / Application Life Cycle Management\nMust havestrong communicationskills and interpersonalskills\nResponsible to work on technical tasks to keep the Rave URL(s) andit sintegrations in a maintained state\nAdhere to AZ core ITIL guidelines and perform tasks as incident, change, problem etc.\nWorking experience in clinicalstandard anddatacollection, clinical data conversion or clinical data programming/testing.\nUnderstandingof CDASH, SDTM,ADaMCDISC controlled TerminologyandFDA/ICH guidelines for clinical studydata\nEffective in stakeholder management, translating technical concepts, and working within team environments.\nSkilled in project management basics and creating clear technical documentation.\nPreferred Skills\nHands on Experience of custom function,SASprogramming\nKnowledge on Coder/ Coder+setup&Lab setup\nHands on Experience of outbound Rave Web Services (ODM Adapter, Biostats Adapter)\nHands on Experience of Rave backend data model,sqlscript writing, stored procedures\nHands-on knowledge in agile management tools like Service NOW\nHands-on experience in performing system testing including authoring and executing testcases\nPassion for learning, innovating, and delivering valuable software to people.\nAny other programming languages/technical skillse.g: Python, JAVA, etc.\nKnowledge of other Medidata products and/or Cloud\nEDCtesting(Rave/Inform/Veeva)experience.\nEducational Requirements\nBachelor or master sdegree in computer science or life science related discipline with IT/CRO/Pharma experience with total of8+ years of experience",Pharmaceutical,"['Medidata Rave EDC', 'Clinical Data Programming (SAS)', 'Clinical Data', 'rave web Services', 'EDC Testing', 'Data Analysis', 'Sdtm', 'Sql', 'Python', 'Java', 'Sas Programming']",2025-06-12 15:27:54
Assistant Manager - Data Analytics,Tata CLiQ,1-7 Years,,"Navi Mumbai, Mumbai City, Mumbai",Role Responsibilities:\nAnalyze data from diverse sources to identify trends and actionable insights\nCollaborate cross-functionally to support data-driven decision-making\nDesign and maintain dashboards and reports using Tableau and Power BI\nApply statistical and machine learning methods for predictive analysis\nKey Deliverables:\nAutomated dashboards and visualizations for key business metrics\nActionable business insights based on analytical models\nData quality and performance monitoring across business functions\nProcess documentation and automation of reporting workflows,"Shopping, Internet","['Sql', 'Python', 'Tableau', 'Google Analytics', 'Machine Learning Algorithms']",2025-06-12 15:27:55
"Data Engineer (Java, Python, SQL, Linux), Aladdin Data, Vice President",Primetrace Technologies,2-7 Years,,Mumbai,"And Ideal candidate would have\nBA/BS in Computer Science or equivalent practical experience\nAt least12+yearsexperienceasdataengineerin a medium to large scale solutions\nAt least 2+ years experience in leading a data engineering team\nMust have experience architecting and building scalable Big Data applications\nMust have clouddevelopment experience (Azure, GCP, AWS)\nDevelopment experienceusingJava, Python, SQL, Linuxisa must\nExperience with Database Modeling, Normalization techniques\nExperience / Familiarity with object-oriented design patterns\nExperience with dev ops tools likeGit, Maven, Jenkins, Gitlab CI, Azure DevOps\nExperience with Agile development concepts and related tools\nAbility to trouble shoot and fix performance issues across the codebase and database queries\nExcellent written and verbal communication skills\nPassion for learning and implementingnew technologies\nAbility tooperateunder fast-paced environment\nSkills that would be a plus\nPerl\nC++\nExperience with Micro services and API\nExperience supporting large scale, complex analytics requirements\nExperience working inFinanceindustry (Investment Banking,Capital Markets etc.)",Information Technology,"['Data engineering experience', 'cloud development', 'Java Python SQL proficiency', 'DevOps tools knowledge', 'team leadership experience', 'Database Modeling']",2025-06-12 15:27:57
CSV Analyst - TAPI,Watson Pharama,2-5 Years,,Noida,"Software Validation - Validation Plan definition, URS/FS/DS, Risk Assessment, Testing Plan and Scrips, Testing Execution, PQ / User Acceptance testing guidance and support, Validation Summary Report, system WI/SOP.\nValidation state maintenance. Periodic Reviews of Laboratory Applications Systems with respect to compliance to SDLC, User permissions and data integrity controls.\nActivities and Documentation of Data Integrity controls, Back-Up/Restore processes, Disaster Recovery processes and User Management processes on Laboratory Application Systems.\nUnderstanding system requirements, discussing business process / proposals / implementation issues / data security / data integrity / compliance to application guidelines, etc.\nDocumenting the changes and new developments as per the documentation standards.\nCompliance with GxP Guidelines, Good Documentation practices & activities.\nYour experience, qualifications & skills\nBackground in computer science or similar IT background, or a background in chemistry or related scientific field\n2-5 years of experience working with CSV within the pharmaceutical or biotech industry.\nExpertise in Pharma Laboratory Application Systems - User Management, BackUp/Restore processes, Data Integrity Controls as per GxP Compliance.\nExperience in software validations.\nExperience with HP gALM system & understanding of SDLC structure.\nDocumentation related to Quality processes such as Change Controls, CAPA, Deviations and Investigations.\nKnowledge of GAMP5 and regulatory guidelines such as 21 CFR Part 11, EU Annex 11, etc.\nOS: Windows\nBasic knowledge of LAN / WAN\nBasic knowledge of Servers / Switches / Routers\nStrong experience with Microsoft Office application\nExposure to File Server/Network Shared Drives/Shared Multifunction Devices",Pharmaceutical,"['gALM', 'Lan', 'Sdlc', 'Wan', 'Gxp']",2025-06-12 15:27:58
Data Scientist ll,Tata CLiQ,1-7 Years,,"Navi Mumbai, Mumbai City, Mumbai","Role Responsibilities:\nCollect, process, and analyze large datasets to support data-driven decisions\nApply deep learning and statistical models to extract actionable insights\nImplement data governance and data quality frameworks\nDevelop data processing tools aligned with business objectives\nKey Deliverables:\nDesign and execute advanced analytical models and forecasts\nEnsure compliance with data governance and sharing protocols\nEstablish data pipelines and processing applications\nNormalize and replicate data for consistency and availability","Shopping, Internet","['Forecasting', 'Data Science', 'Data Architecture', 'Deep Learning', 'Data Quality']",2025-06-12 15:27:59
Junior Associate - Forecasting,Astrazeneca,1-3 Years,,Bengaluru,"As a Forecast Junior Associate, you will collaborate closely with the GIBEx/GA&I Forecast Leader and Commercial teams to develop brand forecasts for strategic brands and early assets. Your role involves understanding therapeutic areas, developing product-specific forecast models, and maintaining up-to-date models that reflect current data and assumptions. Youll identify key business issues impacting forecasts, incorporate primary research results, and define forecasting processes. Additionally, youll develop forecasts using patient-based models, ensuring alignment with business partners across various departments.\nEssential Skills/Experience\n1+ years of direct Pharmaceutical forecasting experience with an in-depth knowledge of forecasting techniques, models and approaches\nGood understanding of one or more disease areas with experience in rare diseases/OBU including how patients move through their respective diagnosis and treatment pathways, including treatment dynamics\nExperience integrating insights from market research and secondary data analysis into forecast assumptions\nExperience applying a range of data sources and analytics involving standard data in the Pharmaceutical industry - e.g. IQVIA (MIDAS, DDD NPA, Monthly Xponent), claims data (Truven, Marketscan), epidemiological data, etc. For US forecasting, experience with TRx / NBRx / NRx data and methodologies\nStrong analytical expertise; excellent Excel, financial modelling and forecasting skills\nStrong written and verbal communication\nDesirable Skills/Experience\nAdvanced degree preferred (e.g., PhD, MBA, Masters)\nKnowledge and experience in sophisticated statistical forecast methodologies and applied AI / ML / automation\nExperience working with Power Apps, specifically PowerBI\nFor US forecasting, proven understanding of US Market Access and gross-to-net",Pharmaceutical,"['Forecasting', 'Data Analysis', 'Financial Modelling', 'Pharmaceutics', 'AI ML', 'Power Bi']",2025-06-12 15:28:00
"Data Engineering, Aladdin Data - Vice President",Primetrace Technologies,9-14 Years,,Gurugram,"Data is at the core of the Aladdin platform, and increasingly, our ability to consume, store, analyze, and gain insight from data is a keycomponentof what differentiates us. As part of Aladdin Studio, The Aladdin Data Cloud (ADC) Engineering teamis responsible forbuilding andmaintainingdata-as-a-service solution for all the data management and transformation needs. We engineer high performance data pipelines, provide a fabric to discover and consume data, and continually evolve our data surface capabilities.\nAs aData engineer in theADCEngineering team,you will: -\nWork alongside our engineers to help design and build scalable data pipelines while evolving the data surface.\nHelp prove out anddeliverCloud Native Infrastructure and tooling to support scalabledata cloud.\nHave fun as part of anamazingteam.\nSpecific Responsibilities:\nLeading and working as part of a multi-disciplinary squad toestablishour next generation of data pipelines and tools.\nBe involved frominceptionof projects, understanding requirements, designing&developing solutions,and incorporating them into the designs of our platforms.\nMentor team members on technology andstandard processes.\nMaintainexcellent knowledge of the technical landscapefor data & cloud tooling\nAssistinsolvingissues, support the operation of production software.\nDesignsolutions and document it.\nDesirable Skills\n8+yearsof industry experience in data engineering area.\nPassionfor engineeringandoptimizingdata sets, data pipelinesandarchitecture.\nAbility to build processes that support data transformation, workload management, data structures,lineage,and metadata.\nKnowledge of SQL and performance tuning.Experience with Snowflake is preferred.\nGood understandingoflanguages such asPython/Java\nUnderstandingofsoftware deployment and orchestration technologies such asairflowetc.\nExperience withdbtishelpful.\nWorking knowledge of building and deploying distributed systems\nExperience in creating and evolving CI/CD pipelines withGitLab orAzure Dev Ops.\nExperience in handling multi-disciplinaryteamand mentoring them.",Information Technology,"['snowflake', 'Airflow', 'dbt', 'Sql', 'Python', 'Azure DevOps', 'data engineering']",2025-06-12 15:28:02
"Quant Data Specialist , Aladdin Financial Engineering - Associate",Primetrace Technologies,2-6 Years,,Gurugram,"We are looking for a person to join the Advanced Data Analytics team with AFE Single Security.Advanced Data Analytics is a team of Quantitative Data and Product Specialists, focused on delivering Single Security Data Content, Governance and Product Solutions and Research Platform.The teamleveragesdata, cloud, and emerging technologies in buildinganinnovativedata platform, with the focus on business and research use cases in theSingleSecurity space.The team uses various statistical/mathematical methodologies to derive insights and generate content to help develop predictive models, clustering, and classification solutions and enable Governance.The team works on Mortgage, Structured & Credit Products.\nWe are looking for a person to help build andexpandData & Analytics Content in the Credit space.The person willbe responsible forbuilding,enhancing,andmaintainingthe Credit Content Suite.The person willwork on the below-\nCredit DerivedData Content\nModel & DataGovernance\nCredit Model & Analytics\nExperience\nExperience on Scala\nKnowledge of ETL, data curation and analytical jobs using distributed computing framework with Spark\nKnowledge and Experience of working with large enterprisedatabaseslike Snowflake, Cassandra& Cloud manged services likeDataproc, Databricks\nKnowledge of financial instruments like Corporate Bonds, Derivatives etc.\nKnowledge of regression methodologies\nAptitude for design and building tools forDataGovernance\nPythonknowledge is a plus\nQualifications\nBachelors/masters in computer sciencewitha majorin Math,Econ,or related field\n3-6years of relevant experience",Information Technology,"['snowflake', 'Credit Analytics', 'Regression methods', 'Quant Data Specialist', 'Scala', 'Spark', 'data curation']",2025-06-12 15:28:03
"Quant Data Specialist , Aladdin Financial Engineering - Associate",Primetrace Technologies,3-6 Years,,Mumbai,"Experience\nExperience on Scala\nKnowledge of ETL, data curation and analytical jobs using distributed computing framework with Spark\nKnowledge and Experience of working with large enterprisedatabaseslike Snowflake, CassandraCloud manged services likeDataproc, Databricks\nKnowledge of financial instruments like Corporate Bonds, Derivatives etc\nKnowledge of regression methodologies\nAptitude for design and building tools forDataGovernance\nPythonknowledge is a plus\nQualifications\nBachelors/masters in computer sciencewitha majorin Math,Econ,or related field\n3-6years of relevant experience",Information Technology,"['Scala experience', 'Spark distributed computing', 'Snowflake and Cassandra databases', 'financial instruments knowledge', 'data governance tools design', 'regression methodologies expertis']",2025-06-12 15:28:05
Data Scientist 1,Xoom,1-3 Years,,Bengaluru,"In your role as a Decision Scientist, you will\nTrackand measureperformance against KPIs and goals toidentifyand mitigate fraud risk and enable growth\nDevelop and own data-driven risk decision strategies to support PayPal srisk andbusiness goals\nUtilize bottom-up, story based, innovative techniques to apply analytical know how onto PayPal s risk strategies\nPlan,driveand execute projects from start to finish, with partners across the company, to developcutting edge, scalable and safe solutions\nWork with product and platform teams to developcutting edge, scalable and safe products, to enhance the experience for our global customers\nGrowongoing communication withpartnersacross the company and share updates with senior leaderseffectively and while translating complex problems into simpler terms\nWhat do you need to bring-\nBachelor s Degree(or above)in one of the following economics, finance, engineering, exact sciences, etc.\n2+ years ofhands-onexperience in relevant positions,e.g.decision scientist, data analyst, data scientist, product manager,background inidentifyingtrends and patterns /risk management/army intelligence units\nTechnical orientation;experience in SQL / R / Python / other programming language -a must\nExcellent English verbal and written communication skills\nProven analytical andproblem solvingskills, business mindset\nQuick-thinker, fast learner,self-starter",FinTech,"['R', 'Data Analysis', 'Risk Management', 'decision science', 'Sql', 'Python']",2025-06-12 15:28:06
CDnA - Data Science Manager,Amgen Technology Private Limited,4-6 Years,,Hyderabad,"Commercial Data Science Predictive Analytics\nOverview:\nIn this vital role, you will collaborate with business partners, service owners, and IS peers to develop predictive models and insights for the US Commercial Organization. This position will drive business impact through advanced analytics techniques aligned with Amgen's mission of helping patients access the therapies they need.\nThis is a Flexible Commuter role to the Amgen India office, with on-site work expected 23 days a week.\nPrimary Responsibilities:\nCollaborate with multi-functional teams to derive insights that drive business value.\nIdentify business needs and propose analytics-based solutions.\nDevelop and deploy frameworks to monitor campaign performance and tactics at a granular level.\nLead measurement and tracking of omnichannel CX enablement initiatives.\nSupport development of data science models, prototypes, and proof of concepts to test omnichannel strategies.\nEffectively communicate analysis concepts, progress, and results to leadership and business stakeholders.\nBasic Qualifications:\nDoctorate degree\nOR\nMaster's degree with 46 years of data science and/or analytics experience\nOR\nBachelor's degree with 68 years of data science and/or analytics experience\nOR\nDiploma with 1012 years of data science and/or analytics experience\nPreferred Qualifications:\nExperience in campaign measurement, marketing analytics, and resource optimization, ideally in the pharmaceutical domain\nProgramming experience in Python, R, or SAS\nFamiliarity with machine learning libraries like scikit-learn, MLlib, or TensorFlow\nExperience working with large datasets and distributed computing tools such as Spark and Hive\nAbility to clearly and practically communicate analytical findings\nA passion for continuous learning and keeping up with developments in advanced analytics\nExperience in the biotech/pharma industry",Biotechnology,"['R', 'Marketing Analytics', 'campaign measurement', 'Python', 'Machine Learning', 'Spark']",2025-06-12 15:28:07
Associate - Data Solutions,Primetrace Technologies,4-9 Years,,Gurugram,"Key Responsibilities:\nAs an Associate Data Steward, your responsibilities will span several key areas:\nBusiness Strategic Acumen:You will collaborate closely with business units to understand evolving data requirements and align data products to meet strategic goals and objectives. You will ensure that data products support various use cases, such as operational efficiencies, risk management, and commercial applications, while defining success criteria for data offerings in collaboration with key stakeholders.\nData Governance Quality:A core aspect of this role is managing data quality through the application of robust data governance controls. You will be responsible for monitoring data health, implementing data quality metrics, and ensuring that data products meet established standards for accuracy, completeness, and consistency. Regular assessments of data sources and processes will be part of your ongoing responsibilities to identify deficiencies and opportunities for improvement.\nData Product Lifecycle Management:You will support the full delivery lifecycle of data products, from ideation to release. This includes working with cross-functional teams such as product managers, engineers, and business stakeholders to plan, design, and deliver data products. In addition, you will contribute to the design and creation of conceptual, logical, and physical data models to ensure that data products meet business requirements.\nRequirements Gathering Documentation:You will be actively involved in gathering, defining, and documenting business requirements for data products. This includes translating business needs into detailed data requirements and user stories for development teams. You will work to break down complex data problems into manageable tasks, ensuring alignment between technical and business requirements.\nTesting Quality Assurance:During the testing phase of data product development, you will collaborate with engineering and quality assurance teams to validate that data is accurately extracted, transformed, and loaded. Ensuring that data governance controls are applied during testing is also part of your role, and you will help resolve any issues that arise.\nVendor Stakeholder Management:You will manage relationships with external data vendors to ensure that data feeds meet business requirements and quality standards. Additionally, you will work with both internal and external stakeholders to ensure that data products align with organizational goals and address customer needs. Regular engagement with stakeholders will be key to soliciting feedback on data products and identifying opportunities for enhancement.\nData Stewardship Support:In addition to data management, you will provide Level 3 support for complex data-related inquiries and issues. You will proactively identify data challenges and offer data-driven solutions to meet business objectives. You will also participate in data governance initiatives, helping to define and implement best practices for data stewardship across the organization.\nCollaboration Communication:You will communicate effectively with both technical and non-technical teams, ensuring that complex data concepts are conveyed clearly. Your collaboration with internal and external teams will ensure that data solutions align with business goals and industry best practices. You will be expected to work in an agile environment, managing multiple priorities to ensure efficient and timely data product delivery.\nQualifications Requirements:\nThe ideal candidate will possess the following qualifications:\nExperience:\nAt least 4 years of experience in data stewardship, data governance, or a related field.\nExperience in the financial services industry is a plus, but not required.\nA strong background in data modeling (logical, conceptual, physical), data governance, and data quality management is essential.\nTechnical Skills:\nProficiency in data management tools and technologies such as SQL,, Unix,, Tableau, etc.\nFamiliarity with data governance platforms (e.g., Aha!, ServiceNow, Erwin Data Modeling, DataHub) and methodologies for data management and quality assurance is preferred.\nKnowledge of databases (Relational, NoSQL, Graph) and cloud-based data platforms (e.g., Snowflake) is also beneficial.\nBusiness Communication Skills:\nStrong business acumen and the ability to align data products with both organizational and client needs.\nYou should be able to effectively communicate complex technical concepts to both technical and non-technical stakeholders.\nStrong organizational skills and the ability to manage multiple tasks and priorities in an agile environment are essential.",Information Technology,"['Stakeholder Management', 'data solutions', 'Data Governance', 'Data Modeling', 'Sql', 'Unix', 'Tableau']",2025-06-12 15:28:09
Assoc. Data Engineer - R&D Precision Medicine Team,Amgen Technology Private Limited,1-3 Years,,Hyderabad,"Roles & Responsibilities\nDesign and build scalable enterprise analytics solutions using Databricks, Power BI, and other modern data management tools.\nLeverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation\nBreak down features into work that aligns with the architectural direction runway\nParticipate hands-on in pilots and proofs-of-concept for new patterns\nCreate robust documentation from data analysis and profiling, and proposed designs and data logic\nDevelop advanced sql queries to profile, and unify data\nDevelop data processing code in sql, along with semantic views to prepare data for reporting\nDevelop PowerBI Models and reporting packages\nDesign robust data models, and processing layers, that support both analytical processing and operational reporting needs.\nDesign and develop solutions based on best practices for data governance, security, and compliance within Databricks and Power BI environments.\nEnsure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.\nDevelop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.\nCollaborate with partners to define data requirements, functional specifications, and project goals.\nContinuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions.\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The professional we seek is someone with these qualifications.\nBasic Qualifications:\nMasters degree with 1 to 3 years of experience in Data Engineering OR\nBachelors degree with 1 to 3 years of experience in Data Engineering\nMust-HaveSkills:\nMinimum of 1 year of hands-on experience with BI solutions (Preferrable Power BI or Business Objects) including report development, dashboard creation, and optimization.\nMinimum of 1 year of hands-on experience building Change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management.\nHands-on experience with Databricks, including data engineering, optimization, and analytics workloads.\nExperience using cloud platforms (AWS), data lakes, and data warehouses.\nWorking knowledge of ETL processes, data pipelines, and integration technologies.\nGood communication and collaboration skills to work with cross-functional teams and senior leadership.\nAbility to assess business needs and design solutions that align with organizational goals.\nExceptional hands-on capabilities with data profiling and data anlysis",Biotechnology,"['Data Lakes', 'Data Pipelines', 'Etl', 'AWS']",2025-06-12 15:28:11
Assoc. Data Engineer - R&D Precision Medicine Team,Amgen Technology Private Limited,1-3 Years,,Hyderabad,"Roles & Responsibilities\nDesign and build scalable enterprise analytics solutions using Databricks, Power BI, and other modern data management tools.\nLeverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation\nBreak down features into work that aligns with the architectural direction runway\nParticipate hands-on in pilots and proofs-of-concept for new patterns\nCreate robust documentation from data analysis and profiling, and proposed designs and data logic\nDevelop advanced sql queries to profile, and unify data\nDevelop data processing code in sql, along with semantic views to prepare data for reporting\nDevelop PowerBI Models and reporting packages\nDesign robust data models, and processing layers, that support both analytical processing and operational reporting needs.\nDesign and develop solutions based on best practices for data governance, security, and compliance within Databricks and Power BI environments.\nEnsure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.\nDevelop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.\nCollaborate with partners to define data requirements, functional specifications, and project goals.\nContinuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions.\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The professional we seek is someone with these qualifications.\nBasic Qualifications:\nMasters degree with 1 to 3 years of experience in Data Engineering OR\nBachelors degree with 1 to 3 years of experience in Data Engineering\nMust-HaveSkills:\nMinimum of 1 year of hands-on experience with BI solutions (Preferrable Power BI or Business Objects) including report development, dashboard creation, and optimization.\nMinimum of 1 year of hands-on experience building Change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management.\nHands-on experience with Databricks, including data engineering, optimization, and analytics workloads.\nExperience using cloud platforms (AWS), data lakes, and data warehouses.\nWorking knowledge of ETL processes, data pipelines, and integration technologies.\nGood communication and collaboration skills to work with cross-functional teams and senior leadership.\nAbility to assess business needs and design solutions that align with organizational goals.\nExceptional hands-on capabilities with data profiling and data anlysis",Biotechnology,"['Data Lakes', 'Data Pipelines', 'Etl', 'AWS']",2025-06-12 15:28:21
Hiring For AIML Lead Engineer - WFH Opportunity,Optimeyes Ai,15-24 Years,,Remote,"Responsibilities:\nUnderstanding business objectives and developing models that help to achieve them, along with metrics to track their progress.\nAnalyzing ML algorithms that could be used to solve a given problem and ranking them by their success probability. Determine and refine machine learning objectives.\nDesigning machine learning systems and self-running artificial intelligence (AI) software to automate predictive models.\nTransforming data science prototypes and applying appropriate ML algorithms and tools.\nExploring and visualizing data to gain an understanding of it, then identifying differences in data distribution that could affect performance when deploying the model in the real world.\nEnsuring that algorithms generate accurate user recommendations.\nVerifying data quality and/or ensuring it via data cleaning.\nSupervising the data acquisition process if more data is needed.\nDefining validation strategies.\nDefining the pre-processing or feature engineering to be done on a given dataset.\nSolving complex problems with multi-layered data sets, as well as optimizing existing machine learning libraries and frameworks.\nDeveloping ML algorithms to analyze huge volumes of historical data to make predictions.\nRunning tests, performing statistical analysis, and interpreting test results.\nDeploying models to production.\nDocumenting machine learning processes.\nKeeping abreast of developments in machine learning.\nPreferred Candidate Profile:\nBachelor's degree in Computer Science, Data Science, Mathematics, or a related field.\nKnowledge as a machine learning engineer.\nProficiency with a deep learning framework such as TensorFlow, XGBoost, WaveNet, Keras, NumPy.\nAdvanced proficiency with Python, Java, and R code writing.\nProficiency with Python and basic libraries for machine learning such as scikit-learn and pandas.\nExtensive knowledge of ML frameworks, libraries, data structures, data modeling, and software architecture in ANN, CNN, RNN with LSTM.\nAbility to select hardware to run an ML model with the required latency.\nIn-depth knowledge of mathematics, statistics, and algorithms.\nSuperb analytical and problem-solving abilities.\nGreat communication and collaboration skills.\nExcellent time management and organizational abilities.\nBenefits of Working with OptimEyes:\nRemote work opportunity (work from home)\nWork opportunity with a top-notch team, cutting-edge technology, and leadership of extremely successful experts\nMonthly Bonus along with Salary\nYearly Bonus",Information Technology,"['TensorFlow/Keras', 'deep learning (CNN/RNN)', 'scikit-learn', 'data preprocessing', 'Machine Learning', 'Python']",2025-06-12 15:28:26
Hiring For AIML Lead Engineer - WFH Opportunity,Optimeyes Ai,15-24 Years,,Remote,"Responsibilities:\nUnderstanding business objectives and developing models that help to achieve them, along with metrics to track their progress.\nAnalyzing ML algorithms that could be used to solve a given problem and ranking them by their success probability. Determine and refine machine learning objectives.\nDesigning machine learning systems and self-running artificial intelligence (AI) software to automate predictive models.\nTransforming data science prototypes and applying appropriate ML algorithms and tools.\nExploring and visualizing data to gain an understanding of it, then identifying differences in data distribution that could affect performance when deploying the model in the real world.\nEnsuring that algorithms generate accurate user recommendations.\nVerifying data quality and/or ensuring it via data cleaning.\nSupervising the data acquisition process if more data is needed.\nDefining validation strategies.\nDefining the pre-processing or feature engineering to be done on a given dataset.\nSolving complex problems with multi-layered data sets, as well as optimizing existing machine learning libraries and frameworks.\nDeveloping ML algorithms to analyze huge volumes of historical data to make predictions.\nRunning tests, performing statistical analysis, and interpreting test results.\nDeploying models to production.\nDocumenting machine learning processes.\nKeeping abreast of developments in machine learning.\nPreferred Candidate Profile:\nBachelor's degree in Computer Science, Data Science, Mathematics, or a related field.\nKnowledge as a machine learning engineer.\nProficiency with a deep learning framework such as TensorFlow, XGBoost, WaveNet, Keras, NumPy.\nAdvanced proficiency with Python, Java, and R code writing.\nProficiency with Python and basic libraries for machine learning such as scikit-learn and pandas.\nExtensive knowledge of ML frameworks, libraries, data structures, data modeling, and software architecture in ANN, CNN, RNN with LSTM.\nAbility to select hardware to run an ML model with the required latency.\nIn-depth knowledge of mathematics, statistics, and algorithms.\nSuperb analytical and problem-solving abilities.\nGreat communication and collaboration skills.\nExcellent time management and organizational abilities.\nBenefits of Working with OptimEyes:\nRemote work opportunity (work from home)\nWork opportunity with a top-notch team, cutting-edge technology, and leadership of extremely successful experts\nMonthly Bonus along with Salary\nYearly Bonus",Information Technology,"['TensorFlow/Keras', 'deep learning (CNN/RNN)', 'scikit-learn', 'data preprocessing', 'Machine Learning', 'Python']",2025-06-12 15:28:28
data mining and data research,Murugesan Janani (Proprietor Of RRR Food Factory),Fresher,INR 2.5 - 4.5 LPA,Coimbatore,"Description\nWe are seeking a motivated individual for the position of Data Mining and Data Research. This role is ideal for freshers/entry-level and Experienced candidates looking to start their career in data analysis and research. The successful candidate will be responsible for conducting data mining and research activities to support various business initiatives.\nResponsibilities\nConduct thorough data mining and research to gather relevant information from various sources.\nAnalyze data sets to identify patterns, trends, and insights that can inform business decisions.\nPrepare detailed reports and presentations based on research findings.\nCollaborate with team members to support ongoing projects and initiatives.\nUtilize data visualization tools to present data in a clear and effective manner.\nSkills and Qualifications\nProficiency in data mining tools and techniques such as SQL, Python, or R.\nFamiliarity with data visualization tools like Tableau, Power BI, or similar.\nStrong analytical and problem-solving skills.\nAbility to work with large data sets and derive meaningful insights.\nExcellent communication skills, both written and verbal.\nAttention to detail and a strong commitment to accuracy.","Database, Data Mining, Data Visualization, Big Data, Data Integration","['Python', 'Sql', 'Data Cleaning', 'Statistical Analysis', 'Machine Learning', 'Data Visualization', 'Web Scraping', 'Data Mining', 'Excel', 'R Programming']",2025-06-12 15:28:30
Global Banking & Markets - Eq Client Analytics Eng - Associate - Bengaluru,Goldman Sachs,3-8 Years,,Bengaluru,"This is an exciting opportunity to join a team leveraging modern technology to deliver analytics that fuel relationship management, identify strategic opportunities, and facilitate pivotal business decisions. As part of our close-knit group, you will experience the dynamism of a small team with a wide impact across the division. The role will be suited to an analytical thinker with strong development skills who has a keen interest in working directly with business users.\nA successful candidate will need to\nDesign and build scalable, high performing and responsive applications tailored to enhance data-driven decision making\nTreat data as a first-class citizen by taking an interest in the accuracy, consistency, and timeliness of our metrics\nCollaborate with business stakeholders across the globe to understand analytical requirements, refine functionalities and address evolving business needs\nCreate well-designed and tested code using best practices, uplift existing applications using industry standards, and bring innovative technologies into the team\nTake responsibility for prompt resolution of queries from management and users and take ownership in the commercial delivery of projects\nBasic Qualifications\nStrong academic background with B.E/BTech in Computer Science or equivalent\nMinimum of 3 years of experience in software development\nStrong technical, analytical and problem-solving skills\nExperience of programming in Python, Java or equivalent language\nExperience and understanding of data storage (databases, SQL, Mongo)\nAbility to work on multiple tasks (requirements gathering, analysis, design, and implementation) while maintaining high standards of accuracy and attention to detail\nWell versed with SDLC (Software Development Lifecycle) and familiarity of associated tools for version control, build, deployment, testing and controls\nStrong written and verbal communication skills",Financial Services,"['Databases', 'Sql', 'Sdlc', 'Python', 'Software Development', 'Data Driven Statistical Analysis']",2025-06-12 15:28:32
Analytics Technical Specialist,Alstom,6-8 Years,,Bengaluru,"Design, develop, and deploy interactive dashboards and reports using MS Fabric & Qlik Cloud, ensuring alignment with business requirements and goals.\nImplement and manage data integration workflows utilizing MS Fabric to ensure efficient data processing and accessibility.\nTranslate business needs to technical specifications and Design, build and deploy solutions.\nUnderstand and integrate Power BI reports into other applications using embedded analytics like Power BI service (SaaS), Teams, SharePoint or by API automation.\nWill be responsible for access management of app workspaces and content.\nIntegration of PowerBi servers with different data sources and timely upgradation/services of PowerBi\nAble to schedule and refresh jobs on Power BI On-premise data gateway.\nConfigure standard system reports, as well as customized reports as required.\nResponsible in helping various kind of database connections (SQL, Oracle, Excel etc.) with Power BI Services\nInvestigate and troubleshoot reporting issues and problems\nMaintain reporting schedule and document reporting procedures\nMonitor and troubleshoot data flow issues, optimizing the performance of MS Fabric applications as needed.\nOptimize application performance and data models in Qlik Cloud while ensuring data accuracy and integrity.\nEnsure collaboration with Functional & Technical Architectsasbusiness cases are setup for each initiative,collaborate with other analytics team to drive and operationalize analytical deployment.Maintain clear and coherent communication, both verbal and written, to understand data needs and report results.\nEnsure compliance with internal policies and regulations\nStrong ability to take the lead and be autonomous Proven planning, prioritization, and organizational skills.Ability to drive change through innovation & process improvement. Be able to report to management and stakeholders in a clear and concise manner.\nGood to havecontribition to the integration and utilization of Denodo for data virtualization, enhancing data access across multiple sources.\nDocument Denodo processes, including data sources and transformations, to support knowledge sharing within the team.\nFacilitate effective communication with stakeholders regarding project updates, risks, and resolutions to ensure transparency and alignment.\nParticipate in team meetings and contribute innovative ideas to improve reporting and analytics solutions.\nEDUCATION\nBachelor's/Master's degree in Computer Science Engineering /Technology or related field\nExperience\nTotal 6 years of experience\nMandatory 2+ years of experience in Power BI End-to-End Development using Power BI Desktop connecting multiple data sources (SAP, SQL, Azure, REST APIs, etc.)\nExperience in MS Fabric Components along with Denodo.\nTechnical competencies\nProficient in using MS Fabric for data integration and automation of ETL processes.\nUnderstanding of data governance principles for quality and security.\nStrong expertise in creating dashboards and reports using Power BI and Qlik.\nKnowledge of data modeling concepts in Qlik and Power BI.\nProficient in writing complex SQL queries for data extraction and analysis.\nSkilled in utilizing analytical functions in Power BI and Qlik.\nExperience in troubleshooting performance issues in MS Fabric and Denodo.\nExperience in Developing visual reports, dashboards and KPI scorecards using Power BI desktop & Qlik\nUnderstand Power BI application security layer model.\nHands on PowerPivot, Role based data security, Power Query, Dax Query, Excel, Pivots/Charts/grid and Power View\nGood to have Power BI Services and Administration knowledge.\nExperience in developing data models using Denodo to support business intelligence and analytics needs.\nProficient in creating base views and derived views for effective data representation.\nAbility to implement data transformations and enrichment within Denodo.\nSkilled in using Denodo's SQL capabilities to write complex queries for data retrieval.\nFamiliarity with integrating Denodo with various data sources, such as databases, web services, and big data platforms.",Transportation,"['MS Fabric', 'Qlik Cloud', 'Power Bi', 'Denodo', 'Data Modeling', 'Sql']",2025-06-12 15:28:33
Data Scientist,Orange Business Services,5-8 Years,,Gurugram,"Work with product, Technical and Customer Support on identifying problems in different areas where machine learning/statistics can help.\nPresent your findings to both technical and non-technical audiences.\nLead traversal technical discussion with CTIO and GDO team\nParticipate in the entire LLM development lifecycle, from problem definition and data preparation to model training, evaluation, and deployment.\nIdentify and own Use case coming from Different BU and lead it from doing POC till industrialization\nDesign and implement novel LLM architectures and training techniques, leveraging deep learning frameworks like TensorFlow and PyTorch.\nBuild and share technical architecture with Business SPOC\nDevelop and maintain efficient pipelines for data preprocessing, training, and inference.\nCollaborate with data scientists to curate, clean, and prepare high-quality training data for LLMs.\nDevelop, contribute and Lead technical framework for AI use case environment\nEvaluate LLM performance using appropriate metrics and identify opportunities for improvement.\nLLM performance evaluation and define metrices\nApply advanced machine learning algorithms, statistical methods and predictive modeling techniques on large and varied data sets that include application logfiles, other online application telemetry, structured and unstructured data sources\nBuilding machine Learning model and application.\nhelp and act as Technical support to Other DS within team.\nAct as technical consultant for Data AI use case.\nDeploy LLMs to production environments and monitor their performance for accuracy, fairness, and efficiency\nBuilding machine Learning model and application.\nhelp and act as Technical support to Other DS within team.\nAct as technical consultant for Data AI use case.\nDesign and develop front-end interfaces for AI-powered applications using modern web technologies such as HTML, CSS, JavaScript, and frameworks like React or Angular.\nLead and Research and development initiative within Team and come up industry standard practice within team to follow in ongoing projects.\nStay up-to-date on the latest advancements in LLM research and identify opportunities to incorporate them into your work\nAct as SPOC from data AI team to leverage AI capabilities with Business team.\nBuild scalable back-end systems and APIs to support AI model integration and data processing using languages such as Python, Java, or Node.js.\nCollaborate with researchers to explore new applications for LLMs in various domains.\nDefine Solution design proposal for business\nManage the infrastructure needed for data scientists to run their experiments and deploy models. This includes setting up compute clusters with GPUs or TPUs for computationally intensive tasks, configuring cloud storage for datasets, and managing containerized environments for model deployment\nDocument your work clearly and concisely, including research papers, technical reports, and code documentation.\nAbout you\nExtensive experience in between 3-5years of the following domains: Data Science, Machine Learning, Deep Learning, LLM application development, DevOps and UI designing.\nA minimum of 5 years hands-on applied research experience developing and implementing machine learning models on large scale data sets\nExpert proficiency interrogating distributed databases (Map/Reduce, Hadoop, Hive, ) is also highly desired should have worked on handling web automation projects.\nPositive work attitude with ability to work well as a team with personnel from other teams/organizations.\nHave extensive experience of the design and delivery of AI or Big Data solutions\nExpertise in machine learning and statistical analysis approaches such as classification, clustering, regression, statistical inference, collaborative filtering, natural\nHands-on experience in conducting analyses on unstructured structured / semi-structured data\nAbility to drive initiatives from within the team and realize them for organization and/or team s benefit.\nExcellent interpersonal and communication skills.",IT Management,"['Data Processing', 'Customer Support', 'Front End', 'Machine Learning', 'Javascript', 'Application Development']",2025-06-12 15:28:34
Lead Data Scientist,Orange Business Services,6-9 Years,,Gurugram,"About the role\nDesign, develop, and deploy machine learning models that meet business needs and deliver measurable impact.\nManage the end-to-end lifecycle of AI/ML models, including data gathering, feature engineering, model training, testing, and deployment.\nCollaborate with data engineers, product managers, and business stakeholders to understand project requirements and deliver impactful solutions.\nGuide the development and optimization of large-scale data pipelines to support model training and deployment.\nDevelop, Implement, and enhance models in areas such as predictive analytics, GenAI, natural language processing (NLP), deep learning, and recommendation systems.\nEnsure the scalability, efficiency, and accuracy of machine learning models in production environments.\nMentor and guide junior data scientists and ML engineers, fostering a culture of learning and innovation.\nStay up-to-date with the latest industry trends, technologies, and research in machine learning, AI, GenaI and data science, and apply relevant insights to projects.\nCommunicate technical concepts and results effectively to business stakeholders and senior leadership.\nWork closely with DevOps and engineering teams to manage the lifecycle of ML models, including versioning, monitoring, and maintenance.\nContribute to the strategic direction of the AI/ML/GenaI practice within the organization.\nAbout you\n6+ years of experience in data science / machine learning, with at least 2 years as a Senior Data Scientist/ML Engineer with the following skills and tools/technologies:\nBachelor s/PG degree in Engineering, Computer Science, Data Science, Statistics or a related field with a focus on analytics skills.\nProven experience leading end-to-end machine learning projects, from conceptualization to deployment.\nStrong knowledge of machine learning algorithms, model evaluation metrics, and best practices for model deployment.\nProficiency in Python used in data science and ML. Familiarity with big data technologies and cloud services.\nWorking knowledge of a variety of machine learning techniques and concepts (regression, time series, classification, Ensemble modeling, Gradient, Boosting, stacking, clustering, decision trees, Neural Networks, image/text processing/NLP, AI, XAI, Transformers/LLM/GenAI and RAG etc.)\nSolid understanding of statistical analysis, data mining, and data visualization techniques.\nGood to have experience with MLOps practices to ensure the smooth operation of models in production.\nHands-on experience with machine learning libraries (e.g., TensorFlow, PyTorch, Scikit-learn, pretrained ML models, Transformers, RAG ), and advanced SQL.\nAbility to tackle complex problems, break them down into actionable steps, and deliver practical solutions.\nExcellent verbal and written communication skills, with the ability to translate complex data insights into clear and actionable recommendations.\nUnderstanding of how to align machine learning and AI solutions with business goals.\nAbility to work cross-functionally with teams across engineering, product, and business domains.\nExperience with cloud platforms, including Google Cloud Platform (GCP) , Azure for machine learning / GenAI.",IT Management,"['Auditing', 'MySQL', 'Data Governance', 'Application Development', 'Data Visualization', 'Python']",2025-06-12 15:28:35
"Asset and Wealth Management, Business Analytics, Associate",Goldman Sachs,1-6 Years,,Bengaluru,"Design, develop and maintain scalable, automated, user-friendly systems, reports, dashboards that will support the firm's analytical and business needs\nTrack and report progress on projects, adhering to the solution delivery lifecycle\nCollaborate with other solution experts and advisors to share ideas and code through showcasing outputs\nSource large sets of data from multiple sources (APIs, Files, Databases), transform and normalize the data; design dimensional data models and load transformed data into relational databases\nResponsible for generation, distribution, and analysis of business performance for periodical reporting to management\nPlay a key role in global initiatives to drive and streamline business critical projects\nBASIC QUALIFICATIONS:\nMaster's degree in any discipline\nUnderstanding of industry standard data transformation and reporting tools such as Tableau, Alteryx, Power BI, and other tools\nExcellent analytical skills: comfortable working with large data sets and presenting findings that tell a clear, insightful, and compelling story\nProficient to advanced skills with MS Office (Excel, PowerPoint, Word, Outlook)\nAbility to organize and prioritize individual workload and deal with multiple priorities\nStrong communication (written and oral) and interpersonal skills\nComfortable with leveraging data and technology to drive business decisions\nPrior understanding of Asset & Wealth Management business is a plus\nSKILLS / EXPERIENCE:\n1+ years of experience in data analytics\nExperience in financial services industry, preferably in an analytical function\nDelivering ETL (Extract, Transform, Load) development of database objects, SQL (Structured Query Language) queries and data analytic capabilities in rational database platforms\nSolid analytical/logical mindset and attention to detail\nTeam-oriented with a strong sense of ownership and accountability\nInquisitive, enthusiastic and a self-starter\nHighly organized with exceptional attention to detail and excellent follow-through\nPositive attitude and strong work ethic",Financial Services,"['Etl', 'Sql', 'Tableau', 'Alteryx', 'Power Bi', 'Data Analytics']",2025-06-12 15:28:37
Senior Software Engineer - ABINITIO,Orange Business Services,4-9 Years,,Gurugram,"In our Ordering team in Sales Technology entity, we are looking for a Data Analyst for an IT application in Ab Initio ETL environment\nThis application interfaces with 25 other applications to send data through 7 data processing chains in the sales, billing and delivery domains\nThe batch processes are scheduled and run during the night For this applicative context, you will participate in the Think, Build and Run phases for data integration flows\nYou will be responsible for those tasks :\nParticipate in study, development, and support phases for data processing solutions with Ab Initio\nAnalyze business requirements and contribute to design customized solutions\nDesign, develop, and optimize ETL workflows using Ab Initio\nTest ETL flows and ensure development quality\nProvide documentation of developed solutions and technical processes\nManage job scheduling with VTOM and optimize execution\nMaintain and optimize existing ETL processes\nManage incident tickets, analyze and resolve technical issues related to data processing\nGive technical and functional support for deployed solutions\nRequired profile:\nHard skills:\nDevelopment skills in Ab Initio (GDE, Express>It, )\nExperience with VTOM for job scheduling and batch process management\nKnowledge in Unix environments and Shell scripting\nGood test practice (unit test, integration test, performance test)\nIncident management skills\nSoft skills:\nExcellent analytical and issue solving skills\nSkills in transversal communication with other teams\nRigor, autonomy, and initiative\nStrong organizational skills and capacity to manage multiple requests simultaneously\nCapacity to work quickly in case of urgent issues",IT Management,"['Data Processing', 'Process Management', 'Unix', 'Shell scripting', 'Ab Initio', 'Incident Management']",2025-06-12 15:28:38
Senior Data Engineer - Big Data & Cloud,Orange Business Services,4-6 Years,,Gurugram,"As part of the Big Data B2B program, OBS set up a shared Big Data platform and a Data Lake for Use cases exploration and industrialization.\nWe are looking for Senior Data Engineer with 4-6 years of experience in building Data Pipelines on Prem and Cloud with below KRA(S):\nAutomate, Industrialize the build and development tasks.\nLead discussion sessions with stakeholders.\nParticipate in all areas of the data Engineering life-cycle and lead the team in requirements gathering and data mapping, systems design, data ingestion development, preparing data mapping documentation, testing and deployment, post implementation support and monitoring.\nResolve and troubleshoot problems and complex issues.\nProvide innovative solutions to complex business problems.\nReport to and work closely with project teams and Business Analysis team on project delivery status.\nPrepare progress update and status report.\nProvide operational support, ongoing maintenance and enhancement after implementation as part of Run Management Activities.\nImplementing Best Data Integration Practices.\nAbout you\nGood understanding of Big Data Ecosystem with frameworks like HADOOP, SPARK.\nGood experience handling large volume data as well as both structured/unstructured data in Streaming and Batch modes.\nHigh Coding proficiency in at least one modern programming language: Python, Java or Scala.\nHands on Experience on NiFi , Hive, SQL/HQL, Spark sql,Spark Steaming, Oozie, Airflow.\nGood Understanding of Data Integration Patterns.\nGood understanding of KAFKA, Rabbit MQ, AIR FLOW.\nGood understanding of API concepts : REST and also microservices architecture\nExperience of Devops tooling: Jenkins, Maven, GitLab, SonarQube, Docker.\nGood Understanding of Devops Concepts and various technologies like Kubernetes, Dockers, Containers.\nGood understanding of ELK stack .\nGood Understanding of Monitoring tools like Prometheus, Grafanna etc.\nGood Understanding of Cloud architecture is must.\nGood to be professionally certified in any of the Hyperscalers especially GCP. Full Understanding of Compute, Network and Storage Services of GCP.\nProficiency in GCP services such as BigQuery, Dataflow, Pub/Sub, Dataproc, and Bigtable.\nGood Understanding of Linux and Shell Scripting\nGood Experience of AGILE methods (Scrum, Kanban)\nGood understanding of JIRA.\nUnderstanding of Data Modelling is value addition.\nUnderstanding of Open Digital Architecture and TMF principles is preferable.\nUnderstanding of Tools Like DSS, Jupyter Notebook is preferable.",IT Management,"['Business Analysis', 'Shell scripting', 'Maven', 'JIRA', 'Sql', 'Python']",2025-06-12 15:28:40
Corporate Treasury-Bengaluru-Associate-Quantitative Engineering,Goldman Sachs,3-8 Years,,Bengaluru,"In this role, you will be provided unique insight into the firm's business activities and asset strategy. You will be responsible for defining, developing models to optimize liquidity, build metric calculators, automated tools to help business get insights into data, predict scenarios and perform better decision making to reduce interest expense for the firm. This front to back model gives the quantitative developer a window into all aspects of CT planning and execution while working on cutting edge industrial technologies.\nJob Duties\nWork as aQuantitative strategistto build, enhance and analyze mathematical models designed to optimize liquidity usage in the firm.\nBuild quantitative tools to attribute, explain and perform scenario analyses on various liquidity metrics.\nWrite model documents and execute model validation process in accordance with firm policy for quantitative models.\nCollaborate with non-engineers to explain model behavior.\nBasic Qualifications\nBachelor's degree\nStrong analytical skills to perform complex functional and technical analyses\nStrong communication skills\nPrior Experience Must Include:\n3+ years inDeveloping mathematical modelsin one of the following: Python, C++ or Java\nMaintaining a production code base and daily production processes.\nPreparing and submitting technical documents to support the validation of mathematical models.\nWorking with techniques of optimization, statistical analysis, including parameter estimation.",Financial Services,"['quantitative modeling', 'Optimization Techniques', 'Python', 'C++', 'Java', 'Statistical Analysis']",2025-06-12 15:28:41
Customer Asset Data Manager,Orange Business Services,10-15 Years,,Gurugram,"The Customer Asset Data Manager (CADM) is accountable and responsible for managing the customer asset and data information on a customer solution through the lifecycle of the customer contract It is a customer-facing role The CADM is the responsible SPOC in front of the customer for all aspects of management of the customer s assets and data The CADM Manager is supported by the CADM Practice\nThe CADM role is a key role in the successful management of Orange s large complex customers\nActivities:\nDuring the Build phase, the CADM Manager is responsible for the following activities:\nEstablish and validate Customer Data Inventory baseline, for all services (hardware and software)\nBridge Orange Business and Customer on data model; Ensure comprehensive documentation that summarizes the key data definitions and data processing, for both customer and Orange\nDefine data Governance model between customer and Orange, covering the lifecycle of the contract\nEnsure an agreed data integrity SLA is established with the customer, covering the build and the run phase\nEnsure the agreed data model is applied across the full scope of the contract at build phase, working with lead project managers of the different project towers\nLead data management for Discovery and Transition; Oversee Discovery actions and be the voice of the customer\nOversee and coordinate Data Inventory upload into Orange Tools, for all services\nEnsure definition of Organic Growth / MACD process, tools, data flows, for all services\nEnsure definition and data model for Customer Catalogue\nEnsure correct data model and effective digital process/tool for Site Survey Data Capture\nEnsure correct data model for the CMDB (reference database for the customer assume SNOW) infrastructure, working with Solution/Technical Consultant\nManage the approach for integration with Vendors or Suppliers or 3 rd parties CMDBs as required, including CI exchange and reconciliation rules\nLead Data Management for the Transformation project phase\nLead data management for the HOTO phase\nDuring the Run phase, the CADM Manager is responsible for the following activities:\nProduce contractual Customer Reporting for Data Quality/Integrity SLA Lead remediation actions\nManage current and historical state of the CIs (Configuration Items) (including decommissioned sites)\nIdentify and record all commercial ordering information, all technical elements including device, vendors and SLAs details, and all software and license elements included in the contract required to ensure operational implementation and support\nManage software Ensure all software and licenses are assigned to the correct account\nIdentify new assets (hardware/software) and Configuration Items to declare, and their categorization (triggered by MACD /Organic growth)\nTrack upcoming contract renewals with vendors and carriers\nEnsure correct Data for Reporting for Billing\nConduct regular audits on CMDB (SNOW) and the complete asset inventory Lead remediation actions\nProduce regular reporting, internal and for customer, on the customer assets and on CADM activities performed for the customer\nIdentify which assets could be optimized (eg an assessment on used/unused WAN/LAN Ports, Application bandwidth consumption, etc)\nAct as the main contact for people (Customer, QO, COCM, PM, CSM and Service Providers) involved in the delivery of the process, for queries and issues within the CADM scope\nManage the relationship and set up good governance with stakeholders (Customer, MSI Desk, GSI TDT and GSAT or Tools owners) to be able to operate the end-to-end Service Asset lifecycle\nBe the Customers SPOC for all aspects of management of the customers assets\n10+ years experience including 5 years in Customer Service Management / Data Management / Asset Management / Project Management\nStrong Data Management Skills advanced Excel, comparison of large data sets, anomaly reports tracking, Versioning management, Scripting skills eg Python, awk, as well as PowerBI skills are a plus\nData Management / Asset Management knowledge and experience\nCustomer Facing skills Customer Service, Customer relationship management\nA good knowledge of Orange Business processes and tools (Gold, Salto, CIS, GINI, Marine, Oceane, FLIP, LOIS, SNOW ecosystem, monitoring tools, Federated Inventory)\nA good knowledge of Orange Business products and services, in particular, Connectivity (LAN, WAN), Security, Cloud\nExperience with working across a multi-supplier environment is a plus\nHe/She will demonstrate a high level of autonomy, is able to take initiatives and to organize transversal working groups with various stakeholders in project mode frameworks\nTransversal Teamwork oriented with strong communication skills in multicultural environments\nMethodical approach with high attention to details\nFluent English reading and writing French a plus\nCertifications : ITIL 4 (IT Asset Management; Service Configuration Management), SIAM (Service Integration And Management)",IT Management,"['Project management', 'Reconciliation', 'Data Processing', 'Customer service', 'Operations', 'Data Management']",2025-06-12 15:28:42
Corporate Treasury-Bengaluru-Associate-Quantitative Engineering,Goldman Sachs,3-8 Years,,Bengaluru,"In this role, you will be provided unique insight into the firm's business activities and asset strategy. You will be responsible for defining, developing models to optimize liquidity, build metric calculators, automated tools to help business get insights into data, predict scenarios and perform better decision making to reduce interest expense for the firm. This front to back model gives the quantitative developer a window into all aspects of CT planning and execution while working on cutting edge industrial technologies.\nJob Duties\nWork as aQuantitative strategistto build, enhance and analyze mathematical models designed to optimize liquidity usage in the firm.\nBuild quantitative tools to attribute, explain and perform scenario analyses on various liquidity metrics.\nWrite model documents and execute model validation process in accordance with firm policy for quantitative models.\nCollaborate with non-engineers to explain model behavior.\nBasic Qualifications\nBachelor's degree\nStrong analytical skills to perform complex functional and technical analyses\nStrong communication skills\nPrior Experience Must Include:\n3+ years inDeveloping mathematical modelsin one of the following: Python, C++ or Java\nMaintaining a production code base and daily production processes.\nPreparing and submitting technical documents to support the validation of mathematical models.\nWorking with techniques of optimization, statistical analysis, including parameter estimation.",Financial Services,"['quantitative modeling', 'Optimization Techniques', 'Python', 'C++', 'Java', 'Statistical Analysis']",2025-06-12 15:28:44
Asset and Wealth Management - Business Unit Leadership - Analyst,Goldman Sachs,0-5 Years,,Bengaluru,"Across Wealth Management, Goldman Sachs helps empower clients and customers around the world to reach their financial goals. Our advisor-led wealth management businesses provide financial planning, investment management, banking and comprehensive advice to a wide range of clients, including ultra- high net worth and high net worth individuals, as well as family offices, foundations and endowments, and corporations and their employees. Our consumer business provides digital solutions for customers to better spend, borrow, invest, and save. Across Wealth Management, our growth is driven by a relentless focus on our people, our clients and customers, and leading-edge technology, data and design.\nJob Responsibilities\nThe Wealth Management Leadership & Regional Management team involvement ranges from internal consulting , strategy, competitor data, expense management , along with management reporting for senior leadership\nBusiness strategy & performance management : Collaborating with Global COOs on initiatives to drive and streamline business critical projects\nLeadership and internal presentations: Responsible for generation, distribution, and analysis of weekly, monthly and annual management reports. Build and deliver analytical insights to the leadership that will help in forming client, sales & product strategies\nAnalyze variances and trends underlying revenues & expenses to provide the business with regular updates through reports supported by well documented commentaries\nBenchmarking against Market competitors, Client Wallet Size Analysis, GS Market share and Ranking\nDevise creative methodologies to improve business efficiency across markets\nDrive resource allocation and headcount management\nBasic Qualifications\nMaster's degree of Finance, Economics, Business or Commerce.\nExperience in financial services industry, preferably in an analytical or strategy function\nStrong communication and interpersonal skills\nAdvanced understanding of Microsoft Office products, tools and utilities for business use, acquaintance to Business Intelligence tools (Tableau, QlikSense, Altreyx) is advantageous\nStrong understanding of wealth management, investment advisory and financial planning products and landscapeanal",Financial Services,"['Strategy', 'performance management', 'Data Analysis', 'Financial Modeling', 'Microsoft Office', 'Business Intelligence Tools']",2025-06-12 15:28:45
Senior Software Engineer - ABINITIO,Orange Business Services,4-9 Years,,Gurugram,"In our Ordering team in Sales Technology entity, we are looking for a Data Analyst for an IT application in Ab Initio ETL environment\nThis application interfaces with 25 other applications to send data through 7 data processing chains in the sales, billing and delivery domains\nThe batch processes are scheduled and run during the night For this applicative context, you will participate in the Think, Build and Run phases for data integration flows\nYou will be responsible for those tasks :\nParticipate in study, development, and support phases for data processing solutions with Ab Initio\nAnalyze business requirements and contribute to design customized solutions\nDesign, develop, and optimize ETL workflows using Ab Initio\nTest ETL flows and ensure development quality\nProvide documentation of developed solutions and technical processes\nManage job scheduling with VTOM and optimize execution\nMaintain and optimize existing ETL processes\nManage incident tickets, analyze and resolve technical issues related to data processing\nGive technical and functional support for deployed solutions\nRequired profile:\nHard skills:\nDevelopment skills in Ab Initio (GDE, Express>It, )\nExperience with VTOM for job scheduling and batch process management\nKnowledge in Unix environments and Shell scripting\nGood test practice (unit test, integration test, performance test)\nIncident management skills\nSoft skills:\nExcellent analytical and issue solving skills\nSkills in transversal communication with other teams\nRigor, autonomy, and initiative\nStrong organizational skills and capacity to manage multiple requests simultaneously\nCapacity to work quickly in case of urgent issues",IT Management,"['Data Processing', 'Process Management', 'Unix', 'Shell scripting', 'Ab Initio', 'Incident Management']",2025-06-12 15:28:56
Risk-Bengaluru-Associate-Model Risk,Goldman Sachs,0-5 Years,,Bengaluru,"This business is ideal for collaborative individuals who have strong ethics and attention to detail.\nWhether assessing the creditworthiness of the firm's counterparties, monitoring market risks\nassociated with trading activities, or offering analytical and regulatory compliance support, our work\ncontributes directly to the firm's success.\nThe MRM group looks for people with strong quantitative and technical backgrounds and a strong\ninterest in financial markets. We seek bright and dynamic individuals with a degree in quantitative\nfields such as math, physics, engineering, computer science, or financial engineering.\nRESPONSIBILITIES\nPerform validation and approval of the firm's models by verifying conceptual soundness,\nmethodology, and implementation, and by identifying limitations and uncertainties\nAssess and quantify model risk by developing alternative benchmark models\nOversee monitoring of ongoing model performance\nCommunicate validation outcomes to key stakeholders and management\nSKILLS AND RELEVANT EXPERIENCE\nExcellent quantitative problem solving skills\nExperience in stochastic modeling, numerical simulation, and data analysis\nMachine learning knowledge\nGood communication skills with the ability to explain complex problems in a simple way\nEagerness and ability to learn new technologies and programming languages\nExcellent organizational skills\nTeam orientation and ability to work in a fast paced environment",Financial Services,"['Quantitative Analysis', 'stochastic modeling', 'Numerical Simulation', 'Data Analysis', 'Machine Learning']",2025-06-12 15:29:00
Risk-Bengaluru-Associate-Model Risk,Goldman Sachs,0-5 Years,,Bengaluru,"This business is ideal for collaborative individuals who have strong ethics and attention to detail.\nWhether assessing the creditworthiness of the firm's counterparties, monitoring market risks\nassociated with trading activities, or offering analytical and regulatory compliance support, our work\ncontributes directly to the firm's success.\nThe MRM group looks for people with strong quantitative and technical backgrounds and a strong\ninterest in financial markets. We seek bright and dynamic individuals with a degree in quantitative\nfields such as math, physics, engineering, computer science, or financial engineering.\nRESPONSIBILITIES\nPerform validation and approval of the firm's models by verifying conceptual soundness,\nmethodology, and implementation, and by identifying limitations and uncertainties\nAssess and quantify model risk by developing alternative benchmark models\nOversee monitoring of ongoing model performance\nCommunicate validation outcomes to key stakeholders and management\nSKILLS AND RELEVANT EXPERIENCE\nExcellent quantitative problem solving skills\nExperience in stochastic modeling, numerical simulation, and data analysis\nMachine learning knowledge\nGood communication skills with the ability to explain complex problems in a simple way\nEagerness and ability to learn new technologies and programming languages\nExcellent organizational skills\nTeam orientation and ability to work in a fast paced environment",Financial Services,"['Quantitative Analysis', 'stochastic modeling', 'Numerical Simulation', 'Data Analysis', 'Machine Learning']",2025-06-12 15:29:02
Hiring For Senior Software Engineer - Data bricks & Power BI,Orange Business Services,4-8 Years,,Noida,"Role & responsibilities\n8*5 development support to customer\nSetup development, test and production environments in on-premise and/or cloud\nAct as a technical liaison between Product Owner, Support and Development team.Responsible for the overall delivery and the solution architecture of the application/ feature/ module team will be working on Guide the teams across solution conceptualization, proof of concept, effort estimation, design, development, implementation, go-live, and support phases\nMaintain coding quality , documentation around development, Release rollout to production, KT, Participating in technology selection and explore the current and emerging technologies and propose changes constantly as needed\nWrite secure, clean, reusable, and well-documented code, Analyzing and documenting requirements and participating in technology selection\nAssessing the system architecture currently in place and working with technical team to recommend solutions to improve it\nKnowledge about basic security fundamentals & protocols is a plus point.\nPreferred candidate profile\nExperience\nMinimum of 4+ years related work experience as Data bricks & Power BI.\nMandatory skill set\nStrong experience on below:\nData ingestion: scripting like powershell, python\nDatabricks & SQL\nPowerBI (DAX )\nNice to have experience on ServiceNow customization\nMust be skillful in Integration/API/Micro services.\nStrong understanding of software design patterns and principles.\nSecondary Skill set\nNice to have experience with CI/CD, Jenkins , Unix/Linux.\nStrong understanding of software design patterns and principles",IT Management,"['Data Bricks', 'Power Bi', 'Sql', 'Python', 'PowerShell', 'Data Lake']",2025-06-12 15:29:03
Power BI/Qliksense Developer,Alstom,1-3 Years,,Bengaluru,"Maintain and enhance several competency development dashboards in PowerBI.Example: Competency and Training dashboard to reflect latest academy structure and link between competencies and allocated trainings, Academy Page Resource Views, Competency Development Analysis Dashboard etc.\nGather business requirements to build ad-hoc reports for business data analysis based on end-user requirements.\nBuild anddevelopdata models, reporting systems, data automation systems, dashboards and performance metrics support that support key business decisions\nCombine complex data sets to provide actionable insights for KPI Improvement and enable faster decision making\nReflect any mtier job structure changes and competency updates in respective dashboards\nMonitor all training quality and participation for global training offer based on existing reports and follow-up on KPIs and other reports as per requests\nSupport any training/certification or transformation development needs in powerapps for example as needed\nRequirements:\nBachelor's/Master's degree in Information Systems, Business, Management, Supply Chain, Industrial Engineering or industry-related curriculum\nBasic experience of developing and using Business Intelligence tools such as Power BI/Power App/Qliksense to develop reporting systems/dashboards integrating various sources of data.\nUnderstanding of different connectors for integration of different data sources\nSolid understanding in Databases and reporting queries.\nComfortable in an international and a multi-cultural environment\nMulti-Tasking in project mode on multiple projects at once\nPower BI/Qliksense programming skills to maintain existing dashboards with special formulas or build new ones on request\nAbility to challenge the status quo\nCritical Thinking / Problem solving\nConflict resolution\nBusiness Acumen\nTeamwork / collaboration\nBe fluent in English",Transportation,"['Business Intelligence', 'KPI analysis', 'Critical Thinking', 'Power Bi', 'Data Modeling', 'Report Automation']",2025-06-12 15:29:04
Data Science,Citiustech Healthcare Technology Private Limited,7-9 Years,,"Bengaluru, Mumbai, Pune","Responsibilities: -\ndesign, development and implementation of NLP projects and solve complex business problems..\nWell versed with Artificial Intelligence, Machine Learning and Deep Learning algorithms and techniques.\nEvaluating and selecting appropriate machine learning models for tasks, as well as building and training working versions of those models using Python and other open-source technologies\nProven experience in developing and deploying NER models.\nExperience with transfer learning and pre-trained language models (e.g., BERT, GPT).\nProficiency in programming languages such as Python, with experience in NLP libraries (e.g., spaCy, NLTK, Stanford NLP, Hugging Face Transformers).\nProficiency in programming languages such as Python, R, and frameworks like TensorFlow or PyTorch, Keras, scikit-learn, Caffe, CNTK\nWorking across client teams to develop and implement LLM solutions. Develop prompts that instruct LLM to generate relevant and accurate responses, RAG Architecture, LLM Fine Tuning\nExpertise in EDA, data engineering, including data curation, cleaning, and preprocessing.\nExcellent problem-solving and analytical skills, with the ability to translate business requirements into technical solutions.\nStrong communication and interpersonal skills, with the ability to collaborate effectively with stakeholders at various levels.\nTrack record of driving innovation and staying updated with the latest AI research and advancements",Health Care,"['Streamlit', 'Gcp', 'Docker', 'Flask', 'Python']",2025-06-12 15:29:05
Data Engineering,Bosch India,6-8 Years,,Bengaluru,"As a Site Reliability Engineer (SRE), you will be responsible for ensuring the reliability, scalability, and performance of the systems necessary for the product and services for the Data Engineering Projects.\nYou will work closely with function developers, Architects and DevOps teams to build and maintain high-availability systems, capable of handling high workloads automate with active monitoring of the infrastructure.\nAs SRE you would ensure system reliability, availability for continuous deployment as part of the Agile practices in solution development.\nMandatory Skills & experience in:\nExperience with cloud platforms specifically Azure.\nHands on experience and proficiency in Cloud infrastructure and CI/CD frameworks for providing IaC -Terraform, ARM, YAML and cloud native containerization & deployment of Services viz. Docker, k8s, etc\nHands-on experience with large scale Azure DevOps and Azure PaaS components.\nMust have tool knowledge Argo, Terraform (CLI), Azure-CLI, KubeCtl, Flux, Helm, Argo (Events and workflows), Istio, Grafana, Kustomize, YAML based coding and debugging skills\nMust have Kubernetes admin skill set, good to have knowledge about tools/extension to Kubernetes\nExperience in understanding of function development of data science solutions & programming languages e.g. Python, Go\nExcellent problem-solving skills and attention to detail.\nHands-on experience with architecting and development of features using u-Service application principles\nDeep understanding of Service Level Objectives (SLOs), Service Level Indicators (SLIs), error budgeting and configuring KPIs for highly sophisticated services.\nExperience with the ELK stack (Elasticsearch, Logstash, Kibana) and Prometheus for monitoring and logging.\nSolid expertise in applying cloud security best practices through DevSecOps principles, with a deep understanding of Kubernetes (k8s) security.\nPreferred Skills & experience in:\nExperience with DevOps, data pipelines and various messaging systems on a Cloud native setup (MS Azure)\nExperience with database technologies (MongoDB, NoSQL, etc.) and cloud native optimization services\nStrong working knowledge in Azure\nMotivating attitude, profound communication, strong interpersonal skills, structured and analytical\nKnowledge of costing, optimization techniques for large scale cloud native services.\nKey Responsibilities:\nSystem Reliability:Design and engineer highly scalable and high availability systems for high throughput workloads.\nContinuous monitoring & active alerting: Develop, deploy, and manage monitoring systems, setting up alerts to proactively identify and resolve issues.\nAutomation: Automate routine tasks such as deployments, monitoring, and policy enforcements using suitable frameworks\nPerformance Tuning: Optimize system performance by identifying bottlenecks and implementing appropriate solutions.\nInfrastructure as Code (IaC): Utilize tools like Terraform, Ansible, or similar to manage infrastructure through code, ensuring consistency and repeatability.\nSecurity: Understand the implement the security policy and enforcements defined by the organization for infrastructure and data\nScaling & Cost Management: Analyze system performance and plan for future scaling needs.\nIssue Handling and resolution:Respond to system outages, perform root cause analysis, and implement fixes to prevent future incidents.\nQualifications\nMaster's degree/ Bachelor Degree in Computer Science or Information Science or equivalent engineering stream.\nAdditional Information\n6-8 Years of hands on experience in maintaining Large scale, High availability Data engineering solutions, services.",Consumer Electronics,"['Azure cloud platform', 'Kubernetes administration', 'Infrastructure as Code', 'Terraform', 'Azure DevOps', 'Python Programming', 'data engineering']",2025-06-12 15:29:07
Lead Data Analytics Engineer,Avalara Technologies,8-13 Years,,Bengaluru,"What Your Responsibilities Will Be\nAvalara is looking for data analytics engineer who can solve and scale real world big data challenges.\nHave end to end analytics experience and a complex data story with data models and reliable and applicable metrics.\nBuild and deploy data science models using complex SQL, Python, DBT data modelling and re-useable visualization components (PowerBI/Tableau/Hex/R-shiny etc.)\nExpert level experience in PowerBI, SQL and Snowflake\nSolve needs on a large scale by applying your software engineering and complex data.\nLead and help develop a roadmap for the area and the team.\nAnalyze fault tolerance and high availability issues, performance, and scale challenges, and solve them.\nLead programs and collaborate with engineers, product managers, and technical program managers across teams.\nUnderstand the trade-offs between consistency, durability, and costs to build solutions that can meet the demands of growing services.\nEnsure the operational readiness of the services and meet the commitments to our customers regarding availability and performance.\nManage end-to-end project plans and ensure on-time delivery.\nCommunicate the status and big picture to the project team and management.\nWork with business and engineering teams to identify scope, constraints, dependencies, and risks.\nIdentify risks and opportunities across the business and guide solutions.\nWhat Youll Need to be Successful\nWhat Youll Need to be Successful\nBachelors Engineering degree in Computer Science or a related field.\n8+ years of experience of enterprise-class experience with large-scale cloud solutions in data science/analytics projects and engineering projects.\nExpert level experience in PowerBI, SQL and Snowflake\nExperience with data visualization, Python, Data Modeling and data storytelling.\nExperience architecting complex data marts applying DBT.\nArchitect and build data solutions that use data quality and anomaly detection best practices.\nExperience building production analytics using the Snowflake data platform.\nExperience in AWS and Snowflake tools and services\nGood to have:\nCertificate in Snowflake is plus\nRelevant certifications in data warehousing or cloud platform.\nExperience architecting complex data marts applying DBT and Airflow.",Information Technology,"['snowflake', 'dbt', 'Powerbi', 'Sql', 'Python', 'AWS']",2025-06-12 15:29:08
Alteryx Developer,BCForward,3-8 Years,,Bengaluru,"Proficient in application development skills for more than one technology as well as multiple design techniques\nWorking proficiency in development toolset to design, develop, test, deploy, maintain and improve software\nStrong understanding of Agile methodologies with ability to work in at least one of the common frameworks\nStrong understanding of techniques such as Continuous Integration, Continuous Delivery, Test\nDriven Development, Cloud Development, application resiliency and security\nProficiency in one or more general purpose programming languages\nWorking proficiency in a portion of software engineering disciplines and demonstrates understanding of overall software skills including business analysis, development, testing, deployment, maintenance and improvement of\nsoftware\nAdditional Skills\nMust Have: Alteryx or UIPath, Jira\nNice to Have: Python, Java, SQL, IDQ, Tableau, API,microservices/cloud/shell scripting/Data base SQL/No SQL.",IT Management,"['Uipath', 'Sql', 'Python', 'Java', 'Idq']",2025-06-12 15:29:09
Alteryx Developer,BCForward,3-8 Years,,Bengaluru,"Proficient in application development skills for more than one technology as well as multiple design techniques\nWorking proficiency in development toolset to design, develop, test, deploy, maintain and improve software\nStrong understanding of Agile methodologies with ability to work in at least one of the common frameworks\nStrong understanding of techniques such as Continuous Integration, Continuous Delivery, Test\nDriven Development, Cloud Development, application resiliency and security\nProficiency in one or more general purpose programming languages\nWorking proficiency in a portion of software engineering disciplines and demonstrates understanding of overall software skills including business analysis, development, testing, deployment, maintenance and improvement of\nsoftware\nAdditional Skills\nMust Have: Alteryx or UIPath, Jira\nNice to Have: Python, Java, SQL, IDQ, Tableau, API,microservices/cloud/shell scripting/Data base SQL/No SQL.",IT Management,"['Uipath', 'Sql', 'Python', 'Java', 'Idq']",2025-06-12 15:29:10
2025_S/4 Data migration consultant_BD/IRA-MDM_Hyderabad,Bosch India,8-10 Years,,Hyderabad,"Experience in Data migration from SAP/Non-SAP to S/4 HANA on-premises with full life cycle implementation\nExpertise of migration SAP BODS and Migration Cockpit, with experience in at least 4 projects\nGood knowledge in ETL designs and build SAP Data services 4.2, S/4 HANA Migration Cockpit and hands on with different migration tools like LTMC, LTMOM\nExperience in BODS Designer Components- Projects, Jobs, Workflow, Data Flow, Scripts, Data Stores and Formats.\nExperience in SAP process areas (Example: O2C, M2S, S2P, B2P, etc.) and different Legacy systems.\nSAP ABAP experience is essential.\nLeading data migration activities including data mapping, authoring migration technical specifications, build migration solutions and designing reconciliation reports.\nUnderstanding the End-to-End Process flows and use of all data objects across those flows and applications (Master Data and Transactional Data)\nWorking collaboratively with functional and business teams to deliver the best data migration solutions and technical designs.\nExhibit effective communication, presentation, and interpersonal skills along with demonstrated experience working with cross-functional teams.\nExcellent written and Oral communication skills\nStrong working experience of data transformations and data migration design and implementation for SAP.\nStrong background in functional analysis and requirements gathering, and solid understanding of SAP.\nRole and Responsibilities\nTechnical migration solutioning/design for S/4 HANA data migration projects/programs\nDesign & build PoC for different S/4 HANA migration scenarios\nDiscussion with subject matter experts on data migration strategic topics\nDevelop data migration programs/strategies for conversion of both Master and Transactional data from different legacy systems into SAP S/4 HANA for all work streams/Modules.\nSupport migration manager to prepare the cut-over plan.\nMust be able to provide legacy to source mapping, define rules for extractionat master and transaction data working with business.\nPerform data mapping of source to target data sets.\nProvide consulting to business to understand their data management issues/gaps and advise on appropriate data management strategies and techniques.\nIdentify and record data conversion requirements, as needed.\nFacilitate and Conduct workshops with business stakeholders, gather requirements, recommend, and present solutions to business stakeholders.\nIdentify data migration extract, transformation, and data load rules.\nCollaborate with functional work teams to achieve business results.\nDisseminate information across the team and keep up to date with functional improvements of solution.\nIdentify, analyze issues, make recommendations, and escalate issues for resolution in a timely manner.\nQualifications\nEducational qualification:\nB.E; B.Tech, or any other degree\nExperience :\n8+\nMandatory/requires Skills :\nBODS, Cockpit\nPreferred Skills :\nSYNITI ADMM",Consumer Electronics,"['S/4 HANA Migration Cockpit', 'data mapping and reconciliation', 'Data Migration', 'Sap Bods', 'Etl Design', 'Sap Abap']",2025-06-12 15:29:11
Lead Data Analytics Engineer,Avalara Technologies,8-10 Years,,"Delhi, Kolkata, Mumbai","with engineers, product managers, and technical program managers across teams.\nUnderstand the trade-offs between consistency, durability, and costs to build solutions that can meet the demands of growing services.\nEnsure the operational readiness of the services and meet the commitments to our customers regarding availability and performance.\nManage end-to-end project plans and ensure on-time delivery.\nCommunicate the status and big picture to the project team and management.\nWork with business and engineering teams to identify scope, constraints, dependencies, and risks.\nIdentify risks and opportunities across the business and guide solutions.\nWhat Youll Need to be Successful\nWhat Youll Need to be Successful\nBachelors Engineering degree in Computer Science or a related field.\n8+ years of experience of enterprise-class experience with large-scale cloud solutions in data science/analytics projects and engineering projects.\nExpert level experience in PowerBI, SQL and Snowflake\nExperience with data visualization, Python, Data Modeling and data storytelling.\nExperience architecting complex data marts applying DBT.\nArchitect and build data solutions that use data quality and anomaly detection best practices.\nExperience building production analytics using the Snowflake data platform.\nExperience in AWS and Snowflake tools and services\nGood to have:\nCertificate in Snowflake is plus\nRelevant certifications in data warehousing or cloud platform.\nExperience architecting complex data marts applying DBT and Airflow.",Information Technology,"['snowflake', 'dbt', 'Powerbi', 'Sql', 'Python', 'AWS']",2025-06-12 15:29:12
Senior Data Scientist,BCForward,5-10 Years,,Bengaluru,"Minimum 5 years of experience indesigning and developing AI/MLmodels and/or various optimization algorithms\nSolid foundation in mathematics, probability, and statistics with demonstrated depth of knowledge and experience in advanced analytics and data science methodologies (e.g. supervised and unsupervised learning, statistics, data science model development)\nProficiency inPythonand working knowledge of cloud AI/ML services (Azure Machine Learning and Databricks preferred) , Gen AI & LLM\nDomain knowledge relevant to the energy sector and working knowledge of Oil and Gas value chain (e.g., upstream, midstream, or downstream) and associated business workflows.\nProven ability to frame data science opportunities, leverage standard foundational tools and Azure services to perform exploratory data analysis (for purposes of data cleaning and discovery), visualize data, and identify actions to reach needed results.\nAbility to quickly assess current state and apply technical concepts across cross-functional business workflows.\nExperience with driving successful execution, deliverables, and accountabilities to meet quality and schedule goals.\nAbility to translate complex data into actionable insights that drive business value.\nDemonstrated ability to engage and establish collaborative relationships both inside and outside immediate workgroup at various organizational levels, across functional and geographic boundaries to achieve desired outcomes.\nDemonstrated ability to adjust behaviour based on feedback and provide feedback to others.\nTeam-oriented mindset with effective communication skills and the ability to work collaboratively.",IT Management,"['AI/ML', 'Gen AI', 'Python 2', 'Databricks', 'Azure']",2025-06-12 15:29:14
Data Scientist,ValueLabs LLP,10-15 Years,,"Indore, Hyderabad","Role & responsibilities\nExperience data acquisition, data mining, data transformation, cleaning structured/unstructured data, modelling, or machine learning, including at least 1 year in Generative AI.\nExperience in Prompt engineering with LLM Development using Langchain. Semantic Kernel, Open Source/ API based LLMs or similar.\nAt least delivered 2 GEN AI based solutions in the past with Open AI, Claude, other LLM\nAWS Lambda service experience exposing services for external consumptions\nExperience in Python and API development\nAt least 2-year experience in Classic ML concepts and design.\nNotice period: Immediate\nPreferred candidate profile:\nExperience data acquisition, data mining, data transformation, cleaning structured/unstructured data, modelling, or machine learning, including at least 1 year in Generative AI.\nExperience in Prompt engineering with LLM Development using Langchain. Semantic Kernel, Open Source/ API based LLMs or similar.\nAt least delivered 2 GEN AI based solutions in the past with Open AI, Claude, other LLM\nAWS Lambda service experience exposing services for external consumptions\nExperience in Python and API development\nAt least 2-year experience in Classic ML concepts and design.\nInterested candidate please share your resume to [HIDDEN TEXT]\nPerks and benefits:\n2 dual appraisals in a year","Consulting, Information Services","['Data Science', 'Machine Learning']",2025-06-12 15:29:15
Lead Data Engineer,Avalara Technologies,4-8 Years,,Pune,"What Your Responsibilities Will Be\nYou will Design, develop, and maintain efficient ETL pipelines using DBT,Airflow to move and transform data from multiple sources into a data warehouse.\nYou will Lead the development and optimization of data models (e.g., star, snowflake schemas) and data structures to support reporting.\nYou will Leverage cloud platforms (e.g., AWS, Azure, Google Cloud) to manage and scale data storage, processing, and transformation processes.\nYou will Work with business teams, marketing, and sales departments to understand data requirements and translate them into actionable insights and efficient data structures.\nYou will Use advanced SQL and Python skills to query, manipulate, and transform data for multiple use cases and reporting needs.\nYou will Implement data quality checks and ensure that the data adheres to governance best practices, maintaining consistency and integrity across datasets.\nYou will Experience using Git for version control and collaborating on data engineering projects.\nWhat Youll Need to be Successful\nBachelors degree with 6+ years of experience in Data Engineering.\nETL/ELT Expertise: experience in building, improving ETL/ELT processes.\nData Modeling: experience with designing and implementing data models such as star and snowflake schemas, and working with denormalized tables to optimize reporting performance.\nExperience with cloud-based data platforms (AWS, Azure, Google Cloud)\nSQL and Python Proficiency: Advanced SQL skills for querying large datasets and Python for automation, data processing, and integration tasks.\nDBT Experience: Hands-on experience with DBT (Data Build Tool) for transforming and managing data models.\nGood to have Skills:\nFamiliarity with AI concepts such as machine learning (ML), (NLP), and generative AI. Work with AI-driven tools and models for data analysis, reporting, and automation.\nOversee and implement DBT models to improve the data transformation process.\nExperience in the marketing and sales domain, with lead management, marketing analytics, and sales data integration.\nFamiliarity with business intelligence reporting tools, Power BI, for building data models and generating insights.",Information Technology,"['dbt', 'Airflow', 'Sql', 'Python', 'AWS', 'Data Modeling']",2025-06-12 15:29:16
Lead Data Engineer,Avalara Technologies,4-8 Years,,Pune,"What Your Responsibilities Will Be\nYou will Design, develop, and maintain efficient ETL pipelines using DBT,Airflow to move and transform data from multiple sources into a data warehouse.\nYou will Lead the development and optimization of data models (e.g., star, snowflake schemas) and data structures to support reporting.\nYou will Leverage cloud platforms (e.g., AWS, Azure, Google Cloud) to manage and scale data storage, processing, and transformation processes.\nYou will Work with business teams, marketing, and sales departments to understand data requirements and translate them into actionable insights and efficient data structures.\nYou will Use advanced SQL and Python skills to query, manipulate, and transform data for multiple use cases and reporting needs.\nYou will Implement data quality checks and ensure that the data adheres to governance best practices, maintaining consistency and integrity across datasets.\nYou will Experience using Git for version control and collaborating on data engineering projects.\nWhat Youll Need to be Successful\nBachelors degree with 6+ years of experience in Data Engineering.\nETL/ELT Expertise: experience in building, improving ETL/ELT processes.\nData Modeling: experience with designing and implementing data models such as star and snowflake schemas, and working with denormalized tables to optimize reporting performance.\nExperience with cloud-based data platforms (AWS, Azure, Google Cloud)\nSQL and Python Proficiency: Advanced SQL skills for querying large datasets and Python for automation, data processing, and integration tasks.\nDBT Experience: Hands-on experience with DBT (Data Build Tool) for transforming and managing data models.\nGood to have Skills:\nFamiliarity with AI concepts such as machine learning (ML), (NLP), and generative AI. Work with AI-driven tools and models for data analysis, reporting, and automation.\nOversee and implement DBT models to improve the data transformation process.\nExperience in the marketing and sales domain, with lead management, marketing analytics, and sales data integration.\nFamiliarity with business intelligence reporting tools, Power BI, for building data models and generating insights.",Information Technology,"['dbt', 'Airflow', 'Sql', 'Python', 'AWS', 'Data Modeling']",2025-06-12 15:29:26
Data Engineer--Operations,Bosch India,3-5 Years,,Bengaluru,"As a Data engineer in Operations, you will work on the operational management, monitoring, and support of scalable data pipelines running in Azure Databricks, Hadoop and Radium. You will ensure the reliability, performance, and availability of data workflows and maintain production environments. You will collaborate closely with data engineers, architects, and platform teams to implement best practices in data pipeline operations and incident management to ensure data availability and data completeness.\nPrimary responsibilities:\nOperational support and incident management for Azure Databricks, Hadoop, Radium data pipelines.\nCollaborating with data engineering and platform teams to define and enforce operational standards, SLAs, and best practices.\nDesigning and implementing monitoring, alerting, and logging solutions for Azure Databricks pipelines.\nCoordinating with central teams to ensure compliance with organizational operational standards and security policies.\nDeveloping and maintaining runbooks, SOPs, and troubleshooting guides for pipeline issues.\nManaging the end-to-end lifecycle of data pipeline incidents, including root cause analysis and remediation.\nOverseeing pipeline deployments, rollbacks, and change management using CI/CD tools such as Azure DevOps.\nEnsuring data quality and validation checks are effectively monitored in production.\nWorking closely with platform and infrastructure teams to address pipeline and environment-related issues.\nProviding technical feedback and mentoring junior operations engineers.\nConducting peer reviews of operational scripts and automation code.\nAutomating manual operational tasks using Scala and Python scripts.\nManaging escalations and coordinating critical production issue resolution.\nParticipating in post-mortem reviews and continuous improvement initiatives for data pipeline operations.\nQualifications\nBachelor's degree in Computer Science, Computer Engineering, or a relevant technical field\n3+ years experience in data engineering, ETL tools, and working with large-scale data sets in Operations.\nProven experience with cloud platforms, particularly Azure Databricks.\nMinimum 3 years of hands-on experience working with distributed cluster environments (e.g., Spark clusters).\nStrong operational experience in managing and supporting data pipelines in production environments.\nAdditional Information\nKey Competencies:\nExperience in Azure Databricks operations or data pipeline support.\nUnderstanding of Scala/ Python programming for troubleshooting in Spark environments.\nHands-on experience with Delta Lake, Azure Data Lake Storage (ADLS), DBFS, Azure Data Factory (ADF).\nSolid understanding of distributed data processing frameworks and streaming data operations.\nUnderstanding and hands-on usage of Kafka as message broker\nExperience with Azure SQL Database and cloud-based data services.\nStrong skills in monitoring tools like Splunk, ELK and Grafana, alerting frameworks, and incident management.\nExperience working with CI/CD pipelines using Azure DevOps or equivalent.\nExcellent problem-solving, investigative, and troubleshooting skills in large-scale data environments.\nExperience defining operational SLAs and implementing proactive monitoring solutions.\nFamiliarity with data governance, security, and compliance best practices in cloud data platforms.\nStrong communication skills and ability to work independently under pressure.\nSoft Skills:\nGood Communication Skills, extensive usage of MS-Teams\nExperience in using Azure board and JIRA\nDecent Level in English as Business Language",Consumer Electronics,"['Scala Python', 'CI/CD Azure DevOps', 'Azure Databricks', 'Hadoop', 'Azure Data Factory', 'Splunk']",2025-06-12 15:29:30
Data Engineer,COTIVITI,4-8 Years,,Hyderabad,"Job description:\nBachelors degree in Computer Science, Engineering, Math, or a related field, or equivalent experience\n4+ years of experience with big data technologies, particularly Hadoop and Spark\nProficiency in programming languages such as Scala or Java\nExperience with Kafka and data streaming platforms\nWorking knowledge of Cloudera, AWS or similar cloud platforms\nExperience building and maintaining microservices\nStrong understanding of data architecture principles\nExperience working in an Agile environment\nProficiency with version control systems like Git/GitLab\nExperience with CI/CD tools like Jenkins\nAbility to work independently and as part of a team\nPreferred Qualifications:\nExperience with Spark/Scala/Akka/Kafka Streams/DataFrame/DataSet\nProficiency with containerization tools like Docker, Kubernetes\nExperience with big data tools like Oozie, Hive or similar\nResponsibilities:\nDevelop and implement Spark based solutions for healthcare applications\nCollaborate with data scientists and other engineers to design and build scalable solutions\nWrite, test, and maintain high-quality code\nParticipate in design and code review sessions\nTroubleshoot and resolve technical issues\nDocument your work and share knowledge with the team\nContribute to continuous improvement initiatives and technology adoption\nWho You Are:\nCurious:You are always looking to deepen your understanding of complex problems.\nCreative:You enjoy coming up with innovative solutions to difficult challenges.\nPractical:You focus on delivering solutions that have real-world applications and value.\nFocused:You maintain a clear vision of your goals and work diligently to achieve them.\nDetermined:You are committed to contributing to the development of advanced machine learning capabilities","IT Management, Information Services","['Hadoop', 'Spark', 'Scala', 'Kafka', 'AWS', 'Microservices', 'data engineering']",2025-06-12 15:29:32
Senior Associate - Data Ops Estimations (Data Business CoE),Bain Company India Pvt. Ltd.,4-6 Years,,Delhi,"Primary responsibilities will include working closely with the Estimations, Data Operations, and Engineering teams.\nThis will include helping and assist our clients on large datasets; focused on data projections, estimations, and validation.\nWork with Estimations & Methods and Data Operations teams to ensure data delivered to clients is of high quality and ready for analysis\nLead monthly data refresh validations, leveraging automation tools and working closely with the Data Operations team to refine process over time\nAssist with research of secondary information to validate data trends\nAssist in implementation of data science methods such as forecasting, regression, anomaly detection, clustering, and other machine learning techniques as part of estimations toolkit where appropriate\nHelp team in improving quality of core data assets as we'll as enhance analytic capabilities of delivered data\nResearch, suggest and implement best practices for management of market measurement data, projections, and analytics from Pyxis vast array of alternative data sources and supplemental datasets\nContribute to internal and external applications by developing analytical services that enable scalable data processes and democratize data techniques across team members\nPrepare various sources of data using data wrangling methods in Python, R and SQL, leveraging infrastructure including Cloud computing solutions and relational database environments\nAssist with creation and documentation of standard operating procedures for repeated data processes, as we'll as knowledge base of data methods\nAbout you\n4-6 years of experience in data mining, statistical modelling, and data analysis, preferably in ecommerce industry, with at least bachelors in mathematics, engineering, science, statistics or technical degree\nExpertise working with large data sets and proficiency in SQL, Excel\nExperience in Python\nExperience in statistical analysis and A/B testing, predictive modeling, computational systems, and optimization techniques\nExcellent verbal, written, and data visualization skills\nIndependently manage stakeholders with frequent communication, expectation management, meeting deadlines and the backlog\nDemonstrated ability to manage projects and work with different functions (product, strategy, engineering, etc)\nExperience working with financial statements and basic knowledge of accounting and finance concepts is a plus\nExperience with Git and modern software development workflow is a plus\nAgile way of working and tools (Jira, Confluence, Miro)",Management Consulting,"['Statistical Modeling', 'Data Operations', 'Python', 'Sql', 'Excel', 'Data Visualization', 'Cloud Computing']",2025-06-12 15:29:34
Lead Data Analytics Engineer,Avalara Technologies,8-13 Years,,Bengaluru,"What Your Responsibilities Will Be\nAvalara is looking for data analytics engineer who can solve and scale real world big data challenges.\nHave end to end analytics experience and a complex data story with data models and reliable and applicable metrics.\nBuild and deploy data science models using complex SQL, Python, DBT data modelling and re-useable visualization components (PowerBI/Tableau/Hex/R-shiny etc.)\nExpert level experience in PowerBI, SQL and Snowflake\nSolve needs on a large scale by applying your software engineering and complex data.\nLead and help develop a roadmap for the area and the team.\nAnalyze fault tolerance and high availability issues, performance, and scale challenges, and solve them.\nLead programs and collaborate with engineers, product managers, and technical program managers across teams.\nUnderstand the trade-offs between consistency, durability, and costs to build solutions that can meet the demands of growing services.\nEnsure the operational readiness of the services and meet the commitments to our customers regarding availability and performance.\nManage end-to-end project plans and ensure on-time delivery.\nCommunicate the status and big picture to the project team and management.\nWork with business and engineering teams to identify scope, constraints, dependencies, and risks.\nIdentify risks and opportunities across the business and guide solutions.\nWhat Youll Need to be Successful\nWhat Youll Need to be Successful\nBachelors Engineering degree in Computer Science or a related field.\n8+ years of experience of enterprise-class experience with large-scale cloud solutions in data science/analytics projects and engineering projects.\nExpert level experience in PowerBI, SQL and Snowflake\nExperience with data visualization, Python, Data Modeling and data storytelling.\nExperience architecting complex data marts applying DBT.\nArchitect and build data solutions that use data quality and anomaly detection best practices.\nExperience building production analytics using the Snowflake data platform.\nExperience in AWS and Snowflake tools and services\nGood to have:\nCertificate in Snowflake is plus\nRelevant certifications in data warehousing or cloud platform.\nExperience architecting complex data marts applying DBT and Airflow.",Information Technology,"['snowflake', 'dbt', 'Powerbi', 'Sql', 'Python', 'AWS']",2025-06-12 15:29:36
2025_S/4 Data migration consultant_BD/IRA-MDM_Hyderabad,Bosch India,8-10 Years,,Hyderabad,"Experience in Data migration from SAP/Non-SAP to S/4 HANA on-premises with full life cycle implementation\nExpertise of migration SAP BODS and Migration Cockpit, with experience in at least 4 projects\nGood knowledge in ETL designs and build SAP Data services 4.2, S/4 HANA Migration Cockpit and hands on with different migration tools like LTMC, LTMOM\nExperience in BODS Designer Components- Projects, Jobs, Workflow, Data Flow, Scripts, Data Stores and Formats.\nExperience in SAP process areas (Example: O2C, M2S, S2P, B2P, etc.) and different Legacy systems.\nSAP ABAP experience is essential.\nLeading data migration activities including data mapping, authoring migration technical specifications, build migration solutions and designing reconciliation reports.\nUnderstanding the End-to-End Process flows and use of all data objects across those flows and applications (Master Data and Transactional Data)\nWorking collaboratively with functional and business teams to deliver the best data migration solutions and technical designs.\nExhibit effective communication, presentation, and interpersonal skills along with demonstrated experience working with cross-functional teams.\nExcellent written and Oral communication skills\nStrong working experience of data transformations and data migration design and implementation for SAP.\nStrong background in functional analysis and requirements gathering, and solid understanding of SAP.\nRole and Responsibilities\nTechnical migration solutioning/design for S/4 HANA data migration projects/programs\nDesign & build PoC for different S/4 HANA migration scenarios\nDiscussion with subject matter experts on data migration strategic topics\nDevelop data migration programs/strategies for conversion of both Master and Transactional data from different legacy systems into SAP S/4 HANA for all work streams/Modules.\nSupport migration manager to prepare the cut-over plan.\nMust be able to provide legacy to source mapping, define rules for extractionat master and transaction data working with business.\nPerform data mapping of source to target data sets.\nProvide consulting to business to understand their data management issues/gaps and advise on appropriate data management strategies and techniques.\nIdentify and record data conversion requirements, as needed.\nFacilitate and Conduct workshops with business stakeholders, gather requirements, recommend, and present solutions to business stakeholders.\nIdentify data migration extract, transformation, and data load rules.\nCollaborate with functional work teams to achieve business results.\nDisseminate information across the team and keep up to date with functional improvements of solution.\nIdentify, analyze issues, make recommendations, and escalate issues for resolution in a timely manner.\nQualifications\nEducational qualification:\nB.E; B.Tech, or any other degree\nExperience :\n8+\nMandatory/requires Skills :\nBODS, Cockpit\nPreferred Skills :\nSYNITI ADMM",Consumer Electronics,"['S/4 HANA Migration Cockpit', 'data mapping and reconciliation', 'Data Migration', 'Sap Bods', 'Etl Design', 'Sap Abap']",2025-06-12 15:29:37
Data Scientist,ValueLabs LLP,10-15 Years,,"Indore, Hyderabad","Role & responsibilities\nExperience data acquisition, data mining, data transformation, cleaning structured/unstructured data, modelling, or machine learning, including at least 1 year in Generative AI.\nExperience in Prompt engineering with LLM Development using Langchain. Semantic Kernel, Open Source/ API based LLMs or similar.\nAt least delivered 2 GEN AI based solutions in the past with Open AI, Claude, other LLM\nAWS Lambda service experience exposing services for external consumptions\nExperience in Python and API development\nAt least 2-year experience in Classic ML concepts and design.\nNotice period: Immediate\nPreferred candidate profile:\nExperience data acquisition, data mining, data transformation, cleaning structured/unstructured data, modelling, or machine learning, including at least 1 year in Generative AI.\nExperience in Prompt engineering with LLM Development using Langchain. Semantic Kernel, Open Source/ API based LLMs or similar.\nAt least delivered 2 GEN AI based solutions in the past with Open AI, Claude, other LLM\nAWS Lambda service experience exposing services for external consumptions\nExperience in Python and API development\nAt least 2-year experience in Classic ML concepts and design.\nInterested candidate please share your resume to [HIDDEN TEXT]\nPerks and benefits:\n2 dual appraisals in a year","Consulting, Information Services","['Data Science', 'Machine Learning']",2025-06-12 15:29:38
Environment Data Specialist II,AECOM Singapore Pte Ltd,2-5 Years,,Bengaluru,"Role and Responsibilities:\nThe ideal candidate will be able to understand requests from environmental subject matter experts.\nBe a good communicator able to share new functions and features with the users and have a good understanding of environmental data and environmental data terminology.\nWorks on issues of diverse scope where analysis of situation or data requires evaluation of a variety of factors, including an understanding of current business trends.\nPrepare and update environmental associated reports sound in understanding environmental data, transforming, and analyzing large and diversified environmental datasets.\nAbility to translate environmental problems through digital and data solutions. Commitment to data quality at all levels and scales.\nExperience in developing custom reports and user-requested queries and views on various platforms of the desired skill set.\nResponsive to client (user) requests. Excellent communication skills\nProvide technical support to field sampling teams and act as a liaison between the project staff, analytical laboratory, data validator, and GIS analysts.\nResearch state and federal regulations necessary to manage action levels or clean-up criteria.\nProfessional qualification & Experience desired\nBachelor's degree in environmental/civil/chemical engineering or science in a related discipline (or similar subject) desirable with a required focus on Environmental Data and 2+ years of experience working in the environmental domain and preferably have relevant experience with environmental data.\nSkills Required:\nAbility to understand data management using excellent computer skills to perform transformations in spreadsheets and databases.\nExpertise and experience with environmental data and database systems (MS SQL Server, MS Access). Expertise with relational databases such as EarthSoft's Environmental Quality Information System (EQuIS) /EIM/ ESdat. Ability to continually analyze data at all stages for problems, logic, and consistency concerning field data collection, analytical reporting, and other expertise on EQUIS sub-tools (Collect, Edge, ArcGIS highly desirable but not essential).\nAssist projects globally and task delivery with high quality and within deadlines. Managing data (geological, Field data, chemical laboratory data) for technical report writing and interpretation as required by the team. Maintaining and updating various project dashboards using the web-based EQuIS Enterprise system; and preparing report-ready data tables, charts, and figures for internal review and external client reports.\nUse of visualization tools like Power BI to help management make effective decisions for the environmental domain is desirable but not essential.\nProgramming and/or coding experience (e.g., Python,R) a plus.\nData engineering, AI/ML, and Data science understanding is highly desirable but not essential. Can be in either academic or work experience.\nIntermediate to the expert level understanding of Office 365, Excel, power query & Power automation.\nStrong attention to detail with excellent analytical, judgment and problem-solving capabilities.\nComfortable running meetings and presentations\nStrong written and oral communication skills",Consulting,"['EQuIS', 'DataTransformation', 'EnvironmentalData', 'Msaccess', 'Powerbi', 'Python']",2025-06-12 15:29:39
Lead Data Analytics Engineer,Avalara Technologies,8-13 Years,,"Hyderabad, Chennai, Pune","What Your Responsibilities Will Be\nAvalara is looking for data analytics engineer who can solve and scale real world big data challenges.\nHave end to end analytics experience and a complex data story with data models and reliable and applicable metrics.\nBuild and deploy data science models using complex SQL, Python, DBT data modelling and re-useable visualization components (PowerBI/Tableau/Hex/R-shiny etc.)\nExpert level experience in PowerBI, SQL and Snowflake\nSolve needs on a large scale by applying your software engineering and complex data.\nLead and help develop a roadmap for the area and the team.\nAnalyze fault tolerance and high availability issues, performance, and scale challenges, and solve them.\nLead programs and collaborate with engineers, product managers, and technical program managers across teams.\nUnderstand the trade-offs between consistency, durability, and costs to build solutions that can meet the demands of growing services.\nEnsure the operational readiness of the services and meet the commitments to our customers regarding availability and performance.\nManage end-to-end project plans and ensure on-time delivery.\nCommunicate the status and big picture to the project team and management.\nWork with business and engineering teams to identify scope, constraints, dependencies, and risks.\nIdentify risks and opportunities across the business and guide solutions.\nWhat Youll Need to be Successful\nWhat Youll Need to be Successful\nBachelors Engineering degree in Computer Science or a related field.\n8+ years of experience of enterprise-class experience with large-scale cloud solutions in data science/analytics projects and engineering projects.\nExpert level experience in PowerBI, SQL and Snowflake\nExperience with data visualization, Python, Data Modeling and data storytelling.\nExperience architecting complex data marts applying DBT.\nArchitect and build data solutions that use data quality and anomaly detection best practices.\nExperience building production analytics using the Snowflake data platform.\nExperience in AWS and Snowflake tools and services\nGood to have:\nCertificate in Snowflake is plus\nRelevant certifications in data warehousing or cloud platform.\nExperience architecting complex data marts applying DBT and Airflow.",Information Technology,"['snowflake', 'dbt', 'Powerbi', 'Sql', 'Python', 'AWS']",2025-06-12 15:29:40
Lead Data Analytics Engineer,Avalara Technologies,8-13 Years,,"Hyderabad, Chennai, Pune","What Your Responsibilities Will Be\nAvalara is looking for data analytics engineer who can solve and scale real world big data challenges.\nHave end to end analytics experience and a complex data story with data models and reliable and applicable metrics.\nBuild and deploy data science models using complex SQL, Python, DBT data modelling and re-useable visualization components (PowerBI/Tableau/Hex/R-shiny etc.)\nExpert level experience in PowerBI, SQL and Snowflake\nSolve needs on a large scale by applying your software engineering and complex data.\nLead and help develop a roadmap for the area and the team.\nAnalyze fault tolerance and high availability issues, performance, and scale challenges, and solve them.\nLead programs and collaborate with engineers, product managers, and technical program managers across teams.\nUnderstand the trade-offs between consistency, durability, and costs to build solutions that can meet the demands of growing services.\nEnsure the operational readiness of the services and meet the commitments to our customers regarding availability and performance.\nManage end-to-end project plans and ensure on-time delivery.\nCommunicate the status and big picture to the project team and management.\nWork with business and engineering teams to identify scope, constraints, dependencies, and risks.\nIdentify risks and opportunities across the business and guide solutions.\nWhat Youll Need to be Successful\nWhat Youll Need to be Successful\nBachelors Engineering degree in Computer Science or a related field.\n8+ years of experience of enterprise-class experience with large-scale cloud solutions in data science/analytics projects and engineering projects.\nExpert level experience in PowerBI, SQL and Snowflake\nExperience with data visualization, Python, Data Modeling and data storytelling.\nExperience architecting complex data marts applying DBT.\nArchitect and build data solutions that use data quality and anomaly detection best practices.\nExperience building production analytics using the Snowflake data platform.\nExperience in AWS and Snowflake tools and services\nGood to have:\nCertificate in Snowflake is plus\nRelevant certifications in data warehousing or cloud platform.\nExperience architecting complex data marts applying DBT and Airflow.",Information Technology,"['snowflake', 'dbt', 'Powerbi', 'Sql', 'Python', 'AWS']",2025-06-12 15:29:41
KYC Analyst,PhonePe,2-5 Years,,Bengaluru,"Key deliverables:\nDefine and monitor critical metrics aligned with pod strategy to drive business impact\nTranslate business problems into analytical problem statements and generate actionable insights\nOwn insight generation and present data-driven recommendations to stakeholders\nMentor analysts, support hiring and onboarding, and act as analytics and visualization SME\nRole responsibilities:\nCollaborate with business and product teams to understand requirements and shape analytics solutions\nDesign and build dashboards for easy consumption of critical data by pods\nApply strong problem-solving skills to analyze large datasets and uncover business insights\nCommunicate findings clearly through storytelling, reports, and presentations to influence decision-making",Mobile Payments,"['Python', 'Sql', 'Power Bi', 'Tableau', 'Qlikview']",2025-06-12 15:29:43
INT4-Polaris program Professional,BCForward,5-10 Years,,Bengaluru,"Contract To Hire(C2H) Role\nHybrid Mode\nAct as an advisor to explain how INT4 can be used in the Polaris program.\nSet up integrations in INT4 for service virtualization.\nEducate teams on how to use INT4.\nMaintain and troubleshoot issues with the tool.\nCoordinate with Tech Integration and D&B teams during the tool setup and required integrations.\nINT4-Polaris program, integrations in INT4 for service virtualization ,Maintain and troubleshoot issues, Coordinate with Tech Integration and D&B teams,INT4\nYears of experience: 5-8 yrs\nMode of interview(Virtual/ In-person)-Virtual\nWork timing11 PM\nWork Mode(Remote/ On-site/ Hybrid)\nHybrid 2 days in office\nNote: Looking for Immediate to 30-Days joiners at most.",IT Management,"['INT4', 'POLARIS', 'D&B', 'Programm']",2025-06-12 15:29:44
SAP Fiori Portal Senior Product Analyst,BCForward,5-10 Years,,Hyderabad,"Analyze business requirements and translate them into Fiori application specifications Configure and support SAP Fiori Launchpad and Portal roles Collaborate with SAP developers to deliver UX enhancements and performance improvements\nTroubleshoot and resolve issues with SAP UI5/Fiori apps Work with SAP Gateway, OData Services, and back-end integration Coordinate with cross-functional teams for project delivery Document solutions and provide end-user training as needed\nNP: Immediate",IT Management,"['SAP', 'Ux', 'UI', 'Odata', 'Fiori']",2025-06-12 15:29:45
Scientific Business Analyst (Specialist) - R&D Omics,Amgen Technology Private Limited,1-5 Years,,Hyderabad,"Role Responsibilities:\nLiaise with research teams to translate scientific needs into actionable requirements\nDefine user stories, process maps, and test cases within SAFe Agile frameworks\nSupport scientific software platforms and ensure data readiness for AI/ML applications\nManage product roadmaps, risk registers, and vendor/software lifecycle across informatics platforms\nKey Deliverables:\nUser requirement documents, acceptance criteria, and validation artifacts\nFunctional and scalable omics data solutions for oncology and drug discovery\nIntegrated dashboards, reports, and system documentation\nResearch-ready platforms aligned with compliance and cybersecurity standards",Pharmaceutical,"['Omics data', 'Bioinformatics', 'Agile', 'AWS', 'Machine Learning']",2025-06-12 15:29:46
MDM Data Engineer,Amgen Technology Private Limited,2-6 Years,,Hyderabad,"Experience Required:\n25 years in Data Engineering with knowledge of MDM platforms\nOverview:\nWe are seeking an MDM Associate Data Engineer to support and enhance our enterprise Master Data Management (MDM) platforms using Informatica and/or Reltio. This role is crucial for delivering high-quality master data solutions across the organization by leveraging modern tools such as Databricks and AWS.\nKey Responsibilities:\nAnalyze and manage customer master data using Reltio or Informatica MDM\nPerform advanced SQL queries and data analysis to ensure master data integrity\nUtilize Python, PySpark, and Databricks for scalable data processing and automation\nCollaborate with business and engineering teams to enhance MDM solutions\nImplement data stewardship workflows including approval and DCR mechanisms\nUse AWS cloud services for MDM-related storage and compute processes\nContribute to metadata management and data modeling activities\nTrack and manage data issues using tools like JIRA and document processes in Confluence\nApply Life Sciences/Pharma industry context to ensure data compliance and standards\nQualifications:\nEducation:\nMaster's degree with 13 years of experience\nOR Bachelor's degree with 25 years of experience\nOR Diploma with 68 years of experience\nFields: Business, Engineering, IT, or related disciplines\nMust-Have Skills:\nStrong proficiency in Advanced SQL and data wrangling\nHands-on experience with:\nPython\nPySpark\nDatabricks\nAWS architecture\nKnowledge of MDM concepts, data governance, stewardship, and profiling\nExperience with Informatica MDM or Reltio MDM\nGood-to-Have Skills:\nExperience with Informatica Data Quality (IDQ), data modeling, and approval workflows\nBackground in the Life Sciences / Pharma domain\nFamiliarity with tools like JIRA and Confluence\nSolid grasp of data engineering concepts\nCertifications (Preferred):\nETL certifications (e.g., Informatica)\nData analysis certifications (SQL, Python, Databricks)\nCloud certifications (AWS, Azure)\nSoft Skills:\nStrong analytical and problem-solving abilities\nExcellent verbal and written communication skills\nAbility to explain complex data concepts to both technical and non-technical stakeholders\nTeam player with experience working in global, virtual teams",Biotechnology,"['Data Analysis', 'Etl', 'Informatica', 'Sql', 'Python', 'Cloud', 'Aws', 'data wrangling']",2025-06-12 15:29:48
Scientific Business Analyst (Specialist) - R&D Omics,Amgen Technology Private Limited,1-5 Years,,Hyderabad,"Role Responsibilities:\nTranslate scientific and technical needs into clear agile user requirements\nCollaborate with global teams and research scientists to prioritize needs\nManage and support scientific software platforms and infrastructure\nDevelop documentation, training plans, and ensure compliance and cybersecurity\nKey Deliverables:\nWell-defined user stories and acceptance criteria in agile systems\nEfficient operation and technical support of scientific platforms\nUp-to-date training and communication materials for end users\nContinuous improvement aligned with industry trends and SAFe practices",Pharmaceutical,"['Business Analysis', 'Omics', 'Lims', 'Agile', 'AWS']",2025-06-12 15:29:58
"Manager, Predictive & Optimization Analyst",SCHNEIDER ELECTRIC SINGAPORE PTE LTD,3-7 Years,,Bengaluru,"As a Remote Support Engineer, you'll play a crucial role in providing proactive and reactive support for electrical distribution systems, leveraging data analytics to ensure optimal asset performance and customer satisfaction.\nResponsibilities\nPossess a strong understanding of Electrical distribution with the ability to perform predictive / condition-based analyses of data generated by EAA Predictive into meaningful insights and actions.\nExtracts and analyzes data, runs analytics (software/expert-based).\nObserves health of assets, including condition-based analytics, and adjusts maintenance plans accordingly.\nDevelops recommendations based on the data, creates reports, shares reports with customers, and provides direction to the customer as to actions required.\nInteracts with customers and provides remote troubleshooting (e.g., phone, chat, email, etc.) as needed.\nActs based on customer requests to create cases, initiates work order creation to dispatch Field Services and/or engages Sales for service or product quotations.\nInteracts with Network of experts for advanced support of data analysis and recommendations to the customer.\nQualifications\nThe Remote Support Engineer must have a proven track record of:\nProviding excellent customer service and must possess a customer-focused mindset.\nEffective communication (written/oral/reading/listening), with a command of the English language.\nAbility to keep relevant stakeholders informed and build a network of relationships that would aid in business development.\nAbility to influence and convince.\nAbility to effectively manage time and remain organized across multiple activities along with an attention to detail.\nCuriosity and interest in new knowledge in various domains.\nMinimum Knowledge & Skill Expectations\nService experience with power/power monitoring equipment and industrial applications.\nKnowledge of electrical distribution systems and proficiency with one-line diagrams.\nStrong Technical writing and computer knowledge, fluency with Microsoft Office tools.\nSuperior analytical abilities and high degree of proficiency in working with data",Electronics,"['Electrical Distribution Systems', 'Data Analysis', 'remote troubleshooting', 'Case Management', 'Predictive Analysis', 'Data Extraction']",2025-06-12 15:30:03
"Manager, Predictive & Optimization Analyst",SCHNEIDER ELECTRIC SINGAPORE PTE LTD,3-7 Years,,Bengaluru,"As a Remote Support Engineer, you'll play a crucial role in providing proactive and reactive support for electrical distribution systems, leveraging data analytics to ensure optimal asset performance and customer satisfaction.\nResponsibilities\nPossess a strong understanding of Electrical distribution with the ability to perform predictive / condition-based analyses of data generated by EAA Predictive into meaningful insights and actions.\nExtracts and analyzes data, runs analytics (software/expert-based).\nObserves health of assets, including condition-based analytics, and adjusts maintenance plans accordingly.\nDevelops recommendations based on the data, creates reports, shares reports with customers, and provides direction to the customer as to actions required.\nInteracts with customers and provides remote troubleshooting (e.g., phone, chat, email, etc.) as needed.\nActs based on customer requests to create cases, initiates work order creation to dispatch Field Services and/or engages Sales for service or product quotations.\nInteracts with Network of experts for advanced support of data analysis and recommendations to the customer.\nQualifications\nThe Remote Support Engineer must have a proven track record of:\nProviding excellent customer service and must possess a customer-focused mindset.\nEffective communication (written/oral/reading/listening), with a command of the English language.\nAbility to keep relevant stakeholders informed and build a network of relationships that would aid in business development.\nAbility to influence and convince.\nAbility to effectively manage time and remain organized across multiple activities along with an attention to detail.\nCuriosity and interest in new knowledge in various domains.\nMinimum Knowledge & Skill Expectations\nService experience with power/power monitoring equipment and industrial applications.\nKnowledge of electrical distribution systems and proficiency with one-line diagrams.\nStrong Technical writing and computer knowledge, fluency with Microsoft Office tools.\nSuperior analytical abilities and high degree of proficiency in working with data",Electronics,"['Electrical Distribution Systems', 'Data Analysis', 'remote troubleshooting', 'Case Management', 'Predictive Analysis', 'Data Extraction']",2025-06-12 15:30:04
Data Analytics Manager Finance,Amgen Technology Private Limited,6-9 Years,,Hyderabad,"What will you do\nIn this vital role as a Data Analytics Manager you will play a meaningful role in fully understanding financial data and associated systems architecture in order to design data integrations for reporting and analysis to support the global Amgen organization. Key responsibilities include but are not limited to:\nDeveloping a robust understanding of Amgen's financial data and systems in order to support data requests and integrations for different initiatives\nWorking in close partnership with clients or team members to design, develop and augment the financial datasets\nProviding client-facing project management support and completing hands-on Databricks/Prophecy development as well as Power BI or Tableau as time and priorities allow\nDesigning and developing the underlying ETL data processes used to build various financial datasets used for reporting and/or dashboards\nIdentifying data enhancements or process improvements to optimize the financial datasets and processes\nUnderstanding the regularly scheduled financial datasets in Databricks and Tableau or Power BI dashboards refresh processes on an ongoing basis\nInvolvement in the Financial Data Product Team, as a finance data subject matter expert.\nKey elements to success in this role include understanding Amgen's financial systems and data, ability to define business requirements, and understanding how to design datasets compatible with Power BI, Tableau or other analytic tool reporting requirements.\nWhat we expect of you\nWe are all different, yet we all use our outstanding contributions to serve patients. The Data Analytics Manager professional we seek is a go-getter with these qualifications.\nBasic Qualifications\nDoctorate degree\nOr\nMaster's degree and 2 years of Finance experience\nOr\nBachelor's degree and 4 years of Finance experience\nOr\nAssociate's degree and 10 years of Finance experience\nOr\nHigh school diploma / GED and 12 years of Finance experience\nPreferred Qualifications\nExperience performing data analysis across one or more areas of the business to derive business logic for data integration\nExperience working with business partners to identify complex functionality and translate it into requirements\nExperience with financial statements and Amgen Finance experience preferred\nExperience with data analysis, data modeling, and data visualization solutions such as Power BI, Tableau, Databricks, and Alteryx\nFamiliar with Hyperion Planning, SAP, scripting languages like SQL or Python, Databricks Prophecy, and AWS services like S3\nAble to work in matrixed teams, across geographic and functional reporting lines\nExcellent analytical and problem-solving skills\nExcellent facilitation, influencing, and negotiation skills\nProficient in MS Office Suite",Biotechnology,"['Finance', 'Financial Analysis', 'Data Analytics', 'Databricks', 'Etl', 'Tableau', 'Sql', 'Sap', 'Aws']",2025-06-12 15:30:05
Senior Data Engineer,Amgen Technology Private Limited,3-7 Years,,Hyderabad,"Role Responsibilities:\nDesign, build, and maintain scalable data pipelines and ETL processes\nDevelop and ensure data quality across large datasets\nCollaborate with cross-functional teams to meet business data needs\nImplement data security and governance standards\nUse cloud platforms (AWS) to build and optimize data solutions\nKey Deliverables:\nRobust ETL pipelines and data integration solutions\nAccurate data models and documentation\nData governance and security compliance\nEfficient data processing on big data frameworks\nTimely sprint planning and project delivery",Pharmaceutical,"['Python', 'Pyspark', 'Sql', 'Databricks', 'Amazon Redshift']",2025-06-12 15:30:07
Principal Data Engineer,Amgen Technology Private Limited,8-10 Years,,Hyderabad,"What you will do\nWe are seeking a seasoned Principal Data Engineer to lead the design, development, and implementation of our data strategy. The ideal candidate possesses a deep understanding of data engineering principles, coupled with strong leadership and problem-solving skills. As a Principal Data Engineer, you will architect and oversee the development of robust data platforms, while mentoring and guiding a team of data engineers.\nRoles & Responsibilities:\nPossesses strong rapid prototyping skills and can quickly translate concepts into working code.\nProvide expert guidance and mentorship to the data engineering team, fostering a culture of innovation and standard methodologies.\nDesign, develop, and implement robust data architectures and platforms to support business objectives.\nOversee the development and optimization of data pipelines, and data integration solutions.\nEstablish and maintain data governance policies and standards to ensure data quality, security, and compliance.\nArchitect and manage cloud-based data solutions, using AWS or other preferred platforms.\nLead and motivate an impactful data engineering team to deliver exceptional results.\nIdentify, analyze, and resolve complex data-related challenges.\nCollaborate closely with business collaborators to understand data requirements and translate them into technical solutions.\nStay abreast of emerging data technologies and explore opportunities for innovation.\nBasic Qualifications:\nMasters degree and 8 to 10 years of computer science and engineering preferred, other Engineering field is considered OR\nBachelors degree and 10 to 14 years of computer science and engineering preferred, other Engineering field is considered;\nDiploma and 14 to 18 years of in computer science and engineering preferred, other Engineering field is considered\nDemonstrated proficiency in using cloud platforms (AWS, Azure, GCP) for data engineering solutions. Strong understanding of cloud architecture principles and cost optimization strategies.\nProficient on experience in Python, PySpark, SQL. Handon experience with bid data ETL performance tuning.\nProven ability to lead and develop impactful data engineering teams.\nStrong problem-solving, analytical, and critical thinking skills to address complex data challenges.\nPreferred Qualifications:\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with Apache Spark, Apache Airflow\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nExperienced with AWS, GCP or Azure cloud services\nProfessional Certifications\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.",Biotechnology,"['data engineering', 'Azure', 'Aws', 'Databricks', 'Devops', 'Data Modeling', 'Apache Spark', 'Python', 'Sql', 'Big Data']",2025-06-12 15:30:08
Associate Data Analytics,Amgen Technology Private Limited,1-3 Years,,Hyderabad,"What you will do\nLet's do this. Let's change the world. We are seeking a detail-oriented and proactive Data Steward to support our pharmaceutical data governance initiatives. The ideal candidate will ensure the quality, consistency, integrity, and security of data across our systems, particularly within commercial, regulatory, R&D, and supply chain functions. You will play a key role in managing master data, supporting compliance (e.g., GxP, GDPR), and facilitating data-driven decision-making across the organization.\nMaintain and monitor master data for key domains (e.g., customer, product, vendor, material)\nIdentify, investigate, and remediate data issues using profiling tools and dashboards.\nDefine and enforce data definitions, naming conventions, and standard operating procedures\nIdentify and correct data quality issues and anomalies using established data validation protocols.\nImplement data cleansing and enrichment processes\nSupport company-wide data governance policies and frameworks.\nEnsure data complies with internal standards and external regulations (e.g., FDA, EMA, GDPR).\nWork closely with regulatory and quality teams to ensure alignment with compliance standards.\nUse statistical tools and modeling techniques to interpret trends, patterns, and correlations\nmaintain data pipelines and manage data cleaning processes.\nHelp identify data gaps and recommend solutions for improvement.\nensure data accuracy, consistency, and integrity across systems\nAnalyze large and complex datasets from internal and external sources (e.g., clinical trials, sales, market data).\nServe as a liaison between business units (e.g., marketing, regulatory, supply chain) and IT.\nCollaborate with data owners and SMEs to define and enforce data standards.\nProvide training and support to end-users on data management best practices.\nMaintain the accuracy and completeness of master data in key systems (e.g., SAP, Veeva, Oracle).\nSupport data migration and integration projects, ensuring clean and standardized data input.\nAssist in data extraction, reporting, and analytics to support business operations and decision-making.\nGenerate periodic data quality reports and metrics\nBasic Qualifications:\nDegree in computer science, Data Management, or related field & engineering preferred with 2-5 years of software development experience\n25 years of experience in a Data Steward, Data Analyst, or related role in the pharmaceutical industry\nExperience with data systems such as SAP, Veeva, Salesforce, Oracle, or Informatica is a plus.\nExcellent communication and collaboration skills.\nPreferred Qualifications:\nExperience with global data governance or enterprise data management initiative\nKnowledge of GxP, FDA/EMA guidelines, and industry-specific data compliance.\nProficiency in SQL and at least one programming language (e.g., Python, R).\nStrong skills in data visualization tools (Power BI, Tableau, Qlik, etc.).\nStrong skills Postgres SQL /Mongo DB SQL database, vector database for large language models, Databricks or RDS, S3 Buckets\nExcellent communication skills, with the ability to present complex data in a simple, understandable way\nGood to Have Skills\nPostgres SQL /Mongo DB SQL database, vector database for large language models, Databricks or RDS, S3 Buckets\ndesign patterns, data structures, data modelling, data algorithms",Biotechnology,"['Data Analysis', 'Power Bi', 'Tableau', 'Python', 'Data Visualization', 'Mongo Db', 'Sql', 'Databricks']",2025-06-12 15:30:09
Master Data Operations Manager,Amgen Technology Private Limited,6-10 Years,,Hyderabad,"Role Responsibilities:\nLead and manage master data operations for Material and Production Master domains\nEnsure compliance with data governance and regulatory standards\nCollaborate with IT and business teams to improve data processes and workflows\nOversee data migration, integration, and system upgrade projects\nKey Deliverables:\nAccurate and timely validation and entry of master data in SAP\nContinuous improvement in data accuracy and operational efficiency\nMonitoring and reporting on master data quality KPIs\nDevelopment and mentoring of a team of specialists and analysts",Pharmaceutical,"['Master Data Management', 'Sap Ecc', 'Sap Mdg', 'SAP TM', 'Data Governance']",2025-06-12 15:30:10
MDM Data Engineer,Amgen Technology Private Limited,2-6 Years,,Hyderabad,"Experience Required:\n25 years in Data Engineering with knowledge of MDM platforms\nOverview:\nWe are seeking an MDM Associate Data Engineer to support and enhance our enterprise Master Data Management (MDM) platforms using Informatica and/or Reltio. This role is crucial for delivering high-quality master data solutions across the organization by leveraging modern tools such as Databricks and AWS.\nKey Responsibilities:\nAnalyze and manage customer master data using Reltio or Informatica MDM\nPerform advanced SQL queries and data analysis to ensure master data integrity\nUtilize Python, PySpark, and Databricks for scalable data processing and automation\nCollaborate with business and engineering teams to enhance MDM solutions\nImplement data stewardship workflows including approval and DCR mechanisms\nUse AWS cloud services for MDM-related storage and compute processes\nContribute to metadata management and data modeling activities\nTrack and manage data issues using tools like JIRA and document processes in Confluence\nApply Life Sciences/Pharma industry context to ensure data compliance and standards\nQualifications:\nEducation:\nMaster's degree with 13 years of experience\nOR Bachelor's degree with 25 years of experience\nOR Diploma with 68 years of experience\nFields: Business, Engineering, IT, or related disciplines\nMust-Have Skills:\nStrong proficiency in Advanced SQL and data wrangling\nHands-on experience with:\nPython\nPySpark\nDatabricks\nAWS architecture\nKnowledge of MDM concepts, data governance, stewardship, and profiling\nExperience with Informatica MDM or Reltio MDM\nGood-to-Have Skills:\nExperience with Informatica Data Quality (IDQ), data modeling, and approval workflows\nBackground in the Life Sciences / Pharma domain\nFamiliarity with tools like JIRA and Confluence\nSolid grasp of data engineering concepts\nCertifications (Preferred):\nETL certifications (e.g., Informatica)\nData analysis certifications (SQL, Python, Databricks)\nCloud certifications (AWS, Azure)\nSoft Skills:\nStrong analytical and problem-solving abilities\nExcellent verbal and written communication skills\nAbility to explain complex data concepts to both technical and non-technical stakeholders\nTeam player with experience working in global, virtual teams",Biotechnology,"['Data Analysis', 'Etl', 'Informatica', 'Sql', 'Python', 'Cloud', 'Aws', 'data wrangling']",2025-06-12 15:30:12
MDM Data Engineer,Amgen Technology Private Limited,2-6 Years,,Hyderabad,"Experience Required:\n25 years in Data Engineering with knowledge of MDM platforms\nOverview:\nWe are seeking an MDM Associate Data Engineer to support and enhance our enterprise Master Data Management (MDM) platforms using Informatica and/or Reltio. This role is crucial for delivering high-quality master data solutions across the organization by leveraging modern tools such as Databricks and AWS.\nKey Responsibilities:\nAnalyze and manage customer master data using Reltio or Informatica MDM\nPerform advanced SQL queries and data analysis to ensure master data integrity\nUtilize Python, PySpark, and Databricks for scalable data processing and automation\nCollaborate with business and engineering teams to enhance MDM solutions\nImplement data stewardship workflows including approval and DCR mechanisms\nUse AWS cloud services for MDM-related storage and compute processes\nContribute to metadata management and data modeling activities\nTrack and manage data issues using tools like JIRA and document processes in Confluence\nApply Life Sciences/Pharma industry context to ensure data compliance and standards\nQualifications:\nEducation:\nMaster's degree with 13 years of experience\nOR Bachelor's degree with 25 years of experience\nOR Diploma with 68 years of experience\nFields: Business, Engineering, IT, or related disciplines\nMust-Have Skills:\nStrong proficiency in Advanced SQL and data wrangling\nHands-on experience with:\nPython\nPySpark\nDatabricks\nAWS architecture\nKnowledge of MDM concepts, data governance, stewardship, and profiling\nExperience with Informatica MDM or Reltio MDM\nGood-to-Have Skills:\nExperience with Informatica Data Quality (IDQ), data modeling, and approval workflows\nBackground in the Life Sciences / Pharma domain\nFamiliarity with tools like JIRA and Confluence\nSolid grasp of data engineering concepts\nCertifications (Preferred):\nETL certifications (e.g., Informatica)\nData analysis certifications (SQL, Python, Databricks)\nCloud certifications (AWS, Azure)\nSoft Skills:\nStrong analytical and problem-solving abilities\nExcellent verbal and written communication skills\nAbility to explain complex data concepts to both technical and non-technical stakeholders\nTeam player with experience working in global, virtual teams",Biotechnology,"['Data Analysis', 'Etl', 'Informatica', 'Sql', 'Python', 'Cloud', 'Aws', 'data wrangling']",2025-06-12 15:30:13
Senior Data Engineer,Amgen Technology Private Limited,4-6 Years,,Hyderabad,"In this vital role, we are looking for a highly motivated and expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions, and frameworks. The ideal candidate will be responsible for designing, developing, and optimizing data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities\nDesign, develop, and maintain scalable ETL/ELT pipelines to support structured, semi-structured, and unstructured data processing across the Enterprise Data Fabric.\nImplement real-time and batch data processing solutions, integrating data from multiple sources into a unified, governed data fabric architecture.\nOptimize big data processing frameworks using Apache Spark, Hadoop, or similar distributed computing technologies to ensure high availability and cost efficiency.\nWork with metadata management and data lineage tracking tools to enable enterprise-wide data discovery and governance.\nEnsure data security, compliance, and role-based access control (RBAC) across data environments.\nOptimize query performance, indexing strategies, partitioning, and caching for large-scale data sets.\nDevelop CI/CD pipelines for automated data pipeline deployments, version control, and monitoring.\nImplement data virtualization techniques to provide seamless access to data across multiple storage systems.\nCollaborate with cross-functional teams, including data architects, business analysts, and DevOps teams, to align data engineering strategies with enterprise goals.\nStay up to date with emerging data technologies and best practices, ensuring continuous improvement of Enterprise Data Fabric architecture.\nBasic Qualifications\nEducation and Experience:Master's degree and 4 to 6 years of Computer Science, IT, or related field experience; OR\nBachelor's degree and 6 to 8 years of Computer Science, IT, or related field experience.\nCertifications (Preferred):AWS Certified Data Engineer\nDatabricks Certificate\nScaled Agile SAFe certification\nFunctional Skills\nMust-Have Skills\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL, Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies.\nProficiency in workflow orchestration, performance tuning on big data processing.\nStrong understanding of AWS services.\nExperience with Data Fabric, Data Mesh, or similar enterprise-wide data architectures.\nAbility to quickly learn, adapt, and apply new technologies.\nStrong problem-solving and analytical skills.\nExcellent communication and collaboration skills.\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills\nDeep expertise in Biotech & Pharma industries.\nExperience in writing APIs to make the data available to the consumers.\nExperienced with SQL/NOSQL database, vector database for large language models.\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases.\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and DevOps.\nSoft Skills\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills.\nAbility to work effectively with global, virtual teams.\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized, and detail-oriented.\nStrong presentation and public speaking skills.",Biotechnology,"['Data Processing', 'AWS IAM', 'Spark SQL', 'Apache Spark', 'Databricks', 'Python']",2025-06-12 15:30:14
Ptp Analyst,BCForward,1-2 Years,,Bengaluru,"Purchase Requisition (PR) to Purchase Order (PO) Management: Create and amend POs based on validated PRs, ensuring compliance with company policies.\nInvoice Processing: Review, validate, and process incoming invoices (PO and Non-PO), ensuring accurate coding and booking in accordance with the company's chart of accounts.\nVendor Relationship Management: Serve as the primary point of contact for suppliers, addressing inquiries related to purchase orders, invoices, and payments.\nCompliance and Policy Adherence: Ensure all procurement activities comply with internal controls, company policies, and regulatory requirements.\nReporting and Analysis: Generate and analyze procurement reports, providing insights into spending patterns, supplier performance, and process efficiencies.\nCollaboration with Internal Teams: Work closely with sourcing, supply chain, and finance teams to streamline procurement processes and resolve discrepancies.\nSystem Management: Utilize ERP systems (e.g., SAP, Oracle, Coupa) to manage procurement activities and maintain accurate records.\nContinuous Improvement: Identify opportunities for process improvements and contribute to initiatives aimed at enhancing procurement efficiency.",IT Management,"['purchase requisition', 'Customer Relation', 'sap', 'oracle', 'Coupa', 'Po']",2025-06-12 15:30:16
Associate Data Engineer,Amgen Technology Private Limited,0-2 Years,,Hyderabad,"Experience Required:\n25 years in Data Engineering with knowledge of MDM platforms\nOverview:\nWe are seeking an MDM Associate Data Engineer to support and enhance our enterprise Master Data Management (MDM) platforms using Informatica and/or Reltio. This role is crucial for delivering high-quality master data solutions across the organization by leveraging modern tools such as Databricks and AWS.\nKey Responsibilities:\nAnalyze and manage customer master data using Reltio or Informatica MDM\nPerform advanced SQL queries and data analysis to ensure master data integrity\nUtilize Python, PySpark, and Databricks for scalable data processing and automation\nCollaborate with business and engineering teams to enhance MDM solutions\nImplement data stewardship workflows including approval and DCR mechanisms\nUse AWS cloud services for MDM-related storage and compute processes\nContribute to metadata management and data modeling activities\nTrack and manage data issues using tools like JIRA and document processes in Confluence\nApply Life Sciences/Pharma industry context to ensure data compliance and standards\nQualifications:\nEducation:\nMaster's degree with 13 years of experience\nOR Bachelor's degree with 25 years of experience\nOR Diploma with 68 years of experience\nFields: Business, Engineering, IT, or related disciplines\nMust-Have Skills:\nStrong proficiency in Advanced SQL and data wrangling\nHands-on experience with:\nPython\nPySpark\nDatabricks\nAWS architecture\nKnowledge of MDM concepts, data governance, stewardship, and profiling\nExperience with Informatica MDM or Reltio MDM\nGood-to-Have Skills:\nExperience with Informatica Data Quality (IDQ), data modeling, and approval workflows\nBackground in the Life Sciences / Pharma domain\nFamiliarity with tools like JIRA and Confluence\nSolid grasp of data engineering concepts\nCertifications (Preferred):\nETL certifications (e.g., Informatica)\nData analysis certifications (SQL, Python, Databricks)\nCloud certifications (AWS, Azure)\nSoft Skills:\nStrong analytical and problem-solving abilities\nExcellent verbal and written communication skills\nAbility to explain complex data concepts to both technical and non-technical stakeholders\nTeam player with experience working in global, virtual teams",Biotechnology,"['Data Analysis', 'Etl', 'Informatica', 'Sql', 'Python', 'Cloud', 'Aws', 'data wrangling']",2025-06-12 15:30:17
Tibco Professional,BCForward,5-10 Years,,Bengaluru,"Job Overview\nType of work :\nC2H role\nMode of Interview :\nVirtual/F2F\nWork :\nHybrid\nNP: Immediate/30-Days\nMust have Skills: Tibco\nDescription:\nTechnical Experience :\nEWM, EMS,\nTibco adapters,\nWeb Services(SOAP),\nEDI-Signals\nEducation:\nBachelors Degree in a technical field\nShould have excellent verbal communication skills\nLooking for Immediate-15 days joiners",IT Management,"['Ewm', 'Ems', 'Soap', 'Edi', 'Tibco']",2025-06-12 15:30:18
Database and Edit Check UAT Tester,Amgen Technology Private Limited,1-5 Years,,Hyderabad,Role Responsibilities:\nExecute database testing to ensure data accuracy and integrity\nDevelop and maintain detailed edit check test specifications and documentation\nConduct user acceptance testing (UAT) for edit checks and validate outcomes\nCollaborate with teams to identify and communicate issues and risks promptly\nSupport audit and inspection activities by preparing documentation and responding to findings\nParticipate in cross-functional groups to promote standard data management practices\nKey Deliverables:\nAccurate and comprehensive database test results\nComplete and up-to-date testing documentation\nTimely identification and resolution of testing issues\nAudit and inspection-ready compliance with data management standards,Pharmaceutical,"['Edit Check Specifications', 'Clinical Data Management', 'Good Clinical Practice', 'Database Testing', 'Uat']",2025-06-12 15:30:20
Scientific Business Analyst (Specialist) - Small Molecule Discovery,Amgen Technology Private Limited,1-3 Years,,Hyderabad,"Role Responsibilities:\nTranslate scientific and technical needs into clear agile user requirements\nCollaborate with global teams and research scientists to prioritize needs\nManage and support scientific software platforms and infrastructure\nDevelop documentation, training plans, and ensure compliance and cybersecurity\nKey Deliverables:\nWell-defined user stories and acceptance criteria in agile systems\nEfficient operation and technical support of scientific platforms\nUp-to-date training and communication materials for end users\nContinuous improvement aligned with industry trends and SAFe practices",Pharmaceutical,"['Business Analysis', 'Omics', 'Lims', 'Agile', 'AWS']",2025-06-12 15:30:30
"Manager, Predictive & Optimization Analyst",SCHNEIDER ELECTRIC SINGAPORE PTE LTD,3-7 Years,,Bengaluru,"As a Remote Support Engineer, you'll play a crucial role in providing proactive and reactive support for electrical distribution systems, leveraging data analytics to ensure optimal asset performance and customer satisfaction.\nResponsibilities\nPossess a strong understanding of Electrical distribution with the ability to perform predictive / condition-based analyses of data generated by EAA Predictive into meaningful insights and actions.\nExtracts and analyzes data, runs analytics (software/expert-based).\nObserves health of assets, including condition-based analytics, and adjusts maintenance plans accordingly.\nDevelops recommendations based on the data, creates reports, shares reports with customers, and provides direction to the customer as to actions required.\nInteracts with customers and provides remote troubleshooting (e.g., phone, chat, email, etc.) as needed.\nActs based on customer requests to create cases, initiates work order creation to dispatch Field Services and/or engages Sales for service or product quotations.\nInteracts with Network of experts for advanced support of data analysis and recommendations to the customer.\nQualifications\nThe Remote Support Engineer must have a proven track record of:\nProviding excellent customer service and must possess a customer-focused mindset.\nEffective communication (written/oral/reading/listening), with a command of the English language.\nAbility to keep relevant stakeholders informed and build a network of relationships that would aid in business development.\nAbility to influence and convince.\nAbility to effectively manage time and remain organized across multiple activities along with an attention to detail.\nCuriosity and interest in new knowledge in various domains.\nMinimum Knowledge & Skill Expectations\nService experience with power/power monitoring equipment and industrial applications.\nKnowledge of electrical distribution systems and proficiency with one-line diagrams.\nStrong Technical writing and computer knowledge, fluency with Microsoft Office tools.\nSuperior analytical abilities and high degree of proficiency in working with data",Electronics,"['Electrical Distribution Systems', 'Data Analysis', 'remote troubleshooting', 'Case Management', 'Predictive Analysis', 'Data Extraction']",2025-06-12 15:30:34
"Manager, Predictive & Optimization Analyst",SCHNEIDER ELECTRIC SINGAPORE PTE LTD,3-7 Years,,Bengaluru,"As a Remote Support Engineer, you'll play a crucial role in providing proactive and reactive support for electrical distribution systems, leveraging data analytics to ensure optimal asset performance and customer satisfaction.\nResponsibilities\nPossess a strong understanding of Electrical distribution with the ability to perform predictive / condition-based analyses of data generated by EAA Predictive into meaningful insights and actions.\nExtracts and analyzes data, runs analytics (software/expert-based).\nObserves health of assets, including condition-based analytics, and adjusts maintenance plans accordingly.\nDevelops recommendations based on the data, creates reports, shares reports with customers, and provides direction to the customer as to actions required.\nInteracts with customers and provides remote troubleshooting (e.g., phone, chat, email, etc.) as needed.\nActs based on customer requests to create cases, initiates work order creation to dispatch Field Services and/or engages Sales for service or product quotations.\nInteracts with Network of experts for advanced support of data analysis and recommendations to the customer.\nQualifications\nThe Remote Support Engineer must have a proven track record of:\nProviding excellent customer service and must possess a customer-focused mindset.\nEffective communication (written/oral/reading/listening), with a command of the English language.\nAbility to keep relevant stakeholders informed and build a network of relationships that would aid in business development.\nAbility to influence and convince.\nAbility to effectively manage time and remain organized across multiple activities along with an attention to detail.\nCuriosity and interest in new knowledge in various domains.\nMinimum Knowledge & Skill Expectations\nService experience with power/power monitoring equipment and industrial applications.\nKnowledge of electrical distribution systems and proficiency with one-line diagrams.\nStrong Technical writing and computer knowledge, fluency with Microsoft Office tools.\nSuperior analytical abilities and high degree of proficiency in working with data",Electronics,"['Electrical Distribution Systems', 'Data Analysis', 'remote troubleshooting', 'Case Management', 'Predictive Analysis', 'Data Extraction']",2025-06-12 15:30:36
Data Platform Engineer,Amgen Technology Private Limited,2-5 Years,,Hyderabad,"What you will do\nRoles & Responsibilities:\nWork as a member of a Data Platform Engineering team that uses Cloud and Big Data technologies to design, develop, implement and maintain solutions to support various functional areas like Manufacturing, Commercial, Research and Development.\nWork closely with the Enterprise Data Lake delivery and platform teams to ensure that the applications are aligned with the overall architectural and development guidelines\nResearch and evaluate technical solutions including Databricks and AWS Services, NoSQL databases, Data Science packages, platforms and tools with a focus on enterprise deployment capabilities like security, scalability, reliability, maintainability, cost management etc.\nAssist in building and maintaining relationships with internal and external business stakeholders\nDevelop basic understanding of core business problems and identify opportunities to use advanced analytics\nAssist in reviewing 3rd party providers for new feature/function/technical fit with department's data management needs.\nWork closely with the Enterprise Data Lake ecosystem leads to identify and evaluate emerging providers of data management & processing components that could be incorporated into data platform.\nWork with platform stakeholders to ensure effective cost observability and control mechanisms are in place for all aspects of data platform management.\nExperience developing in an Agile development environment, and comfortable with Agile terminology and ceremonies.\nKeen on embracing new responsibilities, facing challenges, and mastering new technologies\nWhat we expect of you\nBasic Qualifications and Experience:\nMaster's degree in computer science or engineering field and 1 to 3 years of relevant experience OR\nBachelor's degree in computer science or engineering field and 3 to 5 years of relevant experience OR\nDiploma and Minimum of 8+ years of relevant work experience\nMust-Have Skills:\nExperience with Databricks (or Snowflake), including cluster setup, execution, and tuning\nExperience with common data processing librariesPandas, PySpark, SQLAlchemy.\nExperience in UI frameworks (Angular.js or React.js)\nExperience with data lake, data fabric and data mesh concepts\nExperience with data modeling, performance tuning, and experience on relational databases\nExperience building ETL or ELT pipelines; Hands-on experience with SQL/NoSQL\nProgram skills in one or more computer languages SQL, Python, Java\nExperienced with software engineering best-practices, including but not limited to version control (Git, GitLab.), CI/CD (GitLab, Jenkins etc.), automated unit testing, and Dev Ops\nExposure to Jira or Jira Align.\nGood-to-Have Skills:\nKnowledge on R language will be considered an advantage\nExperience in Cloud technologies AWS preferred.\nCloud Certifications -AWS, Databricks, Microsoft\nFamiliarity with the use of AI for development productivity, such as Github Copilot, Databricks Assistant, Amazon Q Developer or equivalent.\nKnowledge of Agile and DevOps practices.\nSkills in disaster recovery planning.\nFamiliarity with load testing tools (JMeter, Gatling).\nBasic understanding of AI/ML for monitoring.\nKnowledge of distributed systems and microservices.\nData visualization skills (Tableau, Power BI).\nStrong communication and leadership skills.\nUnderstanding of compliance and auditing requirements",Biotechnology,"['Data Processing', 'UI Frameworks', 'Relational Databases', 'R', 'Databricks', 'React Js', 'Data Modeling', 'Sql', 'python', 'Java', 'Aws', 'Devops', 'Data Visualization']",2025-06-12 15:30:37
Master Data Operations Manager,Amgen Technology Private Limited,6-10 Years,,Hyderabad,"Role Responsibilities:\nLead and manage master data operations for Material and Production Master domains\nEnsure compliance with data governance and regulatory standards\nCollaborate with IT and business teams to improve data processes and workflows\nOversee data migration, integration, and system upgrade projects\nKey Deliverables:\nAccurate and timely validation and entry of master data in SAP\nContinuous improvement in data accuracy and operational efficiency\nMonitoring and reporting on master data quality KPIs\nDevelopment and mentoring of a team of specialists and analysts",Pharmaceutical,"['Master Data Management', 'Sap Ecc', 'Sap Mdg', 'SAP TM', 'Data Governance']",2025-06-12 15:30:38
Data Analytics Manager Finance,Amgen Technology Private Limited,6-9 Years,,Hyderabad,"What will you do\nIn this vital role as a Data Analytics Manager you will play a meaningful role in fully understanding financial data and associated systems architecture in order to design data integrations for reporting and analysis to support the global Amgen organization. Key responsibilities include but are not limited to:\nDeveloping a robust understanding of Amgen's financial data and systems in order to support data requests and integrations for different initiatives\nWorking in close partnership with clients or team members to design, develop and augment the financial datasets\nProviding client-facing project management support and completing hands-on Databricks/Prophecy development as well as Power BI or Tableau as time and priorities allow\nDesigning and developing the underlying ETL data processes used to build various financial datasets used for reporting and/or dashboards\nIdentifying data enhancements or process improvements to optimize the financial datasets and processes\nUnderstanding the regularly scheduled financial datasets in Databricks and Tableau or Power BI dashboards refresh processes on an ongoing basis\nInvolvement in the Financial Data Product Team, as a finance data subject matter expert.\nKey elements to success in this role include understanding Amgen's financial systems and data, ability to define business requirements, and understanding how to design datasets compatible with Power BI, Tableau or other analytic tool reporting requirements.\nWhat we expect of you\nWe are all different, yet we all use our outstanding contributions to serve patients. The Data Analytics Manager professional we seek is a go-getter with these qualifications.\nBasic Qualifications\nDoctorate degree\nOr\nMaster's degree and 2 years of Finance experience\nOr\nBachelor's degree and 4 years of Finance experience\nOr\nAssociate's degree and 10 years of Finance experience\nOr\nHigh school diploma / GED and 12 years of Finance experience\nPreferred Qualifications\nExperience performing data analysis across one or more areas of the business to derive business logic for data integration\nExperience working with business partners to identify complex functionality and translate it into requirements\nExperience with financial statements and Amgen Finance experience preferred\nExperience with data analysis, data modeling, and data visualization solutions such as Power BI, Tableau, Databricks, and Alteryx\nFamiliar with Hyperion Planning, SAP, scripting languages like SQL or Python, Databricks Prophecy, and AWS services like S3\nAble to work in matrixed teams, across geographic and functional reporting lines\nExcellent analytical and problem-solving skills\nExcellent facilitation, influencing, and negotiation skills\nProficient in MS Office Suite",Biotechnology,"['Finance', 'Financial Analysis', 'Data Analytics', 'Databricks', 'Etl', 'Tableau', 'Sql', 'Sap', 'Aws']",2025-06-12 15:30:39
Senior Data Engineer,Amgen Technology Private Limited,3-7 Years,,Hyderabad,"Role Responsibilities:\nDesign, build, and maintain scalable data pipelines and ETL processes\nDevelop and ensure data quality across large datasets\nCollaborate with cross-functional teams to meet business data needs\nImplement data security and governance standards\nUse cloud platforms (AWS) to build and optimize data solutions\nKey Deliverables:\nRobust ETL pipelines and data integration solutions\nAccurate data models and documentation\nData governance and security compliance\nEfficient data processing on big data frameworks\nTimely sprint planning and project delivery",Pharmaceutical,"['Python', 'Pyspark', 'Sql', 'Databricks', 'Amazon Redshift']",2025-06-12 15:30:40
Principal Data Engineer,Amgen Technology Private Limited,8-10 Years,,Hyderabad,"What you will do\nWe are seeking a seasoned Principal Data Engineer to lead the design, development, and implementation of our data strategy. The ideal candidate possesses a deep understanding of data engineering principles, coupled with strong leadership and problem-solving skills. As a Principal Data Engineer, you will architect and oversee the development of robust data platforms, while mentoring and guiding a team of data engineers.\nRoles & Responsibilities:\nPossesses strong rapid prototyping skills and can quickly translate concepts into working code.\nProvide expert guidance and mentorship to the data engineering team, fostering a culture of innovation and standard methodologies.\nDesign, develop, and implement robust data architectures and platforms to support business objectives.\nOversee the development and optimization of data pipelines, and data integration solutions.\nEstablish and maintain data governance policies and standards to ensure data quality, security, and compliance.\nArchitect and manage cloud-based data solutions, using AWS or other preferred platforms.\nLead and motivate an impactful data engineering team to deliver exceptional results.\nIdentify, analyze, and resolve complex data-related challenges.\nCollaborate closely with business collaborators to understand data requirements and translate them into technical solutions.\nStay abreast of emerging data technologies and explore opportunities for innovation.\nBasic Qualifications:\nMasters degree and 8 to 10 years of computer science and engineering preferred, other Engineering field is considered OR\nBachelors degree and 10 to 14 years of computer science and engineering preferred, other Engineering field is considered;\nDiploma and 14 to 18 years of in computer science and engineering preferred, other Engineering field is considered\nDemonstrated proficiency in using cloud platforms (AWS, Azure, GCP) for data engineering solutions. Strong understanding of cloud architecture principles and cost optimization strategies.\nProficient on experience in Python, PySpark, SQL. Handon experience with bid data ETL performance tuning.\nProven ability to lead and develop impactful data engineering teams.\nStrong problem-solving, analytical, and critical thinking skills to address complex data challenges.\nPreferred Qualifications:\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with Apache Spark, Apache Airflow\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nExperienced with AWS, GCP or Azure cloud services\nProfessional Certifications\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.",Biotechnology,"['data engineering', 'Azure', 'Aws', 'Databricks', 'Devops', 'Data Modeling', 'Apache Spark', 'Python', 'Sql', 'Big Data']",2025-06-12 15:30:42
Business Intelligence Engineer,Amgen Technology Private Limited,1-6 Years,,Hyderabad,"As a Business Intelligence Engineer, you will solve unique and complex problems at a rapid pace, utilizing the latest technologies to create solutions that are highly scalable. This role involves working closely with product managers, designers, and other engineers to create high-quality, scalable solutions and responding to requests for rapid releases of analytical outcomes.\nWhat You Will Do\nDesign, develop, and maintain interactive dashboards, reports, and data visualizations using BI tools (e.g., Power BI, Tableau, Cognos, others).\nAnalyze datasets to identify trends, patterns, and insights that inform business strategy and decision-making.\nPartner with leaders and stakeholders across Finance, Sales, Customer Success, Marketing, Product, and other departments to understand their data and reporting requirements.\nStay abreast of the latest trends and technologies in business intelligence and data analytics, inclusive of AI use in BI.\nElicit and document clear and comprehensive business requirements for BI solutions, translating business needs into technical specifications and solutions.\nCollaborate with Data Engineers to ensure efficient up-system transformations and create data models/views that will hydrate accurate and reliable BI reporting.\nContribute to data quality and governance efforts to ensure the accuracy and consistency of BI data.\nBasic Qualifications\nMaster's degree and 1 to 3 years of Computer Science, IT, or related field experience; OR\nBachelor's degree and 3 to 5 years of Computer Science, IT, or related field experience; OR\nDiploma and 7 to 9 years of Computer Science, IT, or related field experience.\nFunctional Skills\n1+ years of experience analyzing and interpreting data with Redshift, Oracle, NoSQL, etc.\nExperience with data visualization using Tableau, Quicksight, or similar tools.\nExperience with data modeling, warehousing, and building ETL pipelines.\nExperience using SQL to pull data from a database or data warehouse and scripting experience (Python) to process data for modeling.\nPreferred Qualifications\nExperience with AWS solutions such as EC2, DynamoDB, S3, and Redshift.\nExperience in data mining, ETL, etc., and using databases in a business environment with large-scale, complex datasets.\nAWS Developer certification (preferred).\nSoft Skills\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills.\nAbility to work effectively with global, virtual teams.\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nStrong presentation and public speaking skills.",Biotechnology,"['Ec2', 'Dynamodb', 'Nosql', 'Oracle', 'Redshift', 'Business Inteligence']",2025-06-12 15:30:43
Business Intelligence Engineer,Amgen Technology Private Limited,1-6 Years,,Hyderabad,"As a Business Intelligence Engineer, you will solve unique and complex problems at a rapid pace, utilizing the latest technologies to create solutions that are highly scalable. This role involves working closely with product managers, designers, and other engineers to create high-quality, scalable solutions and responding to requests for rapid releases of analytical outcomes.\nWhat You Will Do\nDesign, develop, and maintain interactive dashboards, reports, and data visualizations using BI tools (e.g., Power BI, Tableau, Cognos, others).\nAnalyze datasets to identify trends, patterns, and insights that inform business strategy and decision-making.\nPartner with leaders and stakeholders across Finance, Sales, Customer Success, Marketing, Product, and other departments to understand their data and reporting requirements.\nStay abreast of the latest trends and technologies in business intelligence and data analytics, inclusive of AI use in BI.\nElicit and document clear and comprehensive business requirements for BI solutions, translating business needs into technical specifications and solutions.\nCollaborate with Data Engineers to ensure efficient up-system transformations and create data models/views that will hydrate accurate and reliable BI reporting.\nContribute to data quality and governance efforts to ensure the accuracy and consistency of BI data.\nBasic Qualifications\nMaster's degree and 1 to 3 years of Computer Science, IT, or related field experience; OR\nBachelor's degree and 3 to 5 years of Computer Science, IT, or related field experience; OR\nDiploma and 7 to 9 years of Computer Science, IT, or related field experience.\nFunctional Skills\n1+ years of experience analyzing and interpreting data with Redshift, Oracle, NoSQL, etc.\nExperience with data visualization using Tableau, Quicksight, or similar tools.\nExperience with data modeling, warehousing, and building ETL pipelines.\nExperience using SQL to pull data from a database or data warehouse and scripting experience (Python) to process data for modeling.\nPreferred Qualifications\nExperience with AWS solutions such as EC2, DynamoDB, S3, and Redshift.\nExperience in data mining, ETL, etc., and using databases in a business environment with large-scale, complex datasets.\nAWS Developer certification (preferred).\nSoft Skills\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills.\nAbility to work effectively with global, virtual teams.\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nStrong presentation and public speaking skills.",Biotechnology,"['Ec2', 'Dynamodb', 'Nosql', 'Oracle', 'Redshift', 'Business Inteligence']",2025-06-12 15:30:44
Associate Data Analytics,Amgen Technology Private Limited,1-3 Years,,Hyderabad,"What you will do\nLet's do this. Let's change the world. We are seeking a detail-oriented and proactive Data Steward to support our pharmaceutical data governance initiatives. The ideal candidate will ensure the quality, consistency, integrity, and security of data across our systems, particularly within commercial, regulatory, R&D, and supply chain functions. You will play a key role in managing master data, supporting compliance (e.g., GxP, GDPR), and facilitating data-driven decision-making across the organization.\nMaintain and monitor master data for key domains (e.g., customer, product, vendor, material)\nIdentify, investigate, and remediate data issues using profiling tools and dashboards.\nDefine and enforce data definitions, naming conventions, and standard operating procedures\nIdentify and correct data quality issues and anomalies using established data validation protocols.\nImplement data cleansing and enrichment processes\nSupport company-wide data governance policies and frameworks.\nEnsure data complies with internal standards and external regulations (e.g., FDA, EMA, GDPR).\nWork closely with regulatory and quality teams to ensure alignment with compliance standards.\nUse statistical tools and modeling techniques to interpret trends, patterns, and correlations\nmaintain data pipelines and manage data cleaning processes.\nHelp identify data gaps and recommend solutions for improvement.\nensure data accuracy, consistency, and integrity across systems\nAnalyze large and complex datasets from internal and external sources (e.g., clinical trials, sales, market data).\nServe as a liaison between business units (e.g., marketing, regulatory, supply chain) and IT.\nCollaborate with data owners and SMEs to define and enforce data standards.\nProvide training and support to end-users on data management best practices.\nMaintain the accuracy and completeness of master data in key systems (e.g., SAP, Veeva, Oracle).\nSupport data migration and integration projects, ensuring clean and standardized data input.\nAssist in data extraction, reporting, and analytics to support business operations and decision-making.\nGenerate periodic data quality reports and metrics\nBasic Qualifications:\nDegree in computer science, Data Management, or related field & engineering preferred with 2-5 years of software development experience\n25 years of experience in a Data Steward, Data Analyst, or related role in the pharmaceutical industry\nExperience with data systems such as SAP, Veeva, Salesforce, Oracle, or Informatica is a plus.\nExcellent communication and collaboration skills.\nPreferred Qualifications:\nExperience with global data governance or enterprise data management initiative\nKnowledge of GxP, FDA/EMA guidelines, and industry-specific data compliance.\nProficiency in SQL and at least one programming language (e.g., Python, R).\nStrong skills in data visualization tools (Power BI, Tableau, Qlik, etc.).\nStrong skills Postgres SQL /Mongo DB SQL database, vector database for large language models, Databricks or RDS, S3 Buckets\nExcellent communication skills, with the ability to present complex data in a simple, understandable way\nGood to Have Skills\nPostgres SQL /Mongo DB SQL database, vector database for large language models, Databricks or RDS, S3 Buckets\ndesign patterns, data structures, data modelling, data algorithms",Biotechnology,"['Data Analysis', 'Power Bi', 'Tableau', 'Python', 'Data Visualization', 'Mongo Db', 'Sql', 'Databricks']",2025-06-12 15:30:45
Ptp Analyst,BCForward,1-2 Years,,Bengaluru,"Purchase Requisition (PR) to Purchase Order (PO) Management: Create and amend POs based on validated PRs, ensuring compliance with company policies.\nInvoice Processing: Review, validate, and process incoming invoices (PO and Non-PO), ensuring accurate coding and booking in accordance with the company's chart of accounts.\nVendor Relationship Management: Serve as the primary point of contact for suppliers, addressing inquiries related to purchase orders, invoices, and payments.\nCompliance and Policy Adherence: Ensure all procurement activities comply with internal controls, company policies, and regulatory requirements.\nReporting and Analysis: Generate and analyze procurement reports, providing insights into spending patterns, supplier performance, and process efficiencies.\nCollaboration with Internal Teams: Work closely with sourcing, supply chain, and finance teams to streamline procurement processes and resolve discrepancies.\nSystem Management: Utilize ERP systems (e.g., SAP, Oracle, Coupa) to manage procurement activities and maintain accurate records.\nContinuous Improvement: Identify opportunities for process improvements and contribute to initiatives aimed at enhancing procurement efficiency.",IT Management,"['purchase requisition', 'Customer Relation', 'sap', 'oracle', 'Coupa', 'Po']",2025-06-12 15:30:47
Associate Data Engineer,Amgen Technology Private Limited,0-2 Years,,Hyderabad,"Experience Required:\n25 years in Data Engineering with knowledge of MDM platforms\nOverview:\nWe are seeking an MDM Associate Data Engineer to support and enhance our enterprise Master Data Management (MDM) platforms using Informatica and/or Reltio. This role is crucial for delivering high-quality master data solutions across the organization by leveraging modern tools such as Databricks and AWS.\nKey Responsibilities:\nAnalyze and manage customer master data using Reltio or Informatica MDM\nPerform advanced SQL queries and data analysis to ensure master data integrity\nUtilize Python, PySpark, and Databricks for scalable data processing and automation\nCollaborate with business and engineering teams to enhance MDM solutions\nImplement data stewardship workflows including approval and DCR mechanisms\nUse AWS cloud services for MDM-related storage and compute processes\nContribute to metadata management and data modeling activities\nTrack and manage data issues using tools like JIRA and document processes in Confluence\nApply Life Sciences/Pharma industry context to ensure data compliance and standards\nQualifications:\nEducation:\nMaster's degree with 13 years of experience\nOR Bachelor's degree with 25 years of experience\nOR Diploma with 68 years of experience\nFields: Business, Engineering, IT, or related disciplines\nMust-Have Skills:\nStrong proficiency in Advanced SQL and data wrangling\nHands-on experience with:\nPython\nPySpark\nDatabricks\nAWS architecture\nKnowledge of MDM concepts, data governance, stewardship, and profiling\nExperience with Informatica MDM or Reltio MDM\nGood-to-Have Skills:\nExperience with Informatica Data Quality (IDQ), data modeling, and approval workflows\nBackground in the Life Sciences / Pharma domain\nFamiliarity with tools like JIRA and Confluence\nSolid grasp of data engineering concepts\nCertifications (Preferred):\nETL certifications (e.g., Informatica)\nData analysis certifications (SQL, Python, Databricks)\nCloud certifications (AWS, Azure)\nSoft Skills:\nStrong analytical and problem-solving abilities\nExcellent verbal and written communication skills\nAbility to explain complex data concepts to both technical and non-technical stakeholders\nTeam player with experience working in global, virtual teams",Biotechnology,"['Data Analysis', 'Etl', 'Informatica', 'Sql', 'Python', 'Cloud', 'Aws', 'data wrangling']",2025-06-12 15:30:48
Senior Analyst - Coupa,SCHNEIDER ELECTRIC SINGAPORE PTE LTD,3-6 Years,INR 3 - 6 LPA,Bengaluru,"Position Summary\nThe Coupa P2P Specialist will provide technical expertise and support for Schneider Electric's Coupa Procure-to-Pay (P2P) process, with a special focus on e-invoicing. This role involves supporting, configuring, integrating, and enhancing the Coupa platform to drive efficiency and business value.\n\nKey Responsibilities\nTechnical Expertise: Provide Level 2/3 support for Coupa P2P, monitoring support queues and resolving issues following Schneider Digital Incident and Change Management protocols.\nSolution Design: Work closely with business stakeholders to understand requirements and design effective Coupa solutions.\nImplementation: Lead and support Coupa implementation projects, ensuring best practices and compliance with industry standards.\nConfiguration & Customization: Configure Coupa to meet business needs and oversee custom solution development when required.\nIntegration: Manage master and transaction data integration with ERP systems (SAP ECC / S4 HANA) via Webservices, REST APIs, and Middleware.\nData Management: Ensure data accuracy and integrity on the Coupa platform, enforcing data governance practices.\nUser Training & Support: Develop training materials, guide end-users, and support deployment activities such as Unit Testing, System Integration Testing (SIT), Cutover, and Go-live preparation.\nTechnical Documentation: Maintain up-to-date documentation on system configurations, integrations, and customizations.\nTroubleshooting: Diagnose and resolve technical issues for Coupa users.",Electronics,"['Coupa P2P', 'e-invoicing', 'Technical support', 'Implementation', 'ERP integration', 'Solution Design', 'Configuration', 'Customization', 'Sap Ecc']",2025-06-12 15:30:50
SAP Fiori Portal Senior Product Analyst,BCForward,5-10 Years,,Hyderabad,"Analyze business requirements and translate them into Fiori application specifications Configure and support SAP Fiori Launchpad and Portal roles Collaborate with SAP developers to deliver UX enhancements and performance improvements\nTroubleshoot and resolve issues with SAP UI5/Fiori apps Work with SAP Gateway, OData Services, and back-end integration Coordinate with cross-functional teams for project delivery Document solutions and provide end-user training as needed\nNP: Immediate",IT Management,"['SAP', 'Ux', 'UI', 'Odata', 'Fiori']",2025-06-12 15:30:51
Data Steward,Amgen Technology Private Limited,3-8 Years,,Hyderabad,"You will play a key role in the implementation and adoption of the data governance framework which will modernize Amgen's data ecosystem, positioning Amgen as a leader in biopharma innovation.\nThis role leverages state-of-the-art technologies, including Generative AI, Machine Learning, and integrated data. This role involves working closely with business stakeholders and data analysts to ensure the implementation and adoption of the data governance framework.\nYou will collaborate with the Product Owner and other Business Analysts to ensure operational support and excellence from the team.\nRoles & Responsibilities\nResponsible for the data governance and data management framework implementation for the Development domain of the biopharma lifecycle.\nContribute to the operationalization of the Enterprise data governance framework and aligning broader stakeholder community with their data governance needs, including data quality, data access controls, compliance with privacy and security regulations, foundational master data management, data sharing, communication, and change management.\nWorks with Enterprise MDM and Reference Data to enforce standards and data reusability.\nContribute to the cross-functional alignment in his/her domain(s) of expertise to ensure adherence to Data Governance principles.\nMaintain documentation on data definitions, data standards, data flows, legacy data structures/hierarchies, common data models, data harmonization, etc., for assigned domains.\nPartner with business teams to identify compliance requirements with data privacy, security, and regulatory policies for the assigned domains.\nJointly with Technology teams, business functions, and enterprise teams (e.g., MDM, Enterprise Data Fabric, etc.) define the specifications shaping the development and implementation of data foundations.\nBuild strong relationships with key business leaders and partners to ensure their needs are met.\nFunctional Skills\nMust-Have Functional Skills\nTechnical skills with knowledge of Pharma processes with specialization in the Development domain of the biopharma lifecycle.\nGeneral knowledge of data management, common data models, metadata management, data quality, master data management, data stewardship, data protection, etc.\nExperience with data products development lifecycle, including the enablement of data dictionaries, business glossary to increase data products reusability and data literacy.\nCustomer-focused with excellent written and verbal communication skills who can confidently work with internal Amgen business stakeholders and external service partners on business process and and technology topics.\nExperience of working with or supporting systems used for data governance framework (e.g., Collibra, Alation).\nExcellent problem-solving skills and committed attention to detail in finding solutions.\nGood-to-Have Functional Skills\nExperience with Agile software development methodologies (Scrum).\nProficiency in data analysis and quality tools (e.g., SQL, Excel, Python, or SAS).\nSoft Skills\nHighly organized and able to work under minimal supervision.\nExcellent analytical and assessment skills.\nAbility to work effectively with global, virtual teams.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAmbitious to further develop their skills and career.\nAbility to build business relationships and understand end-to-end data use and needs.\nExcellent interpersonal skills (team player). People management skills either in matrix or direct line function.\nStrong verbal and written communication skills.\nHigh degree of initiative and self-motivation.\nGood presentation and public speaking skills.\nStrong attention to detail, quality, time management, and customer focus.\nBasic Qualifications\nBachelor's degree and 3 to 5 years of Life Science/Biotechnology/Pharmacology/Information Systems experience; OR\nDiploma and 7 to 9 years of Life Science/Biotechnology/Pharmacology/Information Systems experience.",Biotechnology,"['Data governance framework', 'Excel', 'SAS', 'Agile Software Development', 'Scrum', 'Sql', 'Python']",2025-06-12 15:31:02
"Director, Commercial Forecasting and Analytics",Amgen Technology Private Limited,9-14 Years,,Hyderabad,"Responsibilities:\nCapability Builder:Establish and grow Amgen India's center of excellence for Forecasting & CI from the ground up.\nPeople Leadership:Hire, develop, and lead a blended team of full-time employees and contract workers. Foster a high-performance, inclusive, and collaborative culture.\nStrategic Partnering:Act as the key offshore liaison for global Forecasting & CI leadership, partnering across US and global collaborators to deliver critical insights and foresight.\nDelivery Oversight:Ensure on-time, high-quality forecasting models and CI deliverables for pipeline and inline products.\nOperational Excellence:Develop scalable processes and methodologies, implement best practices in forecast modeling, scenario planning, and competitor landscape analysis.\nVendor & Contractor Management:Manage external vendors and contract staff, ensuring high-quality standards, governance, and value delivery.\nInnovation & Technology Enablement:Champion advanced analytics tools, automation, and AI/ML methodologies in forecasting and intelligence.\nStakeholder Engagement:Translate business questions into actionable insights. Effectively communicate findings to senior cross-functional leaders to influence strategic decisions\nFinancial management:Oversee budget associated with offshore work in India, ensuring best negotiated rates and overall value\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients.\nBasic Qualifications\nDoctorate degree and 4 years of pharmaceutical forecasting and/or Competitive Intelligence experience OR\nMaster's degree and 14 to 16 years of pharmaceutical forecasting and/or Competitive Intelligence experience OR\nBachelor's degree and 16 to 18 pharmaceutical forecasting and/or Competitive Intelligence experience\nManagerial experience directly managing people and/or leadership experience leading teams, projects, programs or directing the allocation of resources\nPreferred Qualifications:\nExperience in Global pharmaceutical/biotech forecasting and/or competitive intelligence\nDeep understanding of forecasting methodologies, epidemiology-based models, analog-based forecasting, and CI frameworks\nExperience building and scaling global teams, especially in offshore environments\nStrong background in managing hybrid teams (FTEs and contract workers)\nExposure to vendor/outsourcing models and budget management\nProficiency in analytics and modeling tools (Excel, Power BI, Python, R, etc.)\nExcellent communication and influencing skills with senior collaborators\nStrong project management skills with a track record of delivering impactful insights on time\nExperience supporting pipeline strategy, launch readiness, or global brand teams\nLeadership experience in building and developing high performance teams, delivering results, and shaping the future",Biotechnology,"['Forecasting', 'delivery operations', 'Vendor Management', 'Financial Management', 'Analytics', 'Python', 'Power Bi']",2025-06-12 15:31:06
"Director, Commercial Forecasting and Analytics",Amgen Technology Private Limited,9-14 Years,,Hyderabad,"Responsibilities:\nCapability Builder:Establish and grow Amgen India's center of excellence for Forecasting & CI from the ground up.\nPeople Leadership:Hire, develop, and lead a blended team of full-time employees and contract workers. Foster a high-performance, inclusive, and collaborative culture.\nStrategic Partnering:Act as the key offshore liaison for global Forecasting & CI leadership, partnering across US and global collaborators to deliver critical insights and foresight.\nDelivery Oversight:Ensure on-time, high-quality forecasting models and CI deliverables for pipeline and inline products.\nOperational Excellence:Develop scalable processes and methodologies, implement best practices in forecast modeling, scenario planning, and competitor landscape analysis.\nVendor & Contractor Management:Manage external vendors and contract staff, ensuring high-quality standards, governance, and value delivery.\nInnovation & Technology Enablement:Champion advanced analytics tools, automation, and AI/ML methodologies in forecasting and intelligence.\nStakeholder Engagement:Translate business questions into actionable insights. Effectively communicate findings to senior cross-functional leaders to influence strategic decisions\nFinancial management:Oversee budget associated with offshore work in India, ensuring best negotiated rates and overall value\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients.\nBasic Qualifications\nDoctorate degree and 4 years of pharmaceutical forecasting and/or Competitive Intelligence experience OR\nMaster's degree and 14 to 16 years of pharmaceutical forecasting and/or Competitive Intelligence experience OR\nBachelor's degree and 16 to 18 pharmaceutical forecasting and/or Competitive Intelligence experience\nManagerial experience directly managing people and/or leadership experience leading teams, projects, programs or directing the allocation of resources\nPreferred Qualifications:\nExperience in Global pharmaceutical/biotech forecasting and/or competitive intelligence\nDeep understanding of forecasting methodologies, epidemiology-based models, analog-based forecasting, and CI frameworks\nExperience building and scaling global teams, especially in offshore environments\nStrong background in managing hybrid teams (FTEs and contract workers)\nExposure to vendor/outsourcing models and budget management\nProficiency in analytics and modeling tools (Excel, Power BI, Python, R, etc.)\nExcellent communication and influencing skills with senior collaborators\nStrong project management skills with a track record of delivering impactful insights on time\nExperience supporting pipeline strategy, launch readiness, or global brand teams\nLeadership experience in building and developing high performance teams, delivering results, and shaping the future",Biotechnology,"['Forecasting', 'delivery operations', 'Vendor Management', 'Financial Management', 'Analytics', 'Python', 'Power Bi']",2025-06-12 15:31:07
Ptp Analyst,BCForward,1-2 Years,,Bengaluru,"Purchase Requisition (PR) to Purchase Order (PO) Management: Create and amend POs based on validated PRs, ensuring compliance with company policies.\nInvoice Processing: Review, validate, and process incoming invoices (PO and Non-PO), ensuring accurate coding and booking in accordance with the company's chart of accounts.\nVendor Relationship Management: Serve as the primary point of contact for suppliers, addressing inquiries related to purchase orders, invoices, and payments.\nCompliance and Policy Adherence: Ensure all procurement activities comply with internal controls, company policies, and regulatory requirements.\nReporting and Analysis: Generate and analyze procurement reports, providing insights into spending patterns, supplier performance, and process efficiencies.\nCollaboration with Internal Teams: Work closely with sourcing, supply chain, and finance teams to streamline procurement processes and resolve discrepancies.\nSystem Management: Utilize ERP systems (e.g., SAP, Oracle, Coupa) to manage procurement activities and maintain accurate records.\nContinuous Improvement: Identify opportunities for process improvements and contribute to initiatives aimed at enhancing procurement efficiency.",IT Management,"['purchase requisition', 'Customer Relation', 'sap', 'oracle', 'Coupa', 'Po']",2025-06-12 15:31:09
Associate Data Engineer,Amgen Technology Private Limited,0-2 Years,,Hyderabad,"Experience Required:\n25 years in Data Engineering with knowledge of MDM platforms\nOverview:\nWe are seeking an MDM Associate Data Engineer to support and enhance our enterprise Master Data Management (MDM) platforms using Informatica and/or Reltio. This role is crucial for delivering high-quality master data solutions across the organization by leveraging modern tools such as Databricks and AWS.\nKey Responsibilities:\nAnalyze and manage customer master data using Reltio or Informatica MDM\nPerform advanced SQL queries and data analysis to ensure master data integrity\nUtilize Python, PySpark, and Databricks for scalable data processing and automation\nCollaborate with business and engineering teams to enhance MDM solutions\nImplement data stewardship workflows including approval and DCR mechanisms\nUse AWS cloud services for MDM-related storage and compute processes\nContribute to metadata management and data modeling activities\nTrack and manage data issues using tools like JIRA and document processes in Confluence\nApply Life Sciences/Pharma industry context to ensure data compliance and standards\nQualifications:\nEducation:\nMaster's degree with 13 years of experience\nOR Bachelor's degree with 25 years of experience\nOR Diploma with 68 years of experience\nFields: Business, Engineering, IT, or related disciplines\nMust-Have Skills:\nStrong proficiency in Advanced SQL and data wrangling\nHands-on experience with:\nPython\nPySpark\nDatabricks\nAWS architecture\nKnowledge of MDM concepts, data governance, stewardship, and profiling\nExperience with Informatica MDM or Reltio MDM\nGood-to-Have Skills:\nExperience with Informatica Data Quality (IDQ), data modeling, and approval workflows\nBackground in the Life Sciences / Pharma domain\nFamiliarity with tools like JIRA and Confluence\nSolid grasp of data engineering concepts\nCertifications (Preferred):\nETL certifications (e.g., Informatica)\nData analysis certifications (SQL, Python, Databricks)\nCloud certifications (AWS, Azure)\nSoft Skills:\nStrong analytical and problem-solving abilities\nExcellent verbal and written communication skills\nAbility to explain complex data concepts to both technical and non-technical stakeholders\nTeam player with experience working in global, virtual teams",Biotechnology,"['Data Analysis', 'Etl', 'Informatica', 'Sql', 'Python', 'Cloud', 'Aws', 'data wrangling']",2025-06-12 15:31:10
Associate Data Engineer,Amgen Technology Private Limited,0-2 Years,,Hyderabad,"Experience Required:\n25 years in Data Engineering with knowledge of MDM platforms\nOverview:\nWe are seeking an MDM Associate Data Engineer to support and enhance our enterprise Master Data Management (MDM) platforms using Informatica and/or Reltio. This role is crucial for delivering high-quality master data solutions across the organization by leveraging modern tools such as Databricks and AWS.\nKey Responsibilities:\nAnalyze and manage customer master data using Reltio or Informatica MDM\nPerform advanced SQL queries and data analysis to ensure master data integrity\nUtilize Python, PySpark, and Databricks for scalable data processing and automation\nCollaborate with business and engineering teams to enhance MDM solutions\nImplement data stewardship workflows including approval and DCR mechanisms\nUse AWS cloud services for MDM-related storage and compute processes\nContribute to metadata management and data modeling activities\nTrack and manage data issues using tools like JIRA and document processes in Confluence\nApply Life Sciences/Pharma industry context to ensure data compliance and standards\nQualifications:\nEducation:\nMaster's degree with 13 years of experience\nOR Bachelor's degree with 25 years of experience\nOR Diploma with 68 years of experience\nFields: Business, Engineering, IT, or related disciplines\nMust-Have Skills:\nStrong proficiency in Advanced SQL and data wrangling\nHands-on experience with:\nPython\nPySpark\nDatabricks\nAWS architecture\nKnowledge of MDM concepts, data governance, stewardship, and profiling\nExperience with Informatica MDM or Reltio MDM\nGood-to-Have Skills:\nExperience with Informatica Data Quality (IDQ), data modeling, and approval workflows\nBackground in the Life Sciences / Pharma domain\nFamiliarity with tools like JIRA and Confluence\nSolid grasp of data engineering concepts\nCertifications (Preferred):\nETL certifications (e.g., Informatica)\nData analysis certifications (SQL, Python, Databricks)\nCloud certifications (AWS, Azure)\nSoft Skills:\nStrong analytical and problem-solving abilities\nExcellent verbal and written communication skills\nAbility to explain complex data concepts to both technical and non-technical stakeholders\nTeam player with experience working in global, virtual teams",Biotechnology,"['Data Analysis', 'Etl', 'Informatica', 'Sql', 'Python', 'Cloud', 'Aws', 'data wrangling']",2025-06-12 15:31:11
Business Analyst (US Healthcare / Medicaid) with SQL,BCForward,7-12 Years,,Bengaluru,"Basic Qualifications\nBachelor's degree or equivalent combination of education and experience\n7 to 10 years of Business analysis experience\nBachelor's degree in computer sciences or related field preferred\nExperience working with different database structures (e.g., transaction based vs. data warehouse)\nGood knowledge of SQL is a must\nUS Healthcare experience strongly desired\nFamiliarity with AWS and/or cloud computing services a plus\nKnowledge and/or certifications in software development methodologies such as Scrum is desired, but not required\nOther Qualifications\nAbility to drive project timelines to completion with a cross-functional team.\nMust be familiar with relational database concepts and client-server concepts.\nAbility to communicate both verbally and in writing, fluently in English.\nAbility to develop professional relationships.\nAbility to be a high-impact player on multiple simultaneous engagements\nWillingness to travel\nWork Environment\nClient or office environment may work remotely\nOccasional evening and weekend work",IT Management,"['Business Analyst', 'COMPUTER SCIENECE', 'DATABASE STRUCTURE', 'Sql', 'Database Optimization']",2025-06-12 15:31:12
Data Engineer,Amgen Technology Private Limited,1-4 Years,,Hyderabad,"Let's do this. Let's change the world. We are looking for highly motivated expert Data Engineer who can own the design, development & maintenance of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nDesign, develop, and maintain complex ETL/ELT data pipelines in Databricks using PySpark, Scala, and SQL to process large-scale datasets\nUnderstand the biotech/pharma or related domains & build highly efficient data pipelines to migrate and deploy complex data across systems\nDesign and Implement solutions to enable unified data access, governance, and interoperability across hybrid cloud environments\nIngest and transform structured and unstructured data from databases (PostgreSQL, MySQL, SQL Server, MongoDB etc.), APIs, logs, event streams, images, pdf, and third-party platforms\nEnsuring data integrity, accuracy, and consistency through rigorous quality checks and monitoring\nExpert in data quality, data validation and verification frameworks\nInnovate, explore and implement new tools and technologies to enhance efficient data processing\nProactively identify and implement opportunities to automate tasks and develop reusable frameworks\nWork in an Agile and Scaled Agile (SAFe) environment, collaborating with cross-functional teams, product owners, and Scrum Masters to deliver incremental value\nUse JIRA, Confluence, and Agile DevOps tools to manage sprints, backlogs, and user stories.\nSupport continuous improvement, test automation, and DevOps practices in the data engineering lifecycle\nCollaborate and communicate effectively with the product teams, with cross-functional teams to understand business requirements and translate them into technical solutions\nMust-Have Skills:\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies.\nProficiency in workflow orchestration, performance tuning on big data processing.\nStrong understanding of AWS services\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nData Engineering experience in Biotechnology or pharma industry\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\nMinimum 5 to 8 years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred",Biotechnology,"['Relational Databases', 'Databricks', 'React Js', 'Data Modeling', 'Sql', 'python', 'Java', 'Aws', 'Devops', 'Data Visualization', 'Scaled Agile Framework']",2025-06-12 15:31:14
