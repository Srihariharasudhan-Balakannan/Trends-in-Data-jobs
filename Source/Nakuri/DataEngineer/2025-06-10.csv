job_title,company,experience,salary,locations,description,industry,department,employment_type,skills,scraped_at
Data Engineer,Motorola Solutions,0 - 2 years,Not Disclosed,['Bengaluru'],"Department Overview The Cloud Platform Engineering team is responsible for the development and operations of critical cloud infrastructure, reliability, security and Business operational services, in support of Motorola Solutions public and hybrid cloud-based Software as a Service (SaaS) solutions for public safety customers. This team is part of Motorola Solutions Software Enterprise division, which offers secure, reliable and efficient team communications, workflow and operational intelligence solutions for mission critical public safety and enterprise markets throughout the world. Our services leverage Cloud Computing infrastructure on Azure, AWS and GCP to build at scale.\nJob Description\nDevelop and maintain ETL pipelines using Python, NumPy, Pandas, PySpark, and Apache Airflow.\nDesign and implement ETL solutions for reporting purposes\nServer-side development skills like multithreading, asynchronous IO, databases\nKnowledge of Azure DevOps and Github\nWork with large-scale data processing and transformation workflows.\nOptimize and enhance ETL performance and scalability.\nCollaborate with data engineers and business teams to ensure efficient data flow.\nTroubleshoot and debug ETL-related issues to ensure data integrity and reliability.\nAs a software engineer on this team, you will be a key contributor to platform development activities. Our teams are developing services, tools, and processes to support other Motorola Solutions engineering teams as well as deliver solutions to our customers. You will be working on a high-velocity, results-oriented team that leverages cutting-edge technologies and techniques. The right individual will be motivated and will have a passion for automation, deployment processes and enabling innovation. Your efforts will help to shape the engineering culture and best practices across Motorola Solutions Software Enterprise organization.\n\nBasic Requirements\n0-2 years of Python experience, with good understanding of Python ETL development.\nProficiency in PySpark, Apache Airflow, NumPy, and Pandas.\nProficiency in working with SQL and Bigquery\nStrong problem-solving skills and the ability to work independently.\nProficiency in cloud-based ETL solutions (AWS, GCP, Azure).\nKnowledge of big data technologies like Hadoop, Spark, or Kafka.",Industry Type: Telecom / ISP,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Cloud computing', 'Automation', 'github', 'Multithreading', 'Workflow', 'Data processing', 'Apache', 'Operations', 'SQL', 'Python']",2025-06-10 14:08:52
Data Engineer,Visa,3 - 8 years,Not Disclosed,['Bengaluru'],"We are seeking a Data Engineer with a strong background in data engineering. This role involves managing system requirements, design, development, integration, quality assurance, implementation, and maintenance of corporate applications.\nWork with product owners, business stakeholders and internal teams to understand business requirements and desired business outcomes.\nAssist in scoping and designing analytic data assets, implementing modelled attributes and contributing to brainstorming sessions.\nBuild and maintain a robust data engineering process to develop and implement self-serve data and tools for Visa s product management teams and data scientists.\nFind opportunities to create, automate and scale repeatable analyses or build self-service tools for business users.\nExecute data engineering projects ranging from small to large either individually or as part of a project team.\nSet the benchmark in the team for good data engineering practices and assist leads and architects in solution design.\nExhibit a passion for optimizing existing solutions and making incremental improvements.\nThis is a hybrid position. Expectation of days in office will be confirmed by your hiring manager.\n\n\nBasic Qualification\n-Bachelors degree, OR 3+ years of relevant work experience\n\nPreferred Qualification\n-Minimum of 1 years experience in building data engineering pipelines.\n-Design and coding skills with Big Data technologies like Hadoop, Spark, Hive and Map reduce.\n-Mastery in Pyspark or Scala.\n-Expertise in any programming like Java or Python. Knowing OOP concepts like inheritance, polymorphism and implementing Design Patterns in programming is needed.\n-Experience with cloud platforms like AWS, GCP, or Azure is good to have.\n-Excellent problem-solving skills and ability to think critically.\n-Experience with any one ETL tool like Informatica, SSIS, Pentaho or Azure Data Factory.\n-Knowledge of successful design, and development of data driven real time and batch systems.\n-Experience in data warehousing and an expert in any one of the RDBMS like SQL Server, Oracle, etc.\n-Nice to have reporting skills on PowerBI/Tableau/QlikView.\n-Strong understanding of cloud architecture and service offerings including compute, storage, databases, networking, AI, and ML.\n-Passionate about delivering zero defect code that meets or exceeds the proposed defect SLA and have high sense of accountability for quality and timelines on deliverables.\n-Experience developing as part of Agile/Scrum team is preferred and hands on with Jira.\n-Understanding basic CI/ CD functionality and Git concepts is must.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Manager Quality Assurance', 'Networking', 'RDBMS', 'Coding', 'Informatica', 'Oracle', 'SSIS', 'SQL', 'Python']",2025-06-10 14:08:54
Data Engineer,PwC India,4 - 8 years,Not Disclosed,['Bengaluru'],"If Interested please fill the below application link : https://forms.office.com/r/Zc8wDfEGEH\n\n\nResponsibilities:\nDeliver projects integrating data flows within and across technology systems.\nLead data modeling sessions with end user groups, project stakeholders, and technology teams to produce logical and physical data models.",,,,"['Pyspark', 'Aws Cloud', 'Azure Cloud', 'Python']",2025-06-10 14:08:57
Data Engineer,Capgemini,3 - 5 years,Not Disclosed,['Pune'],"Capgemini Invent\n\nCapgemini Invent is the digital innovation, consulting and transformation brand of the Capgemini Group, a global business line that combines market leading expertise in strategy, technology, data science and creative design, to help CxOs envision and build whats next for their businesses.\n\nYour Role\nHas data pipeline implementation experience with any of these cloud providers - AWS, Azure, GCP.\nExperience with cloud storage, cloud database, cloud data warehousing and Data Lake solutions like Snowflake, Big query, AWS Redshift, ADLS, S3.\nHas good knowledge of cloud compute services and load balancing.\nHas good knowledge of cloud identity management, authentication and authorization.\nProficiency in using cloud utility functions such as AWS lambda, AWS step functions, Cloud Run, Cloud functions, Azure functions.\nExperience in using cloud data integration services for structured, semi structured and unstructured data such as Azure Databricks, Azure Data Factory, Azure Synapse Analytics, AWS Glue, AWS EMR, Dataflow, Dataproc.\nYour Profile\nGood knowledge of Infra capacity sizing, costing of cloud services to drive optimized solution architecture, leading to optimal infra investment vs performance and scaling.\nAble to contribute to making architectural choices using various cloud services and solution methodologies.\nExpertise in programming using python.\nVery good knowledge of cloud Dev-ops practices such as infrastructure as code, CI/CD components, and automated deployments on cloud.\nMust understand networking, security, design principles and best practices in cloud.\nWhat you will love about working here\nWe recognize the significance of flexible work arrangements to provide support. Be it remote work, or flexible work hours, you will get an environment to maintain healthy work life balance.\nAt the heart of our mission is your career growth. Our array of career growth programs and diverse professions are crafted to support you in exploring a world of opportunities.\nEquip yourself with valuable certifications in the latest technologies such as Generative AI.\nAbout Capgemini\n\nCapgemini is a global business and technology transformation partner, helping organizations to accelerate their dual transition to a digital and sustainable world, while creating tangible impact for enterprises and society. It is a responsible and diverse group of 340,000 team members in more than 50 countries. With its strong over 55-year heritage, Capgemini is trusted by its clients to unlock the value of technology to address the entire breadth of their business needs. It delivers end-to-end services and solutions leveraging strengths from strategy and design to engineering, all fueled by its market leading capabilities in AI, cloud and data, combined with its deep industry expertise and partner ecosystem. The Group reported 2023 global revenues of 22.5 billion.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'networking', 'ops', 'design principles', 'aws', 'azure databricks', 'snowflake', 'azure data lake', 'glue', 'amazon redshift', 'azure synapse', 'microsoft azure', 'data warehousing', 'azure data factory', 'emr', 'aws lambda', 'data engineering', 'dataproc', 'aws glue', 'gcp', 'bigquery', 'data flow']",2025-06-10 14:09:00
Data Engineer,Motorola Solutions,2 - 3 years,Not Disclosed,['Bengaluru'],"Department Overview The Cloud Platform Engineering team is responsible for the development and operations of critical cloud infrastructure, reliability, security and Business operational services, in support of Motorola Solutions public and hybrid cloud-based Software as a Service (SaaS) solutions for public safety customers. This team is part of Motorola Solutions Software Enterprise division, which offers secure, reliable and efficient team communications, workflow and operational intelligence solutions for mission critical public safety and enterprise markets throughout the world. Our services leverage Cloud Computing infrastructure on Azure, AWS and GCP to build at scale.\nJob Description\nDevelop and maintain ETL pipelines using Python, NumPy, Pandas, PySpark, and Apache Airflow.\nDesign and implement ETL solutions for reporting purposes\nServer-side development skills like multithreading, asynchronous IO, databases\nKnowledge of Azure DevOps and Github\nWork with large-scale data processing and transformation workflows.\nOptimize and enhance ETL performance and scalability.\nCollaborate with data engineers and business teams to ensure efficient data flow.\nTroubleshoot and debug ETL-related issues to ensure data integrity and reliability.\nAs a software engineer on this team, you will be a key contributor to platform development activities. Our teams are developing services, tools, and processes to support other Motorola Solutions engineering teams as well as deliver solutions to our customers. You will be working on a high-velocity, results-oriented team that leverages cutting-edge technologies and techniques. The right individual will be motivated and will have a passion for automation, deployment processes and enabling innovation. Your efforts will help to shape the engineering culture and best practices across Motorola Solutions Software Enterprise organization.\n\nBasic Requirements\n3+ years of Python experience, with 2+ years dedicated to Python ETL development.\nProficiency in PySpark, Apache Airflow, NumPy, and Pandas.\nExperience in working with SQL and Bigquery\nStrong problem-solving skills and the ability to work independently.\nExperience in cloud-based ETL solutions (AWS, GCP, Azure).\nKnowledge of big data technologies like Hadoop, Spark, or Kafka.",Industry Type: Telecom / ISP,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Cloud computing', 'Automation', 'github', 'Multithreading', 'Workflow', 'Data processing', 'Apache', 'Operations', 'SQL', 'Python']",2025-06-10 14:09:02
Data Engineer - Hadoop,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Wipro Limited (NYSEWIT, BSE507685, NSEWIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients’ most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.\n About The Role  _x000D_\n\n \n\nRole Purpose  \nThe purpose of the role is to support process delivery by ensuring daily performance of the Production Specialists, resolve technical escalations and develop technical capability within the Production Specialists.\n\n\n ? _x000D_\n\n \n\nDo  \n\n\nOversee and support process by reviewing daily transactions on performance parameters\nReview performance dashboard and the scores for the team\nSupport the team in improving performance parameters by providing technical support and process guidance\nRecord, track, and document all queries received, problem-solving steps taken and total successful and unsuccessful resolutions\nEnsure standard processes and procedures are followed to resolve all client queries\nResolve client queries as per the SLA’s defined in the contract\nDevelop understanding of process/ product for the team members to facilitate better client interaction and troubleshooting\nDocument and analyze call logs to spot most occurring trends to prevent future problems\nIdentify red flags and escalate serious client issues to Team leader in cases of untimely resolution\nEnsure all product information and disclosures are given to clients before and after the call/email requests\nAvoids legal challenges by monitoring compliance with service agreements\n\n\n ? _x000D_\n\n\nHandle technical escalations through effective diagnosis and troubleshooting of client queries\nManage and resolve technical roadblocks/ escalations as per SLA and quality requirements\nIf unable to resolve the issues, timely escalate the issues to TA & SES\nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers’ and clients’ business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract SLA’s\n\n\n ? _x000D_\n\n\nBuild people capability to ensure operational excellence and maintain superior customer service levels of the existing account/client\nMentor and guide Production Specialists on improving technical knowledge\nCollate trainings to be conducted as triage to bridge the skill gaps identified through interviews with the Production Specialist\nDevelop and conduct trainings (Triages) within products for production specialist as per target\nInform client about the triages being conducted\nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nIdentify and document most common problems and recommend appropriate resolutions to the team\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\n\n ? _x000D_\n\nDeliver\nNoPerformance ParameterMeasure1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback, NSAT/ ESAT2Team ManagementProductivity, efficiency, absenteeism3Capability developmentTriages completed, Technical Test performance\n\nMandatory\n\nSkills:\nHadoop_x000D_.\n\nExperience5-8 Years_x000D_.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'data engineering', 'spark', 'troubleshooting', 'hadoop', 'cloudera', 'python', 'scala', 'big data analytics', 'oozie', 'airflow', 'pyspark', 'data warehousing', 'apache pig', 'machine learning', 'sql', 'mapreduce', 'sqoop', 'big data', 'aws', 'etl', 'hbase']",2025-06-10 14:09:05
Data Engineer-Data Modelling,IBM,2 - 5 years,Not Disclosed,['Pune'],"Translates business needs into a data model, providing expertise on data modeling tools and techniques for designing data models for applications and related systems.\nSkills include logical and physical data modeling, and knowledge of ERWin, MDM, and/or ETL.\nData modeling is a process used to define and analyze data requirements needed to support the business processes within the scope of corresponding information systems in organizations\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nTranslates business needs into a data model, providing expertise on data modeling tools and techniques for designing data models for applications and related systems.\nSkills include logical and physical data modeling, and knowledge of ERWin, MDM, and/or ETL.\nData modeling is a process used to define and analyze data requirements needed to support the business processes within the scope of corresponding information systems in organizations.\nTherefore, the process of data modeling involves professional data modelers working closely with business stakeholders, as well as potential users of the information system.\nThere are three different types of data models produced while progressing from requirements to the actual database to be used for the information system.\n\n\nPreferred technical and professional experience\nTranslates business needs into a data model, providing expertise on data modeling tools and techniques for designing data models for applications and related systems.\nSkills include logical and physical data modeling, and knowledge of ERWin, MDM, and/or ETL.\nData modeling is a process used to define and analyze data requirements needed to support the business processes within the scope of corresponding information systems in organizations.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['erwin', 'data engineering', 'data modeling', 'mdm', 'etl', 'hive', 'python', 'oracle', 'data analysis', 'data warehousing', 'power bi', 'microsoft azure', 'business intelligence', 'sql server', 'sql', 'plsql', 'tableau', 'unix shell scripting', 'spark', 'hadoop', 'big data', 'aws', 'informatica', 'unix']",2025-06-10 14:09:08
Data Engineer - Marketing,Kimberly-Clark Corporation,7 - 12 years,Not Disclosed,['Bengaluru'],"Our Data Engineers play a crucial role in designing and operationalizing transformational enterprise data solutions on Cloud Platforms, integrating Azure services, Snowflake technology, and other third-party data technologies.\nCloud Data Engineers will work closely with a multidisciplinary agile team to build high-quality data pipelines that drive analytic solutions. These solutions will generate insights from our connected data, enabling Kimberly-Clark to advance its data-driven decision-making capabilities.\nThe ideal candidate will have a deep understanding of data architecture, data engineering, data warehousing, data analysis, reporting, and data science techniques and workflows. They should be skilled in creating data products that support analytic solutions and possess proficiency in working with APIs and understanding data structures to serve them.",,,,"['Performance tuning', 'Data modeling', 'RDBMS', 'Coding', 'Flex', 'Agile', 'Data structures', 'Troubleshooting', 'SQL', 'Python']",2025-06-10 14:09:10
AZURE DATA ENGINEER,Capgemini,4 - 9 years,Not Disclosed,['Bengaluru'],"Your Role \n\nAs a senior software engineer with Capgemini, you should have 4 + years of experience in Azure Data Engineer with strong project track record\n\nIn this role you will play a key role in\nStrong customer orientation, decision making, problem solving, communication and presentation skills\nVery good judgement skills and ability to shape compelling solutions and solve unstructured problems with assumptions\nVery good collaboration skills and ability to interact with multi-cultural and multi-functional teams spread across geographies\nStrong executive presence andspirit\nSuperb leadership and team building skills with ability to build consensus and achieve goals through collaboration rather than direct line authority\n\n\n Your Profile \n\nExperience with Azure Data Bricks, Data Factory\nExperience with Azure Data components such as Azure SQL Database, Azure SQL Warehouse, SYNAPSE Analytics\nExperience in Python/Pyspark/Scala/Hive Programming.\nExperience with Azure Databricks/ADB\nExperience with building CI/CD pipelines in Data environments\n\n\n\nPrimary Skills\nADF (Azure Data Factory) OR\nADB (Azure Data Bricks)\n\n\n\nSecondary Skills\nExcellent verbal and written communication and interpersonal skills\n\n Skills (competencies) \n\nAb Initio\nAgile (Software Development Framework)\nApache Hadoop\nAWS Airflow\nAWS Athena\nAWS Code Pipeline\nAWS EFS\nAWS EMR\nAWS Redshift\nAWS S3\nAzure ADLS Gen2\nAzure Data Factory\nAzure Data Lake Storage\nAzure Databricks\nAzure Event Hub\nAzure Stream Analytics\nAzure Sunapse\nBitbucket\nChange Management\nClient Centricity\nCollaboration\nContinuous Integration and Continuous Delivery (CI/CD)\nData Architecture Patterns\nData Format Analysis\nData Governance\nData Modeling\nData Validation\nData Vault Modeling\nDatabase Schema Design\nDecision-Making\nDevOps\nDimensional Modeling\nGCP Big Table\nGCP BigQuery\nGCP Cloud Storage\nGCP DataFlow\nGCP DataProc\nGit\nGoogle Big Tabel\nGoogle Data Proc\nGreenplum\nHQL\nIBM Data Stage\nIBM DB2\nIndustry Standard Data Modeling (FSLDM)\nIndustry Standard Data Modeling (IBM FSDM))\nInfluencing\nInformatica IICS\nInmon methodology\nJavaScript\nJenkins\nKimball\nLinux - Redhat\nNegotiation\nNetezza\nNewSQL\nOracle Exadata\nPerformance Tuning\nPerl\nPlatform Update Management\nProject Management\nPySpark\nPython\nR\nRDD Optimization\nSantOs\nSaS\nScala Spark\nShell Script\nSnowflake\nSPARK\nSPARK Code Optimization\nSQL\nStakeholder Management\nSun Solaris\nSynapse\nTalend\nTeradata\nTime Management\nUbuntu\nVendor Management",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data factory', 'oracle adf', 'azure databricks', 'presentation skills', 'decision making', 'hive', 'continuous integration', 'scala', 'ci/cd', 'pyspark', 'sql', 'spark', 'gcp', 'linux', 'jenkins', 'shell scripting', 'hadoop', 'bigquery', 'adb', 'snowflake', 'python', 'customer care', 'javascript', 'athena', 'aws']",2025-06-10 14:09:12
Data Engineer,Grid Dynamics,4 - 6 years,Not Disclosed,['Bengaluru'],"We are seeking a skilled Data Engineer & Data Analyst with over 4 years of experience to design, build, and maintain scalable data pipelines and perform advanced data analysis to support business intelligence and data-driven decision-making. The ideal candidate will have a strong foundation in computer science principles, extensive experience with SQL and big data tools, and proficiency in cloud platforms and data visualization tools.\nKey Responsibilities:\nDesign, develop, and maintain robust, scalable ETL pipelines using Apache Airflow, DBT, Composer (GCP), Control-M, Cron, Luigi, and similar tools.\nBuild and optimize data architectures including data lakes and data warehouses.\nIntegrate data from multiple sources ensuring data quality and consistency.\nCollaborate with data scientists, analysts, and stakeholders to translate business requirements into technical solutions.\nAnalyze complex datasets to identify trends, generate actionable insights, and support decision-making.\nDevelop and maintain dashboards and reports using Tableau, Power BI, and Jupyter Notebooks for visualization and pipeline validation.\nManage and optimize relational and NoSQL databases such as MySQL, PostgreSQL, Oracle, MongoDB, and DynamoDB.\nWork with big data tools and frameworks including Hadoop, Spark, Hive, Kafka, Informatica, Talend, SSIS, and Dataflow.\nUtilize cloud data services and warehouses like AWS Glue, GCP Dataflow, Azure Data Factory, Snowflake, Redshift, and BigQuery.\nSupport CI/CD pipelines and DevOps workflows using Git, Docker, Terraform, and related tools.\nEnsure data governance, security, and compliance standards are met.\nParticipate in Agile and DevOps processes to enhance data engineering workflows.\n\nRequired Qualifications:\n4+ years of professional experience in data engineering and data analysis roles.\nStrong proficiency in SQL and experience with database management systems such as MySQL, PostgreSQL, Oracle, and MongoDB.\nHands-on experience with big data tools like Hadoop and Apache Spark.\nProficient in Python programming.\nExperience with data visualization tools such as Tableau, Power BI, and Jupyter Notebooks.\nProven ability to design, build, and maintain scalable ETL pipelines using tools like Apache Airflow, DBT, Composer (GCP), Control-M, Cron, and Luigi.\nFamiliarity with data engineering tools including Hive, Kafka, Informatica, Talend, SSIS, and Dataflow.\nExperience working with cloud data warehouses and services (Snowflake, Redshift, BigQuery, AWS Glue, GCP Dataflow, Azure Data Factory).\nUnderstanding of data modeling concepts and data lake/data warehouse architectures.\nExperience supporting CI/CD practices with Git, Docker, Terraform, and DevOps workflows.\nKnowledge of both relational and NoSQL databases, including PostgreSQL, BigQuery, MongoDB, and DynamoDB.\nExposure to Agile and DevOps methodologies.\nExperience with at least one cloud platform:\nGoogle Cloud Platform (BigQuery, Dataflow, Composer, Cloud Storage, Pub/Sub)\nAmazon Web Services (S3, Glue, Redshift, Lambda, Athena)\nMicrosoft Azure (Data Factory, Synapse Analytics, Blob Storage)\n\nPreferred Skills:\nStrong problem-solving and communication skills.\nAbility to work independently and collaboratively in a team environment.\nExperience with service development, REST APIs, and automation testing is a plus.\nFamiliarity with version control systems and workflow automation.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['SQL', 'Pyspark', 'Azure', 'Data Engineering', 'Amazon Web Services', 'GCP', 'Hadoop', 'Spark', 'Google Cloud Platforms', 'AWS', 'Python']",2025-06-10 14:09:15
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Hyderabad'],"Project Role :Data Engineer\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\nMust have skills :Data Engineering\n\n\nGood to have skills :NAMinimum\n\n12 year(s) of experience is required\n\n\nEducational Qualification :15 years full time education\nSummary:seeking a hands-on Senior Engineering Manager of Data Platform to spearhead the development of capabilities that power Vertex products while providing a connected experience for our customers. This role demands a deep engineering background with hands-on experience in building and scaling production-level systems. The ideal candidate will excel in leading teams to deliver high-quality data products and will provide mentorship, guidance, and leadership.In this role, you will work to increase the domain data coverage and adoption of the Data Platform by promoting a connected user experience through data. You will increase data literacy and trust by leading our Data Governance and Master Data Management initiatives. You will contribute to the vision and roadmap of self-serve capabilities through the Data Platform.\nRoles & Responsibilities:Be hands-on in leading the development of features that enhance self-service capabilities of our data platform, ensuring the platform is scalable, reliable, and fully aligned with business objectives, and defining and implementing best practices in data architecture, data modeling, and data governance.Work closely with Product, Engineering, and other departments to ensure the data platform meets business requirements.Influence cross-functional initiatives related to data tools, governance, and cross-domain data sharing. Ensure technical designs are thoroughly evaluated and aligned with business objectives.Determine appropriate recruiting of staff to achieve goals and objectives. Interview, recruit, develop and retain top talent.Manage and mentor a team of engineers, fostering a collaborative and high-performance culture, and encouraging a growth mindset and accountability for outcomes. Interpret how the business strategy links to individual roles and responsibilities.Provide career development opportunities and establish processes and practices for knowledge sharing and communication.Partner with external vendors to address issues, and technical challenges.Stay current with emerging technologies and industry trends in field to ensure the platform remains cutting-edge.\nProfessional & Technical\n\n\nSkills:\n12+ years of hands-on experience in software development (preferably in the data space), with 3+ years of people management experience, demonstrating success in building, growing, and managing multiple teams.Extensive experience in architecting and building complex data platforms and products. In-depth knowledge of cloud-based services and data tools such as Snowflake, AWS, Azure, with expertise in data ingestion, normalization, and modeling.Strong experience in building and scaling production-level cloud-based data systems utilizing data ingestion tools like Fivetran, Data Quality and Observability tools like Monte Carlo, Data Catalog like Atlan and Master Data tools like Reltio or Informatica.Thorough understanding of best practices regarding agile software development and software testing.Experience of deploying cloud-based applications using automated CI/CD processes and container technologies.Understanding of security best practices when architecting SaaS applications on cloud Infrastructure.Ability to understand complex business systems and a willingness to learn and apply new technologies as needed.Proven ability to influence and deliver high-impact initiatives. Forward-thinking mindset with the ability to define and drive the teams mission, vision, and long-term strategies.Excellent leadership skills with a track record of managing teams and collaborating effectively across departments. Strong written and communication skills.Proven ability to work with and lead remote teams to achieve sustainable long-term success.Work together and Get Stuff Done attitude without losing sight of quality, and a sense of responsibility to customers and the team.\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Data Engineering.- This position is based at our Hyderabad office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['architecting', 'software testing', 'data engineering', 'data ingestion', 'agile software development', 'continuous integration', 'snowflake', 'software development', 'microsoft azure', 'reltio', 'data architecture', 'data quality', 'data modeling', 'data governance', 'aws', 'etl', 'informatica']",2025-06-10 14:09:18
Data Engineer,Grid Dynamics,5 - 10 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Job Summary:\nWe are looking for a skilled Data Engineer to join our team. The ideal candidate will be responsible for building and maintaining scalable data pipelines and ensuring efficient data flow across systems. Youll work closely with data scientists, analysts, and business stakeholders to enable data-driven decision-making.\n\nKey Responsibilities:\nDesign, build, and maintain scalable and reliable data pipelines.\nDevelop and optimize ETL (Extract, Transform, Load) workflows.\nIntegrate data from various sources including APIs, databases, and third-party services.\nEnsure data quality, integrity, and consistency across platforms.\nImplement data warehousing solutions using tools like Snowflake, Redshift, or BigQuery.\nCollaborate with data analysts and scientists to support analytics and reporting needs.\nMonitor and troubleshoot data pipelines to ensure performance and reliability.\nDocument data architecture, processes, and standards.\nRequired Skills & Qualifications:\nBachelors or Masters degree in Computer Science, Engineering, or related field.\n5+ years of experience in data engineering or related roles.\nStrong proficiency in SQL and experience with relational and non-relational databases (e.g., PostgreSQL, MongoDB).\nExperience with big data tools (Hadoop, Spark) and cloud platforms (AWS, GCP, Azure).\nProficient in programming languages such as Python, Scala, or Java.\nExperience with workflow orchestration tools like Apache Airflow, Luigi, or Prefect.\nFamiliarity with data modeling, data warehousing, and architecture best practices.\nPreferred Qualifications:\nExperience with containerization tools (Docker, Kubernetes).\nKnowledge of CI/CD pipelines and version control systems (e.g., Git).\nExposure to real-time data processing (Kafka, Flink, etc.).\nKnowledge of data privacy and governance practices.",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['Data Engineering', 'Big Data']",2025-06-10 14:09:20
Data Engineer,DXC Technology,3 - 5 years,Not Disclosed,['Bengaluru'],"Job Description:\nSr. Snowflake, DBT, SQL Developer:\nWe are looking for an experienced senior Backend developer (Snowflake, DBT and SQL) The person should have proven hands-on experience in Snowflake, DBT and SQL. Azure and DBT is always an added advantage as Nestle is using DBT for Transformation.\nThe overall experience of 7+ years in SQL and with minimum 3-5 years of experience in Snowflake.\nExperience with SnowFlake data warehouse\nExperience with Data Ingestion into SnowFlake such as Snowpipe and DBT\nDBT Development: Design, develop, and maintain DBT models, transformations, and SQL code to build efficient data pipelines for analytics and reporting.\nGood understanding of SnowFlake architecture and processing\nHands-on experience in Snowflake Cloud Development\nExperience in writing complicated SQLs, analyzing query performance, query tuning, database indexes partitions, and stored procedure development.\nImplementing ETL pipelines within and outside of a data warehouse using Python and Snowflakes Snow SQL\nQuerying Snowflake using SQL\ngood knowledge of SQL language and data warehousing concepts,\no Experience with technologies such as SQL Server 2008, as well as with newer ones like SSIS and stored procedures\no Designing database tables and structures.\no Creating views, functions, and stored procedures.\no Writing optimized SQL queries for integration with other applications.\no Creating database triggers for use in automation.\no Maintaining data quality and overseeing database security.\no Exceptional experience developing codes, testing for quality assurance, administering RDBMS, and monitoring of database\no Strong knowledge and experience with relational databases and database administration (indexes, optimization, etc. )\no Querying Snowflake using SQL, Experience with SQL and PLSQL, SQL query tuning, database performance tuning and data warehousing concepts, Knowledge of DBT and Azure (ADF) is desirable.\nRecruitment fraud is a scheme in which fictitious job opportunities are offered to job seekers typically through online services, such as false websites, or through unsolicited emails claiming to be from the company. These emails may request recipients to provide personal information or to make payments as part of their illegitimate recruiting process. DXC does not make offers of employment via social media networks and DXC never asks for any money or payments from applicants at any point in the recruitment process, nor ask a job seeker to purchase IT or other equipment on our behalf. More information on employment scams is available here .",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Automation', 'Manager Quality Assurance', 'RDBMS', 'Database administration', 'Stored procedures', 'SSIS', 'Analytics', 'Monitoring', 'Python']",2025-06-10 14:09:23
Senior GenAI Data Engineer,"NTT DATA, Inc.",2 - 5 years,Not Disclosed,"['New Delhi', 'Chennai', 'Bengaluru']","Your day at NTT DATA\nSenior GenAI Data Engineer\nWe are seeking an experienced Senior Data Engineer to join our team in delivering cutting-edge Generative AI (GenAI) solutions to clients. The successful candidate will be responsible for designing, developing, and deploying data pipelines and architectures that support the training, fine-tuning, and deployment of LLMs for various industries. This role requires strong technical expertise in data engineering, problem-solving skills, and the ability to work effectively with clients and internal teams.\nWhat you'll be doing\nKey Responsibilities:\nDesign, develop, and manage data pipelines and architectures to support GenAI model training, fine-tuning, and deployment\nData Ingestion and Integration: Develop data ingestion frameworks to collect data from various sources, transform, and integrate it into a unified data platform for GenAI model training and deployment.\nGenAI Model Integration: Collaborate with data scientists to integrate GenAI models into production-ready applications, ensuring seamless model deployment, monitoring, and maintenance.\nCloud Infrastructure Management: Design, implement, and manage cloud-based data infrastructure (e.g., AWS, GCP, Azure) to support large-scale GenAI workloads, ensuring cost-effectiveness, security, and compliance.\nWrite scalable, readable, and maintainable code using object-oriented programming concepts in languages like Python, and utilize libraries like Hugging Face Transformers, PyTorch, or TensorFlow\nPerformance Optimization: Optimize data pipelines, GenAI model performance, and infrastructure for scalability, efficiency, and cost-effectiveness.\nData Security and Compliance: Ensure data security, privacy, and compliance with regulatory requirements (e.g., GDPR, HIPAA) across data pipelines and GenAI applications.\nClient Collaboration: Collaborate with clients to understand their GenAI needs, design solutions, and deliver high-quality data engineering services.\nInnovation and R&D: Stay up to date with the latest GenAI trends, technologies, and innovations, applying research and development skills to improve data engineering services.\nKnowledge Sharing: Share knowledge, best practices, and expertise with team members, contributing to the growth and development of the team.\nRequirements:\nBachelors degree in computer science, Engineering, or related fields (Master's recommended)\nExperience with vector databases (e.g., Pinecone, Weaviate, Faiss, Annoy) for efficient similarity search and storage of dense vectors in GenAI applications\n5+ years of experience in data engineering, with a strong emphasis on cloud environments (AWS, GCP, Azure, or Cloud Native platforms)\nProficiency in programming languages like SQL, Python, and PySpark\nStrong data architecture, data modeling, and data governance skills\nExperience with Big Data Platforms (Hadoop, Databricks, Hive, Kafka, Apache Iceberg), Data Warehouses (Teradata, Snowflake, BigQuery), and lakehouses (Delta Lake, Apache Hudi)\nKnowledge of DevOps practices, including Git workflows and CI/CD pipelines (Azure DevOps, Jenkins, GitHub Actions)\nExperience with GenAI frameworks and tools (e.g., TensorFlow, PyTorch, Keras)\nNice to have:\nExperience with containerization and orchestration tools like Docker and Kubernetes\nIntegrate vector databases and implement similarity search techniques, with a focus on GraphRAG is a plus\nFamiliarity with API gateway and service mesh architectures\nExperience with low latency/streaming, batch, and micro-batch processing\nFamiliarity with Linux-based operating systems and REST APIs\nLocation: Delhi or Bangalore\nWorkplace type:\nHybrid Working",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GenAI', 'hive', 'continuous integration', 'kubernetes', 'ci/cd', 'pyspark', 'data architecture', 'sql', 'docker', 'tensorflow', 'git', 'data modeling', 'gcp', 'devops', 'linux', 'jenkins', 'pytorch', 'keras', 'hadoop', 'bigquery', 'python', 'microsoft azure', 'data engineering', 'data bricks', 'data governance', 'aws']",2025-06-10 14:09:25
Starburst Data Engineer/ Architect,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Starburst Data Engineer/Architect \nExpertise in Starburst and policy management like Ranger or equivalent.\nIn-depth knowledge of data modelling principles and techniques, including relational and dimensional.\nExcellent problem solving skills and the ability to troubleshoot and debug complex data related issues.\nStrong awareness of data tools and platforms like: Starburst, Snowflakes, Databricks and programming languages like SQL.\nIn-depth knowledge of data management principles, methodologies, and best practices with excellent analytical, problem-solving and decision making skills.\nDevelop, implement and maintain database systems using SQL.\nWrite complex SQL queries for integration with applications.\nDevelop and maintain data models (Conceptual, physical and logical) to meet organisational needs.\n\n\n ? \n\nDo\n\n1. Managing the technical scope of the project in line with the requirements at all stages\n\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\n\nb. Develop record management process and policies\n\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\n\nd. Providing sales data, proposals, data insights and account reviews to the client base\n\ne. Identify areas to increase efficiency and automation of processes\n\nf. Set up and maintain automated data processes\n\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\n\nh. Produce and track key performance indicators\n\n\n\n2. Analyze the data sets and provide adequate information\n\na. Liaise with internal and external clients to fully understand data content\n\nb. Design and carry out surveys and analyze survey data as per the customer requirement\n\nc. Analyze and interpret complex data sets relating to customer??s business and prepare reports for internal and external audiences using business analytics reporting tools\n\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\n\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\n\nf. Develop predictive models and share insights with the clients as per their requirement\n\n ? \n\nDeliver\nNoPerformance ParameterMeasure1.Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\n\n ? \n\n ? \nMandatory\n\nSkills:\nStartburst.\n\nExperience5-8 Years.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data management', 'sql', 'data bricks', 'data modeling', 'policy management', 'hive', 'snowflake', 'python', 'data mining', 'data warehousing', 'power bi', 'dbms', 'data architecture', 'sql server', 'plsql', 'tableau', 'unix shell scripting', 'spark', 'hadoop', 'etl', 'ssis', 'data integration', 'informatica']",2025-06-10 14:09:27
Data Engineer,Accenture,12 - 17 years,Not Disclosed,['Hyderabad'],"Project Role :Data Engineer\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\nMust have skills :Talend ETL\n\n\n\nGood to have skills :NA\nMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer Lead, you will design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems. A typical day involves working on data solutions and ETL processes.\nRoles & Responsibilities:\nExpected to be an SME.\nCollaborate and manage the team to perform.\nResponsible for team decisions.\nEngage with multiple teams and contribute on key decisions.\nExpected to provide solutions to problems that apply across multiple teams.\nLead data architecture design.\nImplement data integration solutions.\nOptimize ETL processes.\nProfessional & Technical\n\n\n\n\nSkills:\nMust To Have\n\n\n\n\nSkills:\nProficiency in Talend ETL.\nStrong understanding of data modeling.\nExperience with SQL and database management.\nKnowledge of cloud platforms like AWS or Azure.\nHands-on experience with data warehousing.\nGood To Have\n\n\n\n\nSkills:\nExperience with data visualization tools.\nAdditional Information:\nThe candidate should have a minimum of 12 years of experience in Talend ETL.\nThis position is based at our Hyderabad office.\nA 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data warehousing', 'sql', 'data modeling', 'talend etl', 'etl', 'hive', 'python', 'amazon redshift', 'talend', 'microsoft azure', 'pyspark', 'data architecture', 'data engineering', 'data quality', 'database management', 'spark', 'data visualization', 'hadoop', 'aws', 'big data', 'informatica', 'etl process']",2025-06-10 14:09:30
Data Engineer,Medtronic,4 - 7 years,Not Disclosed,['Pune'],"A Day in the Life\nWere a mission-driven leader in medical technology and solutions with a legacy of integrity and innovation, join our new Minimed India Hub as Digital Engineer. We are working to improve how healthcare addresses the needs of more people, in more ways and in more places around the world. As a PySpark Data Engineer, you will be responsible for designing, developing, and maintaining data pipelines using PySpark. You will work closely with data scientists, analysts, and other stakeholders to ensure the efficient processing and analysis of large datasets, while handling complex transformations and aggregations.",,,,"['Pyspark', 'SQL', 'Python', 'Data Engineering', 'Hadoop', 'Kafka', 'Hive', 'GCP', 'SCALA', 'Microsoft Azure', 'Spark', 'Data Warehousing', 'AWS']",2025-06-10 14:09:32
Data Engineer(Spark/Scala/Cloudera,Photon,6 - 9 years,Not Disclosed,"['Pune', 'Chennai']","Job Title: Data Engineer (Spark/Scala/Cloudera)\nLocation: Chennai/Pune\nJob Type: Full time\nExperience Level: 6- 9 years\nJob Summary:\n\nWe are seeking a skilled and motivated Data Engineer to join our data engineering team. The ideal candidate will have deep experience with Apache Spark, Scala, and Cloudera Hadoop ecosystem. You will be responsible for building scalable data pipelines, optimizing data processing workflows, and ensuring the reliability and performance of our big data platform.",,,,"['Data Engineering', 'Scala Programming', 'Cloudera', 'Spark Programming', 'Big Data']",2025-06-10 14:09:34
Data Engineer,Blend360 India,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Description\nWe are looking for an experienced Senior Data Engineer with a strong foundation in Python, SQL, and Spark , and hands-on expertise in AWS, Databricks . In this role, you will build and maintain scalable data pipelines and architecture to support analytics, data science, and business intelligence initiatives. You ll work closely with cross-functional teams to drive data reliability, quality, and performance.\nResponsibilities:\nDesign, develop, and optimize scalable data pipelines using Databricks in AWS such as Glue, S3, Lambda, EMR, Databricks notebooks, workflows and jobs.\nBuilding data lake in WS Databricks.\nBuild and maintain robust ETL/ELT workflows using Python and SQL to handle structured and semi-structured data.\nDevelop distributed data processing solutions using Apache Spark or PySpark .\nPartner with data scientists and analysts to provide high-quality, accessible, and well-structured data.\nEnsure data quality, governance, security, and compliance across pipelines and data stores.\nMonitor, troubleshoot, and improve the performance of data systems and pipelines.\nParticipate in code reviews and help establish engineering best practices.\nMentor junior data engineers and support their technical development.\n\n\nQualifications\nRequirements\nBachelors or masters degree in computer science, Engineering, or a related field.\n5+ years of hands-on experience in data",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Version control', 'GIT', 'Workflow', 'Data quality', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-10 14:09:37
Data Engineer,Accenture,5 - 10 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\nMust have skills :SAP HCM On Premise ABAP\n\n\nGood to have skills :SAP HCM Organizational Management\nMinimum\n\n5 year(s) of experience is required\n\n\nEducational Qualification :15 years full time education\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems. Be involved in the end-to-end data management process.\nRoles & Responsibilities:\nExpected to be an SME.\nCollaborate and manage the team to perform.\nResponsible for team decisions.\nEngage with multiple teams and contribute on key decisions.\nProvide solutions to problems for their immediate team and across multiple teams.\nDevelop and maintain data pipelines.\nEnsure data quality and integrity.\nImplement ETL processes for data migration and deployment.\nProfessional & Technical Skills:\nMust To Have Skills:Proficiency in SAP HCM On Premise ABAP.\nGood To Have Skills:Experience with SAP HCM Organizational Management.\nStrong understanding of data management principles.\nExperience in designing and implementing data solutions.\nProficient in ETL processes and data migration techniques.\nAdditional Information:\nThe candidate should have a minimum of 5 years of experience in SAP HCM On Premise ABAP.\nThis position is based at our Bengaluru office.\nA 15 years full-time education is required.\n\n\nQualifications\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data management', 'data migration', 'sap hcm', 'abap', 'etl process', 'hive', 'python', 'data analysis', 'oracle', 'data warehousing', 'data engineering', 'business intelligence', 'sql server', 'sql', 'plsql', 'data quality', 'data modeling', 'spark', 'hadoop', 'etl', 'aws', 'big data', 'informatica', 'unix']",2025-06-10 14:09:39
Associate Analyst - Data Engineer,Pepsico,2 - 7 years,Not Disclosed,['Hyderabad'],"Overview\n\nPepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT. The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCo s global business scale to enable business insights, advanced analytics, and new product development. PepsiCo s Data Management and Operations team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations, and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.\nMaintain a predictable, transparent, global operating rhythm that ensures always-on access to high-quality data for stakeholders across the company.\nResponsible for day-to-day data collection, transportation, maintenance/curation, and access to the PepsiCo corporate data asset\nWork cross-functionally across the enterprise to centralize data and standardize it for use by business, data science or other stakeholders.\nIncrease awareness about available data and democratize access to it across the company.\nAs a data engineer, you will be the key technical expert building PepsiCo's data productsto drive a strong vision. You'll be empowered to create data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company.\nAs a member of the data engineering team, you will help developingvery large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premises data sources as well as cloud and remote systems.\n\nResponsibilities\nAct as a subject matter expert across different digital projects.\nOversee work with internal clients and external partners to structure and store data into unified taxonomies and link them together with standard identifiers.\nManage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.\nBuild and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.\nResponsible for implementing best practices around systems integration, security, performance, and data management.\nEmpower the business by creating value through the increased adoption of data, data science and business intelligence landscape.\nCollaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.\nEvolve the architectural capabilities and maturity of the data platform by engaging with enterprise architects and strategic internal and external partners.\nDevelop and optimize procedures to productionalize data science models.\nDefine and manage SLA s for data products and processes running in production.\nSupport large-scale experimentation done by data scientists.\nPrototype new approaches and build solutions at scale.\nResearch in state-of-the-art methodologies.\nCreate documentation for learnings and knowledge transfer.\nCreate and audit reusable packages or libraries.\n\nQualifications\n4+ years of overall technology experience that includes at least 3+ years of hands-on software development, data engineering, and systems architecture.\n3+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.\n3+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).\n2+ years in cloud data engineering experience in Azure.\nFluent with Azure cloud services. Azure Certification is a plus.\nExperience in Azure Log Analytics\nExperience with integration of multi cloud services with on-premises technologies.\nExperience with data modelling, data warehousing, and building high-volume ETL/ELT pipelines.\nExperience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.\nExperience building/operatinghighly available, distributed systems of data extraction, ingestion, and processing of large data sets.\nExperience with at least one MPP database technology such as Redshift, Synapse or Snowflake.\nExperience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.\nExperience with Statistical/ML techniques is a plus.\nExperience with building solutions in the retail or in the supply chain space is a plus.\nExperience with version control systems like Github and deployment & CI tools.\nWorking knowledge of agile development, including DevOps and DataOps concepts.\nB Tech/ BA/ BS in Computer Science, Math, Physics, or other technical fields.\nSkills, Abilities, Knowledge:\nExcellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.\nStrong change manager. Comfortable with change, especially that which arises through company growth.\nAbility to understand and translate business requirements into data and technical requirements.\nHigh degree of organization and ability to manage multiple, competing projects and priorities simultaneously.\nPositive and flexible attitude to enable adjusting to different needs in an ever-changing environment.\nStrong organizational and interpersonal skills; comfortable managing trade-offs.\nFoster a team culture of accountability, communication, and self-management.\nProactively drives impact and engagement while bringing others along.\nConsistently attain/exceed individual and team goals.\n4+ years of overall technology experience that includes at least 3+ years of hands-on software development, data engineering, and systems architecture.\n3+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.\n3+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).\n2+ years in cloud data engineering experience in Azure.\nFluent with Azure cloud services. Azure Certification is a plus.\nExperience in Azure Log Analytics\nExperience with integration of multi cloud services with on-premises technologies.\nExperience with data modelling, data warehousing, and building high-volume ETL/ELT pipelines.\nExperience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.\nExperience building/operatinghighly available, distributed systems of data extraction, ingestion, and processing of large data sets.\nExperience with at least one MPP database technology such as Redshift, Synapse or Snowflake.\nExperience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.\nExperience with Statistical/ML techniques is a plus.\nExperience with building solutions in the retail or in the supply chain space is a plus.\nExperience with version control systems like Github and deployment & CI tools.\nWorking knowledge of agile development, including DevOps and DataOps concepts.\nB Tech/BA/BS in Computer Science, Math, Physics, or other technical fields.",Industry Type: Beverage,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'Data Lake Infrastructure', 'Azure Data Factory', 'PySpark', 'Scala', 'Azure Log Analytics', 'Azure Databricks', 'Data Warehousing', 'Data Analytics', 'data collection', 'Python', 'SQL']",2025-06-10 14:09:42
Senior Data Engineer,Impetus Technologies,5 - 10 years,Not Disclosed,['United Arab Emirates'],"The Opportunity:We are seeking a highly motivated and technically strong Module Lead Software Engineer with significant expertise in Python, PySpark, and Palantir Foundry. In this role, you will be responsible for the end-to-end technical ownership, design, and delivery of a specific module or component within our enterprise data platform. You will combine hands-on development with technical leadership, ensuring the highest standards of code quality, performance, and reliability.\n\nKey Responsibilities:\n\nModule Technical Leadership & Ownership: Take full technical ownership of a specific module or component within the data platform on Palantir Foundry. This includes defining its technical roadmap, architecture, design patterns, and ensuring its integration into the broader data ecosystem.\nHands-on Development and Complex Problem Solving: Act as a lead individual contributor, developing sophisticated data pipelines, transformations, and applications using Python and PySpark within Palantir Foundry's various tools (e.g., Code Workbook, Pipeline Builder). Tackle the most challenging technical problems and implement core functionalities for the module.\nQuality Assurance and Best Practices Advocacy: Drive and enforce high standards for code quality, test coverage, documentation, and operational excellence within your module. Conduct rigorous code reviews, provide constructive feedback, and mentor engineers within your immediate scope to elevate their technical skills.\nCross-Functional Collaboration and Module Integration: Collaborate extensively with other module leads, architects, data scientists, and business stakeholders to ensure seamless integration of your module's deliverables. Proactively identify and manage technical dependencies and ensure the module aligns with overall project goals and architectural vision.\n• Performance Optimization and Troubleshooting: Continuously monitor and optimize the performance of your module's data pipelines and applications. Efficiently troubleshoot and resolve complex technical issues, data quality concerns, and system failures specific to your module.\n\nRequired Qualifications:\n\nExperience: 6-8 years of progressive experience in software development with a strong focus on data engineering.\nPython Proficiency: Expert-level proficiency in Python, including advanced programming concepts, data structures, and performance optimization techniques.\nPySpark Expertise: Strong experience with PySpark for large-scale distributed data processing, transformations, and analytics.\nPalantir Foundry: Proven, hands-on experience designing, developing, and deploying solutions within Palantir Foundry is essential.\nDeep familiarity with Foundry's data integration capabilities, Code Workbook, Pipeline Builder, Data Health checks, and Ontology modeling.\nExperience with Foundry's approach to data governance and versioning.\nSQL Skills: Excellent SQL skills for complex data querying, manipulation, and optimization.\nData Warehousing/Lakes: Solid understanding of data warehousing concepts, data lake architectures, and ETL/ELT principles.\nCloud Platforms: Experience with at least one major cloud platform (AWS, Azure, GCP), particularly with data-related services.\nVersion Control: Strong experience with Git and collaborative development workflows.\n\nPreferred Qualifications (Nice-to-Have):\n\nExperience mentoring or leading small technical teams/pods.\nFamiliarity with containerization technologies (Docker, Kubernetes).\nExperience with streaming data technologies (e.g., Kafka, Kinesis).\nUnderstanding of CI/CD pipelines for data solutions.\nKnowledge of data governance, data quality, and metadata management best practices.\nExperience in [specific industry, e.g., Financial Services, Manufacturing, Healthcare].",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Palantir Foundry', 'Python', 'Spark', 'Data Warehousing', 'SQL']",2025-06-10 14:09:44
Data/ML Ops Engineer,"NTT DATA, Inc.",2 - 5 years,Not Disclosed,['Bengaluru'],"Additional Career Level Description:\n\n\nKnowledge and application:\nSeasoned, experienced professional; has complete knowledge and understanding of area of specialization.\nUses evaluation, judgment, and interpretation to select right course of action.\n\n\n\nProblem solving:\nWorks on problems of diverse scope where analysis of information requires evaluation of identifiable factors.\nResolves and assesses a wide range of issues in creative ways and suggests variations in approach.\n\n\n\nInteraction:\nEnhances relationships and networks with senior internal/external partners who are not familiar with the subject matter often requiring persuasion.\nWorks with others outside of own area of expertise, with the ability to adapt style to differing audiences and often advises others on difficult matters.\n\n\n\nImpact:\nImpacts short to medium term goals through personal effort or influence over team members.\n\n\n\nAccountability:\nAccountable for own targets with work reviewed at critical points.\nWork is done independently and is reviewed at critical points.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ML Ops', 'python', 'spark', 'big data', 'data engineering', 'artificial intelligence', 'ml', 'sql']",2025-06-10 14:09:47
Data Engineer/Consultant Specialist,Hsbc,7 - 8 years,Not Disclosed,['Hyderabad'],"Some careers shine brighter than others.\nIf you re looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further.\nHSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions.\nWe are currently seeking an experienced professional to join our team in the role of Marketing Title.\nIn this role, you will:\nThis role will be responsible for:\nDevelop and support new feeds ingestion / understand the existing framework and do the development as per the business rules and requirements.\nDevelopment and maintenance of new changes / enhancements in Data Ingestion / Juniper and promoting and supporting those in the production environment within the stipulated timelines.\nNeed to get familiar with the Data Ingestion / Data Refinery / Common Data Model / Compdata frameworks quickly and contribute to the application development as soon as possible.\nMethodical and measured approach with a keen eye for attention to detail;\nAbility to work under pressure and remain calm in the face of adversity;\nAbility to collaborate, interact and engage with different business, technical and subject matter experts;\nGood, concise, written and verbal communication\nAbility to manage workload from multiple requests and to balance priorities;\nPro-active, a can do mind-set and attitude;\nGood documentation skills\n\n\n\n\n\n\n\n\n\n\nRequirements\n\n\n\nTo be successful in this role, you should meet the following requirements:\nExperience (1 = essential, 2 = very useful, 3 = nice to have):\n1. Hadoop / Hive / GCP\n2. Agile / Scrum\n3. LINUX\nTechnical skills (1 = essential, 2 = useful, 3 = nice to have):\n1. Any ETL tool\n1. Analytical trouble shooting.\n2. Hive QL\n1. On-Prem / Cloud infra knowledgeYou ll achieve more when you join HSBC.",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'Linux', 'GCP', 'Analytical', 'business rules', 'Application development', 'Refinery', 'Troubleshooting', 'ETL tool', 'Financial services']",2025-06-10 14:09:49
Data Engineer/ Consultant Specialist,Hsbc,7 - 8 years,Not Disclosed,['Pune'],"Some careers shine brighter than others.\nIf you re looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further.\nHSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions.\nWe are currently seeking an experienced professional to join our team in the role of Marketing Title.\nIn this role, you will:\nThis role will be responsible for:\nDevelop and support new feeds ingestion / understand the existing framework and do the development as per the business rules and requirements.\nDevelopment and maintenance of new changes / enhancements in Data Ingestion / Juniper and promoting and supporting those in the production environment within the stipulated timelines.\nNeed to get familiar with the Data Ingestion / Data Refinery / Common Data Model / Compdata frameworks quickly and contribute to the application development as soon as possible.\nMethodical and measured approach with a keen eye for attention to detail;\nAbility to work under pressure and remain calm in the face of adversity;\nAbility to collaborate, interact and engage with different business, technical and subject matter experts;\nGood, concise, written and verbal communication\nAbility to manage workload from multiple requests and to balance priorities;\nPro-active, a can do mind-set and attitude;\nGood documentation skills\n\n\n\n\n\n\n\n\n\n\nRequirements\n\n\n\nTo be successful in this role, you should meet the following requirements:\nExperience (1 = essential, 2 = very useful, 3 = nice to have):\n1. Hadoop / Hive / GCP\n2. Agile / Scrum\n3. LINUX\nTechnical skills (1 = essential, 2 = useful, 3 = nice to have):\n1. Any ETL tool\n1. Analytical trouble shooting.\n2. Hive QL\n1. On-Prem / Cloud infra knowledge",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'Linux', 'GCP', 'Analytical', 'business rules', 'Application development', 'Refinery', 'Troubleshooting', 'ETL tool', 'Financial services']",2025-06-10 14:09:51
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Bengaluru'],"As a BigData Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\n\nAs a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nBig Data Developer, Hadoop, Hive, Spark, PySpark, Strong SQL.\nAbility to incorporate a variety of statistical and machine learning techniques. Basic understanding of Cloud (AWS,Azure, etc).\nAbility to use programming languages like Java, Python, Scala, etc., to build pipelines to extract and transform data from a repository to a data consumer\nAbility to use Extract, Transform, and Load (ETL) tools and/or data integration, or federation tools to prepare and transform data as needed.\nAbility to use leading edge tools such as Linux, SQL, Python, Spark, Hadoop and Java\n\n\nPreferred technical and professional experience\nBasic understanding or experience with predictive/prescriptive modeling skills\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['scala', 'sql', 'spark', 'hadoop', 'python', 'hive', 'data management', 'pyspark', 'data warehousing', 'apache pig', 'plsql', 'java', 'unix shell scripting', 'linux', 'big data', 'etl', 'hbase', 'data analysis', 'oracle', 'microsoft azure', 'machine learning', 'data engineering', 'aws', 'data integration', 'informatica']",2025-06-10 14:09:53
Data Engineer | Scala - Noida (WFO),CloudKeeper,3 - 6 years,Not Disclosed,['Noida'],"About CloudKeeper\nCloudKeeper is a cloud cost optimization partner that combines the power of\ngroup buying & commitments management, expert cloud consulting & support,\nand an enhanced visibility & analytics platform to reduce cloud cost & help\nbusinesses maximize the value from AWS, Microsoft Azure, & Google Cloud.\nA certified AWS Premier Partner, Azure Technology Consulting Partner,\nGoogle,Cloud Partner, and FinOps Foundation Premier Member, CloudKeeper\nhas helped 400+ global companies save an average of 20% on their cloud bills,\nmodernize their cloud set-up and maximize value all while maintaining\nflexibility and avoiding any long-term commitments or cost.\nCloudKeeper hived off from TO THE NEW, digital technology services company\nwith 2500+ employees and an 8-time GPTW winner.\nPosition Overview:\nWe are looking for an experienced and driven Data Engineer to join our team.\nThe ideal candidate will have a strong foundation in big data technologies,\nparticularly Spark, and a basic understanding of Scala to design and implement\nefficient data pipelines. As a Data Engineer at CloudKeeper, you will be\nresponsible for building and maintaining robust data infrastructure, integrating\nlarge datasets, and ensuring seamless data flow for analytical and operational\npurposes.\n\nKey Responsibilities:\nDesign, develop, and maintain scalable data pipelines and ETL processes to collect, process, and store data from various sources.\nWork with Apache Spark to process large datasets in a distributed environment, ensuring optimal performance and scalability.\nDevelop and optimize Spark jobs and data transformations using Scala for large-scale data processing.\nCollaborate with data analysts and other stakeholders to ensure data pipelines meet business and technical requirements.\nIntegrate data from different sources (databases, APIs, cloud storage, etc.) into a unified data platform.\nEnsure data quality, consistency, and accuracy by building robust data validation and cleansing mechanisms.\nUse cloud platforms (AWS, Azure, or GCP) to deploy and manage data processing and storage solutions.\nAutomate data workflows and tasks using appropriate tools and frameworks.\nMonitor and troubleshoot data pipeline performance, optimizing for efficiency and cost-effectiveness.\nImplement data security best practices, ensuring data privacy and compliance with industry standards.\nRequired Qualifications:\n4- 6 years of experience required as a Data Engineer or an equivalent role\nStrong experience working with Apache Spark with Scala for distributed data processing and big data handling.\nBasic knowledge of Python and its application in Spark for writing efficient data transformations and processing jobs.\nProficiency in SQL for querying and manipulating large datasets.ing technologies.\nExperience with cloud data platforms, preferably AWS (e.g., S3, EC2, EMR, Redshift) or other cloud-based solutions.\nStrong knowledge of data modeling, ETL processes, and data pipeline orchestration.\nFamiliarity with containerization (Docker) and cloud-native tools for deploying data solutions.\nKnowledge of data warehousing concepts and experience with tools like AWS Redshift, Google BigQuery, or Snowflake is a plus.\nExperience with version control systems such as Git.\nStrong problem-solving abilities and a proactive approach to resolving technical challenges.\nExcellent communication skills and the ability to work collaboratively within cross-functional teams.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Temporary/Contractual","['Data Engineering', 'SCALA', 'Pyspark', 'Scala Programming', 'Python Framework', 'SQL Queries', 'Spark', 'Python', 'SQL']",2025-06-10 14:09:56
Associate Data Engineer,"NTT DATA, Inc.",1 - 3 years,Not Disclosed,"['New Delhi', 'Chennai', 'Bengaluru']","Your day at NTT DATA\nWe are seeking an experienced Data Engineer to join our team in delivering cutting-edge Generative AI (GenAI) solutions to clients. The successful candidate will be responsible for designing, developing, and deploying data pipelines and architectures that support the training, fine-tuning, and deployment of LLMs for various industries. This role requires strong technical expertise in data engineering, problem-solving skills, and the ability to work effectively with clients and internal teams.\n\nWhat youll be doing\n\nKey Responsibilities:\nDesign, develop, and manage data pipelines and architectures to support GenAI model training, fine-tuning, and deployment\nData Ingestion and Integration: Develop data ingestion frameworks to collect data from various sources, transform, and integrate it into a unified data platform for GenAI model training and deployment.\nGenAI Model Integration: Collaborate with data scientists to integrate GenAI models into production-ready applications, ensuring seamless model deployment, monitoring, and maintenance.\nCloud Infrastructure Management: Design, implement, and manage cloud-based data infrastructure (e.g., AWS, GCP, Azure) to support large-scale GenAI workloads, ensuring cost-effectiveness, security, and compliance.\nWrite scalable, readable, and maintainable code using object-oriented programming concepts in languages like Python, and utilize libraries like Hugging Face Transformers, PyTorch, or TensorFlow\nPerformance Optimization: Optimize data pipelines, GenAI model performance, and infrastructure for scalability, efficiency, and cost-effectiveness.\nData Security and Compliance: Ensure data security, privacy, and compliance with regulatory requirements (e.g., GDPR, HIPAA) across data pipelines and GenAI applications.\nClient Collaboration: Collaborate with clients to understand their GenAI needs, design solutions, and deliver high-quality data engineering services.\nInnovation and R&D: Stay up to date with the latest GenAI trends, technologies, and innovations, applying research and development skills to improve data engineering services.\nKnowledge Sharing: Share knowledge, best practices, and expertise with team members, contributing to the growth and development of the team.\n\nBachelors degree in computer science, Engineering, or related fields (Masters recommended)\nExperience with vector databases (e.g., Pinecone, Weaviate, Faiss, Annoy) for efficient similarity search and storage of dense vectors in GenAI applications\n5+ years of experience in data engineering, with a strong emphasis on cloud environments (AWS, GCP, Azure, or Cloud Native platforms)\nProficiency in programming languages like SQL, Python, and PySpark\nStrong data architecture, data modeling, and data governance skills\nExperience with Big Data Platforms (Hadoop, Databricks, Hive, Kafka, Apache Iceberg), Data Warehouses (Teradata, Snowflake, BigQuery), and lakehouses (Delta Lake, Apache Hudi)\nKnowledge of DevOps practices, including Git workflows and CI/CD pipelines (Azure DevOps, Jenkins, GitHub Actions)\nExperience with GenAI frameworks and tools (e.g., TensorFlow, PyTorch, Keras)\nNice to have:\nExperience with containerization and orchestration tools like Docker and Kubernetes\nIntegrate vector databases and implement similarity search techniques, with a focus on GraphRAG is a plus\nFamiliarity with API gateway and service mesh architectures\nExperience with low latency/streaming, batch, and micro-batch processing\nFamiliarity with Linux-based operating systems and REST APIs",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Apache Iceberg', 'Faiss', 'PySpark', 'Kafka', 'Pinecone', 'GitHub Actions', 'Snowflake', 'Apache Hudi', 'AWS', 'Azure DevOps', 'Python', 'Azure', 'BigQuery', 'Hadoop', 'Annoy', 'Teradata', 'SQL', 'Jenkins', 'Hive', 'Cloud Native platforms', 'GCP', 'Delta Lake', 'Databricks', 'Weaviate']",2025-06-10 14:09:58
Data Engineer,Acenet,5 - 7 years,Not Disclosed,"['Pune', 'Gurugram', 'Bengaluru']","About Us:\nJob Summary :\nWe are seeking a highly skilled individual to join our team as a Data Engineering/Operations Specialist. This role will be responsible for maintaining and evolving data pipeline architecture, orchestrating new data sources for further processing, and ensuring the up-to-date documentation of pipelines and data feeds.\nKey Responsibilities:\n*Maintain, upgrade and evolve data pipeline architectures to ensure optimal performance and scalability.\n*Orchestrate the integration of new data sources into existing pipelines for further processing and analysis.\n*Keep documentation up to date for pipelines and data feeds to facilitate smooth operations and collaboration within the team.\n*Collaborate with cross-functional teams to understand data requirements and optimize pipeline performance accordingly.\n*Troubleshoot and resolve any issues related to pipeline architecture and data processing.\nRole Requirements and Qualifications:\n*Experience with Cloud platform for deployment and management of data pipelines.\n*Familiarity with AWS / Azure for efficient data processing workflows.\n*Experience with constructing FAIR data products is highly desirable.\n*Basic understanding of computational clusters to optimize pipeline performance.\n*Prior experience in data engineering or operations roles, preferably in a cloud-based environment.\n*Proven track record of successfully maintaining and evolving data pipeline architectures.\n*Strong problem-solving skills and ability to troubleshoot technical issues independently.\n*Excellent communication skills to collaborate effectively with cross-functional teams.\nWhy Join Us:\n*Opportunities to work on transformative projects, cutting-edge technology and innovative solutions with leading global firms across industry sectors.\n*Continuous investment in employee growth and professional development with a strong focus on up & re-skilling.\n*Competitive compensation & benefits, ESOPs and international assignments.\n*Supportive environment with healthy work-life balance and a focus on employee well-being.\n*Open culture that values diverse perspectives, encourages transparent communication and rewards contributions.\nHow to Apply:\nIf you are interested in joining our team and meet the qualifications listed above, please apply and submit your resume highlighting why you are the ideal candidate for this position.",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Supply chain', 'Architecture', 'Manager Technology', 'Healthcare', 'Data processing', 'Business strategy', 'Fund raising', 'Troubleshooting', 'Financial services', 'Logistics']",2025-06-10 14:10:02
Data Engineer,7dxperts,5 - 8 years,15-20 Lacs P.A.,['Bengaluru'],"Role & responsibilities\n3+ years of experience in Spark, Databricks, Hadoop, Data and ML Engineering.\n3+ Years on experience in designing architectures using AWS cloud services & Databricks.\nArchitecture, design and build Big Data Platform (Data Lake / Data Warehouse / Lake house) using Databricks services and integrating with wider AWS cloud services.\nKnowledge & experience in infrastructure as code and CI/CD pipeline to build and deploy data platform tech stack and solution.\nHands-on spark experience in supporting and developing Data Engineering (ETL/ELT) and Machine learning (ML) solutions using Python, Spark, Scala or R languages.\nDistributed system fundamentals and optimising Spark distributed computing.\nExperience in setting up batch and streams data pipeline using Databricks DLT, jobs and streams.\nUnderstand the concepts and principles of data modelling, Database, tables and can produce, maintain, and update relevant data models across multiple subject areas.\nDesign, build and test medium to complex or large-scale data pipelines (ETL/ELT) based on feeds from multiple systems using a range of different storage technologies and/or access methods, implement data quality validation and to create repeatable and reusable pipelines\nExperience in designing metadata repositories, understanding range of metadata tools and technologies to implement metadata repositories and working with metadata.\nUnderstand the concepts of build automation, implementing automation pipelines to build, test and deploy changes to higher environments.\nDefine and execute test cases, scripts and understand the role of testing and how it works.\n\nPreferred candidate profile\nBig Data technologies Databricks, Spark, Hadoop, EMR or Hortonworks.\nSolid hands-on experience in programming languages Python, Spark, SQL, Spark SQL, Spark Streaming, Hive and Presto\nExperience in different Databricks components and API like notebooks, jobs, DLT, interactive and jobs cluster, SQL warehouse, policies, secrets, dbfs, Hive Metastore, Glue Metastore, Unity Catalog and ML Flow.\nKnowledge and experience in AWS Lambda, VPC, S3, EC2, API Gateway, IAM users, roles & policies, Cognito, Application Load Balancer, Glue, Redshift, Spectrum, Athena and Kinesis.\nExperience in using source control tools like git, bit bucket or AWS code commit and automation tools like Jenkins, AWS Code build and Code deploy.\nHands-on experience in terraform and Databricks API to automate infrastructure stack.\nExperience in implementing CI/CD pipeline and ML Ops pipeline using Git, Git actions or Jenkins.\nExperience in delivering project artifacts like design documents, test cases, traceability matrix and low-level design documents.\nBuild references architectures, how-tos, and demo applications for customers.\nReady to complete certifications",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'Data Bricks', 'Python', 'ML', 'ML Engineering', 'Pyspark', 'MLops', 'Ci Cd Pipeline', 'GIT', 'Machine Learning', 'SQL']",2025-06-10 14:10:04
Data Engineer,Bebo Technologies,4 - 9 years,Not Disclosed,['Chandigarh'],"Design, build, and maintain scalable and reliable data pipelines on Databricks, Snowflake, or equivalent cloud platforms.\nIngest and process structured, semi-structured, and unstructured data from a variety of sources including APIs, RDBMS, and file systems.\nPerform data wrangling, cleansing, transformation, and enrichment using PySpark, Pandas, NumPy, or similar libraries.\nOptimize and manage large-scale data workflows for performance, scalability, and cost-efficiency.\nWrite and optimize complex SQL queries for transformation, extraction, and reporting.\nDesign and implement efficient data models and database schemas with appropriate partitioning and indexing strategies for Data Warehouse or Data Mart.\nLeverage cloud services (e.g., AWS S3, Glue, Kinesis, Lambda) for storage, processing, and orchestration.\nUse orchestration tools like Airflow, Temporal, or AWS Step Functions to manage end-to-end workflows.\nBuild containerized solutions using Docker and manage deployment pipelines via CI/CD tools such as Azure DevOps, GitHub Actions, or Jenkins.\nCollaborate closely with data scientists, analysts, and business stakeholders to understand requirements and deliver data solutions.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'python', 'Snowflake', 'Data Bricks', 'sql']",2025-06-10 14:10:06
Data Engineer,IBM,2 - 4 years,Not Disclosed,['Bengaluru'],"Ingest new data from relational and non-relational source database systems into our warehouse. Connect data from various sources.\n\nIntegrate data from external sources to warehouse by building facts and dimensions based on the EPM data model requirements.\n\n\n\nAutomate data exchange and processing through serverless data pipelines.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience in data analysis and integration.\nExperience in data building and consuming fact and dimension tables.\nExperience in automating data integration through data pipelines.\nExperience with object-oriented programing languages such as Python.\nExperience with structured data processing languages such as SQL and Spark SQL.\nExperience with REST APIs and JSON\nExperience in IBM Cloud data processing services such as IBM Code Engine, IBM Event Streams (Apache Kafka).\nStrong understanding of Datawarehouse concepts and various data warehouse architectures\n\n\nPreferred technical and professional experience\nExperience with IBM Cloud architecture\nExperience with DevOps.\nKnowledge of Agile development methodologies\nExperience with building containerized applications and running them in serverless environments on the Cloud such as IBM Code Engine, Kubernetes, or Satellite.\nExperience with IBM Cognitive Enterprise Data Platform and CodeHub.\nExperience with data integration tools such as IBM DataStage or Informatica",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data warehousing', 'sql', 'spark', 'data warehousing concepts', 'hive', 'kubernetes', 'rest', 'data analysis', 'ibm cloud', 'datastage', 'data processing', 'data engineering', 'apache', 'cloud architecture', 'cognitive', 'devops', 'ibm datastage', 'kafka', 'json', 'agile', 'hadoop', 'informatica']",2025-06-10 14:10:08
DATA ENGINEER-ADVANCED ANALYTICS,IBM,5 - 10 years,Not Disclosed,['Bengaluru'],"Develop, test and support future-ready data solutions for customers across industry verticals\nDevelop, test, and support end-to-end batch and near real-time data flows/pipelines\nDemonstrate understanding in data architectures, modern data platforms, big data, analytics, cloud platforms, data governance and information management and associated technologies\nCommunicates risks and ensures understanding of these risks.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nMinimum of 5+ years of related experience required\nExperience in modeling and business system designs\nGood hands-on experience on DataStage, Cloud based ETL Services\nHave great expertise in writing TSQL code\nWell versed with data warehouse schemas and OLAP techniques\n\n\nPreferred technical and professional experience\nAbility to manage and make decisions about competing priorities and resources.\nAbility to delegate where appropriate\nMust be a strong team player/leader\nAbility to lead Data transformation project with multiple junior data engineers\nStrong oral written and interpersonal skills for interacting and throughout all levels of the organization.\nAbility to clearly communicate complex business problems and technical solutions.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data architecture', 'big data', 'information management', 'cloud platforms', 'data governance', 'schema', 't-sql', 'ansible', 'docker', 'sql', 'java', 'git', 'devops', 'linux', 'jenkins', 'j2ee', 'shell scripting', 'mysql', 'etl', 'python', 'data analytics', 'datastage', 'microsoft azure', 'warehouse', 'olap', 'aws']",2025-06-10 14:10:11
Sr Eng Data Engineering,Johnson & Johnson,8 - 13 years,Not Disclosed,['Bengaluru'],"Johnson & Johnson MedTech is seeking a Sr Eng Data Engineering for Digital Surgery Platform (DSP) in Bangalore, India.\n\nJohnson & Johnson (J&J) stands as the world's leading manufacturer of healthcare products and a service provider in the pharmaceutical and medical device sectors. At Johnson & Johnson MedTech's Digital Surgery Platform, we are groundbreaking the future of healthcare by harnessing the power of people and technology, transitioning to a digital-first MedTech enterprise. With a focus on innovation and an ambitious strategic vision, we are integrating robotic-assisted surgery platforms, connected medical devices, surgical instruments, medical imaging, surgical efficiency solutions, and OR workflow into the next-generation MedTech platform. This initiative will also foster new surgical insights, improve supply chain innovation, use cloud infrastructure, incorporate cybersecurity, collaborate with hospital EMRs, and elevate our digital solutions.\nWe are a diverse and growing team, that nurture creativity, deep understanding of data processing techniques, and the use of sophisticated analytics technologies to deliver results.\n\nOverview\nAs a Sr Eng Data Engineering for J&J MedTech Digital Surgery Platform (DSP), you will play a pivotal role in building the modern cloud data platform by demonstrating your in-depth technical expertise and interpersonal skills. In this role, you will be required to focus on accelerating digital product development as part of the multifunctional and fast-paced DSP data platform team and will give to the digital transformation through innovative data solutions.\nOne of the key success criteria for this role is to ensure the quality of DSP software solutions and demonstrate the ability to collaborate effectively with the core infrastructure and other engineering teams and work closely with the DSP security and technical quality partners.\n\nResponsibilities\nWork with platform data engineering, core platform, security, and technical quality to design, implement and deploy data engineering solutions.\nDevelop pipelines for ingestion, transformation, orchestration, and consumption of various types of data.\nDesign and deploy data layering pipelines that use modern Spark based data processing technologies such as Databricks and Delta Live Table (DLT).\nIntegrate data engineering solutions with Azure data governance components not limited to Purview and Databricks Unity Catalog.\nImplement and support security monitoring solutions within Azure Databricks ecosystem.\nDesign, implement, and support data monitoring solutions in data analytical workspaces.\nConfigure and deploy Databricks Analytical workspaces in Azure with IaC (Terraform, Databricks API) with J&J DevOps automation tools within JPM/Xena framework.\nImplement automated CICD processes for data processing pipelines.\nSupport DataOps for the distributed DSP data architecture.\nFunction as a data engineering SME within the data platform.\nManage authoring and execution of automated test scripts.\nBuild effective partnerships with DSP architecture, core infrastructure and other domains to design and deploy data engineering solutions.\nWork closely with the DSP Product Managers to understand business needs, translate them to system requirements, demonstrate in-depth understanding of use cases for building prototypes and solutions for data processing pipelines.\nOperate in SAFe Agile DevOps principles and methodology in building quality DSP technical solutions. Author and implement automated test scripts as mandates DSP quality requirements.\n\nQualifications\nRequired\nBachelors degree or equivalent experience in software or computer science or data engineering.\n8+ years of overall IT experience.\n5-7 years of experience in cloud computing and data systems.\nAdvanced Python programming skills.\nExpert level in Azure Databricks Spark technology and data engineering (Python) including Delta Live Tables (DLT).\nExperience in design and implementation of secure Azure data solutions.\nIn-depth knowledge of the data architecture infrastructure, network components, data processing\nProficiency in building data pipelines in Azure Databricks.\nProficiency in configuration and administration of Azure Databricks workspaces and Databricks Unity Catalog.\nDeep understanding of principles of modern data Lakehouse.\nDeep understanding of Azure system capabilities, data services, and ability to implement security controls.\nProficiency with enterprise DevOps tools including Bitbucket, Jenkins, Artifactory.\nExperience with DataOps.\nExperience with quality software systems.\nDeep understanding of and experience in SAFe Agile.\nUnderstanding of SDLC.\n\nPreferred\nMaster’s degree or equivalent.\nProven healthcare experience.\nAzure Databricks certification.\nAbility to analyze use cases and translate them into system requirements, make data driven decisions\nDevOps's automation tools with JPM/Xena framework.\nExpertise in automated testing.\nExperience in AI and MLs.\nExcellent verbal and written communication skills.\nAbility to travel up to 10% of domestic required.\n\nJohnson & Johnson is an Affirmative Action and Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, or protected veteran status and will not be discriminated against on the basis of disability.",Industry Type: Pharmaceutical & Life Sciences,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Advanced Python', 'Spark', 'Devops', 'Data Bricks', 'Agile Safe', 'Cloud Computing', 'data systems']",2025-06-10 14:10:13
Data Engineer,Lance Labs,7 - 12 years,Not Disclosed,"['Noida', 'Chennai']","Deployment, configuration & maintenance of Databricks clusters & workspaces\nSecurity & Access Control\nAutomate administrative task using tools like Python, PowerShell &Terraform\nIntegrations with Azure Data Lake, Key Vault & implement CI/CD pipelines\n\nRequired Candidate profile\nAzure, AWS, or GCP; Azure experience is preferred\nStrong skills in Python, PySpark, PowerShell & SQL\nExperience with Terraform\nETL processes, data pipeline &big data technologies\nSecurity & Compliance",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Azure Databricks', 'SQL', 'Terraform', 'Python', 'Powershell', 'Ci/Cd', 'Data Pipeline', 'GCP', 'Azure Cloud', 'Azure Data Lake', 'ETL', 'AWS', 'Data Governance', 'Azure Devops']",2025-06-10 14:10:15
Data Engineer,Forbes Global 2000 IT Services Firm,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Title: Big Data Engineer Java, Scala & Spark\nLocation: Hyderabad\nWork Mode: Onsite (5 days a week)\nExperience: 5 to 10 Years\nJob Summary:\nWe are hiring an experienced Big Data Engineer with strong expertise in Java, Scala, Apache Spark, and Big Data technologies. You will be responsible for designing and implementing scalable data pipelines that support real-time and batch processing for data-driven applications.\nKey Responsibilities:\nDevelop and maintain scalable batch and streaming data pipelines using Java, Scala, and Apache Spark\nWork with Hadoop, Hive, Kafka, and HDFS to manage and process large datasets\nCollaborate with data analysts, scientists, and other engineering teams to understand data requirements\nOptimize Spark jobs and ensure performance and reliability in production\nMaintain data quality, governance, and security best practices\nRequired Skills:\n5–10 years of hands-on experience in data engineering or related roles\nStrong programming skills in both Java and Scala\nExpertise in Apache Spark for data processing and transformation\nGood understanding of Big Data frameworks: Hadoop, Hive, Kafka, HDFS\nExperience with distributed systems and large-scale data processing\nFamiliarity with cloud platforms such as AWS, GCP, or Azure\nGood to Have:\nExperience with workflow orchestration tools like Airflow or NiFi\nKnowledge of containerization (Docker, Kubernetes)\nExposure to CI/CD pipelines and version control (e.g., Git)\nEducation:\nBachelor’s or Master’s degree in Computer Science, Engineering, or related field\nWhy Join Us:\nBe part of a high-impact data engineering team\nWork on modern data platforms with the latest open-source tools\nStrong tech culture with career growth opportunities",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'SCALA', 'Spark', 'Hive', 'Hadoop']",2025-06-10 14:10:17
Informatica Data Engineer,Malomatia,7 - 12 years,20-35 Lacs P.A. (Including Variable: 3%),['Pune'],"Job Title: Data Engineer Informatica IDMC\nLocation: Remote/Contract\nExperience Level: Mid to Senior (7+ years)\nJob Summary:\nWe are seeking a highly skilled Data Engineer with a minimum of 5 years of hands-on experience in Informatica Intelligent Data Management Cloud (IDMC). The successful candidate will design, implement, and maintain scalable data integration solutions using Informatica Cloud services. Experience with CI/CD pipelines is required to ensure efficient development and deployment cycles. Familiarity with Informatica Catalog, Data Governance, and Data Quality or Azure Data Factory is considered a strong advantage.\nKey Responsibilities:\nDesign, build, and optimize end-to-end data pipelines using Informatica IDMC, including Cloud Data Integration and Cloud Application Integration.\nImplement ETL/ELT processes to support data lakehouse, and EDW use cases.\nDevelop and maintain CI/CD pipelines to support automated deployment and version control.\nWork closely with data architects, analysts, and business stakeholders to translate data requirements into scalable solutions.\nMonitor job performance, troubleshoot issues, and ensure compliance with SLAs and data quality standards.\nDocument technical designs, workflows, and integration processes following best practices.\nCollaborate with DevOps and cloud engineering teams to ensure seamless integration within the cloud ecosystem.\nRequired Qualifications:\nBachelor’s degree in Computer Science, Information Systems, Engineering, or a related field.\nMinimum 5 years of hands-on experience with Informatica IDMC.\nExperience in building and deploying CI/CD pipelines using tools such as Git, or Azure DevOps.\nProficient in SQL, data modeling, and transformation logic.\nExperience with cloud platforms (Azure or OCI).\nStrong problem-solving skills in data operations and pipeline performance.\nPreferred / Nice-to-Have Skills:\nExperience with Informatica Data Catalog for metadata and lineage tracking.\nFamiliarity with Informatica Data Governance tools such as Axon and Business Glossary.\nHands-on experience with Informatica Data Quality (IDQ) for data profiling, cleansing.\nExperience developing data pipelines using Azure Data Factory.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'ADF', 'Informatica IDMC', 'Devops', 'Python', 'Azure Data Factory', 'SCALA', 'ETL', 'Azure Storage']",2025-06-10 14:10:20
Data Engineer-Data Platforms,IBM,4 - 7 years,Not Disclosed,['Bengaluru'],"A Data Engineer specializing in enterprise data platforms, experienced in building, managing, and optimizing data pipelines for large-scale environments. Having expertise in big data technologies, distributed computing, data ingestion, and transformation frameworks.\nProficient in Apache Spark, PySpark, Kafka, and Iceberg tables, and understand how to design and implement scalable, high-performance data processing solutions.What you’ll doAs a Data Engineer – Data Platform Services, responsibilities include:\n\nData Ingestion & Processing\nDesigning and developing data pipelines to migrate workloads from IIAS to Cloudera Data Lake.\nImplementing streaming and batch data ingestion frameworks using Kafka, Apache Spark (PySpark).\nWorking with IBM CDC and Universal Data Mover to manage data replication and movement.\nBig Data & Data Lakehouse Management\nImplementing Apache Iceberg tables for efficient data storage and retrieval.\nManaging distributed data processing with Cloudera Data Platform (CDP).\nEnsuring data lineage, cataloging, and governance for compliance with Bank/regulatory policies.\nOptimization & Performance Tuning\nOptimizing Spark and PySpark jobs for performance and scalability.\nImplementing data partitioning, indexing, and caching to enhance query performance.\nMonitoring and troubleshooting pipeline failures and performance bottlenecks.\nSecurity & Compliance\nEnsuring secure data access, encryption, and masking using Thales CipherTrust.\nImplementing role-based access controls (RBAC) and data governance policies.\nSupporting metadata management and data quality initiatives.\nCollaboration & Automation\nWorking closely with Data Scientists, Analysts, and DevOps teams to integrate data solutions.\nAutomating data workflows using Airflow and implementing CI/CD pipelines with GitLab and Sonatype Nexus.\nSupporting Denodo-based data virtualization for seamless data access\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n4-7 years of experience in big data engineering, data integration, and distributed computing.\nStrong skills in Apache Spark, PySpark, Kafka, SQL, and Cloudera Data Platform (CDP).\nProficiency in Python or Scala for data processing.\nExperience with data pipeline orchestration tools (Apache Airflow, Stonebranch UDM).\nUnderstanding of data security, encryption, and compliance frameworks\n\n\nPreferred technical and professional experience\nExperience in banking or financial services data platforms.\nExposure to Denodo for data virtualization and DGraph for graph-based insights.\nFamiliarity with cloud data platforms (AWS, Azure, GCP).\nCertifications in Cloudera Data Engineering, IBM Data Engineering, or AWS Data Analytics",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'scala', 'pyspark', 'sql', 'spark', 'cloudera', 'continuous integration', 'data analytics', 'data processing', 'airflow', 'big data technologies', 'ci/cd', 'microsoft azure', 'data engineering', 'distributed computing', 'gcp', 'kafka', 'data ingestion', 'gitlab', 'big data', 'aws', 'data integration']",2025-06-10 14:10:23
Data engineer with Gen Ai,Leading Client,4 - 6 years,Not Disclosed,['Chennai'],"We are seeking a skilled Data Engineer who can function as a Data Architect, designing scalable data pipelines, table structures, and ETL workflows. The ideal candidate will be responsible for recommending cost-effective and high-performance data architecture solutions, collaborating with cross-functional teams to enable efficient analytics and data science initiatives.\n\nKey Responsibilities:\n\nDesign and implement ETL workflows, data pipelines, and table structures to support business analytics and data science.\n\nOptimize data storage, retrieval, and processing for cost-efficiency and high performance.\n\nCollaborate with Analytics and Data Science teams for feature engineering and KPI computations.\n\nDevelop and maintain data models for structured and unstructured data.\n\nEnsure data quality, integrity, and security across systems.\n\nWork with cloud platforms (AWS/ Azure/ GCP) to design and manage scalable data architectures.\n\nTechnical Skills Required:\n\nSQL & Python Strong proficiency in writing optimized queries and scripts.\n\nPySpark Hands-on experience with distributed data processing.\n\nCloud Technologies (AWS/ Azure/ GCP) Experience with cloud-based data solutions.\n\nSpark & Airflow Experience with big data frameworks and workflow orchestration.\n\nGen AI (Preferred) Exposure to generative AI applications is a plus.\n\nPreferred Qualifications:\n\nExperience in data modeling, ETL optimization, and performance tuning.\n\nStrong problem-solving skills and ability to work in a fast-paced environment.\n\nPrior experience working with large-scale data processing.",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Gen Ai', 'bigdata frameworks', 'python', 'performance tuning', 'data processing', 'airflow', 'microsoft azure', 'pyspark', 'business analytics', 'data architecture', 'data engineering', 'sql', 'gen', 'data quality', 'query optimization', 'data modeling', 'data science', 'spark', 'gcp', 'hadoop', 'aws', 'etl', 'big data']",2025-06-10 14:10:26
Sr. Data Engineer,Leading Client,6 - 11 years,Not Disclosed,['Bengaluru'],"Were looking for an experienced Senior Data Engineer to lead the design and development\n\nof scalable data solutions at our company. The ideal candidate will have extensive hands-on\n\nexperience in data warehousing, ETL/ELT architecture, and cloud platforms like AWS,\n\nAzure, or GCP. You will work closely with both technical and business teams, mentoring\n\nengineers while driving data quality, security, and performance optimization.\n\nResponsibilities:\n\nLead the design of data warehouses, lakes, and ETL workflows.\nCollaborate with teams to gather requirements and build scalable solutions.\nEnsure data governance, security, and optimal performance of systems.\nMentor junior engineers and drive end-to-end project delivery.:\n6+ years of experience in data engineering, including at least 2 full-cycle datawarehouse projects.\nStrong skills in SQL, ETL tools (e.g., Pentaho, dbt), and cloud platforms.\nExpertise in big data tools (e.g., Apache Spark, Kafka).\nExcellent communication skills and leadership abilities.PreferredExperience with workflow orchestration tools (e.g., Airflow), real-time data,and DataOps practices.",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'hive', 'scala', 'data warehousing', 'sql', 'plsql', 'apache', 'unix shell scripting', 'etl tool', 'spark', 'gcp', 'devops', 'linux', 'pentaho', 'hadoop', 'big data', 'etl', 'python', 'airflow', 'microsoft azure', 'cloud platforms', 'elt', 'sql server', 'data quality', 'kafka', 'data governance', 'aws', 'unix']",2025-06-10 14:10:28
Data Engineer-Data Platforms-Google,IBM,2 - 5 years,Not Disclosed,['Hyderabad'],"As an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\n\n\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['elastic search', 'gcp', 'splunk', 'hadoop', 'big data', 'hive', 'python', 'data management', 'presentation skills', 'microsoft azure', 'machine learning', 'javascript', 'sql', 'docker', 'java', 'git', 'spark', 'linux', 'jenkins', 'html', 'mysql', 'aws']",2025-06-10 14:10:31
Data Engineer-Data Platforms-Google,IBM,2 - 5 years,Not Disclosed,['Gurugram'],"As an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\n\n\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['elastic search', 'gcp', 'splunk', 'hadoop', 'big data', 'hive', 'python', 'data management', 'presentation skills', 'microsoft azure', 'machine learning', 'javascript', 'sql', 'docker', 'java', 'git', 'spark', 'linux', 'jenkins', 'html', 'mysql', 'aws']",2025-06-10 14:10:34
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Mumbai'],"As a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution.\nYour primary responsibilities include:\nLead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements.\nStrive for continuous improvements by testing the build solution and working under an agile framework.\nDiscover and implement the latest technologies trends to maximize and build creative solutions\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience with Apache Spark (PySpark)In-depth knowledge of Spark’s architecture, core APIs, and PySpark for distributed data processing.\nBig Data TechnologiesFamiliarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering\n\nSkills:\nStrong understanding of ETL pipelines, data modeling, and data warehousing concepts.\nStrong proficiency in PythonExpertise in Python programming with a focus on data processing and manipulation. Data Processing FrameworksKnowledge of data processing libraries such as Pandas, NumPy.\nSQL ProficiencyExperience writing optimized SQL queries for large-scale data analysis and transformation.\nCloud PlatformsExperience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems\n\n\nPreferred technical and professional experience\nDefine, drive, and implement an architecture strategy and standards for end-to-end monitoring.\nPartner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering,\nGood to have detection and prevention tools for Company products and Platform and customer-facing",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data processing', 'pyspark', 'data modeling', 'spark', 'data analysis', 'microsoft azure', 'data warehousing', 'cloud platforms', 'numpy', 'data engineering', 'sql', 'pandas', 'spring boot', 'etl pipelines', 'java', 'gcp', 'kafka', 'data warehousing concepts', 'hadoop', 'big data', 'aws', 'etl']",2025-06-10 14:10:37
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Navi Mumbai'],"As a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution.\nYour primary responsibilities include:\nLead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements.\nStrive for continuous improvements by testing the build solution and working under an agile framework.\nDiscover and implement the latest technologies trends to maximize and build creative solutions\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience with Apache Spark (PySpark)In-depth knowledge of Spark’s architecture, core APIs, and PySpark for distributed data processing.\nBig Data TechnologiesFamiliarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering\n\nSkills:\nStrong understanding of ETL pipelines, data modeling, and data warehousing concepts.\nStrong proficiency in PythonExpertise in Python programming with a focus on data processing and manipulation. Data Processing FrameworksKnowledge of data processing libraries such as Pandas, NumPy.\nSQL ProficiencyExperience writing optimized SQL queries for large-scale data analysis and transformation.\nCloud PlatformsExperience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems\n\n\nPreferred technical and professional experience\nDefine, drive, and implement an architecture strategy and standards for end-to-end monitoring.\nPartner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering,\nGood to have detection and prevention tools for Company products and Platform and customer-facing",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data processing', 'pyspark', 'data modeling', 'spark', 'data analysis', 'microsoft azure', 'data warehousing', 'cloud platforms', 'numpy', 'data engineering', 'sql', 'pandas', 'spring boot', 'etl pipelines', 'java', 'gcp', 'kafka', 'data warehousing concepts', 'hadoop', 'big data', 'aws', 'etl']",2025-06-10 14:10:39
Data Engineer,Grid Dynamics,8 - 13 years,Not Disclosed,['Hyderabad'],"Role & responsibilities Details on tech stack\ndatabricks, python, pyspark, Snowflake, SQL\nMin requirements to the candidate\nAdvanced SQL queries, scripts, stored procedures, materialized views, and views\nFocus on ELT to load data into database and perform transformations in database\nAbility to use analytical SQL functions\nSnowflake experience \nCloud Data Warehouse solutions experience (Snowflake, Azure DW, or Redshift); data modeling, analysis, programming\nExperience with DevOps models utilizing a CI/CD tool\nWork in hands-on Cloud environment in Azure Cloud Platform (ADLS, Blob)\nAirflow\nGD Requirements \nGood interpersonal skills; comfort and competence in dealing with different teams within the organization.\nRequires an ability to interface with multiple constituent groups and build sustainable relationships.\nStrong and effective communication skills (verbal and written).\nStrong analytical, problem-solving skills.\nExperience of working in a matrix organization.\nProactive problem solver.\nAbility to prioritize and deliver.\nResults-oriented, flexible, adaptable.\nWork well independently and lead a team.\nVersatile, creative temperament, ability to think out-of-the box while defining sound and practical solutions.\nAbility to master new skills.\nFamiliar with Agile practices and methodologies\nProfessional data engineering experience focused on batch and real-time data pipelines using Spark, Python, SQL\nData warehouse (data modeling, programming)\nExperience working with Snowflake\nExperience working on a cloud environment, preferably, Microsoft Azure Cloud Data Warehouse solutions (Snowflake, Azure DW)\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Pyspark', 'Azure Data Engineering', 'Azure Databricks', 'Python', 'Airflow', 'snowflake', 'airflo', 'SQL']",2025-06-10 14:10:42
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Mumbai'],"Experience with Scala object-oriented/object function Strong SQL background.\nExperience in Spark SQL, Hive, Data Engineer.\nSQL Experience with data pipelines & Data Lake Strong background in distributed comp.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nSQL Experience with data pipelines & Data Lake Strong background in distributed comp\nExperience with Scala object-oriented/object function Strong SQL background\n\n\nPreferred technical and professional experience\nCore Scala Development Experience",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'scala', 'sql', 'spark', 'data lake', 'amazon redshift', 'pyspark', 'data warehousing', 'emr', 'java', 'data modeling', 'mysql', 'hadoop', 'big data', 'etl', 'python', 'microsoft azure', 'machine learning', 'data engineering', 'sql server', 'nosql', 'amazon ec2', 'kafka', 'sqoop', 'aws']",2025-06-10 14:10:45
Senior Data Engineer,Paypal,8 - 13 years,Not Disclosed,['Bengaluru'],"PayPal Marketing Technology team is dedicated to creating a best-in-class platform. We are looking for highly talented, professional, and motivated engineers to join our team. As a Lead Data Engineer on our Marketing Technology Platform, you will be at the forefront of designing and developing backend data pipelines using GCP (BigQuery, Bigtable and Dataproc), Python programming. As part of your responsibilities, you need to provide technical leadership and contribute to the definition, development, integration, test, documentation, and support across multiple platforms including Unix, Python, GCP services.\n\nMeet our team\n\nAt PayPal Marketing Technology Platform, we are very supportive, forward-thinking community of customer-centric technologists. We celebrate our successes, learn from our challenges, and always keep pushing forward. Whether we're brainstorming the next big feature, tackling complex technical challenges, or sharing insights from our latest project, theres a shared sense of purpose and excitement for what we're building. Together, we share a common goal: to build seamless, secure, and scalable solutions that empower individuals and businesses around the globe.\n  Your way to impact\nYour work will directly contribute to PayPal s overarching mission of revolutionizing customer engagement globally. By building, enhancing, and scaling the back-end data pipelines that underpin our marketing technology experiences, you will be a key player in enabling seamless and innovative ways of engagement of our customers worldwide. Your efforts in developing high-quality, secure, and performant software solutions will not only improve user experiences but also drive inclusion and flexibility that is critical in todays digital economy. Your role goes beyond coding; its about making a tangible impact on the lives of millions.\n  Your day to day\nLead, develop, and grow a high performance, multi-function team of talented and passionate professionals, who are results driven to take the business forward and demonstrate superior leadership in line with the PayPal values.\nBuild scalable systems, lead technical discussions, participate in code reviews, and guide the team in engineering best practices.\nWrite quality code and build secure, highly available systems.\nProvide technical leadership and contribute to the definition, development, integration, test, documentation, and support across multiple platforms including Unix, Python, GCP services.\nManage your own project deliverables, timelines, and priorities, effectively balancing multiple tasks to meet project deadlines and performance targets.\nSharing your knowledge and experience to new members to help onboard them onto the team quickly and efficiently, fostering a culture of learning and continuous improvement.\nWhat do you need to bring-\nA bachelors degree in computer science or an equivalent combination of technical education and work experience.\nAt least 8+ years of ETL Expertise ie managing data extraction, transformation, and loading from various sources using advanced SQL and Jupyter Notebooks/Python.\nAt least 3+ years of experience in GCP Cloud services & Streaming Integrations (must).\nAt least 4+ year Experience in design and building highly scalable distributed applications capable of handling very high volume of data in GCP using BigQuery and python.\nStrong conceptual knowledge in Data warehouses, Data marts, distributed data platforms and data lakes, Data Modeling, Schema design and CI/CD\nProven experience in leading teams within a global, complex matrix organization.\nExperience working on SaaS platform(s): Adobe-RTCDP is a plus.\nExperience using Atlassian JIRA, Service Now, Atlassian Confluence tools.\nExperience in delivering projects using Agile Methodology.",Industry Type: FinTech / Payments,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Unix', 'Coding', 'Data modeling', 'Schema', 'Adobe', 'Continuous improvement', 'Customer engagement', 'SQL', 'Data extraction']",2025-06-10 14:10:47
Data Engineer,Grid Dynamics,8 - 13 years,Not Disclosed,['Hyderabad'],"Job Description:\nAdvanced SQL queries, scripts, stored procedures, materialized views, and views\nFocus on ELT to load data into database and perform transformations in database\nAbility to use analytical SQL functions\nSnowflake experience \nCloud Data Warehouse solutions experience (Snowflake, Azure DW, or Redshift); data modeling, analysis, programming\nExperience with DevOps models utilizing a CI/CD tool\nWork in hands-on Cloud environment in Azure Cloud Platform (ADLS, Blob)\nAirflow\n\nPreferred candidate profile\nGood interpersonal skills; comfort and competence in dealing with different teams within the organization.\nRequires an ability to interface with multiple constituent groups and build sustainable relationships.\nStrong and effective communication skills (verbal and written).\nStrong analytical, problem-solving skills.\nExperience of working in a matrix organization.\nProactive problem solver.\nAbility to prioritize and deliver.\nResults-oriented, flexible, adaptable.\nWork well independently and lead a team.\nVersatile, creative temperament, ability to think out-of-the box while defining sound and practical solutions.\nAbility to master new skills.\nFamiliar with Agile practices and methodologies\nProfessional data engineering experience focused on batch and real-time data pipelines using Spark, Python, SQL\nData warehouse (data modeling, programming)\nExperience working with Snowflake\nExperience working on a cloud environment, preferably, Microsoft Azure Cloud Data Warehouse solutions (Snowflake, Azure DW)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Airflow', 'Snowflake', 'Azure Databricks', 'Pyspark', 'Spark', 'Python']",2025-06-10 14:10:49
Data Engineer-Data Platforms-Azure,IBM,3 - 6 years,Not Disclosed,['Bengaluru'],"As Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in the development of data solutions using Spark Framework with Python or Scala on Hadoop and Azure Cloud Data Platform\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nStrong and proven background in Information Technology & working knowledge of .NET Core, C#, REST API, LINQ, Entity Framework, XUnit.\nTroubleshooting issues related to code performance.\nWorking knowledge of Angular 15 or later, Typescript, Jest Framework, HTML 5 and CSS 3 & MS SQL Databases, troubleshooting issues related to DB performance\nGood understanding of CQRS, mediator, repository pattern.\nGood understanding of CI/CD pipelines and SonarQube & messaging and reverse proxy\n\n\nPreferred technical and professional experience\nGood understanding of AuthN and AuthZ techniques like (windows, basic, JWT).\nGood understanding of GIT and it’s process like Pull request.\nMerge, pull, commit Methodology skills like AGILE, TDD, UML",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'css', 'information technology', 'html', 'python', 'cqrs', 'scala', 'ci/cd', 'sql', 'git', 'spark', 'uml', 'typescript', 'hadoop', 'big data', 'rest', 'entity framework', 'sonarqube', 'jest', 'xunit', 'data engineering', 'sql server', 'azure cloud', 'angular', 'linq', 'tdd', 'troubleshooting', 'agile']",2025-06-10 14:10:51
Principal Data Engineer (RDU IT Data Engineering),AstraZeneca India Pvt. Ltd,10 - 15 years,Not Disclosed,['Bengaluru'],"As a Principal Data Engineer at Alexion, you will be at the forefront of developing cutting-edge data integration solutions that cater to the dynamic needs of our global data platforms. Your expertise will be pivotal in designing, implementing, and managing robust data pipelines and integration paradigms. Collaborate closely with diverse IT teams to support data-driven decision-making and strategic initiatives. Your mission will be to build scalable, reliable, and resilient data solutions, enhance data quality and observability, and ensure compliance with industry standards and regulations. Become an advocate for data governance and best practices, empowering Alexion to leverage its data assets for business innovation and success.\nAccountabilities:\nDevelop and maintain high-quality data integration solutions to support business needs and strategic initiatives.\nCollaborate with IT teams to identify data needs, structure problems, and deliver integrated information solutions.\nEnsure the quality and security of Alexion s data through the implementation of best practices in data governance and compliance.\nStay abreast of industry trends and emerging technologies to drive continuous improvement in data engineering practices.\nEssential Skills/Experience:\nmasters Degree in Computer Science, Information Systems, Engineering, or a related field.\nA minimum of 10 years of experience in data engineering, data management, and analytics.\nProven track record of delivering large-scale, scalable, secure, and robust data solutions in the pharmaceutical or life sciences industry.\nStrong experience with SQL, Python, ETL/ELT frameworks, and building data orchestration pipelines.\nExpertise in cloud architectures, particularly AWS.\nProficiency in Snowflake and its features (resource monitors, RBAC controls, etc), dbT, Fivetran, Apache Airflow.\nStrong analytical, problem-solving, and organizational skills.\nAbility to effectively communicate complex data insights and solutions to diverse audiences, including senior leaders.\nAdvanced understanding of data warehousing methodologies and data modeling techniques (Kimball, 3NF, Star Schema, ).\nUnderstanding of data governance, compliance standards (GDPR, HIPAA), and FAIR and TRUSTED data principles.\nDesirable Skills/Experience:\nExtensive experience (5+ years) within the biotech/pharma industry.\nFamiliarity with Kubernetes, Docker/containerization, and Terraform.\nKnowledge of data quality and observability tools and methodologies to enhance data reliability.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data management', 'Data modeling', 'Analytical', 'Pharma', 'Apache', 'biomedical', 'Analytics', 'SQL', 'Python']",2025-06-10 14:10:54
Data Engineer-Data Platforms,IBM,6 - 11 years,Not Disclosed,['Mysuru'],"As an Application Developer, you will lead IBM into the future by translating system requirements into the design and development of customized systems in an agile environment.\nThe success of IBM is in your hands as you transform vital business needs into code and drive innovation. Your work will power IBM and its clients globally, collaborating and integrating code into enterprise systems. You will have access to the latest education, tools and technology, and a limitless career path with the world’s technology leader. Come to IBM and make a global impact\nResponsibilities:\nResponsible to manage end to end feature development and resolve challenges faced in implementing the same\nLearn new technologies and implement the same in feature development within the time frame provided\nManage debugging, finding root cause analysis and fixing the issues reported on Content Management back end software system\nfixing the issues reported on Content Management back end software system\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nOverall, more than 6 years of experience with more than 4+ years of Strong Hands on experience in Python and Spark\nStrong technical abilities to understand, design, write and debug to develop applications on Python and Pyspark.\nGood to Have;- Hands on Experience on cloud technology AWS/GCP/Azure\nstrong problem-solving skill\n\n\nPreferred technical and professional experience\nGood to Have;- Hands on Experience on cloud technology AWS/GCP/Azure",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['pyspark', 'spark', 'gcp', 'python', 'aws', 'hive', 'scala', 'apache pig', 'sql', 'docker', 'java', 'linux', 'mysql', 'hadoop', 'big data', 'etl', 'hbase', 'natural language processing', 'oozie', 'microsoft azure', 'machine learning', 'mapreduce', 'kafka', 'sqoop', 'unix']",2025-06-10 14:10:56
Data Engineer,Global Banking Organization,5 - 10 years,Not Disclosed,['Hyderabad'],"Key Skills: Data Engineer, AI (Artificial intelligence), SQL, Python, Java.\nRoles and Responsibilities:\nArchitect and implement modern, scalable data solutions on cloud platforms, specifically Google Cloud Platform (GCP).\nCollaborate with cross-functional teams to assess, redesign, and modernize legacy data systems.\nDesign and develop efficient ETL pipelines for data extraction, transformation, and loading to support analytics and ML models.\nEnsure robust data governance by maintaining high standards of data security, integrity, and compliance with regulatory requirements.\nMonitor, troubleshoot, and optimize data workflows and pipelines for enhanced system performance and scalability.\nProvide hands-on technical expertise and guidance across data engineering projects, with a focus on cloud adoption and automation.\nWork in an agile environment and contribute to continuous delivery and improvement initiatives.\nExperience Requirements:\n5-10 years experience in designing and implementing data engineering solutions in GCP or other leading cloud platforms.\nSolid understanding of legacy data infrastructure with demonstrated success in modernization and migration projects.\nProficiency in programming languages such as Python and Java for building data solutions and automation scripts.\nStrong SQL skills, with experience in working with both relational (SQL) and non-relational (NoSQL) databases.\nFamiliarity with data warehousing concepts, tools, and practices.\nHands-on experience with data integration tools and frameworks.\nExcellent analytical, problem-solving, and communication skills.\nExperience working in fast-paced agile environments and collaborating with multi-disciplinary teams.\nEducation: B.tech, M.tech, B.com, M.com, MBA, any PG.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Data Engineer', 'Python']",2025-06-10 14:10:59
Data Engineer-Data Platforms-Azure,IBM,5 - 10 years,Not Disclosed,['Bengaluru'],"As an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nWe are seeking a skilled Azure Data Engineer with 5+ years of experience\nIncluding 3+ years of hands-on experience with ADF/Databricks\nThe ideal candidate Data bricks,Data Lake, Phyton programming skills.\nThe candidate will also have experience for deploying to data bricks.\nFamiliarity with Azure Data Factory\n\n\nPreferred technical and professional experience\nGood communication skills.\n3+ years of experience with ADF/DB/DataLake.\nAbility to communicate results to technical and non-technical audiences",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data factory', 'data bricks', 'oracle adf', 'data lake', 'db', 'hive', 'azure databricks', 'python', 'data management', 'azure data lake', 'scala', 'microsoft azure', 'pyspark', 'data warehousing', 'power bi', 'data engineering', 'sql', 'sql azure', 'spark', 'hadoop', 'sqoop', 'big data', 'etl', 'ssis']",2025-06-10 14:11:03
Data Engineer-Data Platforms-Azure,IBM,5 - 7 years,Not Disclosed,['Bengaluru'],"As Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in the development of data solutions using Spark Framework with Python or Scala on Hadoop and Azure Cloud Data Platform\nResponsibilities\nExperienced in building data pipelines to Ingest, process, and transform data from files, streams and databases. Process the data with Spark, Python, PySpark and Hive, Hbase or other NoSQL databases on Azure Cloud Data Platform or HDFS\nExperienced in develop efficient software code for multiple use cases leveraging Spark Framework / using Python or Scala and Big Data technologies for various use cases built on the platform\nExperience in developing streaming pipelines\nExperience to work with Hadoop / Azure eco system components to implement scalable solutions to meet the ever-increasing data volumes, using big data/cloud technologies Apache Spark, Kafka, any Cloud computing etc\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nTotal 5 - 7+ years of experience in Data Management (DW, DL, Data Platform, Lakehouse) and Data Engineering skills\nMinimum 4+ years of experience in Big Data technologies with extensive data engineering experience in Spark / Python or Scala. Minimum 3 years of experience on Cloud Data Platforms on Azure\nExperience in DataBricks / Azure HDInsight / Azure Data Factory, Synapse, SQL Server DB\nExposure to streaming solutions and message brokers like Kafka technologies\nExperience Unix / Linux Commands and basic work experience in Shell Scripting\n\n\nPreferred technical and professional experience\nCertification in Azure and Data Bricks or Cloudera Spark Certified developers",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data management', 'big data technologies', 'microsoft azure', 'data engineering', 'hadoop', 'hive', 'python', 'scala', 'pyspark', 'azure data factory', 'azure hdinsight', 'azure cloud', 'sql server', 'nosql', 'sql', 'data bricks', 'spark', 'kafka', 'linux', 'shell scripting', 'big data', 'hbase', 'unix']",2025-06-10 14:11:06
Senior Data Engineer - Azure,Blend360 India,6 - 11 years,Not Disclosed,['Hyderabad'],"Job Description\nAs a Sr Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\nQualifications\n7+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar fiel",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-10 14:11:09
Senior Data Engineer,Dun & Bradstreet,5 - 9 years,Not Disclosed,['Hyderabad'],"About Us:\nOur global community of colleagues bring a diverse range of experiences and perspectives to our work. You'll find us working from a corporate office or plugging in from a home desk, listening to our customers and collaborating on solutions. Our products and solutions are vital to businesses of every size, scope and industry. And at the heart of our work youll find our core values: to be data inspired, relentlessly curious and inherently generous. Our values are the constant touchstone of our community; they guide our behavior and anchor our decisions.\n\nDesignation: Software Engineer II\nLocation: Hyderabad\n\nKEY RESPONSIBILITIES\n\nDesign, build, and deploy new data pipelines within our Big Data Eco-Systems using Streamsets/Talend/Informatica BDM etc. Document new/existing pipelines, Datasets.\nDesign ETL/ELT data pipelines using StreamSets, Informatica or any other ETL processing engine. Familiarity with Data Pipelines, Data Lakes and modern Data Warehousing practices (virtual data warehouse, push down analytics etc.)\nExpert level programming skills on Python\nExpert level programming skills on Spark\nCloud Based Infrastructure: GCP\nExperience with one of the ETL Informatica, StreamSets in creation of complex parallel loads, Cluster Batch Execution and dependency creation using Jobs/Topologies/Workflows etc.,\nExperience in SQL and conversion of SQL stored procedures into Informatica/StreamSets, Strong exposure working with web service origins/targets/processors/executors, XML/JSON Sources and Restful APIs.\nStrong exposure working with relation databases DB2, Oracle & SQL Server including complex SQL constructs and DDL generation.\nExposure to Apache Airflow for scheduling jobs\nStrong knowledge of Big data Architecture (HDFS), Cluster installation, configuration, monitoring, cluster security, cluster resources management, maintenance, and performance tuning\nCreate POCs to enable new workloads and technical capabilities on the Platform.\nWork with the platform and infrastructure engineers to implement these capabilities in production.\nManage workloads and enable workload optimization including managing resource allocation and scheduling across multiple tenants to fulfill SLAs.\nParticipate in planning activities, Data Science and perform activities to increase platform skills\n\nKEY Requirements\n\nMinimum 6 years of experience in ETL/ELT Technologies, preferably StreamSets/Informatica/Talend etc.,\nMinimum of 6 years hands-on experience with Big Data technologies e.g. Hadoop, Spark, Hive.\nMinimum 3+ years of experience on Spark\nMinimum 3 years of experience in Cloud environments, preferably GCP\nMinimum of 2 years working in a Big Data service delivery (or equivalent) roles focusing on the following disciplines:\nAny experience with NoSQL and Graph databases\nInformatica or StreamSets Data integration (ETL/ELT)\nExposure to role and attribute based access controls\nHands on experience with managing solutions deployed in the Cloud, preferably on GCP\nExperience working in a Global company, working in a DevOps model is a plus\n\nDun & Bradstreet is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, creed, sex, age, national origin, citizenship status, disability status, sexual orientation, gender identity or expression, pregnancy, genetic information, protected military and veteran status, ancestry, marital status, medical condition (cancer and genetic characteristics) or any other characteristic protected by law.\n\nWe are committed to Equal Employment Opportunity and providing reasonable accommodations to qualified candidates and employees. If you are interested in applying for employment with Dun & Bradstreet and need special assistance or an accommodation to use our website or to apply for a position, please send an e-mail with your requesttoacquisitiont@dnb.com Determinationon requests for reasonable accommodation are made on a case-by-case basis.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Bigquery', 'Spark', 'Google Cloud Platforms', 'Python', 'sql', 'Airflow', 'Docker', 'Streamsets', 'Informatica', 'ETL', 'Talend', 'AWS']",2025-06-10 14:11:11
Data Engineer-Data Platforms-Azure,IBM,6 - 7 years,Not Disclosed,['Bengaluru'],"As Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in the development of data solutions using Spark Framework with Python or Scala on Hadoop and Azure Cloud Data Platform\n\nResponsibilities:\nExperienced in building data pipelines to Ingest, process, and transform data from files, streams and databases. Process the data with Spark, Python, PySpark and Hive, Hbase or other NoSQL databases on Azure Cloud Data Platform or HDFS\nExperienced in develop efficient software code for multiple use cases leveraging Spark Framework / using Python or Scala and Big Data technologies for various use cases built on the platform\nExperience in developing streaming pipelines\nExperience to work with Hadoop / Azure eco system components to implement scalable solutions to meet the ever-increasing data volumes, using big data/cloud technologies Apache Spark, Kafka, any Cloud computing etc\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nTotal 6 - 7+ years of experience in Data Management (DW, DL, Data Platform, Lakehouse) and Data Engineering skills\nMinimum 4+ years of experience in Big Data technologies with extensive data engineering experience in Spark / Python or Scala;\nMinimum 3 years of experience on Cloud Data Platforms on Azure;\nExperience in DataBricks / Azure HDInsight / Azure Data Factory, Synapse, SQL Server DB\nGood to excellent SQL skills\n\n\nPreferred technical and professional experience\nCertification in Azure and Data Bricks or Cloudera Spark Certified developers",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data management', 'big data technologies', 'sql', 'python', 'data engineering', 'hive', 'scala', 'pyspark', 'dbms', 'azure data factory', 'spark', 'ssrs', 'hadoop', 'big data', 'cloud computing', 'hbase', 'ssas', 'microsoft azure', 'azure hdinsight', 'azure cloud', 'sql server', 'nosql', 'data bricks', 'kafka', 'ssis']",2025-06-10 14:11:13
Data Engineer,Suzva Software Technologies,7 - 8 years,Not Disclosed,"['Mumbai', 'Hyderabad', 'Chennai']","Data Engineer (Contract | 6 Months)\n\nWe are seeking an experienced Data Engineer to join our team for a 6-month contract assignment. The ideal candidate will work on data warehouse development, ETL pipelines, and analytics enablement using Snowflake, Azure Data Factory (ADF), dbt, and other tools.\n\nThis role requires strong hands-on experience with data integration platforms, documentation, and pipeline optimizationespecially in cloud environments such as Azure and AWS.\n\n#KeyResponsibilities\nBuild and maintain ETL pipelines using Fivetran, dbt, and Azure Data Factory\n\nMonitor and support production ETL jobs\n\nDevelop and maintain data lineage documentation for all systems\n\nDesign data mapping and documentation to aid QA/UAT testing\n\nEvaluate and recommend modern data integration tools\n\nOptimize shared data workflows and batch schedules\n\nCollaborate with Data Quality Analysts to ensure accuracy and integrity of data flows\n\nParticipate in performance tuning and improvement recommendations\n\nSupport BI/MDM initiatives including Data Vault and Data Lakes\n\n#RequiredSkills\n7+ years of experience in data engineering roles\n\nStrong command of SQL, with 5+ years of hands-on development\n\nDeep experience with Snowflake, Azure Data Factory, dbt\n\nStrong background with ETL tools (Informatica, Talend, ADF, dbt, etc.)\n\nBachelor's in CS, Engineering, Math, or related field\n\nExperience in healthcare domain (working with PHI/PII data)\n\nFamiliarity with scripting/programming (Python, Perl, Java, Linux-based environments)\n\nExcellent communication and documentation skills\n\nExperience with BI tools like Power BI, Cognos, etc.\n\nOrganized, self-starter with strong time-management and critical thinking abilities\n\n#NiceToHave\nExperience with Data Lakes and Data Vaults\n\nQA & UAT alignment with clear development documentation\nMulti-cloud experience (especially Azure, AWS)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Azure Data Factory', 'ADF', 'Power BI', 'Cognos', 'Snowflake', 'Informatica', 'ETL', 'UAT testing']",2025-06-10 14:11:15
Data Engineer-Talend DQ,IBM,10 - 15 years,Not Disclosed,['Mumbai'],"Role Overview :\n We are hiring aTalend Data Quality Developerto design and implement robust data quality (DQ) frameworks in a Cloudera-based data lakehouse environment. The role focuses on building rule-driven validation and monitoring processes for migrated data pipelines, ensuring high levels of data trust and regulatory compliance across critical banking domains. \n\n Key Responsibilities :\nDesign and implement data quality rules using Talend DQ Studio , tailored to validate customer, account, transaction, and KYC datasets within the Cloudera Lakehouse.\nCreate reusable templates for profiling, validation, standardization, and exception handling.\nIntegrate DQ checks within PySpark-based ingestion and transformation pipelines targeting Apache Iceberg tables .\nEnsure compatibility with Cloudera components (HDFS, Hive, Iceberg, Ranger, Atlas) and job orchestration frameworks (Airflow/Oozie).\n\n\nPerform initial and ongoing data profiling on source and target systems to detect data anomalies and drive rule definitions.\nMonitor and report DQ metrics through dashboards and exception reports.\nWork closely with data governance, architecture, and business teams to align DQ rules with enterprise definitions and regulatory requirements.\nSupport lineage and metadata integration with tools like Apache Atlas or external catalogs.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n Experience 5–10 years in data management, with 3+ years in  Talend Data Quality  tools.\n\n Platforms Experience in  Cloudera Data Platform (CDP) , with understanding of  Iceberg ,  Hive ,  HDFS , and  Spark ecosystems.\n\n Languages/Tools Talend Studio (DQ module), SQL, Python (preferred), Bash scripting.\n\n Data Concepts Strong grasp of data quality dimensions—completeness, consistency, accuracy, timeliness, uniqueness.\n\n Banking Exposure Experience with financial services data (CIF, AML, KYC, product masters) is highly preferred.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data management', 'talend', 'data quality', 'spark', 'quality tools', 'hive', 'cloudera', 'python', 'metadata', 'data validation', 'oozie', 'airflow', 'financial services', 'data engineering', 'dashboards', 'sql', 'apache', 'data governance', 'apache atlas', 'bash', 'hadoop', 'bash scripting', 'data profiling']",2025-06-10 14:11:18
Data Engineer,Blend360 India,5 - 10 years,Not Disclosed,['Hyderabad'],"We are looking for an experienced Senior Data Engineer with a strong foundation in Python, SQL, and Spark , and hands-on expertise in AWS, Databricks . In this role, you will build and maintain scalable data pipelines and architecture to support analytics, data science, and business intelligence initiatives. You ll work closely with cross-functional teams to drive data reliability, quality, and performance.\nResponsibilities:\nDesign, develop, and optimize scalable data pipelines using Databricks in AWS such as Glue, S3, Lambda, EMR, Databricks notebooks, workflows and jobs.\nBuilding data lake in WS Databricks.\nBuild and maintain robust ETL/ELT workflows using Python and SQL to handle structured and semi-structured data.\nDevelop distributed data processing solutions using Apache Spark or PySpark .\nPartner with data scientists and analysts to provide high-quality, accessible, and well-structured data.\nEnsure data quality, governance, security, and compliance across pipelines and data stores.\nMonitor, troubleshoot, and improve the performance of data systems and pipelines.\nParticipate in code reviews and help establish engineering best practices.\nMentor junior data engineers and support their technical development.\n\n\nRequirements\nBachelors or masters degree in computer science, Engineering, or a related field.\n5+ years of hands-on experience in data engineering , with at lea",Industry Type: Industrial Automation,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Version control', 'GIT', 'Workflow', 'Data quality', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-10 14:11:20
Cloud Data Engineer,PwC India,3 - 8 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Job Description:\n\nWe are seeking skilled and dynamic Cloud Data Engineers specializing in AWS, Azure, Databricks. The ideal candidate will have a strong background in data engineering, with a focus on data ingestion, transformation, and warehousing. They should also possess excellent knowledge of PySpark or Spark, and a proven ability to optimize performance in Spark job executions.\n\nKey Responsibilities:",,,,"['AWS OR Azure', 'Azure Data Engineer OR AWS Data Engineer', 'Azure', 'AWS']",2025-06-10 14:11:23
Data Engineer-Data Platforms-Google,IBM,2 - 5 years,Not Disclosed,['Hyderabad'],"As an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\n\n\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['elastic search', 'gcp', 'splunk', 'hadoop', 'big data', 'hive', 'python', 'data management', 'presentation skills', 'microsoft azure', 'machine learning', 'javascript', 'sql', 'docker', 'java', 'git', 'spark', 'linux', 'jenkins', 'html', 'mysql', 'aws']",2025-06-10 14:11:25
Data Engineer-Data Warehouse,IBM,2 - 5 years,Not Disclosed,['Bengaluru'],"As an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviour’s.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nStrong experience in SQL.\nStrong experience in DBT.\nStrong experience in Data warehousing concepts.\nStrong experience in AWS or any other Cloud knowledge.\nRedshift is good to have\n\n\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['amazon redshift', 'data warehousing', 'sql', 'data warehousing concepts', 'aws', 'hive', 'python', 'sql development', 'data management', 'oracle', 'ssas', 'datastage', 'warehouse', 'sql server', 'plsql', 'unix shell scripting', 'data modeling', 'ssrs', 'hadoop', 'big data', 'etl', 'ssis', 'informatica', 'unix']",2025-06-10 14:11:28
Data Analyst,Capgemini,4 - 6 years,Not Disclosed,['Bengaluru'],"Overview\nWe are seeking a highly motivated Data Analyst with strong technical and analytical skills to join our ADAS (Advanced Driver Assistance Systems) team. This role involves working with large-scale data from vehicle systems to drive insights, support data science initiatives, and contribute to the development of safer and smarter automotive technologies.\n\nResponsibilities:\nPerform data cleansing, aggregation, and analysis on large, complex datasets related to ADAS components and systems.\nBuild, maintain, and update dashboards and data visualizations to communicate insights effectively (Power BI preferred).\nDevelop and optimize data pipelines and ETL processes.\nCreate and maintain technical documentation, including data catalogs and process documentation.\nCollaborate with cross-functional teams including data scientists, software engineers, and system engineers.\nContribute actively to the internal data science community by sharing knowledge, tools, and best practices.\nWork independently on assigned projects, managing priorities and delivering results in a dynamic, unstructured environment.Required Qualifications:\nBachelors degree or higher in Computer Science, Data Science, or a related field.\nMinimum 3 years of experience in the IT industry, with at least 2 years in data analytics or data engineering roles.\nProficient in Python or Pyspark with solid software development fundamentals.\nStrong experience with SQL and relational databases.\nHands-on experience with data science, data engineering, or machine learning techniques.\nKnowledge of data modeling, data warehousing concepts, and ETL processes.\nFamiliarity with data visualization tools (Power BI preferred).\nBasic understanding of cloud platforms such as Azure or AWS.\nFundamental knowledge of ADAS functionalities is a plus.\nStrong problem-solving skills, self-driven attitude, and the ability to manage projects independently.Preferred Skills:\nExperience in automotive data or working with sensor data (e.g., radar, lidar, cameras).\nFamiliarity with agile development methodologies.\nUnderstanding of big data tools and platforms such as Databricks or Spark. Works in the area of Software Engineering, which encompasses the development, maintenance and optimization of software solutions/applications.1. Applies scientific methods to analyse and solve software engineering problems.2. He/she is responsible for the development and application of software engineering practice and knowledge, in research, design, development and maintenance.3. His/her work requires the exercise of original thought and judgement and the ability to supervise the technical and administrative work of other software engineers.4. The software engineer builds skills and expertise of his/her software engineering discipline to reach standard software engineer skills expectations for the applicable role, as defined in Professional Communities.5. The software engineer collaborates and acts as team player with other software engineers and stakeholders.- Grade SpecificIs fully competent in it's own area and has a deep understanding of related programming concepts software design and software development principles. Works autonomously with minimal supervision. Able to act as a key contributor in a complex environment, lead the activities of a team for software design and software development. Acts proactively to understand internal/external client needs and offers advice even when not asked. Able to assess and adapt to project issues, formulate innovative solutions, work under pressure and drive team to succeed against its technical and commercial goals. Aware of profitability needs and may manage costs for specific project/work area. Explains difficult concepts to a variety of audiences to ensure meaning is understood. Motivates other team members and creates informal networks with key contacts outside own area.Skills (competencies)Verbal Communication",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'software development', 'pyspark', 'relational databases', 'sql', 'data analytics', 'software design', 'data warehousing', 'microsoft azure', 'power bi', 'machine learning', 'data engineering', 'data bricks', 'data science', 'data modeling', 'spark', 'adas', 'agile', 'etl', 'aws', 'big data']",2025-06-10 14:11:30
Data Engineer,sanas.ai,2 - 5 years,Not Disclosed,['Bengaluru'],"Sanas is revolutionizing the way we communicate with the world s first real-time algorithm, designed to modulate accents, eliminate background noises, and magnify speech clarity. Pioneered by seasoned startup founders with a proven track record of creating and steering multiple unicorn companies, our groundbreaking GDP-shifting technology sets a gold standard.\n\nSanas is a 200-strong team, established in 2020. In this short span, we ve successfully secured over $100 million in funding. Our innovation have been supported by the industry s leading investors, including Insight Partners, Google Ventures, Quadrille Capital, General Catalyst, Quiet Capital, and other influential investors. Our reputation is further solidified by collaborations with numerous Fortune 100 companies. With Sanas, you re not just adopting a product; you re investing in the future of communication.\n\nWe re looking for a sharp, hands-on Data Engineer to help us build and scale the data infrastructure that powers cutting-edge audio and speech AI products. You ll be responsible for designing robust pipelines, managing high-volume audio data, and enabling machine learning teams to access the right data fast.\n\nAs one of the first dedicated data engineers on the team, youll play a foundational role in shaping how we handle data end-to-end, from ingestion to training-ready features. Youll work closely with ML engineers, research scientists, and product teams to ensure data is clean, accessible, and structured for experimentation and production.\nKey Responsibilities :\nBuild scalable, fault-tolerant pipelines for ingesting, processing, and transforming large volumes of audio and metadata.\nDesign and maintain ETL workflows for training and evaluating ML models, using tools like Airflow or custom pipelines.\nCollaborate with ML research scientists to make raw and derived audio features (e.g., spectrograms, MFCCs) efficiently available for training and inference.\nManage and organize datasets, including labeling workflows, versioning, annotation pipelines, and compliance with privacy policies.\nImplement data quality, observability, and validation checks across critical data pipelines.\nHelp optimize data storage and compute strategies for large-scale training.\nQualifications :\n2-5 years of experience as a Data Engineer, Software Engineer, or similar role with a focus on data infrastructure.\nProficient in Python, SQL, and working with distributed data processing tools (e.g., Spark, Dask, Beam).\nExperience with cloud data infrastructure (AWS/GCP), object storage (e.g.,S3), and data orchestration tools.\nFamiliarity with audio data and its unique challenges (large file sizes, time-series features, metadata handling) is a strong plus.\nComfortable working in a fast-paced, iterative startup environment where systems are constantly evolving.\nStrong communication skills and a collaborative mindset you ll be working cross-functionally with ML, infra, and product teams.\nNice to Have :\nExperience with data for speech models like ASR, TTS, or speaker verification.\nKnowledge of real-time data processing (e.g., Kafka, WebSockets, or low-latency APIs).\nBackground in MLOps, feature engineering, or supporting model lifecycle workflows.\nExperience with labeling tools, audio annotation platforms, or human-in-the-loop systems.\nJoining us means contributing to the world s first real-time speech understanding platform revolutionizing Contact Centers and Enterprises alike.\n\n\nOur technology empowers agents, transforms customer experiences, and drives measurable growth. But this is just the beginning. Youll be part of a team exploring the vast potential of an increasingly sonic future",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ASR', 'Training', 'metadata', 'orchestration', 'GCP', 'Machine learning', 'Data processing', 'Data quality', 'SQL', 'Python']",2025-06-10 14:11:32
Aws Data Engineer,PwC India,4 - 8 years,Not Disclosed,['Bengaluru'],"If Interested, please apply on the link- https://forms.office.com/r/F6vDxdBwuq\n\n\n\nAWS Data Engineer Technology: AWS (EMR, Redshift, S3, Glue, Kinesis and Lambda), SQL, ETL, Python/Java\nExperience: 4 to 8 years\nJob Location: Bangalore",,,,"['Java', 'ETL', 'AWS', 'SQL', 'Python']",2025-06-10 14:11:34
Data Engineer,KC International School,8 - 13 years,Not Disclosed,['Chennai'],"KC International School is looking for Data Engineer to join our dynamic team and embark on a rewarding career journey\nLiaising with coworkers and clients to elucidate the requirements for each task.\nConceptualizing and generating infrastructure that allows big data to be accessed and analyzed.\nReformulating existing frameworks to optimize their functioning.\nTesting such structures to ensure that they are fit for use.\nPreparing raw data for manipulation by data scientists.\nDetecting and correcting errors in your work.\nEnsuring that your work remains backed up and readily accessible to relevant coworkers.\nRemaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.\n\n\nThe DE at KC will design, develop and maintain all school data infrastructure ensuring accurate and efficient data management.",Industry Type: Education / Training,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['hive', 'cloudera', 'python', 'data analysis', 'scala', 'oozie', 'airflow', 'data warehousing', 'pyspark', 'apache pig', 'machine learning', 'data engineering', 'sql', 'mapreduce', 'spark', 'hadoop', 'sqoop', 'big data', 'aws', 'etl', 'hbase']",2025-06-10 14:11:36
Data Engineer-Data Platforms-Google,IBM,2 - 5 years,Not Disclosed,['Gurugram'],"As an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets\n\nIn this role, your responsibilities may include\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\n\n\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['elastic search', 'gcp', 'splunk', 'hadoop', 'big data', 'hive', 'python', 'data management', 'presentation skills', 'microsoft azure', 'machine learning', 'javascript', 'sql', 'docker', 'java', 'git', 'spark', 'linux', 'jenkins', 'html', 'mysql', 'aws']",2025-06-10 14:11:39
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Kochi'],"As a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution\nYour primary responsibilities include:\nLead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements.\nStrive for continuous improvements by testing the build solution and working under an agile framework.\nDiscover and implement the latest technologies trends to maximize and build creative solutions\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience with Apache Spark (PySpark)In-depth knowledge of Spark’s architecture, core APIs, and PySpark for distributed data processing.\nBig Data TechnologiesFamiliarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering\n\nSkills:\nStrong understanding of ETL pipelines, data modeling, and data warehousing concepts.\n*\nStrong proficiency in PythonExpertise in Python programming with a focus on data processing and manipulation. Data Processing FrameworksKnowledge of data processing libraries such as Pandas, NumPy.\nSQL ProficiencyExperience writing optimized SQL queries for large-scale data analysis and transformation.\nCloud PlatformsExperience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems\n\n\nPreferred technical and professional experience\nDefine, drive, and implement an architecture strategy and standards for end-to-end monitoring.\nPartner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering,\nGood to have detection and prevention tools for Company products and Platform and customer-facing",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data processing', 'pyspark', 'data modeling', 'spark', 'data analysis', 'microsoft azure', 'data warehousing', 'cloud platforms', 'numpy', 'data engineering', 'sql', 'pandas', 'spring boot', 'etl pipelines', 'java', 'gcp', 'kafka', 'data warehousing concepts', 'hadoop', 'big data', 'aws', 'etl']",2025-06-10 14:11:42
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Pune'],"As a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution.\nYour primary responsibilities include:\nLead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements.\nStrive for continuous improvements by testing the build solution and working under an agile framework.\nDiscover and implement the latest technologies trends to maximize and build creative solutions\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience with Apache Spark (PySpark)In-depth knowledge of Spark’s architecture, core APIs, and PySpark for distributed data processing.\nBig Data TechnologiesFamiliarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering\n\nSkills:\nStrong understanding of ETL pipelines, data modeling, and data warehousing concepts.\nStrong proficiency in PythonExpertise in Python programming with a focus on data processing and manipulation. Data Processing FrameworksKnowledge of data processing libraries such as Pandas, NumPy.\nSQL ProficiencyExperience writing optimized SQL queries for large-scale data analysis and transformation.\nCloud PlatformsExperience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems\n\n\nPreferred technical and professional experience\nDefine, drive, and implement an architecture strategy and standards for end-to-end monitoring.\nPartner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering,\nGood to have detection and prevention tools for Company products and Platform and customer-facing",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data processing', 'pyspark', 'data modeling', 'spark', 'data analysis', 'microsoft azure', 'data warehousing', 'cloud platforms', 'numpy', 'data engineering', 'sql', 'pandas', 'spring boot', 'etl pipelines', 'java', 'gcp', 'kafka', 'data warehousing concepts', 'hadoop', 'big data', 'aws', 'etl']",2025-06-10 14:11:45
Lead Data Engineer,Acuity Knowledge Partners,8 - 13 years,20-25 Lacs P.A.,"['Pune', 'Bangalore Rural', 'Gurugram']","Desired Skills and experience\n9+ years of experience in software development with a focus on data projects using Python, PySpark, and associated frameworks.\nProven experience as a Data Engineer with experience in Azure cloud.\nExperience implementing solutions using Azure cloud services, Azure Data Factory, Azure Lake Gen 2, Azure Databases, Azure Data Fabric, API Gateway management, Azure Functions.",,,,"['Data Engineering', 'Python', 'Pyspark', 'ETL', 'SQL']",2025-06-10 14:11:47
Data Engineer-Data Platforms-Google,IBM,5 - 7 years,Not Disclosed,['Bengaluru'],"Skilled Multiple GCP services - GCS, BigQuery, Cloud SQL, Dataflow, Pub/Sub, Cloud Run, Workflow, Composer, Error reporting, Log explorer etc.\nMust have Python and SQL work experience & Proactive, collaborative and ability to respond to critical situation\nAbility to analyse data for functional business requirements & front face customer\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n5 to 7 years of relevant experience working as technical analyst with Big Query on GCP platform.\nSkilled in multiple GCP services - GCS, Cloud SQL, Dataflow, Pub/Sub, Cloud Run, Workflow, Composer, Error reporting, Log explorer\nYou love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies\nAmbitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work\n\n\nPreferred technical and professional experience\nCreate up to 3 bullets maxitive individual with an ability to manage change and proven time management\nProven interpersonal skills while contributing to team effort by accomplishing related results as needed\nUp-to-date technical knowledge by attending educational workshops, reviewing publications (encouraging then to focus on required skills)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'gcp', 'bigquery', 'cloud sql', 'python', 'hive', 'gen', 'java', 'postgresql', 'spark', 'linux', 'mysql', 'hadoop', 'big data', 'pubsub', 'airflow', 'application engine', 'machine learning', 'sql server', 'dataproc', 'cloud storage', 'bigtable', 'agile', 'sqoop', 'aws', 'data flow']",2025-06-10 14:11:49
DATA ENGINEER-ADVANCED ANALYTICS,IBM,5 - 10 years,Not Disclosed,['Gurugram'],"Develop, test and support future-ready data solutions for customers across industry verticals\nDevelop, test, and support end-to-end batch and near real-time data flows/pipelines\nDemonstrate understanding in data architectures, modern data platforms, big data, analytics, cloud platforms, data governance and information management and associated technologies\nCommunicates risks and ensures understanding of these risks.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nMinimum of 5+ years of related experience required\nExperience in modeling and business system designs\nGood hands-on experience on DataStage, Cloud based ETL Services\nHave great expertise in writing TSQL code\nWell versed with data warehouse schemas and OLAP techniques\n\n\nPreferred technical and professional experience\nAbility to manage and make decisions about competing priorities and resources.\nAbility to delegate where appropriate\nMust be a strong team player/leader\nAbility to lead Data transformation project with multiple junior data engineers\nStrong oral written and interpersonal skills for interacting and throughout all levels of the organization.\nAbility to clearly communicate complex business problems and technical solutions.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data architecture', 'big data', 'information management', 'cloud platforms', 'data governance', 'schema', 't-sql', 'ansible', 'docker', 'sql', 'java', 'git', 'devops', 'linux', 'jenkins', 'j2ee', 'shell scripting', 'mysql', 'etl', 'python', 'data analytics', 'datastage', 'microsoft azure', 'warehouse', 'olap', 'aws']",2025-06-10 14:11:51
Data Engineer-Data Platforms,IBM,6 - 11 years,Not Disclosed,['Pune'],"As an Application Developer, you will lead IBM into the future by translating system requirements into the design and development of customized systems in an agile environment.\n\nThe success of IBM is in your hands as you transform vital business needs into code and drive innovation. Your work will power IBM and its clients globally, collaborating and integrating code into enterprise systems. You will have access to the latest education, tools and technology, and a limitless career path with the world’s technology leader. Come to IBM and make a global impact\n\nResponsibilities:\nResponsible to manage end to end feature development and resolve challenges faced in implementing the same\nLearn new technologies and implement the same in feature development within the time frame provided\nManage debugging, finding root cause analysis and fixing the issues reported on Content Management back end software system\nfixing the issues reported on Content Management back-end software system\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nOverall, more than 6 years of experience with more than 4+ years of Strong Hands on experience in Python and Spark\nStrong technical abilities to understand, design, write and debug to develop applications on Python and Pyspark.\nGood to Have;- Hands on Experience on cloud technology AWS/GCP/Azure\nstrong problem-solving skill\n\n\nPreferred technical and professional experience\n\nGood to Have;- Hands on Experience on cloud technology AWS/GCP/Azure",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['pyspark', 'spark', 'gcp', 'python', 'aws', 'hive', 'scala', 'apache pig', 'sql', 'docker', 'java', 'linux', 'mysql', 'hadoop', 'big data', 'etl', 'hbase', 'natural language processing', 'oozie', 'microsoft azure', 'machine learning', 'mapreduce', 'kafka', 'sqoop', 'unix']",2025-06-10 14:11:54
Data Engineer-Data Platforms,IBM,4 - 7 years,Not Disclosed,['Gurugram'],"A Data Engineer specializing in enterprise data platforms, experienced in building, managing, and optimizing data pipelines for large-scale environments. Having expertise in big data technologies, distributed computing, data ingestion, and transformation frameworks.\nProficient in Apache Spark, PySpark, Kafka, and Iceberg tables, and understand how to design and implement scalable, high-performance data processing solutions.What you’ll doAs a Data Engineer – Data Platform Services, responsibilities include:\n\nData Ingestion & Processing\nDesigning and developing data pipelines to migrate workloads from IIAS to Cloudera Data Lake.\nImplementing streaming and batch data ingestion frameworks using Kafka, Apache Spark (PySpark).\nWorking with IBM CDC and Universal Data Mover to manage data replication and movement.\nBig Data & Data Lakehouse Management\nImplementing Apache Iceberg tables for efficient data storage and retrieval.\nManaging distributed data processing with Cloudera Data Platform (CDP).\nEnsuring data lineage, cataloging, and governance for compliance with Bank/regulatory policies.\nOptimization & Performance Tuning\nOptimizing Spark and PySpark jobs for performance and scalability.\nImplementing data partitioning, indexing, and caching to enhance query performance.\nMonitoring and troubleshooting pipeline failures and performance bottlenecks.\nSecurity & Compliance\nEnsuring secure data access, encryption, and masking using Thales CipherTrust.\nImplementing role-based access controls (RBAC) and data governance policies.\nSupporting metadata management and data quality initiatives.\nCollaboration & Automation\nWorking closely with Data Scientists, Analysts, and DevOps teams to integrate data solutions.\nAutomating data workflows using Airflow and implementing CI/CD pipelines with GitLab and Sonatype Nexus.\nSupporting Denodo-based data virtualization for seamless data access\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n4-7 years of experience in big data engineering, data integration, and distributed computing.\nStrong skills in Apache Spark, PySpark, Kafka, SQL, and Cloudera Data Platform (CDP).\nProficiency in Python or Scala for data processing.\nExperience with data pipeline orchestration tools (Apache Airflow, Stonebranch UDM).\nUnderstanding of data security, encryption, and compliance frameworks\n\n\nPreferred technical and professional experience\nExperience in banking or financial services data platforms.\nExposure to Denodo for data virtualization and DGraph for graph-based insights.\nFamiliarity with cloud data platforms (AWS, Azure, GCP).\nCertifications in Cloudera Data Engineering, IBM Data Engineering, or AWS Data Analytics",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'scala', 'pyspark', 'sql', 'spark', 'cloudera', 'continuous integration', 'data analytics', 'data processing', 'airflow', 'big data technologies', 'ci/cd', 'microsoft azure', 'data engineering', 'distributed computing', 'gcp', 'kafka', 'data ingestion', 'gitlab', 'big data', 'aws', 'data integration']",2025-06-10 14:11:56
Data Engineer-Data Platforms,IBM,5 - 10 years,Not Disclosed,['Mumbai'],"As a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\n\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.\nGood working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\n\n\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['scala', 'hadoop spark', 'spark', 'big data', 'python', 'hive', 'cloudera', 'pyspark', 'sql', 'java', 'git', 'postgresql', 'devops', 'jenkins', 'data ingestion', 'mysql', 'hadoop', 'etl', 'hbase', 'data analysis', 'dynamo db', 'oozie', 'microsoft azure', 'impala', 'data engineering', 'lambda expressions', 'kafka', 'sqoop', 'aws']",2025-06-10 14:11:59
Data Engineer-Data Platforms-AWS,IBM,5 - 7 years,Not Disclosed,['Bengaluru'],"As Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in the development of data solutions using Spark Framework with Python or Scala on Hadoop and AWS Cloud Data Platform\nExperienced in building data pipelines to Ingest, process, and transform data from files, streams and databases. Process the data with Spark, Python, PySpark, Scala, and Hive, Hbase or other NoSQL databases on Cloud Data Platforms (AWS) or HDFS\nExperienced in develop efficient software code for multiple use cases leveraging Spark Framework / using Python or Scala and Big Data technologies for various use cases built on the platform\nExperience in developing streaming pipelines\nExperience to work with Hadoop / AWS eco system components to implement scalable solutions to meet the ever-increasing data volumes, using big data/cloud technologies Apache Spark, Kafka, any Cloud computing etc\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nTotal 5 - 7+ years of experience in Data Management (DW, DL, Data Platform, Lakehouse) and Data Engineering skills\nMinimum 4+ years of experience in Big Data technologies with extensive data engineering experience in Spark / Python or Scala.\nMinimum 3 years of experience on Cloud Data Platforms on AWS; Exposure to streaming solutions and message brokers like Kafka technologies.\nExperience in AWS EMR / AWS Glue / DataBricks, AWS RedShift, DynamoDB\nGood to excellent SQL skills\n\n\nPreferred technical and professional experience\nCertification in AWS and Data Bricks or Cloudera Spark Certified developers\n AWS S3 ,  Redshift , and  EMR  for data storage and distributed processing.\n AWS Lambda ,  AWS Step Functions , and  AWS Glue  to build  serverless, event-driven data workflows  and orchestrate ETL processes",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data management', 'scala', 'big data technologies', 'sql', 'data engineering', 'hive', 'cloudera', 'glue', 'amazon redshift', 'pyspark', 'emr', 'spark', 'aws cloud', 'aws emr', 'hadoop', 'big data', 'etl', 'hbase', 'python', 'dynamo db', 'serverless', 'aws lambda', 'nosql', 'aws glue', 'data bricks', 'kafka', 'aws']",2025-06-10 14:12:01
Data Engineer-Data Platforms,IBM,5 - 10 years,Not Disclosed,['Pune'],"As a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\n\nYour primary responsibilities include\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.\nGood working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\n\n\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['scala', 'hadoop spark', 'spark', 'big data', 'python', 'hive', 'cloudera', 'pyspark', 'sql', 'java', 'git', 'postgresql', 'devops', 'jenkins', 'data ingestion', 'mysql', 'hadoop', 'etl', 'hbase', 'data analysis', 'dynamo db', 'oozie', 'microsoft azure', 'impala', 'data engineering', 'lambda expressions', 'kafka', 'sqoop', 'aws']",2025-06-10 14:12:03
Data Engineer-Data Platforms,IBM,5 - 10 years,Not Disclosed,['Navi Mumbai'],"As a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\n\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.\nGood working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\n\n\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['scala', 'hadoop spark', 'spark', 'big data', 'python', 'hive', 'cloudera', 'pyspark', 'sql', 'java', 'git', 'postgresql', 'devops', 'jenkins', 'data ingestion', 'mysql', 'hadoop', 'etl', 'hbase', 'data analysis', 'dynamo db', 'oozie', 'microsoft azure', 'impala', 'data engineering', 'lambda expressions', 'kafka', 'sqoop', 'aws']",2025-06-10 14:12:06
Sr. Azure Data Engineer,Tech Mahindra,5 - 8 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","Roles and Responsibilities :\nDesign, develop, and maintain large-scale data pipelines using Azure Data Factory (ADF) to extract, transform, and load data from various sources into Azure Databricks.\nCollaborate with cross-functional teams to understand business requirements and design scalable solutions for big data processing using PySpark on Azure Databricks.\nDevelop complex SQL queries to optimize database performance and troubleshoot issues in Azure SQL databases.\nEnsure high availability of critical systems by implementing monitoring tools such as Prometheus and Grafana.\nJob Requirements :\nExperience in designing and developing large-scale data pipelines using ADF or similar technologies.\nStrong proficiency in Python programming language with experience working with libraries like Pandas, NumPy, etc.\nExperience working with Azure Databricks platform including creating clusters, managing workloads, and optimizing resource utilization.\nProficiency in writing complex SQL queries for querying relational databases.",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Pyspark', 'Azure Data Lake', 'Python', 'SQL']",2025-06-10 14:12:09
Data Engineer III - PostgreSQL & MongoDB Admin,JPMorgan Chase Bank,0 - 6 years,Not Disclosed,['Hyderabad'],"Be part of a dynamic team where your distinctive skills will contribute to a winning culture and team.\nAs a Data Engineer III at JPMorgan Chase within the Corporate Technology , you serve as a seasoned member of an agile team to design and deliver trusted data collection, storage, access, and analytics solutions in a secure, stable, and scalable way. You are responsible for developing, testing, and maintaining critical data pipelines and architectures across multiple technical areas within various business functions in support of the firm s business objectives.\nJob responsibilities\nSupport the review of controls to ensure sufficient protection of enterprise data.\nAdvise and make custom configuration changes in one to two tools to generate a product at the business or customer request.\nUpdate logical or physical data models based on new use cases.\nFrequently use SQL and understand NoSQL databases and their niche in the marketplace.\nImplement backup, recovery, and disaster recovery (DR) plans.\nMonitor database capacity, space, logs, and performance metrics.\nManage database security, access control, and user privilege management.\nConduct database health checks, troubleshoot, and resolve issues in real-time.\nScript using Shell and Python, and utilize database tools.\nConfigure and maintain MongoDB replica sets, sharding, and failover mechanisms.\nRequired qualifications, capabilities, and skills\nFormal training or certification in software engineering concepts and 3+ years of applied experience.\nExperience across the data lifecycle\nAdvanced at SQL (e. g. , joins and aggregations)\nWorking understanding of NoSQL databases\nExpertise in PostgreSQL MongoDB DBA.\nStrong expertise in SQL, PL/pgSQL, and NoSQL (MongoDB queries, aggregation, and indexing).\nHands-on experience with PostgreSQL replication, partitioning, and tuning.\nExperience managing MongoDB Atlas, Sharded Clusters, and Performance tuning.\nFamiliarity with database monitoring tools such as Prometheus, Grafana, or CloudWatch.\nStrong knowledge of database security best practices and encryption techniques.\nExperience in automating DB tasks using Bash, Python, or Ansible.\nPreferred qualifications, capabilities, and skills\nMongoDB Certified DBA\nPostgreSQL Professional Certification\nAWS Certified Database - Specialty / Data Engineer (preferred but not mandatory)\nExperience working with cloud-based databases (AWS RDS, Azure Cosmos DB, GCP Cloud SQL) is a plus.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'NoSQL', 'Postgresql', 'Disaster recovery', 'Database administration', 'Agile', 'MongoDB', 'Analytics', 'SQL', 'Python']",2025-06-10 14:12:11
Data Engineer-Data Integration,IBM,2 - 5 years,Not Disclosed,['Pune'],"As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExpertise in Data warehousing/ information Management/ Data Integration/Business Intelligence using ETL tool Informatica PowerCenter\nKnowledge of Cloud, Power BI, Data migration on cloud skills.\nExperience in Unix shell scripting and python\nExperience with relational SQL, Big Data etc\n\n\nPreferred technical and professional experience\nKnowledge of MS-Azure Cloud\nExperience in Informatica PowerCenter\nExperience in Unix shell scripting and python",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['information management', 'data warehousing', 'business intelligence', 'etl', 'data integration', 'python', 'data management', 'informatica powercenter', 'power bi', 'relational sql', 'data migration', 'azure cloud', 'sql', 'unix shell scripting', 'java', 'etl tool', 'big data', 'informatica', 'unix']",2025-06-10 14:12:14
Data Engineer-Data Integration,IBM,2 - 5 years,Not Disclosed,['Pune'],"As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\n\n\n Your primary responsibilities include: \nDevelop & maintain data pipelines for batch & stream processing using informatica power centre or cloud ETL/ELT tools.\nLiaise with business team and technical leads, gather requirements, identify data sources, identify data quality issues, design target data structures, develop pipelines and data processing routines, perform unit testing and support UAT.\nWork with data scientist and business analytics team to assist in data ingestion and data-related technical issues.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExpertise in Data warehousing/ information Management/ Data Integration/Business Intelligence using ETL tool Informatica PowerCenter\nKnowledge of Cloud, Power BI, Data migration on cloud skills.\nExperience in Unix shell scripting and python\nExperience with relational SQL, Big Data etc\n\n\nPreferred technical and professional experience\nKnowledge of MS-Azure Cloud\nExperience in Informatica PowerCenter\nExperience in Unix shell scripting and python",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['information management', 'data warehousing', 'business intelligence', 'etl', 'data integration', 'python', 'informatica powercenter', 'power bi', 'relational sql', 'data migration', 'azure cloud', 'sql', 'elastic search', 'unix shell scripting', 'splunk', 'agile', 'big data', 'informatica']",2025-06-10 14:12:17
Sr Manager of Data Engineering,JPMorgan Chase Bank,5 - 10 years,Not Disclosed,['Hyderabad'],"You have the opportunity to unleash your full potential at a world-renowned company and take the lead in shaping the future of technology.\nAs a Senior Manager of Data Engineering at JPMorgan Chase within the CCB, you serve in a leadership role by providing technical coaching and advisory for multiple technical teams, as we'll as anticipate the needs and potential dependencies of other data users within the firm. As an expert in your field, your insights influence budget and technical considerations to advance operational efficiencies and functionalities.\n  Job responsibilities\nArchitect and oversee the design of complex data solutions that meet diverse business needs and customer requirements.\nGuide the evolution of logical and physical data models to support emerging business use cases and technological advancements.\nBuild and manage end-to-end cloud-native data pipelines in AWS, leveraging your hands-on expertise with AWS components.\nBuild analytical systems from the ground up, providing architectural direction, translating business issues into specific requirements, and identifying appropriate data to support solutions.\nWork across the Service Delivery Lifecycle on engineering major/minor enhancements and ongoing maintenance of existing applications.\nHelp others build code to extract raw data, coach the team on techniques to validate its quality, and apply your deep data knowledge to ensure the correct data is ingested across the pipeline.\nGuide the development of data tools used to transform, manage, and access data, and advise the team on writing and validating code to test the storage and availability of data platforms for resilience.\nOversee the implementation of performance monitoring protocols across data pipelines, coaching the team on building visualizations and aggregations to monitor pipeline health.\nCoach others on implementing solutions and self-healing processes that minimize points of failure across multiple product features.\nAdds to team culture of diversity, equity, inclusion, and respect\nRequired qualifications, capabilities, and skills\nFormal training or certification on software engineering concepts and 5+ years applied experience\nExtensive experience in managing the full lifecycle of data, from collection and storage to analysis and reporting.\nProficiency in one or more large-scale data processing distributions such as JavaSpark along with knowledge on Data Pipeline (DPL), Data Modeling, Data warehouse, Data Migration and so-on.\nHands-on practical experience in system design, application development, testing, and operational stability\nProficient in coding in one or more modern programming languages\nShould have good hands-on experience with AWS services and its components along with good understanding on Kubernetes.\nExperience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages.\nStrong understanding of domain driven design, micro-services patterns, and architecture\nOverall knowledge of the Software Development Life Cycle along with experience with IBM MQ, Apache Kafka\nSolid understanding of agile methodologies such as CI/CD, Application Resiliency, and Security\nDemonstrated knowledge of software applications and technical processes within a technical discipline (eg, cloud, LLMs etc)\nPreferred qualifications, capabilities, and skills\nFamiliarity with modern front-end technologies\nExperience designing and building REST API services using Java\nExposure to cloud technologies - knowledge on Hybrid cloud architectures is highly desirable.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data migration', 'Front end', 'Data modeling', 'Coding', 'Analytical', 'Debugging', 'Agile', 'System design', 'Application development', 'Apache']",2025-06-10 14:12:19
Aws Data Engineer | Gurgaon | Deloitte,Deloitte,4 - 9 years,12-22 Lacs P.A.,"['Gurugram', 'Bengaluru']","To Apply - Submit Details via Google Form - https://forms.gle/8SUxUV2cikzjvKzD9\n\nAs a Senior Consultant in our Consulting team, youll build and nurture positive working relationships with teams and clients with the intention to exceed client expectations\nSeeking experienced AWS Data Engineers to design, implement, and maintain robust data pipelines and analytics solutions using AWS services. The ideal candidate will have a strong background in AWS data services, big data technologies, and programming languages. \n\nRole & responsibilities\n1. Design and implement scalable, high-performance data pipelines using AWS services \n2. Develop and optimize ETL processes using AWS Glue, EMR, and Lambda \n3. Build and maintain data lakes using S3 and Delta Lake \n4. Create and manage analytics solutions using Amazon Athena and Redshift \n5. Design and implement database solutions using Aurora, RDS, and DynamoDB \n6. Develop serverless workflows using AWS Step Functions \n7. Write efficient and maintainable code using Python/PySpark, and SQL/PostgrSQL \n8. Ensure data quality, security, and compliance with industry standards \n9. Collaborate with data scientists and analysts to support their data needs \n10. Optimize data architecture for performance and cost-efficiency \n11. Troubleshoot and resolve data pipeline and infrastructure issues \n\nPreferred candidate profile\n1. Bachelors degree in computer science, Information Technology, or related field \n2. Relevant years of experience as a Data Engineer, with at least 60% of experience focusing on AWS \n3. Strong proficiency in AWS data services: Glue, EMR, Lambda, Athena, Redshift, S3\n4. Experience with data lake technologies, particularly Delta Lake \n5. Expertise in database systems: Aurora, RDS, DynamoDB, PostgreSQL\n6. Proficiency in Python and PySpark programming \n7. Strong SQL skills and experience with PostgreSQL\n8. Experience with AWS Step Functions for workflow orchestration \n\nTechnical Skills: \n- AWS Services: Glue, EMR, Lambda, Athena, Redshift, S3, Aurora, RDS, DynamoDB, Step Functions \n- Big Data: Hadoop, Spark, Delta Lake\n- Programming: Python, PySpark\n- Databases: SQL, PostgreSQL, NoSQL\n- Data Warehousing and Analytics \n- ETL/ELT processes \n- Data Lake architectures \n- Version control: Git \n- Agile methodologies",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Aws Data Lake', 'Etl Process', 'Python', 'Postgresql', 'Hadoop', 'Aws Emr', 'Aws Dms', 'Aurora Db', 'Aws Lambda', 'Data Pipeline', 'Redshift Aws', 'Aws Aurora', 'Hadoop Spark', 'AWS', 'Etl Pipelines', 'Pyspark', 'Aura Framework', 'Aws Glue', 'Amazon Redshift', 'Dynamo Db', 'Delta Lake', 'Nosql Databases', 'Data Lake', 'ETL', 'Athena', 'Amazon Rds']",2025-06-10 14:12:22
Data Engineer-Data Platforms-Azure,IBM,6 - 7 years,Not Disclosed,['Kochi'],"As an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nTotal Exp-6-7 Yrs (Relevant-4-5 Yrs)\nMandatory\n\nSkills:\nAzure Databricks, Python/PySpark, SQL, Github, - Azure Devops - Azure Blob\nAbility to use programming languages like Java, Python, Scala, etc., to build pipelines to extract and transform data from a repository to a data consumer\nAbility to use Extract, Transform, and Load (ETL) tools and/or data integration, or federation tools to prepare and transform data as needed.\nAbility to use leading edge tools such as Linux, SQL, Python, Spark, Hadoop and Java\n\n\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['pyspark', 'azure devops', 'sql', 'azure databricks', 'python', 'data management', 'scala', 'azure data factory', 'java', 'sql azure', 'spark', 'linux', 'ssrs', 'hadoop', 'big data', 'etl', 'github', 'azure data lake', 'ssas', 'azure blob', 'microsoft azure', 'power bi', 'sql server', 'ssis', 'data integration']",2025-06-10 14:12:24
NLP Data Engineer - Risk Insights & Monitoring,MNC IT,10 - 12 years,25-30 Lacs P.A.,"['Pune', 'Mumbai (All Areas)']","Design and implement state-of-the-art NLP models, including but not limited to text classification, semantic search, sentiment analysis, named entity recognition, and summary generation.\nconduct data preprocessing, and feature engineering to improve model accuracy and performance.\nStay updated with the latest developments in NLP and ML, and integrate cutting-edge techniques into our solutions.\ncollaborate with Cross-Functional Teams: Work closely with data scientists, software engineers, and product managers to align NLP projects with business objectives.\ndeploy models into production environments and monitor their performance to ensure robustness and reliability.\nmaintain comprehensive documentation of processes, models, and experiments, and report findings to stakeholders.\nimplement and deliver high quality software solutions / components for the Credit Risk monitoring platform.\nleverage his/her expertise to mentor developers; review code and ensure adherence to standards.\napply a broad range of software engineering practices, from analyzing user needs and developing new features to automated testing and deployment\nensure the quality, security, reliability, and compliance of our solutions by applying our digital principles and implementing both functional and non-functional requirements\nbuild observability into our solutions, monitor production health, help to resolve incidents, and remediate the root cause of risks and issues\nunderstand, represent, and advocate for client needs\nshare knowledge and expertise with colleagues , help with hiring, and contribute regularly to our engineering culture and internal communities.\nExpertise -\nBachelor of Engineering or equivalent.\nIdeally 8-10Yrs years of experience in NLP based applications focused on Banking / Finance sector.\nPreference for experience in financial data extraction and classification.\nInterested in learning new technologies and practices, reuse strategic platforms and standards, evaluate options, and make decisions with long-term sustainability in mind.\nProficiency in programming languages such as Python & Java. Experience with frameworks like TensorFlow, PyTorch, or Keras.\nIn-depth knowledge of NLP techniques and tools, including spaCy, NLTK, and Hugging Face.\nExperience with data handling and processing tools like Pandas, NumPy, and SQL.\nPrior experience in agentic AI, LLMs ,prompt engineering and generative AI is a plus.\nBackend development and microservices using Java Spring Boot, J2EE, REST for implementing projects with high SLA of data availability and data quality.\nExperience of building cloud ready and migrating applications using Azure and understanding of the Azure Native Cloud services, software design and enterprise integration patterns.\nKnowledge of SQL and PL/SQL (Oracle) and UNIX, writing queries, packages, working with joins, partitions, looking at execution plans, and tuning queries.\nA real passion for and experience of Agile working practices, with a strong desire to work with baked in quality subject areas such as TDD, BDD, test automation and DevOps principles\nExperience in Azure development including Databricks , Azure Services , ADLS etc.\nExperience using DevOps toolsets like GitLab, Jenkins",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['data engineer', 'Natural Language Processing', 'Python', 'Java']",2025-06-10 14:12:27
Azure Data Engineer,Infosys,5 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","We are looking for Azure Data Engineer's resources having minimum 5 to 9 years of Experience.\n\nTo Apply, use the below link:\nhttps://career.infosys.com/jobdesc?jobReferenceCode=INFSYS-EXTERNAL-210775&rc=0\n\nRole & responsibilities\nBlend of technical expertise with 5 to 9 year of experience, analytical problem-solving, and collaboration with cross-functional teams. Design and implement Azure data engineering solutions (Ingestion & Curation)\nCreate and maintain Azure data solutions including Azure SQL Database, Azure Data Lake, and Azure Blob Storage.\nDesign, implement, and maintain data pipelines for data ingestion, processing, and transformation in Azure.\nUtilizing Azure Data Factory or comparable technologies, create and maintain ETL (Extract, Transform, Load) operations\nUse Azure Data Factory and Databricks to assemble large, complex data sets\nImplementing data validation and cleansing procedures will ensure the quality, integrity, and dependability of the data.\nEnsure data quality / security and compliance.\nOptimize Azure SQL databases for efficient query performance.\nCollaborate with data engineers, and other stakeholders to understand requirements and translate them into scalable and reliable data platform architectures.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Databricks', 'Azure Synapse', 'Pyspark', 'Azure Data Warehouse', 'Azure Data Lake', 'Azure Blob Storage', 'SQL Azure']",2025-06-10 14:12:29
Data Engineer,reycruit,7 - 12 years,35-40 Lacs P.A.,['Hyderabad'],"Looking for 8+ years\nPython+Azure/Aws cloud is mandatory\n1st round- virtual\n2nd round- F2F\nMust have 7+ years of relevant experience- should have hands-on experience with ETL/ELT processes, cloud-based data solutions, and big data technologies.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Pipeline', 'Big Data', 'Elt', 'ETL']",2025-06-10 14:12:32
Data Engineer,TOP OEM,5 - 10 years,Not Disclosed,['Faridabad'],"Role & responsibilities\nAs a Data Engineer dedicated to projects, you will play a crucial role in designing and maintaining robust data architectures. This position requires 100% dedication to projects and Data Architecture Design: Design and implement scalable, reliable, and maintainable data architectures.\nData Integration: Develop ETL processes to integrate data from various sources into a centralized data warehouse. Ensure data quality and integrity throughout the integration process.\nDatabase Management: Administer and optimize databases for high performance and availability. Implement security measures to safeguard data against unauthorized access.\nData Modelling: Create and maintain data models for efficient storage and retrieval of information. Collaborate with data scientists and analysts to translate data needs into effective structures.\nCoding and Scripting: Utilize programming languages (e.g., Python, SQL) for developing and maintaining data pipelines.\nPerformance Monitoring: Monitor data processing systems to identify and resolve performance bottlenecks.\n\nGood to have Skills:\n1. Capability to create data pipelines for KPI Dashboards.\n2. Optimization of databases (Cloud) for efficient resource utilization.\n3. Expertise in various database technologies with the ability to apply technology to use cases.\n4. Experience with database caching, decoupling the database from reports.\n5. Proficient in using microservices for data ingestion.",Industry Type: Automobile,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'Data Engineering', 'Big Data', 'SQL']",2025-06-10 14:12:34
Technical Lead - Sr. Data Engineer,Edgematics Consulting,8 - 13 years,Not Disclosed,['Pune'],"About This Role :\n\nWe are looking for a talented and experienced Data Engineer with Tech Lead with hands-on expertise in any ETL Tool with full knowledge about CI/CD practices with leading a team technically more than 5 and client facing and create Data Engineering, Data Quality frameworks. As a tech lead must ensure to build ETL jobs, Data Quality Jobs, Big Data Jobs performed performance optimization by understanding the requirements, create re-usable assets and able to perform production deployment and preferably worked in DWH appliances Snowflake / redshift / Synapse\n\nResponsibilities\nWork with a team of engineers in designing, developing, and maintaining scalable and efficient data solutions using Any Data Integration (any ETL tool like Talend / Informatica) and any Big Data technologies.\nDesign, develop, and maintain end-to-end data pipelines using Any ETL Data Integration (any ETL tool like Talend / Informatica) to ingest, process, and transform large volumes of data from heterogeneous sources.\nHave good experience in designing cloud pipelines using Azure Data Factory or AWS Glues/Lambda.\nImplemented Data Integration end to end with any ETL technologies.\nImplement database solutions for storing, processing, and querying large volumes of structured and unstructured and semi-structured data\nImplement Job Migrations of ETL Jobs from Older versions to New versions.\nImplement and write advanced SQL scripts in SQL Database at medium to expert level. \nWork with technical team with client and provide guidance during technical challenges.\nIntegrate and optimize data flows between various databases, data warehouses, and Big Data platforms.\nCollaborate with cross-functional teams to gather data requirements and translate them into scalable and efficient data solutions.\nOptimize ETL, Data Load performance, scalability, and cost-effectiveness through optimization techniques.\nInteract with Client on a daily basis and provide technical progress and respond to technical questions.\nImplement best practices for data integration.\nImplement complex ETL data pipelines or similar frameworks to process and analyze massive datasets.\nEnsure data quality, reliability, and security across all stages of the data pipeline.\nTroubleshoot and debug data-related issues in production systems and provide timely resolution.\nStay current with emerging technologies and industry trends in data engineering technologies, CI/CD, and incorporate them into our data architecture and processes.\nOptimize data processing workflows and infrastructure for performance, scalability, and cost-effectiveness.\nProvide technical guidance and foster a culture of continuous learning and improvement.\nImplement and automate CI/CD pipelines for data engineering workflows, including testing, deployment, and monitoring.\nPerform migration to production deployment from lower environments, test & validate\n\nMust Have Skills\nMust be certified in any ETL tools, Database, Cloud.(Snowflake certified is more preferred)\nMust have implemented at least 3 end-to-end projects in Data Engineering.\nMust have worked on performance management optimization and tuning for data loads, data processes, data transformation in big data\nMust be flexible to write code using JAVA/Scala/Python etc. as required\nMust have implemented CI/CD pipelines using tools like Jenkins, GitLab CI, or AWS CodePipeline.\nMust have managed a team technically of min 5 members and guided the team technically.\nMust have the Technical Ownership capability of Data Engineering delivery.\nStrong communication capabilities with client facing.\nBachelor's or Master's degree in Computer Science, Engineering, or a related field.\n5 years of experience in software engineering or a related role, with a strong focus on Any ETL Tool, database, integration.\nProficiency in Any ETL tools like Talend , Informatica etc for Data Integration for building and orchestrating data pipelines.\nHands-on experience with relational databases such as MySQL, PostgreSQL, or Oracle, and NoSQL databases such as MongoDB, Cassandra, or Redis.\nSolid understanding of database design principles, data modeling, and SQL query optimization.\nExperience with data warehousing, Data Lake , Delta Lake concepts and technologies, data modeling, and relational databases.",Industry Type: IT Services & Consulting,"Department: Production, Manufacturing & Engineering","Employment Type: Full Time, Permanent","['Data Engineering', 'Snowflake', 'ETL', 'Azure Aws', 'Data Management', 'Big Data', 'Ci/Cd', 'Data Integration', 'Data Quality', 'Data Pipeline', 'Data Warehousing', 'Data Modeling', 'Data Governance']",2025-06-10 14:12:36
Senior Data Engineer,R Systems International,10 - 14 years,20-30 Lacs P.A.,"['Noida', 'Delhi / NCR']","Solid understanding of data pipeline architecture, cloud infrastructure, and best practices in data engineering.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and collaborate effectively in a team environment.\nSkilled in independently analyzing large datasets, identifying discrepancies and inconsistencies, and recommending corrective actions.\nDemonstrated expertise in working with SQL Server, Oracle, Azure SQL Databases, and APIs.\nExperience with at least one programming language (Python, Java, C#, etc.).",,,,"['Azure Data Factory', 'Data Engineering', 'Azure Databricks', 'SQL', 'Python', 'Pyspark', 'Java', 'Azure Cloud', 'Runbooks', 'Azure Logic Apps', 'ETL', 'Powershell Scripting']",2025-06-10 14:12:38
Data Engineer - Senior,Iris Solutions,4 - 8 years,Not Disclosed,['Noida'],"Design, implement, and maintain data pipelines for processing large datasets, ensuring data availability, quality, and efficiency for machine learning model training and inference.\n\nCollaborate with data scientists to streamline the deployment of machine learning models, ensuring scalability, performance, and reliability in production environments.\n\nDevelop and optimize ETL (Extract, Transform, Load) processes, ensuring data flow from various sources into structured data storage systems.\n\nAutomate ML workflows using ML Ops tools and frameworks (e. g. , Kubeflow, MLflow, TensorFlow Extended (TFX)).\n\nEnsure effective model monitoring, versioning, and logging to track performance and metrics in a production setting.\n\nCollaborate with cross-functional teams to improve data architectures and facilitate the continuous integration and deployment of ML models.\n\nWork on data storage solutions, including databases, data lakes, and cloud-based storage systems (e. g. , AWS, GCP, Azure).\n\nEnsure data security, integrity, and compliance with data governance policies.\n\nPerform troubleshooting and root cause analysis on production-level machine learning systems.\n\nSkills: Glue, Pyspark, AWS Services, Strong in SQL; Nice to have : Redshift, Knowledge of SAS Dataset\n\nMandatory Competencies\nDevOps - CLOUD AWS\nDevOps - Docker\nETL - AWS Glue\nDevOps - Kubernetes\nDatabase - SQL\nBig Data - PySpark\nDatabase - Redshift\nCloud - Azure\nData Science - Azure ML\nData on Cloud - Azure Data Lake (ADL)\nBeh - Communication and collaboration\n\n\nAt Iris Software, we offer world-class benefits designed to support the financial, health and well-being needs of our associates to help achieve harmony between their professional and personal growth. From comprehensive health insurance and competitive salaries to flexible work arrangements and ongoing learning opportunities, were committed to providing a supportive and rewarding work environment.\n\nJoin us and experience the difference of working at a company that values its employees success and happiness.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Health insurance', 'data science', 'SAS', 'data security', 'GCP', 'Machine learning', 'Cloud', 'data governance', 'Troubleshooting', 'Monitoring']",2025-06-10 14:12:40
Data Engineer - Mixed media modelling,Damco Solutions,5 - 10 years,Not Disclosed,['Pune'],"Role: Data Engineer Mixed Media Model (MMM)\nExp: 4 years +\nLocation: Pune/ Remote\n\nJob Summary:\nThe ideal candidate will have strong experience building scalable ETL pipelines and working with both online and offline marketing data to support MMM, attribution, and ROI analysis.\nThe role requires close collaboration with data scientists and marketing teams to deliver clean, structured datasets for modeling.\n\nMandatory Skills:\nStrong proficiency in SQL and Python or Scala\nHands-on experience with cloud platforms (preferably GCP/BigQuery)\nProven experience with ETL tools like Apache Airflow or DBT\nExperience integrating data from multiple sources: digital platforms (Google Ads, Meta), CRM, POS, TV, Radio, etc.\nUnderstanding of Media Mix Modeling (MMM) and attribution methodologies\n\nGood to have skill:\n\nExperience with data visualization tools (Tableau, Looker, Power BI)\nExposure to statistical modeling techniques\n\n\nPlease share your resume at Neesha1@damcogroup.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Mixed media modelling', 'SQL', 'Python', 'Power Bi', 'GCP', 'Bigquery', 'Cloud', 'Apache airflow', 'Data Visualization', 'Tableau', 'ETL']",2025-06-10 14:12:43
Data Engineer-Data Platforms-Azure,IBM,3 - 6 years,Not Disclosed,['Bengaluru'],"Establish and implement  best practices  for  DBT workflows , ensuring efficiency, reliability, and maintainability.\nCollaborate with  data analysts, engineers, and business teams  to align data transformations with business needs.\nMonitor and troubleshoot  data pipelines  to ensure accuracy and performance.\nWork with  Azure-based cloud technologies  to support data storage, transformation, and processing\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nStrong MS SQL, Azure Databricks experience\nImplement and manage data models in DBT, data transformation and alignment with business requirements.\nIngest raw, unstructured data into structured datasets to cloud object store.\nUtilize DBT to convert raw, unstructured data into structured datasets, enabling efficient analysis and reporting.\nWrite and optimize SQL queries within DBT to enhance data transformation processes and improve overall performance\n\n\nPreferred technical and professional experience\nEstablish best DBT processes to improve performance, scalability, and reliability.\nDesign, develop, and maintain scalable data models and transformations using DBT in conjunction with Databricks\nProven interpersonal skills while contributing to team effort by accomplishing related results as required",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure databricks', 'sql queries', 'sql server', 'sql', 'ssis', 'microsoft power bi', 'python', 'azure data lake', 'ssas', 'microsoft azure', 'power bi', 'data warehousing', 'pyspark', 'azure data factory', 't-sql', 'azure logic apps', 'azure functions', 'sql azure', 'spark', 'ssrs', 'azure cosmosdb', 'etl', 'msbi']",2025-06-10 14:12:46
Data Engineer,Diverse Lynx,5 - 10 years,Not Disclosed,['Kolkata'],Diverse Lynx is looking for Data Engineer to join our dynamic team and embark on a rewarding career journey\nLiaising with coworkers and clients to elucidate the requirements for each task.\nConceptualizing and generating infrastructure that allows big data to be accessed and analyzed.\nReformulating existing frameworks to optimize their functioning.\nTesting such structures to ensure that they are fit for use.\nPreparing raw data for manipulation by data scientists.\nDetecting and correcting errors in your work.\nEnsuring that your work remains backed up and readily accessible to relevant coworkers.\nRemaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['hive', 'cloudera', 'python', 'data analysis', 'scala', 'oozie', 'airflow', 'data warehousing', 'pyspark', 'apache pig', 'machine learning', 'data engineering', 'sql', 'mapreduce', 'spark', 'hadoop', 'sqoop', 'big data', 'aws', 'etl', 'hbase']",2025-06-10 14:12:48
Big Data Engineer - Python+ PySpark + Spark,Hexaware Technologies,9 - 12 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","Experience - 9 years - 12 years\nLocation - Mumbai / Chennai / Bangalore / Pune\n\nDevelop and maintain scalable data pipelines using PySpark and Spark SQL for processing large datasets efficiently.\nWrite clean, reusable, and optimized code in Python for data manipulation, analysis, and automation tasks.",,,,"['PySpark', 'Spark', 'Python', 'SQL']",2025-06-10 14:12:50
Senior AWS Data Engineer,Genspark,5 - 10 years,Not Disclosed,"['Chennai', 'Coimbatore', 'Bengaluru']","Job Summary:\nWe are looking for a highly skilled Senior AWS Data Engineer to design, develop, and lead enterprise-grade data solutions on the AWS cloud. This position requires a blend of deep AWS technical proficiency, hands-on PySpark experience, and the ability to engage with business stakeholders in solution design. The ideal candidate will build scalable, secure, and high-performance data platforms using AWS-native tools and best practices.\n\nRole & responsibilities:\nDesign and implement scalable AWS cloud-native data architectures, including data lakes, warehouses, and streaming pipelines\nDevelop ETL/ELT pipelines using AWS Glue (PySpark/Scala), Lambda, and Step Functions\nOptimize Redshift-based data warehouses including schema design, data distribution, and materialized views\nLeverage Athena, Glue Data Catalog, and S3 for efficient serverless query patterns\nImplement IAM-based data access control, lineage tracking, and encryption for secure data workflows\nAutomate infrastructure and data deployments using CDK, Terraform, or CloudFormation\nDrive data modelling standards (Star/Snowflake, 3NF, Data Vault) and ensure data quality and governance\nCollaborate with data scientists, DevOps, and business stakeholders to deliver end-to-end data solutions\nMentor junior engineers and lead code reviews and architecture discussions\nParticipate in client-facing activities including requirements gathering, technical proposal preparation, and solution demos\n\nMust-Have Qualifications:\nAWS Expertise: Proven hands-on experience with AWS Glue, Redshift, Athena, S3, Lake Formation, Kinesis, Lambda, Step Functions, EMR, and Cloud Watch\nPySpark & Big Data: Minimum 2 years of hands-on PySpark/Spark experience for large-scale data processing\nETL/ELT Engineering:Expertise in Python, dbt, or similar automation frameworks\nData Modelling: Proficiency in designing and implementing normalized and dimensional models\nPerformance Optimization:Ability to tune Spark jobs with custom partitioning, broadcast joins, and memory management\nCI/CD & Automation: Experience with GitHub Actions, Code Pipeline, or similar tools\nConsulting & Pre-sales: Prior exposure to client-facing roles including proposal drafting and cost estimation\nGood-to-Have Skills:\nKnowledge of Iceberg, Hudi, or Delta Lake file formats\nExperience with Athena Federated Queries and AWS OpenSearch\nFamiliarity with Data Zone, Data Brew, and data profiling tools\nUnderstanding of compliance frameworks like GDPR, HIPAA, SOC2\nBI integration skills using Power BI, Quick Sight, or Tableau\nKnowledge of event-driven architectures (e.g., Kinesis, MSK, Lambda)\nExposure to lake house or data mesh architectures\nExperience with Lucid chart, Miro, or other documentation/storyboarding tools\n\nWhy Join Us?\nWork on cutting-edge AWS data platforms\nCollaborate with a high-performing team of engineers and architects\nOpportunity to lead key client engagements and shape large-scale solutions\nFlexible work environment and strong learning culture",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Pyspark', 'Big Data', 'AWS Expertise', 'Data Modeling', 'ETL/ELT Engineering', 'Automation', 'CI/CD']",2025-06-10 14:12:52
Sr Data Engineer,Leading Client,1 - 3 years,Not Disclosed,['Chennai'],"Strong experience in Python\nGood experience in Databricks\nExperience working in AWS/Azure Cloud Platform.\nExperience working with REST APIs and services, messaging and event technologies.\nExperience with ETL or building Data Pipeline tools\nExperience with streaming platforms such as Kafka.\nDemonstrated experience working with large and complex data sets.\nAbility to document data pipeline architecture and design\nExperience in Airflow is nice to have\nTo build complex Deltalake",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'hive', 'cloudera', 'scala', 'amazon redshift', 'pyspark', 'data warehousing', 'sql', 'spark', 'data pipeline architecture', 'hadoop', 'big data', 'etl', 'hbase', 'rest', 'python', 'oozie', 'airflow', 'microsoft azure', 'azure cloud', 'nosql', 'data bricks', 'mapreduce', 'kafka', 'sqoop', 'aws']",2025-06-10 14:12:55
Sr Data Engineer,Leading Client,3 - 6 years,Not Disclosed,['Bengaluru'],"Skills:\nMicrosoft Azure, Hadoop, Spark, Databricks, Airflow, Kafka, Py spark RequirmentsExperience working with distributed technology tools for developing Batch and Streaming pipelines using. SQL, Spark, Python Airflow Scala Kafka Experience in Cloud Computing, e.g., AWS, GCP, Azure, etc. Able to quickly pick up new programming languages, technologies, and frameworks. Strong skills building positive relationships across Product and Engineering. Able to influence and communicate effectively, both verbally and written, with team members and business stakeholders Experience with creating/ configuring Jenkins pipeline for smooth CI/CD process for Managed Spark jobs, build Docker images, etc. Working knowledge of Data warehousing, Data modelling, Governance and Data Architecture Experience working with Data platforms, including EMR, Airflow, Data bricks (Data Engineering & Delta Lake components) Experience working in Agile and Scrum development process. Experience in EMR/ EC2, Data bricks etc. Experience working with Data warehousing tools, including SQL database, Presto, and Snowflake Experience architecting data product in Streaming, Server less and Microservices Architecture and platform.",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL', 'scala', 'pyspark', 'data warehousing', 'emr', 'data architecture', 'docker', 'data modeling', 'spark', 'gcp', 'jenkins', 'hadoop', 'cloud computing', 'snowflake', 'python', 'airflow', 'microsoft azure', 'data engineering', 'data bricks', 'amazon ec2', 'kafka', 'scrum', 'agile', 'aws', 'presto', 'sql database']",2025-06-10 14:12:58
Data Engineer 2 at Fintech Platform,Talent 24/7,3 - 5 years,15-22.5 Lacs P.A.,[],"Role & responsibilities\nDesign real-time data pipelines for structured and unstructured sources.\nCollaborate with analysts and data scientists to create impactful data solutions.\nContinuously improve data infrastructure based on team feedback.\nTake full ownership of complex data problems and iterate quickly.\nPromote strong documentation and engineering best practices.\nMonitor, detect, and fix data quality issues with custom tools.\n\nPreferred candidate profile\nExperience with big data tools like Spark, Hadoop, Hive, and Kafka.\nProficient in SQL and working with relational databases.\nHands-on experience with cloud platforms (AWS, GCP, or Azure).\nFamiliar with workflow tools like Airflow.",Industry Type: FinTech / Payments,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hadoop', 'Data Streaming', 'Data Bricks']",2025-06-10 14:13:01
Data Engineer-Data Warehouse,IBM,2 - 5 years,Not Disclosed,['Bengaluru'],"As an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviour’s.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nGood Hands on experience in DBT is required.\nETL Datastage and snowflake - preferred.\nAbility to use programming languages like Java, Python, Scala, etc., to build pipelines to extract and transform data from a repository to a data consumer\nAbility to use Extract, Transform, and Load (ETL) tools and/or data integration, or federation tools to prepare and transform data as needed.\nAbility to use leading edge tools such as Linux, SQL, Python, Spark, Hadoop and Java\n\n\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['scala', 'java', 'spark', 'hadoop', 'python', 'hive', 'data management', 'autosys', 'data warehousing', 'sql', 'plsql', 'unix shell scripting', 'etl tool', 'linux', 'etl datastage', 'shell scripting', 'big data', 'etl', 'teradata sql', 'snowflake', 'oracle', 'datastage', 'warehouse', 'sql server', 'informatica', 'unix']",2025-06-10 14:13:03
"Director, Data Engineering (AI/ML, GenAI, Spark, Java)",Visa,10 - 15 years,Not Disclosed,['Bengaluru'],"Payments Industry is a very exciting and fast-developing area with lot of new and innovative solutions coming to market. With strong demand for new solutions in this space, it promises to be an exciting area of innovation. VISA is a strong leader in the payment industry and is rapidly transitioning into a technology company with significant investments in this area.\nIf you want to be in the exciting payment space, learn fast and make big impacts, Ecosystem & Operational Risk technology which is part of Visa s Value-Added Services business unit is an ideal place for you!\n\nIn Ecosystem & Operational Risk (EOR) technology group, the Payment Fraud Disruption team is responsible for building critical risk and fraud detection and prevention applications and services at Visa. This includes idea generation, architecture, design, development, and testing of products, applications, and services that provide Visa clients with solutions to detect, prevent, and mitigate fraud for Visa and Visa client payment systems.\nWe are in search of inquisitive, creative, and skillful technologists to join our ranks. We are currently looking for a Director of Software Engineering who will take the lead and manage several strategic initiatives within our organization.\nThe right candidate will possess strong software engineering background, with demonstrated leadership experience in driving technical architecture, design and delivery of products and services that have created business value and delivered impact within the payments or payments risk domain or similar industries.\nThis position is ideal for an experienced engineering leader who is passionate about collaborating with business and technology partners and engineers to solve challenging business problems. You will be a key driver in the effort to define the shared strategic vision for the Payment Fraud Disruption platform and defining tools and services that safeguard Visa s payment systems.\nThis is a hybrid position. Expectation of days in office will be confirmed by your Hiring Manager.\n\n\nBasic Qualifications\n10+ years of relevant work experience and a Bachelors degree, OR 13+ years of relevant work experience\n\nPreferred Qualifications\n12 or more years of work experience with a Bachelor s Degree or 8-10 years of experience with an Advanced Degree (e.g. Masters, MBA, JD, MD) or 6+ years of work experience with a PhD\nExperience leading delivery and deployment of ML models, model refresh, and experimentation.\nExperience leading product development & delivery of AI/ML solutions, applied to real-world problems\nExperience leading design and development of mission-critical, secure, reliable systems\nExperience leading delivery across multiple technologies: Python, Java/J2EE, Apache Kafka, Apache Flink, Hive, MySQL, Hadoop, Spark, Scala, design patterns, test automation frameworks\nExperience leading delivery of streaming analytics platforms\nExcellent understanding of algorithms and data structures\nExcellent problem solving and analytics skills. Capable of forming and advocating an independent viewpoint\nStrong experience with agile methodologies\nExcel in partnering with Product leaders and technical product managers on requirements workshops, helping define joint product/technology roadmaps & driving prioritization\nExperience driving continuous improvements to processes/tools for better developer efficiency and productivity\nDemonstrated ability to drive measurable improvements across engineering, delivery and performance metrics\nDemonstrated success in leading high performing, multi-disciplinary and geographically distributed engineering teams. Demonstrated ability to hire, develop and retain high-caliber talent\nMust demonstrate longer-term strategic thinking and staying abreast with latest technologies to assess what s possible technically\nStrong collaboration and effective communication, with focus on win-win outcomes",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Payment systems', 'Operational risk', 'MySQL', 'SCALA', 'Agile', 'Manager Technology', 'Data structures', 'Apache', 'Analytics', 'Python']",2025-06-10 14:13:06
Data Engineer-Business Intelligence,IBM,3 - 8 years,Not Disclosed,['Pune'],"Provide expertise in analysis, requirements gathering, design, coordination, customization, testing and support of reports, in client’s environment\nDevelop and maintain a strong working relationship with business and technical members of the team\nRelentless focus on quality and continuous improvement\nPerform root cause analysis of reports issues\nDevelopment / evolutionary maintenance of the environment, performance, capability and availability.\nAssisting in defining technical requirements and developing solutions\nEffective content and source-code management, troubleshooting and debugging.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nCognos Developer & Admin Required. EducationThe resource should be full time MCA/M. Tech/B. Tech/B.E. and should preferably have relevant certifications\nExperienceThe resource should have a minimum of 3 years of experience of working in the in BI DW projects in areas pertaining to reporting and visualization using cognos.\nThe resources shall have worked in at least two projects where they were involved in developing reporting/ visualization\nHe shall have good understanding of UNIX.\nShould be well conversant in English and should have excellent writing, MIS, communication, time management and multi-tasking skill\n\n\nPreferred technical and professional experience\nExperience with various cloud and integration platforms (e.g. AWS, Google, Azure)\nAgile mindset – ability to process changes of priorities and requests, ownership, critical thinking\nExperience with an ETL/Data Integration tool (eg. IBM InfoSphere DataStage, Azure Data Factory, Informatica PowerCenter)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['informatica powercenter', 'microsoft azure', 'mis', 'bi dw', 'etl', 'datastage', 'bi', 'azure data factory', 'cognos', 'root cause analysis', 'business intelligence', 'sql', 'dw', 'ibm datastage', 'troubleshooting', 'debugging', 'agile', 'data visualization', 'aws', 'data integration', 'informatica', 'unix']",2025-06-10 14:13:09
Data Engineer,Meritus Management Service,5 - 10 years,10-20 Lacs P.A.,"['Nagpur', 'Pune']","We are looking for a skilled Data Engineer to design, build, and manage scalable data pipelines and ensure high-quality, secure, and reliable data infrastructure across our cloud and on-prem platforms.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Synapse Analytics', 'SQL', 'Azure Data Factory', 'Python', 'API Integration', 'Postgresql', 'Data Bricks', 'Scripting', 'SCALA', 'Data Lake', 'MongoDB', 'Data Warehousing', 'Data Modeling', 'ETL', 'Azure Devops']",2025-06-10 14:13:12
Data Engineer-Data Platforms-Google,IBM,5 - 7 years,Not Disclosed,['Hyderabad'],"Skilled Multiple GCP services - GCS, BigQuery, Cloud SQL, Dataflow, Pub/Sub, Cloud Run, Workflow, Composer, Error reporting, Log explorer etc.\nMust have Python and SQL work experience & Proactive, collaborative and ability to respond to critical situation\nAbility to analyse data for functional business requirements & front face customer\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n5 to 7 years of relevant experience working as technical analyst with Big Query on GCP platform.\nSkilled in multiple GCP services - GCS, Cloud SQL, Dataflow, Pub/Sub, Cloud Run, Workflow, Composer, Error reporting, Log explorer\nYou love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies\nAmbitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work\n\n\nPreferred technical and professional experience\nCreate up to 3 bullets maxitive individual with an ability to manage change and proven time management\nProven interpersonal skills while contributing to team effort by accomplishing related results as needed\nUp-to-date technical knowledge by attending educational workshops, reviewing publications (encouraging then to focus on required skills)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'gcp', 'bigquery', 'cloud sql', 'python', 'hive', 'gen', 'java', 'postgresql', 'spark', 'linux', 'mysql', 'hadoop', 'big data', 'pubsub', 'airflow', 'application engine', 'machine learning', 'sql server', 'dataproc', 'cloud storage', 'bigtable', 'agile', 'sqoop', 'aws', 'data flow']",2025-06-10 14:13:14
"Senior Data Engineer (Exp into Azure Databricks,Pyspark, SQL)",Adecco India,7 - 12 years,22.5-30 Lacs P.A.,"['Pune', 'Bengaluru']","Job Role & responsibilities:-\n\nUnderstanding operational needs by collaborating with specialized teams\nSupporting key business operations. This involves architecture designing, building and deploying data systems, pipelines etc\nDesigning and implementing agile, scalable, and cost efficiency solution on cloud data services.\nLead a team of developers, implement Sprint planning and executions to ensure timely deliveries\n\nTechnical Skill, Qualification & experience required:-\n\n7-10 years of experience in Azure Cloud Data Engineering, Azure Databricks, datafactory , Pyspark, SQL,Python\nHands on experience in Data Engineer, Azure Databricks, Data factory, Pyspark, SQL\nProficient in Cloud Services Azure\nArchitect and implement ETL and data movement solutions.\nMigrate data from traditional database systems to Cloud environment\nStrong hands-on experience for working with Streaming dataset\nBuilding Complex Notebook in Databricks to achieve business Transformations.\nHands-on Expertise in Data Refinement using Pyspark and Spark SQL\nFamiliarity with building dataset using Scala.\nFamiliarity with tools such as Jira and GitHub\nExperience leading agile scrum, sprint planning and review sessions\nGood communication and interpersonal skills\nComfortable working in a multidisciplinary team within a fast-paced environment\n\n* Immediate Joiners will be preferred only",Industry Type: Insurance,Department: Other,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineer', 'Azure data engineer', 'Data Bricks', 'SQL', 'Data Engineering', 'Python']",2025-06-10 14:13:16
Azure Data Engineer (Standard),Infogain,5 - 6 years,Not Disclosed,['Bengaluru'],"Primary Skills: ADF, Databricks, Log Analytics\nSecondary Skills: Data Warehouse, Logic Apps, Log Analytics, Datadog, Atlan, Attacama\nEXPERIENCE\n4.5-6 Years\nSKILLS\nPrimary Skill: Data Engineering\nSub Skill(s): Data Engineering\nAdditional Skill(s): synapse, databricks, Azure Datalake, Azure Data Factory",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SUB', 'Data warehousing', 'Analytics']",2025-06-10 14:13:18
Data Engineer-Data Platforms-Google,IBM,6 - 11 years,Not Disclosed,['Bengaluru'],"6+ years of industry work experience\nExperience extracting data from a variety of sources, and a desire to expand those skills\nWorked on Google Looker tool\nWorked on Big Query and GCP technologies\nStrong SQL and Spark knowledge\nExcellent Data Analysis skills. Must be comfortable with querying and analyzing large amount of data on Hadoop HDFS using Hive and Spark\nKnowledge of Financial Accounting is a bonus\nWork independently with cross functional team and drive towards the resolution\nExperience with Object oriented programming using python and its design patterns\nExperience handling Unix systems, for optimal usage to host enterprise web applications GCP certifications preferred.\nPayments Industry Background good to have\nCandidate who has been part to google Cloud Migration is an ideal Fit\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n3-5 years of experience\nIntuitive individual with an ability to manage change and proven time management\nProven interpersonal skills while contributing to team effort by accomplishing related results as needed\nUp-to-date technical knowledge by attending educational workshops, reviewing publications\n\n\nPreferred technical and professional experience\n6+ years of industry work experience\nExperience extracting data from a variety of sources, and a desire to expand those skills\nWorked on Google Looker tool",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data analysis', 'sql', 'spark', 'hadoop', 'hive', 'snowflake', 'data warehousing', 'power bi', 'google', 'business intelligence', 'sql server', 'qlikview', 'tableau', 'data modeling', 'gcp', 'financial accounting', 'bigquery', 'object oriented programming', 'data visualization', 'etl', 'ssis', 'unix']",2025-06-10 14:13:20
Lead AWS Glue Data Engineer,DXC Technology,4 - 9 years,Not Disclosed,['Chennai'],"We are seeking a skilled Lead AWS Data Engineer with strong programming and SQL skills to join our team. The ideal candidate will have hands-on experience with AWS Data Analytics services and a basic understanding of general AWS services. Additionally, prior experience with Oracle and Postgres databases and secondary skills in Python and Azure DevOps will be an advantage.\n  Key Responsibilities:\nDesign, develop, and optimize data pipelines using AWS Data Analytics services such as RDS, DMS, Glue, Lambda, Redshift, and Athena .\nImplement data migration and transformation processes using AWS DMS and Glue .\nWork with SQL (Oracle & Postgres) to query, manipulate, and analyse large datasets.\nDevelop and maintain ETL/ELT workflows for data ingestion and transformation.\nUtilize AWS services like S3, IAM, CloudWatch, and VPC to ensure secure and efficient data operations.\nWrite clean and efficient Python scripts for automation and data processing.\nCollaborate with DevOps teams using Azure DevOps for CI/CD pipelines and infrastructure management.\nMonitor and troubleshoot data workflows to ensure high availability and performance.\nPreferred Qualifications:\nAWS certifications in Data Analytics, Solutions Architect, or DevOps.\nExperience with data warehousing concepts and data lake implementations.\nHands-on experience with Infrastructure as Code (IaC) tools like Terraform or CloudFormation.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'Data migration', 'Infrastructure management', 'Social media', 'devops', 'Data processing', 'Data analytics', 'Dms', 'AWS', 'Python']",2025-06-10 14:13:22
Senior Data Engineer,Expian Technologies,5 - 10 years,Not Disclosed,['Bengaluru( MG Road )'],"Role & responsibilities\nCollaborate with cross-functional teams to understand data requirements and design scalable and efficient data processing solutions.\nDevelop and maintain data pipelines using PySpark and SQL on the Databricks platform.\nOptimize and tune data processing jobs for performance and reliability.\nImplement automated testing and monitoring processes to ensure data quality and reliability.\nWork closely with data scientists, data analysts, and other stakeholders to understand their data needs and provide effective solutions.\nTroubleshoot and resolve data-related issues, including performance bottlenecks and data quality problems.\nStay up to date with industry trends and best practices in data engineering and Databricks.\n\nPreferred candidate profile\n5+ years of experience as a Data Engineer, with a focus on Databricks and cloud-based data platforms with a minimum of 2 years of experience in writing unit/end-to-end tests for data pipelines and ETL processes on Databricks.\nHands-on experience in PySpark programming for data manipulation, transformation, and analysis.\nStrong experience in SQL and writing complex queries for data retrieval and manipulation.\nExperience in developing and implementing test cases for data processing pipelines using a test-driven development approach.\nExperience in Docker for containerising and deploying data engineering applications is good to have.\nExperience in the scripting language Python is mandatory.\nStrong knowledge of Databricks platform and its components, including Databricks notebooks, clusters, and jobs.\nExperience in designing and implementing data models to support analytical and reporting needs will be an added advantage.\nStrong Knowledge of Azure Data Factory for Data orchestration, ETL workflows, and data integration is good to have.\nGood to have knowledge of cloud-based storage such as Amazon S3 and Azure Blob Storage.\nBachelor's or Master's degree in Computer Science, Engineering, or a related field.\nStrong analytical and problem-solving skills.\nStrong English communication skills, both written and spoken, are crucial.\nCapability to solve complex technical issues and comprehend risks prior to the circumstance.",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pytest', 'databricks', 'pyspark', 'Data Modeling', 'SQL']",2025-06-10 14:13:24
Data Engineer - SAS Migration,Crisil,2 - 4 years,Not Disclosed,['Mumbai'],"The SAS to Databricks Migration Developer will be responsible for migrating existing SAS code, data processes, and workflows to the Databricks platform. This role requires expertise in both SAS and Databricks, with a focus on converting SAS logic into scalable PySpark and Python code. The developer will design, implement, and optimize data pipelines, ensuring seamless integration and functionality within the Databricks environment. Collaboration with various teams is essential to understand data requirements and deliver solutions that meet business needs",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'scala', 'pyspark', 'data warehousing', 'data migration', 'sql', 'spark', 'gcp', 'mysql', 'hadoop', 'bigquery', 'big data', 'etl', 'python', 'sas', 'teradata', 'airflow', 'microsoft azure', 'data engineering', 'sql server', 'dataproc', 'data bricks', 'cloud data flow', 'kafka', 'migration', 'sqoop', 'data flow']",2025-06-10 14:13:27
Data Engineer,Bay Area Tek Solutions LLC,5 - 8 years,Not Disclosed,['Bengaluru'],"Strong on programming languages like Python, Java Must have one cloud hands-on experience (GCP preferred) Must have: Experience working with Dockers Must have: Environments managing (e.g venv, pip, poetry, etc.) Must have: Experience with orchestrators like Vertex AI pipelines, Airflow, etc Must have: Data engineering, Feature Engineering techniques Proficient in either Apache Spark or Apache Beam or Apache Flink Must have: Advance SQL knowledge Must be aware of Streaming concepts like Windowing , Late arrival , Triggers etc",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Beam', 'GCP', 'spark', 'Cloud', 'Programming', 'Management', 'SQL', 'Python']",2025-06-10 14:13:29
Data Engineer,DATA ENGINEER,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Title: Data Engineer\nExperience: 5+ Years\nLocation: Hyderabad (Onsite)\nCompany: Cognizant\nAvailability: Immediate Joiners Preferred\nJob Description:\nWe are seeking an experienced Data Engineer with a strong background in Java, Spark, and Scala to join our dynamic team in Hyderabad. The ideal candidate will be responsible for building scalable data pipelines, optimizing data processing workflows, and supporting data-driven solutions for enterprise-grade applications. This is a full-time onsite role.\nKey Responsibilities:\nDesign, develop, and maintain robust and scalable data processing pipelines.\nWork with large-scale data using distributed computing technologies like Apache Spark.\nDevelop applications and data integration workflows using Java and Scala.\nCollaborate with cross-functional teams including Data Scientists, Analysts, and Product Managers.\nEnsure data quality, integrity, and security in all data engineering solutions.\nMonitor and troubleshoot performance and data issues in production systems.\nMust-Have Skills:\nStrong hands-on experience with Java, Apache Spark, and Scala.\nProven experience working on large-scale data processing systems.\nSolid understanding of distributed systems and performance tuning.\nGood-to-Have Skills:\nExperience with Hadoop, Hive, and HDFS.\nFamiliarity with data warehousing concepts and ETL processes.\nExposure to cloud data platforms is a plus.\nDesired Candidate Profile:\n5+ years of relevant experience in data engineering or big data technologies.\nStrong problem-solving and analytical skills.\nExcellent communication and collaboration skills.\nAbility to work independently in a fast-paced environment.\nAdditional Details:\nWork Mode: Onsite (Hyderabad)\nEmployment Type: Full-time\nNotice Period: Immediate joiners highly preferred, candidates serving notice period.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'SCALA', 'Spark', 'Hive', 'Hadoop', 'Kafka']",2025-06-10 14:13:31
Data Engineer-Data Platforms-AWS,IBM,4 - 9 years,Not Disclosed,['Kochi'],"As Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in the development of data solutions using Spark Framework with Python or Scala on Hadoop and AWS Cloud Data Platform\n\nResponsibilities:\nExperienced in building data pipelines to Ingest, process, and transform data from files, streams and databases. Process the data with Spark, Python, PySpark, Scala, and Hive, Hbase or other NoSQL databases on Cloud Data Platforms (AWS) or HDFS\nExperienced in develop efficient software code for multiple use cases leveraging Spark Framework / using Python or Scala and Big Data technologies for various use cases built on the platform\nExperience in developing streaming pipelines\nExperience to work with Hadoop / AWS eco system components to implement scalable solutions to meet the ever-increasing data volumes, using big data/cloud technologies Apache Spark, Kafka, any Cloud computing etc\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nMinimum 4+ years of experience in Big Data technologies with extensive data engineering experience in Spark / Python or Scala.\nMinimum 3 years of experience on Cloud Data Platforms on AWS;\nExperience in AWS EMR / AWS Glue / DataBricks, AWS RedShift, DynamoDB\nGood to excellent SQL skills\nExposure to streaming solutions and message brokers like Kafka technologies.\n\n\nPreferred technical and professional experience\nCertification in AWS and Data Bricks or Cloudera Spark Certified developers.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['scala', 'sql', 'python', 'data engineering', 'aws', 'hive', 'cloudera', 'glue', 'amazon redshift', 'big data technologies', 'pyspark', 'emr', 'cloud technologies', 'apache', 'spark', 'aws cloud', 'aws emr', 'hadoop', 'big data', 'cloud computing', 'hbase', 'dynamo db', 'nosql', 'aws glue', 'data bricks', 'kafka']",2025-06-10 14:13:33
"Big Data Engineer (Spark, Scala) , SQL",Black and white Business Solution,7 - 10 years,Not Disclosed,"['Bhubaneswar', 'Pune', 'Bengaluru']","About Client\n\nHiring for One of the Most Prestigious Multinational Corporations\n\nJob Title: Big Data Engineer (Spark, Scala) , SQL\n\nExperience: 6 to 10 years\n\nKey Responsibilities:\nDesign, develop, and optimize scalable big data pipelines using Apache Spark and Scala.\nBuild batch and real-time data processing workflows to ingest, transform, and aggregate large datasets.\nWrite high-performance SQL queries to support data analysis and reporting.\nCollaborate with data architects, data scientists, and business stakeholders to understand requirements and deliver high-quality data solutions.\nEnsure data quality, integrity, and governance across systems.\nParticipate in code reviews and maintain best practices in data engineering.\nTroubleshoot and optimize performance of Spark jobs and SQL queries.\nMonitor and maintain production data pipelines and perform root cause analysis of data issues.\n\nTechnical Skills:\n\n6 to10 years of overall experience in software/data engineering.\n4+ years of hands-on experience with Apache Spark using Scala.\nStrong proficiency in Scala and functional programming concepts.\nExtensive experience with SQL (preferably in distributed databases like Hive, Presto, Snowflake, or BigQuery).\nExperience working in Hadoop ecosystem (HDFS, Hive, HBase, Oozie, etc.).\nKnowledge of data modeling, data architecture, and ETL frameworks.\nFamiliarity with version control (Git), CI/CD pipelines, and DevOps practices.\nExperience with cloud platforms (AWS, Azure, or GCP) is a plus.\nStrong analytical and problem-solving skills.\nExcellent communication and collaboration abilities.\n\nNotice period : Till 60 days\n\nLocation: BLR//BBSR/PUNE\n\nMode of Work :WFO(Work From Office)\n\n\nThanks & Regards,\nSWETHA\nBlack and White Business Solutions Pvt.Ltd.\nBangalore,Karnataka,INDIA.\nContact Number:8067432433\nrathy@blackwhite.in |www.blackwhite.in",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['spark', 'scala', 'Big data', 'SQL']",2025-06-10 14:13:35
Sr Analyst II Data Engineering,DXC Technology,9 - 12 years,Not Disclosed,['Mumbai'],"Job Description:\nFor Linux L2 resource, please find below JD:\nLinux Technical Experience: 6+ yrs\nTotal IT Experience: 8+ yrs\nAcademic: Engineering or IT Academic.\n\n1. Carry out the advance level of troubleshooting on Linux OS and application\n2. Work in a 24 X 7 Environment.\n3. Extend technical support the Clients over email and phone.\n4. Good communication written and verbal.\n5. Responsible for the uptime of the various client projects being managed / monitored.\n6. Ensure that tickets raised per shift are closed as per SLA if not escalate per escalation Matrix.\n7. Raise technically update the tickets in ticketing system for all the issues handled/worked upon.\n8. Shift report generation on daily issues handled / escalated in the form of Daily reports.\n9. Work on the change request.\n10. Apply patches to the OS or upgrade the OS as and when required, as per the approval from the respective Team Leaders.\n11. Monitoring management of agents like Deep security, SIEM BMC TSA/TSOM, Commvault.\n12. Ensure earlier resolution to the issues to meet SLA.\n13. Following up with Vendors or other teams - hardware software whenever required resolve issues by minimizing downtimes.\n14. Commissioning and Decommissioning of servers.\n15. Installation of infra applications as applicable.\n16. Working on project work as applicable.\n17. Driving initiatives as applicable.\n18. Proactively maintain develop all Linux infrastructure to maintain a 24x7x365 uptime service.\n19. Maintain best practices on managing systems and services across all environments.\n20. Fault finding, analysis and of logging information for reporting of performance exceptions.\n21. Proactively monitoring system performance and capacity planning.\n22. Manage, coordinate, and implement software upgrades, patches on servers.\n23. Create and modify scripts to perform daily operational tasks.\n24. Provide input on ways to improve the stability, security, efficiency, and scalability of environment.\n25. Collaborate with other teams members to develop automation strategy deployment processes.\n26. Linux server administration in virtualized environment.\n27. Knowledge of Linux: RHEL 8 9.\n28. Installing maintaining Linux native services as well as Bind, Apache, postfix, MTA, squid, nginx.\n29. Familiarity with load balancing, firewalls, etc.\n30. Knowledge of protocols such as DNS, HTTP, LDAP, SMTP and SNMP\n31. Proficient with network tools such as iptables, tcpdump, HA-Proxy, etc.\n32. Experience with virtualization technologies, such as VMware, KVM, etc.\n33. Experience with Nagios or any other monitoring tools.\n34. Additional Linux certifications (RHCE and RHCA) will be considered an advantage.\n35. Experience in RHEL patching activity to close VA observations.\n36. LVM (logical volume manager) administration User administration.\n37. SCD, VAPT OS Hardening\n38. Minor Major version upgrade\nRecruitment fraud is a scheme in which fictitious job opportunities are offered to job seekers typically through online services, such as false websites, or through unsolicited emails claiming to be from the company. These emails may request recipients to provide personal information or to make payments as part of their illegitimate recruiting process. DXC does not make offers of employment via social media networks and DXC never asks for any money or payments from applicants at any point in the recruitment process, nor ask a job seeker to purchase IT or other equipment on our behalf. More information on employment scams is available here .",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['VMware', 'Ticketing', 'Automation', 'Linux', 'Social media', 'DNS', 'Troubleshooting', 'Operations', 'Technical support', 'Capacity planning']",2025-06-10 14:13:37
Data Engineer,wits inovation lab,15 - 20 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Work closely with the Technology Product Owner and Engineering Leads/SMEs to understand the business requirements and develop technology solutions.\nActively engage in the whole delivery lifecycle from inception, design, development, testing, deployment, operations, monitoring and continuous improvement of systems and services.\nAccountable for the technical excellence of the squad, including technical debt management, technical risk management and ensuring alignment to standards and frameworks. Recognised\nas a technical authority and escalation point for the team, guiding them on the HOW .\nBring a proven record in managing and mentoring teams, with a commitment to building team capabilities and fostering a positive, inclusive culture.\nStay abreast of the latest market developments in technologies and data engineering practices and recommend potential innovative technologies and tools to enhance the capabilities of the engineering team.\nContribute to engineering communities and provide ongoing support of platforms as required.\n\nWhat will you bring\nTo grow and be successful in this role, you will ideally bring the following:\nAt least 5 years of relevant work experience in application delivery with hands on experience with SQL Server Database and Microsoft MSBI Technology (SQL Server Integration Services and SQL Server Reporting Services)\nProven ability to develop and maintain complex SQL queries for data analysis and reporting.\nExtensive hands-on experience in data analysis, profiling and technical solution delivery in line with data warehousing principles in SQL Server.\nGood understanding of networking concepts (protocols, security, virtual network, monitoring and troubleshooting etc.)\nGood to have knowledge around scalable web tier architecture such as J2EE framework, common webserver configuration (JBOSS/IIS), security best practices\nGood experience with DevOps practices, including CI/CD pipelines and observability tools\nGood understanding of SDLC and Agile methodologies. Solid understanding of software engineering principles, patterns, and practices.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'data analysis', 'SQL Server Reporting Services', 'Agile methodologies', 'JBOSS', 'SQL Server Database', 'SDLC', 'software engineering principles', 'IIS', 'DevOps', 'J2EE framework', 'CI/CD', 'SQL Server Integration Services']",2025-06-10 14:13:40
Data Engineer Intern,Infoweave,6 months duration,"20,000/month",['Bengaluru'],"DataWeave is a cutting-edge AI-powered digital commerce analytics platform that empowers retailers with competitive intelligence and equips consumer brands with digital shelf analytics on a global scale. By harnessing the power of DataWeave, retailers gain the ability to make smarter pricing and merchandising decisions, while consumer brands can optimize their digital shelf performance across key performance indicators such as share of search, content quality, price competitiveness, and stock availability.\n\nAt the heart of DataWeave's capabilities lies its state-of-the-art AI-powered proprietary technology, which aggregates and analyzes 500+ billion data points, covering over 400,000 brands, 4,000+ websites, and spanning more than 20 industry verticals.\nWe are a globally distributed team, composed of over 200 talented engineers, product managers, and eCommerce experts located across San Francisco, Seattle, Austin, and Toronto in North America, complemented by our technology-focused offices in Bangalore and Coimbatore.\n\nData Engineering and Delivery @DataWeave\nWe the Delivery / Data engineering team at DataWeave, deliver the Intelligence with actionable data to the customer. One part of the work is to write effective crawler bots to collect data over the web, which calls for reverse engineering and writing scalable python code. Other part of the job is to crunch data with our big data stack / pipeline. Underpinnings are Tooling, domain\nawareness, fast paced delivery, and pushing the envelope.\n\nHow we work?\nIt's hard to tell what we love more, problems or solutions! Every day, we choose to address some of the hardest data problems that there are. We are in the business of making sense of messy public data on the web. At serious scale! Read more on Become a DataWeaver\n\nWhat do we offer?\nSome of the most challenging data problems. Huge text and image datasets that you can play with!\nAbility to see the impact of your work and the value you're adding to our customers almost immediately.\nOpportunity to work on different problems and explore a wide variety of tools to figure out what really excites you.\nA culture of openness. Fun work environment. A flat hierarchy. Organization wide visibility. Flexible working hours.\nLearning opportunities with courses and tech conferences. Mentorship from seniors in the team.\nLast but not the least, competitive salary packages and fast paced growth opportunities.\n\nRelevant set of skills\nCandidate must have good written and oral communication skills, be a fast learner and have the ability to adapt quickly to a fast-paced development environment\nHas a strong grasp of CS fundamentals\nHands on with Python programming\nGood understanding of RDBMS / SQL\nExcellent problem-solving and analytical abilities\n\nOptional Skills\nKnowledge of building crawlers and data mining is a plus.\nWorking knowledge of open-source tools such as MySQL, Solr, Elasticsearch, Cassandra (data stores) would be a plus.\n\nRole and responsibilities\nInclined towards working in a start-up environment.\nDesign and build robust and scalable data engineering solutions for structured and unstructured data for delivering business insights, reporting and analytics.\nHelp continually improve ongoing analysis processes, optimizing or simplifying self-service support for customers\nImplement various bots Configure and write Regex to extract data from simple to complex websites.\nDevelop tools & techniques related to the various processes or data extraction automation.\nWork on data lake platform and different components in the data lake such as Hadoop, Amazon S3 etc.\nWork on SQL technologies on Hadoop such as Spark, Hive, Impala etc.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['RDBMS', 'Cs Fundamentals', 'Problem Solving', 'Python', 'SQL', 'Analytical Ability']",2025-06-10 14:13:43
Software Engineer II - Data Engineer,JPMorgan Chase Bank,0 - 4 years,Not Disclosed,['Hyderabad'],"You thrive on diversity and creativity, and we welcome individuals who share our vision of making a lasting impact. Your unique combination of design thinking and experience will help us achieve new heights.\nAs a Data Engineer II at JPMorgan Chase within the Consumer Community Banking Team, you are part of an agile team that works to enhance, design, and deliver the data collection, storage, access, and analytics solutions in a secure, stable, and scalable way. As an emerging member of a data engineering team, you execute data solutions through the design, development, and technical troubleshooting of multiple components within a technical product, application, or system, while gaining the skills and experience needed to grow within your role.\nJob responsibilities\nExecutes software solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problems\nSupports review of controls to ensure sufficient protection of enterprise data\nAdvises and makes custom configuration changes in one to two tools to generate a product at the business or customer request\nUpdates logical or physical data models based on new use cases\nFrequently uses SQL and understands NoSQL databases and their niche in the marketplace\nAdds to team culture of diversity, equity, inclusion, and respect\nContributes to software and data engineering communities of practice and events that explore new and emerging technologies\nGathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of software applications and systems\nRequired qualifications, capabilities, and skills\nFormal training or certification on Data Engineering concepts and 3+ years applied experience in AWS and Kubernetes\nProficiency in one or more large-scale data processing distributions such as JavaSpark/PySpark along with knowledge on Data Pipeline (DPL), Data Modeling, Data warehouse, Data Migration and so-on.\nExperience across the data lifecycle along with expertise with consuming data in any of batch (file), near real-time (IBM MQ, Apache Kafka), streaming (AWS kinesis, MSK)\nGood at SQL (e. g. , joins and aggregations)\nWorking understanding of NoSQL databases\nExperience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages.\nSolid understanding of agile methodologies such as CI/CD, Application Resiliency, and Security\nSignificant experience with statistical data analysis and ability to determine appropriate tools and data patterns to perform analysis\nExperience customizing changes in a tool to generate product\nPreferred qualifications, capabilities, and skills\nFamiliarity with modern front-end technologies\nExperience designing and building REST API services using Java\nExposure to cloud technologies - knowledge on Hybrid cloud architectures is highly desirable.\nAWS Developer/Solutions Architect Certification is highly desired",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'Data migration', 'Front end', 'Data modeling', 'Debugging', 'Agile', 'Apache', 'Troubleshooting', 'Analytics', 'SQL']",2025-06-10 14:13:46
Azure Data Engineer,PwC India,7 years,Not Disclosed,['Gurugram'],"Position: Azure Data Engineer\nExperience: 4-7 Years\nLocation: Gurugram\nType: Full Time\nNotice period : Immediate to 30 days\n\nPreferred Certifications: Azure Data Engineer Associate, Databricks",,,,"['Azure Data Factory', 'Synapse Analytics', 'Data Bricks']",2025-06-10 14:13:48
Data Engineer-Data Platforms-Google,IBM,2 - 5 years,Not Disclosed,['Pune'],"As an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\n\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\n\n\nIn this role, your responsibilities may include:\n\n\n\n\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nStrong MS SQL, Azure Databricks experience\nImplement and manage data models in DBT, data transformation and alignment with business requirements.\nIngest raw, unstructured data into structured datasets to cloud object store.\nUtilize DBT to convert raw, unstructured data into structured datasets, enabling efficient analysis and reporting.\nWrite and optimize SQL queries within DBT to enhance data transformation processes and improve overall performance\n\n\nPreferred technical and professional experience\nEstablish best DBT processes to improve performance, scalability, and reliability.\nDesign, develop, and maintain scalable data models and transformations using DBT in conjunction with Databricks\nProven interpersonal skills while contributing to team effort by accomplishing related results as required",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure databricks', 'data management', 'sql server', 'sql', 'data transformation', 'python', 'data analysis', 'data analytics', 'data warehousing', 'power bi', 'azure data factory', 'machine learning', 'business intelligence', 'tableau', 'data extraction', 'data modeling', 'data visualization', 'etl']",2025-06-10 14:13:50
Data Engineer-Data Warehouse,IBM,2 - 5 years,Not Disclosed,['Pune'],"As an Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nDesign and implement efficient database schemas and data models using Teradata.\nOptimize SQL queries and stored procedures for performance.\nPerform database administration tasks including installation, configuration, and maintenance of Teradata systems\n\n\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'elastic search', 'data modeling', 'sql queries', 'splunk', 'data management', 'manual testing', 'data warehousing', 'plsql', 'java', 'ssrs', 'mysql', 'hadoop', 'big data', 'etl', 'jira', 'python', 'sql development', 'oracle', 'data analysis', 'warehouse', 'sql server', 'tableau', 'ssis', 'aws', 'informatica', 'unix']",2025-06-10 14:13:53
Data Engineer (Python and SQL),Infraveo Technologies,2 - 5 years,Not Disclosed,[],"AI patterns recognition: you'll be developing multiple AI features, from user categorization to autofilled descriptions, market and conversations sentiment analysis and AI insights to help crypto marketers.\nSDK, User Graph improvement and reliability: we're constantly improving our proprietary user graph by matching wallets to social profiles. Your mission will be supporting new crypto wallets and networks, automating a system to match more users to their identities, and applying data checks to ensure reliability.\nIntegrations: As a customer data platform, we're expanding the number of data sources our customers can import from Web2 and Web3. you'll manage multiple API endpoints and integrate new third-party tools like Mixpanel, Amplitude, Segment, Dune Analytics, and DeFi Llama. This involves not only integration work but also data modeling and architectural design.\nSocial Data analysis: you'll work with social data APIs like Twitter to analyze Key Opinion Leaders performance and trends.\nRequirements\nProven track record as a Data Engineer delivering complex data solutions.\nAdvanced SQL skills and expertise with complex queries.\nMastery in Python development and strong experience with PySpark.\nExtensive proficiency managing cloud services, including AWS Redshift, RDS Postgres, S3, Lambda, Kinesis, SQS, ECS, EC2.\nStrong competency implementing and supporting various data models, such as highly normalized, star schema and Data Vault.\nPractical experience with orchestration tools like Airflow, Dagster or Prefect.\nDemonstrated proficiency consuming and automating interactions with APIs.\nHands-on experience creating data pipelines using dbt for various platforms (ideally AWS Redshift and Postgres).\nExperience in SaaS analytics, marketing, or crypto companies is a plus.\nBenefits\nWork Location: Remote\n5 days working",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Data modeling', 'Cloud Services', 'Architectural design', 'SDK', 'Management', 'AWS', 'Analytics', 'SQL', 'Python']",2025-06-10 14:13:56
AWS Data Engineer (Associate),Infogain,2 - 3 years,Not Disclosed,['Bengaluru'],"C#, AWS, SQL skill set required with 2-3 years experience and immediate joiner\nC#, AWS, SQL skill set required with 2-3 years experience and immediate joiner\nEXPERIENCE\n2-3 Years\nSKILLS\nPrimary Skill: Data Engineering\nSub Skill(s): Data Engineering\nAdditional Skill(s): C#, Python, AWS - CloudFormation, Apache Hive, SQL",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['hive', 'SUB', 'C', 'Apache', 'AWS', 'SQL', 'Python']",2025-06-10 14:13:59
"Data Engineer II, Payroll Tech",Amazon,3 - 8 years,Not Disclosed,['Hyderabad'],"Payroll Technology at Amazon is all about enabling our business to perform at scale as efficiently as possible with no defects. As Amazons workforce grows, both in size and geography, Amazons payroll operations become increasingly complex, and our customers are asked to do more with less. Process can only get them so far, and thats where we come in with technology solutions to integrate and automate systems, detect defects before payment, and provide insights. As a data engineer in payroll, you will have to onboard payroll vendors across various geographies by building versatile and scalable design solutions. Having strong written and verbal communication, and the ability to communicate with end users in non-technical terms, is vital to your long-term success.\n\nThe ideal candidate will have experience working with large datasets, distributed computing technologies and service-oriented architecture. The candidate should relish working with large volumes of data, and enjoys the challenge of highly complex technical contexts. He/she should be an expert with data modeling, ETL design and business intelligence tools and has hand-on knowledge on columnar databases. He/she is a self-starter, comfortable with ambiguity, able to think big and enjoys working in a fast-paced team.\n\nResponsibilities:\n\nDesign, build and own all the components of a high-volume data warehouse end to end.\nBuild efficient data models using industry best practices and metadata for ad-hoc and pre-built reporting\nProvide wing-to-wing data engineering support for project lifecycle execution (design, execution and risk assessment)\nInterface with business customers, gathering requirements and delivering complete data & reporting solutions owning the design, development, and maintenance of ongoing metrics, reports, dashboards, etc. to drive key business decisions\nContinually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers\nInterface with other technology teams to extract, transform, and load (ETL) data from a wide variety of data sources\nOwn the functional and nonfunctional scaling of software systems in your ownership area.\nImplement big data solutions for distributed computing.\nWilling to learn and develop strong skill set in AWS technologies\n\n\nAs a DE on our team, you will be responsible for leading the data modelling, database design, and launch of some of the core data pipelines. You will have significant influence on our overall strategy by helping define the data model, drive the database design, and spearhead the best practices to delivery high quality products.\n\nA day in the life\nYou are expected to do data modelling, database design, build data pipelines as per Amazon standards, design reviews, and supporting data privacy and security initiatives. You will attend regular stand-up meetings and provide your updates. You will keep an eye out for opportunities to improve the product or user experience and suggest those enhancements. You will participate in requirement grooming meetings to ensure the use cases we deliver are complete and functional. You will take your turn at on-call and own production operational maintenance. You will respond to customer issues and monitor databases for healthy state and performance.\n\nAbout the team\nOur mission is to build applications which can solve challenges Global Payroll Operations teams face on daily basis, automate the tasks they perform manually, provide them seamless experience by integrating with other dependent systems, and eventually reduce Pay Defects and improve pay accuracy 3+ years of data engineering experience\n4+ years of SQL experience\nExperience with data modeling, warehousing and building ETL pipelines Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions\nExperience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)",,,,"['Payroll', 'metadata', 'SOA', 'data engineer ii', 'Database design', 'Risk assessment', 'data privacy', 'Business intelligence', 'Operations', 'SQL']",2025-06-10 14:14:01
Data Engineer-Enterprise Content Management,IBM,2 - 5 years,Not Disclosed,['Hyderabad'],"Experience is creating templates in Open Text Extreme CE 23.4 Version\nResponsible to design and develop different documents and business forms using OpenText Exstream.\n* Understanding of different input, output file formats, and Print file formats(PDF etc). Perform unit testing of templates/documents.\nApply styles and images to document design.\nUse output comparison tools to compare different outputs\nShould have experience working with Exstream Design Manager & Exstream Designer Tool.\nShould have prior knowledge on working with Exstream Web Service.\nDesigning Templates, Objects, Rules, Variables and creation of Documents based on Templates.\nUnderstand current SmartCOMM Templates and create templates based on that\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nIntuitive individual with an ability to manage change and proven time management\nProven interpersonal skills while contributing to team effort by accomplishing related results as needed\nUp-to-date technical knowledge by attending educational workshops, reviewing publications\n\n\nPreferred technical and professional experience\nExperience is creating templates in Open Text Extreme CE 23.4 Version\nResponsible to design and develop different documents and business forms using OpenText Exstream.\nUnderstanding of different input, output file formats, and Print file formats(PDF etc). Perform unit testing of templates/documents",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data management', 'unit testing', 'spring', 'java', 'general surgery', 'com', 'pile foundation', 'c++', 'c', 'data analysis', 'laparoscopic surgery', 'openshift', 'sql server', 'docker', 'test data management', 'urology', 'sapui5', 'aws', 'big data']",2025-06-10 14:14:04
Data Engineer-Data Integration,IBM,2 - 5 years,Not Disclosed,['Pune'],"As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You’ll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you’ll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you’ll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExpertise in designing and implementing scalable data warehouse solutions on Snowflake, including schema design, performance tuning, and query optimization.\nStrong experience in building data ingestion and transformation pipelines using Talend to process structured and unstructured data from various sources.\nProficiency in integrating data from cloud platforms into Snowflake using Talend and native Snowflake capabilities.\nHands-on experience with dimensional and relational data modelling techniques to support analytics and reporting requirements\n\n\nPreferred technical and professional experience\nUnderstanding of optimizing Snowflake workloads, including clustering keys, caching strategies, and query profiling.\nAbility to implement robust data validation, cleansing, and governance frameworks within ETL processes.\nProficiency in SQL and/or Shell scripting for custom transformations and automation tasks",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'sql', 'pipeline', 'data ingestion', 'shell scripting', 'python', 'data management', 'performance tuning', 'talend', 'data warehousing', 'cloud platforms', 'warehouse', 'docker', 'ansible', 'database design', 'elastic search', 'java', 'devops', 'splunk', 'etl', 'big data', 'aws', 'data integration']",2025-06-10 14:14:07
Data Engineer-Data Integration,IBM,2 - 5 years,Not Disclosed,['Chennai'],"As a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\n\n\n\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nDesign, develop, and maintain Ab Initio graphs for extracting, transforming, and loading (ETL) data from diverse sources to various target systems.\nImplement data quality and validation processes within Ab Initio.\nData Modeling and Analysis:.\nCollaborate with data architects and business analysts to understand data requirements and translate them into effective ETL processes.\nAnalyze and model data to ensure optimal ETL design and performance.\nAb Initio Components:. . Utilize Ab Initio components such as Transform Functions, Rollup, Join, Normalize, and others to build scalable and efficient data integration solutions.\nImplement best practices for reusable Ab Initio components\n\n\nPreferred technical and professional experience\nOptimize Ab Initio graphs for performance, ensuring efficient data processing and minimal resource utilization.\nConduct performance tuning and troubleshooting as needed.\nCollaboration:. .\nWork closely with cross-functional teams, including data analysts, database administrators, and quality assurance, to ensure seamless integration of ETL processes.\nParticipate in design reviews and provide technical expertise to enhance overall solution quality.\nDocumentation:.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ab initio', 'performance tuning', 'data modeling', 'troubleshooting', 'etl', 'hive', 'python', 'data analysis', 'oracle', 'data warehousing', 'data engineering', 'sql', 'plsql', 'data quality', 'tableau', 'unix shell scripting', 'spark', 'hadoop', 'big data', 'aws', 'data integration', 'informatica', 'unix']",2025-06-10 14:14:10
Data Engineer-Data Integration,IBM,2 - 5 years,Not Disclosed,['Pune'],"As a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our client’s business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nDesign, develop, and maintain Ab Initio graphs for extracting, transforming, and loading (ETL) data from diverse sources to various target systems.\nImplement data quality and validation processes within Ab Initio.\nData Modeling and Analysis:.\nCollaborate with data architects and business analysts to understand data requirements and translate them into effective ETL processes.\nAnalyze and model data to ensure optimal ETL design and performance.\nAb Initio Components:. . Utilize Ab Initio components such as Transform Functions, Rollup, Join, Normalize, and others to build scalable and efficient data integration solutions.\nImplement best practices for reusable Ab Initio components\n\n\nPreferred technical and professional experience\nOptimize Ab Initio graphs for performance, ensuring efficient data processing and minimal resource utilization.\nConduct performance tuning and troubleshooting as needed.\nCollaboration:. .\nWork closely with cross-functional teams, including data analysts, database administrators, and quality assurance, to ensure seamless integration of ETL processes.\nParticipate in design reviews and provide technical expertise to enhance overall solution quality.\nDocumentation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ab initio', 'performance tuning', 'data modeling', 'troubleshooting', 'etl', 'hive', 'python', 'data analysis', 'oracle', 'data warehousing', 'data engineering', 'sql', 'plsql', 'data quality', 'tableau', 'unix shell scripting', 'spark', 'hadoop', 'big data', 'aws', 'data integration', 'informatica', 'unix']",2025-06-10 14:14:13
Data Engineer,Truefirms,6 - 9 years,7-14 Lacs P.A.,['Hyderabad'],"Role Overview:\nWe are seeking a talented and forward-thinking Data Engineer for one of the large financial services GCC based in Hyderabad with responsibilities that include designing and constructing data pipelines, integrating data from multiple sources, developing scalable data solutions, optimizing data workflows, collaborating with cross-functional teams, implementing data governance practices, and ensuring data security and compliance.\n\nTechnical Requirements:\n\n1. Proficiency in ETL, Batch, and Streaming Process\n2. Experience with BigQuery, Cloud Storage, and CloudSQL\n3. Strong programming skills in Python, SQL, and Apache Beam for data processing\n4. Understanding of data modeling and schema design for analytics\n5. Knowledge of data governance, security, and compliance in GCP\n6. Familiarity with machine learning workflows and integration with GCP ML tools\n7. Ability to optimize performance within data pipelines\n\nFunctional Requirements:\n\n1. Ability to collaborate with Data Operations, Software Engineers, Data Scientists, and Business SMEs to develop Data Product Features\n2. Experience in leading and mentoring peers within an existing development team\n3. Strong communication skills to craft and communicate robust solutions\n4. Proficient in working with Engineering Leads, Enterprise and Data Architects, and Business Architects to build appropriate data foundations\n5. Willingness to work on contemporary data architecture in Public and Private Cloud environments T\nhis role offers a compelling opportunity for a seasoned Data Engineering to drive transformative cloud initiatives within the financial sector, leveraging unparalleled experience and expertise to deliver innovative cloud solutions that align with business imperatives and regulatory requirements.\nQualification Engineering Grad / Postgraduate\nCRITERIA\n1. Proficient in ETL, Python, and Apache Beam for data processing efficiency.\n2. Demonstrated expertise in BigQuery, Cloud Storage, and CloudSQL utilization.\n3. Strong collaboration skills with cross-functional teams for data product development.\n4. Comprehensive knowledge of data governance, security, and compliance in GCP.\n5. Experienced in optimizing performance within data pipelines for efficiency.\n6. Relevant Experience: 6-9 years\n\nConnect at 9993809253",Industry Type: IT Services & Consulting,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['Cloud Storage', 'GCC', 'ETL Tool', 'Gcp Cloud', 'integration with GCP ML tools', 'Bigquery', 'Apache Beam', 'Data Modeling', 'Python']",2025-06-10 14:14:16
Data Engineer - Azure,Blend360 India,5 - 9 years,Not Disclosed,['Hyderabad'],"Job Description\nAs a Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\nQualifications\n3+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar fiel",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-10 14:14:17
Data Engineer-Master Data Management,IBM,6 - 11 years,Not Disclosed,['Bengaluru'],"Design, develop, and maintain applications using Java/J2EE and IBM Infosphere MDM Advanced Edition v11.x.\nParticipate in development and support projects including fail & fix and maintenance activities.\nCollaborate with clients and internal teams to understand requirements and deliver solutions.\nProvide timely and effective support for multiple clients, ensuring quick resolution of issues.\nEngage directly with clients in a client-facing role, ensuring clear communication and understanding of business needs.\nParticipate in on-call support and be flexible to extend support after hours when required.\nFollow software engineering best practices, including continuous integration and continuous delivery (CI/CD\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n6+ years of hands-on experience in Java/J2EE technologies.\n4+ years of hands-on development experience in IBM Infosphere MDM Advanced Edition v11.x.\nStrong understanding and practical experience with CI/CD tools and practices.\nProven experience in development and support projects, including maintenance and issue resolution.\nStrong client interaction and communication skills, with experience in direct client-facing roles\n\n\nPreferred technical and professional experience\nPrior experience in the Banking domain or financial services industry.\nExposure to Agile methodologies and collaborative development environments.\nFamiliarity with other enterprise data management tools or technologies",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'java', 'cd tools', 'client interaction', 'ci / cd tools', 'kubernetes', 'data management', 'ci/cd', 'ansible', 'docker', 'sql', 'plsql', 'git', 'devops', 'linux', 'j2ee', 'jenkins', 'shell scripting', 'ibm infosphere', 'python', 'maven', 'mdm', 'terraform', 'agile', 'aws', 'j2ee technologies', 'unix']",2025-06-10 14:14:20
Senior Data Engineer,Denodo Technologies,8 - 12 years,Not Disclosed,['Chennai'],"Denodo is a leader in data management. The award-winning Denodo Platform is the leading data integration, management, and delivery platform using a logical approach to enable self- service BI, data science, hybrid/multi-cloud data integration, and enterprise data services. Realizing more than 400% ROI and millions of dollars in benefits, Denodo s large enterprise and mid-market customers across 30+ industries have received payback in less than 6 months. For more information, visit www.denodo.com .\nWe are a fast-growing, international organization with teams across four continents and we work with a cutting-edge technology, but thats not all we have to offer. At Denodo, we are like a family and it is of the utmost importance to us that we help support your professional growth every step of the way\nJob Description\nThe Opportunity\nDenodo is always looking for technical, passionate people to join our Customer Success team. We want a professional who will travel, consult, develop, train and troubleshoot to enhance our clients journey around Data Virtualization.\nYour mission: to help people realize their full potential through accelerated adoption and productive use of Denodo solutions.\nIn this role you will successfully employ a combination of high technical expertise and client management skills to conduct on-site and off-site consulting, product implementation and solutions development in either short or long-term engagements being a critical point of contact for getting things done among Denodo, partners and client teams.\nJob Responsibilities & Duties\nAs a Sr. Data Engineer you will successfully employ a combination of high technical expertise, research and investigative know-how, troubleshooting and problem-solving techniques, and communication skills between clients and internal Denodo teams to achieve your mission.\nObtain and maintain strong knowledge of the Denodo Platform, be able to deliver a superb technical discussion, including an overview of our key and advanced features and benefits, services offerings, differentiation, and competitive positioning.\nConstantly learn new things and maintain an overview of modern technologies.\nProvide technical consulting, training and support.\nDiagnose and resolve clients inquiries related to operating Denodo software products in their environment.\nParticipate in problem escalation and call prevention activities to help clients and other technical specialists increase their efficiency when using Denodo products.\nBe able to address a majority of technical questions concerning customization, integration, enterprise architecture and general feature/functionality of our product.\nProvide timely, prioritized and complete customer-based feedback to Product Management, Sales, Support and/or Development regarding client s business cases, requirements and issues.\nTrain and engage clients in the product architecture, configuration, and use of the Denodo Platform.\nPromote knowledge and best practices while managing deliverables and timelines.\nCapable of building and/or leading the development of custom deployments based on and even beyond client s requirements.\nManage client expectations, establish credibility at all levels within the client and build problem-solving partnerships with the client, partners and colleagues.\nBe willing to travel as necessary to address or service customer needs.\nLocation\nChennai, INDIA\nFunction\nCustomer Success\nDesired Skills & Experience\nRequired Skills\n8 - 12 years of experience in SQL/ ETL / Data warehousing / Data Integration / Data Virtualization technologies.\nBS or higher degree in Computer Science.\nSolid understanding of SQL and good grasp of relational and analytical database management theory and practice.\nGood knowledge of JDBC, XML and Web Services APIs.\nExcellent verbal and written communication skills to be able to interact with technical and business counterparts.\nActive listener.\nStrong analytical and problem solving abilities.\nLots of curiosity. You never stop learning new things.\nCreativity. We love to be surprised with innovative solutions.\nWillingness to travel on occasion.\nBe a team worker with a positive attitude.\nWe Value\nPrior experience in Denodo Platform.\nDenodo Certified Administrator (9.0 / 8.0) and Denodo Certified Developer (9.0 / 8.0).\nExperience working with GIT or other version control systems.\nExperience working with modern data architecture like lakehouse\nKnowledge and experience with systems and services hosted in the main cloud vendors (AWS, Azure, GCP).\nExperience in Windows & Linux (and UNIX) operating systems in server environments.\nBusiness software implementation and integration projects (e.g. ETL/Data Warehouse architectures, CEP, BPM).\nIntegration with packaged applications (e.g. relational databases, SAP, Siebel, Oracle Financials, Business Intelligence tools, )\nExperience / Knowledge in Containerization and Orchestration\nIndustry experience in supporting mission critical software components.\nExperience in attending customer engagements and writing technical documentation.\nForeign language skills are a plus.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Unix', 'SAP', 'Siebel', 'Linux', 'XML', 'JDBC', 'Windows', 'Business intelligence', 'Virtualization', 'SQL']",2025-06-10 14:14:23
Senior Data Engineer,TechAffinity,6 - 10 years,Not Disclosed,['Chennai'],"We are looking for a highly skilled Senior Data Engineer with strong expertise in Data Warehousing & Analytics to join our team. The ideal candidate will have extensive experience in designing and managing data solutions, advanced SQL proficiency, and hands-on expertise in Python.\nKey Responsibilities:\nDesign, develop, and maintain scalable data warehouse solutions.\nWrite and optimise complex SQL queries for data extraction, transformation, and Reporting.\nDevelop and automate data pipelines using Python.\nWork with AWS cloud services for data storage, processing, and analytics.\nCollaborate with cross-functional teams to provide data-driven insights and solutions.\nEnsure data integrity, security, and performance optimisation.\n\nRequired Skills & Experience:\n6-10 years of experience in Data Warehousing & Analytics.\nStrong proficiency in writing complex SQL queries with deep understanding of query optimization, stored procedures, and indexing.\nHands-on experience with Python for data processing and automation.\nExperience working with AWS cloud services.\nAbility to work independently and collaborate with teams across different time zones.\nGood to Have:\nExperience in the SAS domain and understanding of financial data structures.\nHands-on experience with reporting tools like Power BI or Tableau.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Power Bi', 'Tableau', 'SQL', 'Python', 'Data Warehousing Concepts', 'SAS', 'Azure Cloud', 'ETL', 'Aws Cloud Services', 'AWS', 'financial data structures.']",2025-06-10 14:14:25
"Sr Data Scientist, Supply Chain AI Delivery",Kimberly-Clark Corporation,4 - 9 years,Not Disclosed,['Bengaluru'],"You we're made to do this work: designing new technologies, diving into data, optimizing digital experiences, and constantly developing better, faster ways to get results. You want to be part of a performance culture dedicated to building technology for a purpose that matters. You want to work in an environment that promotes sustainability, inclusion, we'llbeing, and career development. In this role, you'll help us deliver better care for billions of people around the world. It starts with YOU.\n  In this role, you will:",,,,"['Procurement', 'Supply chain', 'Data analysis', 'Configuration management', 'Flex', 'Data processing', 'Monitoring', 'SQL', 'Logistics', 'Python']",2025-06-10 14:14:29
Data Engineer-Data Platforms-Azure,IBM,7 - 12 years,Not Disclosed,['Mumbai'],"Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.\n7+ Yrs total experience in Data Engineering projects & 4+ years of relevant experience on Azure technology services and Python\nAzure Azure data factory, ADLS- Azure data lake store, Azure data bricks,\nMandatory Programming languages Py-Spark, PL/SQL, Spark SQL\nDatabase SQL DB\nExperience with AzureADLS, Databricks, Stream Analytics, SQL DW, COSMOS DB, Analysis Services, Azure Functions, Serverless Architecture, ARM Templates\nExperience with relational SQL and NoSQL databases, including Postgres and Cassandra.\nExperience with object-oriented/object function scripting languagesPython, SQL, Scala, Spark-SQL etc.\nData Warehousing experience with strong domain\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nIntuitive individual with an ability to manage change and proven time management\nProven interpersonal skills while contributing to team effort by accomplishing related results as needed\nUp-to-date technical knowledge by attending educational workshops, reviewing publications\n\n\nPreferred technical and professional experience\nExperience with AzureADLS, Databricks, Stream Analytics, SQL DW, COSMOS DB, Analysis Services, Azure Functions, Serverless Architecture, ARM Templates\nExperience with relational SQL and NoSQL databases, including Postgres and Cassandra.\nExperience with object-oriented/object function scripting languagesPython, SQL, Scala, Spark-SQL etc.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data warehousing', 'sql', 'plsql', 'db', 'azure databricks', 'azure data lake', 'ssas', 'scala', 'microsoft azure', 'azure data factory', 'cosmos', 'data engineering', 'nosql', 'data bricks', 'azure functions', 'azure infrastructure', 'postgresql', 'spark', 'cassandra', 'mysql', 'engineering projects']",2025-06-10 14:14:31
Data Engineer,MNC Group,5 - 8 years,Not Disclosed,['Pune( Pune Nagar Road )'],"Knowledge and hands-on experience writing effective SQL queries and statements Understanding of AWS services At least 5 years of experience in a similar capacity At least 3 years of proficiency in using Python to develop and modify scripts At least 3 years of proficiency in managing data ingestion and DAG maintenance in airflow Preferred Requirements Knowledge in Hadoop Ecosystem like Spark or PySpark Knowledge in AWS services like S3, Data Lake, Redshift, EMR, EC2, Lambda, Glue, Aurora, RDS, Airflow.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Temporary/Contractual","['Pyspark', 'Airflow', 'Glue', 'AWS', 'Python', 'SQL']",2025-06-10 14:14:33
Software Engineer II - Data Engineer,JPMorgan Chase Bank,0 - 4 years,Not Disclosed,['Hyderabad'],"You thrive on diversity and creativity, and we welcome individuals who share our vision of making a lasting impact. Your unique combination of design thinking and experience will help us achieve new heights.\n\n\n\n\n\n\nAs a Data Engineer II at JPMorgan Chase within the Consumer Community Banking Team, you are part of an agile team that works to enhance, design, and deliver the data collection, storage, access, and analytics solutions in a secure, stable, and scalable way. As an emerging member of a data engineering team, you execute data solutions through the design, development, and technical troubleshooting of multiple components within a technical product, application, or system, while gaining the skills and experience needed to grow within your role.\n\n\nJob responsibilities\n\n\n\nOrganizes, updates, and maintains gathered data that will aid in making the data actionable\n\nDemonstrates basic knowledge of the data system components to determine controls needed to ensure secure data access\n\nBe responsible for making custom configuration changes in one to two tools to generate a product at the business or customer request\n\nUpdates logical or physical data models based on new use cases with minimal supervision\n\nAdds to team culture of diversity, equity, inclusion, and respect\n\nGathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of software applications and systems\n\nProactively identifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture\n\n\n\n\n\nRequired qualifications, capabilities, and skills\n\n\n\nFormal training or certification on Data Engineering concepts and 3+ years applied experience in AWS and Kubernetes\n\nProficiency in one or more large-scale data processing distributions such as JavaSpark/PySpark along with knowledge on Data Pipeline (DPL), Data Modeling, Data warehouse, Data Migration and so-on.\n\nExperience across the data lifecycle along with expertise with consuming data in any of batch (file), near real-time (IBM MQ, Apache Kafka), streaming (AWS kinesis, MSK)\n\nAdvanced at SQL (e. g. , joins and aggregations)\n\nWorking understanding of NoSQL databases\n\nExperience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages.\n\nSolid understanding of agile methodologies such as CI/CD, Application Resiliency, and Security\n\nSignificant experience with statistical data analysis and ability to determine appropriate tools and data patterns to perform analysis\n\nExperience customizing changes in a tool to generate product\n\n\n\nPreferred qualifications, capabilities, and skills\n\n\n\nFamiliarity with modern front-end technologies\n\nExperience designing and building REST API services using Java\n\nExposure to cloud technologies - knowledge on Hybrid cloud architectures is highly desirable.\n\nAWS Developer/Solutions Architect Certification is highly desired",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['System architecture', 'Data analysis', 'Data modeling', 'Coding', 'Debugging', 'Agile', 'Apache', 'Troubleshooting', 'Analytics', 'SQL']",2025-06-10 14:14:35
Data Engineer-Data Integration,IBM,2 - 5 years,Not Disclosed,['Pune'],"As an Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nVery good experience on Continuous Flow Graph tool used for point based development\nDesign, develop, and maintain ETL processes using Ab Initio tools.\nWrite, test, and deploy Ab Initio graphs, scripts, and other necessary components.\nTroubleshoot and resolve data processing issues and improve performance.\nData Integration:Extract, transform, and load data from various sources into data warehouses, operational data stores, or other target systems.\nWork with different data formats, including structured, semi-structured, and unstructured data\n\n\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ab initio', 'elastic search', 'splunk', 'etl', 'data integration', 'hlookup', 'conditional formatting', 'macros', 'charts', 'data management', 'data analysis', 'rtd', 'pivot table', 'data warehousing', 'vlookup', 'sql', 'control valves', 'advanced excel', 'pivot', 'instrumentation', 'big data', 'unix']",2025-06-10 14:14:37
Data Engineer-Data Integration,IBM,2 - 5 years,Not Disclosed,['Hyderabad'],"As an Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nVery good experience on Continuous Flow Graph tool used for point based development\nDesign, develop, and maintain ETL processes using Ab Initio tools.\nWrite, test, and deploy Ab Initio graphs, scripts, and other necessary components.\nTroubleshoot and resolve data processing issues and improve performance\n\nData Integration:\n\nExtract, transform, and load data from various sources into data warehouses, operational data stores, or other target systems.\n\nWork with different data formats, including structured, semi-structured, and unstructured data\n\n\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ab initio', 'elastic search', 'splunk', 'etl', 'data integration', 'hlookup', 'conditional formatting', 'macros', 'charts', 'data management', 'data analysis', 'rtd', 'pivot table', 'data warehousing', 'vlookup', 'sql', 'control valves', 'advanced excel', 'pivot', 'instrumentation', 'big data', 'unix']",2025-06-10 14:14:39
Data Engineer-Enterprise Content Management,IBM,2 - 5 years,Not Disclosed,['Hyderabad'],"Experience is creating templates in Open Text Extreme CE 23.4 Version\nResponsible to design and develop different documents and business forms using OpenText Exstream.\nUnderstanding of different input, output file formats, and Print file formats(PDF etc). Perform unit testing of templates/documents.\nApply styles and images to document design.\nUse output comparison tools to compare different outputs\nShould have experience working with Exstream Design Manager & Exstream Designer Tool.\nShould have prior knowledge on working with Exstream Web Service.\nDesigning Templates, Objects, Rules, Variables and creation of Documents based on Templates.\nUnderstand current SmartCOMM Templates and create templates based on that\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nIntuitive individual with an ability to manage change and proven time management\nProven interpersonal skills while contributing to team effort by accomplishing related results as needed\nUp-to-date technical knowledge by attending educational workshops, reviewing publications\n\n\nPreferred technical and professional experience\nExperience is creating templates in Open Text Extreme CE 23.4 Version\nResponsible to design and develop different documents and business forms using OpenText Exstream.\n* Understanding of different input, output file formats, and Print file formats (PDF etc). Perform unit testing of templates/documents",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['switching', 'data management', 'unit testing', 'networking', 'ospf', 'bgp', 'juniper', 'nortel', 'vrrp', 'asa', 'temenos t24', 'test data management', 'routing', 'firewall', 't24', 'hsrp', 'mpls', 'big data', 'cisco', 'ccna', 'fortigate']",2025-06-10 14:14:41
Data Engineer-Business Intelligence,IBM,2 - 5 years,Not Disclosed,['Pune'],"As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You’ll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you’ll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you’ll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviour’s.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\nExpertise in designing and implementing scalable data warehouse solutions on Snowflake, including schema design, performance tuning, and query optimization.\nStrong experience in building data ingestion and transformation pipelines using Talend to process structured and unstructured data from various sources.\nProficiency in integrating data from cloud platforms into Snowflake using Talend and native Snowflake capabilities.\nHands-on experience with dimensional and relational data modelling techniques to support analytics and reporting requirements.\nUnderstanding of optimizing Snowflake workloads, including clustering keys, caching strategies, and query profiling.\nAbility to implement robust data validation, cleansing, and governance frameworks within ETL processes.\nProficiency in SQL and/or Shell scripting for custom transformations and automation tasks\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nTableau Desktop Specialist, SQL -Strong understanding of SQL for Querying database\nGood to have - Python ; Snowflake, Statistics, ETL experience.\nExtensive knowledge on using creating impactful visualization using Tableau.\nMust have thorough understanding of SQL & advance SQL (Joining & Relationships)\n\n\nPreferred technical and professional experience\nMust have experience in working with different databases and how to blend & create relationships in Tableau.\nMust have extensive knowledge to creating Custom SQL to pull desired data from databases.\nTroubleshooting capabilities to debug Data controls. Capable of converting business requirements into workable model. Good communication skills, willingness to learn new technologies, Team Player, Self-Motivated, Positive Attitude.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['advance sql', 'snowflake', 'sql', 'shell scripting', 'database queries', 'python', 'performance tuning', 'talend', 'dbms', 'cloud platforms', 'warehouse', 'machine learning', 'business intelligence', 'pipeline', 'elastic search', 'tableau', 'data ingestion', 'splunk', 'etl', 'big data', 'statistics']",2025-06-10 14:14:44
Data Engineer,IntraEdge Technology,8 - 13 years,Not Disclosed,['Chennai'],"The Engineer will need to work on backend data model and data architecture, data migration, using python/java to build data ingestion pipelines and data validation processes, develop task schedulers for different data sources",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data migration', 'Backend', 'Data validation', 'Data modeling', 'Application development', 'Python', 'Data architecture']",2025-06-10 14:14:46
Data Engineer-Having Stratup-Mid-Size company Exp.@ Bangalore_Urgent,"As a leader in this space, we deliver wo...",8 - 13 years,Not Disclosed,['Bengaluru'],"Data Engineer\n\nLocation: Bangalore - Onsite\nExperience: 8 - 15 years\nType: Full-time\n\nRole Overview\n\nWe are seeking an experienced Data Engineer to build and maintain scalable, high-performance data pipelines and infrastructure for our next-generation data platform. The platform ingests and processes real-time and historical data from diverse industrial sources such as airport systems, sensors, cameras, and APIs. You will work closely with AI/ML engineers, data scientists, and DevOps to enable reliable analytics, forecasting, and anomaly detection use cases.\nKey Responsibilities\nDesign and implement real-time (Kafka, Spark/Flink) and batch (Airflow, Spark) pipelines for high-throughput data ingestion, processing, and transformation.\nDevelop data models and manage data lakes and warehouses (Delta Lake, Iceberg, etc) to support both analytical and ML workloads.\nIntegrate data from diverse sources: IoT sensors, databases (SQL/NoSQL), REST APIs, and flat files.\nEnsure pipeline scalability, observability, and data quality through monitoring, alerting, validation, and lineage tracking.\nCollaborate with AI/ML teams to provision clean and ML-ready datasets for training and inference.\nDeploy, optimize, and manage pipelines and data infrastructure across on-premise and hybrid environments.\nParticipate in architectural decisions to ensure resilient, cost-effective, and secure data flows.\nContribute to infrastructure-as-code and automation for data deployment using Terraform, Ansible, or similar tools.\n\n\nQualifications & Required Skills\n\nBachelors or Master’s in Computer Science, Engineering, or related field.\n6+ years in data engineering roles, with at least 2 years handling real-time or streaming pipelines.\nStrong programming skills in Python/Java and SQL.\nExperience with Apache Kafka, Apache Spark, or Apache Flink for real-time and batch processing.\nHands-on with Airflow, dbt, or other orchestration tools.\nFamiliarity with data modeling (OLAP/OLTP), schema evolution, and format handling (Parquet, Avro, ORC).\nExperience with hybrid/on-prem and cloud platforms (AWS/GCP/Azure) deployments.\nProficient in working with data lakes/warehouses like Snowflake, BigQuery, Redshift, or Delta Lake.\nKnowledge of DevOps practices, Docker/Kubernetes, Terraform or Ansible.\nExposure to data observability, data cataloging, and quality tools (e.g., Great Expectations, OpenMetadata).\nGood-to-Have\nExperience with time-series databases (e.g., InfluxDB, TimescaleDB) and sensor data.\nPrior experience in domains such as aviation, manufacturing, or logistics is a plus.\n\nRole & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['aviation', 'Data Modeling', 'Python', 'OLAP', 'Cloud', 'ORC', 'logistics', 'Avro', 'Terraform', 'Snowflake', 'manufacturing', 'AWS', 'Parquet', 'Java', 'Azure', 'BigQuery', 'Data', 'Redshift', 'SQL', 'TimescaleDB', 'GCP', 'InfluxDB', 'dbt', 'Ansible', 'OLTP', 'Kubernetes']",2025-06-10 14:14:49
Data Engineering Consultant- ETL/pyspark/SQL,Optum,6 - 11 years,Not Disclosed,['Hyderabad'],"Primary Responsibilities:\nDesign, code, test, document, and maintain high-quality and scalable data pipelines/solutions in cloud \nWork in both dev and ops and should be open to work in ops with flexible timings in ops\nIngest and transform data using variety of technologies from variety of sources (APIs, streaming, Files, Databases)\nDevelop reusable patterns and encourage innovation that will increase team’s velocity\nDesign and develop applications in an agile environment, deploy using CI/CD\nParticipate with prototypes as well as design and code reviews, own or assist with incident and problem management\nSelf-starter who can learn things quickly, who is enthusiastic and actively engaged\nComply with the terms and conditions of the employment contract, company policies and procedures, and any and all directives (such as, but not limited to, transfer and/or re-assignment to different work locations, change in teams and/or work shifts, policies in regards to flexibility of work benefits and/or work environment, alternative work arrangements, and other decisions that may arise due to the changing business environment). The Company may adopt, vary or rescind these policies and directives in its absolute discretion and without any limitation (implied or otherwise) on its ability to do so\n\nRequired Qualifications:\nBachelor's degree in technical domain.\nRequired experience with the following:\nDatabricks, Python, Spark, pyspark, SQL, Azure Data factory\nDesign and Implementation of Datawarehouse/Datalake (Databricks/snowflake)\nData architecture, Data modelling\nOperations Processes, reporting from operations, Incident resolutions\nGithub actions/Jenkins or similar CICD tool, Cloud CICD, GitHub\nNoSQL and relational databases\nPreferred Qualifications:\nExperience or knowledge in Apache Kafka\nExperience or knowledge in Data ingestions from variety of API’s\nWorking in Agile/Scrum environment",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Pyspark', 'ETL', 'Data Bricks', 'SQL']",2025-06-10 14:14:51
Gcp Data Engineer,PwC India,5 - 8 years,Not Disclosed,"['Navi Mumbai', 'Gurugram', 'Mumbai (All Areas)']","Job Description (GCP Data Engineer) :-\n\nGCP Developer 5to 9 years of experience\nLocation : Mumbai and Gurugram\n\ndesigning, building and deploying cloud solution for enterprise applications, with expertise in Cloud Platform Engineering.",,,,"['GCP', 'Bigquery', 'Kubernetes']",2025-06-10 14:14:53
Data Engineer-Data Platforms-Google,IBM,6 - 11 years,Not Disclosed,['Gurugram'],"6+ years of industry work experience\nExperience extracting data from a variety of sources, and a desire to expand those skills\nWorked on Google Looker tool\nWorked on Big Query and GCP technologies\nStrong SQL and Spark knowledge\nExcellent Data Analysis skills. Must be comfortable with querying and analyzing large amount of data on Hadoop HDFS using Hive and Spark\nKnowledge of Financial Accounting is a bonus\nWork independently with cross functional team and drive towards the resolution\nExperience with Object oriented programming using python and its design patterns\nExperience handling Unix systems, for optimal usage to host enterprise web applications GCP certifications preferred.\nPayments Industry Background good to have\nCandidate who has been part to google Cloud Migration is an ideal Fit\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n3-5 years of experience\nIntuitive individual with an ability to manage change and proven time management\nProven interpersonal skills while contributing to team effort by accomplishing related results as needed\nUp-to-date technical knowledge by attending educational workshops, reviewing publications\n\n\nPreferred technical and professional experience\n6+ years of industry work experience\nExperience extracting data from a variety of sources, and a desire to expand those skills\nWorked on Google Looker tool",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data analysis', 'sql', 'spark', 'hadoop', 'hive', 'snowflake', 'data warehousing', 'power bi', 'google', 'business intelligence', 'sql server', 'qlikview', 'tableau', 'data modeling', 'gcp', 'financial accounting', 'bigquery', 'object oriented programming', 'data visualization', 'etl', 'ssis', 'unix']",2025-06-10 14:14:55
Senior Data Engineer,Sailpoint Technologies,4 - 9 years,Not Disclosed,['Pune'],"Want to be on a team that full of results-driven individuals who are constantly seeking to innovate? Want to make an impact? At SailPoint, our Engineering team does just that. Our engineering is where high-quality professional engineering meets individual impact. Our team creates products are built on a mature, cloud-native event-driven microservices architecture hosted in AWS.\nSailPoint is seeking a Backend Software Engineer to help build a new cloud-based SaaS identity analytics product. We are looking for well-rounded backend or full stack engineers who are passionate about building and delivering reliable, scalable microservices and infrastructure for SaaS products.\nAs one of the first members on the team, you will be integral in building this product and will be part of an agile team that is in startup mode. This is a unique opportunity to build something from scratch but have the backing of an organization that has the muscle to take it to market quickly, with a very satisfied customer base.\nResponsibilities\nDeliver efficient, maintainable data pipelines\nDeliver robust, bug free code Java based micro services\nBuild and maintain Data Analytics and Machine Learning features\nProduce designs and rough estimates, and implement features based on product requirements.\nCollaborate with peers on designs, code reviews, and testing.\nProduce unit and end-to-end tests to improve code quality and maximize code coverage for new and existing features.\nResponsible for on-call production support\nRequirements\n4+ years of professional software development experience\nStrong Python, SQL, Java experience\nGreat communication skills\nBS in Computer Science, or a related field\nComprehensive experience with object-oriented analysis and design skills\nExperience with Workflow engines\nExperience with Continuous Delivery, Source control\nExperience with Observability platforms for performance metrics collection and monitoring.\nPreferred\nStrong Experience in AirFlow, Snowflake, DBT\nExperience with ML Pipelines (SageMaker)\nExperience with Continuous Delivery\nExperience working on a Big Data/Machine Learning product\nCompensation and benefits\nExperience a Small-company Atmosphere with Big-company Benefits.\nRecharge your batteries with a flexible vacation policy and paid holidays.\nGrow with us with both technical and career growth opportunities.\nEnjoy a healthy work-life balance with flexible hours, family-friendly company events and charitable work.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Backend', 'Production support', 'Machine learning', 'Agile', 'sailpoint', 'Workflow', 'Monitoring', 'SQL', 'Python']",2025-06-10 14:14:58
Manager Data Engineering,Renew,6 - 11 years,Not Disclosed,['Haryana'],"About Company\nJob Description\nKey responsibilities:\n1. Understand, implement, and automate ETL pipelines with better industry standards \n2. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, design infrastructure for greater scalability, etc \n3. Developing, integrating, testing, and maintaining existing and new applications \n4. Design, and create data pipelines (data lake / data warehouses) for real world energy analytical solutions \n5. Expert-level proficiency in Python (preferred) for automating everyday tasks \n6. Strong understanding and experience in distributed computing frameworks, particularly Spark, Spark-SQL, Kafka, Spark Streaming, Hive, Azure Databricks etc \n7. Limited experience in using other leading cloud platforms preferably Azure. \n8. Hands on experience on Azure data factory, logic app, Analysis service, Azure blob storage etc.\n9. Ability to work in a team in an agile setting, familiarity with JIRA and clear understanding of how Git works\n10. Must have 5-7 years of experience",Industry Type: Power,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'scala', 'data warehousing', 'azure data factory', 'distributed computing', 'sql', 'cloud', 'spark streaming', 'spark', 'gcp', 'hadoop', 'etl', 'big data', 'data lake', 'hbase', 'jira', 'azure databricks', 'python', 'oozie', 'microsoft azure', 'warehouse', 'data engineering', 'sql server', 'framework', 'mapreduce', 'kafka', 'hdfs', 'sqoop', 'aws']",2025-06-10 14:15:00
Senior Data Engineer,Go Digital Technology Consulting,3 - 8 years,Not Disclosed,"['Mumbai', 'Pune']","Role: Senior Data Engineer\nLocation: Mumbai & Pune\nExperience: 3yrs to 8yrs\n\nTechnologies / Skills: Advanced SQL, Python and associated libraries like Pandas, NumPy etc., Pyspark , Shell scripting, Data-Modelling, Big data, Hadoop, Hive, ETL pipelines.\n\nResponsibilities: • Proven success in communicating with users, other technical teams, and\nsenior management to collect requirements, describe data modeling decisions and develop data\nengineering strategy.\n• Ability to work with business owners to define key business requirements and convert to user stories with required technical specifications.\n• Communicate results and business impacts of insight initiatives to key stakeholders to\ncollaboratively solve business problems.\n• Working closely with the overall Enterprise Data & Analytics Architect and Engineering practice leads to ensure adherence with the best practices and design principles.\n• Assures quality, security and compliance requirements are met for supported area.\n• Design and create fault-tolerance data pipelines running on cluster\n• Excellent communication skills with the ability to influence client business and IT teams\n• Should have design data engineering solutions end to end. Ability to come up with scalable and\nmodular solutions.\n\nRequired Qualification:\n• 3+ years of hands-on experience Designing and developing Data Pipelines for Data Ingestion or\nTransformation using Python (PySpark)/Spark SQL in AWS cloud\n• Experience in design and development of data pipelines and processing of data at scale.\n• Advanced experience in writing and optimizing efficient SQL queries with Python and Hive handling Large Data Sets in Big-Data Environments\n• Experience in debugging, tunning and optimizing PySpark data pipelines\n• Should have implemented concepts and have good knowledge of Pyspark data frames, joins,\ncaching, memory management, partitioning, parallelism etc.\n• Understanding of Spark UI, Event Timelines, DAG, Spark config parameters, in order to tune the\nlong running data pipelines.\n• Experience working in Agile implementations\n• Experience with building data pipelines in streaming and batch mode.\n• Experience with Git and CI/CD pipelines to deploy cloud applications\n• Good knowledge of designing Hive tables with partitioning for performance.\n\nDesired Qualification:\n• Experience in data modelling.\n• Hands on creating workflows on any Scheduling Tool like Autosys, CA Workload Automation.\n• Proficiency in using SDKsfor interacting with native AWS services.\n• Strong understanding of concepts of ETL, ELT and data modeling.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Python', 'SQL', 'Hadoop', 'Big Data', 'Kafka', 'Spark', 'AWS']",2025-06-10 14:15:02
Sr Azure Data Engineer,Zealogics.com,3 - 6 years,Not Disclosed,[],"Responsibilities : Able to participate in business discussions and assist gathering data requirements. Good analytical and problem-solving skills to help address data challenges.\nProficiency in writing complex SQL queries for data extraction, transformation, and analysis. Knowledge of SQL functions, joins, subqueries, and performance tuning. Able to navigate source systems with minimal guidance to understand how data is related and use like data profiling to gain a better understanding of the data. Hands on experience with PySQL/Pyspark etc.\nHands on Experience in creating and managing data pipelines using Azure Data Factory. Understanding of data integration, transformation, and workflow orchestration in Azure environments.\nKnowledge of data engineering workflows and best practices in Databricks. Able to understand existing templates and patterns for development. Hands on experience with Unity Catalog and Databricks workflow.\nProficiency in using Git for version control and collaboration in data projects. Ability to work effectively in a team environment, especially in agile or collaborative settings.\nClear and effective communication skills to articulate findings and recommendations for other team members. Ability to document processes, workflows, and data analysis results effectively.\nWillingness to learn new tools, technologies, and techniques as the field of data analytics evolves. Being adaptable to changing project requirements and priorities.\n\nSkills\n\nAzure Databricks, Data Lakehouse architectures, and Azure Data Factory.\nExpertise in optimizing data workflows and predictive modeling.\nDesigning and implementing data pipelines using Databricks, Spark,\nExpertise in batch and streaming data solutions, automating workflows with CI/CD tools like Jenkins and Azure DevOps, and ensuring data governance with Delta Lake\nSpark, PySpark, Delta Lake, Azure DevOps, Python.\nAdvance SQL expertise",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Data analysis', 'Analytical', 'Agile', 'data governance', 'Workflow', 'Predictive modeling', 'SQL', 'Data extraction', 'Python']",2025-06-10 14:15:05
Data Engineering Specialist - Level 2,AstraZeneca India Pvt. Ltd,5 - 8 years,Not Disclosed,['Chennai'],"In this role, you'll be responsible for analyzing and improving ETL Data Integration standards, optimizing performance through automation, and supporting the development of automation solutions. you'll join a team of ETL authorities providing support across numerous projects, working closely with global business partners to ensure successful delivery. Your expertise will be crucial in coordinating database design and development, fix technical issues, and taking ownership of evolving CI/CD processes.\nEssential Skills/Experience:\nAdministering the ETL Platform including:\nConfiguration and Licensing\nUser Management\nProject Creation (including Bitbucket repositories)\nJob Server and Job management\nHandling of Software Updates/Patches\nMajor version migration\n8+ years coordinating Enterprise ETL platforms and working with ETL Data Integration in an enterprise scale environment.\nTechnical support to applications on fix Environment, software and application level issues.\nKnowledge and experience of working on Unix shell scripting or python script.\nExperience fix or debugging on Linux server is required.\nKnowledge and experience on AWS Cloud ecosystem.\nETL License Management.\nDesign and Code Reviews.\nAiding in the establishment of team development standard methodologies.\nData warehousing and database design concepts.\nPrior experience of Migration and Patching activity.\nDesirable Skills / Experience:\nMaintain and develop CI/CD pipeline using groovy scripts.\nMaintain and develop Ansible scripts for ETL service and infrastructure management.\nDesign and Develop tools to standardize and automate monitoring, application support activities.\nExcellent written and oral communication skills.\nAbility to work optimally independently or as part of a team to achieve objectives.\nA self-starter with high levels of drive, energy, and resilience.\nWilling to be involved in all areas of deployment process including testing and release planning.\nEager to learn and develop new tech skills as required.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'Application support', 'Linux', 'Database design', 'Debugging', 'Technical support', 'Unix shell scripting', 'Monitoring', 'Python', 'Recruitment']",2025-06-10 14:15:08
Senior Data Engineer,Kryon Knowledge Works,6 - 9 years,Not Disclosed,['Hyderabad'],"We are looking for a skilled Data Engineer with hands-on experience in Airflow, Python, AWS, and Big Data technologies like Spark to join our dynamic team.\n\nKey Responsibilities\n\nDesign and implement data pipelines and workflows using Apache Airflow\nDevelop robust and scalable data processing applications using Python\nLeverage AWS services (S3, EMR, Lambda, Glue, Redshift, etc.) for data engineering and ETL pipelines\nWork with Big Data technologies like Apache Spark to process large-scale datasets\nOptimize and monitor data pipelines for performance, reliability, and scalability\nCollaborate with Data Scientists, Analysts, and Business teams to understand data needs and deliver solutions\nEnsure data quality, consistency, and governance across all data pipelines\nDocument processes, pipelines, and best practices\nMandatory Skills\n\nApache Airflow - workflow orchestration and scheduling\nPython - strong programming skills for data engineering\nAWS - hands-on experience with core AWS data services\nBig Data technologies particularly Apache Spark\n\nLocation: Hyderabad (Hybrid)\nPlease share your resume with +91 9361912009",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Airflow', 'python', 'Spark', 'AWS']",2025-06-10 14:15:11
Data Engineering Advisor,ManipalCigna Health Insurance,11 - 13 years,Not Disclosed,['Hyderabad'],"Data Engineering Advisor - HIH - Evernorth\nAbout Evernorth:\nEvernorth Health Services, a division of The Cigna Group (NYSE: CI), creates pharmacy, care, and benefits solutions to improve health and increase vitality. We relentlessly innovate to make the prediction, prevention, and treatment of illness and disease more accessible to millions of people.\nRole Title: Software Engineering Advisor\nPosition Summary:\nData engineer on the Data integration team\nJob Description & Responsibilities:\nWork with business and technical leadership to understand requirements.\nDesign to the requirements and document the designs\nAbility to write product-grade performant code for data extraction, transformations and loading using Spark, Py-Spark\nDo data modeling as needed for the requirements.\nWrite performant queries using Teradata SQL, Hive SQL and Spark SQL against Teradata and Hive\nImplementing dev-ops pipelines to deploy code artifacts on to the designated platform/servers like AWS.\nTroubleshooting the issues, providing effective solutions and jobs monitoring in the production environment\nParticipate in sprint planning sessions, refinement/story-grooming sessions, daily scrums, demos and retrospectives.\nExperience Required:\nOverall 11-13 years of experience\nExperience Desired:\nStrong development experience in Spark, Py-Spark, Shell scripting, Teradata.\nStrong experience in writing complex and effective SQLs (using Teradata SQL, Hive SQL and Spark SQL) and Stored Procedures\nHealthcare domain knowledge is a plus\nEducation and Training Required:\nPrimary Skills:\nExcellent work experience on Databricks as Data Lake implementations\nExperience in Agile and working knowledge on DevOps tools (Git, Jenkins, Artifactory)\nAWS (S3, EC2, SNS, SQS, Lambda, ECS, Glue, IAM, and CloudWatch)\nDatabricks (Delta lake, Notebooks, Pipelines, cluster management, Azure/AWS integration\nAdditional Skills:\nExperience in Jira and Confluence\nExercises considerable creativity, foresight, and judgment in conceiving, planning, and delivering initiatives.\nLocation & Hours of Work\nHyderabad /General Shift (11:30 AM - 8:30 PM IST / 1:00 AM - 10:00 AM EST / 2:00 AM - 11:00 AM EDT)\nEqual Opportunity Statement\nEvernorth is an Equal Opportunity Employer actively encouraging and supporting organization-wide involvement of staff in diversity, equity, and inclusion efforts to educate, inform and advance both internal practices and external work with diverse client populations\n\nAbout Evernorth Health Services",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GIT', 'Data modeling', 'Shell scripting', 'Agile', 'Healthcare', 'Stored procedures', 'Troubleshooting', 'JIRA', 'Monitoring', 'Data extraction']",2025-06-10 14:15:13
Senior Specialist- Data Engineer,Leading Client,8 - 12 years,Not Disclosed,['Pune'],"Roles & Responsibilities:\nTotal 8-10 years of working experience Experience/Needs\n8-10 Years of experience with big data tools like Spark, Kafka, Hadoop etc.\nDesign and deliver consumer-centric high performant systems.\nYou would be dealing with huge volumes of data sets arriving through batch and streaming platforms.\nYou will be responsible to build and deliver data pipelines that process, transform, integrate and enrich data to meet various demands from business\nMentor team on infrastructural, networking, data migration, monitoring and troubleshooting aspects\nFocus on automation using Infrastructure as a Code (IaaC), Jenkins, devOps etc.\nDesign, build, test and deploy streaming pipelines for data processing in real time and at scale\nExperience with stream-processing systems like Storm, Spark-Streaming, Flink etc..\nExperience with object-oriented/object function scripting languagesScala, Java, etc.\nDevelop software systems using test driven development employing CI/CD practices\nPartner with other engineers and team members to develop software that meets business needs\nFollow Agile methodology for software development and technical documentation\nGood to have banking/finance domain knowledge\nStrong written and oral communication, presentation and interpersonal skills.\nExceptional analytical, conceptual, and problem-solving abilities\nAble to prioritize and execute tasks in a high-pressure environment\nExperience working in a team-oriented, collaborative environment\n8-10 years of hand on coding experience\nProficient in Java, with a good knowledge of its ecosystems\nExperience with writing Spark code using scala language\nExperience with BigData tools like Sqoop, Hive, Pig, Hue\nSolid understanding of object-oriented programming and HDFS concepts\nFamiliar with various design and architectural patterns\nExperience with big data toolsHadoop, Spark, Kafka, fink, Hive, Sqoop etc.\nExperience with relational SQL and NoSQL databases like MySQL, PostgreSQL, Mongo dB and Cassandra\nExperience with data pipeline tools like Airflow, etc.\nExperience with AWS cloud servicesEC2, S3, EMR, RDS, Redshift, BigQuery\nExperience with stream-processing systemsStorm, Spark-Streaming, Flink etc.\nExperience with object-oriented/object function scripting languagesPython, Java, Scala, etc.\nExpertise in design / developing platform components like caching, messaging, event processing, automation, transformation and tooling frameworks\nLocation:Pune/ Mumbai/ Bangalore/ Chennai",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Senior Specialist- Data Engineer', 'hive', 'continuous integration', 'scala', 'amazon redshift', 'apache storm', 'ci/cd', 'apache pig', 'emr', 'java', 'postgresql', 'aws cloud', 'spark', 'devops', 'jenkins', 'mysql', 'bigquery', 'hadoop', 'big data', 'mongodb', 'presentation skills', 'amazon rds', 'apache flink', 'nosql', 'cassandra', 'kafka', 'sqoop']",2025-06-10 14:15:15
Senior Data Engineer - Azure,Blend360 India,6 - 11 years,Not Disclosed,['Hyderabad'],"As a Sr Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\n7+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar field\nMust have experience e",Industry Type: Industrial Automation,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-10 14:15:18
Data Engineering Advisor,ManipalCigna Health Insurance,8 - 10 years,Not Disclosed,['Hyderabad'],"Data Engineering Advisor - HIH - Evernorth\nAbout Evernorth:\nEvernorth Health Services, a division of The Cigna Group (NYSE: CI), creates pharmacy, care, and benefits solutions to improve health and increase vitality. We relentlessly innovate to make the prediction, prevention, and treatment of illness and disease more accessible to millions of people.\nPosition Summary:\nData engineer on the Data integration team\nJob Description & Responsibilities:\nWork with business and technical leadership to understand requirements.\nDesign to the requirements and document the designs\nAbility to write product-grade performant code for data extraction, transformations and loading using Spark, Py-Spark\nDo data modeling as needed for the requirements.\nWrite performant queries using Teradata SQL, Hive SQL and Spark SQL against Teradata and Hive\nImplementing dev-ops pipelines to deploy code artifacts on to the designated platform/servers like AWS.\nTroubleshooting the issues, providing effective solutions and jobs monitoring in the production environment\nParticipate in sprint planning sessions, refinement/story-grooming sessions, daily scrums, demos and retrospectives.\nExperience Required:\nOverall 8-10 years of experience\nExperience Desired:\nStrong development experience in Spark, Py-Spark, Shell scripting, Teradata.\nStrong experience in writing complex and effective SQLs (using Teradata SQL, Hive SQL and Spark SQL) and Stored Procedures\nHealth care domain knowledge is a plus\nPrimary Skills:\nExcellent work experience on Databricks as Data Lake implementations\nExperience in Agile and working knowledge on DevOps tools (Git, Jenkins, Artifactory)\nAWS (S3, EC2, SNS, SQS, Lambda, ECS, Glue, IAM, and CloudWatch)\nDatabricks (Delta lake, Notebooks, Pipelines, cluster management, Azure/AWS integration\nAdditional Skills:\nExperience in Jira and Confluence\nExercises considerable creativity, foresight, and judgment in conceiving, planning, and delivering initiatives.\nLocation & Hours of Work\nHyderabad /General Shift (11:30 AM - 8:30 PM IST / 1:00 AM - 10:00 AM EST / 2:00 AM - 11:00 AM EDT)\nEqual Opportunity Statement\nEvernorth is an Equal Opportunity Employer actively encouraging and supporting organization-wide involvement of staff in diversity, equity, and inclusion efforts to educate, inform and advance both internal practices and external work with diverse client populations\nAbout Evernorth Health Services",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GIT', 'Data modeling', 'Shell scripting', 'Agile', 'Engineering Manager', 'Stored procedures', 'Troubleshooting', 'JIRA', 'Monitoring', 'Data extraction']",2025-06-10 14:15:20
Junior Data Engineer,Xequalto Analytics,3 - 4 years,Not Disclosed,['Bengaluru'],"Location: Remote\nEmployment Type: Full-Time\nExperience: 3+ Years\nAbout Us\nWe're a fast-growing company driven by data\nWe're looking for a skilled and enthusiastic Junior Data Engineer to join our team and help us shape the future of our data infrastructure\nThis is a fully remote role work from wherever you're most productive\nIf you're passionate about data and eager to make a real impact, we want to hear from you\nAbout the Role\nAs a Junior Data Engineer, you'll be a key player in our data engineering efforts\nYou'll be working hands-on, collaborating with a talented team, and contributing directly to the development and maintenance of our data pipelines and infrastructure\nThis role offers a unique opportunity to learn, grow, and make a tangible difference in how we leverage data\nWhat You'll Do:\nDesign and build robust data pipelines using tools like Databricks, Spark, and PySpark\nDevelop and maintain our data warehouse and data models, ensuring they meet the needs of our analytics and operations teams\nDive into data transformation and processing with SQL and Python\nPartner with engineers, analysts, and stakeholders across the company to understand their data needs and deliver effective solutions\nMaintain clean and organized code using Git\nContribute to our ongoing efforts to improve data quality and ensure data integrity\nWhat You'll Need:\n3+ years of experience in data engineering\nA solid understanding of cloud platforms like AWS or Azure\nStrong skills in Python, SQL, Spark, and PySpark\nPractical experience with cloud-based ETL tools\nA genuine passion for problem-solving and a desire to learn and grow\nExcellent communication skills and a collaborative spirit\nBonus Points:\nExperience with DevOps tools (Docker, Terraform, Airflow, GitHub Actions the more the merrier)\nFamiliarity with CI/CD pipelines and infrastructure as code\nA knack for optimizing workflows and boosting performance\nWhat We Offer:\n100% remote work work from anywhere!\nA supportive and collaborative team environment\nOpportunities for professional development and growth\nA competitive salary and benefits package\nShow more Show less",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data bricks', 'python', 'spark', 'pyspark', 'microsoft azure', 'data engineering', 'sql', 'communication skills']",2025-06-10 14:15:21
Software Engineer III - Data Engineer,JPMorgan Chase Bank,3 - 8 years,Not Disclosed,['Hyderabad'],"As a Data Engineer III at JPMorgan Chase within the Consumer & Community Banking Team, you serve as a seasoned member of an agile team to design and deliver trusted data collection, storage, access, and analytics solutions in a secure, stable, and scalable way. You are responsible for developing, testing, and maintaining critical data pipelines and architectures across multiple technical areas within various business functions in support of the firm s business objectives.\nJob responsibilities\nExecutes software solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problems\nSupports review of controls to ensure sufficient protection of enterprise data\nAdvises and makes custom configuration changes in one to two tools to generate a product at the business or customer request\nUpdates logical or physical data models based on new use cases\nFrequently uses SQL and understands NoSQL databases and their niche in the marketplace\nAdds to team culture of diversity, equity, inclusion, and respect\nContributes to software and data engineering communities of practice and events that explore new and emerging technologies\nGathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of software applications and systems\nRequired qualifications, capabilities, and skills\nFormal training or certification on Data Engineering concepts and 3+ years applied experience in AWS and Kubernetes\nProficiency in one or more large-scale data processing distributions such as JavaSpark/PySpark along with knowledge on Data Pipeline (DPL), Data Modeling, Data warehouse, Data Migration and so-on.\nExperience across the data lifecycle along with expertise with consuming data in any of batch (file), near real-time (IBM MQ, Apache Kafka), streaming (AWS kinesis, MSK)\nAdvanced at SQL (eg, joins and aggregations)\nWorking understanding of NoSQL databases\nExperience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages.\nSolid understanding of agile methodologies such as CI/CD, Application Resiliency, and Security\nSignificant experience with statistical data analysis and ability to determine appropriate tools and data patterns to perform analysis\nExperience customizing changes in a tool to generate product\nPreferred qualifications, capabilities, and skills\nFamiliarity with modern front-end technologies\nExperience designing and building REST API services using Java\nExposure to cloud technologies - knowledge on Hybrid cloud architectures is highly desirable.\nAWS Developer/Solutions Architect Certification is highly desired",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'Data migration', 'Front end', 'Data modeling', 'Debugging', 'Agile', 'Apache', 'Troubleshooting', 'Analytics', 'SQL']",2025-06-10 14:15:24
Lead Data Engineer - VP,JPMorgan Chase Bank,11 - 15 years,Not Disclosed,['Hyderabad'],"Join us as we embark on a journey of collaboration and innovation, where your unique skills and talents will be valued and celebrated. Together we will create a brighter future and make a meaningful difference.\nAs a Lead Data Engineer at JPMorgan Chase within the CCB (Connected Commerce), you are an integral part of an agile team that works to enhance, build, and deliver data collection, storage, access, and analytics solutions in a secure, stable, and scalable way. As a core technical contributor, you are responsible for maintaining critical data pipelines and architectures across multiple technical areas within various business functions in support of the firm s business objectives.\n  Job responsibilities\nArchitect and oversee the design of complex data solutions that meet diverse business needs and customer requirements.\nGuide the evolution of logical and physical data models to support emerging business use cases and technological advancements.\nBuild and manage end-to-end cloud-native data pipelines in AWS, leveraging your hands-on expertise with AWS components.\nBuild analytical systems from the ground up, providing architectural direction, translating business issues into specific requirements, and identifying appropriate data to support solutions.\nWork across the Service Delivery Lifecycle on engineering major/minor enhancements and ongoing maintenance of existing applications.\nConduct feasibility studies, capacity planning, and process redesign/re-engineering of complex integration solutions.\nHelp others build code to extract raw data, coach the team on techniques to validate its quality, and apply your deep data knowledge to ensure the correct data is ingested across the pipeline.\nGuide the development of data tools used to transform, manage, and access data, and advise the team on writing and validating code to test the storage and availability of data platforms for resilience.\nOversee the implementation of performance monitoring protocols across data pipelines, coaching the team on building visualizations and aggregations to monitor pipeline health.\nCoach others on implementing solutions and self-healing processes that minimize points of failure across multiple product features.\nRequired qualifications, capabilities, and skills\nFormal training or certification on software engineering concepts and 5+ years applied experience\nExtensive experience in managing the full lifecycle of data, from collection and storage to analysis and reporting.\nProficiency in one or more large-scale data processing distributions such as JavaSpark along with knowledge on Data Pipeline (DPL), Data Modeling, Data warehouse, Data Migration and so-on.\nHands-on practical experience in system design, application development, testing, and operational stability\nProficient in coding in one or more modern programming languages\nShould have good hands-on experience on AWS services and its components along with good understanding on Kubernetes.\nExperience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages.\nStrong understanding of domain driven design, micro-services patterns, and architecture\nOverall knowledge of the Software Development Life Cycle along with experience with IBM MQ, Apache Kafka\nSolid understanding of agile methodologies such as CI/CD, Application Resiliency, and Security\nDemonstrated knowledge of software applications and technical processes within a technical discipline (eg, cloud, LLMs etc)\nPreferred qualifications, capabilities, and skills\nFamiliarity with modern front-end technologies\nExperience designing and building REST API services using Java\nExposure to cloud technologies - knowledge on Hybrid cloud architectures is highly desirable.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data migration', 'Front end', 'Data modeling', 'Coding', 'Debugging', 'Agile', 'System design', 'Application development', 'Apache', 'Analytics']",2025-06-10 14:15:27
Assistant Manager - Data Engineer,Ajanta Pharma,5 - 10 years,6-12 Lacs P.A.,['Mumbai (All Areas)'],"Position Overview:\nThe Assistant Manager - Data Engineer will play a pivotal role in the design, development, and maintenance of data pipelines that ensure the efficiency, scalability, and reliability of our data infrastructure. This role will involve optimizing and automating ETL/ELT processes, as well as developing and refining databases, data warehouses, and data lakes. As an Assistant Manager, you will also mentor junior engineers and collaborate closely with cross-functional teams to support business goals and drive data excellence.\n\nKey Responsibilities:\nData Pipeline Development: Design, build, and maintain efficient, scalable, and reliable data pipelines to support data analytics, reporting, and business intelligence initiatives.\nDatabase and Data Warehouse Management: Develop, optimize, and manage databases, data warehouses, and data lakes to enhance data accessibility and business decision-making.\nETL/ELT Optimization: Automate and optimize data extraction, transformation, and loading (ETL/ELT) processes, ensuring efficient data flow and improved system performance.\nData Modeling & Architecture: Develop and maintain data models to enable structured data storage, analysis, and reporting in alignment with business needs.\nWorkflow Management Systems: Implement, optimize, and maintain workflow management tools (e.g., Apache Airflow, Talend) to streamline data engineering tasks and improve operational efficiency.\nTeam Leadership & Mentorship: Guide, mentor, and support junior data engineers to enhance their skills and contribute effectively to projects.\nCollaboration with Cross-Functional Teams: Work closely with data scientists, analysts, business stakeholders, and IT teams to understand requirements and deliver solutions that align with business objectives.\nPerformance Optimization: Continuously monitor and optimize data pipelines and storage solutions to ensure maximum performance and cost efficiency.\nDocumentation & Process Improvement: Create and maintain documentation for data models, workflows, and systems. Contribute to the continuous improvement of data engineering practices.\n\nQualifications:\nEducational Background: B.E., B.Tech., MCA\nProfessional Experience: At least 5 to 7 years of experience in a data engineering or similar role, with hands-on experience in building and optimizing data pipelines, ETL processes, and database management.\nTechnical Skills:\nProficiency in Python and SQL for data processing, transformation, and querying.\nExperience with modern data warehousing solutions (e.g., Amazon Redshift, Snowflake, Google BigQuery, Azure Data Lake).\nStrong background in data modeling (dimensional, relational, star/snowflake schema).\nHands-on experience with ETL tools (e.g., Apache Airflow, Talend, Informatica) and workflow management systems.\nFamiliarity with cloud platforms (AWS, Azure, Google Cloud) and distributed data processing frameworks (e.g., Apache Spark).\nData Visualization & Exploration: Familiarity with data visualization tools (e.g., Tableau, Power BI) for analysis and reporting.\nLeadership Skills: Demonstrated ability to manage and mentor a team of junior data engineers while fostering a collaborative and innovative work environment.\nProblem-Solving & Analytical Skills: Strong analytical and troubleshooting skills with the ability to optimize complex data systems for performance and scalability.\nExperience in Pharma/Healthcare (preferred but not required): Knowledge of the pharmaceutical industry and experience with data in regulated environments.\nDesired Skills:\nFamiliarity with industry-specific data standards and regulations.\nExperience working with machine learning models or data science pipelines is a plus.\nStrong communication skills with the ability to present technical data to non-technical stakeholders.\n\nWhy Join Us:\nImpactful Work: Contribute to the pharmaceutical industry by improving data-driven decisions that impact public health.\nCareer Growth: Opportunities to develop professionally in a fast-growing industry and company.\nCollaborative Environment: Work with a dynamic and talented team of engineers, data scientists, and business stakeholders.\nCompetitive Benefits: Competitive salary, health benefits and more.",Industry Type: Pharmaceutical & Life Sciences,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Pipeline Development', 'ELT optimisation', 'Data Visualization', 'Data Warehouse Management', 'Data Modeling', 'Workflow Management Systems', 'Data Warehousing', 'ETL', 'machine learning', 'Google Cloud Platforms', 'Python', 'azure']",2025-06-10 14:15:30
Sr. Data Engineer,Greystar,7 - 12 years,Not Disclosed,['Mohali'],"Overview\nGreystar is looking for dedicated and hard-working individuals who want to help us continue to be the best at what we do. Today, we are the largest rental housing operator and developer in the US and one of the largest global investment management companies, delivering industry-leading services to investors, clients, and residents. We offer unrivaled professional development and career growth opportunities to our team members and look forward to welcoming you to Greystar, where our people are what make us the Global Leader in Rental Housing.",,,,"['Pyspark', 'Azure Databricks', 'SQL']",2025-06-10 14:15:32
Azure Data Engineer,Cognitio Analytics,3 - 8 years,Not Disclosed,['Gurugram'],"Sr. Data Scientist\nGurugram (Hybrid)\n3 To 6 years\n+ Job Description\nApply\nIdeal qualifications, skills and experiences we are looking for are:\nWe are actively seeking a talented and results-driven Data Scientist to join our team and take ownership of deliverables through the power of data analytics and insights.\nYour contributions will be instrumental in making data-informed decisions, identifying growth opportunities, and propelling our organization to new levels of success.\nDoctorate/Master s/bachelor s degree in data science, Statistics, Computer Science, Mathematics, Economics, commerce or a related field.\nMinimum of 3 years of experience working as a Data Scientist or in a similar analytical role, with experience leading data science projects and teams.\nExperience in Healthcare domain with exposure to clinical operations, financial, risk rating, fraud, digital, sales and marketing, and wellness, e-commerce or the ed tech industry is a plus.\nExpertise in programming languages such as SQL, Python/PySpark and proficiency with data manipulation, analysis, and visualization libraries (e.g., pandas, NumPy, Matplotlib, seaborn).\nVery strong python and exceptional with pandas, NumPy, advanced python (pytest, class, inheritance, docstrings).\nDeep understanding of machine learning algorithms, model evaluation, and feature engineering. Experience with frameworks like scikit-learn, TensorFlow, or Py torch.",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'data science', 'Analytical', 'Financial risk', 'Machine learning', 'Programming', 'Healthcare', 'Wellness', 'SQL', 'Python']",2025-06-10 14:15:34
"Senior Staff Engineer, Big Data AWS Engineer",Nagarro,10 - 13 years,Not Disclosed,['Gurugram'],"We're Nagarro.\n\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal experience 9+ years.\nHands-on experience in Big Data Engineering.\nStrong expertise in Apache Spark and PySpark/Python.\nDeep technical knowledge of AWS Glue (Crawler, Data Catalog).\nHands on working experience in Python.\nStrong working experience with AWS services, including S3, Lambda, SNS, Secret Manager, and Athena.\nProven experience with Infrastructure as Code using CloudFormation and Terraform.\nSolid experience in Snowflake.\nProficiency in setting up and maintaining CI/CD pipelines with GitHub Actions.\nFamiliarity with tools like Jira and GitHub.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\n\nRESPONSIBILITIES:\nUnderstanding the clients business use cases and technical requirements and be able to convert them into technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design documents explaining overall architecture, framework, and high-level design of the application for the developers\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken\nCarrying out POCs to make sure that suggested design/technologies meet the requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Spark', 'Aws Glue', 'Python', 'Pyspark', 'Aws Lambda', 'Snowflake']",2025-06-10 14:15:36
SAS Data Scientist,Lericon Informatics,2 - 6 years,Not Disclosed,"['Kolkata', 'Pune', 'Chennai']","Job Summary:\nWe are seeking a skilled Data Scientist with experience in both Python and SAS to join our high-performing analytics team. The ideal candidate will be adept at developing predictive models, analyzing large datasets, and delivering actionable insights using modern data science tools and platforms.\n\nKey Responsibilities:\n\nDesign and develop machine learning and statistical models using Python and SAS.\nConduct data exploration, preprocessing, and analysis on structured and unstructured datasets.\nUse SAS (Base, Advanced, Enterprise Guide, Visual Analytics, or SAS Viya) for reporting, data preparation, and statistical modeling as required.\nWork with large-scale datasets using Python libraries such as Pandas, NumPy, Scikit-learn, and TensorFlow.\nTranslate business requirements into technical solutions in collaboration with cross-functional teams.\nBuild data pipelines and automate workflows for data analysis and model deployment.\nPresent data-driven insights through visualizations using tools such as Seaborn, Plotly, Matplotlib, or SAS VA.\nDocument models, code, and methodologies for reproducibility and auditability.\n\nQualifications:\n\n2-10 years of experience in data science, machine learning, or advanced analytics.\nProficient in Python (Pandas, NumPy, Scikit-learn, TensorFlow, PyTorch, etc.) and SAS (Base, Advanced, DI, VA, or Viya).\nStrong knowledge of SQL and experience working with relational databases.\nSolid foundation in statistical analysis, hypothesis testing, regression, classification, clustering, etc.\nExperience in building, evaluating, and deploying predictive models.\nExcellent communication skills and ability to convey complex findings to non-technical stakeholders.\n\nPreferred Qualifications :\n\nExperience with big data tools (Spark, Hive, Hadoop).\nExposure to MLOps tools (MLflow, Airflow, Docker, etc.).\nFamiliarity with cloud platforms (AWS, Azure, GCP).\nUnderstanding of SAS integration with cloud or open-source tools.\nExperience with NLP, image recognition, or deep learning frameworks.\nLocation: Pan India-Delhi / NCR,Bangalore/Bengaluru,Hyderabad/Secunderabad,Chennai,Pune,Kolkata,Ahmedabad,Mumbai",Industry Type: Banking,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SAS', 'Predictive Modeling', 'Data Engineering', 'Scikit-learn', 'Machine Learning', 'NumPy', 'SQL', 'Data Science', 'Cloud Platforms', 'SAS Viya', 'Pandas', 'Data Analysis', 'Statistical Modeling', 'Data Visualization', 'Python']",2025-06-10 14:15:38
Data Engineer Oracle Analytics Cloud & OCI Big Data,Malomatia,7 - 12 years,25-35 Lacs P.A.,['Pune'],"Job Title: Data Engineer Oracle Analytics Cloud & OCI Big Data\nLocation: Remote / Contract\nExperience Level: Mid to Senior (6+ years)\nJob Summary:\nWe are seeking a skilled and experienced Data Engineer with a minimum of 5 years of hands-on experience in building and optimizing data solutions on Oracle Analytics Cloud (OAC), OCI Data Flow, and OCI Big Data platforms. The ideal candidate will play a critical role in designing, developing, and maintaining scalable data pipelines and analytics solutions that support data lakehouse, enterprise reporting and advanced data analytics use cases.\nKey Responsibilities:\nDesign and implement robust data ingestion and transformation pipelines using OCI Data Flow and Big Data services.\nDevelop data models, datasets, and reports within Oracle Analytics Cloud to support business intelligence and analytics needs.\nIntegrate data from multiple sources (structured and unstructured) into cloud-based data platforms using Oracle-native tools and services.\nEnsure data quality and consistency through standardized processes and validation routines.\nOptimize performance of ETL/ELT jobs and troubleshoot data-related issues in Oracle Cloud Infrastructure environments.\nMaintain data platform infrastructure using OCI-native services and automation tools.\nDocument technical processes, data models, and data pipeline architecture.\nRequired Qualifications:\nBachelor’s in Computer Science, Information Systems, Engineering, or a related field.\nMinimum 5 years of hands-on experience in data engineering, with proven expertise in Oracle Analytics Cloud (OAC), OCI Data Flow, and OCI Big Data.\nExperience with Oracle Autonomous Data Warehouse and Data Lakehouse architecture.\nStrong understanding of Oracle Cloud Infrastructure (OCI) and related data services.\nExperience with Spark, PySpark, Hive, and Big Data ecosystems.\nSolid experience in SQL, PL/SQL, and data wrangling using scripting languages (e.g., Python).\nExperience in ETL/ELT development, performance tuning, and data pipeline optimization.\nFamiliarity with data security, privacy, and governance best practices in cloud environments.\nExcellent problem-solving skills and ability to work in a fast-paced, team-oriented environment.\nKnowledge of DevOps practices and CI/CD tools in the context of data platform operations.\nExposure to data cataloging and metadata management tools.\nMust have:\n5+ years hands on data engineering experience using Oracle Analytics Cloud, OCI Data Flow, OCI Big Data, Oracle Autonomous Data Warehouse\nKnowledge of DevOps practices and CI/CD\nCertifications (Nice to Have):\nOracle Cloud Infrastructure (OCI) Data Engineer or Architect Certification\nOracle Analytics Cloud Certification",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Oac', 'OCI Data Flow', 'OCI Big Data']",2025-06-10 14:15:40
Cloud Data Engineer,Acesoft,7 - 9 years,19-20 Lacs P.A.,['Bengaluru'],"Hi all,\nWe are hiring for the role Cloud Data Engineer\nExperience: 7 - 9 years\nLocation: Bangalore\nNotice Period: Immediate - 15 Days\nSkills:\nOverall, 7 to 9 years of experience in cloud data and analytics platforms such as AWS, Azure, or GCP\n• Including 3+ years experience with Azure cloud Analytical tools is a must\n• Including 5+ years of experience working with data & analytics concepts such as SQL, ETL, ELT, reporting and report building, data visualization, data lineage, data importing & exporting, and data warehousing\n• Including 3+ years of experience working with general IT concepts such as integrations, encryption, authentication & authorization, batch processing, real-time processing, CI/CD, automation\n• Advanced knowledge of cloud technologies and services, specifically around Azure Data Analytics tools\no Azure Functions (Compute)\no Azure Blob Storage (Storage)\no Azure Cosmos DB (Databases)\no Azure Synapse Analytics (Databases)\no Azure Data Factory (Analytics)\no Azure Synapse Serverless SQL Pools (Analytics)\no Azure Event Hubs (Analytics- Realtime data)\n• Strong coding skills in languages such as\no SQL\no Python\no PySpark\n• Experience in data streaming technologies such as Kafka or Azure Event Hubs\n• Experience in handling unstructured streaming data is highly desired\n• Knowledge of Business Intelligence Dimensional Modelling, Star Schemas, Slowly Changing Dimensions\n• Broad understanding of data engineering methodologies and tools, including Data warehousing, DevOps/DataOps, Data ingestion, ELT/ETL and Data visualization tools\n• Knowledge of database management systems, data modelling, and data warehousing best practices\n• Experience in software development on a team using Agile methodology\n• Knowledge of data governance and security practices\n\nIf you are interested drop your resume at mojesh.p@acesoftlabs.com\nCall: 9701971793",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Cloud Analytics', 'Azure Cloud', 'Data Warehousing', 'AWS', 'Python', 'Azure Data Analytics tools', 'authentication & authorization', 'PySpark', 'batch processing', 'Elt', 'sql', 'automation', 'encryption', 'Azure Cosmos DB', 'real-time processing', 'CI/CD', 'etl', 'integrations']",2025-06-10 14:15:42
Data Engineer-Master Data Management,IBM,6 - 11 years,Not Disclosed,['Pune'],"Design, develop, and maintain applications using Java/J2EE and IBM Infosphere MDM Advanced Edition v11.x.\nParticipate in development and support projects including fail & fix and maintenance activities.\nCollaborate with clients and internal teams to understand requirements and deliver solutions.\nProvide timely and effective support for multiple clients, ensuring quick resolution of issues.\nEngage directly with clients in a client-facing role, ensuring clear communication and understanding of business needs.\nParticipate in on-call support and be flexible to extend support after hours when required.\nFollow software engineering best practices, including continuous integration and continuous delivery (CI/CD\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n6+ years of hands-on experience in Java/J2EE technologies.\n4+ years of hands-on development experience in IBM Infosphere MDM Advanced Edition v11.x.\nStrong understanding and practical experience with CI/CD tools and practices.\nProven experience in development and support projects, including maintenance and issue resolution.\nStrong client interaction and communication skills, with experience in direct client-facing roles\n\n\nPreferred technical and professional experience\nPrior experience in the Banking domain or financial services industry.\nExposure to Agile methodologies and collaborative development environments.\nFamiliarity with other enterprise data management tools or technologies",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'java', 'cd tools', 'client interaction', 'ci / cd tools', 'kubernetes', 'data management', 'ci/cd', 'ansible', 'docker', 'sql', 'plsql', 'git', 'devops', 'linux', 'j2ee', 'jenkins', 'shell scripting', 'ibm infosphere', 'python', 'maven', 'mdm', 'terraform', 'agile', 'aws', 'j2ee technologies', 'unix']",2025-06-10 14:15:45
Snowflake Data Engineer,Randomtrees,6 - 11 years,Not Disclosed,['Chennai'],"Data Architect/Engineer and implement data solutions across Retail industry(SCM, Marketing, Sales, and Customer Service, using technologies such as DBT, Snowflake, and Azure/AWS/GCP.\nDesign and optimize data pipelines that integrate various data sources (1st party, 3rd party, operational) to support business intelligence and advanced analytics.\nDevelop data models and data flows that enable personalized customer experiences and support omnichannel marketing and customer engagement.\nLead efforts to ensure data governance, data quality, and data security, adhering to compliance with regulations such as and .",,,,"['Snowflake', 'Data Build Tool', 'Data Warehousing', 'Data Modeling', 'SQL', 'Azure', 'GCP', 'DBT', 'ETL', 'AWS', 'Python']",2025-06-10 14:15:48
"Data Engineer- ETL (AWS Glue)/Redshift ,Java ,SQL -P3",Nielsen Sports,5 - 8 years,Not Disclosed,['Gurugram'],"Job Description:\nWe are seeking an experienced Developer with expertise in ETL, AWS Glue and Data Engineer experience, combined with strong skills in Java and SQL. The ideal candidate will have 5-8 years of experience designing, developing, and implementing ETL processes and data integration solutions. Responsibilities include developing ETL pipelines using AWS Glue, managing data workflows with Informatica, and writing complex SQL queries. Strong problem-solving abilities and experience with data warehousing are essential.\nKey Skills:\nProficiency in AWS Glue and Informatica ETL tools\n\nStrong Java programming skills\n\nAdvanced SQL querying and optimization\n\nExperience with data integration and data warehousing\n\nExcellent problem-solving and analytical skills",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Analytical skills', 'SQL queries', 'Data management', 'Social media', 'Programming', 'Informatica', 'Data warehousing', 'AWS', 'SQL']",2025-06-10 14:15:50
"Data Engineer- ETL (AWS Glue)/Redshift ,Java ,SQL -P3",Nielsen Sports,5 - 8 years,Not Disclosed,['Mumbai'],"Job Description:\nWe are seeking an experienced Developer with expertise in ETL, AWS Glue and Data Engineer experience, combined with strong skills in Java and SQL. The ideal candidate will have 5-8 years of experience designing, developing, and implementing ETL processes and data integration solutions. Responsibilities include developing ETL pipelines using AWS Glue, managing data workflows with Informatica, and writing complex SQL queries. Strong problem-solving abilities and experience with data warehousing are essential.\nKey Skills:\nProficiency in AWS Glue and Informatica ETL tools\n\nStrong Java programming skills\n\nAdvanced SQL querying and optimization\n\nExperience with data integration and data warehousing\n\nExcellent problem-solving and analytical skills",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Analytical skills', 'SQL queries', 'Data management', 'Social media', 'Programming', 'Informatica', 'Data warehousing', 'AWS', 'SQL']",2025-06-10 14:15:52
Azure or AWS Data Engineer,Cignex Datamatics,5 - 10 years,Not Disclosed,['Bengaluru'],"We are looking for a highly skilled Lead Data Engineer with expertise in Azure or AWS and Databricks to join our team. The ideal candidate will lead the design, development, and implementation of data engineering solutions, ensuring scalability, security, and efficiency in our data infrastructure. This role requires strong technical skills, and experience in managing large-scale data processing pipelines.\nKey Responsibilities:\nLead the design and development of scalable and reliable data pipelines using Azure Data Services or AWS Data Services and Databricks.\nArchitect, implement, and optimize ETL/ELT processes to process large volumes of structured and unstructured data.\nDevelop and maintain data models, data lakes, and data warehouses to support analytics and business intelligence needs.\nCollaborate with data scientists, analysts, and business stakeholders to ensure data availability and integrity.\nImplement and enforce data governance, security, and compliance best practices.\nOptimize and monitor performance of data processing frameworks (Spark, Databricks, etc.).\nAutomate and orchestrate data workflows using tools such as Apache Airflow, Azure Data Factory, AWS Step Functions, or Glue.\nGuide and mentor junior data engineers in best practices and modern data engineering techniques.\nMandatory Qualifications:\n5+ years of experience in data engineering\nStrong expertise in Azure Data Services (Azure Data Lake, Azure Synapse, Azure Data Factory) or AWS Data Services (S3, Redshift, Glue, Lambda, Step Functions, EMR).\nProficiency in Databricks and experience with Apache Spark for large-scale data processing.\nStrong programming skills in Python\nExperience working with SQL and NoSQL databases such as PostgreSQL, MySQL, DynamoDB, or CosmosDB.\nSolid understanding of data governance, security, and compliance (GDPR, HIPAA, etc.) is a plus.\nExperience with real-time streaming technologies such as Kafka, Kinesis, or Event Hubs is a plus.\nExcellent problem-solving skills and the ability to work in a fast-paced, agile environment.\nGood to have Qualifications:\nExperience with machine learning pipelines and MLOps.\nFamiliarity with data visualization and BI tools like Power BI, Tableau, or Looker.\nStrong communication and leadership skills to drive best practices across the team.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Postgresql', 'MySQL', 'HIPAA', 'Machine learning', 'Agile', 'Data processing', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-10 14:15:55
Senior Data Engineer,Aventior Digital,7 - 12 years,Not Disclosed,[],"Designation: Senior Data Engineer\nPreferred Experience: 7+ years\n\nResponsibilities:\nDesign, develop, and maintain data pipelines for ingesting, processing, and transforming data from various sources into actionable insights.\nIntegrate data from disparate sources (databases, APIs, and files) into a unified data platform using ETL processes and data integration techniques.\nDesign and implement data models, schemas, and data structures to support analytical queries, reporting, and business intelligence needs.\nOptimize database performance, query execution, and data processing workflows for efficiency, scalability, and reliability.\nEnsure data quality, integrity, and consistency through data validation, cleansing, deduplication, and error-handling mechanisms.\nArchitect and implement data solutions on Azure cloud platforms, leveraging Azure services for data storage, processing, and analytics.\nImplement data security measures, encryption techniques, and access controls to protect sensitive data and ensure compliance with regulations (e.g., GDPR, HIPAA).\nWork closely with cross-functional teams, data scientists, analysts, and stakeholders to understand data requirements, provide data solutions, and communicate insights effectively.\nDocument data processes, workflows, and best practices, and promote data governance standards, data lineage, and metadata management.\nStay updated with emerging technologies, industry trends, and best practices in data engineering, cloud computing, and data analytics to drive innovation and continuous improvement.\n\nRequired Skills:\nProficiency in writing complex SQL queries, stored procedures, and functions for data extraction, transformation, and analysis.\nExperience in database design, optimization, and management using SQL Server/Azure SQL Database.\nKnowledge of data modeling techniques, including entity-relationship diagrams, dimensional modeling, and data normalization, is needed to design efficient data structures.\nFamiliarity with ETL (Extract, Transform, Load) processes and tools such as Azure Data Factory, SSIS (SQL Server Integration Services), or other data integration platforms.\nHands-on experience with Azure cloud services, including Azure SQL Database, Azure Data Factory and Azure Storage.\nExperience handling large-scale data processing and analytics with strong analytical skills and attention to detail.\nStrong statistical knowledge.\nExcellent communication skills.\n\nGood To Have Skills:\nExperience working with C#, .Net Framework\nExperience with other Azure cloud services and sharepoint\nExperience with Python as programming language\nExperience manipulating unstructured data with regular expressions.\nBasic understanding of machine learning concepts and algorithms for data mining, predictive modeling, and statistical analysis.\nKnowledge of data warehousing concepts, methodologies, and tools for building and maintaining data warehouses or data marts.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL Queries', 'Azure Data Factory', 'Data Engineering', 'ETL', 'SSIS', 'SQL']",2025-06-10 14:15:57
Data Engineering,Stack Digital,3 - 7 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Job Description\nDesign, develop, and optimize large-scale data processing pipelines using PySpark .\nWork with various Apache tools and frameworks (like Hadoop, Hive, HDFS, etc.) to ingest, transform, and manage large datasets.\nEnsure high performance and reliability of ETL jobs in production.\nCollaborate with Data Scientists, Analysts, and other stakeholders to understand data needs and deliver robust data solutions.\nImplement data quality checks and data lineage tracking for transparency and auditability.\nWork on data ingestion, transformation, and integration from multiple structured and unstructured sources.\nLeverage Apache NiFi for automated and repeatable data flow management (if applicable).\nWrite clean, efficient, and maintainable code in Python and Java .\nContribute to architectural decisions, performance tuning, and scalability planning.\nRequired Skills:\n5 7 years of experience.\nStrong hands-on experience with PySpark for distributed data processing.\nDeep understanding of Apache ecosystem (Hadoop, Hive, Spark, HDFS, etc.).\nSolid grasp of data warehousing , ETL principles , and data modeling .\nExperience working with large-scale datasets and performance optimization.\nFamiliarity with SQL and NoSQL databases.\nProficiency in Python and basic to intermediate knowledge of Java .\nExperience in using version control tools like Git and CI/CD pipelines.\nNice-to-Have Skills:\nWorking experience with Apache NiFi for data flow orchestration.\nExperience in building real-time streaming data pipelines .\nKnowledge of cloud platforms like AWS , Azure , or GCP .\nFamiliarity with containerization tools like Docker or orchestration tools like Kubernetes .\nSoft Skills:\nStrong analytical and problem-solving skills.\nExcellent communication and collaboration abilities.\nSelf-driven with the ability to work independently and as part of a team.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Version control', 'GIT', 'orchestration', 'Architecture', 'Data modeling', 'Analytical', 'Data processing', 'Data quality', 'Apache', 'SQL']",2025-06-10 14:15:59
Data Engineer - Associate,FedEx,1 - 2 years,Not Disclosed,"['Gurugram', 'Bengaluru', 'Mumbai (All Areas)']","Role & responsibilities :\n\nDevelop and maintain data workflows using Ab Initio tools.\nAnalyze data, troubleshoot issues, and resolve defects within data pipelines.\nParticipate in Agile ceremonies including daily stand-ups, sprint planning, and reviews.\nApply CI/CD practices using tools like Jenkins and work within Unix/Linux environments.\nMaintain documentation including metadata definitions, onboarding materials, and SOPs.\nContribute to modernization and cloud-readiness efforts, particularly leveraging Azure and Databricks.\n\n\n\nPreferred candidate profile\n\n1-2 years of hands-on experience in data engineering, ETL development, or data analytics.\nExperience with SQL and working knowledge of Unix/Linux systems.\nFamiliarity with ETL tools such as Ab Initio.\nAbility to analyze, debug, and optimize data workflows.\nExposure to CI/CD pipelines and version control practices.\nStrong analytical thinking, attention to detail, and documentation skills.\nComfortable working in Agile development environments",Industry Type: Courier / Logistics (Logistics Tech),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Ab Initio', 'ETL', 'Unix', 'Linux', 'Informatica', 'Data Modeling', 'Data Warehousing', 'SQL']",2025-06-10 14:16:01
Data Engineer-Business Intelligence,IBM,2 - 5 years,Not Disclosed,['Pune'],"Provide expertise in analysis, requirements gathering, design, coordination, customization, testing and support of reports, in client’s environment\nDevelop and maintain a strong working relationship with business and technical members of the team\nRelentless focus on quality and continuous improvement\nPerform root cause analysis of reports issues\nDevelopment / evolutionary maintenance of the environment, performance, capability and availability.\nAssisting in defining technical requirements and developing solutions\nEffective content and source-code management, troubleshooting and debugging\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nTableau Desktop Specialist, SQL -Strong understanding of SQL for Querying database Good to have - Python ; Snowflake, Statistics, ETL experience.\nExtensive knowledge on using creating impactful visualization using Tableau.\nMust have thorough understanding of SQL & advance SQL (Joining & Relationships).\nMust have experience in working with different databases and how to blend & create relationships in Tableau.\nMust have extensive knowledge to creating Custom SQL to pull desired data from databases. Troubleshooting capabilities to debug Data controls\n\n\nPreferred technical and professional experience\nTroubleshooting capabilities to debug Data controls Capable of converting business requirements into workable model.\nGood communication skills, willingness to learn new technologies, Team Player, Self-Motivated, Positive Attitude.\nMust have thorough understanding of SQL & advance SQL (Joining & Relationships",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['advance sql', 'dbms', 'sql', 'debugging', 'database queries', 'snowflake', 'python', 'data analysis', 'data analytics', 'power bi', 'business analysis', 'data warehousing', 'business analytics', 'business intelligence', 'sql server', 'tableau', 'troubleshooting', 'data visualization', 'etl', 'statistics']",2025-06-10 14:16:05
Data Engineer - Analyst,FedEx,3 - 5 years,Not Disclosed,"['Gurugram', 'Bengaluru', 'Mumbai (All Areas)']","Role & responsibilities :\n\nDesign,develop, and maintain ETL workflows using Ab Initio.\nManage and support critical data pipelines and data sets across complex,high-volume environments.\nPerform data analysis and troubleshoot issues across Teradata and Oracle data sources.\nCollaborate with DevOps for CI/CD pipeline integration using Jenkins, and manage deployments in Unix/Linux environments.\nParticipate in Agile ceremonies including stand-ups, sprint planning, and roadmap discussions.\nSupport cloud migration efforts, including potential adoption of Azure,Databricks, and PySparkbased solutions.\nContribute to project documentation, metadata management (LDM, PDM), onboarding guides, and SOPs\n\n\n\nPreferred candidate profile\n\n3 years of experience in data engineering, with proven expertise in ETL development and maintenance.\nProficiency with Ab Initio tools (GDE, EME, Control Center).\nStrong SQL skills, particularly with Oracle or Teradata.\nSolid experience with Unix/Linux systems and scripting.\nFamiliarity with CI/CD pipelines using Jenkins or similar tools.\nStrong communication skills and ability to collaborate with cross-functional teams.",Industry Type: Courier / Logistics (Logistics Tech),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Ab Initio', 'Data Modeling', 'ETL', 'Cicd Pipeline', 'Informatica', 'Data Warehousing', 'Databricks', 'Teradata', 'Oracle', 'SQL']",2025-06-10 14:16:07
Data Scientist - L3,Wipro,3 - 6 years,Not Disclosed,['Bengaluru'],"Wipro Limited (NYSEWIT, BSE507685, NSEWIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients’ most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.\n About The Role  \n\nRole Purpose\n\nThe purpose of the role is to define, architect and lead delivery of machine learning and AI solutions.\n\n ? \n\nDo\n\n1. Demand generation through support in Solution development\n\na. Support Go-To-Market strategy\n\ni. Contribute to development solutions, proof of concepts aligned to key offerings to enable solution led sales\n\nb. Collaborate with different colleges and institutes for research initiatives and provide data science courses\n\n2. Revenue generation through Building & operationalizing Machine Learning, Deep Learning solutions\n\na. Develop Machine Learning / Deep learning models for decision augmentation or for automation solutions\n\nb. Collaborate with ML Engineers, Data engineers and IT to evaluate ML deployment options\n\n3. Team Management\n\na. Talent Management\n\ni. Support on boarding and training to enhance capability & effectiveness\n\n ? \n\nDeliver\n\n\nNo.\n\nPerformance Parameter\n\nMeasure 1. Demand generation # PoC supported 2. Revenue generation through delivery Timeliness, customer success stories, customer use cases 3. Capability Building & Team Management # Skills acquired\n\n\n ? \n\n ? \nMandatory\n\nSkills:\nData Analysis.\n\nExperience3-5 Years.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analysis', 'machine learning', 'deep learning', 'data science', 'ml', 'python', 'natural language processing', 'scikit-learn', 'neural networks', 'ml deployment', 'data engineering', 'artificial intelligence', 'sql', 'tensorflow', 'r', 'predictive modeling', 'statistical modeling', 'statistics']",2025-06-10 14:16:10
DBA DATA Engineer || 12 Lakhs CTC,Robotics Technologies,5 - 9 years,12 Lacs P.A.,['Hyderabad( Banjara hills )'],"Dear Candidate,\nWe are seeking a skilled and experienced DBA Data Engineer to join our growing data team. The ideal candidate will play a key role in designing, implementing, and maintaining our databases and data pipeline architecture. You will collaborate with software engineers, data analysts, and DevOps teams to ensure efficient data flow, data integrity, and optimal database performance across all systems. This role requires a strong foundation in database administration, SQL performance tuning, data modeling, and experience with both on-prem and cloud-based environments.\n\nRequirements:\nBachelors degree in Computer Science, Information Systems, or a related field.\n5+ years of experience in database administration and data engineering.\nProven expertise in RDBMS (Oracle, MySQL, and PostgreSQL) and NoSQL systems (MongoDB, Cassandra).\nExperience managing databases in cloud environments (AWS, Azure, or GCP).\nProficiency in ETL processes and tools (e.g., Apache NiFi, Talend, Informatica, AWS Glue).\nStrong experience with scripting languages such as Python, Bash, or PowerShell.\n\nDBA Data Engineer Roles & Responsibilities:\nDesign and maintain scalable and high-performance database architectures.\nMonitor and optimize database performance using tools like CloudWatch, Oracle Enterprise Manager, pgAdmin, Mongo Compass, or Dynatrace.\nDevelop and manage ETL/ELT pipelines to support business intelligence and analytics.\nEnsure data integrity and security through best practices in backup, recovery, and encryption.\nAutomate regular database maintenance tasks using scripting and scheduled jobs.\nImplement high availability, failover, and disaster recovery strategies.\nConduct performance tuning of queries, stored procedures, indexes, and table structures.\nCollaborate with DevOps to automate database deployments using CI/CD and IaC tools (e.g., Terraform, AWS CloudFormation).\nDesign and implement data models, including star/snowflake schemas for data warehousing.\nDocument data flows, data dictionaries, and database configurations.\nManage user access and security policies using IAM roles or database-native permissions.\nAnalyze existing data systems and propose modernization or migration plans (on-prem to cloud, SQL to NoSQL, etc.).\nUse AWS RDS, Amazon Redshift, Azure SQL Database, or Google BigQuery as needed.\nStay up-to-date with emerging database technologies and make recommendations.\n\nMust-Have Skills:\nDeep knowledge of SQL and database performance tuning.\nHands-on experience with database migrations and replication strategies.\nFamiliarity with data governance, data quality, and compliance frameworks (GDPR, HIPAA, etc.).\nStrong problem-solving and troubleshooting skills.\nExperience with data streaming platforms such as Apache Kafka, AWS Kinesis, or Apache Flink is a plus.\nExperience with data lake and data warehouse architectures.\nExcellent communication and documentation skills.\n\nSoft Skills:\nProblem-Solving: Ability to analyze complex problems and develop effective solutions.\nCommunication Skills: Strong verbal and written communication skills to effectively collaborate with cross-functional teams.\nAnalytical Thinking: Ability to think critically and analytically to solve technical challenges.\nTime Management: Capable of managing multiple tasks and deadlines in a fast-paced environment.\nAdaptability: Ability to quickly learn and adapt to new technologies and methodologies.\n\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database Administration', 'Database Migration', 'Dba Skills', 'High Availability', 'Db Administration', 'Hadr', 'Database Security', 'Disaster Recovery', 'Dr Testing', 'DR', 'Luw', 'Db Upgrade', 'Brtools', 'UDDI', 'Os Migration', 'Dbase', 'DBMS', 'IBM DB2', 'Db Migration', 'Disaster Recovery Planning', 'Db Dba', 'Backup And Recovery']",2025-06-10 14:16:12
Application Developer-Google Cloud Migration,IBM,2 - 6 years,Not Disclosed,['Bengaluru'],"As an Application Developer, you will lead IBM into the future by translating system requirements into the design and development of customized systems in an agile environment. The success of IBM is in your hands as you transform vital business needs into code and drive innovation. Your work will power IBM and its clients globally, collaborating and integrating code into enterprise systems. You will have access to the latest education, tools and technology, and a limitless career path with the world’s technology leader. Come to IBM and make a global impact!\nIBM’s Cloud Services are focused on supporting clients on their cloud journey across any platform to achieve their business goals. It encompasses Cloud Advisory, Architecture, Cloud Native Development, Application Portfolio Migration, Modernization, and Rationalization as well as Cloud Operations.\n\nCloud Services supports all public/private/hybrid Cloud deployments:\nIBM Bluemix/IBM Cloud/Red Hat/AWS/ Azure/Google and client private environments.\nCloud Services has the best Cloud developer architect Complex SI, Sys Ops and delivery talent delivered through our GEO CIC Factory model.\nAs a member of our Cloud Practice you will be responsible for defining and implementing application cloud migration, modernisation and rationalisation solutions for clients across all sectors. You will support mobilisation and help to lead the quality of our programmes and services, liaise with clients and provide consulting services including:\nCreate cloud migration strategies; defining delivery architecture, creating the migration plans, designing the orchestration plans and more.\nAssist in creating and executing of migration run books\nEvaluate source cloud (Physical Virtual and Cloud) and target Workloads\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nGCP using pub sub, big query, dataflow, cloud workflow/cloud scheduler, cloud run, data proc, Cloud Function Cloud data engineers with GCP PDE certification and working experience with GCP.\nBuilding end to end data pipelines in GCP using pub sub, big query, dataflow, cloud workflow/cloud scheduler, cloud run, data proc, Cloud Function\nExperience in logging and monitoring of GCP services and Experience in Terraform and infrastructure automation.\nExpertise in Python coding language\nDevelops data engineering solutions on Google Cloud ecosystem and supports and maintains data engineering solutions on Google Cloud ecosystem\n\n\nPreferred technical and professional experience\nStay updated with the latest trends and advancements in cloud technologies, frameworks, and tools.\nConduct code reviews and provide constructive feedback to maintain code quality and ensure adherence to best practices.\nTroubleshoot and debug issues and deploy applications to the cloud platform",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'dataproc', 'gcp', 'terraform', 'bigquery', 'cloud services', 'cloud migration', 'redhat linux', 'microsoft azure', 'cloud deployment', 'data engineering', 'cloud technologies', 'cloud platform', 'cloud native', 'java', 'hybrid cloud', 'linux', 'cloud operations', 'troubleshooting', 'agile', 'aws']",2025-06-10 14:16:15
Staff Data Engineer - Machine Learning,Netradyne,5 - 8 years,22.5-35 Lacs P.A.,['Bengaluru'],"Role and Responsibilities:\n\nYou will be embedded within a team of machine learning engineers and data scientists; responsible for building and productizing generative AI and deep learning solutions. You will:\nDesign, develop and deploy production ready scalable solutions that utilizes GenAI, Traditional ML models, Data science and ETL pipelines\nCollaborate with cross-functional teams to integrate AI-driven solutions into business operations.\nBuild and enhance frameworks for automation, data processing, and model deployment.\nUtilize Gen-AI tools and workflows to improve the efficiency and effectiveness of AI solutions.\nConduct research and stay updated with the latest advancements in generative AI and related technologies.\nDeliver key product features within cloud analytics.\n\nRequirements:\n\nB. Tech, M. Tech or PhD in Computer Science, Data Science, Electrical Engineering, Statistics, Maths, Operations Research or related domain.\nStrong programming skills in Python, SQL and solid fundamentals in computer science, particularly in algorithms, data structures, and OOP.\nExperience with building end-to-end solutions on AWS cloud infra.\nGood understanding of internals and schema design for various data stores (RDBMS, Vector databases and NoSQL).\nExperience with Gen-AI tools and workflows, and large language models (LLMs).\nExperience with cloud platforms and deploying models at scale.\nStrong analytical and problem-solving skills with a keen attention to detail.\nStrong knowledge of statistics, probability, and estimation theory.\n\nDesired Skills:\n\nFamiliarity with frameworks such as PyTorch, TensorFlow and Hugging Face.\nExperience with data visualization tools like Tableau, Graphana, Plotly-Dash.\nExposure to AWS services like Kinesis, SQS, EKS, ASG, lambda etc.\nExpertise in at least one popular Python web-framework (like FastAPI, Django or Flask).\nExposure to quick prototyping using Streamlit, Gradio, Dash etc.\nExposure to Big Data processing (Snowflake, Redshift, HDFS, EMR)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'AWS', 'Generative Artificial Intelligence', 'Python', 'Big Data Technologies']",2025-06-10 14:16:17
DBA DATA Engineer || 12 Lakhs CTC,Robotics Technologies,6 - 9 years,12 Lacs P.A.,['Hyderabad( Banjara hills )'],"Dear Candidate,\nWe are seeking a skilled and experienced DBA Data Engineer to join our growing data team. The ideal candidate will play a key role in designing, implementing, and maintaining our databases and data pipeline architecture. You will collaborate with software engineers, data analysts, and DevOps teams to ensure efficient data flow, data integrity, and optimal database performance across all systems. This role requires a strong foundation in database administration, SQL performance tuning, data modeling, and experience with both on-prem and cloud-based environments.\n\nRequirements:\nBachelors degree in Computer Science, Information Systems, or a related field.\n6+ years of experience in database administration and data engineering.\nProven expertise in RDBMS (Oracle, MySQL, and PostgreSQL) and NoSQL systems (MongoDB, Cassandra).\nExperience managing databases in cloud environments (AWS, Azure, or GCP).\nProficiency in ETL processes and tools (e.g., Apache NiFi, Talend, Informatica, AWS Glue).\nStrong experience with scripting languages such as Python, Bash, or PowerShell.\n\nDBA Data Engineer Roles & Responsibilities:\nDesign and maintain scalable and high-performance database architectures.\nMonitor and optimize database performance using tools like CloudWatch, Oracle Enterprise Manager, pgAdmin, Mongo Compass, or Dynatrace.\nDevelop and manage ETL/ELT pipelines to support business intelligence and analytics.\nEnsure data integrity and security through best practices in backup, recovery, and encryption.\nAutomate regular database maintenance tasks using scripting and scheduled jobs.\nImplement high availability, failover, and disaster recovery strategies.\nConduct performance tuning of queries, stored procedures, indexes, and table structures.\nCollaborate with DevOps to automate database deployments using CI/CD and IaC tools (e.g., Terraform, AWS CloudFormation).\nDesign and implement data models, including star/snowflake schemas for data warehousing.\nDocument data flows, data dictionaries, and database configurations.\nManage user access and security policies using IAM roles or database-native permissions.\nAnalyze existing data systems and propose modernization or migration plans (on-prem to cloud, SQL to NoSQL, etc.).\nUse AWS RDS, Amazon Redshift, Azure SQL Database, or Google BigQuery as needed.\nStay up-to-date with emerging database technologies and make recommendations.\n\nMust-Have Skills:\nDeep knowledge of SQL and database performance tuning.\nHands-on experience with database migrations and replication strategies.\nFamiliarity with data governance, data quality, and compliance frameworks (GDPR, HIPAA, etc.).\nStrong problem-solving and troubleshooting skills.\nExperience with data streaming platforms such as Apache Kafka, AWS Kinesis, or Apache Flink is a plus.\nExperience with data lake and data warehouse architectures.\nExcellent communication and documentation skills.\n\nSoft Skills:\nProblem-Solving: Ability to analyze complex problems and develop effective solutions.\nCommunication Skills: Strong verbal and written communication skills to effectively collaborate with cross-functional teams.\nAnalytical Thinking: Ability to think critically and analytically to solve technical challenges.\nTime Management: Capable of managing multiple tasks and deadlines in a fast-paced environment.\nAdaptability: Ability to quickly learn and adapt to new technologies and methodologies.\n\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database Administration', 'Database Migration', 'Dba Skills', 'High Availability', 'Db Administration', 'Hadr', 'Database Security', 'Disaster Recovery', 'Dr Testing', 'DR', 'Luw', 'Db Upgrade', 'Brtools', 'UDDI', 'Os Migration', 'Dbase', 'DBMS', 'IBM DB2', 'Db Migration', 'Disaster Recovery Planning', 'Db Dba', 'Backup And Recovery']",2025-06-10 14:16:20
DataBricks - Data Engineering,Wipro,2 - 7 years,Not Disclosed,['Hyderabad'],"About The Role  \n\nRole Purpose\n\nThe purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\n\n ? \n\nDo\n\n1. Instrumental in understanding the requirements and design of the product/ software\nDevelop software solutions by studying information needs, studying systems flow, data usage and work processes\nInvestigating problem areas followed by the software development life cycle\nFacilitate root cause analysis of the system issues and problem statement\nIdentify ideas to improve system performance and impact availability\nAnalyze client requirements and convert requirements to feasible design\nCollaborate with functional teams or systems analysts who carry out the detailed investigation into software requirements\nConferring with project managers to obtain information on software capabilities\n\n\n ? \n\n2. Perform coding and ensure optimal software/ module development\nDetermine operational feasibility by evaluating analysis, problem definition, requirements, software development and proposed software\nDevelop and automate processes for software validation by setting up and designing test cases/scenarios/usage cases, and executing these cases\nModifying software to fix errors, adapt it to new hardware, improve its performance, or upgrade interfaces.\nAnalyzing information to recommend and plan the installation of new systems or modifications of an existing system\nEnsuring that code is error free or has no bugs and test failure\nPreparing reports on programming project specifications, activities and status\nEnsure all the codes are raised as per the norm defined for project / program / account with clear description and replication patterns\nCompile timely, comprehensive and accurate documentation and reports as requested\nCoordinating with the team on daily project status and progress and documenting it\nProviding feedback on usability and serviceability, trace the result to quality risk and report it to concerned stakeholders\n\n\n ? \n\n3. Status Reporting and Customer Focus on an ongoing basis with respect to project and its execution\nCapturing all the requirements and clarifications from the client for better quality work\nTaking feedback on the regular basis to ensure smooth and on time delivery\nParticipating in continuing education and training to remain current on best practices, learn new programming languages, and better assist other team members.\nConsulting with engineering staff to evaluate software-hardware interfaces and develop specifications and performance requirements\nDocument and demonstrate solutions by developing documentation, flowcharts, layouts, diagrams, charts, code comments and clear code\nDocumenting very necessary details and reports in a formal way for proper understanding of software from client proposal to implementation\nEnsure good quality of interaction with customer w.r.t. e-mail content, fault report tracking, voice calls, business etiquette etc\nTimely Response to customer requests and no instances of complaints either internally or externally\n\n\n ? \n\nDeliver\n\n\nNo.\n\nPerformance Parameter\n\nMeasure 1. Continuous Integration, Deployment & Monitoring of Software 100% error free on boarding & implementation, throughput %, Adherence to the schedule/ release plan 2. Quality & CSAT On-Time Delivery, Manage software, Troubleshoot queries, Customer experience, completion of assigned certifications for skill upgradation 3. MIS & Reporting 100% on time MIS & report generation\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['software development life cycle', 'continuous integration', 'software development', 'mis', 'root cause analysis', 'data bricks', 'analyzing information', 'software validation', 'software management', 'data engineering', 'digital transformation', 'customer experience']",2025-06-10 14:16:22
Data Engineer-Business Intelligence,IBM,5 - 10 years,Not Disclosed,['Hyderabad'],"Provide expertise in analysis, requirements gathering, design, coordination, customization, testing and support of reports, in client’s environment\nDevelop and maintain a strong working relationship with business and technical members of the team\nRelentless focus on quality and continuous improvement\nPerform root cause analysis of reports issues\nDevelopment / evolutionary maintenance of the environment, performance, capability and availability.\nAssisting in defining technical requirements and developing solutions\nEffective content and source-code management, troubleshooting and debugging\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n5+ years of experience with BI tools, with expertise and/or certification in at least one major BI platform – Tableau preferred.\nAdvanced knowledge of SQL, including the ability to write complex stored procedures, views, and functions.\nProven capability in data storytelling and visualization, delivering actionable insights through compelling presentations.\nExcellent communication skills, with the ability to convey complex analytical findings to non-technical stakeholders in a clear, concise, and meaningful way.\n5.Identifying and analyzing industry trends, geographic variations, competitor strategies, and emerging customer behavior\n\n\nPreferred technical and professional experience\nTroubleshooting capabilities to debug Data controls Capable of converting business requirements into workable model.\nGood communication skills, willingness to learn new technologies, Team Player, Self-Motivated, Positive Attitude.\nMust have thorough understanding of SQL & advance SQL (Joining & Relationships)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['advance sql', 'sql', 'bi tools', 'debugging', 'troubleshooting', 'python', 'data analysis', 'data analytics', 'bi', 'data warehousing', 'power bi', 'business analysis', 'machine learning', 'business intelligence', 'sql server', 'qlikview', 'tableau', 'r', 'data visualization', 'etl', 'ssis']",2025-06-10 14:16:24
"Team Manager (L4), Ring Data Engineering Services",Amazon,1 - 5 years,Not Disclosed,['Hyderabad'],"Amazon is an E-commerce and Cloud Computing company with headquarters in Seattle, Washington. Since 1995, Amazon has focused on being the world s most customer centric company.\n\n\nAs Team Manager, you will be responsible for :\nManaging a team of ML Data Associates.\nExecutes plans for the team to handle multiple ML queues within a defined process area.\nIdentifies risks and ensures proper escalation; maintains confidentiality and compliance.\nResponsible for meeting SLAs and task completion targets for specific queues within capacity limits.\nCollaborates with internal and external teams to achieve business goals.\nAnalyzes data to highlight trends and gaps; reports key metrics.\nPresents data during business meetings and reviews.\nDesigns and implements process improvement projects affecting team performance; participates in new workflow rollout projects.\nProvides guidance on work types and prioritization; manages straightforward objectives as well as ad hoc requests.\nCreates and implements performance improvement plans for associates; offers regular coaching and feedback on quality, performance, behavior, and career development.\nManages team performance based on metrics and deliverables.\nHolds team members accountable for performance, adherence to rules, and guidelines.\nSupport hiring and training of new Associates\n\nA day in the life\nAs Team Manager, you will be responsible to :\nManage monitor performance on job or queue adherence, volume, and quality\nSupport hiring and training of new Associates\nEnsure productivity is maximized through supervision, training, analysis, and feedback of performance data on a periodic basis\nTrack quality and utilization metrics\nProvide regular, formal informal feedback to direct reports\nIdentify and help implement process-related improvement using methodologies\nCommunicate effectively\n\nAbout the team\nArtificial General Intelligence Data Services (AGI DS) mission is to provide high-quality labelled data at high-speed and low-cost for machine learning (ML) technologies. Bachelor Degree (Any Stream) or advanced college education or experience in a Leadership or related position with management.\nProficiency in verbal and written communication skills\nExperience in understanding performance metrics and developing them to measure progress against key performance indicators\nOverall 4+ yrs of work experience out of which, min 1+ yrs of people management experience\nMust have driven process improvements in the current role Experience with process improvement/quality control tools and methods\nDemonstrated ability to lead diverse talent within a team, work cross-functionally, and build consensus on difficult issues\nExcellent communication, strong organizational skills and very detail-oriented\nStrong interest in hiring and developing people in their respective roles\nLeadership experience in coaching and performance management\nExperience in managing process and operational escalations\nExperience with aspects of speech and language technology",,,,"['Engineering services', 'Career development', 'Cloud computing', 'Performance management', 'Process improvement', 'Machine learning', 'Training analysis', 'Workflow', 'Operations', 'Performance improvement']",2025-06-10 14:16:26
Amazing Hiring For GCP Data Engineer,HTC Global Services,4 - 9 years,5-15 Lacs P.A.,['Chennai( Chennai Central RS )'],"Hello Everyone,\n\nGreetings from HTC!\nPosition Description:\nWe're seeking a Data Engineer to lead our India-based supplier delivery team in migrating eight Teradata databases to the Google Cloud Platform (GCP). Oversee the entire migration process, ensuring successful data ingestion, quality assurance, and data protection using standard engineering patterns. Responsibilities: Lead the lift and refactor efforts for full data migration. Drive business adoption of the new GCP platform. Oversee the decommissioning of Teradata and related technologies. Collaborate with technical leads, program managers, and other stakeholders to execute the migration plan within an Agile framework. Qualifications: Hands-on experience with GCP services, data pipelines, BigQuery, SQL, and Python. Proven ability to manage technical and process-related requests to maintain project timelines. Strong collaboration and communication skills",,,,"['Bigquery', 'Gcp Cloud', 'Cloud Sql', 'Data Flow', 'Dataproc', 'Python']",2025-06-10 14:16:29
Data Engineer (Azure),Reqroots,7 - 12 years,Not Disclosed,['Coimbatore'],"Data Engineer (Azure) Jobs | 7+ years | Coimbatore, Tamil Nadu(Full-time)\nJob Description\nPurpose:\nIT applications (Implementation and support): The employee should be able to develop ETL pipelines, data visualizations and analysis.\nMinimum Qualifications and Knowledge:\nBachelor s degree in computer science or information technologies.\nMinimum Experience:\n8+ years in Data Engineering (design, model and building ETL pipelines, data/lake house design and development)\n4+ years in Python and Spark (strong knowledge in PySpark, Pandas, Numpy)\n4+ year in Azure Data Factory and Databricks (strong Performance Optimization skills)\n8+ years in SQL\nJob-Specific Skills:\nSelf-motivated and committed to quality. Interpersonal and analytical skills.\nProblem solving skills.\nExcellent organization skills.\nRequired for 6-months, with the possibility of renewal based on project requirements\nFinance industry working experience is preferred (Insurance background is 01st preference).\nCandidates currently working in any region is fine ( Abu Dhabi/India)\nApplication Development:\nData modelling, development and deployment of end-to-end data analytics solutions in cloud environment - Azure.\nData processing including big data processing, integration, data modelling in line with the concepts of lake house medallion architecture.\nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\nCollaborate with vendor to create pipelines for Open Insurance project for four Line of Business Motor, Home, Medical & Travel\nCreate robust data models, data catalogue & data mesh architecture for selfservice analytics, facilitating user-friendly data exploration and reporting.\nData Governance - Ensure data quality, security, discoverability\nApplication Support:\nMaintain and execute performance tuning and data quality fixes.\nRequired Knowledge, Skills, and Abilities\nhttps://www.tebs.com\nWhom we are looking for\np Purpose: br IT applications (Implementation and support): The employee should be able to develop ETL pipelines, data visualizations and analysis. br /p p Minimum Qualifications and Knowledge: br Bachelor s degree in computer science or information technologies. br Minimum Experience: br 8+ years in Data Engineering (design, model and building ETL pipelines, data/lake house design and development) br 4+ years in Python and Spark (strong knowledge in PySpark, Pandas, Numpy) br 4+ year in Azure Data Factory and Databricks (strong Performance Optimization skills) br 8+ years in SQL br Job-Specific Skills: br Self-motivated and committed to quality. Interpersonal and analytical skills. br Problem solving skills. br Excellent organization skills. br Required for 6-months, with the possibility of renewal based on project requirements br Finance industry working experience is preferred (Insurance background is 01st preference). br Candidates currently working in any region is fine ( Abu Dhabi/India) br /P p Application Development: br Data modelling, development and deployment of end-to-end data analytics solutions in cloud environment - Azure. br Data processing including big data processing, integration, data modelling in line with the concepts of lake house medallion architecture. br Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. br Collaborate with vendor to create pipelines for Open Insurance project for four Line of Business Motor, Home, Medical & Travel br Create robust data models, data catalogue & data mesh architecture for selfservice analytics, facilitating user-friendly data exploration and reporting. br Data Governance - Ensure data quality, security, discoverability br /p p Application Support: br Maintain and execute performance tuning and data quality fixes. br /p",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Performance tuning', 'Root cause analysis', 'Application support', 'data governance', 'Application development', 'Data quality', 'Engineering Design', 'SQL', 'Python']",2025-06-10 14:16:31
Consultant Data Engineering - Connected Medicine,Eli Lilly And Company,7 - 10 years,Not Disclosed,['Bengaluru'],"The role will be responsible for setting up the data warehouses necessary to handle large volumes of data, create meaningful analyses, and deliver recommendations to leadership.\nCore Responsibilities\nCreate and maintain optimal data pipeline architecture ETL/ ELT into structured data\nAssemble large, complex data sets that meet business requirements and create and maintain multi-dimensional modelling like Star Schema and Snowflake Schema, normalization, de-normalization, joining of datasets.\nExpert level experience in creating a scalable data warehouse including Fact tables, Dimensional tables and ingest datasets into cloud based tools.\nIdentify, design, and implement internal process improvements including automating manual processes, optimizing data delivery and re-designing infrastructure for greater scalability.\nCollaborate with stakeholders to ensure seamless integration of data with internal data marts, enhancing advanced reporting\nSetup and maintain data ingestion, streaming, scheduling, and job monitoring automation using AWS services. Setup Lambda, code pipeline (CI/CD), Glue, S3, Redshift, Power BI needs to be maintained for uninterrupted automation.\nBuild analytics tools that utilize the data pipeline to provide actionable insight into customer acquisition, operational efficiency, and other key business performance metrics.\nWork with stakeholders to assist with data-related technical issues and support their data infrastructure needs.\nUtilize GitHub for version control, code collaboration, and repository management. Implement best practices for code reviews, branching strategies, and continuous integration.\nCreate data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader\nEnsure data privacy and compliance with relevant regulations (eg, GDPR) when handling customer data.\nMaintain data quality and consistency within the application, addressing data-related issues as they arise.\nRequired\n7-10 years of relevant experience\nAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as we'll as working familiarity with a variety of databases and Cloud Data warehouse like AWS Redshift\nExperience in creating scalable, efficient schema designs to support diverse business needs.\nExperience with database normalization, schema evolution, and maintaining data integrity\nProactively share best practices, contributing to team knowledge and improving schema design transitions.\nDevelop data models, create dimensions and facts, and establish views and procedures to enable automation programmability.\nCollaborate effectively with cross-functional teams to gather requirements, incorporate feedback, and align analytical work with business objectives\nPrior Data Modelling, OLAP cube modelling\nData compression into PARQUET to improve processing and finetuning SQL programming skills.\nExperience building and optimizing big data data pipelines, architectures and data sets.\nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\nExperience with manipulating, processing and extracting value from large disconnected unrelated datasets\nStrong analytic skills related to working with structured and unstructured datasets.\nWorking knowledge of message queuing, stream processing, and highly scalable big data stores.\nExperience supporting and working with cross-functional teams and Global IT.\nFamiliarity of working in an agile based working models.\nPreferred Qualifications/Expertise\nExperience with relational SQL and NoSQL databases, especially AWS Redshift.\nExperience with AWS cloud services Preferable : S3, EC2, Lambda, Glue, EMR, Code pipeline highly preferred. Experience with similar services on another platform would also be considered.\nEducation:\nbachelors or masters degree on Technology and Computer Science background",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'Process improvement', 'Analytical', 'Healthcare', 'Scheduling', 'Data quality', 'Operations', 'Analytics', 'Monitoring', 'SQL']",2025-06-10 14:16:34
MDM Associate Data Engineer,Horizon Therapeutics,1 - 8 years,Not Disclosed,['Hyderabad'],"Career Category Information Systems Job Description\nABOUT AMGEN\nAmgen harnesses the best of biology and technology to fight the world s toughest diseases, and make people s lives easier, fuller and longer. We discover, develop, manufacture and deliver innovative medicines to help millions of patients. Amgen helped establish the biotechnology industry more than 40 years ago and remains on the cutting-edge of innovation, using technology and human genetic data to push beyond what s known today.\nABOUT THE ROLE\nRole Description:\nWe are seeking an MDM Associate Data Engineer with 2 -5 years of experience to support and enhance our enterprise MDM (Master Data Management) platforms using Informatica/Reltio. This role is critical in delivering high-quality master data solutions across the organization, utilizing modern tools like Databricks and AWS to drive insights and ensure data reliability. The ideal candidate will have strong SQL, data profiling, and experience working with cross-functional teams in a pharma environment. To succeed in this role, the candidate must have strong data engineering experience along with MDM knowledge, hence the candidates having only MDM experience are not eligible for this role. Candidate must have data engineering experience on technologies like (SQL, Python, PySpark , Databricks, AWS etc ), along with knowledge of MDM (Master Data Management)\nRoles Responsibilities:\nAnalyze and manage customer master data using Reltio or Informatica MDM solutions.\nPerform advanced SQL queries and data analysis to validate and ensure master data integrity.\nLeverage Python, PySpark , and Databricks for scalable data processing and automation.\nCollaborate with business and data engineering teams for continuous improvement in MDM solutions.\nImplement data stewardship processes and workflows, including approval and DCR mechanisms.\nUtilize AWS cloud services for data storage and compute processes related to MDM.\nContribute to metadata and data modeling activities.\nTrack and manage data issues using tools such as JIRA and document processes in Confluence.\nApply Life Sciences/Pharma industry context to ensure data standards and compliance.\nBasic Qualifications and Experience:\nMaster s degree with 1 - 3 years of experience in Business, Engineering, IT or related field OR\nBachelor s degree with 2 - 5 years of experience in Business, Engineering, IT or related field OR\nDiploma with 6 - 8 years of experience in Business, Engineering, IT or related field\nFunctional Skills:\nMust-Have Skills:\nAdvanced SQL expertise and data wrangling.\nStrong experience in Python and PySpark for data transformation workflows.\nStrong experience with Databricks and AWS architecture.\nMust have knowledge of MDM, data governance, stewardship, and profiling practices.\nIn addition to above, candidates having experience with Informatica or Reltio MDM platforms will be preferred.\nGood-to-Have Skills:\nExperience with IDQ, data modeling and approval workflow/DCR.\nBackground in Life Sciences/Pharma industries.\nFamiliarity with project tools like JIRA and Confluence.\nStrong grip on data engineering concepts.\nProfessional Certifications :\nAny ETL certification ( e. g. Informatica)\nAny Data Analysis certification (SQL, Python, Databricks)\nAny cloud certification (AWS or AZURE)\nSoft Skills:\nStrong analytical abilities to assess and improve master data processes and solutions.\nExcellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders.\nEffective problem-solving skills to address data-related issues and implement scalable solutions.\nAbility to work effectively with global, virtual teams\nEQUAL OPPORTUNITY STATEMENT\nAmgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.\nWe will ensure that individuals with disabilities are provided with reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.\n.",Industry Type: Biotechnology,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'metadata', 'Automation', 'Data modeling', 'Master data management', 'Life sciences', 'Informatica', 'AWS', 'SQL', 'Python']",2025-06-10 14:16:36
Data Engineer - GCP,Egen Solutions,4 - 6 years,Not Disclosed,['Hyderabad'],"Job Overview:\n\nWe are looking for a skilled and motivated Data Engineer with strong experience in Python programming and Google Cloud Platform (GCP) to join our data engineering team. The ideal candidate will be responsible for designing, developing, and maintaining robust and scalable ETL (Extract, Transform, Load) data pipelines. The role involves working with various GCP services, implementing data ingestion and transformation logic, and ensuring data quality and consistency across systems.\nKey Responsibilities:\nDesign, develop, test, and maintain scalable ETL data pipelines using Python.\nWork extensively on Google Cloud Platform (GCP) services such as:\nDataflow for real-time and batch data processing\nCloud Functions for lightweight serverless compute\nBigQuery for data warehousing and analytics\nCloud Composer for orchestration of data workflows (based on Apache Airflow)\nGoogle Cloud Storage (GCS) for managing data at scale\nIAM for access control and security\nCloud Run for containerized applications\nPerform data ingestion from various sources and apply transformation and cleansing logic to ensure high-quality data delivery.\nImplement and enforce data quality checks, validation rules, and monitoring.\nCollaborate with data scientists, analysts, and other engineering teams to understand data needs and deliver efficient data solutions.\nManage version control using GitHub and participate in CI/CD pipeline deployments for data projects.\nWrite complex SQL queries for data extraction and validation from relational databases such as SQL Server, Oracle, or PostgreSQL.\nDocument pipeline designs, data flow diagrams, and operational support procedures.\nRequired Skills:\n4-6 years of hands-on experience in Python for backend or data engineering projects.\nStrong understanding and working experience with GCP cloud services (especially Dataflow, BigQuery, Cloud Functions, Cloud Composer, etc.).\nSolid understanding of data pipeline architecture, data integration, and transformation techniques.\nExperience in working with version control systems like GitHub and knowledge of CI/CD practices.\nStrong experience in SQL with at least one enterprise database (SQL Server, Oracle, PostgreSQL, etc.).\n\nGood to Have (Optional Skills):\nExperience working with Snowflake cloud data platform.\nHands-on knowledge of Databricks for big data processing and analytics.\nFamiliarity with Azure Data Factory (ADF) and other Azure data engineering tools.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Backend', 'Version control', 'Postgresql', 'Data quality', 'Oracle', 'Apache', 'Analytics', 'Monitoring', 'Python', 'Data extraction']",2025-06-10 14:16:38
Senior DBT Engineer,DXC Technology,4 - 12 years,Not Disclosed,['Bengaluru'],"Job Description:\nSenior DBT Engineer\nJob Location : Hyderabad / Bangalore / Chennai / Kolkata / Noida/ Gurgaon / Pune / Indore / Mumbai\nResponsibilities:\n- Experience Level - 4 -12 years.\n- Design, develop, and maintain DBT models, transformations, and SQL code to build efficient data pipelines for analytics and reporting.\n- Design, develop, and maintain ETL/ELT pipelines using DBT and pulling data from Snowflake.\n- Define and implement data modelling best practices, including data warehousing, ETL processes, and data transformations using DBT.\n- Build complex SQL queries within DBT to build incremental models, enhancing data processing efficiency.\n- Establish data governance practices and ensure data accuracy, quality, and consistency within the data transformation process.\n- Collaborate with data engineers, data analysts, and other stakeholders to understand and meet data requirements for various business units.\n- Identify and address performance bottlenecks in data transformation processes and optimize DBT models for faster query performance.\n- Maintain thorough documentation of DBT models, transformations, and data dictionaries to ensure transparency and accessibility to team members.\n- Implement data security measures to protect sensitive information and comply with data privacy regulations.\n- Stay updated on industry best practices and new features in DBT, and continuously improve the data transformation processes.\n- Provide training and support to other team members in using DBT effectively.\n- Implement data quality checks and validation processes to ensure data accuracy and consistency.\n- Hands-on experience in implementing data governance, data quality rules and validation mechanisms within Collibra is added plus.\n- Knowledge of workflow orchestration tools like Tidal.\n- Experience with Python or other scripting languages is a plus.\n- Familiarity with Azure cloud platforms.\n- Exposure to DevOps practices and CI/CD pipelines for data engineering.\nRecruitment fraud is a scheme in which fictitious job opportunities are offered to job seekers typically through online services, such as false websites, or through unsolicited emails claiming to be from the company. These emails may request recipients to provide personal information or to make payments as part of their illegitimate recruiting process. DXC does not make offers of employment via social media networks and DXC never asks for any money or payments from applicants at any point in the recruitment process, nor ask a job seeker to purchase IT or other equipment on our behalf. More information on employment scams is available here .",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL queries', 'data security', 'Social media', 'data governance', 'Data processing', 'Workflow', 'Data quality', 'data privacy', 'Analytics', 'Python']",2025-06-10 14:16:41
Data Engineer-Enterprise Content Management,IBM,2 - 5 years,Not Disclosed,['Hyderabad'],"Design and Development of ECM Solutions\nFileNet API Integration\nConfigure and manage Events, Subscriptions, and Triggers in the Content Engine to support business process automation.\nDefine and enforce security policies, including Access Control Lists (ACLs), role-based access control (RBAC), and document-level security configurations\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nFileNet Developer with IBM FileNet P8 platform.\nThe ideal candidate will be responsible for designing, developing, implementing, and supporting enterprise content management solutions using FileNet, including customization of IBM Content Navigator (ICN), working with FileNet APIs, managing Records Manager configurations, and executing large-scale content migrations.\nRequired Skills and ExperienceExperience in IBM FileNet P8 platform (5.2/5.5 or higher). Strong hands-on experience with FileNet Java APIs (CE/PE APIs)\nUnderstanding, configuration and management of Event, Triggers in Content Engine to automate business logic. Implement and maintain FileNet security, including ACLs, role-based access control (RBAC), and document-level security\nGood communication skills and ability to work independently or as part of a team\n\n\nPreferred technical and professional experience\nIBM FileNet certification (e.g., IBM Certified Specialist - FileNet Content Manager).\nExperience with workflow design using FileNet BPM/Case Manager.\nExperience integrating FileNet with other enterprise systems via REST/SOAP APIs",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ibm filenet', 'java', 'rest', 'filenet', 'soap', 'data management', 'web services', 'jsp', 'hibernate', 'ecm', 'ibm content manager', 'sql', 'spring', 'j2ee', 'html', 'api', 'ce', 'oracle', 'datacap', 'javascript', 'sql server', 'servlets', 'enterprise content management', 'websphere application server', 'dojo']",2025-06-10 14:16:44
Application Lead,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Application Lead\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\nMust have skills :PySpark\n\n\nGood to have skills :NAMinimum\n\n5 year(s) of experience is required\n\n\nEducational Qualification :15 years full time education\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. Your typical day will involve collaborating with various teams to ensure that application development aligns with business objectives, overseeing project timelines, and facilitating communication among stakeholders to drive project success. You will also engage in problem-solving activities, providing guidance and support to your team while ensuring that best practices are followed throughout the development process. Your role will be pivotal in shaping the direction of application projects and ensuring that they meet the needs of the organization and its clients.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Facilitate training and development opportunities for team members to enhance their skills.- Monitor project progress and implement necessary adjustments to ensure timely delivery.\nProfessional & Technical\n\n\nSkills:\n- Must To Have\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\nSkills:\nExperience with cloud computing platforms such as AWS or Azure.- Strong understanding of data engineering principles and practices.- Experience in application development using modern programming languages.- Familiarity with Agile methodologies and project management tools.\nAdditional Information:- The candidate should have minimum 5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['pyspark', 'data analytics', 'microsoft azure', 'data engineering', 'aws', 'hive', 'scala', 'data warehousing', 'azure data factory', 'sql', 'plsql', 'java', 'spark', 'ssrs', 'hadoop', 'big data', 'etl', 'python', 'oracle', 'sql server', 'application development', 'data bricks', 'agile', 'sqoop', 'ssis', 'unix']",2025-06-10 14:16:47
AI Architect,Zensar,10 - 20 years,35-50 Lacs P.A.,"['Hyderabad', 'Pune', 'Bengaluru']","Experience: 10+ years overall | Minimum 10 full-cycle AI/ML project implementations, including GenAI experience\n\nRole Summary:\nWe are seeking a AI Architect to lead strategic AI transformation initiatives within the retail domain. This role demands deep hands-on experience in AI, Machine Learning (ML), and Generative AI (GenAI), along with the ability to engage directly with C-level stakeholders, align technical delivery with business objectives, and drive enterprise-wide adoption of advanced AI solutions.The ideal candidate is a techno-strategic leader who can take AI/ML/GenAI projects from ideation to productionbuilding architectures, leading cross-functional teams, and ensuring regulatory and operational alignment in retail environments.Key Responsibilities:Consulting & Business Alignment\nPartner with senior business and IT leadership, including CIOs, CDOs, and COOs, to identify high-impact use cases across e-commerce, retail operations, supply chain, and customer experience.\nTranslate complex retail challenges into technically feasible and scalable AI/ML/GenAI solutions.\nCreate strategic roadmaps, capability assessments, and PoV/PoC execution plans that align with business KPIs and regulatory needs.\nSolution Architecture & Delivery Leadership\nDesign and lead delivery of AI/ML/GenAI pipelines covering data ingestion, model training, validation, deployment, and monitoring.\nBuild and scale GenAI-based solutions like LLM-driven chatbots, intelligent product recommendation engines, RAG pipelines, summarization tools, and virtual assistants.\nArchitect cloud-native AI platforms using AWS (SageMaker, Bedrock), Azure (ML, OpenAI), or GCP (Vertex AI, BigQuery, LangChain).\nDefine and implement MLOps and LLMOps frameworks for versioning, retraining, CI/CD, and production observability.\nEnsure adherence to Responsible AI principles, including explainability, bias mitigation, auditability, and compliance with retail regulations (GDPR, CCPA, SOC2, etc.).\n\nEngineering & Integration\nWork closely with data engineering teams to acquire, transform, and pipeline data from retail systems, customer databases, inventory management systems, and real-time feeds.\nDesign architecture for data lakes, feature stores, and vector databases supporting AI and GenAI use cases.\nEnable seamless integration of AI capabilities into enterprise workflows, customer platforms, and decision engines via APIs and microservices.\n\nRequired Skills & Experience:\n10+ years of experience in AI/ML, data engineering, and cloud architecture.\nMinimum of 5 end-to-end AI/ML project implementations from use case discovery through to productionization.\nProven expertise in: (Any One)\nAI/ML frameworks: scikit-learn, XGBoost, TensorFlow, PyTorch\nGenAI/LLM platforms: OpenAI, Cohere, Mistral, LangChain, Hugging Face, vector DBs (Pinecone, FAISS, Chroma)\nCloud platforms: AWS, Azure, GCP including AI/ML & GenAI native services\nMLOps/LLMOps tools: MLflow, Kubeflow, SageMaker Pipelines, Vertex AI Pipelines\nStrong experience with data security, governance, model risk management, and AI compliance frameworks relevant to retail.\nAbility to lead large cross-functional teams and engage both technical teams and senior stakeholders.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Gen AI', 'Generative Ai', 'Aiml', 'LLM']",2025-06-10 14:16:50
Azure Data Engineer - Remote,Software Company,4 - 8 years,8-13 Lacs P.A.,[],"Azure Cloud Technologies, Azure Data Factory, Azure Databricks (Advance Knowledge), PySpark, CI/CD Pipeline (Jenkins, GitLab CVCD or Azure DevOps), Data Ingestion, SOL\ndesigning, developing, & optimizing scalable data solutions.\n\nRequired Candidate profile\nAzure Databricks, Azure Data Factory expertise, PySpark proficiency, Big Data CI/CD, Troubleshoot, Jenkins, Gitlab CI/ CD, Data Pipeline Development & Deployment",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['Azure Develops', 'Azure Data Engineer', 'Azure Data Factory', 'Jenkins', 'Azure Cloud Technologies', 'PySpark', 'Azure Databricks', 'GitLab CI/CD', 'Data Injection, SOL', 'GitLab CVCD']",2025-06-10 14:16:52
Hiring For Azure Data Engineer with MNC client-FTE-Hyd/Bangalore/Pune,The It Mind Services,4 - 6 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","JOB DESCRIPTION:\n\n• Strong experience in Azure Datafactory,Databricks, Eventhub, Python,PySpark ,Azure Synapse and SQL\n• Azure Devops experience to deploy the ADF pipelines.\n• Knowledge/Experience with Azure cloud stack.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Azure Synapse', 'Eventhub', 'Azure Datafactory', 'Databricks', 'Python', 'PySpark', 'Azure Devops']",2025-06-10 14:16:54
Cloud Data Engineer,Vesz Consultancy Services,5 - 10 years,9-15.6 Lacs P.A.,['Chennai'],"SQL, Python, Spark\nAWS Glue, Lambda, Step Functions, Azure Data Factory / Data bricks\nData validation, transformation, and quality assurance.\nBuilding and maintaining automated data pipelines, for data integrity\nWorking with large datasets\nGood Comm.",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Cloud', 'Spark', 'AWS', 'Python', 'Jenkins', 'Github', 'Postgresql']",2025-06-10 14:16:56
Data Governance Engineer,TELUS Digital,5 - 8 years,8-16 Lacs P.A.,"['Gandhinagar', 'Ahmedabad']","About TELUS Digital\nTELUS Digital (NYSE and TSX: TIXT) designs, builds, and delivers next-generation digital\nsolutions to enhance the customer experience (CX) for global and disruptive brands. The\ncompanys services support the full lifecycle of its clients digital transformation journeys and\nenable them to more quickly embrace next-generation digital technologies to deliver better\nbusiness outcomes. TELUS Digitals integrated solutions and capabilities span digital strategy,\ninnovation, consulting and design, digital transformation and IT lifecycle solutions, data\nannotation, and intelligent automation, and omnichannel CX solutions that include content",,,,"['Mdm Informatica', 'collibra', 'Data Lineage', 'Informatica', 'Data Governance', 'Metadata Management', 'Data Catalog', 'Data Maintenance', 'Governance', 'Data Governance Analyst', 'sql', 'MDM', 'Data Warehousing', 'Data Analytics', 'Python']",2025-06-10 14:16:58
Snowflake Cloud Data Engineer-Tech / POD Lead - Pan India,Pan India Opening-HYBRID,7 - 12 years,27.5-35 Lacs P.A.,"['Hyderabad', 'Bengaluru', 'india']","Job Title: Data & Analytics Cloud Platform Manager AWS & Snowflake\nJob Summary:\nWe are seeking an experienced Data & Analytics Cloud Platform Manager to oversee and optimize a Snowflake-based data platform on AWS. The ideal candidate will have deep expertise in managing operations, automation, support governance, SLI optimization, and change management to ensure reliability and performance. This role requires strong leadership skills to drive efficiency, automation, and operational excellence in a cloud-native data environment.\nKey Responsibilities:\nCloud Platform Management: Oversee and optimize the AWS-based data & analytics infrastructure, ensuring performance, security, and scalability.\nIncident Management: Lead incident resolution using structured problem management practices, ensuring minimal business impact.\nSupport Governance: Establish robust support frameworks to drive consistency in monitoring, issue resolution, and operations.\nSLI/SLO Management: Define, track, and optimize Service Level Indicators (SLIs) & Service Level Objectives (SLOs) for platform reliability.\nEnd-to-End Automation: Implement automation for incident response, monitoring, alerting, and self-healing systems to streamline support operations.\nChange Management: Manage platform updates, deployments, and data governance changes with minimal disruption.\nSecurity & Compliance: Ensure data security, access controls, and regulatory compliance across AWS and Snowflake environments.\nCollaboration & Leadership: Work closely with engineering, analytics, DevOps, and business teams to drive efficient platform operations.\nRequired Skills & Qualifications:\nExperience managing cloud-based data platforms, especially AWS & Snowflake.\nStrong expertise in Incident Management, Support Governance, and SLI/SLO optimization.\nHands-on experience in automating data operations and support processes.\nDeep understanding of AWS cloud services, Snowflake architecture, and data pipeline optimizations.\nSolid knowledge of ITIL frameworks, DevOps, and cloud-native operations.\nProficiency in SQL, Python, Spark, and data orchestration tools.\nExperience with monitoring tools like AWS CloudWatch, Snowflake Query Performance Dashboard, and Application Insights.\nExcellent problem-solving, analytical, and communication skills.\nPreferred Qualifications:\nExposure to CI/CD pipelines for data applications.\nExperience with Infrastructure-as-Code (IaC) tools such as Terraform or CloudFormation.\nKnowledge of machine learning integration for optimizing platform reliability.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Cortex', 'AWS', 'Spark', 'SQL', 'Python']",2025-06-10 14:17:00
SCALA DEVELOPER,Capgemini,3 - 8 years,Not Disclosed,['Chennai'],"\n\nYour Profile \n\nAs a senior software engineer with Capgemini, you will have 3 + years of experience in Scala with strong project track record\nHands On experience in Scala/Spark developer\nHands on SQL writing skills on RDBMS (DB2) databases\nExperience in working with different file formats like JSON, Parquet, AVRO, ORC and XML.\nMust have worked in a HDFS platform development project.\nProficiency in data analysis, data profiling, and data lineage\nStrong oral and written communication skills\nExperience working in Agile projects.\n\n\n \n\nYour Role \nWork on Hadoop, Spark, Hive &SQL query\nAbility to perform code optimization for performance, Scalability and configurability\nData application development at scale in the Hadoop ecosystem.\n\n\n \n\nWhat youll love about working here \nChoosingCapgeminimeans having the opportunity to make a difference, whetherfor the worlds leading businesses or for society. It means getting the support youneed to shape your career in the way that works for you. It means when the futuredoesnt look as bright as youd like, youhave the opportunity tomake changetorewrite it.\nWhen you join Capgemini, you dont just start a new job. You become part of something bigger. A diverse collective of free-thinkers, entrepreneurs and experts, all working together to unleash human energy through technology, for an inclusive and sustainable future. At Capgemini, people are at the heart of everything we do! You can exponentially grow your career by being part of innovative projects and taking advantage of our extensiveLearning & Developmentprograms. With us, you will experience aninclusive, safe, healthy, andflexiblework environment to bring out the best\nin you! You also get a chance to make positive social change and build a better world by taking an active role in ourCorporate Social ResponsibilityandSustainabilityinitiatives. And whilst you make a difference, you will also have a lot offun.\n\n\n \n\nAbout Company",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['platform development', 'scala', 'spark', 'data profiling', 'data analysis', 'hive', 'pyspark', 'dbms', 'apache pig', 'sql', 'parquet', 'microservices', 'docker', 'spring', 'java', 'xml', 'flume', 'json', 'hadoop', 'big data', 'hbase', 'avro', 'orc', 'python', 'rdbms', 'oozie', 'spring boot', 'mapreduce', 'kafka', 'agile', 'sqoop', 'aws']",2025-06-10 14:17:03
Data Engineer 3,Vichara Technologies,7 - 12 years,30-40 Lacs P.A.,"['Pune', 'Bengaluru', 'Delhi / NCR']","Support enhancements to the MDM platform\nDevelop pipelines using snowflake python SQL and airflow\nTrack System Performance\nTroubleshoot issues\nResolve production issues\n\nRequired Candidate profile\n5+ years of hands on expert level Snowflake, Python, orchestration tools like Airflow\nGood understanding of investment domain\nExperience with dbt, Cloud experience (AWS, Azure)\nDevOps",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Airflow', 'SQL Server', 'Snowflake', 'dbt', 'Python', 'Pyspark', 'Markit Edm', 'Data Engineering', 'Github', 'SQL Development', 'Fast Api', 'Semarchy', 'Aws Glue', 'GIT', 'Orchestration Framework', 'MDM', 'Etl Development', 'Data Modeling', 'ETL', 'React.Js', 'flask', 'Database Development', 'SAP MDM']",2025-06-10 14:17:06
Aws Data Engineer,Innova Solutions,4 - 7 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities\nLooking For AWS data Eng-Immediate joiners for Hyderabad,Chennai,Noida,Pune,Bangalore locations.\nMandatory Skill-Python,Pyspark,SQL,Aws Glue\n\nStrong technical skills in services like S3,Athena, Lambda, RGlue and Glue(Pyspark), SQL, Data Warehousing, Informatica, OracleDesign, develop, and implement custom solutions within the Collibra platform to support data governance initiatives.\n\nPreferred candidate profile\nSnowflake, Agile methodology and Tableau. Proficiency in Python/Scala, Spark architecture, complex SQL, and RDBMS. Hands-on experience with ETL tools (e.g., Informatica) and SCD1, SCD2. 2-6 years of DWH, AWS Services and ETL design knowledge.\n\nDevelop ETL processes for data ingestion, transformation, and loading into data lakes and warehouses.\nCollaborate with data scientists and analysts to ensure data availability for analytics and reporting.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Glue', 'ETL', 'Aws Cloud']",2025-06-10 14:17:09
Hiring- Azure Cloud Automation and Data Analytics Engineer PAN India,"NTT DATA, Inc.",5 - 8 years,Not Disclosed,"['Hyderabad', 'Gurugram', 'Bengaluru']","Overview:\n\nWe are seeking a dynamic and experienced Azure Cloud Automation and Data Analytics Engineer with a proactive attitude and strong independence. This role requires expertise in scripting and automating processes using IAC / scription, along with proficiency in languages like PowerShell, Python, and Bash. The ideal candidate will also have a background in Data Analytics and SQL. We are looking for a candidate who excels in automation and development, rather than merely operational tasks.\n\nList of Key Responsibilities:\n- Design, implement, and manage cloud infrastructure on Azure using Terraform and scripting languages.\n- Automate deployment, configuration, and management of cloud resources.\n- Develop and maintain data analytics solutions, including data pipelines and ETL processes.\n- Write, optimize, and manage SQL queries and databases and be aware of the results\n- Ensure security and compliance of cloud environments.\n- Collaborate on designing and implementing networking solutions within Azure.\n- Conduct performance tuning, troubleshooting, and root cause analysis.\n- Implement and manage monitoring, logging, and alerting systems.\n- Being able to interact with other teams in the company to GTD from other teams we depend by tracking of the tickets\nBut at the same time independent to understand what is required.\n- Participate in on-call rotations and provide support for cloud operations.\n\nQualifications:\n- Proven experience in cloud infrastructure management, specifically with Microsoft Azure with scripting approach more than just click-ops\n- Expertise in scripting and automation using Terraform, PowerShell, Python, and Bash.\n- Background in Data Analytics, including proficiency in SQL.\n- Knowledge of security best practices in a cloud environment.\n- Familiarity with Azure networking concepts and services.\n- Experience with DevOps practices and tools, including CI/CD pipelines and version control.\n\nMust Have Skills: Azure Storage / Azure Services / Azure Permissions, Azure Databricks / Spark , Azure SQL Server / Databases /SQL , Docker and Containers / Azure Container Registry ,Azure Devops Pipeline, Terraform, Cloud Platform.\n\nIf interested in the position, please apply to procced further-",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Analytics', 'Terraform', 'Azure Cloud Automation', 'Ci/Cd', 'Azure Devops', 'Azure Cloud Security', 'Docker', 'Aks Kubernetes', 'Python']",2025-06-10 14:17:11
Tech. PM - Data Engineering-Data Analytics@ Gurgaon/Blore_Urgent,A global leader in delivering innovative...,5 - 10 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Job Title - Technical Project Manager\n\nLocation - Gurgaon/ Bangalore\n\nNature of Job - Permanent\n\nDepartment - data analytics\n\nWhat you will be doing\n\n\nDemonstrated client servicing and business analytics skills with at least 5 - 9 years of experience as data engineer, BI developer, data analyst, technical project manager, program manager etc.\nTechnical project management- drive BRD, project scope, resource allocation, team\ncoordination, stakeholder communication, UAT, Prod fix, change requests, project governance\nSound knowledge of banking industry (payments, retail operations, fraud etc.)\nStrong ETL experience or experienced Teradata developer\nManaging team of business analysts, BI developers, ETL developers to ensure that projects are completed on time\nResponsible for providing thought leadership and technical advice on business issues\nDesign methodological frameworks and solutions.\n\n\nWhat were looking for\n\n\nBachelors/masters degree in computer science/data science/AI/statistics, Certification in Gen AI. Masters degree Preferred.\nManage multiple projects, at a time, from inception to delivery\nSuperior problem-solving, analytical, and quantitative skills\nEntrepreneurial mindset, coupled with a “can do” attitude\nDemonstrated ability to collaborate with cross-functional, cross-border teams and coach / mentor colleagues.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Technical Project Manager', 'Data Engineering', 'multiple projects', 'Technical project management', 'Data Analytics', 'project scope', 'ETL Pipeline', 'team coordination', 'resource allocation', 'Prod fix', 'drive BRD', 'program manager', 'Big data']",2025-06-10 14:17:13
AWS Data Engineer,Leading Client,1 - 3 years,Not Disclosed,['Chennai'],"Mandatory Skills:\nAWS, Python, SQL, spark, Airflow, SnowflakeResponsibilities\nCreate and manage cloud resources in AWS\nData ingestion from different data sources which exposes data using different technologies, such asRDBMS, REST HTTP API, flat files, Streams, and Time series data based on various proprietary systems. Implement data ingestion and processing with the help of Big Data technologies\nData processing/transformation using various technologies such as Spark and Cloud Services. You will need to understand your part of business logic and implement it using the language supported by the base data platform\nDevelop automated data quality check to make sure right data enters the platform and verifying the results of the calculations\nDevelop an infrastructure to collect, transform, combine and publish/distribute customer data.\nDefine process improvement opportunities to optimize data collection, insights and displays.\nEnsure data and results are accessible, scalable, efficient, accurate, complete and flexible\nIdentify and interpret trends and patterns from complex data sets\nConstruct a framework utilizing data visualization tools and techniques to present consolidated analytical and actionable results to relevant stakeholders.\nKey participant in regular Scrum ceremonies with the agile teams\nProficient at developing queries, writing reports and presenting findings\nMentor junior members and bring best industry practices",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AWS', 'hive', 'scala', 'amazon redshift', 'big data technologies', 'pyspark', 'apache pig', 'microservices', 'sql', 'java', 'spark', 'data ingestion', 'api', 'hadoop', 'big data', 'etl', 'hbase', 'rest', 'snowflake', 'cloud services', 'python', 'data processing', 'airflow', 'kafka', 'http', 'scrum', 'agile', 'sqoop']",2025-06-10 14:17:16
Lead Data Engineer (IDMC-CDI),Mchoovers Consulting,7 - 12 years,Not Disclosed,[],"Job Title: Lead Data Engineer (IDMC-CDI)\nPrimary Skills: SQL, IDMC (CDI), ETL Concepts\nBase Location: Gurugram (Delhi/NCR)\nMode of Work: Work from Home\nExperience: 8+ Years\n\nAbout the Job\nAre you someone with a deep understanding of ETL and a strong background in developing IDMC ETL-based solutions, capable of developing, documenting, unit testing, and maintaining ETL applications while delivering code that meets customer expectations? If yes, this opportunity could be your next career step.\n\nKey Responsibilities\nLead the design, development, and implementation of complex data integration solutions using Informatica Intelligent Data Management Cloud (IDMC).\nDevelop, document, unit test, and maintain high-quality ETL applications that meet customer expectations and adhere to best practices.\nProvide technical leadership and guidance to junior team members through code reviews, mentoring, and knowledge sharing.\nCollaborate closely with project managers, data analysts, and business stakeholders to understand requirements, define technical solutions, and ensure successful project delivery.\nParticipate in all phases of the software development lifecycle, including requirements gathering, design, development, testing, deployment, and maintenance.\nEnsure data quality and integrity throughout the data pipeline by implementing appropriate data validation and cleansing mechanisms.\nTroubleshoot and resolve complex data integration issues, ensuring timely resolution and minimal disruption to business operations.\nStay abreast of the latest advancements in data integration technologies and best practices.\nContribute to the continuous improvement of our data integration processes and methodologies.\n\nRequired Skills\n8+ years of experience in IT with a focus on Data Management.\n1+ year of experience as a Lead handling both technical and non-technical activities.\n4+ years of experience with Informatica products.\n2+ years of hands-on experience with Informatica IDMC.\nMust have implemented at least one full lifecycle IDMC projects.\nHands-on experience with Informatica CDI.\nMust have led large projects involving large teams.\nExperience integrating external business applications with Informatica IDMC using batch processes, API calls, and message queues.\nIn-depth knowledge of at least one relational databases (e.g., Oracle, SQL Server, MySQL).\nExperience with data warehousing concepts and methodologies (e.g., Kimball, Inmon).\nExcellent analytical and problem-solving skills, with the ability to identify and resolve complex data integration challenges.\nStrong communication and interpersonal skills, capable of effectively collaborating with cross-functional teams.\nExperience with Agile methodologies in a fast-paced, iterative environment.\n\nDesired Skills\nExperience with cloud platforms (e.g., AWS, Azure, GCP) and cloud-based data integration tools.\nKnowledge of data quality tools and techniques.\nExperience with scripting languages (e.g., Python, Shell).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['idmc', 'ETL Tool', 'SQL', 'Cdi']",2025-06-10 14:17:19
"Google Cloud Platform Data Engineer -GCP,BigQuery,SQL, Cloud Function",Tredence,5 - 10 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","Role - GCP Data Engineer\nExperience:4+ years\nPreferred - Data Engineering Background\nLocation - Bangalore, Chennai, Pune, Gurgaon, Kolkata\nRequired Skills - GCP DE Experience, Big query, SQL, Cloud compressor/Python, Cloud functions, Dataproc+pyspark, Python injection, Dataflow+PUB/SUB\n\nHere is the job description for the same -\nJob Requirement:\nHave Implemented and Architected solutions on Google Cloud Platform using the components of GCP\nExperience with Apache Beam/Google Dataflow/Apache Spark in creating end to end data pipelines.\nExperience in some of the following: Python, Hadoop, Spark, SQL, Big Query, Big Table Cloud Storage, Datastore, Spanner, Cloud SQL, Machine Learning.\nExperience programming in Java, Python, etc.\nExpertise in at least two of these technologies: Relational Databases, Analytical Databases, NoSQL databases.\nCertified in Google Professional Data Engineer/ Solution Architect is a major Advantage",,,,"['Pubsub', 'GCP', 'Bigquery', 'Google Cloud Platforms', 'SQL', 'Data Flow', 'Dataproc']",2025-06-10 14:17:22
Urgent Hiring For Data Engineer with Product based MNC Pune,Peoplefy,7 - 12 years,Not Disclosed,['Pune'],Greetings from Peoplefy Infosolutions !!!\n\nWe are hiring for one of our reputed MNC client based in Pune.\nWe are looking for candidates with 7 + years of experience in below skills -\n\nPrimary skills :\nUnderstanding of AI ML in DE\nPython\nData Engineers\nDatabase -Big query or Snowflake\n\n\nInterested candidates for above position kindly share your CVs on chitralekha.so@peoplefy.com with below details -\n\nExperience :\nCTC :\nExpected CTC :\nNotice Period :\nLocation :,Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Aiml', 'ETL', 'Python', 'Bigquery', 'AWS']",2025-06-10 14:17:25
Senior Data Engineer,Reuters,6 - 11 years,Not Disclosed,"['Mumbai', 'Hyderabad']","Develop and maintain data solutions using resources such as dbt, Alteryx, and Python.\nDesign and optimize data pipelines, ensuring efficient data flow and processing.\nWork extensively with databases, SQL, and various data formats including JSON, XML, and CSV.\nTune and optimize queries to enhance performance and reliability.\nDevelop high-quality code in SQL, dbt, and Python, adhering to best practices.\nUnderstand and implement data automation and API integrations.\nLeverage AI capabilities to enhance data engineering practices.\nUnderstand integration points related to upstream and downstream requirements.\nProactively manage tasks and work towards completion against tight deadlines.\nAnalyze existing processes and offer suggestions for improvement.\nAbout You\nStrong interest and knowledge in data engineering principles and methods.\n6+ years of experience developing data solutions or pipelines.\n6+ years of hands-on experience with databases and SQL.\n2+ years of experience programming in an additional language.\n2+ years of experience in query tuning and optimization.\nExperience working with SQL, JSON, XML, and CSV content.\nUnderstanding of data automation and API integration.\nFamiliarity with AI capabilities and their application in data engineering.\nAbility to adhere to best practices for developing programmatic solutions.\nStrong problem-solving skills and ability to work independently.\nWhat s in it For You\nHybrid Work Model: we've adopted a flexible hybrid working environment (2-3 days a week in the office depending on the role) for our office-based roles while delivering a seamless experience that is digitally and physically connected.\nFlexibility Work-Life Balance: Flex My Way is a set of supportive workplace policies designed to help manage personal and professional responsibilities, whether caring for family, giving back to the community, or finding time to refresh and reset. This builds upon our flexible work arrangements, including work from anywhere for up to 8 weeks per year, empowering employees to achieve a better work-life balance.\nCareer Development and Growth: By fostering a culture of continuous learning and skill development, we prepare our talent to tackle tomorrow s challenges and deliver real-world solutions. Our Grow My Way programming and skills-first approach ensures you have the tools and knowledge to grow, lead, and thrive in an AI-enabled future.\nIndustry Competitive Benefits: We offer comprehensive benefit plans to include flexible vacation, two company-wide Mental Health Days off, access to the Headspace app, retirement savings, tuition reimbursement, employee incentive programs, and resources for mental, physical, and financial we'llbeing.\nCulture: Globally recognized, award-winning reputation for inclusion and belonging, flexibility, work-life balance, and more. We live by our values: Obsess over our Customers, Compete to Win, Challenge (Y)our Thinking, Act Fast / Learn Fast, and Stronger Together.\nSocial Impact: Make an impact in your community with our Social Impact Institute. We offer employees two paid volunteer days off annually and opportunities to get involved with pro-bono consulting projects and Environmental, Social, and Governance (ESG) initiatives.\nMaking a Real-World Impact: We are one of the few companies globally that helps its customers pursue justice, truth, and transparency. Together, with the professionals and institutions we serve, we help uphold the rule of law, turn the wheels of commerce, catch bad actors, report the facts, and provide trusted, unbiased information to people all over the world.",Industry Type: Internet,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Career development', 'Automation', 'XML', 'Consulting', 'Flex', 'JSON', 'Taxation', 'Downstream', 'SQL', 'Python']",2025-06-10 14:17:26
Senior Data Science Engineer – Computer Vision,Uplers,6 - 11 years,Not Disclosed,['Bengaluru'],"Looking for a skilled Senior Data Science Engineer with 6-12 years of experience to lead the development of advanced computer vision models and systems.\nThe ideal candidate will have hands-on experience with state-of-the-art architectures and a deep understanding of the complete ML lifecycle. This position is based in Bengaluru.\nRoles and Responsibility\nLead the development and implementation of computer vision models for tasks such as object detection, tracking, image retrieval, and scene understanding.\nDesign and execute end-to-end pipelines for data preparation, model training, evaluation, and deployment.\nPerform fine-tuning and transfer learning on large-scale vision-language models to meet application-specific needs.\nOptimize deep learning models for edge inference (NVIDIA Jetson, TensorRT, OpenVINO) and real-time performance.\nDevelop scalable and maintainable ML pipelines using tools such as MLflow, DVC, and Kubeflow.\nAutomate experimentation and deployment processes using CI/CD workflows.\nCollaborate cross-functionally with MLOps, backend, and product teams to align technical efforts with business needs.\nMonitor, debug, and enhance model performance in production environments.\nStay up-to-date with the latest trends in CV/AI research and rapidly prototype new ideas for real-world use.\nJob Requirements\n6-7+ years of hands-on experience in data science and machine learning, with at least 4 years focused on computer vision.\nStrong experience with deep learning frameworks: PyTorch (preferred), TensorFlow, Hugging Face Transformers.\nIn-depth understanding and practical experience with Class-incremental learning and lifelong learning systems.\nProficient in Python, including data processing libraries like NumPy, Pandas, and OpenCV.\nStrong command of version control and reproducibility tools (e.g., MLflow, DVC, Weights & Biases).\nExperience with training and optimizing models for GPU inference and edge deployment (Jetson, Coral, etc.).\nFamiliarity with ONNX, TensorRT, and model quantization/conversion techniques.\nDemonstrated ability to analyze and work with large-scale visual datasets in real-time or near-real-time systems.\nExperience working in fast-paced startup environments with ownership of production AI systems.\nExposure to cloud platforms such as AWS (SageMaker, Lambda), GCP, or Azure for ML workflows.\nExperience with video analytics, real-time inference, and event-based vision systems.\nFamiliarity with monitoring tools for ML systems (e.g., Prometheus, Grafana, Sentry).\nPrior work in domains such as retail analytics, healthcare, or surveillance/IoT-based CV applications.\nContributions to open-source computer vision libraries or publications in top AI/ML conferences (e.g., CVPR, NeurIPS, ICCV).\nComfortable mentoring junior engineers and collaborating with cross-functional stakeholders.",Industry Type: Banking,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'SageMaker', 'Azure', 'MLOps', 'PyTorch', 'GCP', 'Pandas', 'CI/CD', 'Computer Vision', 'AWS', 'Lambda', 'NumPy']",2025-06-10 14:17:29
Data Engineer with Machine Learning Specialization,Ortseam Technologies,5 - 10 years,Not Disclosed,[],"Job Requirement for Offshore Data Engineer (with ML expertise)\nWork Mode: Remote\nBase Location: Bengaluru\nExperience: 5+ Years\n\nTechnical Skills & Expertise:\n\nPySpark & Apache Spark:\nExtensive experience with PySpark and Spark for big data processing and transformation.\nStrong understanding of Spark architecture, optimization techniques, and performance tuning.\nAbility to work with Spark jobs in distributed computing environments like Databricks.\nData Mining & Transformation:\nHands-on experience in designing and implementing data mining workflows.\nExpertise in data transformation processes, including ETL (Extract, Transform, Load) pipelines.\nExperience in large-scale data ingestion, aggregation, and cleaning.\nProgramming Languages:\nPython & Scala: Proficient in Python for data engineering tasks, including using libraries like Pandas and NumPy. Scala proficiency is preferred for Spark job development.\nBig Data Concepts: In-depth knowledge of big data frameworks and paradigms, such as distributed file systems, parallel computing, and data partitioning.\nBig Data Technologies:\nCassandra & Hadoop: Experience with NoSQL databases like Cassandra and distributed storage systems like Hadoop.\nData Warehousing Tools: Proficiency with Hive for data warehousing solutions and querying.\nETL Tools: Experience with Beam architecture and other ETL tools for large-scale data workflows.\nCloud Technologies (GCP):\nExpertise in Google Cloud Platform (GCP), including core services like Cloud Storage, BigQuery, and DataFlow.\nExperience with DataFlow jobs for batch and stream processing.\nFamiliarity with managing workflows using Airflow for task scheduling and orchestration in GCP.\nMachine Learning & AI:\nGenAI Experience: Familiarity with Generative AI and its applications in ML pipelines.\nML Model Development: Knowledge of basic ML model building using tools like Pandas, NumPy, and visualization with Matplotlib.\nML Ops Pipeline: Experience in managing end-to-end ML Ops pipelines for deploying models in production, particularly LLM (Large Language Models) deployments.\nRAG Architecture: Understanding and experience in building pipelines using Retrieval-Augmented Generation (RAG) architecture to enhance model performance and output.\n\nTech stack : Spark, Pyspark, Python, Scala, GCP data flow, Data composer (Air flow), ETL, Databricks, Hadoop, Hive, GenAI, ML Modeling basic knowledge, ML Ops experience , LLM deployment, RAG",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Machine Learning', 'Gcp Cloud', 'Python', 'Airflow', 'Hadoop', 'Data Bricks', 'Hive', 'SCALA', 'Data Flow', 'Spark', 'ETL']",2025-06-10 14:17:31
Senior Data engineer,Apexon,7 - 12 years,Not Disclosed,['Hyderabad'],"We enable #HumanFirstDigital\n\nJob Summary:\nWe are seeking a highly skilled Lead/Senior Data Engineer to provide L2 and L3 production support for enterprise-scale data platforms. The ideal candidate should have strong experience in Informatica PowerCenter, Informatica PowerExchange, Yellowbrick (or any RDBMS), Unix Shell Scripting, and Tivoli Workload Scheduler. You will be responsible for ensuring seamless, reliable, and efficient data processing operations.\nKey Responsibilities:\nProvide L2/L3 production support for complex ETL/data integration workflows using Informatica PowerCenter and PowerExchange.\nMonitor and troubleshoot ETL failures, data quality issues, and performance problems in batch and real-time pipelines.\nSupport and maintain data pipelines involving Yellowbrick/ Netezza or other RDBMS platforms.\nDevelop and enhance Unix Shell Scripts to automate operational and support tasks.\nManage job scheduling, dependencies, and batch cycles using Tivoli Workload Scheduler (TWS).\nPerform incident triage, root cause analysis, and implement long-term fixes.\nCollaborate with development, infrastructure, and business teams to resolve issues and drive stability.\nMaintain and update operational documentation, SOPs, and knowledge base articles.\nParticipate in on-call rotation and adhere to SLA and escalation procedures.\nRequired Skills and Experience:\n7 to 12 years of experience in Data Engineering/ETL with a strong focus on L2/L3 support.\nExpertise in Informatica PowerCenter and PowerExchange (including support for mainframe, flat files, and RDBMS sources/targets).\nStrong SQL and database experience with Yellowbrick or any other RDBMS (e.g., Netezza, Oracle, PostgreSQL, SQL Server).\nProficient in Unix/Linux Shell Scripting.\nHands-on experience with Tivoli Workload Scheduler (TWS) or equivalent job scheduling tools.\nStrong problem-solving skills for production issue analysis and resolution.\nUnderstanding of data warehousing concepts, job orchestration, and data quality principles.\nExperience with ticketing and incident management tools like ServiceNow or JIRA.\nExcellent communication, documentation, and coordination skills.\nOur Commitment to Diversity & Inclusion:\nOur Perks and Benefits:\nOur benefits and rewards program has been thoughtfully designed to recognize your skills and contributions, elevate your learning/upskilling experience and provide care and support for you and your loved ones. As an Apexon Associate, you get continuous skill-based development, opportunities for career advancement, and access to comprehensive health and well-being benefits and assistance.\nWe also offer:\no Group Health Insurance covering family of 4\no Term Insurance and Accident Insurance\no Paid Holidays & Earned Leaves\no Paid Parental LeaveoLearning & Career Development\no Employee Wellness\nJob Location : Hyderabad, India",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Linux', 'Production support', 'RDBMS', 'Shell scripting', 'Wellness', 'Informatica', 'Oracle', 'Unix shell scripting', 'SQL']",2025-06-10 14:17:33
Manager/Sr. Manager Data Engineering,Cognitio Analytics,8 - 12 years,Not Disclosed,['Gurugram'],"Clinical Coder(Contract)\nGurugram (Hybrid)\n7 To 9 years\n+ Job Description\nApply\nKnowledge, and Expectations:\nAdvanced knowledge of medical coding and billing systems, groupers, crosswalks, and classification systems including proficiency in regulatory requirements.\nPossess thorough knowledge of anatomical and medical terminology, demonstrating a natural curiosity and analytical mindset.\nAbility to create and maintain crosswalks matching up/ recommend the equivalent codes based on coding guidelines.\nResearch and bring in international and regional medical coding schemas/ classifications, crosswalks, risk adjustment tools, reference lists/ value sets and drug/ medical device directories to the database, enhancing company s medical coding assets and highlighting the standards licensing requirements (wherever applied).\nAnalyse and interpret claims line level descriptions, and other documentation, and convert them into codable format to the best of clinical and coding knowledge.\nReview and verify codes for diagnoses, procedures and treatment, and observations for coding inaccuracies and deficiencies as part of codes quality checks.\nServe as resource and subject matter expert to other coding staff.",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Claims', 'Coding', 'Medical coding', 'Analytical', 'Billing', 'Database', 'Subject Matter Expert', 'Research', 'Licensing']",2025-06-10 14:17:35
Automation Engineer,Capgemini,3 - 6 years,Not Disclosed,['Bengaluru'],"\n\nDesign, develop, and implement MLOps pipelines for the continuous deployment and integration of machine learning models.\n\nCollaborate with data scientists and engineers to understand model requirements and optimize deployment processes.\n\nTake offline models data scientists build and turn them into a real machine learning production system.\n\nAutomate the training, testing and deployment processes for machine learning models.\n\nContinuously monitor and maintain models in production, ensuring optimal performance, accuracy and reliability.\n\nImplement best practices for version control, model reproducibility and governance.\n\nOptimize machine learning pipelines for scalability, efficiency and cost-effectiveness.\n\nTroubleshoot and resolve issues related to model deployment and performance.\n\nEnsure compliance with security and data privacy standards in all MLOps activities.\n\nKeep up-to-date with the latest MLOps tools, technologies and trends.\n\nProvide support and guidance to other team members on MLOps practices.\n\nCommunicate with a team of data scientists, data engineers and architect, document the processe\n\nExperience in designing and implementing pipelines MLOps on AWS, Azure, or GCP.\n\nHands on building CI/CD pipelines orchestration using TeamCity, Jenkins, Airflow or similar tools.\n\nExperience with MLOps Frameworks like Kubeflow, MLFlow, DataRobot, Airflow etc., experience with Docker and Kubernetes, OpenShift.\n\nProgramming languages like Python, Go, Ruby or Bash, good understanding of Linux, knowledge of frameworks such as scikit-learn, Keras, PyTorch, Tensorflow, etc.\n\nAbility to understand tools used by data scientist and experience with software development and test automation.\n\nFluent in English, good communication skills and ability to work in a team.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'kubernetes', 'python', 'ci/cd', 'aws', 'software testing', 'scikit-learn', 'automation testing', 'openshift', 'airflow', 'golang', 'microsoft azure', 'docker', 'ruby', 'tensorflow', 'gcp', 'automation engineering', 'linux', 'jenkins', 'pytorch', 'keras', 'bash', 'teamcity']",2025-06-10 14:17:38
PYSPARK DEVELOPER,Capgemini,2 - 7 years,Not Disclosed,['Pune'],"\n\nYour Role \n\nPyspark Data Engineer\nAs a Pyspark developer you Must have 2+ years in Pyspark.\nStrong programming experience, Python, Pyspark, Scala is preferred.\nExperience indesigning and implementing CI/CD, Build Management, and Development strategy.\nExperience with SQL and SQL Analytical functions, experience participating in key business, architectural and technical decisions\nScope to get trained on AWS cloud technology\n\n\n \n\nYour Profile \nPyspark\nSQL\nData Engineer\n\n\n \n\nWhat you will love about working here \n\nChoosing Capgemini means having the opportunity to make a difference, whether for the worlds leading businesses or for society. It means getting the support you need to shape your career in the way that works for you. It means when the future doesnt look as bright as youd like, you have the opportunity to make changeto rewrite it.\nWhen you join Capgemini, you dont just start a new job. You become part of something bigger. A diverse collective of free-thinkers, entrepreneurs and experts, all working together to unleash human energy through technology, for an inclusive and sustainable future. At Capgemini, people are at the heart of everything we do! You can exponentially grow your career by being part of innovative projects and taking advantage of our extensive Learning & Development programs. With us, you will experience an inclusive, safe, healthy, and flexible work environment to bring out the best\nin you! You also get a chance to make positive social change and build a better world by taking an active role in our Corporate Social Responsibility and Sustainability initiatives. And whilst you make a difference, you will also have a lot of fun.\n\n \n\nAbout Capgemini",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'scala', 'pyspark', 'ci/cd', 'python', 'hive', 'kubernetes', 'sql', 'docker', 'ansible', 'java', 'git', 'spark', 'devops', 'linux', 'jenkins', 'shell scripting', 'mysql', 'hadoop', 'big data', 'jira', 'github', 'maven', 'microsoft azure', 'sqoop', 'aws']",2025-06-10 14:17:41
Technology Architect,Infosys,8 - 11 years,Not Disclosed,['Bengaluru'],"Responsibilities\nJob Responsibilities:\nArchitect and Design scalable and efficient AI solutions, leveraging technologies such as LangChain, Agentic AI, RAG, Event driven architecture using Kafka etc.\nCollaborate with cross-functional teams to identify business needs and develop tailored solutions\nProvide technical leadership and guidance to junior team members\nStay up-to-date with the latest advancements in AI, machine learning, and data science, and apply this knowledge to improve our solutions\nCommunicate complex technical concepts to non-technical stakeholders and team members\nTroubleshoot and resolve technical issues, and provide support to ensure high system uptime and performance\nDevelop and maintain technical documentation, and ensure that all solutions are well-documented and easily maintainable\nTechnical and Professional Requirements:\nPreferred Qualifications:\nExperience with Agentic Frameworks such LangGraph, AutoGen, CrewAI\nExperience with cloud-based technologies, such as AWS or Azure\nFamiliarity with containerization using Docker, and orchestration using Kubernetes\nFamiliarity with agile development methodologies, such as Scrum or Kanban\nExperience with AI-related tools and frameworks, such as TensorFlow or PyTorch\nKnowledge of data engineering, data warehousing, and data governance\nExperience with agile development methodologies, such as Scrum or Kanban\nCertification in data science, machine learning, or a related field\nExperience with leadership and mentoring, with a proven track record of guiding junior team members and helping them grow in their careers\nStrong business acumen, with the ability to understand business needs and develop solutions that drive business growth and improvement.\nPreferred Skills:\nTechnology->Artificial Intelligence->Artificial Intelligence - ALL\nTechnology->Machine Learning->Generative AI\nTechnology->Machine Learning->AI/ML Solution Architecture and Design->generative ai\nTechnology->Machine Learning->Python\nAdditional Responsibilities:\nRequired Qualifications:\nB.E/B.Tech/M.E/M.Tech/MCA degree in Computer Science, Information Technology, or a related field\nAt least 8 years of experience in software development, with at least 2 years of experience in Generative AI\nProficiency in LangChain, Python, Generative AI, Agentic AI, Kafka, and Advanced Prompt Engineering Techniques\nStrong understanding of software architecture, design patterns, and principles\nExcellent problem-solving skills, with the ability to analyze complex technical problems and develop creative solutions\nStrong communication and teamwork skills, with the ability to collaborate with cross-functional teams and communicate technical concepts to non-technical stakeholders\nTech Skill: LangChain, Python, Fast/Flask API, Gen AI, Agentic AI, Advanced Prompt Engineering, Machine Learning, SQL, Kafka\nSoft Skill: Communication, Team Work, Problem Solving\nEducational Requirements\nBachelor of Engineering\nService Line\nInformation Systems\nLocation of posting is subject to business requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Technology Architecture', 'LangGraph', 'AutoGen', 'CrewAI', 'PyTorch', 'Docker', 'Artificial Intelligence', 'data warehousing', 'data engineering', 'Machine Learning', 'TensorFlow', 'Python']",2025-06-10 14:17:43
Azure Developer,Infosys,2 - 7 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","Job Title\nAzure Developer\n\n Responsibilities\nA day in the life of an Infoscion\nAs part of the Infosys delivery team, your primary role would be to ensure effective Design, Development, Validation and Support activities, to assure that our clients are satisfied with the high levels of service in the technology domain.\nYou will gather the requirements and specifications to understand the client requirements in a detailed manner and translate the same into system requirements.\nYou will play a key role in the overall estimation of work requirements to provide the right information on project estimations to Technology Leads and Project Managers.\nYou would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\n\n Technical and Professional Requirements:\nPrimary skills:Technology->Cloud Platform->Azure Development & Solution Architecting\n Preferred\n\nSkills:\n\n\nTechnology->Cloud Platform->Azure Development & Solution Architecting\n\n Additional Responsibilities:\nKnowledge of design principles and fundamentals of architecture\nUnderstanding of performance engineering\nKnowledge of quality processes and estimation techniques\nBasic understanding of project domain\nAbility to translate functional / nonfunctional requirements to systems requirements\nAbility to design and code complex programs\nAbility to write test cases and scenarios based on the specifications\nGood understanding of SDLC and agile methodologies\nAwareness of latest technologies and trends\nLogical thinking and problem solving skills along with an ability to collaborate\n\n Educational Requirements\nMCA,MSc,MTech,Bachelor of Engineering,BCA,BSc,BTech\n\n Service Line\nLocation- PAN INDIA",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['aws', 'cloud platform', 'azure devops', 'microsoft azure']",2025-06-10 14:17:46
GEN AI Engineer,HCLTech,3 - 8 years,Not Disclosed,"['Noida', 'Chennai', 'Bengaluru']",Skill Needed\nJob:\nDefine the Agentic Function for Industrial Quality Inspection\nWorkflow design\nBring up the LLM and AI baseline framework -open source (Llava),,,,"['GEN AI', 'RAG', 'LLM', 'Open source', 'ML', 'Tensorflow']",2025-06-10 14:17:49
Application Developer-Google Cloud FullStack,IBM,3 - 6 years,Not Disclosed,['Gurugram'],"As a Software Developer you'll participate in many aspects of the software development lifecycle, such as design, code implementation, testing, and support. You will create software that enables your clients' hybrid-cloud and AI journeys.\n\nYour primary responsibilities include:\nComprehensive Feature Development and Issue ResolutionWorking on the end to end feature development and solving challenges faced in the implementation.\nStakeholder Collaboration and Issue ResolutionCollaborate with key stakeholders, internal and external, to understand the problems, issues with the product and features and solve the issues as per SLAs defined.\nContinuous Learning and Technology IntegrationBeing eager to learn new technologies and implementing the same in feature development\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nSQL authoring, query, and cost optimisation primarily on Big Query. Python as an object-oriented scripting language.\nData pipeline, data streaming and workflow management toolsDataflow, Pub Sub, Hadoop, spark-streaming\nVersion control systemGIT & Preferable knowledge of Infrastructure as CodeTerraform.\nAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.\nExperience performing root cause analysis on internal and external data and processes to answer specific business questions\n\n\nPreferred technical and professional experience\nExperience building and optimising data pipelines, architectures and data sets. Build processes supporting data transformation, data structures, metadata, dependency and workload management.\nWorking knowledge of message queuing, stream processing, and highly scalable data stores.\nExperience supporting and working with cross-functional teams in a dynamic environment. We are looking for a candidate with experience in a Data Engineer role, who are also familiar with Google Cloud Platform",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['spark', 'gcp', 'bigquery', 'hadoop', 'python', 'hive', 'kubernetes', 'metadata', 'sql', 'docker', 'ansible', 'java', 'git', 'devops', 'linux', 'jenkins', 'shell scripting', 'mysql', 'cloud computing', 'pubsub', 'software testing', 'vmware', 'microsoft azure', 'cloud platform', 'hybrid cloud', 'kafka', 'cost optimization', 'aws']",2025-06-10 14:17:51
Snowflake Data Engineer,Prudent Globaltech Solutions,7 - 12 years,16-27.5 Lacs P.A.,['Hyderabad( Madhapur )'],"Job Description Data Engineer\nWe are seeking a highly skilled Data Engineer with extensive experience in Snowflake, Data Build Tool (dbt), Snaplogic, SQL Server, PostgreSQL, Azure Data Factory, and other ETL tools. The ideal candidate will have a strong ability to optimize SQL queries and a good working knowledge of Python. A positive attitude and excellent teamwork skills are essential.\n\nRole & responsibilities\nData Pipeline Development: Design, develop, and maintain scalable data pipelines using Snowflake, DBT, Snaplogic, and ETL tools.",,,,"['Snowflake', 'Azure Data Factory', 'ADF', 'Data Engineer', 'ETL', 'SQL']",2025-06-10 14:17:53
Data Analytics Lead (Consulting),Capgemini,10 - 15 years,Not Disclosed,['Pune'],"Capgemini Invent \n\nCapgemini Invent is the digital innovation, consulting and transformation brand of the Capgemini Group, a global business line that combines market leading expertise in strategy, technology, data science and creative design, to help CxOs envision and build whats next for their businesses.\n\n Your Role \nUse Design thinking and a consultative approach to conceive cutting edge technology solutions for business problems, mining core Insights as a service model\nEngage with project activities across the Information lifecycle.\nUnderstanding client requirements, develop data analytics strategy and solution that meets client requirements\nApply knowledge and explain the benefits to organizations adopting strategies relating to NextGen/ New age Data Capabilities\nBe proficient in evaluating new technologies and identifying practical business cases to develop enhanced business value and increase operating efficiency\nArchitect large scale AI/ML products/systems impacting large scale clients across industry\nOwn end to end solutioning and delivery of data analytics/transformation programs\nMentor and inspire a team of data scientists and engineers solving AI/ML problems through R&D while pushing the state-of-the-art solution\nLiaise with colleagues and business leaders across Domestic & Global Regions to deliver impactful analytics projects and drive innovation at scale\nAssist sales team in reviewing RFPs, Tender documents, and customer requirements\nDeveloping high-quality and impactful demonstrations, proof of concept pitches, solution documents, presentations, and other pre-sales assets\nHave in-depth business knowledge across a breath of functional areas across sectors such as CPRD/FS/MALS/Utilities/TMT\n\n\n Your Profile \nB.E. / B.Tech. + MBA (Systems / Data / Data Science/ Analytics / Finance) with a good academic background\nMinimum 10 years + on Job experience in data analytics with at least 7 years ofCPRD, FS, MALS, Utilities, TMT or other relevant domain experience required\nSpecialization in data science, data engineering or advance analytics filed is strongly recommended\nExcellent understanding and hand-on experience of data-science and machine learning techniques & algorithms for supervised & unsupervised problems, NLP and computer vision\nGood, applied statistics skills, such as distributions, statistical inference & testing, etc.\nExcellent understanding and hand-on experience on building Deep-learning models for text & image analytics (such as ANNs, CNNs, LSTM, Transfer Learning, Encoder and decoder, etc).\nProficient in coding in common data science language & tools such as R, Python, Go, SAS, Matlab etc.\nAt least 7 years experience deploying digital and data science solutions on large scale project is required\nAt least 7 years experience leading / managing a data Science team is required\n\nExposure or knowledge in cloud (AWS/GCP/Azure) and big data technologies such as Hadoop, Hive\n\n What you will love about working here \nWe recognize the significance of flexible work arrangements to provide support. Be it remote work, or flexible work hours, you will get an environment to maintain healthy work life balance.\nAt the heart of our mission is your career growth. Our array of career growth programs and diverse professions are crafted to support you in exploring a world of opportunities.\nEquip yourself with valuable certifications in the latest technologies such as Generative AI.\n\n\n About Capgemini \n\nCapgemini is a global business and technology transformation partner, helping organizations to accelerate their dual transition to a digital and sustainable world, while creating tangible impact for enterprises and society. It is a responsible and diverse group of 340,000 team members in more than 50 countries. With its strong over 55-year heritage, Capgemini is trusted by its clients to unlock the value of technology to address the entire breadth of their business needs. It delivers end-to-end services and solutions leveraging strengths from strategy and design to engineering, all fueled by its market leading capabilities in AI, cloud and data, combined with its deep industry expertise and partner ecosystem. The Group reported 2023 global revenues of 22.5 billion.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['fs', 'data science', 'tmt', 'python', 'data analytics', 'hive', 'pq', 'golang', 'neural networks', 'artificial intelligence', 'deep learning', 'gcp', 'hadoop', 'ml', 'matlab', 'cnn', 'oq', 'sas', 'natural language processing', 'iq', 'microsoft azure', 'machine learning', 'data engineering', 'r', 'aws', 'statistics']",2025-06-10 14:17:55
Analytics Data science and IOT Engineer,Hexaware Technologies,4 - 6 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","Job Description:\n\nMinimum 2 years of hands-on experience in Generative AI (GenAI), including working with various LLMs, prompt engineering, fine-tuning, Retrieval-Augmented Generation (RAG), and deploying GenAI solutions in production environments.\nAt least 3 years of experience in Python and SQL, with strong fundamentals in data manipulation and backend logic.",,,,"['Generative Ai', 'Python', 'Aws Sagemaker', 'Retrieval Augmented Generation', 'SQL']",2025-06-10 14:17:58
Azure Data Engineer,Tek Ninjas,4 - 9 years,Not Disclosed,[],"he JD for the Azure DE + Palantir Resource is as below:\nMandatory - Strong proficiency in python, SQL, pyspark, Azure databricks, Azure Data Factory, strong communication skills as part of the Azure Data Engineer Role\nAdded to the Data engineering experience, the must have for Palantir are:\nRequired (hands-on resource who can demonstrate this during the interview)\no             Data Integration: building pipelines, python transform & source types reference, data connection, codes & repos\no             Model Integration: models, business logic, templated analysis & reports\no             Ontology Manager: object types, functions, object views, actions, permissions\no             Application Building:  Workshop, writeback, advanced actions\no             Security: data foundation, data protection & governance, restricted access & restricted views\no             AIP",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Palantir', 'Azure Databricks', 'SQL', 'Python', 'Pyspark', 'Data Bricks']",2025-06-10 14:18:00
Azure Data Engineer,Meritus Management Service,4 - 7 years,10-20 Lacs P.A.,['Pune'],"Experience in designing, developing, implementing, and optimizing data solutions on Microsoft Azure. Proven expertise in leveraging Azure services for ETL processes, data warehousing and analytics, ensuring optimal performance and scalability.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Pipeline', 'Synapse Analytics', 'Azure Databricks', 'Spark', 'Python', 'Pyspark', 'Ci/Cd', 'azure']",2025-06-10 14:18:03
Software Engineer - Snowflake Data Engineer,Beyond Key,4 - 9 years,Not Disclosed,['Indore'],"About Beyond Key:\n\nWe are a Microsoft Gold Partner and a Great Place to Work-certified company. ""Happy Team Members, Happy Clients"" is a principle we hold dear. We are an international IT consulting and software services firm committed to providing. Cutting-edge services and products that satisfy our clients global needs. Our company was established in 2005, and since then weve expanded our team by including more than 350+ Talented skilled software professionals. Our clients come from the United States, Canada, Europe, Australia, the Middle East, and India, and we create and design IT solutions for them. If you need any more details, you can get them at https: / / www.beyondkey.com / about.\n\nJob Description:\n\nWe are seeking an experienced Snowflake Data Engineer with strong expertise in DBT, Azure Data Factory (ADF), and Azure DevOps CI/CD to design, develop, and optimize data solutions. The ideal candidate will have hands-on experience in writing Snowflake SQL & JavaScript procedures, building ADF pipelines, and implementing DBT with Jinja templating and macros.\n\nResponsibilities:\n\nDesign, develop, and optimize Snowflake data models, stored procedures (SQL & JavaScript), and workflows.\nImplement DBT (data build tool) transformations with expertise in Jinja templating and macro creation.\nBuild and manage Azure Data Factory (ADF) pipelines for ETL/ELT processes.\nSet up and maintain CI/CD pipelines in Azure DevOps for automated deployments.\nIntegrate Azure Logic Apps for workflow automation where applicable.\nTroubleshoot performance bottlenecks and optimize data processes.\nCollaborate with cross-functional teams to ensure seamless data delivery.\nMaintain documentation and adhere to best practices in data engineering.\n\nQualifications:\n\n4+ years of hands-on experience in Snowflake (modeling, scripting, optimization).\n3+ years of experience in DBT, with strong knowledge of Jinja & macros.\n2+ years in Azure Data Factory (ADF) - building and managing pipelines.\n2+ years in Azure DevOps CI/CD (YAML pipelines, deployments).\nProficiency in Snowflake SQL & JavaScript stored procedures.\nExperience with Azure LogicApps is a plus.\nStrong problem-solving skills and ability to debug complex data issues.\nExcellent communication skills for stakeholder collaboration.\n\nPreferred Qualifications:\n\nCertification in Snowflake, Azure Data Engineer, or DBT.\nExposure to data orchestration tools (e.g., Airflow, Databricks).\nKnowledge of data governance and security best practices.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'orchestration', 'Javascript', 'data governance', 'Workflow', 'Stored procedures', 'microsoft', 'Macros', 'Software services', 'SQL']",2025-06-10 14:18:04
"Senior Engineer, MS - Data Center Operations","NTT DATA, Inc.",2 - 5 years,Not Disclosed,['Mumbai'],"Additional Career Level Description:\nKnowledge and application:\nApplies learned techniques, as well as company policies and procedures to resolve a variety of issues.\nProblem solving:\nWorks on problems of moderate scope, often varied and nonroutine where analysis requires a review of a variety of factors.\nFocuses on providing standard professional advice and creating initial analysis for review.\nInteraction:\nBuilds productive internal/external working relationships to resolve mutual problems by collaborating on procedures or transactions.\nImpact:\nWork mainly impacts short term team performance and occasionally medium-term goals.\nSupports the achievement of goals through own personal effort, assessing own progress.\nAccountability:\nExercises some of own judgement and is responsible for meeting own targets, normally receiving little instruction on day-to-day work, general instructions on new assignments.\nManages own impact on cost and profitability.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Center Operations', 'system administration', 'VMware', 'technical support', 'active directory', 'windows administration', 'networking']",2025-06-10 14:18:06
Azure Data Engineer,Leading Client,3 - 6 years,Not Disclosed,['Chennai'],Azure Data Factory\nAzure Databricks\nAzure SQL database\nSynapse Analytics\nLogic App\nAzure Functions\nAzure Analysis Service\nActive Directory\nAzure Devops\nPython\nPyspark,Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'azure databricks', 'python', 'ssas', 'microsoft azure', 'pyspark', 'power bi', 'data warehousing', 'azure data factory', 'azure analysis services', 'azure devops', 'sql server', 'sql', 'azure functions', 'sql azure', 'active directory', 'azure analysis', 'ssrs', 'dax', 'ssis', 'etl', 'msbi', 'sql database']",2025-06-10 14:18:09
Data Engineer - Azure,Blend360 India,5 - 9 years,Not Disclosed,['Hyderabad'],"As a Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\n3+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar field\nMust have experience e",Industry Type: Industrial Automation,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-10 14:18:12
AI Architect,Valuebound,10 - 14 years,Not Disclosed,[],"We are seeking a seasoned Senior AI/ML Engineer to lead the design, development, and deployment of advanced machine learning models and AI solutions. This role involves collaborating with cross-functional teams to drive innovation and deliver impactful AI-driven products. This senior, cross-functional role is responsible for setting our strategic direction on GenAI, Intelligent automation, and data-powered operations. You will guide the implementation of AI solutions across the business, delivering meaningful process improvements, with a keen eye on AI governance.\n\nWhat you will do:\nLead the design, development, and deployment of AI-powered business workflows using the latest LLMs, agents, and intelligent automation tools.\nGuide AI governance, ethical use, and standardisation across all AI-enabled solutions.\nSupport business units by training staff and promoting the effective use of AI tools in daily operations.\nStay current on the latest AI developments (releases, tooling, methodologies) and recommend relevant applications that could improve all areas of the business.\nSupervise, mentor, and support a small but growing AI engineering team, fostering a culture of collaboration, innovation, and driving improvements across the business.\nEngage in hands-on development as required, building prototypes and working solutions for internal stakeholders.\nWork alongside our Power Platform team to enhance advanced model-driven app solutions and Business Reporting using AI capabilities.\nContribute to long-term planning, vendor evaluations, and strategy for scalable and secure AI adoption.\nMatch the business needs with industry standards on governance like ISO 27001 to allow the business to remain compliant.\nCollaborate with global colleagues across time zones; some flexibility will be required.\nWhat we are looking for:\nDeep understanding of generative AI, LLMs, and agent frameworks.\nTrack record in designing, building, and deploying AI-enabled workflows and solutions.\nHands-on experience developing workflows and using Microsoft s Power Platform.\nExperience implementing AI solutions with ISO 27001 is desirable (not audit level).\nExperience designing and implementing AI solutions with CRM Systems is desirable.\nDemonstrated ability to lead, inspire, and mentor technical teams.\n\nExperience:\nVery strong organisational skills.\n10+ years in a technical role, including 5+ years building AI/ML powered workflows and automation solutions.\nLeadership or team lead experience, particularly within AI Innovation.\nHands-on experience with Azure AI, Open AI Apis, Copilot MLOps, and Cloud-native data engineering.\nExperience working across departments and coaching non-technical stakeholders.\nExperience with Azure OpenAI studio or similar enterprise AI environments.\nExposure to model-driven apps and Dataverse integration.\nWorking knowledge of Microsoft Power Platform suites.\nExperience contributing to AI/ML strategy at an organisational level.\nAwareness of AI safety, compliance, and ethical frameworks.\nPowerShell scripting, SQL Query creations, Power platform workflow creation.\nUser rollout of cloud-based IT solutions.\nKnowledge and Skills:\nKeen attention to detail.\nA confident and charismatic person, able to engage with any level of user within the business.\nA good level of numeracy.\nThe ability to lead AI automation projects and work within project scopes.\nAbility to juggle different tasks effectively.\nAbility to lead a team and provide mentorship.\nThe ability to understand how users learn and adapt to technology.\nFlexibility with working hours due to the nature of the role.\nStrong written and verbal communication skills are a must.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'ISO 27001', 'Cloud', 'Design development', 'Workflow', 'Mentor', 'microsoft', 'CRM', 'Auditing', 'SQL']",2025-06-10 14:18:14
Data Engineer- AWS and Databricks Expert,Acuity It Solutions,6 - 10 years,11-21 Lacs P.A.,['Bengaluru'],"Design and implement scalable data ingestion and transformation pipelines using Databricks and AWS.\nDevelop and optimize ETL/ELT workflows in PySpark and Spark SQL, ensuring performance and reliability, and use CI/CD tools.\n\nRequired Candidate profile\nExperience Required 6 to 9 years. Minimum 4+ years of experience with Databricks and AWS.Design and develop scalable ETL/ELT pipelines, PySpark SQL and Python. Immediate joiner to 30 days NP required.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ETL', 'AWS', 'Data Bricks', 'Pyspark', 'Python', 'SQL']",2025-06-10 14:18:15
AWS Data Engineer,Kryon Knowledge Works,5 - 8 years,Not Disclosed,['Hyderabad'],"Location: Hyderabad (Hybrid)\n\nKey Responsibilities:\n\n1. Data Engineering (AWS Glue & AWS Services):\nDesign, develop, and optimize ETL pipelines using AWS Glue (PySpark).\nManage and transform structured and unstructured data from multiple sources into AWS S3, Redshift, or Snowflake.\nWork with AWS Lambda, S3, Athena, Redshift for data orchestration.\nImplement data lake and data warehouse solutions in AWS.\n\n2. Infrastructure as Code (Terraform & AWS Services):\nDesign and deploy AWS infrastructure using Terraform.\nAutomate resource provisioning and manage Infrastructure as Code.\nMonitor and optimize cloud costs, security, and compliance.\nMaintain and improve CI/CD pipelines for deploying data applications.\n\n3. Business Intelligence (Tableau Development & Administration):\nDevelop interactive dashboards and reports using Tableau.\nConnect Tableau with AWS data sources such as Redshift, Athena, and Snowflake.\nOptimize SQL queries and extracts for performance efficiency.\nManage Tableau Server administration, including security, access controls, and performance tuning.\n\nRequired Skills & Experience:\n5+ years of experience in AWS Data Engineering with Glue, Redshift, and S3.\nStrong expertise in ETL development using AWS Glue (PySpark, Scala, or Python).\nExperience with Terraform for AWS infrastructure automation.\nProficiency in SQL, Python, or Scala for data processing.\nHands-on experience in Tableau development & administration.\nStrong understanding of cloud security, IAM roles, and permissions.\nExperience with CI/CD pipelines (Git, Jenkins, AWS Code Pipeline, etc.).\nKnowledge of data modeling, warehousing, and performance optimization.\n\n\nPlease share your resume to: +91 9361912009",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Business Intelligence', 'Terraform', 'Glue AWS', 'Tableau Development', 'AWS Data Engineer', 'Ci Cd Pipeline', 'Redshift Aws', 'Etl Development', 'Python', 'SQL']",2025-06-10 14:18:18
Cloudera Data Platform- Hive/Impala/Kudo,IBM,3 - 8 years,Not Disclosed,['Mumbai'],"Role Overview :\nAs a Big Data Engineer, you'll design and build robust data pipelines on Cloudera using Spark (Scala/PySpark) for ingestion, transformation, and processing of high-volume data from banking systems.\n\n Key Responsibilities :\nBuild scalable batch and real-time ETL pipelines using Spark and Hive\nIntegrate structured and unstructured data sources\nPerform performance tuning and code optimization\nSupport orchestration and job scheduling (NiFi, Airflow)\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience 3-15 Years\nProficiency in PySpark/Scala with Hive/Impala\nExperience with data partitioning, bucketing, and optimization\nFamiliarity with Kafka, Iceberg, NiFi is a must\nKnowledge of banking or financial datasets is a plus",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'scala', 'impala', 'apache nifi', 'kafka', 'cloudera', 'pyspark', 'sql', 'etl pipelines', 'apache', 'java', 'unix shell scripting', 'spark', 'gcp', 'shell scripting', 'hadoop', 'big data', 'etl', 'hbase', 'python', 'performance tuning', 'oozie', 'airflow', 'microsoft azure', 'nosql', 'cassandra', 'sqoop', 'aws', 'unix']",2025-06-10 14:18:21
Azure Data Engineer,UK Client,5 - 8 years,Not Disclosed,[],"* 4–8 yrs exp as Azure Data Engineer\n\n* Strong in Azure Databricks, Data Lake & Data Factory\n\n* Experience with Azure Synapse Analytics\n\n* Hands-on with PySpark & Python\n\n* Build & optimize scalable data pipelines\n\n* 6Months Contract & Extendable job",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Temporary/Contractual","['Azure Data Factory', 'Azure Synapse', 'Pyspark', 'Azure Databricks', 'Python', 'Azure Data Lake', 'SQL']",2025-06-10 14:18:24
Junior Data Engineer,Talent Corner Hr Services,1 - 5 years,7-10 Lacs P.A.,['Bengaluru( JP Nagar )'],"We are looking for a Junior Data Engineer with 1–3 years of experience, primarily focused on database management and data processing using MySQL. The candidate will support the data engineering team in maintaining reliable data pipelines",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MySQL', 'Query Optimization', 'Data Processing']",2025-06-10 14:18:27
ML Engineer,HARMAN,3 - 5 years,Not Disclosed,['Bengaluru'],"As a technology leader that is rapidly on the move, HARMAN is filled with people who are focused on making life better. Innovation, inclusivity and teamwork are a part of our DNA. When you add that to the challenges we take on and solve together, you'll discover that at HARMAN you can grow, make a difference and be proud of the work you do everyday.\n  Introduction: Digital Transformation Solutions (DTS)\nwe're a global, multi-disciplinary team that s putting the innovative power of technology to work and transforming tomorrow. As a member of HARMAN Lifestyle, you connect consumers with the power of superior sound.",,,,"['C++', 'Linux', 'Image processing', 'Analytical', 'Machine learning', 'Debugging', 'Automotive', 'Python', 'Android']",2025-06-10 14:18:30
Automation & Data Engineer,Vyometra Global Llp,2 - 3 years,3.6-3.96 Lacs P.A.,['Bengaluru'],"Work with ops teams to digitize manual processes.\nCapture data via PLCs/IoT, build backend (Python, Flask/Django), SQL DB & frontend dashboard.\nTrack OEE, downtime, rejections. Deploy, maintain & contribute to future product roadmap.",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Automation', 'Automation', 'Machine Learning', 'Javascript', 'Python', 'Ai Techniques', 'Artificial Intelligence', 'Software Testing', 'Flask Web Framework', 'Numpy', 'Scikit-Learn', 'English', 'Pandas', 'Data Analysis', 'SQL Database', 'Ai Builder', 'Flask']",2025-06-10 14:18:32
Director - Analytics,Astar Data,14 - 22 years,Not Disclosed,['Bengaluru'],"VP - Data Analytics\nThis role will be a leadership position in the data science group at Sigmoid.\nAn ideal person will come from a services industry background with a good mix of experience in\nsolving complex business intelligence and data analytics problems, team management, delivery\nmanagement and customer handling. This position will give you an immense opportunity to work\non challenging business problems faced by fortune 500 companies across the globe. The role is\npart of the leadership team and includes accountability for a part of her/his team and customers.\nThe person is expected to be someone who can contribute in developing the practice with\nrelevant experience in the domain, nurturing the talent in the team and working with customers to\ngrow accounts.\nResponsibilities Include\n\nBuild trust with senior stakeholders through strategic insight and delivery credibility.\nAbility to translate ambiguous client business problems into BI solutions and ability to\nimplement them\nOversight of multi-client BI and analytics programs with competing priorities and\ntimelines, while collaborating with Data Engineering and other functions on a common\ngoal.\nEnsure scalable, high-quality deliverables aligned with business impact.\nHelp recruiting and onboarding team members; directly manage 15 - 20 team members\nYou would be required to own customer deliverables and ensure, along with project\nmanagers, that the project schedules are in line with the expectations set to the\ncustomers\nExperience and Qualifications\n15+ years of overall experience with a minimum of 10+ years in data analytics execution.\nStrong organizational and multitasking skills with the ability to balance multiple priorities.\nHighly analytical with the ability to collate, analyze and present data and drive clear\ninsights to lead decisions that improve KPIs.\nAbility to effectively communicate and manage relationships with senior management,\nother departments and partners.\nMastery of BI tools (Power BI, Tableau, Qlik), backend systems (SQL, ETL frameworks),\nand data modeling.\nExperience with cloud-native platforms (Snowflake, Databricks, Azure, AWS), data\nlakes\nExpertise in managing compliance, access controls, and data quality frameworks is a\nplus\nExperience working in CPG, Supply Chain, Manufacturing and Marketing domains are a\nplus\nStrong problem-solving skills and ability to prioritize conflicting requirements\nExcellent written and verbal communication skills and ability to succinctly summarize the\nkey findings",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Intelligence', 'Marketing Analytics', 'Data Analytics', 'Analytics', 'Business Analytics', 'Advanced Analytics', 'Predictive Analytics']",2025-06-10 14:18:33
Technical Lead - L1,Wipro,4 - 8 years,Not Disclosed,['Bengaluru'],"Python, NumPy, Pandas, Rest API\nStrong in Data Engineering and Analysis, SQL Server (Complex SQL)\nThis is hands on role. The role will involve design, coding, testing, working with product owners / scrum master for scrum planning, estimation, demos & leading guiding junior developers as needed.\nYears of experience 6 to 10 years\nMandatory tech skills - Python, NumPy, Pandas, Strong in Data Engineering and Analysis, SQL Server\nGood in writing Python code.\nHands-on experience with pandas and NumPy stack\nAble to perform data cleanup. Summarization using NumPy/pandas.\nSQL knowledge is Essential.\nMust have expertise in Rest API development\nCloud experience is preferred (Azure).\nUses pertinent data and facts to identify and solve a range of problems within area of expertiseDo\nOversee and support process by reviewing daily transactions on performance parameters\nReview performance dashboard and the scores for the team\nSupport the team in improving performance parameters by providing technical support and process guidance\nRecord, track, and document all queries received, problem-solving steps taken and total successful and unsuccessful resolutions\nEnsure standard processes and procedures are followed to resolve all client queries\nResolve client queries as per the SLAs defined in the contract\nDevelop understanding of process/ product for the team members to facilitate better client interaction and troubleshooting\nDocument and analyze call logs to spot most occurring trends to prevent future problems\nIdentify red flags and escalate serious client issues to Team leader in cases of untimely resolution\nEnsure all product information and disclosures are given to clients before and after the call/email requests\nAvoids legal challenges by monitoring compliance with service agreement\nHandle technical escalations through effective diagnosis and troubleshooting of client queries\nManage and resolve technical roadblocks/ escalations as per SLA and quality requirements\nIf unable to resolve the issues, timely escalate the issues to TA & SES\nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers and clients business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract SLAs\nBuild people capability to ensure operational excellence and maintain superior customer service levels of the existing account/client\nMentor and guide Production Specialists on improving technical knowledge\nCollate trainings to be conducted as triage to bridge the skill gaps identified through interviews with the Production Specialist\nDevelop and conduct trainings (Triages) within products for production specialist as per target\nInform client about the triages being conducted\nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nIdentify and document most common problems and recommend appropriate resolutions to the team\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\nDeliver No Performance Parameter Measure\n1 ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback, NSAT/ ESAT\n2 Team ManagementProductivity, efficiency, absenteeism\n3 Capability developmentTriages completed, Technical Test performance",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'Data Engineering', 'Pandas', 'SQL Server', 'REST API', 'Numpy']",2025-06-10 14:18:35
Data Bricks,PwC India,7 - 12 years,Not Disclosed,['Bengaluru'],"Job Summary:\n\nWe are seeking a talented Data Engineer with strong expertise in Databricks, specifically in Unity Catalog, PySpark, and SQL, to join our data team. Youll play a key role in building secure, scalable data pipelines and implementing robust data governance strategies using Unity Catalog.\n\nKey Responsibilities:",,,,"['DataBricks', 'Data Bricks', 'Pyspark', 'Delta Lake', 'Databricks Engineer', 'Unity Catalog', 'SQL']",2025-06-10 14:18:38
Application Developer-Google Cloud FullStack,IBM,3 - 6 years,Not Disclosed,['Bengaluru'],"As an entry level Application Developer at IBM, you'll work with clients to co-create solutions to major real-world challenges by using best practice technologies, tools, techniques, and products to translate system requirements into the design and development of customized systems. In your role, you may be responsible for:\nWorking across the entire system architecture to design, develop, and support high quality, scalable products and interfaces for our clients\nCollaborate with cross-functional teams to understand requirements and define technical specifications for generative AI projects\nEmploying IBM's Design Thinking to create products that provide a great user experience along with high performance, security, quality, and stability\nWorking with a variety of relational databases (SQL, Postgres, DB2, MongoDB), operating systems (Linux, Windows, iOS, Android), and modern UI frameworks (Backbone.js, AngularJS, React, Ember.js, Bootstrap, and JQuery)\nCreating everything from mockups and UI components to algorithms and data structures as you deliver a viable product\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nSQL authoring, query, and cost optimisation primarily on Big Query. Python as an object-oriented scripting language.\nData pipeline, data streaming and workflow management toolsDataflow, Pub Sub, Hadoop, spark-streaming\nVersion control systemGIT & Preferable knowledge of Infrastructure as CodeTerraform.\nAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.\nExperience performing root cause analysis on internal and external data and processes to answer specific business questions.\n\n\nPreferred technical and professional experience\nExperience building and optimising data pipelines, architectures and data sets. Build processes supporting data transformation, data structures, metadata, dependency and workload management.\nWorking knowledge of message queuing, stream processing, and highly scalable data stores.\nExperience supporting and working with cross-functional teams in a dynamic environment. We are looking for a candidate with experience in a Data Engineer role, who are also familiar with Google Cloud Platform.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'react.js', 'backbone.js', 'python', 'angularjs', 'metadata', 'css', 'android', 'dbms', 'bootstrap', 'ios', 'jquery', 'docker', 'java', 'postgresql', 'spark', 'gcp', 'linux', 'html', 'bigquery', 'hadoop', 'mongodb', 'pubsub', 'ember.js', 'cloud platform', 'javascript', 'system architecture', 'microsoft windows', 'aws']",2025-06-10 14:18:40
Sr. Analytics Engineer,Adobe,12 - 13 years,Not Disclosed,['Bengaluru'],"Adobe DMe B2B analytics team is looking for a Senior Analytics Engineer to build the foundational data assets and analytical reports for the B2B customer journey analytics. The data assets and reports will provide quantitative and qualitative analysis for acquisition, engagement and retention. In addition, the candidate will contribute on multi-functional projects to help business achieve its potential in terms of revenue, customer success, and operational excellence.\nWhat you'll Do\nAnalyze complex business needs, profile diverse datasets, and optimize scalable data products on various data platforms, like Databricks, prioritizing data quality, performance, and efficiency.\nBuild and maintain foundational datasets and reporting solutions that power dashboards, self-service tools, and ad hoc analytics across the organization.\nPartner closely with Business Intelligence, Product Management, and Business Stakeholders to understand data needs.\nGuide and assist junior analytics engineers and analysts through code reviews, advocating standard methodologies, and encouraging a culture of code simplicity, clarity, and technical excellence.\nWhat you need to succeed\nStrong analytical approach with critical thinking and problem-solving skills.\nAbility to quickly understand Adobe products, business processes, existing data structures, and legacy code.\nMinimum of 8 years of experience in data analytics, data engineering, or a related field.\nSolid expertise in data warehousing, data modeling, and crafting scalable data solutions.\nProficiency in SQL and/or Python for building, optimizing, and fixing data pipelines.\nExperience using AI tools (eg, GitHub Copilot, Claude) to speed up and improve development workflows.\nExcellent communication skills needed to effectively work with business collaborators, engineers, and analysts",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Product management', 'github', 'Data modeling', 'Analytical', 'Data structures', 'Data quality', 'Business intelligence', 'Adobe', 'SQL', 'Python']",2025-06-10 14:18:43
Engineer-Data Science,Hitachi Energy,5 - 10 years,Not Disclosed,['Vadodara'],". Analysis may be applied to various areas of the business (e.g., Market Economics, Supply Chain, Marketing/Advertising, Scientific Research, etc.). Researching and applying knowledge of existing and emerging data science principles, theories, and techniques to inform business decisions. At higher career levels, may conduct scientific research projects with the goal of breaking new ground in data analytics An Experienced Professional (P2) applies practical knowledge of job area typically obtained through advanced education and work experience. May require the following proficiency: Works independently with general supervision. Problems faced are difficult but typically not complex. May influence others within the job area through explanation of facts, policies and practices.\nHow you ll make an impact\nThe success candidate will be the part of an International Design and Engineering Team heavily specialized in Power Transformers design covering US factory.\nResponsible for building visualizations in PBI based on various sources and datasets of power transformers factories\nResponsible for DAX queries / DAX functions / Power Query Editor\nResponsible for development of transformer dashboard in coordination with global Hitachi Energy factory based on requirement.\nExpertise in using advance level calculations on the data set.\nAble to develop tabular and multidimensional models that are compatible with warehouse standards.\nAble to properly understand the business requirements and develop data models accordingly by taking care of the resources.\nFamiliar with Row Level Security (RLS)\nBasic knowledge and skills for secondary tools such as Microsoft Azure, SQL data warehouse, SSAS, Visual Studio, Power Apps etc.\nLiving Hitachi Energy s core values of safety and integrity, which means taking responsibility for your own actions while caring for your colleagues and the business.\nResponsible to ensure compliance with applicable external and internal regulations, procedures, and guidelines.\nYour Background\nBachelor s degree of Electrical or Mechanical or Data Science Engineering.\n5 - 10 years experience working in Data Analytics from start to end process. Candidates with higher experience also to be considered.\nExperience of manufacturing industry is an additional advantage.\nExtended MS Office knowledge & skills, especially excel but also eg PowerPoint, etc.\nExperienced in building MS Teams Space & SharePoint pages.\nSpecialist on building visualizations in PBI based on various sources and datasets.\nStrong capabilities of DAX queries / DAX functions / Power Query Editor. DA-100 certification preferred.\nExperience with SAP / S4 HANA -Data handling preferred, data sources in Power BI.\nProficiency in both spoken & written English language is required.\n.",Industry Type: Power,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Supply chain', 'Data analysis', 'SAP', 'Data analytics', 'Visual Studio', 'Editor', 'MS Office', 'Data mining', 'Business intelligence', 'SQL']",2025-06-10 14:18:46
Data Architect,Ford,14 - 17 years,Not Disclosed,['Chennai'],"We are looking for Data Solution Architect to join FC India IT Architecture team. In this role, you will define analytics solutions and guide engineering teams to implement big data solutions on the cloud. Work involves migrating data from legacy on-prem warehouses to Google cloud data platform. This role will provide architecture assistance to data engineering teams in India, with key responsibility of supporting applications globally. This role will also drive business adoption of the new platform and sunset of legacy platforms.\nGoogle Professional Solution Architect certification.\n8+ years of relevant work experience in analytics application and data architecture, with deep understanding of cloud hosting concepts and implementations.\n5+ years experience in Data and Solution Architecture in analytics space. Solid knowledge of cloud data architecture, data modelling principles, and expertise in Data Modeling tools.\nExperience in migrating legacy analytics applications to Cloud platform and business adoption of these platforms to build insights and dashboards through deep knowledge of traditional and cloud Data Lake, Warehouse and Mart concepts.\nGood understanding of domain driven design and data mesh principles.\nExperience with designing, building, and deploying ML models to solve business challenges using Python/BQML/Vertex AI on GCP.\nKnowledge of enterprise frameworks and technologies. Strong in architecture design patterns, experience with secure interoperability standards and methods, architecture tolls and process.\nDeep understanding of traditional and cloud data warehouse environment, with hands on programming experience building data pipelines on cloud in a highly distributed and fault-tolerant manner. Experience using Dataflow, pub/sub, Kafka, Cloud run, cloud functions, Bigquery, Dataform, Dataplex , etc.\nStrong understanding on DevOps principles and practices, including continuous integration and deployment (CI/CD), automated testing & deployment pipelines.\nGood understanding of cloud security best practices and be familiar with different security tools and techniques like Identity and Access Management (IAM), Encryption, Network Security, etc. Strong understanding of microservices architecture.\nNice to Have\nBachelor s degree in Computer science/engineering, Data science or related field.\nStrong leadership, communication, interpersonal, organizing, and problem-solving skills\nGood presentation skills with ability to communicate architectural proposals to diverse audiences (user groups, stakeholders, and senior management).\nExperience in Banking and Financial Regulatory Reporting space.\nAbility to work on multiple projects in a fast paced & dynamic environment.\nExposure to multiple, diverse technologies, platforms, and processing environments.\nUtilize Google Cloud Platform & Data Services to modernize legacy applications.\nUnderstand technical business requirements and define architecture solutions that align to Ford Motor & Credit Companies Patterns and Standards.\nCollaborate and work with global architecture teams to define analytics cloud platform strategy and build Cloud analytics solutions within enterprise data factory.\nProvide Architecture leadership in design & delivery of new Unified data platform on GCP.\nUnderstand complex data structures in analytics space as well as interfacing application systems. Develop and maintain conceptual, logical & physical data models. Design and guide Product teams on Subject Areas and Data Marts to deliver integrated data solutions.\nProvide architectural guidance for optimal solutions considering regional Regulatory needs.\nProvide architecture assessments on technical solutions and make recommendations that meet business needs and align with architectural governance and standard.\nGuide teams through the enterprise architecture processes and advise teams on cloud-based design, development, and data mesh architecture.\nProvide advisory and technical consulting across all initiatives including PoCs, product evaluations and recommendations, security, architecture assessments, integration considerations, etc.\nLeverage cloud AI/ML Platforms to deliver business and technical requirements.",Industry Type: Auto Components,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Solution architecture', 'Data modeling', 'Access management', 'Enterprise architecture', 'Consulting', 'Network security', 'Data structures', 'Analytics', 'Python']",2025-06-10 14:18:48
Sr. Project Manager,Useready,15 - 18 years,30-40 Lacs P.A.,"['Mohali', 'Bengaluru']","Job Summary:\nWe are seeking an experienced and detail-oriented Technical Project Manager, with strong interpersonal skills to lead and manage Data, Business Intelligence (BI), and Analytics initiatives across single and multiple client engagements. The ideal candidate will have a solid background in data project delivery, knowledge of modern cloud platforms, and familiarity with tools like Snowflake, Tableau, and Power BI. Understanding of AI and machine learning projects is a strong plus.\nThis role requires strong communication and leadership skills, with the ability to translate complex technical requirements into actionable plans and ensure successful, timely, and high-quality delivery with attention to details.\nKey Responsibilities:\nProject & Program Delivery\nManage end-to-end, the full lifecycle of data engineering and analytics, projects including data platform migrations, dashboard/report development, and advanced analytics initiatives.\nDefine project scope, timelines, milestones, resource needs, and deliverables in alignment with stakeholder objectives.\nManage budgets, resource allocation, and risk mitigation strategies to ensure successful program delivery.\nUse Agile, Scrum, or hybrid methodologies to ensure iterative delivery and continuous improvement.\nMonitor performance, track KPIs, and adjust plans to maintain scope, schedule, and quality.\nExcellence in execution and ensure client satisfaction\nClient & Stakeholder Engagement\nServe as the primary point of contact for clients and internal teams across all data initiatives.\nTranslate business needs into actionable technical requirements and facilitate alignment across teams.\nConduct regular status meetings, monthly and quarterly reviews, executive updates, and retrospectives.\nManage Large teams\nAbility to manage up to 50+ resources working on different projects for different clients.\nWork with practice and talent acquisition teams for resourcing needs\nManage P & L\nManage allocation, gross margin, utilization etc effectively\nTeam Coordination\nLead and coordinate cross-functional teams including data engineers, BI developers, analysts, and QA testers.\nEnsure appropriate allocation of resources across concurrent projects and clients.\nFoster collaboration, accountability, and a results-oriented team culture.\n  Data, AI and BI Technology Oversight\nManage project delivery using modern cloud data platforms\nOversee BI development using Tableau and/or Power BI, ensuring dashboards meet user needs and follow visualization best practices. Conduct UATs\nManage initiatives involving ETL/ELT processes, data modeling, and real-time analytics pipelines.\nEnsure compatibility with data governance, security, and privacy requirements.\nManage AL ML projects\nData & Cloud Understanding\nOversee delivery of solutions involving cloud data platforms (e.g., Azure, AWS, GCP), data lakes, and modern data stacks.\nSupport planning for data migrations, ETL processes, data modeling, and analytics pipelines.\nBe conversant in tools such as Power BI, Tableau, Snowflake, Databricks, Azure Synapse, or BigQuery.\nRisk, Quality & Governance\nIdentify and mitigate risks related to data quality, project timelines, and resource availability.\nEnsure adherence to governance, compliance, and data privacy standards (e.g., GDPR, HIPAA).\nMaintain thorough project documentation including charters, RACI matrices, RAID logs, and retrospectives.\nQualifications:\n  Bachelor’s degree in Computer Science, Information Systems, Business, or a related field.\nCertifications (Preferred):\nPMP, PRINCE2, or Certified ScrumMaster (CSM)\nCloud certifications (e.g., AWS Cloud Practitioner, Azure Fundamentals, Google Cloud Certified)\nBI/analytics certifications (e.g., Tableau Desktop Specialist, Power BI Data Analyst Associate, DA-100)\nMust Have Skills:\nStrong communication skills\nStrong interpersonal\nAbility to work collaboratively\nExcellent Organizing skills\nStakeholder Management\nCustomer Management\nPeople Management\nContract Management\nRisk & Compliance Management\nC-suite reporting\nTeam Management\nResourcing\nExperience using tools like JIRA, MS Plan etc.\nDesirable Skills:\n15 years of IT experience with 8+ years of proven project management experience, in delivering data, AI Ml, BI / analytics-focused environments.\nExperience delivering projects with cloud platforms (e.g., Azure, AWS, GCP) and data platforms like Snowflake.\nProficiency in managing BI projects preferably Tableau and/or Power BI.\nKnowledge or hands on experience on legacy tools is a plus.\nSolid understanding of the data lifecycle including ingestion, transformation, visualization, and reporting.\nComfortable using PM tools like Jira, Azure DevOps, Monday.com, or Smartsheet.\nExperience managing projects involving data governance, metadata management, or master data management (MDM).",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['delivery', 'bi projects', 'project management', 'data', 'interpersonal skills', 'microsoft azure', 'power bi', 'aiml', 'machine learning', 'business intelligence', 'artificial intelligence', 'tableau', 'stakeholder management', 'gcp', 'leadership', 'project delivery', 'scrum', 'agile', 'organizing', 'aws', 'communication skills']",2025-06-10 14:18:50
MLOps Engineer / Data Scientist,CEVA Logistics,3 - 7 years,Not Disclosed,['Mumbai'],"CEVA Logistics provides global supply chain solutions to connect people, products, and providers all around the world\nPresent in 170+ countries and with more than 110,000 employees spread over 1,500 sites, we are proud to be a Top 5 global 3PL\nWe believe that our employees are the key to our success\nWe want to engage and empower our diverse, global team to co-create value with our customers through our solutions in contract logistics and air, ocean, ground, and finished vehicle transport\nThat is why CEVA Logistics offers a dynamic and exceptional work environment that fosters personal growth, innovation, and continuous improvement\nDARE TO GROW! Join CEVA Logistics, and you will be part of a team that values imagination and continued learning and is committed to excellence in everything we do\nJoin us in our mission to shape the future of global logistics\nAs we continue growing at a fast pace, will you Dare to Grow with us\nJoin the forefront of AI-driven logistics innovation as a MLOps Engineer/Data Scientist at CEVA Logistics, a global leader in supply chain solutions\nYOUR ROLE\nAs a MLOps Engineer/Data scientist, you will play a pivotal role in CEVA LogisticsGlobal IT Data & Digital organization, reporting directly to the Global IT Data & Digital BI & Advanced Analytics AI & Data Science Manager\nThis hybrid role combines the expertise of data science with operational practices focused on the industrialization and deployment of AI and machine learning solutions at scale\nYou will collaborate with cross-functional teams in IT, Corporate Support Functions, Business Development, and Operations to drive the development and industrialization of machine learning models\nYour work will ensure that AI-driven insights and use cases are seamlessly integrated, scalable, and continuously optimized to provide ongoing value to CEVA Logistics and its customers\nThis position requires expertise not only in data science and machine learning but also in the processes that turn innovative solutions into robust, enterprise-grade systems\nThis position is open in Spain (Madrid / Barcelona) or Poland (Warsaw) or India (Mumbai)\nWHAT ARE YOU GOING TO DO\nExpertise in data science and machine learning techniques with experience in designing, building, and deploying models to solve business challenges\nProficiency in industrializing use cases, focusing on turning proof-of-concept models into full-scale, production-ready AI solutions that are both scalable and sustainable\nHands-on experience with MLOps tools and technologies (such as Kubernetes, Docker, Jenkins, MLflow, TensorFlow, etc-) for automating the deployment, monitoring, and maintenance of machine learning models\nStrong experience with cloud platforms (AWS, Google Cloud, Azure) and modern deployment architectures for AI/ML workloads\nAdvanced proficiency in Python, SQL, and experience with big data technologies to work with large datasets\nStrong understanding of model governance, model monitoring, and model retraining to ensure AI solutions deliver consistent and ongoing business value\nAbility to work collaboratively across cross-functional teams to ensure smooth transitions from development to deployment and operationalization\nExcellent communication skills to explain complex technical concepts to non-technical stakeholders and influence business strategies through AI-driven insights\nWHAT ARE WE LOOKING FOR\nA minimum of 5 years of professional experience with proven results in AI, Data Science & MLOps\nMasters degree in computer science, Data Science, Engineering, Mathematics, or a related field\nAdvanced certifications in AI, Data Science, Machine Learning, or MLOps would be plus\nProven ability to collaborate effectively with cross-functional teams, including business stakeholders, IT, and operations, to translate business needs into technical solutions\nStrong written and verbal communication skills, with the ability to explain complex technical concepts to non-technical audiences\nTechnical Expertise:\nData Science:\nSolid understanding of machine learning algorithms, statistical methods, and predictive modeling\nProficiency in Python and libraries such as TensorFlow, Keras, scikit-learn, XGBoost, and PyTorch\nMLOps:\nHands-on experience implementing MLOps practices for model deployment, automation, and continuous integration/continuous delivery (CI/CD)\nFamiliarity with tools like MLflow, Kubeflow, Jenkins, Docker, Kubernetes, and Terraform for automating machine learning pipelines\nData Integration & Management:\nProficient in using Snowflake for data integration, storage, and processing at scale\nExperience with Dataiku for creating and managing end-to-end data science workflows\nBig Data Technologies:\nFamiliarity with big data platforms and tools to process and analyze large datasets\nData Visualization & Reporting:\nExpertise in using Qlik Sense, Streamlite, or other BI, webapp & data visualization tools for developing interactive and insightful dashboards and reports to communicate complex results to non-technical stakeholders\nModel Development & Industrialization:\nExperience in transitioning machine learning models from prototype to production at scale, ensuring robustness, scalability, and reliability in real-world applications\nFamiliarity with best practices in model versioning, testing, monitoring, and retraining to maintain model accuracy and performance over time\nCloud Platforms:\nStrong experience with cloud-based platforms like AWS, Google Cloud and Azure for deploying, managing, and scaling AI/ML models\nCollaboration & Communication:\nProven ability to collaborate effectively with cross-functional teams, including business stakeholders, IT, and operations, to translate business needs into technical solutions\nStrong written and verbal communication skills, with the ability to explain complex technical concepts to non-technical audiences\nProblem Solving & Innovation:\nStrong analytical and problem-solving skills, with a creative mindset for leveraging AI and machine learning to solve business problems and drive innovation\nAbility to stay up to date with the latest developments in AI, machine learning, and MLOps, and apply cutting-edge techniques to business challenges\nSoft Skills:\nSelf-motivated, adaptable, and able to thrive in a fast-paced and dynamic work environment\nStrong attention to detail with a focus on quality, accuracy, and reliability in all work\nEffective time management and organizational skills, with the ability to handle multiple projects simultaneously and meet deadlines\nAdditional Information:\nThis position offers the opportunity to work with both existing tools at CEVA and new, modern tools available in the Cloud, leveraging our newly developed CEVA Data Platform\nThe role requires a proactive individual with a strong commitment to driving data-driven strategies and solutions within the organization\nWHAT DO WE HAVE TO OFFER\nWith a genuine culture of recognition, we want our employees to grow, develop and be part of our journey\nYou have access to the CEVA academy for training\nYou receive healthcare benefits, reimbursement of the transportation card (50%) and meal vouchers for each working day\nWe are a team in every sense, and we support each other and work collaboratively to achieve our goals together\nIt is our goal that you will be compensated for your hard work and commitment, so if youd like to work for one of the top Logistics providers in the world then lets work together to help you find your new role",Industry Type: Courier / Logistics,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['model monitoring', 'snowflake', 'python', 'predictive modeling', 'cloud platforms', 'machine learning algorithms', 'sql', 'communication skills']",2025-06-10 14:18:53
Data Architect,Ford,8 - 13 years,Not Disclosed,['Chennai'],"We are looking for Data Solution Architect to join FC India IT Architecture team. In this role, you will define analytics solutions and guide engineering teams to implement big data solutions on the cloud. Work involves migrating data from legacy on-prem warehouses to Google cloud data platform. This role will provide architecture assistance to data engineering teams in India, with key responsibility of supporting applications globally. This role will also drive business adoption of the new platform and sunset of legacy platforms.\nGoogle Professional Solution Architect certification.\n8+ years of relevant work experience in analytics application and data architecture, with deep understanding of cloud hosting concepts and implementations.\n5+ years experience in Data and Solution Architecture in analytics space. Solid knowledge of cloud data architecture, data modelling principles, and expertise in Data Modeling tools.\nExperience in migrating legacy analytics applications to Cloud platform and business adoption of these platforms to build insights and dashboards through deep knowledge of traditional and cloud Data Lake, Warehouse and Mart concepts.\nGood understanding of domain driven design and data mesh principles.\nExperience with designing, building, and deploying ML models to solve business challenges using Python/BQML/Vertex AI on GCP.\nKnowledge of enterprise frameworks and technologies. Strong in architecture design patterns, experience with secure interoperability standards and methods, architecture tolls and process.\nDeep understanding of traditional and cloud data warehouse environment, with hands on programming experience building data pipelines on cloud in a highly distributed and fault-tolerant manner. Experience using Dataflow, pub/sub, Kafka, Cloud run, cloud functions, Bigquery, Dataform, Dataplex , etc.\nStrong understanding on DevOps principles and practices, including continuous integration and deployment (CI/CD), automated testing & deployment pipelines.\nGood understanding of cloud security best practices and be familiar with different security tools and techniques like Identity and Access Management (IAM), Encryption, Network Security, etc. Strong understanding of microservices architecture.\nNice to Have\nBachelor s degree in Computer science/engineering, Data science or related field.\nStrong leadership, communication, interpersonal, organizing, and problem-solving skills\nGood presentation skills with ability to communicate architectural proposals to diverse audiences (user groups, stakeholders, and senior management).\nExperience in Banking and Financial Regulatory Reporting space.\nAbility to work on multiple projects in a fast paced & dynamic environment.\nExposure to multiple, diverse technologies, platforms, and processing environments.\nUtilize Google Cloud Platform & Data Services to modernize legacy applications.\nUnderstand technical business requirements and define architecture solutions that align to Ford Motor & Credit Companies Patterns and Standards.\nCollaborate and work with global architecture teams to define analytics cloud platform strategy and build Cloud analytics solutions within enterprise data factory.\nProvide Architecture leadership in design & delivery of new Unified data platform on GCP.\nUnderstand complex data structures in analytics space as well as interfacing application systems. Develop and maintain conceptual, logical & physical data models. Design and guide Product teams on Subject Areas and Data Marts to deliver integrated data solutions.\nProvide architectural guidance for optimal solutions considering regional Regulatory needs.\nProvide architecture assessments on technical solutions and make recommendations that meet business needs and align with architectural governance and standard.\nGuide teams through the enterprise architecture processes and advise teams on cloud-based design, development, and data mesh architecture.\nProvide advisory and technical consulting across all initiatives including PoCs, product evaluations and recommendations, security, architecture assessments, integration considerations, etc.\nLeverage cloud AI/ML Platforms to deliver business and technical requirements.",Industry Type: Automobile,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Solution architecture', 'Data modeling', 'Access management', 'Enterprise architecture', 'Consulting', 'Network security', 'Data structures', 'Analytics', 'Python']",2025-06-10 14:18:55
Life sciences - Sr.Project Manager,Agilisium,10 - 17 years,Not Disclosed,['Chennai( Perungudi )'],"Job Title: IT Project Manager (Life Sciences)\nLocation: OMR, Chennai\nWork Mode: On-site (Ready to work from office)\nExperience: 12+ Years\nJob Description:\nWe are looking for an IT Project Manager with deep expertise in Life Sciences (Pharma/Biotech/MedTech) and hands-on experience managing Data Engineering projects. The ideal candidate will have 12+ years of experience leading IT initiatives, including data pipelines, cloud-based analytics, and regulatory-compliant data solutions in the Life Sciences domain.",,,,"['Life Sciences', 'Project Management', 'Profit And Loss', 'Project Monitoring', 'Project Documentation', 'Project Planning', 'Project Scheduling', 'Salesforce']",2025-06-10 14:18:57
TS - GCP data engineer,Srivango,6 - 11 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","Exp: 6-9Yrs\nSkill: GCP, Data engineer",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Engineering', 'GCP', 'Google Cloud Platforms']",2025-06-10 14:18:59
Lead Data Engineer,Wiser Solutions,2 - 3 years,Not Disclosed,['Mysuru'],"Job Description\nWhen looking to buy a product, whether it is in a brick and mortar store or online, it can be hard enough to find one that not only has the characteristics you are looking for but is also at a price that you are willing to pay. It can also be especially frustrating when you finally find one, but it is out of stock. Likewise, brands and retailers can have a difficult time getting the visibility they need to ensure you have the most seamless experience as possible in selecting their product. We at Wiser believe that shoppers should have this seamless experience, and we want to do that by providing the brands and retailers the visibility they need to make that belief a reality.\nOur goal is to solve a messy problem elegantly and cost effectively. Our job is to collect, categorize, and analyze lots of structured and semi-structured data from lots of different places every day (whether it s 20 million+ products from 500+ websites or data collected from over 300,000 brick and mortar stores across the country). We help our customers be more competitive by discovering interesting patterns in this data they can use to their advantage, while being uniquely positioned to be able to do this across both online and instore.\nWe are looking for a lead-level software engineer to lead the charge on a team of like-minded individuals responsible for developing the data architecture that powers our data collection process and analytics platform. If you have a passion for optimization, scaling, and integration challenges, this may be the role for you.\nWhat You Will Do\nThink like our customers - you will work with product and engineering leaders to define data solutions that support customers business practices.\nDesign/develop/extend our data pipeline services and architecture to implement your solutions - you will be collaborating on some of the most important and complex parts of our system that form the foundation for the business value our organization provides\nFoster team growth - provide mentorship to both junior team members and evangelizing expertise to those on others.\nImprove the quality of our solutions - help to build enduring trust within our organization and amongst our customers by ensuring high quality standards of the data we manage\nOwn your work - you will take responsibility to shepherd your projects from idea through delivery into production\nBring new ideas to the table - some of our best innovations originate within the team\nTechnologies We Use\nLanguages: SQL, Python\nInfrastructure: AWS, Docker, Kubernetes, Apache Airflow, Apache Spark, Apache Kafka, Terraform\nDatabases: Snowflake, Trino/Starburst, Redshift, MongoDB, Postgres, MySQL\nOthers: Tableau (as a business intelligence solution)\n\n\nQualifications\nBachelors/Master s degree in Computer Science or relevant technical degree\n10+ years of professional software engineering experience\nStrong proficiency with data languages such as Python a",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Linux', 'MySQL', 'Agile', 'Data structures', 'OLAP', 'Apache', 'Business intelligence', 'SQL', 'Python']",2025-06-10 14:19:02
Senior Machine Learning Engineer,Ortseam Technologies,5 - 10 years,Not Disclosed,[],"Job Title: Senior Machine Learning Engineer\nWork Mode: Remote\nBase Location: Bengaluru\nExperience: 5+ Years\n\nStrong problem-solving skills and ability to work in a fast-paced, collaborative environment.\nStrong programming skills in Python and experience with ML frameworks.\nProficiency in containerization (Docker) and orchestration (Kubernetes) technologies.\nSolid understanding of CI/CD principles and tools (e.g., Jenkins, GitLab CI, GitHub Actions).\nKnowledge of data engineering concepts and experience building data pipelines.\nStrong understandings on Computational, Storage and Orchestration resources on cloud platforms.\nDeploying and managing ML models especially on GCP (cloud platform agnostic though) services such as Cloud Run, Cloud Functions, and Vertex AI.\nImplementing MLOps best practices, including model version tracking, governance, and monitoring for performance degradation and drift.\nCreating and using benchmarks, metrics, and monitoring to measure and improve services\nCollaborating with data scientists and engineers to integrate ML workflows from onboarding to decommissioning.\nExperience with MLOps tools like Kubeflow, MLflow, and Data Version Control (DVC).\nManage ML models on any of the following: AWS (SageMaker), Azure (Machine Learning), and GCP (Vertex AI).\n\nTech Stack:\n\nAws or GCP or Azure Experience. (More GCP Specific)\nmust have done Py spark,\nDatabricks is good.\nML Experience,\nDocker and Kubernetes.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Machine Learning', 'Docker', 'GCP', 'Kubernetes', 'Vertex', 'Hadoop', 'Data Bricks', 'Hive', 'Azure Cloud', 'SCALA', 'Cicd Pipeline', 'AWS', 'Python']",2025-06-10 14:19:05
Application Developer-Adobe Commerce,IBM,6 - 7 years,Not Disclosed,['Navi Mumbai'],"Collaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\n\nIn this role, your responsibilities may include\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nTotal Exp-6-7 Yrs (Relevant-4-5 Yrs)\nMandatory\n\nSkills:\nAzure Databricks, Python/PySpark, SQL, Github, - Azure Devops - Azure Blob\nAbility to use programming languages like Java, Python, Scala, etc., to build pipelines to extract and transform data from a repository to a data consumer\nAbility to use Extract, Transform, and Load (ETL) tools and/or data integration, or federation tools to prepare and transform data as needed.\nAbility to use leading edge tools such as Linux, SQL, Python, Spark, Hadoop and Java\n\n\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['pyspark', 'azure devops', 'sql', 'azure databricks', 'python', 'scala', 'elastic search', 'java', 'spark', 'devops', 'linux', 'ssrs', 'hadoop', 'big data', 'etl', 'github', 'ssas', 'azure blob', 'machine learning', 'sql server', 'javascript', 'application development', 'splunk', 'agile', 'ssis', 'data integration']",2025-06-10 14:19:08
Sr. Software Engineer - SQL,Yash Technologies,5 - 8 years,Not Disclosed,['Pune'],"We are looking forward to hire SQL Professionals in the following areas :\n:\nWe are seeking a highly skilled Data Engineer with 5 to 8 years of experience to join our growing data team. The ideal candidate will have strong expertise in Azure technologies and advanced Python programming skills. This role involves designing, building, and optimizing data pipelines, ensuring data security, and enabling analytical capabilities across the organization.\nIn this Role, Your Responsibilities Will Be:",,,,"['Computer science', 'Manager Quality Assurance', 'Data modeling', 'Agile', 'microsoft', 'Information technology', 'Analytics', 'SQL', 'Python', 'Technical documentation']",2025-06-10 14:19:10
Lead Data Engineer ( Exp: 6+ Years ),Atyeti,6 - 11 years,Not Disclosed,"['Hyderabad', 'Pune']",Job Description :\n\nStrong experience on Python programming.\nExperience on Databricks.\nExperience on Database like SQL\nPerform database performance tuning and optimization. Databricks Platform\nWork with Databricks platform for big data processing and analytics.,,,,"['Pyspark', 'ETL', 'Data Bricks', 'Python', 'SQL']",2025-06-10 14:19:12
Data Scientist,Ford,3 - 7 years,Not Disclosed,['Chennai'],"As a Data Scientist, you will be part of a high performing team working on exciting opportunities in AI/ML within Ford Credit. We are looking for a seasoned Data Scientist with proven expertise in implementing Machine Learning/Optimization solutions with familiarity in Generative AI and a good grasp of Statistics.\nDevelop Machine Learning (Supervised/Unsupervised learning), Neural Networks (ANN, CNN, RNN, LSTM, Decision tree, Encoder, Decoder), Natural Language Processing, Generative AI (LLMs, Lang Chain, RAG, Vector Database) .\nHands-on Expertise in Python programming (OOPs concepts), SQL (relational/non-relational databases), experience in handling various data science libraries (Pandas, NumPy, SciPy, Sklearn, TensorFlow, Keras, Pytorch, etc. ) would be a necessary requirement.\nExposure to Cloud technologies (e. g. , Google Cloud/AWS/Azure), including executing Machine Learning algorithms on Cloud is necessary.\nExposure to Generative AI technologies.\nProfessional Qualification:\nPotential candidates should possess 3 to 7 years of working experience as a Data Scientist.\nBE/MSc/ MTech /ME/PhD (Computer Science/Maths, Statistics).\nPossess a strong analytical mindset and be very comfortable with data.\nExperience with handling both relational and non-relational data.\nHands-on with analytics methods (descriptive / predictive / prescriptive) , Statistical Analysis, Probability and Data Visualization tools (Python-Matplotlib, Seaborn).\nBackground of Computer Science with excellent Data Science working experience.\nTechnical Experience:\nDevelop Machine Learning (Supervised/Unsupervised learning), Neural Networks (ANN, CNN, RNN, LSTM, Decision tree, Encoder, Decoder), Natural Language Processing, Generative AI (LLMs, Lang Chain, RAG, Vector Database) .\nHands-on Expertise in Python programming (OOPs concepts), SQL (relational/non-relational databases), experience in handling various data science libraries (Pandas, NumPy, SciPy, Sklearn, TensorFlow, Keras, Pytorch, etc. ) would be a necessary requirement.\nExposure to Cloud technologies (e. g. , Google Cloud/AWS/Azure), including executing Machine Learning algorithms on Cloud is necessary.\nExposure to Generative AI technologies.\nAbility to scope the problem statement, data preparation, training and making the AI/ML model production ready.\nWork with business partners to understand the problem statement, translate the same into analytical problem.\nAbility to manipulate structured and unstructured data.\nDevelop, test and improve existing machine learning models.\nAnalyse large and complex data sets to derive valuable insights.\nResearch and implement best practices to enhance existing machine learning infrastructure. Develop prototypes for future exploration.\nDesign and evaluate approaches for handling large volume of real data streams.\nAbility to determine appropriate analytical methods to be used.\nCollaborate with data engineers, solutions architects, application engineers, and product teams across time zones to develop data and model pipelines",Industry Type: Auto Components,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'data science', 'Neural networks', 'Analytical', 'Machine learning', 'Natural language processing', 'data visualization', 'Analytics', 'SQL', 'Python']",2025-06-10 14:19:15
Associate Data Engineer | Pricing | Full Time Contract 18 Months,Argus India Price Reporting Services,1 - 3 years,5-11 Lacs P.A.,['Mumbai (All Areas)'],"Associate Data Engineer - (Fixed Term Contract)\nMumbai\nJob Purpose:\nDue to the continued growth of the business and the importance of the data we use on a daily basis, we are currently looking for a for a junior data engineer to join our global Data team in Mumbai.\nYou will work closely with internal clients of the data team to support and maintain R (incl. R Shiny), Excel and database-based processes for gathering data, calculating prices and producing the reports and data feeds. The work also involves writing robust automated processes in R.\nThe team supports Argus key business processes every day, as such you will be required to work on a shift-based rota with other members of the team supporting the business until 8pm. Typically support hours run from 11pm to 8pm with each member of the team participating 2/3 times a week.\n\nKey Responsibilities:\nSupport and development of data processing systems\nClient support with queries relating to\nintegration of Argus data and metadata into client systems\ndata validation\nprovision of data\nData and systems support to Argus staff\nProject development\nMaintenance and development of existing systems\nmetadata modification\ndata cleansing\ndata checking\nRequired Skills and Experience:\nSignificant recent experience of developing tools using R (incl., R Shiny applications) in a commercial (work) environment.\nGood knowledge and experience of SQL, preferably in Oracle or MySQL, including stored procedures, functions, and triggers.\nExperience with version control systems (e.g., Git) and Unit Testing in R is required.\nAbility to work both as part of a team and autonomously\nExcellent communication skills.\n\nDesired Skills and Experience:\nBS degree in Computer Science, Mathematics, Business, Engineering or related field.\nExperience in visualisation techniques is desirable.\nAny experience working with energy markets and commodities data is highly desirable\nExperience developing production-grade scalable applications in R.\n\nPersonal Attributes:\nAbility to interact with non-technical people in plain language\nInnovative thinker with good problem-solving abilities and attention to detail\nNumerically proficient\nSelf-motivated with ability to work independently, prioritising tasks to meet deadlines\nCustomer service focused.\n\nBenefits:\nCompetitive salary\nFlexible Working Policy\nGroup healthcare scheme\n18 days annual leave\n8 days casual leave\nExtensive internal and external training",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['R', 'R Shiny', 'Unit Testing', 'Version Control Systems Svn', 'SQL']",2025-06-10 14:19:17
Data Scientist,Ford,3 - 7 years,Not Disclosed,['Chennai'],"As a Data Scientist, you will be part of a high performing team working on exciting opportunities in AI/ML within Ford Credit. We are looking for a seasoned Data Scientist with proven expertise in implementing Machine Learning/Optimization solutions with familiarity in Generative AI and a good grasp of Statistics.\nDevelop Machine Learning (Supervised/Unsupervised learning), Neural Networks (ANN, CNN, RNN, LSTM, Decision tree, Encoder, Decoder), Natural Language Processing, Generative AI (LLMs, Lang Chain, RAG, Vector Database) .\nHands-on Expertise in Python programming (OOPs concepts), SQL (relational/non-relational databases), experience in handling various data science libraries (Pandas, NumPy, SciPy, Sklearn, TensorFlow, Keras, Pytorch, etc.) would be a necessary requirement.\nExposure to Cloud technologies (e.g., Google Cloud/AWS/Azure), including executing Machine Learning algorithms on Cloud is necessary.\nExposure to Generative AI technologies.\nProfessional Qualification:\nPotential candidates should possess 3 to 7 years of working experience as a Data Scientist.\nBE/MSc/ MTech /ME/PhD (Computer Science/Maths, Statistics).\nPossess a strong analytical mindset and be very comfortable with data.\nExperience with handling both relational and non-relational data.\nHands-on with analytics methods (descriptive / predictive / prescriptive) , Statistical Analysis, Probability and Data Visualization tools (Python-Matplotlib, Seaborn).\nBackground of Computer Science with excellent Data Science working experience.\nTechnical Experience:\nDevelop Machine Learning (Supervised/Unsupervised learning), Neural Networks (ANN, CNN, RNN, LSTM, Decision tree, Encoder, Decoder), Natural Language Processing, Generative AI (LLMs, Lang Chain, RAG, Vector Database) .\nHands-on Expertise in Python programming (OOPs concepts), SQL (relational/non-relational databases), experience in handling various data science libraries (Pandas, NumPy, SciPy, Sklearn, TensorFlow, Keras, Pytorch, etc.) would be a necessary requirement.\nExposure to Cloud technologies (e.g., Google Cloud/AWS/Azure), including executing Machine Learning algorithms on Cloud is necessary.\nExposure to Generative AI technologies.\nAbility to scope the problem statement, data preparation, training and making the AI/ML model production ready.\nWork with business partners to understand the problem statement, translate the same into analytical problem.\nAbility to manipulate structured and unstructured data.\nDevelop, test and improve existing machine learning models.\nAnalyse large and complex data sets to derive valuable insights.\nResearch and implement best practices to enhance existing machine learning infrastructure. Develop prototypes for future exploration.\nDesign and evaluate approaches for handling large volume of real data streams.\nAbility to determine appropriate analytical methods to be used.\nCollaborate with data engineers, solutions architects, application engineers, and product teams across time zones to develop data and model pipelines",Industry Type: Automobile,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'data science', 'Neural networks', 'Analytical', 'Machine learning', 'Natural language processing', 'data visualization', 'Analytics', 'SQL', 'Python']",2025-06-10 14:19:19
Data Scientist-Artificial Intelligence,IBM,3 - 7 years,Not Disclosed,['Bengaluru'],"As an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\n In this role, your responsibilities may include: \nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours’.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nProof of Concept (POC) DevelopmentDevelop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions.\nHelp in showcasing the ability of Gen AI code assistant to refactor/rewrite and document code from one language to another\nDocument solution architectures, design decisions, implementation details, and lessons learned.\nStay up to date with the latest trends and advancements in AI, foundation models, and large language models.\nEvaluate emerging technologies, tools, and frameworks to assess their potential impact on solution design and implementation\n\n\nPreferred technical and professional experience\nExperience and working knowledge in COBOL & JAVA would be preferred\nHaving experience in Code generation, code matching & code translation leveraging LLM capabilities would be a Big plus\nDemonstrate a growth mindset to understand clients' business processes and challenges",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['elastic search', 'java', 'proof of concept', 'cobol', 'splunk', 'targetlink', 'simulink', 'data management', 'stateflow', 'sil', 'big data', 'can bus', 'matlab', 'python', 'c', 'predictive', 'machine learning', 'presales', 'autosar', 'code generation', 'rtw', 'rfi', 'embedded c', 'model based development', 'rfp']",2025-06-10 14:19:22
Data Scientist-Advanced Analytics,IBM,3 - 7 years,Not Disclosed,['Bengaluru'],"As an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviour’s.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nProof of Concept (POC) DevelopmentDevelop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions.\nHelp in showcasing the ability of Gen AI code assistant to refactor/rewrite and document code from one language to another\nDocument solution architectures, design decisions, implementation details, and lessons learned.\nStay up to date with the latest trends and advancements in AI, foundation models, and large language models.\nEvaluate emerging technologies, tools, and frameworks to assess their potential impact on solution design and implementation\n\n\nPreferred technical and professional experience\nExperience and working knowledge in COBOL & JAVA would be preferred\nHaving experience in Code generation, code matching & code translation leveraging LLM capabilities would be a Big plus\nDemonstrate a growth mindset to understand clients' business processes and challenges",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['elastic search', 'java', 'code generation', 'cobol', 'splunk', 'matlab', 'time series analysis', 'financial analysis', 'simulink', 'python', 'c', 'predictive analytics', 'analytics data', 'stateflow', 'machine learning', 'financial projections', 'sil', 'embedded c', 'clustering', 'big data']",2025-06-10 14:19:24
Proactive Hiring For Databricks,HCLTech,5 - 10 years,Not Disclosed,"['Noida', 'Chennai', 'Bengaluru']","Responsibilities\nLead the design, development, and implementation of big data solutions using Apache Spark and Databricks.\nArchitect and optimize data pipelines and workflows to process large volumes of data efficiently.\nUtilize Databricks features such as Delta Lake, Databricks SQL, and Databricks Workflows to enhance data processing and analytics capabilities.",,,,"['apache spark', 'Databricks Engineer', 'SQL']",2025-06-10 14:19:27
Opening For DevOps Engineer,Randstad Digital,2 - 6 years,8-10 Lacs P.A.,['Chennai'],"Job Type - Third Party Payroll\nPayroll Company Name - Randstad Digital\nPosition - DevOps Engineer\nExperience - 3+Years\nLocation - Chennai only\nNotice Period - Immediate to 15days\nInterview Type - Virtual (Final Round Face2Face Interview)\nWork Mode - 5 days office\n\n\nIf you're interested, kindly share your updated resume and the following details to this below email ID or LinkedIn\n\n\nEmail Id: gayathri.nambi@randstaddigital.com\nLinkedIn: https://www.linkedin.com/in/gayathri-alagia-nambi-0827921a5/\n\n\nFull Name:\nPan Number:\nExperience:\nRelevant Experience:\nNotice period:\nCTC:\nExpected CTC:\nCurrent company:\nPayroll Company:\nLocation:\nPreferred location:\nOffer in hand : (Y/N)\nReason for Job Change:\n\nKey Responsibilities\nCollaborate closely with Product teams, Software, and Data engineers to maintain and develop tools and infrastructure, driving innovation and optimising existing and new product value streams.\nLead and advise on the modernisation of end-to-end ETL and real-time streaming pipelines, refining development processes, and introducing relevant technologies and tools.\nWork extensively with AWS cloud platforms and tools, applying Infrastructure as Code (IaC) practicesparticularly with Terraformand prioritising both cost optimisation and secure network configurations.\nEnsure robust security by implementing and managing cybersecurity frameworks and tools, including [specific security tools currently in use], to protect systems against threats.\nApply network knowledge to optimise and secure infrastructure, ensuring efficient communication and data flow across systems.\nImplement a proactive patch management strategy to keep all systems updated with the latest security patches.\nAdhere to change management protocols and version control standards, ensuring a secure code repository with reliable backup strategies for key intellectual property. (maybe we can rephrase it better)\nEffectively present and explain advanced technical designs to both technical and non-technical stakeholders.\n\nWhat we are looking for:\nEssential\nBachelor’s degree in software, network, or cybersecurity (or demonstrable equal experience)\nAt least 2 years’ experience in a DevOps Engineer role\nProven experience or certification in AWS\nProven experience in troubleshooting and debugging SQL, SNS, SQS, Kinesis, C# (.net Core), S3, FTP, AWS Lambda. HTTP level REST API experience (and previous API standards, e.g SOAP)\nProven experience in operating system administration (on both Wintel and Linux).\nProven experience in networking components and systems, including webservers, firewalls and network routing.\nFamiliarity with automation tooling (such as Jenkins) and runtime integrated analysis tooling (such as New Relic)\nFamiliarity with the design, development and maintenance of best-in-class analytical capabilities, including data warehousing (Redshift, OpenSearch, Athena, SQL, etc)\nFamiliarity of architectural design patterns for micro-services leveraging relational and big data technologies\nDisciplined, self-starter attitude driven to improve systems and processes and a willingness to learn\nExcellent documentation of processes\nProficiency in written and spoken English\nDesirable\nProvisioning new technical assets (e.g EC2 build), Kubernetes, Docker and associated virtualisation or containerisation technology.\nComplete familiarity with Agile development process\nExcellent documentation of processes",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['DevOps', 'Terraform', 'Docker', 'AWS', 'Kubernetes']",2025-06-10 14:19:30
Data Warehouse Engineer,Pando,6 - 11 years,20-35 Lacs P.A.,['Chennai'],"Technical Lead AI & Data Warehouse (DWH)\nPando is a global leader in supply chain technology, building the world's quickest time-to-value Fulfillment Cloud platform. Pandos Fulfillment Cloud provides manufacturers, retailers, and 3PLs with a single pane of glass to streamline end-to-end purchase order fulfillment and customer order fulfillment to improve service levels, reduce carbon footprint, and bring down costs. As a partner of choice for Fortune 500 enterprises globally, with a presence across APAC, the Middle East, and the US, Pando is recognized as a Technology Pioneer by the World Economic Forum (WEF), and as one of the fastest growing technology companies by Deloitte.\n\nRole\nAs the Senior Lead for AI and Data Warehouse at Pando, you will be responsible for building and scaling the data and AI services team. You will drive the design and implementation of highly scalable, modular, and reusable data pipelines, leveraging big data technologies and low-code implementations. This is a senior leadership position where you will work closely with cross-functional teams to deliver solutions that power advanced analytics, dashboards, and AI-based insights.\n\nKey Responsibilities\nLead the development of scalable, high-performance data pipelines using PySpark or Big Data ETL pipeline technologies.\nDrive data modeling efforts for analytics, dashboards, and knowledge graphs.\nOversee the implementation of parquet-based data lakes.\nWork on OLAP databases, ensuring optimal data structure for reporting and querying.\nArchitect and optimize large-scale enterprise big data implementations with a focus on modular and reusable low-code libraries.\nCollaborate with stakeholders to design and deliver AI and DWH solutions that align with\nbusiness needs.\nMentor and lead a team of engineers, building out the data and AI services organization.\n\nRequired\n8-10 years of experience in big data and AI technologies, with expertise in PySpark or similar Big Data ETL pipeline technologies.\nStrong proficiency in SQL and OLAP database technologies.\nFirsthand experience with data modeling for analytics, dashboards, and knowledge graphs.\nProven experience with parquet-based data lake implementations.\nExpertise in building highly scalable, high-volume data pipelines.\nExperience with modular, reusable, low-code-based implementations.\nInvolvement in large-scale enterprise big data implementations.\nInitiative-taker with strong motivation and the ability to lead a growing team.\n\nPreferred\nExperience leading a team or building out a new department.\nExperience with cloud-based data platforms and AI services.\nFamiliarity with supply chain technology or fulfilment platforms is a plus.",Industry Type: Film / Music / Entertainment,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Agentic Ai', 'Pyspark', 'Generative Ai', 'Data Pipeline', 'Artificial Intelligence', 'AI', 'Data Warehousing', 'Data Modeling', 'ETL', 'AWS']",2025-06-10 14:19:32
Data Analytics and Applied AI Engineer,Nvidia,4 - 8 years,Not Disclosed,['Bengaluru'],"NVIDIA has continuously reinvented itself. Our invention of the GPU sparked the growth of the PC gaming market, redefined modern computer graphics, and revolutionized parallel computing. Today, research in artificial intelligence is booming worldwide, which calls for highly scalable and massively parallel computation horsepower that NVIDIA GPUs excel.\nNVIDIA is a learning machine that constantly evolves by adapting to new opportunities that are hard to solve, that only we can address, and that matter to the world. This is our life s work , to amplify human creativity and intelligence. As an NVIDIAN, you ll be immersed in a diverse, supportive environment where everyone is inspired to do their best work. Come join our diverse team and see how you can make a lasting impact on the world! Design-for-Test Engineering at NVIDIA works on groundbreaking innovations involving crafting creative solutions for DFT architecture, verification, and post-silicon validation on some of the industrys most complex semiconductor chips.\nWhat youll be doing:\nAs an integral member in our team, you will work on exploring Applied AI solutions for DFX and VLSI problem statements.\nArchitect end-to-end generative AI solutions with a focus on LLMs, RAGs Agentic AI workflows.\nWork on deploying predictive ML models for efficient Silicon Lifecycle Management of NVIDIAs chips.\nCollaborate closely with various VLSI DFX teams to understand their language-related engineering challenges and design tailored solutions.\nPartner closely with cross-functional AI teams to provide feedback and contribute to the evolution of generative AI technologies.\nWork closely with DFX teams to integrate Agentic AI workflows into their applications and systems and stay abreast of the latest developments in language models and generative AI technologies.\nDefine how data will be collected, stored, consumed and managed for next-generation AI use cases.\nYou will also help mentor junior engineers on test designs and trade-offs including cost and quality.\nWhat we need to see:\nBSEE or MSEE from reputed institutions with 2+ years of experience in DFT, VLSI Applied Machine Learning\nExperience in Applied ML solutions for chip design problems\nSignificant experience in deploying generative AI solutions for engineering use cases\nGood understanding of fundamental DFT VLSI concepts - ATPG, scan, RTL clocks design, STA, place-n-route and power\nExperience in application of AI for EDA-related problem-solving is a plus\nExcellent knowledge in using statistical tools for data analysis insights\nStrong programming and scripting skills in Perl, Python, C++ or TCL desired\nStrong organization and time management skills to work in a fast-pace multi-task environment\nSelf-motivated, independent, ability to work independently with minimal day-to-day direction\nOutstanding written and oral communication skills with the curiosity to work on rare challenges\nNVIDIA offers highly competitive salaries and a comprehensive benefits package. We have some of the most brilliant and talented people in the world working for us and, due to unprecedented growth, our world-class engineering teams are growing fast. If youre a creative and autonomous engineer with real passion for technology, we want to hear from you!\n#LI-Hybrid",Industry Type: Electronic Components / Semiconductors,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['C++', 'Data analysis', 'Semiconductor', 'DFT', 'VLSI', 'Chip design', 'Machine learning', 'Perl', 'Gaming', 'Python']",2025-06-10 14:19:33
Java Developer,Repro,3 - 6 years,8-10 Lacs P.A.,"['Noida', 'Greater Noida']","Role & responsibilities :-\nDevelop robust and scalable applications based on needs (new requirements/bug fixes/enhancements)\nWrite clean, readable, portable, reliable codes and review the same\nDeliberate/build/develop and implement solutions for automated unit testing/ functional testing/ load testing in TDD environment\nCollaborate with other development team, product team, vendor partners for efficient integration of the application modules\nDeploy and configure products in cloud environment\nPrepare & maintain product and release documents\nProperly maintain release versions, release plans and repository\n\n\nPreferred candidate profile :-\nExperience of back end technology stack mainly in Java (Must)\nHands-on experience in Microservices, Service oriented development, Spring Boot, REST APIs (Must)\nGood experience building and deploying applications in cloud environment like AWS/Azure (Must)\nExperience with multi-threading and concurrency programming (Must)\nExperience/knowledge of Elastic search, Kibana, Alfresco (Preferred)\nGood experience with structured and unstructured databases/DBMS (Preferred)\nHands-on/knowledge of front-end technology like HTML, CSS, Javascript, Bootstrap (Preferred)\nWorked on building scalable products, where large scale systems are built on low latency (Desired)\nAptitude for system design and ability to write clean, readable, portable, and reliable code (Desired)",Industry Type: Printing & Publishing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Rest', 'Spring Boot', 'Microservices']",2025-06-10 14:19:36
Lead AVP - Business Consulting,Hsbc,14 - 16 years,Not Disclosed,['Bengaluru'],"We are currently seeking an experienced professional to join our team in the role of Lead Assistant Vice President Business Consulting\nPrincipal responsibilities\nAbility to convert business problem to an analytical problem and then finding pertinent solutions\nOverall business understanding of BFSI domain\nProviding high-quality analysis and recommendations to business problems.\nEfficient project management and delivery\nAbility to conceptualize data driven solutions for the business problem at hand for multiple businesses/region to facilitate efficient decision making\nFocus on driving efficiency gains and enhancement of processes.\nUse of data to improve customer outcomes through the provision of insight and challenge\nAlso drive business benefit through self-initiated projects\nThe role may require one to liaise with different stakeholders business teams, business analysts, product owners, data engineers, platform developers etc.\nThe person should demonstrate leadership qualities in terms of bringing the junior members up the learning curve and handholding them whenever necessary\nWorking with an Agile mindset, using regular project management tools like Jira, Confluence, GitHub etc. to share updates and remaining on top of the work\nRequirements\nMaster s degree from reputed university in Computer Science, Information Technology or any other quantitative fields\nExperience in managing significant data volume with in-depth knowledge of relational database management along with Consulting experience in the data analytics domain.\nExcellent Project Management skill is required.\nThorough knowledge in SQL and Python must. Having knowledge/experience in Pyspark would be preferred. Knowledge of SAS is a plus\nKnowledge of Big Data is required.\nKnowledge in Google Cloud Platform or any On-Prem and corresponding tooling solutions would be preferred\nAdvanced knowledge of MS Excel including macro based excel sheets.\nStrong analytical skills with business analysis experience or equivalent.\nKnowledge and understanding of financial-services/ banking-operations preferred.\nAdvanced capability in translating complex and/or technical work and insights into straightforward business language.\nExcellent interpersonal skills with highly developed capacity to constructively challenge and influence management.\nProven stakeholder management capabilities\nProven problem solving skills with ability to consider alternative and innovative solutions\nBesides strong written and verbal communication skills the incumbent is also expected to have strong presentation skills",Industry Type: Consumer Electronics & Appliances,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'SAS', 'Business analysis', 'Project management', 'Consulting', 'Agile', 'Stakeholder management', 'Information technology', 'Financial services', 'SQL']",2025-06-10 14:19:38
Application Developer-Cloud FullStack,IBM,3 - 7 years,Not Disclosed,['Chennai'],"As an Associate Software Developer at IBM, you'll work with clients to co-create\nsolutions to major real-world challenges by using best practice technologies, tools,\ntechniques, and products to translate system requirements into the design and\ndevelopment of customized systems\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nCore Java,\nSpring Boot, Java2/EE,\nMicrosservices - Hadoop Ecosystem (HBase, Hive, MapReduce, HDFS, Pig, Sqoop etc)\nSpark Good to have Python\n\n\nPreferred technical and professional experience\nNone",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'apache pig', 'spark', 'hadoop', 'hbase', 'statutory compliance', 'ecc', 'training', 'hibernate', 'docker', 'microservices', 'spring', 'talent acquisition', 'ees', 'java', 'apache', 'recruitment', 'od', 'pms', 'linux', 'jenkins', 'mysql', 'python', 'ir', 'microsoft azure', 'hrsd', 'er', 'spring boot', 'mapreduce', 'sqoop', 'aws']",2025-06-10 14:19:40
Engineer - Investment Data Platform - 3+ Years - Pune,Crescendo Global,3 - 6 years,Not Disclosed,['Pune'],"Engineer - Investment Data Platform - 3+ Years - Pune\n\nWe are hiring a skilled Engineer to join the Investment Data Platform in Pune in Financial services. If you're passionate about data software and engineering and delivering high-quality software solutions using Azure and .Net technologies, this opportunity is for you.\n\nLocation: Pune\n\nYour Future Employer: Our client is a leading financial services firm with a global presence. They are committed to creating an inclusive and diverse workplace where all employees feel valued and have the opportunity to reach their full potential.\n\nResponsibilities:\nDeveloping and maintain software solutions aligned with business outcomes.\nCollaborating within agile teams to review user stories and implement features.\nMaintaining existing data platform artefacts and contribute to continuous improvement.\nBuilding scalable, robust software adhering to data engineering best practices.\nSupporting development of data ingestion, modeling, transformation, and deployment pipelines.\n\nRequirements:\n3+ years of experience in software engineering and 2+ years in data engineering.\nProficiency in C#, .Net Framework, SQL; exposure to Python, Java, PowerShell, or JavaScript.\nExperience with Azure Data Factory, CI/CD pipelines, and DevOps principles.\nStrong interpersonal and communication skills.\nBachelor's degree in computer science, engineering, finance or related field\n\nWhat is in it for you:\nJoin a high-performing team at a global investment leader\nExposure to cutting-edge Azure data platform technologies\nCompetitive compensation with hybrid work flexibility\n\nReach us: If you think this role is aligned with your career, kindly write to me along with your updated CV at aayushi.goyal@crescendogroup.in for a confidential discussion on the role.\n\nDisclaimer: Crescendo Global specializes in Senior to C-level niche recruitment. We are passionate about empowering job seekers and employers with an engaging and memorable job search and leadership hiring experience. Crescendo Global does not discriminate based on race, religion, color, origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\n\nNote: We receive a lot of applications daily, so it may not be possible to respond to each one individually. Please assume that your profile has not been shortlisted if you don't hear from us in a week. Thank you for your understanding.\n\nScammers can misuse Crescendo Globals name for fake job offers. We never ask for money, purchases, or system upgrades. Verify all opportunities at www.crescendo-global.com and report fraud immediately. Stay alert!\n\nProfile Keywords: Azure Data Engineering, C# Developer, .Net Engineer, SQL Data Engineer, DevOps Data, Databricks Jobs, Data Ingestion Engineer, Financial Services Tech Jobs, Asset Management IT, Financial Services",Industry Type: Financial Services (Asset Management),Department: Other,"Employment Type: Full Time, Permanent","['C#', 'dot net', 'SQL', 'Financial markets', 'asset management process', 'Azure Databricks', 'Azure DevOps']",2025-06-10 14:19:42
Sr. AI Consultant /JMR Infotech /Bangalore,JMR Infotech,4 - 9 years,15-25 Lacs P.A.,['Bengaluru'],"Role & responsibilities\n\nJob Description\nJob Overview We are seeking an experienced AI/ML Consultant with 8-12 years of experience to lead the design, development, and deployment of AI and Machine Learning solutions for enterprise-level projects. The ideal candidate will have strong expertise in deep learning, machine learning algorithms, cloud AI services, and MLOps, with a focus on solving real-world business challenges using AI. Key Responsibilities AI/ML Strategy & Consulting • Work closely with stakeholders to understand business challenges and identify opportunities where AI/ML can drive value. • Develop AI/ML roadmaps, proof-of-concept models, and scalable AI strategies. • Advise on AI governance, model explainability, and ethical AI practices. Model Development & Deployment • Design, build, and optimize machine learning models for classification, regression, NLP, computer vision, and recommendation systems. • Implement and fine-tune deep learning architectures (CNNs, RNNs, Transformers). • Work with large-scale datasets for feature engineering, data preprocessing, and model training. • Deploy models using MLOps best practices, ensuring efficient monitoring, retraining, and lifecycle management. Large Language Model (LLM) Development & AI Agents • Fine-tune, train, and deploy LLMs (GPT-4, LLaMA, Mistral, Falcon, Claude, Gemini) for chatbots, knowledge management, and intelligent automation. • Implement Retrieval-Augmented Generation (RAG) to improve response accuracy and integrate knowledge bases. • Develop AI Agents that can handle complex workflows, automate customer interactions, and execute multi-step reasoning. • Experiment with multi-modal AI (text, image, and video) for industry-specific use cases. Technical Leadership & Innovation • Lead AI/ML teams in building scalable solutions using Python, TensorFlow, PyTorch, and cloud platforms (AWS/GCP/Azure). • Stay up to date with AI research and recommend cutting-edge solutions for business applications. • Implement best practices in model interpretability, fairness, and bias mitigation. Collaboration & Knowledge Sharing • Work with cross-functional teams, including data engineers, software developers, and business analysts, to integrate AI models into production systems. • Mentor junior AI engineers and provide technical guidance on ML frameworks and architectures. • Conduct workshops and knowledge-sharing sessions on AI trends and best practices.\nMandatory Qualifications\nPreferred Qualifications: • Masters or Ph.D. in Computer Science, AI/ML, Data Science, or related fields. • Experience with generative AI models/LLMs • AI certifications from AWS, Google, or Microsoft.\nMandatory Skills\nRequired Qualifications & Skills Technical Skills: • Programming: Python, R, SQL, Java (optional). • ML Frameworks: TensorFlow, PyTorch, Scikit-learn, XGBoost. • Data Handling: Pandas, NumPy, Spark, Snowflake. • MLOps & Deployment: Docker, Kubernetes, MLflow, TensorFlow Serving, FastAPI. • Cloud Services: AWS (SageMaker, Lambda, S3), GCP (Vertex AI), Azure (ML Studio). • LLMs & Generative AI: GPT-4, LLaMA, Claude, Gemini, Mistral, Falcon. • NLP & Transformers: Hugging Face, LangChain, OpenAI API, Sentence Transformers. • Vector Search & RAG: FAISS, ChromaDB, Pinecone, Weaviate Soft Skills: • Strong analytical and problem-solving abilities. • Ability to communicate complex AI concepts to non-technical stakeholders. • Experience working in agile environments. • Ability to lead AI initiatives and mentor teams.\nDesirable Skills\nRequired Qualifications & Skills Technical Skills: • Programming: Python, R, SQL, Java (optional). • ML Frameworks: TensorFlow, PyTorch, Scikit-learn, XGBoost. • Data Handling: Pandas, NumPy, Spark, Snowflake. • MLOps & Deployment: Docker, Kubernetes, MLflow, TensorFlow Serving, FastAPI. • Cloud Services: AWS (SageMaker, Lambda, S3), GCP (Vertex AI), Azure (ML Studio). • LLMs & Generative AI: GPT-4, LLaMA, Claude, Gemini, Mistral, Falcon. • NLP & Transformers: Hugging Face, LangChain, OpenAI API, Sentence Transformers. • Vector Search & RAG: FAISS, ChromaDB, Pinecone, Weaviate Soft Skills: • Strong analytical and problem-solving abilities. • Ability to communicate complex AI concepts to non-technical stakeholders. • Experience working in agile environments. • Ability to lead AI initiatives and mentor teams.\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['PyTorch', 'Artificial Intelligence', 'Machine Learning', 'Python', 'TensorFlow', 'Scikit-learn', 'SQL']",2025-06-10 14:19:45
Analytics Data Engineer (Infrastructure as Code),Unison Consulting,2 - 4 years,Not Disclosed,['Mumbai'],"IAC\ndeploy scalable, secure, and high-performing Snowflake environments in line with data governance and security in palce using Terraform and other automation scripit\nAutomate infrastructure provisioning, testing, and deployment for seamless operations.\nStrong SQL & DBT Expertise\nExperience building and maintaining scalable data models in DBT .\nProficient in modular SQL , Jinja templating , testing strategies, and DBT best practices.\nData Warehouse Proficiency\nHands-on experience with Snowflake including:\nDimensional and data vault modeling (star/snowflake schemas)\nPerformance optimization and query tuning\nRole-based access and security management\nData Pipeline & Integration Tools\nExperience with Kafka (or similar event streaming tools) for ingesting real-time data.\nFamiliarity with SnapLogic for ETL/ELT workflow design, orchestration, and monitoring.\nVersion Control & Automation\nProficient in Git and GitHub for code versioning and collaboration.\nExperience with GitHub Actions or other CI/CD tools to automate DBT model testing, deployment, and documentation updates.\nData Quality & Governance\nStrong understanding of data validation, testing (e.g., dbt tests), and lineage tracking.\nEmphasis on maintaining data trust across pipelines and models.\nStakeholder Management\nPartner with business and technical stakeholders to define data needs and deliver insights.\nAbility to explain complex data concepts in clear, non-technical terms.\nDocumentation & Communication\nMaintain clear documentation for models, metrics, and data transformations (using DBT docs or similar).\nStrong verbal and written communication skills; able to work cross-functionally across teams.\nProblem-Solving & Ownership\nProactive in identifying and resolving data gaps or issues.\nSelf-starter with a continuous improvement mindset and a focus on delivering business value through data.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Version control', 'GIT', 'Security management', 'Workflow', 'Data quality', 'Analytics', 'Monitoring', 'Testing', 'SQL']",2025-06-10 14:19:48
Sr. Software Development Engineer,Acentra Health,5 - 7 years,Not Disclosed,['Chennai'],"Design,develop, and maintain scalable data pipelines and systems to support thecollection, integration, and analysis of healthcare and enterprise data. Theprimary responsibilities of this role include designing and implementingefficient data pipelines, architecting robust data models, and adhering to datamanagement best practices. In this position, you will play a crucial part intransforming raw data into meaningful insights, through development of semanticdata layers, enabling data-driven decision-making across the organization. Theideal candidate will possess strong technical skills, a keen understanding ofdata architecture, and a passion for optimizing data processes.",,,,"['snow flake schema', 'project management', 'python', 'hipaa', 'amazon redshift', 'microsoft azure', 'data warehousing', 'dms', 'elt', 'sql', 'star schema', 'data modeling', 'spark', 'gcp', 'written communication', 'leadership', 'writing', 'hadoop', 'bigquery', 'aws', 'etl', 'programming', 'communication skills']",2025-06-10 14:19:50
Machine Learning Engineer,Catalyst Clinical Research,5 - 10 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","{""company"":""\nCatalyst Clinical Research provides customizable solutions to the biopharmaceutical and biotechnology industries through , a full-service oncology CRO, and multi-therapeutic global functional and CRO services through . The companys customer-centric flexible service model, innovative technology, expert team members, and global presence advance clinical studies. Visit .\n\nThe Machine Learning Engineer is a pivotal contributor responsible for designing and implementing cutting-edge machine learning solutions with a focus on generative AI technologies. You will drive the development and deployment of advanced models and pipelines that enable the creation of AI-driven applications and enhance organizational decision-making capabilities. Additionally, you will support data engineering initiatives to enable utilization of data across the organization. Collaborating closely with internal and external stakeholders, you will translate complex requirements into innovative solutions that advance Catalysts AI strategies while ensuring alignment with broader enterprise goals.\n"",""role"":""\nPosition Responsibilities/ Accountabilities:\n\nDesign, build, and optimize machine learning workflows, with a focus on generative AI models such as large language models (LLMs) and diffusion-based architectures.\nDevelop and deploy scalable machine learning pipelines using frameworks like TensorFlow, PyTorch, and Databricks MLflow.\nDevelop AI solutions using tools like Azure AI/Copilot Studio and Databricks AI Builder.\nLead the creation of domain-specific generative AI models, ensuring ethical AI practices and bias mitigation throughout the model lifecycle.\nDesign, build, and maintain scalable data pipelines with Delta Live Tables for model integration into enterprise applications.\nEnhance and expand CI/CD strategies for automated testing, model monitoring, and continuous delivery of ML artifacts.\nManage data preprocessing, feature engineering, and synthetic data generation for machine learning use cases.\nCollaborate with cross-functional teams to align AI-driven solutions with business goals and ensure high availability for end-to-end systems.\nProvide technical expertise in the exploration of novel generative AI methods, tools, and frameworks.\nSupport team members in understanding data science and AI best practices, encouraging a culture of innovation and continuous learning. Represent AI as a key member of the Data & Architecture Review Committee.\n\nPosition Qualification Requirements :\nEducation\n: B.S. or M.S. Computer Science, Engineering, Economics, Mathematics, related field, or relevant experience.\n\nExperience:\n5+ years of experience in machine learning engineering, including model development and deployment.\nHands-on experience with generative AI models (e.g., GPT, GANs, VAEs) and frameworks like PyTorch or TensorFlow.\n5+ years of experience with cloud computing technologies (Azure, AWS, GCP), especially AI and ML services.\nProficiency in developing data pipelines and integrating ML models into production environments.\nExpertise in model evaluation and monitoring, including techniques for explainability and fairness in AI.\nExperience collaborating with DevOps and MLOps teams to ensure scalability and reliability of AI solutions.\nFamiliarity with project management tools such as JIRA.\n\nRequired Skills:\nAdvanced proficiency in Python or PySpark for ML applications.\nDeep understanding of generative AI principles, model architecture, and training methodologies.\nExpertise in large-scale data processing and engineering using Spark, Kafka, and Databricks.\nProficiency with big data technologies and data structures like delta, parquet, YAML, JSON, and HTML.\nStrong knowledge of cloud-based AI platforms (e.g. Databricks, Azure ML, etc).\nSolid understanding of machine learning pipelines and MLOps practices.\nExceptional problem-solving and analytical skills.\nAbility to manage priorities and workflow effectively.\nProven ability to handle multiple projects and meet tight deadlines.\nStrong interpersonal skills with an ability to work collaboratively across teams.\nCommitment to excellence and high standards.\nCreative, flexible, and innovative team player.\nAbility to work independently and as part of various committees and teams.\nNice to Have: Data Engineering experience, including Webhooks, API, ELT/ETL, rETL, Data Lakehouse Architecture, and Event-Driven Architectures.\n\nFamiliarity with deep learning frameworks for generative AI (e.g., Hugging Face Transformers).\nKnowledge of synthetic data generation techniques and tools.\nExperience with data visualization tools (e.g., Tableau, Power BI) for AI model interpretability.\nFamiliarity with ethical AI principles, including explainability and bias reduction strategies.\nExperience with containerization and orchestration tools like Docker and Kubernetes.\nBackground or familiarity with clinical trials or pharmaceutical development.\n\nWorking Hours\nEveryday: 1:30 PM - 9:00 PM IST\nOR\nMonday, Wednesday, Friday: 2:30 PM - 10:30 PM IST\nTuesday, Thursday: 9:00 AM - 5:00 PM IST\nNote: Working hours may vary based on individual seniority, business demand, and ability to work independently. This will be evaluated on a case-by-case basis.\n""},""",Industry Type: Miscellaneous,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'CRO', 'Project management', 'Pharma', 'Clinical trials', 'Clinical research', 'Data processing', 'Workflow', 'HTML', 'Monitoring']",2025-06-10 14:19:52
Python Backend Developer,Derisk360,6 - 11 years,Not Disclosed,"['Chennai', 'Gurugram', 'Bengaluru']","Join our engineering team as a Senior Backend Engineer and lead the development of cloud-native, scalable microservices RESTful APIs using modern Python frameworks . Youll work with CI/CD tools to build robust backend systems powering next-gen platforms. If you have hands-on experience with , and are skilled in distributed systems relational/NoSQL databases , we want to hear from you. Key Responsibilities: Microservices Development Design, build, and optimize microservices architecture using patterns like Service Discovery Circuit Breaker API Gateway Saga orchestration REST API Engineering Develop high-performance frameworks like Django REST Framework Cloud-Native Backend Systems Build and deploy containerized applications . Familiarity with Kubernetes (K8s) for orchestration is a plus. CI/CD Automation Create and maintain DevOps pipelines GitLab CI/CD GitHub Actions for automated testing and deployment. Source Code Management Collaborate through Git-based version control , ensuring code quality via pull requests peer reviews on platforms like Event-Driven Architecture Implement and manage data streaming messaging pipelines Apache Kafka Amazon Kinesis , or equivalent. Database Engineering Work with , and optionally solutions such as Cloud Infrastructure Architect and manage AWS backend services Big Data Integration (Desirable) for distributed data processing and scalable ETL workflows in data engineering Polyglot Collaboration Integrate with backend services or data processors developed in , or other enterprise technologies. Required Skills & Qualifications: Bachelors or Masters in Computer Science Software Engineering , or a related technical field. 6+ years in backend development Proven expertise in API development cloud-native applications Proficiency in database schema design , and query optimization. Strong grasp of DevOps best practices Git workflows code quality standards Experience with streaming platforms message queues event-driven design Nice to Have: Exposure to big data tools (e.g., Familiarity with Agile/Scrum methodologies cross-functional teams Competitive salary and performance-based bonuses Opportunity to build next-gen backend platforms for global-scale applications. Work with a team that values",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Backend', 'Version control', 'MySQL', 'Schema', 'Apache', 'Distribution system', 'SQL', 'Python']",2025-06-10 14:19:55
Senior Big Data Developer - Youappi,Affle,7 - 10 years,Not Disclosed,"['Gurugram', 'Bengaluru']","We are looking for an experienced Senior Big Data Developer to join our team and help build and optimize high-performance, scalable, and resilient data processing systems. You will work in a fast-paced startup environment, handling highly loaded systems and developing data pipelines that process billions of records in real time.\nAs a key member of the Big Data team, you will be responsible for architecting and optimizing distributed systems, leveraging modern cloud-native technologies, and ensuring high availability and fault tolerance in our data infrastructure.\nPrimary Responsibilities:\nDesign, develop, and maintain real-time and batch processing pipelines using Apache Spark, Kafka, and Kubernetes.\nArchitect high-throughput distributed systems that handle large-scale data ingestion and processing.\nWork extensively with AWS services, including Kinesis, DynamoDB, ECS, S3, and Lambda.\nManage and optimize containerized workloads using Kubernetes (EKS) and ECS.\nImplement Kafka-based event-driven architectures to support scalable, low-latency applications.\nEnsure high availability, fault tolerance, and resilience of data pipelines.\nWork with MySQL, Elasticsearch, Aerospike, Redis, and DynamoDB to store and retrieve massive datasets efficiently.\nAutomate infrastructure provisioning and deployment using Terraform, Helm, or CloudFormation.\nOptimize system performance, monitor production issues, and ensure efficient resource utilization.\nCollaborate with data scientists, backend engineers, and DevOps teams to support advanced analytics and machine learning initiatives.\nContinuously improve and modernize the data architecture to support growing business needs.\nRequired Skills:\n7-10+ years of experience in big data engineering or distributed systems development.\nExpert-level proficiency in Scala, Java, or Python.\nDeep understanding of Kafka, Spark, and Kubernetes in large-scale environments.\nStrong hands-on experience with AWS (Kinesis, DynamoDB, ECS, S3, etc.).\nProven experience working with highly loaded, low-latency distributed systems.\nExperience with Kafka, Kinesis, Flink, or other streaming technologies for event-driven architectures.\nExpertise in SQL and database optimizations for MySQL, Elasticsearch, and NoSQL stores.\nStrong experience in automating infrastructure using Terraform, Helm, or CloudFormation.\nExperience managing production-grade Kubernetes clusters (EKS).\nDeep knowledge of performance tuning, caching strategies, and data consistency models.\nExperience working in a startup environment, adapting to rapid changes and building scalable solutions from scratch.\nNice to Have\nExperience with machine learning pipelines and AI-driven analytics.\nKnowledge of workflow orchestration tools such as Apache Airflow.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['kubernetes', 'python', 'aws iam', 'performance tuning', 'dynamo db', 'scala', 'data engineering', 'redis', 'helm', 'sql', 'nosql', 'database optimization', 'elastic search', 'java', 'system', 'lambda expressions', 'ecs', 'spark', 'automating', 'kafka', 'caching techniques', 'mysql', 'aws', 'big data']",2025-06-10 14:19:58
Gcp Data Engineer,One Of the MNC company,6 - 8 years,Not Disclosed,"['Hyderabad', 'Pune', 'Mumbai (All Areas)']","Role & responsibilities\n\nDevelop, implement, and optimize ETL/ELT pipelines for processing large datasets efficiently.\n• Work extensively with BigQuery for data processing, querying, and optimization.\n• Utilize Cloud Storage, Cloud Logging, Dataproc, and Pub/Sub for data ingestion, storage, and event-driven processing.\n• Perform performance tuning and testing of the ELT platform to ensure high efficiency and scalability.\n• Debug technical issues, perform root cause analysis, and provide solutions for production incidents.\n• Ensure data quality, accuracy, and integrity across data pipelines.\n• Collaborate with cross-functional teams to define technical requirements and deliver solutions.\n• Work independently on assigned tasks while maintaining high levels of productivity and efficiency.\nSkills Required:\n• Proficiency in SQL and PL/SQL for querying and manipulating data.\n• Experience in Python for data processing and automation.\n• Hands-on experience with Google Cloud Platform (GCP), particularly:\no BigQuery (must-have)\no Cloud Storage\no Cloud Logging\no Dataproc\no Pub/Sub\n• Experience with GitHub and CI/CD pipelines for automation and deployment.\n• Performance tuning and performance testing of ELT processes.\n• Strong analytical and debugging skills to resolve data and pipeline issues efficiently.\n• Self-motivated and able to work independently as an individual contributor.\n• Good understanding of data modeling, database design, and data warehousing concepts.\n\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GCP', 'Bigquery', 'ETL', 'PL/SQL', 'SQL']",2025-06-10 14:20:00
Chief Analytics Office (CAO) - Data Scientist,IBM,2 - 4 years,Not Disclosed,['Bengaluru'],"Role Overview: \n\nAs a Data Scientist within IBM's Chief Analytics Office, you will support AI-driven projects across the enterprise. You will apply your technical skills in AI, machine learning, and data analytics to assist in implementing data-driven solutions that align with business goals. This role involves working with team members to translate data insights into actionable recommendations.\n\n  \n\n Key Responsibilities: \n Technical Execution and Leadership: \nDevelop and deploy AI models and data analytics solutions.\nSupport the implementation and optimization of AI-driven strategies per business stakeholder requirements.\nHelp refine data-driven methodologies for transformation projects.\n Data Science and AI: \nDesign and implement machine learning solutions and statistical models, from problem formulation through deployment, to analyze complex datasets and generate actionable insights.\nLearn and utilize cloud platforms to ensure the scalability of AI solutions.\nLeverage reusable assets and apply IBM standards for data science and development.\n Project Support: \nLead and contribute to various stages of AI and data science projects, from data exploration to model development.\nMonitor project timelines and help resolve technical challenges.\nDesign and implement measurement frameworks to benchmark AI solutions, quantifying business impact through KPIs.\n Collaboration: \nEnsure alignment to stakeholders’ strategic direction and tactical needs.\nWork with data engineers, software developers, and other team members to integrate AI solutions into existing systems.\nContribute technical expertise to cross-functional teams.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n Education: Bachelor’s or Master’s in Computer Science, Data Science, Statistics, or a related field is required; an advanced degree strongly preferred\n\n Experience: \n2-4 yearsof experience in data science, AI, or analytics with a focus on implementing data-driven solutions\nExperience with data cleaning, data analysis, A/B testing, and data visualization\nExperience with AI technologies through coursework or projects\n\n\n Technical\n\nSkills:\n \nProficiency in SQL and Python for performing data analysis and developing machine learning models\nKnowledge of common machine learning algorithms and frameworkslinear regression, decision trees, random forests, gradient boosting (e.g., XGBoost, LightGBM), neural networks, and deep learning frameworks such as TensorFlow and PyTorch\nExperience with cloud-based platforms and data processing frameworks\nUnderstanding of large language models (LLMs)\nFamiliarity with IBM’s watsonx product suite\nFamiliarity with object-oriented programming\n\n\n Analytical\n\nSkills:\n \nStrong problem-solving abilities and eagerness to learn\nAbility to work with datasets and derive insights\n\n\n Other : \nGood communication skills, with the ability to explain technical concepts clearly\nEnthusiasm for learning and applying new technologies\nStrong project management skills, with the ability to balance multiple initiatives, prioritize tasks effectively, and meet deadlines in a fast-paced environment\nSuccessful completion of Coding Assessment\n\n\nPreferred technical and professional experience\n\nAdvanced degree strongly preferred",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'project management', 'artificial intelligence', 'sql', 'data science', 'data analytics', 'data analysis', 'neural networks', 'random forest', 'machine learning', 'data cleansing', 'tensorflow', 'ab testing', 'pytorch', 'statistical modeling', 'data visualization', 'statistics']",2025-06-10 14:20:03
Data Scientist Lead - L1,Wipro,4 - 9 years,Not Disclosed,['Bengaluru'],"About The Role  \n\nRole Purpose\n\nThe purpose of the role is to define, architect and lead delivery of machine learning and AI solutions\n\n ? \n\nDo\n\n1. Demand generation through support in Solution development\n\na. Support Go-To-Market strategy\n\ni. Collaborate with sales, pre-sales &consulting team to assist in creating solutions and propositions for proactive demand generation\n\nii. Contribute to development solutions, proof of concepts aligned to key offerings to enable solution led sales\n\nb. Collaborate with different colleges and institutes for recruitment, joint research initiatives and provide data science courses\n\n2. Revenue generation through Building & operationalizing Machine Learning, Deep Learning solutions\n\na. Develop Machine Learning / Deep learning models for decision augmentation or for automation solutions\n\nb. Collaborate with ML Engineers, Data engineers and IT to evaluate ML deployment options\n\nc. Integrate model performance management tools into the current business infrastructure\n\n3. Team Management\n\na. Resourcing\n\ni. Support recruitment process to on-board right resources for the team\n\nb. Talent Management\n\ni. Support on boarding and training for the team members to enhance capability & effectiveness\n\nii. Manage team attrition\n\nc. Performance Management\n\ni. Conduct timely performance reviews and provide constructive feedback to own direct reports\n\nii. Be a role model to team for five habits\n\niii. Ensure that the Performance Nxt is followed for the entire team\n\nd. Employee Satisfaction and Engagement\n\ni. Lead and drive engagement initiatives for the team\n\n ? \n\nDeliver\n\n\nNo.\n\nPerformance Parameter\n\nMeasure 1. Demand generation Order booking 2. Revenue generation through delivery Timeliness, customer success stories, customer use cases 3. Capability Building & Team Management % trained on new skills, Team attrition %\n\n\n ? \n\n ? \nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['team management', 'machine learning', 'deep learning', 'data science', 'performance management', 'python', 'natural language processing', 'neural networks', 'ml deployment', 'data engineering', 'artificial intelligence', 'sql', 'tensorflow', 'predictive modeling', 'statistical modeling', 'ml']",2025-06-10 14:20:06
Mis Business Analyst,Phonepe,3 - 6 years,Not Disclosed,['Bengaluru'],"Role & responsibilities:\n1. Data Collection and Integration: Gather and integrate data from multiple internal and external sources.\nEnsure data accuracy, integrity, and consistency across various data streams.\nDevelop and maintain databases and data systems necessary for projects and department functions.\n2. Data Analysis and Reporting: Analyze complex data sets to identify trends, patterns, and insights\n. Prepare detailed business views and reports, highlighting key metrics and performance indicators.\nCreate dashboards and visualizations to present data in an understandable and actionable manner.\n3. Business Metrics and Health Monitoring: Define and track key performance indicators (KPIs) to monitor business health.\nDevelop methodologies for measuring and reporting on business performance.\nRegularly update and maintain reports and dashboards to reflect current business status.\n4. Representation and Communication:\nRepresent the team in various forums, including management meetings, strategy sessions, and cross-functional working groups.\nCommunicate findings, insights, and recommendations effectively to stakeholders at all levels. Ensure that the team's work is visible and understood across the organization.\n5. Collaboration and Stakeholder Management: Work closely with different departments to understand their data needs and provide necessary support.\nCollaborate with IT and data engineering teams to ensure seamless data flow and integration. Foster strong relationships with key stakeholders to facilitate effective communication and collaboration.\n6. Continuous Improvement:\nStay updated with the latest industry trends, tools, and technologies in data analytics and business intelligence.\nPropose and implement process improvements to enhance data quality and reporting efficiency.\nParticipate in professional development opportunities to expand skillset and knowledge bas\n\n\nPreferred candidate profile",Industry Type: FinTech / Payments,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Excel', 'MIS', 'data', 'powerbi', 'Management Information System', 'data analyst', 'sql']",2025-06-10 14:20:08
Business Intelligence Engineer,Amazon,5 - 10 years,Not Disclosed,['Bengaluru'],"Amazon s Spectrum Analytics team is looking for a Business Intelligence Engineer to help build the next generation of analytics solutions for Selling Partner Developer Services. This is an opportunity to get in on the ground floor as we transform from a reactive, request-directed team to a proactive, roadmap-driven organization that accelerates the business.\nWe need someone who is passionate about data and the insights that large amounts of data can provide. In addition to broad experience with data technologies from ingestion to visualization and consumption (e.g. data pipelines, ETL, reporting and dashboarding), the ideal candidate will have strong analysis skills and an insatiable curiosity to answer the question ""why?"". You will also be able to articulate the story the data is telling with compelling verbal and written communication.\n\n\nDeliver minimally to moderately complex data analysis; collaborating as needed with Data Science as complexity increases.\nDevelopment of dashboards and reports.\nDevelopment of minimally to moderately complex data processing jobs using appropriate technologies (e.g. SQL, Python, Spark, AWS Lambda, etc.), collaborating with Data Engineers as needed.\nCollaborate with stakeholders to understand business domains, requirements, and expectations. Additionally, working with owners of data source systems to understand capabilities and limitations.\nManage the deliverables of projects, anticipate risks and resolve issues.\nAdopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.\n\nAbout the team\nSpectrum offers a world-class suite of data products and experiences to empower the creation of innovative solutions on behalf of Partners. Our foundational systems and tools solve for cross-cutting Builder needs in externalizing data, and are easily extensible using federated policy and reusable technology. Experience with data visualization using Tableau, Quicksight, or similar tools\nExperience with data modeling, warehousing and building ETL pipelines\nExperience using SQL to pull data from a database or data warehouse and scripting experience (Python) to process data for modeling\n5+ years of relevant professional experience in business intelligence, analytics, statistics, data engineering, data science or related field.\nDemonstrated data analysis and visualization skills.\nHighly proficient with SQL.\nKnowledge of AWS products such as Redshift, Quicksight, and Lambda.\nExcellent verbal/written communication & data presentation skills; ability to succinctly summarize key findings and effectively communicate with both business and technical teams. Experience in data mining, ETL, etc. and using databases in a business environment with large-scale, complex datasets",,,,"['Data analysis', 'Data modeling', 'Test design', 'Data processing', 'data visualization', 'Business intelligence', 'Data mining', 'Analytics', 'SQL', 'Python']",2025-06-10 14:20:10
Walkin || Cognizant is hiring For Databricks developer,Cognizant,4 - 9 years,Not Disclosed,['Kochi'],"Greetings from Cognizant!! #MegaWalkIn\n\nWe have an exciting opportunity for the #Databricks Developer Role with Cognizant, join us if you are an aspirant for matching the below criteria!!\n\nPrimary Skill: Databricks Developer\nExperience: 4-9 years\nJob Location: PAN India",,,,"['Azure Databricks', 'Data Bricks', 'ADB']",2025-06-10 14:20:13
"Senior Data Scientist, Operations || Mumbai || 29 LPA",Argus India Price Reporting Services,5 - 10 years,20-25 Lacs P.A.,"['Mumbai Suburban', 'Navi Mumbai', 'Mumbai (All Areas)']","Senior Data Scientist, Operations\nMumbai, India\nAbout Argus:\n\nArgus is the leading independent provider of market intelligence to the global energy and commodity markets. We offer essential price assessments, news, analytics, consulting services, data science tools and industry conferences to illuminate complex and opaque commodity markets.\nHeadquartered in London with 1,500 staff, Argus is an independent media organisation with 30 offices in the worlds principal commodity trading hubs.\nCompanies, trading firms and governments in 160 countries around the world trust Argus data to make decisions, analyse situations, manage risk, facilitate trading and for long-term planning. Argus prices are used as trusted benchmarks around the world for pricing transportation, commodities and energy.\nFounded in 1970, Argus remains a privately held UK-registered company owned by employee shareholders and global growth equity firm General Atlantic.\n\nWhat were looking for:\nJoin our Generative AI team as a Senior Data Scientist, reporting directly to the Lead Data Scientist in India. You will play a crucial role in building, optimizing, and maintaining AI-ready data infrastructure for advanced Generative AI applications. Your focus will be on hands-on implementation of cutting-edge data extraction, curation, and metadata enhancement techniques for both text and numerical data. You will be a key contributor to the development of innovative solutions, ensuring rapid iteration and deployment, and supporting the Lead in achieving the team's strategic goals.\n\nWhat will you be doing:\nAI-Ready Data Development: Design, develop, and maintain high-quality AI-ready datasets, ensuring data integrity, usability, and scalability to support advanced Generative AI models.\nAdvanced Data Processing: Drive hands-on efforts in complex data extraction, cleansing, and curation for diverse text and numerical datasets. Implement sophisticated metadata enrichment strategies to enhance data utility and accessibility for AI systems.\nAlgorithm Implementation & Optimization: Implement and optimize state-of-the-art algorithms and pipelines for efficient data processing, feature engineering, and data transformation tailored for LLM and GenAI applications.\nGenAI Application Development: Apply and integrate frameworks like LangChain and Hugging Face Transformers to build modular, scalable, and robust Generative AI data pipelines and applications.\nPrompt Engineering Application: Apply advanced prompt engineering techniques to optimize LLM performance for specific data extraction, summarization, and generation tasks, working closely with the Lead's guidance.\nLLM Evaluation Support: Contribute to the systematic evaluation of Large Language Models (LLMs) outputs, analysing quality, relevance, and accuracy, and supporting the implementation of LLM-as-a-judge frameworks.\nRetrieval-Augmented Generation (RAG) Contribution: Actively contribute to the implementation and optimization of RAG systems, including working with embedding models, vector databases, and, where applicable, knowledge graphs, to enhance data retrieval for GenAI.\nTechnical Mentorship: Act as a technical mentor and subject matter expert for junior data scientists, providing guidance on best practices in coding and PR reviews, data handling, and GenAI methodologies.\nCross-Functional Collaboration: Collaborate effectively with global data science teams, engineering, and product stakeholders to integrate data solutions and ensure alignment with broader company objectives.\nOperational Excellence: Troubleshoot and resolve data-related issues promptly to minimize potential disruptions, ensuring high operational efficiency and responsiveness.\nDocumentation & Code Quality: Produce clean, well-documented, production-grade code, adhering to best practices for version control and software engineering.\n\nSkills and Experience:\nAcademic Background: Advanced degree in AI, statistics, mathematics, computer science, or a related field.\nProgramming and Frameworks: 2+ years of hands-on experience with Python, TensorFlow or PyTorch, and NLP libraries such as spaCy and Hugging Face.\nGenAI Tools: 1+ years Practical experience with LangChain, Hugging Face Transformers, and embedding models for building GenAI applications.\nPrompt Engineering: Deep expertise in prompt engineering, including prompt tuning, chaining, and optimization techniques.\nLLM Evaluation: Experience evaluating LLM outputs, including using LLM-as-a-judge methodologies to assess quality and alignment.\nRAG and Knowledge Graphs: Practical understanding and experience using vector databases. In addition, familiarity with graph-based RAG architectures and the use of knowledge graphs to enhance retrieval and reasoning would be a strong plus.\nCloud: 2+ years of experience with Gemini/OpenAI models and cloud platforms such as AWS, Google Cloud, or Azure. Proficient with Docker for containerization.\nData Engineering: Strong understanding of data extraction, curation, metadata enrichment, and AI-ready dataset creation.\nCollaboration and Communication: Excellent communication skills and a collaborative mindset, with experience working across global teams.\n\nWhats in it for you:\nCompetitive salary\nHybrid Working Policy (3 days in Mumbai office/ 2 days WFH once fully inducted)\nGroup healthcare scheme\n18 days annual leave\n8 days of casual leave\nExtensive internal and external training\n\nHours:\nThis is a full-time position operating under a hybrid model, with three days in the office and up to two days working remotely.\nThe team supports Argus key business processes every day, as such you will be required to work on a shift-based rota with other members of the team supporting the business until 8pm. Typically support hours run from 11am to 8pm with each member of the team participating up to 2/3 times a week.\n\nFor more details about the company and to apply please make sure you send your CV and cover letter via our website: www.argusmedia.com/en/careers/open-positions\nBy submitting your job application, you automatically acknowledge and consent to the collection, use and/or disclosure of your personal data to the Company. Argus is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, sexual orientation, gender identity, disability or veteran status.",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pytorch', 'Artificial Intelligence', 'LangChain', 'hugging face', 'Spacy', 'Tensorflow']",2025-06-10 14:20:15
Data Scientist-Artificial Intelligence,IBM,5 - 7 years,Not Disclosed,['Mumbai'],"Work with broader team to build, analyze and improve the AI solutions.\nYou will also work with our software developers in consuming different enterprise applications\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nResource should have 5-7 years of experience. Sound knowledge of Python and should know how to use the ML related services.\nProficient in Python with focus on Data Analytics Packages.\nStrategy Analyse large, complex data sets and provide actionable insights to inform business decisions. Strategy Design and implementing data models that help in identifying patterns and trends. Collaboration Work with data engineers to optimize and maintain data pipelines.\nPerform quantitative analyses that translate data into actionable insights and provide analytical, data-driven decision-making. Identify and recommend process improvements to enhance the efficiency of the data platform. Develop and maintain data models, algorithms, and statistical models\n\n\nPreferred technical and professional experience\nExperience with conversation analytics. Experience with cloud technologies\nExperience with data exploration tools such as Tableu",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['algorithms', 'python', 'data analytics', 'tableau', 'ml', 'hive', 'data analysis', 'natural language processing', 'pyspark', 'data warehousing', 'machine learning', 'artificial intelligence', 'sql', 'pandas', 'deep learning', 'java', 'data science', 'spark', 'kafka', 'hadoop', 'big data', 'aws', 'etl']",2025-06-10 14:20:18
Solution Architect - L2,Wipro,10 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of the role is to create exceptional architectural solution design and thought leadership and enable delivery teams to provide exceptional client engagement and satisfaction.\nDo\n1.Develop architectural solutions for the new deals/ major change requests in existing deals\nCreates an enterprise-wide architecture that ensures systems are scalable, reliable, and manageable.\nProvide solutioning of RFPs received from clients and ensure overall design assurance\nDevelop a direction to manage the portfolio of to-be-solutions including systems, shared infrastructure services, applications in order to better match business outcome objectives\nAnalyse technology environment, enterprise specifics, client requirements to set a collaboration solution design framework/ architecture\nProvide technical leadership to the design, development and implementation of custom solutions through thoughtful use of modern technology\nDefine and understand current state solutions and identify improvements, options & tradeoffs to define target state solutions\nClearly articulate, document and sell architectural targets, recommendations and reusable patterns and accordingly propose investment roadmaps\nEvaluate and recommend solutions to integrate with overall technology ecosystem\nWorks closely with various IT groups to transition tasks, ensure performance and manage issues through to resolution\nPerform detailed documentation (App view, multiple sections & views) of the architectural design and solution mentioning all the artefacts in detail\nValidate the solution/ prototype from technology, cost structure and customer differentiation point of view\nIdentify problem areas and perform root cause analysis of architectural design and solutions and provide relevant solutions to the problem\nCollaborating with sales, program/project, consulting teams to reconcile solutions to architecture\nTracks industry and application trends and relates these to planning current and future IT needs\nProvides technical and strategic input during the project planning phase in the form of technical architectural designs and recommendation\nCollaborates with all relevant parties in order to review the objectives and constraints of solutions and determine conformance with the Enterprise Architecture\nIdentifies implementation risks and potential impacts\n2.Enable Delivery Teams by providing optimal delivery solutions/ frameworks\nBuild and maintain relationships with executives, technical leaders, product owners, peer architects and other stakeholders to become a trusted advisor\nDevelops and establishes relevant technical, business process and overall support metrics (KPI/SLA) to drive results\nManages multiple projects and accurately reports the status of all major assignments while adhering to all project management standards\nIdentify technical, process, structural risks and prepare a risk mitigation plan for all the projects\nEnsure quality assurance of all the architecture or design decisions and provides technical mitigation support to the delivery teams\nRecommend tools for reuse, automation for improved productivity and reduced cycle times\nLeads the development and maintenance of enterprise framework and related artefacts\nDevelops trust and builds effective working relationships through respectful, collaborative engagement across individual product teams\nEnsures architecture principles and standards are consistently applied to all the projects\nEnsure optimal Client Engagement\nSupport pre-sales team while presenting the entire solution design and its principles to the client\nNegotiate, manage and coordinate with the client teams to ensure all requirements are met and create an impact of solution proposed\nDemonstrate thought leadership with strong technical capability in front of the client to win the confidence and act as a trusted advisor\n3.Competency Building and Branding\nEnsure completion of necessary trainings and certifications\nDevelop Proof of Concepts (POCs),case studies, demos etc. for new growth areas based on market and customer research\nDevelop and present a point of view of Wipro on solution design and architect by writing white papers, blogs etc.\nAttain market referencability and recognition through highest analyst rankings, client testimonials and partner credits\nBe the voice of Wipros Thought Leadership by speaking in forums (internal and external)\nMentor developers, designers and Junior architects in the project for their further career development and enhancement\nContribute to the architecture practice by conducting selection interviews etc\n4. Team Management\nResourcing\nAnticipating new talent requirements as per the market/ industry trends or client requirements\nHire adequate and right resources for the team\nTalent Management\nEnsure adequate onboarding and training for the team members to enhance capability & effectiveness\nBuild an internal talent pool and ensure their career progression within the organization\nManage team attrition\nDrive diversity in leadership positions\nPerformance Management\nSet goals for the team, conduct timely performance reviews and provide constructive feedback to own direct reports\nEnsure that the Performance Nxt is followed for the entire team\nEmployee Satisfaction and Engagement\nLead and drive engagement initiatives for the team\nTrack team satisfaction scores and identify initiatives to build engagement within the team\nMandatory Skills: DataBricks - Data Engineering.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['solution architecture', 'Data Engineering', 'Data Bricks', 'Software Development', 'automation', 'Performance Management', 'Talent Management', 'maintenance']",2025-06-10 14:20:20
Immediate Openings For ETL Developer - Delhi,Trigyn Technologies,5 - 10 years,Not Disclosed,['New Delhi'],"We are seeking a skilled Data Engineer with at least 5 years of experience to join our data analytics team, focusing on building robust data pipelines and systems to support the creation of dynamic dashboards. The role involves designing, building, and optimizing data architecture, enabling real-time data flow for visualization and analytics. The Data Engineer will be responsible for managing ETL processes, ensuring data quality, and supporting the scalable integration of various data sources into our analytics platform.\nThe ideal candidate should have extensive experience in working with complex data architectures, managing ETL workflows, and ensuring seamless data integration across platforms. They should also have a deep understanding of cloud technologies and database management.\nKey Responsibilities:\n•Data Pipeline Development\no Design, build, and maintain scalable ETL (Extract, Transform, Load) processes for collecting, storing, and processing structured and unstructured data from multiple sources.\no Develop workflows to automate data extraction from APIs, databases, and external sources.\no Ensure data pipelines are optimized for performance and handle large data volumes with minimal latency.\n•Data Integration and Management\no Integrate data from various sources (e.g., databases, APIs, cloud storage) into the centralized daIta warehouse or data lake to support real-time dashboards.\no Ensure smooth data flow and seamless integration with analytics tools like Power BI and Tableau.\no Manage and maintain data storage solutions, including relational (SQL-based) and NoSQL databases.\nData Quality and Governance\no Implement data validation checks and quality assurance processes to ensure data accuracy, consistency, and integrity.\no Develop monitoring systems to identify and troubleshoot data inconsistencies, duplications, or errors during ingestion and processing.\no Ensure compliance with data governance policies and standards, including data protection regulations such as the Digital Personal Data Protection (DPDP) Act.\n•Database Management and Optimization\no Design and manage both relational and NoSQL databases, ensuring efficient storage, query performance, and reliability.\no Optimize database performance, ensuring fast query execution times and efficient data retrieval for dashboard visualization.\no Implement data partitioning, indexing, and replication strategies to support large-scale data operations.\n•Data Security and Compliance\no Ensure that all data processes adhere to security best practices, including encryption, authentication, and access control.\no Implement mechanisms for secure data storage and transmission, especially for sensitive government or public sector data.\no Conduct regular audits of data pipelines and storage systems to ensure compliance with relevant data protection regulations.\n• Cloud Infrastructure and Deployment\nDeploy and manage cloud-based data solutions using AWS, Azure, or GCP, including data lakes, data warehouses, and cloud-native ETL tools.\no Set up cloud infrastructure to support high availability, fault tolerance, and scalability of data systems.\no Monitor cloud usage and optimize costs for data storage, processing, and retrieval.\n•Performance Monitoring and Troubleshooting\no Continuously monitor data pipeline performance and data ingestion times to identify bottlenecks and areas for improvement\nTroubleshoot and resolve any data flow issues, ensuring high availability and reliability of data for dashboards and analytics.\no Implement logging and alerting mechanisms to detect and address any operational issues proactively.\nQualifications:\n•Education: Bachelors degree in Computer Science, Information Technology, Data Engineering, or a related field. A Master’s degree is a plus.\n•Experience: At least 5 years of hands-on experience as a Data Engineer, preferably in a data analytics or dashboarding environment.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Informatica', 'ETL', 'Azure Data Factory', 'SQL']",2025-06-10 14:20:22
Big Data Lead,Persistent,6 - 10 years,Not Disclosed,['Bengaluru'],"About Persistent\nWe are a trusted Digital Engineering and Enterprise Modernization partner, combining deep technical expertise and industry experience to help our clients anticipate what’s next. Our offerings and proven solutions create a unique competitive advantage for our clients by giving them the power to see beyond and rise above. We work with many industry-leading organizations across the world including 12 of the 30 most innovative US companies, 80% of the largest banks in the US and India, and numerous innovators across the healthcare ecosystem.\nOur growth trajectory continues, as we reported $1,231M annual revenue (16% Y-o-Y). Along with our growth, we’ve onboarded over 4900 new employees in the past year, bringing our total employee count to over 23,500+ people located in 19 countries across the globe.",,,,"['big data technologies', 'hive', 'cloudera', 'scala', 'data warehousing', 'apache pig', 'sql', 'streaming', 'java', 'apache', 'data modeling', 'spark', 'design', 'flume', 'hadoop', 'big data', 'etl', 'big data hadoop', 'hbase', 'python', 'oracle', 'oozie', 'talend', 'data processing', 'machine learning', 'sql server', 'nosql', 'mapreduce', 'kafka', 'hdfs', 'sqoop', 'aws']",2025-06-10 14:20:25
Python Developer/ Python Data Engineer (Python+ ETL+ Pandas),Atyeti,5 - 8 years,Not Disclosed,['Hyderabad'],"Role & responsibilities\nB.Tech or M.Tech in Computer Science, or equivalent experience.\n5+ years of experience working professionally as a Python Software Developer.\nOrganized, self-directed, and resourceful.\nExcellent written and verbal communication skills.\nExpert in python & pandas.\nExperience in building data pipelines, ETL and ELT processes.",,,,"['Pandas', 'ETL', 'Python', 'SQL']",2025-06-10 14:20:27
"Manager, Data & Analytics - Financial Services",RSM US in India,5 - 10 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","RSM is looking for an experienced Hands-On Technical Manager with expertise in big data technologies and multi-cloud platforms to lead our technical team for the financial services industry. The ideal candidate will possess a strong background in big data architecture, cloud computing, and a deep understanding of the financial services industry. As a Technical Manager, you will be responsible for leading technical projects, hands-on development, delivery management, sales and ensuring the successful implementation of data solutions across multiple cloud platforms. This role requires a unique blend of technical proficiency, sales acumen, and presales experience to drive business growth and deliver innovative data solutions to our clients.\nResponsibilities:\nProvide technical expertise and guidance on the selection, and hands-on implementation, and optimization of big data platforms, tools, and technologies across multiple cloud environments (e.g., AWS, Azure, GCP, Snowflake, etc.)\nArchitect and build scalable and secure data pipelines, data lakes, and data warehouses to support the storage, processing, and analysis of large volumes of structured and unstructured data.\nLead and mentor a team of technical professionals in the design, development, and implementation of big data solutions and data analytics projects within the financial services domain.\nStay abreast of emerging trends, technologies, and industry developments in big data, cloud computing, and financial services, and assess their potential impact on the organization.\nDevelop and maintain best practices, standards, and guidelines for data management, data governance, and data security in alignment with regulatory requirements and industry standards.\nCollaborate with the sales and business development teams to identify customer needs, develop solution proposals, and present technical demonstrations and presentations to prospective clients.\nCollaborate with cross-functional teams including data scientists, engineers, business analysts, and stakeholders to define project requirements, objectives, and timelines.\nBasic Qualifications:\nBachelor's degree or higher in Computer Science, Information Technology, Business Administration, Engineering or related field.\nMinimum of ten years of overall technical experience in solution architecture, design, hands-on development with a focus on big data technologies, multi-cloud platforms, and with at-least 5 years of experience specifically in financial services.\nStrong understanding of the financial services industry - capital markets, retail and business banking, asset management, insurance, etc.\nIn-depth knowledge of big data technologies such as Hadoop, Spark, Kafka, and cloud platforms such as AWS, Azure, GCP, Snowflake, Databricks, etc.\nExperience with SQL, Python, Pyspark or other programming languages used for data transformation, analysis, and automation.\nExcellent communication, presentation, and interpersonal skills, with the ability to articulate technical concepts to both technical and non-technical audiences.\nHands-on experience extracting (ETL using CDC, Transaction Logs, Incremental) and processing large data sets for Streaming and Batch data loads.\nAbility to work from our Hyderabad, India office at least twice a week\nPreferred Qualifications:\nProfessional certifications in cloud computing (e.g., AWS Certified Solutions Architect, Microsoft Certified Azure Solutions Architect, Azure Data Engineer, SnowPro Core) and/or big data technologies.\nExperience with Power BI, Tableau or other Reporting and Data Visualization tools\nFamiliarity with DevOps practices, CI/CD pipelines, and infrastructure-as-code tools\nEducation/Experience:\nBachelor s degree in MIS, CS, Engineering or equivalent field.\nMaster s degree is CS or MBA is preferred.\nAdvanced Data and Cloud Certifications are a plus.",Industry Type: Banking,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Financial Services', 'Azure', 'CI/CD pipelines', 'GCP', 'DevOps practices', 'Snowflake', 'Databricks', 'AWS']",2025-06-10 14:20:29
Senior Data Scientist,Comviva Technology,2 - 5 years,Not Disclosed,['Gurugram'],"Job Description for Sr Data Scientist (relevant experience: 2 to 5 years)\n\nWhat we look for:\nExpertise in Big Data/ML: Should be able to build cutting edge credit/churn/usage models using advanced algorithms in a Big data/Machine Learning environment. Should have hands on experience and track record of delivering projects in individual capacity.\n- Process oriented: Should help in building a process that maximizes operating efficiency while maintaining risk across multiple lending cycles. There needs to be an obsession with collecting and analyzing data to drive business iterations and improvements.\n- Willingness to go above and beyond: For start-ups the responsibilities and needs of the business change quickly. Were looking for someone who is not afraid to take on calculated risks and can deal with ambiguity.\n\nJob Responsibilities:\n- Develop innovative credit risk / churn / usage models using mobile wallet transaction data, Call data, Telecom usage data, customer bureau data etc\n- Partner with the Data Engineering team to define the required data pipelines to build and enhance the feature bank (foundational capability) to build/deploy the various ML algorithms both for batch and real time use cases\n- Collaborate with credit policy/portfolio mgmt. team to drive P&L outcomes.\n- Building reports for model monitoring and drive enhancements\nRequired Qualifications & Skills:\nSolid expertise in end-to-end risk model lifecycle management (develop, deploy, monitor)\nPrevious hands on in credit, fraud, churn model development and deployment\nPrevious experience in PD/EAD/LGD model development/validation\nExperience in CSI/PSI model monitoring process\nHands on experience in data extraction using SQL/Pyspark SQL; data cleaning, feature creation and building models using Py Spark/Python on Spark; Scale will be a plus.\nPrevious exposure to below algorithms (preferably multiple):\nLogistic Regression\nRandom forest\nXGBOOST\nMarkov Chain\nPSI/CSI for model monitoring\nStrategy performance tracking and swap in / swap out analysis\n- Strong entrepreneurial drive\n- Good to have:\no Good business understanding of the fintech/consumer finance space\no Experience in working with credit card / personal lending space, esp fintech\no hands on experience in working with Telecom data",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Telecom', 'Logistic regression', 'Consumer finance', 'Machine learning', 'model development', 'Deployment', 'Business understanding', 'SQL', 'Python', 'Data extraction']",2025-06-10 14:20:32
Data Security and Privacy Engineer,Finastra,3 - 8 years,Not Disclosed,['Bengaluru'],"What will you contribute?\nAs a Data Security and Privacy Engineer, you will be part of the Information Security team, reporting to the Chief Data Security Architect. Your role involves governing and protecting Finastra s product and customer data. You will collaborate with Security, Legal, Product Development, Data Engineering, and other teams to classify data and ensure compliance with confidentiality, integrity, availability, and data privacy requirements. You will also enhance the data governance framework, policies, processes, and standards, and handle tasks such as data classification, asset registration, enterprise data security, product risk assessments, and risk remediation. Additionally, you will develop process automation solutions for repeatable tasks.\nResponsibilities & Deliverables\nComplete data governance tasks and maintain due diligence records, ensuring compliance with policies, standards, and regulations.\nConduct data classification reviews, identify data minimization opportunities, prescribe de-identification methods, and track risk remediation.\nReview asset metadata records for anomalies and guide asset owners on corrections.\nCoordinate with stakeholders to achieve data governance framework goals, including data ownership, stewardship, dictionaries, flow diagrams, handling methods, usage rights, metadata management, and regulatory compliance.\nConduct training and awareness on data governance and protection standards and procedures.\nDevelop metrics and reports for continued compliance with the data governance framework and company standards.\nAssist with requirements, implementation, integration, and training for new data security tools, standards, and processes.\nIndependently carry out assignments, coordinate work with others, and keep team members informed of project statuses and risks.\nRequired Experience\nBachelor s or advanced degree in Information Technology or related field and 3+ years of professional experience in Data Governance, Data Security, or a similar role.\nStrong organizational skills and motivation for self-improvement and achieving results.\nExcellent verbal and written communication skills, with the ability to effectively communicate with technical personnel, business stakeholders, and management.\nUnderstanding of key data governance concepts and data management practices.\nFamiliarity with common data privacy concepts and compliance methodologies.\nAbility to make independent judgments and accurately identify risks.\nFamiliarity with API calls, SQL queries and practical scripting/coding experience (e.g., Python, PowerShell).\nNice to Haves\nCertification as a Certified Data Management Professional (CDMP), Certified Information Privacy Technologist (CIPT), or other Security certification (CISSP, CCSP, CISA).\nExperience with JIRA, ServiceNow, Power BI, and data management software.\nExposure to cloud-native application hosting, containers, and APIs.\nWe are proud to offer a range of incentives to our employees worldwide. These benefits are available to everyone, regardless of grade, and reflect the values we uphold:\n\nFlexibility: Enjoy unlimited vacation, based on your location and business priorities. Hybrid working arrangements, and inclusive policies such as paid time off for voting, bereavement, and sick leave.\nWell-being: Access confidential one-on-one therapy through our Employee Assistance Program, unlimited personalized coaching via our coaching app, and access to our Gather Groups for emotional and mental support.\nMedical, life & disability insurance, retirement plan, lifestyle and other benefits*\nESG: Benefit from paid time off for volunteering and donation matching.\nDEI: Participate in multiple DE&I groups for . We learn from one another, embrace and celebrate our differences, and create an environment where everyone feels safe to be themselves.\nBe unique, be exceptional, and help us make a difference at Finastra!",Industry Type: Financial Services,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Process automation', 'Career development', 'Due diligence', 'Usage', 'Data management', 'Cisa', 'Coding', 'Information security', 'Information technology', 'Financial services']",2025-06-10 14:20:35
Group Manager UX Research & Service Design,Angel One,10 - 15 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","About The Role\nAbout Angel one :\nAngel One Limited is a Fintech company providing broking services, margin trading facility, research services, depository services, investment education and financial products distribution to its clients, on a mission to become the No. 1 fintech organization in India. With over 2 crore+ registered clients, we are onboarding an average of over 700K new clients every month. We are working to build personalized financial journeys for our clients via a single app, powered by new-age tech, AI, Machine Learning and Data Science. We apply and amplify personalization within our products and services.",,,,"['Health insurance', 'Team management', 'Quantitative research', 'data science', 'financial products', 'Machine learning', 'Tool design', 'Wellness', 'Depository services', 'Strategic leadership']",2025-06-10 14:20:38
Solution Architect,Maddisoft Solutions,10 - 20 years,Not Disclosed,['Chennai'],"Maddisoft has the following immediate opportunity, let us know if you or someone you know would be interested. Send in your resume ASAP. Send in resume along with LinkedIn profile without which applications will not be considered. Call us NOW!\n\nJob Title: Solution Architect\nJob Location: Hyderabad, India\n\nResponsibilities\nInterprets and delivers impactful strategic plans improving data integration, data quality, and data delivery in support of business initiatives and roadmaps\nDesigns the structure and layout of data systems, including databases, warehouses, and lakes\nSelects and implements database management systems that meet the organizations needs by defining data schemas, optimizing data storage, and establishing data access controls and security measures\nDefines and implements the long-term technology strategy and innovations roadmaps across analytics, data engineering, and data platforms\nDesigns and implements processes for the ETL process from various sources into the organizations data systems\nTranslates high-level business requirements into data models and appropriate metadata, test data, and data quality standards\nManages senior business stakeholders to secure strong engagement and ensures that the delivery of the project aligns with longer-term strategic roadmaps\nSimplifies the existing data architecture, delivering reusable services and cost-saving opportunities in line with the policies and standards of the company\nLeads and participates in the peer review and quality assurance of project architectural artifacts across the EA group through governance forums\nDefines and manages standards, guidelines, and processes to ensure data quality\nWorks with IT teams, business analysts, and data analytics teams to understand data consumers needs and develop solutions\nEvaluates and recommends emerging technologies for data management, storage, and analytics\n\nJob Requirements\nBachelor's degree in Computer Science, Information Sciences or related discipline and 5 - 8 years of relevant experience (ex: IT solutions architecture, enterprise architecture, and systems & application design) or 12 -15 years or related experience\nBroad technical expertise in at least one area, such as application development, enterprise applications or IT systems engineering\nExcellent communications skills - Able to effectively communicate highly technical information in non-technical terminology (written and verbal)\nExpert in change management principles associated with new technology implementations\nDeep understanding of project management principles\n\nPreferred Qualifications\nStrong understanding of Azure cloud services\nDevelop and maintain strong relationships with various business areas and IT Teams to understand their needs and challenges.\nProactively identify opportunities for collaboration and engagement across IT Teams.\nAt least five years of relevant experience in design and implementation of data models for enterprise data warehouse initiatives\nExperience leading projects involving data warehousing, data modeling, and data analysis\nDesign experience in Azure Databricks, PySpark, and Power BI/Tableau\nStrong ability in programming languages such as Java, Python, and C/C++\nAbility in data science languages/tools such as SQL, R, SAS, or Excel\nProficiency in the design and implementation of modern data architectures and concepts such as cloud services (AWS, Azure, GCP), real-time data distribution (Kafka, Dataflow), and modern data warehouse tools (Snowflake, Databricks)\nExperience with database technologies such as SQL, NoSQL, Oracle, Hadoop, or Teradata\nUnderstanding of entity-relationship modeling, metadata systems, and data quality tools and techniques\nAbility to think strategically and relate architectural decisions and recommendations to business needs and client culture\nAbility to assess traditional and modern data architecture components based on business needs\nExperience with business intelligence tools and technologies such as ETL, Power BI, and Tableau\nAbility to regularly learn and adopt new technology, especially in the ML/AI realm\nStrong analytical and problem-solving skills\nAbility to synthesize and clearly communicate large volumes of complex information to senior management of various technical understandings\nAbility to collaborate and excel in complex, cross-functional teams involving data scientists, business analysts, and stakeholders\nAbility to guide solution design and architecture to meet business needs.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Azure Cloud', 'ETL', 'Python', 'SQL', 'Power Bi', 'AWS', 'Data warehouse']",2025-06-10 14:20:40
Specialist Data Science,Merck Sharp & Dohme (MSD),5 - 7 years,Not Disclosed,"['Hyderabad', 'Pune']","Specialist - Data Science\nAt our company we are leveraging analytics and technology, as we invent for life on behalf of patients around the world. We are seeking those who have a passion for using data, analytics, and insights to drive decision making, that will allow us to tackle some of the world s greatest health threats.\nWithin our commercial Insights, Analytics, and Data organization we are transforming to better power decision-making across our end-to-end commercialization process, from business development to late lifecycle management. As we endeavor, we are seeking a dynamic talent to serve in the role of Analyst - Data Science.\nThis role involves working with our partners in different Therapeutic areas (e.g. Oncology, Vaccines, Pharma & Rare Disease, etc.) and Domain areas (HCP Analytics, Patient Analytics, Segmentation & targeting, Market Access, etc.) across the organization to help create scalable and production-grade analytics solutions, ranging from data visualization and reporting to advanced statistical and AI/ML models.\nYou will work in one of the three therapeutic areas of Brand Strategy and Performance Analytics - Oncology/Vaccines/Pharma & Rare Disease, where you will play a pivotal role in leveraging your statistical and machine learning expertise to address critical business challenges and derive insights to drive key decisions. Working alongside experienced data scientists and business analysts, you will have the opportunity to collaborate in translating business queries into analytical problems, employing your critical thinking, problem-solving, statistical, machine learning, and data visualization skills to deliver impactful solutions.\nWe are seeking candidates with prior experience in the healthcare analytics or consulting sectors, prior hands-on experience in Data Science (building end-to-end ML models). It is preferred that you have a good understanding of Physician and Patient-level data (PLD) from leading vendors such as IQVIA, Komodo, and Optum. Familiarity with HCP Analytics, PLD analytics, concepts like persistence, compliance, line of therapy, etc., or Segmentation & Targeting is highly desirable. You will be part of a dynamic team that collaborates with our partners across therapeutic areas. Furthermore, effective communication skills are crucial, as this role requires interfacing with executive and business stakeholders.\nWho you are:\nYou understand the foundations of statistics and machine learning and can work in high performance computing/cloud environments, with experience/knowledge in aspects across statistical analysis, machine learning, model development, data engineering, data visualization, and data interpretation\nYou are self-motivated, and have demonstrated abilities to think independently as a data scientist\nYou structure your data science approach according to the necessary task, while appropriately applying the correct level of model complexity to the problem at hand\nYou have an agile mindset of continuous learning and will focus on integrating enterprise value into team culture\nYou are kind, collaborative, and capable of seeking and giving candid feedback that effectively contributes to a more seamless day-to-day execution of tasks\nKey responsibilities:\nLead a team of Analysts - Data Science to solve complex business problems.\nLead the team in understanding the business requirements and translating them into analytical problem statements.\nDefine technical requirements (datasets, business rules, technical architecture), provide technical direction to the team and manage end-to-end projects\nCollaborate with cross-functional teams to design and implement solutions that meet business requirements\nPresent the findings to senior business stakeholders in a clear and concise manner\nManage and mentor junior data scientists through technical and professional guidance, trainings etc.\nDevelop deep expertise in the therapeutic area of interest, contribute to thought leadership in the domain through publications and conference presentations.\nMinimum Qualifications:\nBachelor s degree with 5-7 years industry experience\nProficiency in Python/R & SQL\nExperience in healthcare analytics or consulting sectors\nExperience working with real world evidence (RWE) and patient level data (PLD) from leading vendors such as IQVIA, Komodo, Optum etc.\nExperience in HCP Analytics, Segmentation and Targeting and Patient Level Data analytics (e.g., creating Patient Cohorts, knowledge of Lines of Therapy, Persistency, Compliance, etc.)\nExperience in leading small sized teams\nStrong Python/R, SQL, Excel skills\nStrong foundations of statistics and machine learning\nPreferred Qualifications:\nAdvanced degree in STEM (MS, MBA, PhD)\nExperience in Oncology/Vaccine/Pharma & Rare Diseases therapeutic area commercial analytics\nKnowledge of statistics, data science and machine learning & commercial\nExperience of supporting End to End Project Management\nCurrent Employees apply HERE\nCurrent Contingent Workers apply HERE\nSearch Firm Representatives Please Read Carefully\nEmployee Status:\nRegular\nRelocation:\nVISA Sponsorship:\nTravel Requirements:\nFlexible Work Arrangements:\nHybrid\nShift:\nValid Driving License:\nHazardous Material(s):\n\nRequired Skills:\nBusiness Intelligence (BI), Database Design, Data Engineering, Data Modeling, Data Science, Data Visualization, Machine Learning, Software Development, Stakeholder Relationship Management, Waterfall Model\n\nPreferred Skills:\nJob Posting End Date:\n07/31/2025\n*A job posting is effective until 11:59:59PM on the day BEFORE the listed job posting end date. Please ensure you apply to a job posting no later than the day BEFORE the job posting end date.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Relationship management', 'data science', 'Data modeling', 'Project management', 'Pharma', 'Analytical', 'Consulting', 'Agile', 'Business intelligence', 'SQL']",2025-06-10 14:20:43
Cloudera Migration Expert,IBM,15 - 20 years,Not Disclosed,['Mumbai'],"LocationMumbai\nExperience15+ years in data engineering/architecture\n\nRole Overview:\nLead the architectural design and implementation of a secure, scalable Cloudera-based Data Lakehouse for one of India’s top public sector banks.\n\nKey Responsibilities:\n* Design end-to-end Lakehouse architecture on Cloudera\n* Define data ingestion, processing, storage, and consumption layers\n* Guide data modeling, governance, lineage, and security best practices\n* Define migration roadmap from existing DWH to CDP\n* Lead reviews with client stakeholders and engineering teams\n\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\nSkills Required:\n* Proven experience with Cloudera CDP, Spark, Hive, HDFS, Iceberg\n* Deep understanding of Lakehouse patterns and data mesh principles\n* Familiarity with data governance tools (e.g., Apache Atlas, Collibra)\n* Banking/FSI domain knowledge highly desirable",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'cloudera', 'spark', 'apache atlas', 'data engineering', 'scala', 'apache pig', 'apache ranger', 'sql', 'apache', 'java', 'data modeling', 'gcp', 'flume', 'hadoop', 'big data', 'hbase', 'python', 'oozie', 'airflow', 'cdp', 'apache nifi', 'mapreduce', 'kafka', 'sqoop', 'aws', 'yarn', 'fsi', 'knox']",2025-06-10 14:20:46
Sr. AWS Developer,Leading Client,3 - 6 years,Not Disclosed,['Chennai'],"AWS\n\nLambda\nGlue\nKafka/Kinesis\nRDBMS Oracle, MySQL, RedShift, PostgreSQL, Snowflake\nGateway\nCloudformation / Terraform\nStep Functions\nCloudwatch\nPython\nPyspark\nJob role & responsibilities:\nLooking for a Software Engineer/Senior Software engineer with hands on experience in ETL projects and extensive knowledge in building data processing systems with Python, pyspark and Cloud technologies(AWS).\nExperience in development in AWS Cloud (S3, Redshift, Aurora, Glue, Lambda, Hive, Kinesis, Spark, Hadoop/EMR)\nRequired Skills:\nAmazon Kinesis, Amazon Aurora, Data Warehouse, SQL, AWS Lambda, Spark, AWS QuickSight\nAdvanced Python Skills\nData Engineering ETL and ELT Skills\nExperience of Cloud Platforms (AWS or GCP or Azure)\nMandatory skills-Datawarehouse, ETL, SQL, Python, AWS Lambda, Glue, AWS Redshift.",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AWS', 'hive', 'glue', 'amazon redshift', 'pyspark', 'data warehousing', 'emr', 'cloud technologies', 'kinesis', 'sql', 'postgresql', 'spark', 'gcp', 'aws cloud', 'mysql', 'hadoop', 'etl', 'python', 'quicksight', 'rdbms', 'oracle', 'data processing', 'microsoft azure', 'aws lambda', 'amazon aurora', 'lambda expressions']",2025-06-10 14:20:48
Solution Architect - L1,Wipro,8 - 10 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of the role is to create exceptional architectural solution design and thought leadership and enable delivery teams to provide exceptional client engagement and satisfaction.\nDo\n1.Develop architectural solutions for the new deals/ major change requests in existing deals\nCreates an enterprise-wide architecture that ensures systems are scalable, reliable, and manageable.\nProvide solutioning of RFPs received from clients and ensure overall design assurance\nDevelop a direction to manage the portfolio of to-be-solutions including systems, shared infrastructure services, applications in order to better match business outcome objectives\nAnalyse technology environment, enterprise specifics, client requirements to set a collaboration solution design framework/ architecture\nProvide technical leadership to the design, development and implementation of custom solutions through thoughtful use of modern technology\nDefine and understand current state solutions and identify improvements, options & tradeoffs to define target state solutions\nClearly articulate, document and sell architectural targets, recommendations and reusable patterns and accordingly propose investment roadmaps\nEvaluate and recommend solutions to integrate with overall technology ecosystem\nWorks closely with various IT groups to transition tasks, ensure performance and manage issues through to resolution\nPerform detailed documentation (App view, multiple sections & views) of the architectural design and solution mentioning all the artefacts in detail\nValidate the solution/ prototype from technology, cost structure and customer differentiation point of view\nIdentify problem areas and perform root cause analysis of architectural design and solutions and provide relevant solutions to the problem\nCollaborating with sales, program/project, consulting teams to reconcile solutions to architecture\nTracks industry and application trends and relates these to planning current and future IT needs\nProvides technical and strategic input during the project planning phase in the form of technical architectural designs and recommendation\nCollaborates with all relevant parties in order to review the objectives and constraints of solutions and determine conformance with the Enterprise Architecture\nIdentifies implementation risks and potential impacts\n2.Enable Delivery Teams by providing optimal delivery solutions/ frameworks\nBuild and maintain relationships with executives, technical leaders, product owners, peer architects and other stakeholders to become a trusted advisor\nDevelops and establishes relevant technical, business process and overall support metrics (KPI/SLA) to drive results\nManages multiple projects and accurately reports the status of all major assignments while adhering to all project management standards\nIdentify technical, process, structural risks and prepare a risk mitigation plan for all the projects\nEnsure quality assurance of all the architecture or design decisions and provides technical mitigation support to the delivery teams\nRecommend tools for reuse, automation for improved productivity and reduced cycle times\nLeads the development and maintenance of enterprise framework and related artefacts\nDevelops trust and builds effective working relationships through respectful, collaborative engagement across individual product teams\nEnsures architecture principles and standards are consistently applied to all the projects\nEnsure optimal Client Engagement\nSupport pre-sales team while presenting the entire solution design and its principles to the client\nNegotiate, manage and coordinate with the client teams to ensure all requirements are met and create an impact of solution proposed\nDemonstrate thought leadership with strong technical capability in front of the client to win the confidence and act as a trusted advisor\n3.Competency Building and Branding\nEnsure completion of necessary trainings and certifications\nDevelop Proof of Concepts (POCs),case studies, demos etc. for new growth areas based on market and customer research\nDevelop and present a point of view of Wipro on solution design and architect by writing white papers, blogs etc.\nAttain market referencability and recognition through highest analyst rankings, client testimonials and partner credits\nBe the voice of Wipros Thought Leadership by speaking in forums (internal and external)\nMentor developers, designers and Junior architects in the project for their further career development and enhancement\nContribute to the architecture practice by conducting selection interviews etc\n4. Team Management\nResourcing\nAnticipating new talent requirements as per the market/ industry trends or client requirements\nHire adequate and right resources for the team\nTalent Management\nEnsure adequate onboarding and training for the team members to enhance capability & effectiveness\nBuild an internal talent pool and ensure their career progression within the organization\nManage team attrition\nDrive diversity in leadership positions\nPerformance Management\nSet goals for the team, conduct timely performance reviews and provide constructive feedback to own direct reports\nEnsure that the Performance Nxt is followed for the entire team\nEmployee Satisfaction and Engagement\nLead and drive engagement initiatives for the team\nTrack team satisfaction scores and identify initiatives to build engagement within the team\nMandatory Skills: DataBricks - Data Engineering.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['solution architecture', 'Data Engineering', 'Data Bricks', 'Software Development', 'automation', 'Performance Management', 'Talent Management', 'maintenance']",2025-06-10 14:20:51
Technical Lead - L1,Wipro,5 - 8 years,Not Disclosed,['Pune'],"Role Purpose\nThe purpose of the role is to support process delivery by ensuring daily performance of the Production Specialists, resolve technical escalations and develop technical capability within the Production Specialists.\n\n\n\n\n\nDo\nOversee and support process by reviewing daily transactions on performance parameters\nReview performance dashboard and the scores for the team\nSupport the team in improving performance parameters by providing technical support and process guidance\nRecord, track, and document all queries received, problem-solving steps taken and total successful and unsuccessful resolutions\nEnsure standard processes and procedures are followed to resolve all client queries\nResolve client queries as per the SLAs defined in the contract\nDevelop understanding of process/ product for the team members to facilitate better client interaction and troubleshooting\nDocument and analyze call logs to spot most occurring trends to prevent future problems\nIdentify red flags and escalate serious client issues to Team leader in cases of untimely resolution\nEnsure all product information and disclosures are given to clients before and after the call/email requests\nAvoids legal challenges by monitoring compliance with service agreements\n\n\n\nHandle technical escalations through effective diagnosis and troubleshooting of client queries\nManage and resolve technical roadblocks/ escalations as per SLA and quality requirements\nIf unable to resolve the issues, timely escalate the issues to TA & SES\nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers and clients business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract SLAs\n\n\n\nBuild people capability to ensure operational excellence and maintain superior customer service levels of the existing account/client\nMentor and guide Production Specialists on improving technical knowledge\nCollate trainings to be conducted as triage to bridge the skill gaps identified through interviews with the Production Specialist\nDevelop and conduct trainings (Triages) within products for production specialist as per target\nInform client about the triages being conducted\nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nIdentify and document most common problems and recommend appropriate resolutions to the team\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\n\n\nDeliver\n\nNoPerformance ParameterMeasure\n1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback, NSAT/ ESAT\n2Team ManagementProductivity, efficiency, absenteeism\n3Capability developmentTriages completed, Technical Test performance\nMandatory Skills: DataBricks - Data Engineering.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'DataBricks', 'hive', 'python', 'technical leadership', 'team management', 'spark', 'troubleshooting', 'hadoop', 'big data', 'sql']",2025-06-10 14:20:53
Walkin || Cognizant is hiring For AWS Services,Cognizant,6 - 9 years,Not Disclosed,['Kochi'],"Greetings from Cognizant!! #MegaWalkIn\n\nWe have an exciting opportunity for the #AWS Services Role with Cognizant, join us if you are an aspirant for matching the below criteria!!\n\nPrimary Skill: AWS Services\nExperience: 6-9 years\nJob Location: PAN India",,,,"['AWS Services', 'Aws Lambda', 'Aws Glue']",2025-06-10 14:20:55
Power Bi Developer @Infosys,Infosys,3 - 8 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']",Hiring for PowerBi Developer with experience range 3 years & above\n\nMandatory Skills: PowerBi Developer\n\nEducation: BE/B.Tech/MCA/M.Tech/MSc./MS\n\nInterview Mode-F2F,Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Power Bi', 'power bi developer', 'Microsoft Power Bi']",2025-06-10 14:20:57
Lead Enterprise Data Architect,Bread Financial,8 - 13 years,Not Disclosed,['Bengaluru'],"Every career journey is personal. Thats why we empower you with the tools and support to create your own success story.\nBe challenged. Be heard. Be valued. Be you ... be here.\nJob Summary\nLead Enterprise Data Architect provides strategic leadership in the development and implementation of complex architectural solutions across the enterprise. Guide projects to ensure alignment with architectural standards.\nthis position is for a broad architecture role with a focus on Data Architecture. Architects in this role are expected to have solid Data Architecture experiences at both the strategical and technical end of the spectrum. Expertise in enterprise data concepts such as MDM, data modeling (both transactional and analytical), various analytical platorm architectures (data lake, data lakehouse, data fabric, data mesh, etc.), data governance (data catalog, quality, lineage), data stream processing, data science, AI/ML, Generative AI. Deep familiarity with data engineering and analysis tools such as Airflow, Microstrategy, cloud data-related services, Snowflake, Collibra, Databricks, and other similar tools would be important.\n\nEssential Job Functions\nCollaborate with both business and IT function to foster innovation and ensure strategic alignment. - (30%)\nDrive the adoption of innovative technologies to improve business processes. - (20%)\nOversee architectural governance and ensure compliance with IT policies. - (20%)\nMentor junior architects and provide thought leadership across the organization. - (20%)\nAct as a subject-matter expert for enterprise architecture. - (10%)\n\nMinimum Qualifications\nBachelor s Degree or equivalent education and / or experience in computer Science, Engineering, Mathematics or related field.\n8+ years in Information Technology\n\nPreferred Qualifications\nRelevant certifications, such as TOGAF or AWS Solutions Architect\n\nSkills\nApplication Development\nBusiness Alignment\nBusiness Process Modeling\nBusiness Case Development\nCode Inspection\nCloud Architectures\nEnterprise Architecture Framework\nIT Architecture\nIT Roadmap\nSolution Architecture\n\nReports To : Senior Manager and above\n\nDirect Reports : 0\n\nWork Environment\nNormal office environment, hybrid.\n\nOther Duties\nThis job description is illustrative of the types of duties typically performed by this job. It is not intended to be an exhaustive listing of each and every essential function of the job. Because job content may change from time to time, the Company reserves the right to add and/or delete essential functions from this job at any time.\nAbout Bread Financial\nAt Bread Financial, you ll have the opportunity to grow your career, give back to your community, and be part of our award-winning culture. We ve been consistently recognized as a best place to work nationally and in many markets and we re proud to promote an environment where you feel appreciated, accepted, valued, and fulfilled both personally and professionally. Bread Financial supports the overall wellness of our associates with a diverse suite of benefits and offers boundless opportunities for career development and non-traditional career progression.\nBread Financial (NYSE: BFH) is a tech-forward financial services company that provides simple, personalized payment, lending, and saving solutions to millions of U.S consumers. Our payment solutions, including Bread Financial general purpose credit cards and savings products, empower our customers and their passions for a better life. Additionally, we deliver growth for some of the most recognized brands in travel & entertainment, health & beauty, jewelry and specialty apparel through our private label and co-brand credit cards and pay-over-time products providing choice and value to our shared customers.\nTo learn more about Bread Financial, our global associates and our sustainability commitments, visit breadfinancial.com or follow us on Instagram and LinkedIn .\nAll job offers are contingent upon successful completion of credit and background checks.\nBread Financial is an Equal Opportunity Employer.\nJob Family:\nInformation Technology\nJob Type:\nRegular",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Microstrategy', 'Solution architecture', 'Computer science', 'Career development', 'Architecture', 'Analytical', 'Wellness', 'Application development', 'Information technology', 'Financial services']",2025-06-10 14:20:59
Senior/Lead Data Scientist,Tiger Analytics,6 - 11 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities\n\nCurious about the role? What your typical day would look like?As a Senior Data Scientist, your work is a combination of hands-on contribution to Loreum Ipsum, Loreum Ipsum, etc. More specifically, this will involve:\nLead and contribute to developing sophisticated machine learning models, predictive analytics, and statistical analyses to solve complex business problems.",,,,"['Data Science', 'Time Series Analysis', 'Machine Learning', 'Python', 'Time Series Forecasting', 'Regression', 'Clustering', 'neural nets', 'Optimization', 'SQL']",2025-06-10 14:21:01
Senior Manager Information Systems,Horizon Therapeutics,8 - 18 years,Not Disclosed,['Hyderabad'],"Career Category Information Systems Job Description\nJoin Amgen s Mission of Serving Patients\nAt Amgen, if you feel like you re part of something bigger, it s because you are. Our shared mission to serve patients living with serious illnesses drives all that we do.\nSince 1980, we ve helped pioneer the world of biotech in our fight against the world s toughest diseases. With our focus on four therapeutic areas -Oncology, Inflammation, General Medicine, and Rare Disease- we reach millions of patients each year. As a member of the Amgen team, you ll help make a lasting impact on the lives of patients as we research, manufacture, and deliver innovative medicines to help people live longer, fuller happier lives.\nOur award-winning culture is collaborative, innovative, and science based. If you have a passion for challenges and the opportunities that lay within them, you ll thrive as part of the Amgen team. Join us and transform the lives of patients while transforming your career.\nSenior Manager Information Systems\nWhat you will do\nLet s do this. Let s change the world. n this vital role you will develop an insight driven sensing capability with a focus on revolutionizing decision making. In this role you will lead the technical delivery for this capability as part of a team data engineers and software engineers. The team will rely on your leadership to own and refine the vision, feature prioritization, partner alignment, and experience leading solution delivery while building this ground-breaking new capability for Amgen. You will drive the software engineering side of the product release and will deliver for the outcomes.\nRoles Responsibilities:\nLead delivery of overall product and product features from concept to end of life management of the product team comprising of technical engineers, product owners and data scientists to ensure that business, quality, and functional goals are met with each product release\nDrives excellence and quality for the respective product releases, collaborating with Partner teams.\nImpacts quality, efficiency and effectiveness of own team. Has significant input into priorities.\nIncorporate and prioritize feature requests into product roadmap; Able to translate roadmap into execution\nDesign and implement usability, quality, and delivery of a product or feature\nPlan releases and upgrades with no impacts to business\nHands on expertise in driving quality and best in class Agile engineering practices\nEncourage and motivate the product team to deliver innovative and exciting solutions with an appropriate sense of urgency\nManages progress of work and addresses production issues during sprints\nCommunication with partners to make sure goals are clear and the vision is aligned with business objectives\nDirect management and staff development of team members\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients.\nBasic Qualifications:\nMaster s degree and 8 to 10 years of Information Systems experience OR\nBachelor s degree and 10 to 14 years ofInformation Systems experience OR\nDiploma and 14 to 18 years of Information Systems experience\nThorough understanding of modern web application development and delivery, Gen AI applications development, Data integration and enterprise data fabric concepts, methodologies, and technologies e. g. AWS technologies, Databricks\nDemonstrated experience in building strong teams with consistent practices.\nDemonstrated experience in navigating matrix organization and leading change.\nPrior experience writing business case documents and securing funding for product team delivery; Financial/Spend management for small to medium product teams is a plus.\nIn-depth knowledge of Agile process and principles.\nDefine success metrics for developer productivity metrics; on a monthly/quarterly basis analyze how the product team is performing against established KPI s.\nFunctional Skills:\nLeadership:\nInfluences through Collaboration : Builds direct and behind-the-scenes support for ideas by working collaboratively with others.\nStrategic Thinking : Anticipates downstream consequences and tailors influencing strategies to achieve positive outcomes.\nTransparent Decision-Making : Clearly articulates the rationale behind decisions and their potential implications, continuously reflecting on successes and failures to enhance performance and decision-making.\nAdaptive Leadership : Recognizes the need for change and actively participates in technical strategy planning.\nPreferred Qualifications:\nStrong influencing skills, influence stakeholders and be able to balance priorities.\nPrior experience in vendor management.\nPrior hands-on experience leading full stack development using infrastructure cloud services (AWS preferred) and cloud-native tools and design patterns (Containers, Serverless, Docker, etc. )\nExperience with developing solutions on AWS technologies such as S3, EMR, Spark,\nAthena, Redshift and others\nFamiliarity with cloud security (AWS /Azure/ GCP)\nConceptual understanding of DevOps tools (Ansible/ Chef / Puppet / Docker /Jenkins)\nProfessional Certifications\nAWS Certified Solutions Architect (preferred)\nCertified DevOps Engineer (preferred)\nCertified Agile Leader or similar (preferred)\nSoft Skills:\nStrong desire for continuous learning to pick new tools/technologies.\nHigh attention to detail is essential with critical thinking ability.\nShould be an active contributor on technological communities/forums\nProactively engages with cross-functional teams to resolve issues and design solutions using critical thinking and analysis skills and best practices.\nInfluences and energizes others toward the common vision and goal. Maintains excitement for a process and drives to new directions of meeting the goal even when odds and setbacks render one path impassable\nEstablished habit of proactive thinking and behavior and the desire and ability to self-start/learn and apply new technologies\nExcellent organizational and time-management skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.\nShift Information:\nThis position requires you to work a later shift and may be assigned a second or third shift schedule. Candidates must be willing and able to work during evening or night shifts, as required based on business requirements.\nWhat you can expect of us\nAs we work to develop treatments that take care of others, we also work to care for your professional and personal growth and well-being. From our competitive benefits to our collaborative culture, we ll support your journey every step of the way.\nIn addition to the base salary, Amgen offers competitive and comprehensive Total Rewards Plans that are aligned with local industry standards.\nApply now and make a lasting impact with the Amgen team.\ncareers. amgen. com\nAs an organization dedicated to improving the quality of life for people around the world, Amgen fosters an inclusive environment of diverse, ethical, committed and highly accomplished people who respect each other and live the Amgen values to continue advancing science to serve patients. Together, we compete in the fight against serious disease.\nAmgen is an Equal Opportunity employer and will consider all qualified applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability status, or any other basis protected by applicable law.\nWe will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.\n.",Industry Type: Biotechnology,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Web application development', 'GCP', 'Time management', 'Focus', 'Agile', 'Oncology', 'Public speaking', 'Business case', 'AWS', 'Downstream']",2025-06-10 14:21:03
Principal Machine Learning Engineer,Horizon Therapeutics,2 - 7 years,Not Disclosed,['Hyderabad'],"Career Category Information Systems Job Description Join Amgen s Mission of Serving Patients\nAt Amgen, if you feel like you re part of something bigger, it s because you are. Our shared mission to serve patients living with serious illnesses drives all that we do.\nSince 1980, we ve helped pioneer the world of biotech in our fight against the world s toughest diseases. With our focus on four therapeutic areas -Oncology, Inflammation, General Medicine, and Rare Disease- we reach millions of patients each year. As a member of the Amgen team, you ll help make a lasting impact on the lives of patients as we research, manufacture, and deliver innovative medicines to help people live longer, fuller happier lives.\nOur award-winning culture is collaborative, innovative, and science based. If you have a passion for challenges and the opportunities that lay within them, you ll thrive as part of the Amgen team. Join us and transform the lives of patients while transforming your career.\nWhat you will do\nLet s do this. Let s change the world. We are seeking a highly skilled Machine Learning Engineer with a strong MLOps background to join our team. You will play a pivotal role in building and scaling our machine learning models from development to production. Your expertise in both machine learning and operations will be essential in creating efficient and reliable ML pipelines.\nRoles & Responsibilities:\nCollaborate with data scientists to develop, train, and evaluate machine learning models.\nBuild and maintain MLOps pipelines, including data ingestion, feature engineering, model training, deployment, and monitoring.\nLeverage cloud platforms (AWS, GCP, Azure) for ML model development, training, and deployment.\nImplement DevOps/MLOps best practices to automate ML workflows and improve efficiency.\nDevelop and implement monitoring systems to track model performance and identify issues.\nConduct A/B testing and experimentation to optimize model performance.\nWork closely with data scientists, engineers, and product teams to deliver ML solutions.\nGuide and mentor junior engineers in the team\nStay updated with the latest trends and advancements\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients.\nBasic Qualifications:\nDoctorate degree and 2 years of Computer Science, Statistics, and Data Science, Machine Learning experience OR\nMaster s degree and 8 to 10 years of Computer Science, Statistics, and Data Science, Machine Learning experience OR\nBachelor s degree and 10 to 14 years of Computer Science, Statistics, and Data Science, Machine Learning experience OR\nDiploma and 14 to 18 years of years of Computer Science, Statistics, and Data Science, Machine Learning experience\nPreferred Qualifications:\nMust-Have Skills:\nStrong foundation in machine learning algorithms and techniques\nExperience in MLOps practices and tools (e.g., MLflow, Kubeflow, Airflow); Experience in DevOps tools (e.g., Docker, Kubernetes, CI/CD)\nProficiency in Python and relevant ML libraries (e.g., TensorFlow, PyTorch, Scikit-learn)\nOutstanding analytical and problem-solving skills; Ability to learn quickly; Excellent communication and interpersonal skills\nGood-to-Have Skills:\nExperience with big data technologies (e.g., Spark), and performance tuning in query and data processing\nExperience with data engineering and pipeline development\nExperience in statistical techniques and hypothesis testing, experience with regression analysis, clustering and classification\nKnowledge of NLP techniques for text analysis and sentiment analysis\nExperience in analyzing time-series data for forecasting and trend analysis\nFamiliar with AWS, Azure, or Google Cloud;\nFamiliar with Databricks platform for data analytics and MLOps\nProfessional Certifications\nCloud Computing and Databricks certificate preferred\nSoft Skills:\nExcellent analytical and fixing skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.\nWhat you can expect of us\nAs we work to develop treatments that take care of others, we also work to care for your professional and personal growth and well-being. From our competitive benefits to our collaborative culture, we ll support your journey every step of the way.\nIn addition to the base salary, Amgen offers competitive and comprehensive Total Rewards Plans that are aligned with local industry standards.\nApply now and make a lasting impact with the Amgen team. careers.amgen.com\nAs an organization dedicated to improving the quality of life for people around the world, Amgen fosters an inclusive environment of diverse, ethical, committed and highly accomplished people who respect each other and live the Amgen values to continue advancing science to serve patients. Together, we compete in the fight against serious disease.\nAmgen is an Equal Opportunity employer and will consider all qualified applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability status, or any other basis protected by applicable law.\nWe will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.\n.",Industry Type: Biotechnology,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Cloud computing', 'Performance tuning', 'GCP', 'Analytical', 'Machine learning', 'Oncology', 'Forecasting', 'Monitoring', 'Python']",2025-06-10 14:21:05
SQL DB Architect,Sutherland,9 - 14 years,Not Disclosed,['Hyderabad'],"Job Description\nJob Title: SQL Database Architect (SQL Server, Azure SQL Server, SSIS, SSRS, Data Migration, Azure Data Factory, Power BI)\n\nJob Summary:\nWe are seeking a highly skilled SQL Database Architect with expertise in SQL Server, Azure SQL Server, SSIS, SSRS, Data Migration, and Power BI to design, develop, and maintain scalable database solutions. The ideal candidate will have experience in database architecture, data integration, ETL processes, cloud-based solutions, and business intelligence reporting. Excellent communication and documentation skills are essential for collaborating with cross-functional teams and maintaining structured database records.",,,,"['adf', 'Data Migration', 'SSRS', 'SQL Database', 'ssis', 'T-SQL', 'Power Bi', 'Backup', 'SQL Server', 'ETL']",2025-06-10 14:21:07
Chennai Career Event- Applications invited For Engagement Lead,ExxonMobil,6 - 11 years,Not Disclosed,['Bengaluru'],"Job Title: Chennai Career Event- Applications invited for Engagement Lead\n\nAbout us\nAt ExxonMobil, our vision is to lead in energy innovations that advance modern living and a net-zero future. As one of the worlds largest publicly traded energy and chemical companies, we are powered by a unique and diverse workforce fueled by the pride in what we do and what we stand for.",,,,"['Power Bi', 'Snowflake', 'Tableau', 'S4 Hana']",2025-06-10 14:21:09
Chennai Career Event- Applications invited For Engagement Lead,Exxon Mobil Corporation,6 - 11 years,Not Disclosed,['Bengaluru'],"About us\nWe invite you to bring your ideas to ExxonMobil to help create sustainable solutions that improve quality of life and meet society s evolving needs. Learn more about our What and our Why and how we can work together .\nExxonMobil s affiliates in India\nExxonMobil s affiliates have offices in India in Bengaluru, Mumbai and the National Capital Region.\nExxonMobil s affiliates in India supporting the Product Solutions business engage in the marketing, sales and distribution of performance as well as specialty products across chemicals and lubricants businesses. The India planning teams are also embedded with global business units for business planning and analytics.",,,,"['Market development', 'Data analysis', 'SAP', 'Networking', 'Project management', 'Analytical', 'Business planning', 'Operations', 'Analytics', 'SQL']",2025-06-10 14:21:11
"Databricks Developer- Kolkata, Hyderabad, Bangalore",Immediate Joiner,7 - 12 years,27.5-35 Lacs P.A.,"['Kolkata', 'Hyderabad', 'Bengaluru']","Band 4c & 4D Skill set -Unity Catalog  + Python , Spark , Kafka\nInviting applications for the role of Lead Consultant- Databricks Developer with experience in Unity Catalog + Python , Spark , Kafka for ETL!\nIn this role, the Databricks Developer is responsible for solving the real world cutting edge problem to meet both functional and non-functional requirements.\nResponsibilities\nDevelop and maintain scalable ETL pipelines using Databricks with a focus on Unity Catalog for data asset management.\nImplement data processing frameworks using Apache Spark for large-scale data transformation and aggregation.\nIntegrate real-time data streams using Apache Kafka and Databricks to enable near real-time data processing.\nDevelop data workflows and orchestrate data pipelines using Databricks Workflows or other orchestration tools.\nDesign and enforce data governance policies, access controls, and security protocols within Unity Catalog.\nMonitor data pipeline performance, troubleshoot issues, and implement optimizations for scalability and efficiency.\nWrite efficient Python scripts for data extraction, transformation, and loading.\nCollaborate with data scientists and analysts to deliver data solutions that meet business requirements.\nMaintain data documentation, including data dictionaries, data lineage, and data governance frameworks.\nQualifications we seek in you!\nMinimum qualifications\nBachelors degree in Computer Science, Data Engineering, or a related field.\nexperience in data engineering with a focus on Databricks development.\nProven expertise in Databricks, Unity Catalog, and data lake management.\nStrong programming skills in Python for data processing and automation.\nExperience with Apache Spark for distributed data processing and optimization.\nHands-on experience with Apache Kafka for data streaming and event processing.\nProficiency in SQL for data querying and transformation.\nStrong understanding of data governance, data security, and data quality frameworks.\nExcellent communication skills and the ability to work in a cross-functional environ\nMust have experience in Data Engineering domain .\nMust have implemented at least 2 project end-to-end in Databricks.\nMust have at least experience on databricks which consists of various components as below\nDelta lake\ndbConnect\ndb API 2.0\nDatabricks workflows orchestration\nMust be well versed with Databricks Lakehouse concept and its implementation in enterprise environments.\nMust have good understanding to create complex data pipeline\nMust have good knowledge of Data structure & algorithms.\nMust be strong in SQL and sprak-sql.\nMust have strong performance optimization skills to improve efficiency and reduce cost.\nMust have worked on both Batch and streaming data pipeline.\nMust have extensive knowledge of Spark and Hive data processing framework.\nMust have worked on any cloud (Azure, AWS, GCP) and most common services like ADLS/S3, ADF/Lambda, CosmosDB/DynamoDB, ASB/SQS, Cloud databases.\nMust be strong in writing unit test case and integration test\nMust have strong communication skills and have worked on the team of size 5 plus\nMust have great attitude towards learning new skills and upskilling the existing skills.\nPreferred Qualifications\nGood to have Unity catalog and basic governance knowledge.\nGood to have Databricks SQL Endpoint understanding.\nGood To have CI/CD experience to build the pipeline for Databricks jobs.\nGood to have if worked on migration project to build Unified data platform.\nGood to have knowledge of DBT.\nGood to have knowledge of docker and Kubernetes.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Kafka', 'Spark', 'Unity Catalog', 'Python']",2025-06-10 14:21:14
Data Consultant-Data Governance,IBM,3 - 6 years,Not Disclosed,['Hyderabad'],"As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You’ll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you’ll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you’ll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\n\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviour’s.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience in the integration efforts between Alation and Manta, ensuring seamless data flow and compatibility.\nCollaborate with cross-functional teams to gather requirements and design solutions that leverage both Alation and Manta platforms effectively.\nDevelop and maintain data governance processes and standards within Alation, leveraging Manta's data lineage capabilities.\nAnalyze data lineage and metadata to provide insights into data quality, compliance, and usage patterns..\n\n\nPreferred technical and professional experience\nLead the evaluation and implementation of new features and updates for both Alation and Manta platforms\nEnsuring alignment with organizational goals and objectives.\nDrive continuous improvement initiatives to enhance the efficiency and effectiveness of data management processes, leveraging Alati",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['elastic search', 'data quality', 'data governance', 'splunk', 'agile', 'ab initio', 'metadata', 'data management', 'data analysis', 'data warehousing', 'business intelligence', 'sql', 'data cleansing', 'tableau', 'data modeling', 'big data', 'etl', 'informatica', 'data integration', 'data profiling']",2025-06-10 14:21:16
Senior Data Scientist,Go Digital Technology Consulting,4 - 7 years,Not Disclosed,"['Pune', 'Mumbai (All Areas)']","Job Title: Sr. Data Scientist\nLocation: Mumbai / Pune\nJob Type: Full-time\nExperience: 4-7 years\n\nAbout the Role:\nWe are seeking a Sr. Data Scientist with experience in Computer Vision to join our team. You will be responsible for building and maintaining models including but not limited to Vision Transformer neural networks. This role involves collaborating with data engineers and backend developers to deliver quality AI solutions.\n\nResponsibilities:\nDesign and implement end-to-end machine learning workflows for image and computer vision applications, from data collection to model deployment.\nCollaborate with cross-functional teams, including data engineers, product managers, and domain experts, to define and prioritize machine learning initiatives.\nDocument technical designs and model specifications, ensuring clarity and accessibility for stakeholders and team members.\nEnsure adherence to best practices in model development, deployment, and monitoring, in alignment with the overall AI strategy.\nMonitor model performance and implement strategies for continuous improvement and retraining as needed.\nDevelop scalable and efficient deep learning models using PyTorch, optimizing for performance and resource utilization.\n\nQualifications:\nBachelors degree in Computer Science or a related field.\n4-7 years of hands-on experience in developing and deploying machine learning models, particularly in computer vision tasks.\nProficient in using PyTorch for developing deep learning models, with a strong understanding of CNNs, transfer learning, vision transformers, and data augmentation techniques.\nSolid understanding of computer vision concepts, including image classification, object detection, and image segmentation.\nStrong programming skills in Python, with experience in data manipulation libraries such as NumPy and Pandas.\nExperience with version control systems like Git.\nExcellent analytical and problem-solving skills, strong communication abilities, and a collaborative mindset.\n\nPreferred Qualifications:\nExperience with cloud platforms (e.g., AWS, GCP, Azure) and their ML services, particularly those related to model deployment and GPU training.\nUnderstanding of MLOps principles and practices, including model monitoring, versioning, and governance.\nKnowledge of GPU computing and tools for managing GPU resources (e.g., CUDA, cuDNN).",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Senior Data Scientist', 'Computer Vision', 'Pytorch', 'Pandas', 'Machine Learning', 'Numpy', 'Python']",2025-06-10 14:21:19
Sr. Python developer with AWS & SQL,Fortune India 500 IT Services Firm,5 - 9 years,Not Disclosed,"['Noida', 'Pune', 'Bengaluru']","We are seeking a skilled Python Developer with strong experience in Amazon Web Services (AWS) and SQL to join our growing development team.\n\nExperience- 5 to 9 years\nLocation- Pan India\n\nKey Responsibilities:\nDevelop and maintain robust backend systems using Python\nDesign and implement cloud-based solutions using AWS services such as EC2, Lambda, S3, RDS, etc.\nWrite efficient and optimized SQL queries for data extraction, transformation, and analysis\nCollaborate with frontend developers, data engineers, and DevOps teams\nOptimize applications for performance, scalability, and reliability\nMaintain high code quality through code reviews, unit testing, and documentation\n\n\n\nInterested candidates share your CV at himani.girnar@alikethoughts.com with below details\n\nCandidate's name-\nEmail and Alternate Email ID-\nContact and Alternate Contact no-\nTotal exp-\nRelevant experience-\nCurrent Org-\nNotice period-\nCCTC-\nECTC-\nCurrent Location-\nPreferred Location-\nPancard No-",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AWS', 'Python', 'SQL', 'Postgresql', 'MySQL', 'SQL Queries']",2025-06-10 14:21:21
DevOps Engineer - Lead,Blend360 India,6 - 8 years,Not Disclosed,['Hyderabad'],"Job Description\nWe are seeking a highly skilled Lead Azure DevOps Engineer to join our team and drive the end-to-end deployment, scalability, and operationalization of machine learning models in production. You will collaborate closely with data scientists, data engineers, and DevOps teams to ensure seamless CI/CD, reproducibility, monitoring, and governance of ML pipelines\nKey Responsibilities\nDesign, implement, and maintain CI/CD pipelines for deploying and monitoring microservices efficiently.\nManage infrastructure as code using Terraform for repeatable and scalable provisioning.\nDeploy and optimize containerized applications using Docker and Azure services (Container Apps, Container Registry, Key Vault, Service Bus, Blob Storage).\nApply best practices for securing Docker images, including vulnerability scanning, reducing image size, and optimizing build efficiency.\nImplement and maintain Azure Monitor for logging, monitoring, and alerting to ensure system reliability.\nEnsure security best practices across cloud environments, including secrets management, access control, and compliance.\n(Nice to have) Design and manage multi-client architectures within shared pipelines and storage accounts in Azure Blob Storage\n\n\nQualifications\n6+ years of experience in DevOps or MLOps with a strong focus on production-grade ML solutions.\nStrong expertise in Azure, particularly with CI/CD, container orchestration, and cloud security. Profi",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'orchestration', 'cloud security', 'devops', 'Machine learning', 'Infrastructure', 'Vulnerability', 'Management', 'Monitoring', 'microservices']",2025-06-10 14:21:23
Sr. Databricks Engineer,"Milestone Technologies, Inc",4 - 6 years,Not Disclosed,['Kochi'],"In this vital role you will be responsible for the development and implementation of our data strategy. The ideal candidate possesses a strong blend of technical expertise and data-driven problem-solving skills. As a Data Engineer, you will play a crucial role in building, and optimizing our data pipelines and platforms in a SAFE Agile product team.\nChip in to the design, development, and implementation of data pipelines, ETL/ELT processes, and data integration solutions.\nDeliver for data pipeline projects from development to deployment, managing, timelines, and risks.\nEnsure data quality and integrity through meticulous testing and monitoring.\nLeverage cloud platforms (AWS, Databricks) to build scalable and efficient data solutions.\nWork closely with product team, and key collaborators to understand data requirements.\nEnforce to data engineering industry standards and standards.\nExperience developing in an Agile development environment, and comfortable with Agile terminology and ceremonies.\nFamiliarity with code versioning using GIT and code migration tools.\nFamiliarity with JIRA.\nStay up to date with the latest data technologies and trends\nWhat we expect of you\nBasic Qualifications:\nDoctorate degree OR\nMaster s degree and 4 to 6 years of Information Systems experience OR\nBachelor s degree and 6 to 8 years of Information Systems experience OR\nDiploma and 10 to 12 years of Information Systems experience.\nDemonstrated hands-on experience with cloud platforms (AWS, Azure, GCP)\nProficiency in Python, PySpark, SQL.\nDevelopment knowledge in Databricks.\nGood analytical and problem-solving skills to address sophisticated data challenges.\nPreferred Qualifications:\nExperienced with data modeling\nExperienced working with ETL orchestration technologies\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and DevOps\nFamiliarity with SQL/NOSQL database\nSoft Skills:\nSkilled in breaking down problems, documenting problem statements, and estimating efforts.\nEffective communication and interpersonal skills to collaborate with multi-functional teams.\nExcellent analytical and problem solving skills.\nStrong verbal and written communication skills\nAbility to work successfully with global teams\nHigh degree of initiative and self-motivation.\nTeam-oriented, with a focus on achieving team goals",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Maven', 'Version control', 'Data modeling', 'Subversion', 'Data quality', 'Unit testing', 'JIRA', 'Monitoring', 'SQL', 'Python']",2025-06-10 14:21:25
Software Applications Development Engineer,"NTT DATA, Inc.",5 - 10 years,Not Disclosed,"['Mumbai', 'Hyderabad', 'Bengaluru']","Your day at NTT DATA\nThe Software Applications Development Engineer is a seasoned subject matter expert, responsible for developing new applications and improving upon existing applications based on the needs of the internal organization and or external clients.\nWhat you'll be doing\nYrs. Of Exp: 5 Yrs.\n\nData Engineer-\nWork closely with Lead Data Engineer to understand business requirements, analyse and translate these requirements into technical specifications and solution design.\nWork closely with Data modeller to ensure data models support the solution design\nDevelop , test and fix ETL code using Snowflake, Fivetran, SQL, Stored proc.\nAnalysis of the data and ETL for defects/service tickets (for solution in production ) raised and service tickets.\nDevelop documentation and artefacts to support projects.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Application Development', 'Data Engineering', 'Snowflake', 'ETL', 'Fivetran', 'SQL']",2025-06-10 14:21:28
Sr Data Architect,HMH,5 - 10 years,Not Disclosed,['Pune'],"The data architect is responsible for designing, creating, and managing an organizations data architecture. This role is critical in establishing a solid foundation for data management within an organization, ensuring that data is organized, accessible, secure, and aligned with business objectives. The data architect designs data models, warehouses, file systems and databases, and defines how data will be collected and organized.\nResponsibilities\nInterprets and delivers impactful strategic plans improving data integration, data quality, and data delivery in support of business initiatives and roadmaps\nDesigns the structure and layout of data systems, including databases, warehouses, and lakes\nSelects and designs database management systems that meet the organizations needs by defining data schemas, optimizing data storage, and establishing data access controls and security measures\nDefines and implements the long-term technology strategy and innovations roadmaps across analytics, data engineering, and data platforms\nDesigns processes for the ETL process from various sources into the organizations data systems\nTranslates high-level business requirements into data models and appropriate metadata, test data, and data quality standards\nManages senior business stakeholders to secure strong engagement and ensures that the delivery of the project aligns with longer-term strategic roadmaps\nSimplifies the existing data architecture, delivering reusable services and cost-saving opportunities in line with the policies and standards of the company\nLeads and participates in the peer review and quality assurance of project architectural artifacts across the EA group through governance forums\nDefines and manages standards, guidelines, and processes to ensure data quality\nWorks with IT teams, business analysts, and data analytics teams to understand data consumers needs and develop solutions\nEvaluates and recommends emerging technologies for data management, storage, and analytics\nDesign, create, and implement logical and physical data models for both IT and business solutions to capture the structure, relationships, and constraints of relevant datasets\nBuild and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions\nEffectively collaborate and communicate with various stakeholders to understand data and business requirements and translate them into data models\nCreate entity-relationship diagrams (ERDs), data flow diagrams, and other visualization tools to represent data models\nCollaborate with database administrators and software engineers to implement and maintain data models in databases, data warehouses, and data lakes\nDevelop data modeling best practices, and use these standards to identify and resolve data modeling issues and conflicts\nConduct performance tuning and optimization of data models for efficient data access and retrieval\nIncorporate core data management competencies, including data governance, data security and data quality\nJob Requirements\nEducation:\nA bachelors degree in computer science, data science, engineering, or related field\nExperience:\nAt least five years of relevant experience in design and implementation of data models for enterprise data warehouse initiatives\nExperience leading projects involving data warehousing, data modeling, and data analysis\nDesign experience in Azure Databricks, PySpark, and Power BI/Tableau\nSkills:\nAbility in programming languages such as Java, Python, and C/C++\nAbility in data science languages/tools such as SQL, R, SAS, or Excel\nProficiency in the design and implementation of modern data architectures and concepts such as cloud services (AWS, Azure, GCP), real-time data distribution (Kafka, Dataflow), and modern data warehouse tools (Snowflake, Databricks)\nExperience with database technologies such as SQL, NoSQL, Oracle, Hadoop, or Teradata\nUnderstanding of entity-relationship modeling, metadata systems, and data quality tools and techniques\nAbility to think strategically and relate architectural decisions and recommendations to business needs and client culture\nAbility to assess traditional and modern data architecture components based on business needs\nExperience with business intelligence tools and technologies such as ETL, Power BI, and Tableau\nAbility to regularly learn and adopt new technology, especially in the ML/AI realm\nStrong analytical and problem-solving skills\nAbility to synthesize and clearly communicate large volumes of complex information to senior management of various technical understandings\nAbility to collaborate and excel in complex, cross-functional teams involving data scientists, business analysts, and stakeholders\nAbility to guide solution design and architecture to meet business needs\nExpert knowledge of data modeling concepts, methodologies, and best practices\nProficiency in data modeling tools such as Erwin or ER/Studio\nKnowledge of relational databases and database design principles\nFamiliarity with dimensional modeling and data warehousing concepts\nStrong SQL skills for data querying, manipulation, and optimization, and knowledge of other data science languages, including JavaScript, Python, and R\nAbility to collaborate with cross-functional teams and stakeholders to gather requirements and align on data models\nExcellent analytical and problem-solving skills to identify and resolve data modeling issues\nStrong communication and documentation skills to effectively convey complex data modeling concepts to technical and business stakeholders",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Architecture', 'Java', 'Azure', 'data analysis', 'SAS', 'PySpark', 'Hadoop', 'Azure Databricks', 'data warehousing', 'Teradata', 'SQL', 'C/C++', 'Power BI/Tableau', 'R', 'NoSQL', 'data modeling', 'GCP', 'Snowflake', 'metadata systems', 'Databricks', 'Oracle', 'AWS', 'Python']",2025-06-10 14:21:30
Application Developer-Process Management (BPM),IBM,3 - 8 years,Not Disclosed,['Bengaluru'],"As an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\n\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\n\n\nIn this role, your responsibilities may include:\nMinimum of 3 years of recent experience with Appian software development and designing.\nGood understanding of database concepts and strong working knowledge on any one of the major databases such as Oracle SQL, Server MySQL.\nDesign, develop, and implement business process workflows using BPM tools (e.g., IBM BPM, Pega, Camunda) Automate business processes to improve efficiency and reduce manual effort.\nIntegrate BPM solutions with enterprise systems (ERP, CRM, etc.) via APIs and web services.\nTroubleshoot, test, and optimize BPM applications for performance. Document process designs, configurations, and technical specifications.\nCollaborate with business analysts, IT teams, and stakeholders to ensure solutions meet business needs. Provide ongoing support and maintenance for deployed BPM applications\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nMinimum of 3 years of recent experience with Appian software development and designing.\nGood understanding of database concepts and strong working knowledge on any one of the major databases such as Oracle SQL, Server MySQL.\nDesign, develop, and implement business process workflows using BPM tools (e.g., IBM BPM, Pega, Camunda) Automate business processes to improve efficiency and reduce manual effort.\nIntegrate BPM solutions with enterprise systems (ERP, CRM, etc.) via APIs and web services\n\n\nPreferred technical and professional experience\n5-7 years of experience in BPM.\n2+ years of experience with Appian.\nAbility to communicate results to technical and non-technical audiences",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['software development', 'dbms', 'ibm bpm', 'appian', 'business process management', 'analysts', 'erp', 'data management', 'mortgage advisors', 'process development', 'process design', 'accounting', 'hibernate', 'camunda bpm', 'spring', 'spring boot', 'java', 'mysql', 'pega', 'oracle sql', 'api', 'crm']",2025-06-10 14:21:33
F2F Drive-14th June-Bangalore LTIM - Data Scientist,Ltimindtree,6 - 11 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']",DS\n\nKey Responsibilities\nCombine expertise in mathematics statistics computer science and domain knowledge to create AIML models to solve various business challenges\nCollaborate closely with the AI Technical Manager and GCC Petro technical professionals and data engineers to integrate models into the business framework\nIdentify and frame opportunities to apply advanced analytics modeling and related technologies to data to help businesses gain insight and improve decision making workflow and automation\nUnderstand and communicate the value of proposed opportunity with team members and other stakeholders\nIdentify needed data and appropriate technology to solve identified business challenges\nClean data and develop and test models\nEstablish the life cycle management process for models\nProvide technical mentoring in modeling and analytics technologies the specifics of the modeling process and general consulting skills\nDrive innovation in AIML to enhance capabilities in data driven decision making\nAligns with team on shared goals and outcomes recognizes others contributions and work collaboratively seek diverse perspectives\nTakes actions to develop self and others beyond existing skillset\nEncourages innovative ideas adapts to change and changing technologies\nUnderstand and communicate data insights and model behaviors to stakeholders with varying levels of technical expertise\nRequired Qualification\nMinimum 5 years of experience in designing and developing AIML models and or various optimization algorithms 5 to 9 years of experience\nSolid foundation in mathematics probability and statistics with demonstrated depth of knowledge and experience in advanced analytics and data science methodologies eg supervised and unsupervised learning statistics data science model development\nProficiency in Python and working knowledge of cloud AIML services Azure Machine Learning and Databricks preferred\nDomain knowledge relevant to the energy sector and working knowledge of Oil and Gas value chain eg upstream midstream or downstream and associated business workflows\nProven ability to frame data science opportunities leverage standard foundational tools and Azure services to perform exploratory data analysis for purposes of data cleaning and discovery visualize data and identify actions to reach needed results\nAbility to quickly assess current state and apply technical concepts across cross functional business workflows\nExperience with driving successful execution deliverables and accountabilities to meet quality and schedule goals\nAbility to translate complex data into actionable insights that drive business val\nDemonstrated ability to engage and establish collaborative relationships both inside and outside immediate workgroup at various organizational levels across functional and geographic boundaries to achieve desired outcomes\nDemonstrated ability to adjust behavior based on feedback and provide feedback to other\nTeam oriented mindset with effective communication skills and the ability to work collaboratively\nStrong problem solving skills and attention to detail\nExcellent communication and collaboration skills,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Machine Learning', 'GCP', 'Data Scientist', 'Python', 'Predictive Modeling', 'Azure', 'Generative AI', 'Natural Language Processing', 'Deep Learning', 'Data Science', 'Azure Machine Learning', 'Computer Vision', 'AWS']",2025-06-10 14:21:36
PYTHON DEVELOPER,Capgemini,2 - 7 years,Not Disclosed,['Mumbai'],"\n\nYour Role \n\nPython Developer\n\nAs a Python developer you must have 2+ years in Python / Pyspark.\nStrong programming experience, Python, Pyspark, Scala is preferred.\nExperience in designing and implementing CI/CD, Build Management, and Development strategy.\nExperience with SQL and SQL Analytical functions, experience participating in key business, architectural and technical decisions\nScope to get trained on AWS cloud technology\n\n \n\nYour Profile \nPython\nSQL\nData Engineer\n\n\n \n\nWhat youll love about working here \n\nChoosing Capgemini means having the opportunity to make a difference, whether for the worlds leading businesses or for society. It means getting the support you need to shape your career in the way that works for you. It means when the future doesnt look as bright as youd like, you have the opportunity to make changeto rewrite it.\nWhen you join Capgemini, you dont just start a new job. You become part of something bigger. A diverse collective of free-thinkers, entrepreneurs and experts, all working together to unleash human energy through technology, for an inclusive and sustainable future. At Capgemini, people are at the heart of everything we do! You can exponentially grow your career by being part of innovative projects and taking advantage of our extensive Learning & Development programs. With us, you will experience an inclusive, safe, healthy, and flexible work environment to bring out the best\nin you! You also get a chance to make positive social change and build a better world by taking an active role in our Corporate Social Responsibility and Sustainability initiatives. And whilst you make a difference, you will also have a lot of fun.\n\n \n\nAbout Capgemini",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'scala', 'pyspark', 'ci/cd', 'python', 'arinc', 'vhdl', 'sql', 'docker', 'java', 'git', 'fpga', 'aws cloud', 'devops', 'linux', 'jenkins', 'mysql', 'github', 'c', 'software testing', 'python development', 'avionics', 'machine learning', 'javascript', 'verilog', 'aws']",2025-06-10 14:21:38
Data Scientist,Qua Xigma IT Solutions Pvt Ltd,2 - 7 years,Not Disclosed,['Tirupati'],"Position Overview:\nWe are seeking a collaborative and analytical Data Scientist who can bridge the gap between business needs and data science capabilities. In this role, you will lead and support projects that apply machine learning, AI, and statistical modeling to generate actionable insights and drive business value.\n\nKey Responsibilities:\nCollaborate with stakeholders to define and translate business challenges into data science solutions.\nConduct in-depth data analysis on structured and unstructured datasets.\nBuild, validate, and deploy machine learning models to solve real-world problems.\nDevelop clear visualizations and presentations to communicate insights.\nDrive end-to-end project delivery, from exploration to production.\nContribute to team knowledge sharing and mentorship activities.\n\nMust-Have Skills:\n3+ years of progressive experience in data science, applied analytics, or a related quantitative role, demonstrating a proven track record of delivering impactful data-driven solutions.\nExceptional programming proficiency in Python, including extensive experience with core libraries such as Pandas, NumPy, Scikit-learn, NLTK and XGBoost.\nExpert-level SQL skills for complex data extraction, transformation, and analysis from various relational databases.\nDeep understanding and practical application of statistical modeling and machine learning techniques, including but not limited to regression, classification, clustering, time series analysis, and dimensionality reduction.\nProven expertise in end-to-end machine learning model development lifecycle, including robust feature engineering, rigorous model validation and evaluation (e.g., A/B testing), and model deployment strategies.\nDemonstrated ability to translate complex business problems into actionable analytical frameworks and data science solutions, driving measurable business outcomes.\nProficiency in advanced data analysis techniques, including Exploratory Data Analysis (EDA), customer segmentation (e.g., RFM analysis), and cohort analysis, to uncover actionable insights.\nExperience in designing and implementing data models, including logical and physical data modeling, and developing source-to-target mappings for robust data pipelines.\nExceptional communication skills, with the ability to clearly articulate complex technical findings, methodologies, and recommendations to diverse business stakeholders (both technical and non-technical audiences).\nExperience in designing and implementing data models, including logical and physical data modeling, and developing source-to-target mappings for robust data pipelines.\nExceptional communication skills, with the ability to clearly articulate complex technical findings, methodologies, and recommendations to diverse business stakeholders (both technical and non-technical audiences).\n\nGood-to-Have Skills:\nExperience with cloud platforms (Azure, AWS, GCP) and specific services like Azure ML, Synapse, Azure Kubernetes and Databricks.\nFamiliarity with big data processing tools like Apache Spark or Hadoop.\nExposure to MLOps tools and practices (e.g., MLflow, Docker, Kubeflow) for model lifecycle management.\nKnowledge of deep learning libraries (TensorFlow, PyTorch) or experience with Generative AI (GenAI) and Large Language Models (LLMs).\nProficiency with business intelligence and data visualization tools such as Tableau, Power BI, or Plotly.\nExperience working within Agile project delivery methodologies.\n\nCompetencies:\nTech Savvy - Anticipating and adopting innovations in business-building digital and technology applications.\nSelf-Development - Actively seeking new ways to grow and be challenged using both formal and informal development channels.\nAction Oriented - Taking on new opportunities and tough challenges with a sense of urgency, high energy, and enthusiasm.\nCustomer Focus - Building strong customer relationships and delivering customer-centric solutions.\nOptimizes Work Processes - Knowing the most effective and efficient processes to get things done, with a focus on continuous improvement.\n\nWhy Join Us?\nBe part of a collaborative and agile team driving cutting-edge AI and data engineering solutions.\nWork on impactful projects that make a difference across industries.\nOpportunities for professional growth and continuous learning.\nCompetitive salary and benefits package.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Python', 'SQL', 'End-to-End ML Development', 'Data Analysis', 'Data Modeling & ETL']",2025-06-10 14:21:40
Scrum Lead- Project Management,Metlife,8 - 12 years,Not Disclosed,['Hyderabad'],"Position Summary\n\nMetLife established a Global capability center (MGCC) in India to scale and mature Data & Analytics, technology capabilities in a cost-effective manner and make MetLife future ready. The center is integral to Global Technology and Operations with a with a focus to protect & build MetLife IP, promote reusability and drive experimentation and innovation. The Data & Analytics team in India mirrors the Global D&A team with an objective to drive business value through trusted data, scaled capabilities, and actionable insights. The operating models consists of business aligned data officers- US, Japan and LatAm & Corporate functions enabled by enterprise COEs- data engineering, data governance and data science.\n\nRole Value Proposition:\nDriven by passion and a zeal to succeed, we are looking for accomplished Program Manager to structure, plan and handle multiple projects with minimum supervision and will be responsible for successful completion of projects supporting MGCC and US D&A leadership with various strategic initiatives in development and successful implementation of governance and process excellence practices.\nThis position would be responsible for complete adherence of the projects and its objectives and support all aspects of project management. This role will support development of best practices, processes and framework to achieve standardization and streamlining across various initiatives.\n\nJob Responsibilities\nServe as analytics program manager on data, analytics projects and POCs working with data engineers, business analysts, data scientists, IT teams, vendors, executive leaders, and business stakeholders\nDrive transparency leveraging tech stack and data, own progress reporting and proactively communicate status\nDrive delivery of projects using Agile methodology for data and analytics programs\nFacilitate scrum ceremonies including Sprint planning, Daily stand ups, sprint reviews and retrospectives\nResponsible for defining relevant program metrics, status reports and continuous measurement of program portfolio best practices\nLead, coach, support and mentor junior team members\nInteract with senior leadership teams across Data and analytics, IT and business teams.\n\nKnowledge, Skills and Abilities\nBachelors degree. Technology/IT specialization is preferred.\nMBA is a preferred qualification\n8-12+ years of progressive experience in project/program management role with proven people influencing experience including with virtual and global teams\nAgile project management/delivery experience is a must preferably with Data and Analytics background\nProficient in MS Office suite: Excel, PowerPoint, Project.\nUnderstanding of analytical tool stack, Azure Devboards, Jira, SharePoint is a plus\nCSM, SAFe Agilist certifications are preferred\nAbility to identify risks to project success and recommend course of action to prevent risk from negatively impacting the project; Effectively recognize when to escalate issues and options to senior management for resolution\nSuperior solutioning techniques, organizational skills and ability to manage multiple ongoing projects.\nExcellent collaboration and communication skills, both written and verbal\nDemonstrated competency with cross-group collaboration, organizational agility, and analytical planning\nStrong leadership & negotiation skills.",Industry Type: Insurance,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Agile Methodology', 'Sprint Planning', 'scrum', 'Azure Devops', 'Agile Framework', 'Backlog Refinement', 'Scrum Development', 'Sprint Review', 'Retrospective']",2025-06-10 14:21:42
Ml Engineer,Solix Technologies,5 - 10 years,15-25 Lacs P.A.,['Hyderabad'],"Solix Technologies Inc. is a leading provider of big data applications for enterprise archiving, data privacy, and advanced analytics. We are on a mission to help organizations manage and leverage their data for maximum value, efficiency, and compliance.\nJob Summary:\nWe are seeking a highly skilled and motivated Machine Learning Engineer with hands-on experience in Natural Language Processing (NLP) and Large Language Models (LLMs). You will play a key role in designing, developing, and deploying scalable ML/NLP solutions that drive intelligent automation and data insight across our platforms.\nKey Responsibilities:\nDesign and develop machine learning models, particularly in the domain of NLP and LLMs.\nFine-tune, evaluate, and deploy transformer-based models (e.g., BERT, GPT, T5, LLaMA, etc.).\nApply techniques such as named entity recognition (NER), text classification, semantic search, summarization, and question answering.\nWork with large-scale datasets to extract insights and build data pipelines.\nCollaborate with cross-functional teams including data engineers, product managers, and software developers.\nConduct experiments, model training, and optimization to improve accuracy and performance.\nStay up-to-date with the latest research in NLP, LLMs, and machine learning.\nRequired Skills and Experience:\nBachelor's or Masters degree in Computer Science, Data Science, AI/ML, or a related field.\nMinimum 5+ years of hands-on experience with NLP and LLMs\nProficient in Python and ML frameworks like TensorFlow, PyTorch, Hugging Face Transformers.\nStrong understanding of modern NLP techniques (tokenization, embeddings, attention mechanisms, etc.).\nExperience with ML lifecycle including model development, evaluation, and deployment (MLOps).\nFamiliarity with data handling libraries (Pandas, NumPy) and cloud platforms (AWS, GCP, or Azure).\nGood understanding of data preprocessing, feature engineering, and model validation techniques.\nExperience with open-source LLM fine-tuning and deployment.\nKnowledge of vector databases (e.g., FAISS, Pinecone) and retrieval-augmented generation (RAG).\nPrior experience with large-scale data systems or enterprise data environments.\nPublished papers or open-source contributions in the ML/NLP space.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['LLm', 'Large Language Model', 'Natural Language Processing', 'Ml Algorithms']",2025-06-10 14:21:44
Scala Developer,Wipro,0 - 4 years,Not Disclosed,['Pune'],"Wipro Limited (NYSEWIT, BSE507685, NSEWIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients’ most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.\n About The Role  \n\n \n\nRole Purpose  \nThe purpose of the role is to support process delivery by ensuring daily performance of the Production Specialists, resolve technical escalations and develop technical capability within the Production Specialists.\n\n\n ? \n\n \n\nDo  \n\n\nOversee and support process by reviewing daily transactions on performance parameters\nReview performance dashboard and the scores for the team\nSupport the team in improving performance parameters by providing technical support and process guidance\nRecord, track, and document all queries received, problem-solving steps taken and total successful and unsuccessful resolutions\nEnsure standard processes and procedures are followed to resolve all client queries\nResolve client queries as per the SLA’s defined in the contract\nDevelop understanding of process/ product for the team members to facilitate better client interaction and troubleshooting\nDocument and analyze call logs to spot most occurring trends to prevent future problems\nIdentify red flags and escalate serious client issues to Team leader in cases of untimely resolution\nEnsure all product information and disclosures are given to clients before and after the call/email requests\nAvoids legal challenges by monitoring compliance with service agreements\n\n\n ? \n\n\nHandle technical escalations through effective diagnosis and troubleshooting of client queries\nManage and resolve technical roadblocks/ escalations as per SLA and quality requirements\nIf unable to resolve the issues, timely escalate the issues to TA & SES\nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers’ and clients’ business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract SLA’s\n\n\n ? \n\n\nBuild people capability to ensure operational excellence and maintain superior customer service levels of the existing account/client\nMentor and guide Production Specialists on improving technical knowledge\nCollate trainings to be conducted as triage to bridge the skill gaps identified through interviews with the Production Specialist\nDevelop and conduct trainings (Triages) within products for production specialist as per target\nInform client about the triages being conducted\nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nIdentify and document most common problems and recommend appropriate resolutions to the team\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\n\n ? \n\nDeliver\nNoPerformance ParameterMeasure1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback, NSAT/ ESAT2Team ManagementProductivity, efficiency, absenteeism3Capability developmentTriages completed, Technical Test performance\n\nMandatory\n\nSkills:\nScala programming.\n\nExperience5-8 Years.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'scala', 'scala programming', 'java', 'spark', 'play framework', 'rest', 'python', 'apache pig', 'akka', 'sql', 'microservices', 'data bricks', 'git', 'mapreduce', 'linux', 'j2ee', 'hadoop', 'sqoop', 'agile', 'hbase']",2025-06-10 14:21:46
Python Software Developer,66degrees,5 - 8 years,18-25 Lacs P.A.,"['Mysuru', 'Pune', 'Bengaluru']","Overview of 66degrees\n66degrees is a leading consulting and professional services company specializing in developing AI-focused, data-led solutions leveraging the latest advancements in cloud technology. With our unmatched engineering capabilities and vast industry experience, we help the world's leading brands transform their business challenges into opportunities and shape the future of work.\nAt 66degrees, we believe in embracing the challenge and winning together. These values not only guide us in achieving our goals as a company but also for our people. We are dedicated to creating a significant impact for our employees by fostering a culture that sparks innovation and supports professional and personal growth along the way.",,,,"['Python Development', 'Java', 'Software Development', 'GCP', 'Javascript', 'Google Cloud Platforms']",2025-06-10 14:21:48
Big Data Developer,Persistent,4 - 8 years,Not Disclosed,['Bengaluru'],"About Persistent\nWe are a trusted Digital Engineering and Enterprise Modernization partner, combining deep technical expertise and industry experience to help our clients anticipate what’s next. Our offerings and proven solutions create a unique competitive advantage for our clients by giving them the power to see beyond and rise above. We work with many industry-leading organizations across the world including 12 of the 30 most innovative US companies, 80% of the largest banks in the US and India, and numerous innovators across the healthcare ecosystem.\nOur growth trajectory continues, as we reported $1,231M annual revenue (16% Y-o-Y). Along with our growth, we’ve onboarded over 4900 new employees in the past year, bringing our total employee count to over 23,500+ people located in 19 countries across the globe.",,,,"['big data administration', 'hive', 'cloudera', 'data', 'business requirements', 'scala', 'big data technologies', 'data architecture', 'coding', 'java', 'spark', 'hadoop', 'big data', 'hbase', 'programming', 'python', 'software development', 'software testing', 'performance tuning', 'engineering', 'angular', 'hdfs', 'agile', 'sdlc', 'informatica']",2025-06-10 14:21:50
Snowflake Implementer,TechStar Group,9 - 14 years,10-20 Lacs P.A.,"['Chennai', 'Bengaluru', 'Mumbai (All Areas)']","JD:\nSnowflake Implementer :\nDesigning, implementing, and managing Snowflake data warehouse solutions, ensuring data integrity, and optimizing performance for clients or internal teams.\nStrong SQL skills: Expertise in writing, optimizing, and troubleshooting SQL queries.\nExperience with data warehousing: Understanding of data warehousing concepts, principles, and best practices.\nKnowledge of ETL /ELT technologies: Experience with tools and techniques for data extraction, transformation, and loading.\nExperience with data modeling: Ability to design and implement data models that meet business requirements.\nFamiliarity with cloud platforms: Experience with cloud platforms like AWS, Azure, or GCP (depending on the specific Snowflake environment).\nProblem-solving and analytical skills: Ability to identify, diagnose, and resolve technical issues.\nCommunication and collaboration skills: Ability to work effectively with cross-functional teams.\nExperience with Snowflake (preferred): Prior experience with Snowflake is highly desirable.\nCertifications (preferred): Snowflake certifications (e.g., Snowflake Data Engineer, Snowflake Database Administrator) can be a plus. Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'ETL', 'Snowflake certifications', 'SQL', 'Cloud Platforms', 'ELT']",2025-06-10 14:21:52
"Senior Developer, Power BI",Hollister Medical India,3 - 8 years,Not Disclosed,['Gurugram'],"Senior Developer, Power BI Job Details | Hollister Incorporated Search by Keyword Search by Location (City, State, Country) Select how often (in days) to receive an alert: Select how often (in days) to receive an alert: Senior Developer, Power BI Jun 6, 2025 Gurugram, HR, IN, 122002 Hollister Global Business Services India Private L Summary:\nThe Senior Power BI Developer is responsible for designing, developing, and maintaining business intelligence solutions using Power BI. The role involves gathering requirements from stakeholders, creating data models, developing interactive dashboards, and optimizing report performance.\nThis position requires strong skills in data analysis, DAX, SQL, and Power BI best practices to ensure accurate and efficient reporting. The Power BI Developer works closely with business teams to transform data into meaningful insights, ensuring reports are clear, secure, and aligned with business needs. Testing, validation, and continuous improvement are key aspects of the role to support data-driven decision-making.\nResponsibilities:\nGather and analyze business requirements for reporting and data visualization needs.\nDesign and develop Power BI dashboards, reports, and data models to provide actionable insights.\nCreate and optimize DAX calculations for performance and accuracy.\nDevelop and maintain SQL queries to extract, transform, and load data.\nEnsure data accuracy, consistency, and security within Power BI reports.\nCollaborate with business users to refine dashboards and improve usability.\nOptimize report performance by managing data sources, relationships, and query efficiency.\nConduct testing and validation to ensure reports meet business needs.\nProvide documentation and training for end-users on Power BI solutions.\nStay updated on Power BI features and best practices to enhance reporting capabilities.\nConfigure and manage workspaces, data refresh schedules, row-level security (RLS), and permissions in Power BI Service\nCollaborate with Data Engineers and Architects to build scalable data models and reporting solutions\nStrong proficiency in efficient data modeling, ensuring optimized performance and scalability using techniques like Aggregations, Indexing and Partitioning\nAbility to use Power BI APIs for scheduled refreshes, subscriptions, and monitoring usage analytics\nAdvanced data transformations using Power Query\nReal-time reporting using DirectQuery and Composite models\nKnowledge of AI & ML features in Power BI would be a bonus\nFamiliarity with Azure DevOps, Git and CI/CD for PowerBI version control and deployment pipelines\nEssential Functions of the Role**:\nFlexibility in work schedule, off-hours for project implementation.\nTravel via plane or automobile both locally and internationally\nWork Experience Requirements\nNumber of Overall Years Necessary: 5-8\nA minimum of 3 years of experience in Microsoft Power BI\nA minimum or 3 years of experience business process analysis and design\nEducation Requirements\nBS/BA , or equivalent business experience in a business related discipline\nSpecialized Skills/Technical Knowledge:\nIn-depth Power BI expertise, including report development, data modeling, DAX calculations, and performance optimization.\nStrong knowledge of SQL, including querying, data transformation, and performance tuning for Power BI datasets.\nUnderstanding of enterprise-wide data structures, integrations, and key business processes relevant to reporting needs.\nExperience working with various data sources such as SQL databases, Excel, APIs, and cloud-based data platforms.\nAbility to analyze and translate business requirements into technical solutions using Power BI.\nFamiliarity with data governance, security, and compliance best practices within Power BI and related tools.\nProficiency in Microsoft Excel, including advanced formulas, pivot tables, and data visualization techniques.\nStrong analytical and problem-solving skills to assess data quality, identify trends, and provide meaningful insights.\nEffective communication skills to work with stakeholders, explain technical concepts in business terms, and document reporting solutions.\nAbility to stay updated on Power BI advancements and apply new features to improve reporting efficiency.",Industry Type: Medical Devices & Equipment,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Data analysis', 'Version control', 'Data modeling', 'Data structures', 'Data quality', 'Business intelligence', 'Information technology', 'Analytics', 'Monitoring']",2025-06-10 14:21:54
Scale Solutions Engineer,Databricks,4 - 8 years,Not Disclosed,['Bengaluru'],"CSQ326R201\nMission:\nAt Databricks, we are on a mission to empower our customers to solve the worlds toughest data problems by utilising the Data Intelligence platform. As a Scale Solution Engineer, you will play a critical role in advising Customers in their onboarding journey. You will directly work with customers to help them onboard and deploy Databricks in their Production environment.\nThe impact you will have:\nYou will ensure new customers have an excellent experience by providing them with technical assistance early in their journey.\nYou will become an expert on the Databricks Platform and guide customers in making the best technical decisions to achieve their goals.\nYou will work on multiple tactical customers to track and report their progress.\nWhat we look for:\n2+ years of industry experience\nEarly-career technical professional ideally in data-driven or cloud-based roles.\nKnowledge of at least one of the public cloud platforms AWS, Azure, or GCP is required.\nKnowledge of a programming language - Python, Scala, or SQL\nKnowledge of end-to-end data analytics workflow\nHands-on professional or academic experience in one or more of the following:\nData Engineering technologies (e.g., ETL, DBT, Spark, Airflow)\nData Warehousing technologies (e.g., SQL, Stored Procedures, Redshift, Snowflake)\nExcellent time management & presentation skills\nBonus - Knowledge of Data Science and Machine Learning (e.g., build and deploy ML Models)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAN', 'GCP', 'spark', 'Diversity and Inclusion', 'Machine learning', 'Workflow', 'Data analytics', 'Stored procedures', 'SQL', 'Python']",2025-06-10 14:21:56
Specialist - Software Engineering,Acentra Health,4 - 8 years,Not Disclosed,['Chennai'],"Job Title: Data Engineer\nLocation: Chennai (Hybrid)\nSummary\nDesign,develop, and maintain scalable data pipelines and systems to support thecollection, integration, and analysis of healthcare and enterprise data. Theprimary responsibilities of this role include designing and implementingefficient data pipelines, architecting robust data models, and adhering to datamanagement best practices. In this position, you will play a crucial part intransforming raw data into meaningful insights, through development of semanticdata layers, enabling data-driven decision-making across the organization. Theideal candidate will possess strong technical skills, a keen understanding ofdata architecture, and a passion for optimizing data processes.",,,,"['snow flake schema', 'schema', 'python', 'hipaa', 'amazon redshift', 'microsoft azure', 'data warehousing', 'dms', 'elt', 'sql', 'star schema', 'coding', 'data modeling', 'spark', 'gcp', 'system performance', 'software engineering', 'hadoop', 'bigquery', 'aws', 'etl', 'programming', 'communication skills']",2025-06-10 14:21:59
Cloud Data Support Streaming Engineer,Infogrowth,10 - 13 years,16-18 Lacs P.A.,['Bengaluru'],"Looking for a Cloud Data Support Streaming Engineer with 8+ years of experience in Azure Data Lake, Databricks, PySpark, and Python. Role includes monitoring, troubleshooting, and support for streaming data pipelines.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Pyspark', 'Azure Data Lake', 'Azure SQL', 'Data Bricks', 'Python', 'Azure Data Factory', 'Application Support', 'Data Streaming', 'API', 'SSIS', 'Monitoring', 'Batch Processing']",2025-06-10 14:22:02
Lead Data Analyst-Business Intelligence,Tresvista Financial Services,6 - 10 years,Not Disclosed,['Bengaluru'],"Roles and Responsibilities\nArchitect and incorporate an effective Data framework enabling end to end Data Solution.\nUnderstand business needs, use cases and drivers for insights and translate them into detailed technical specifications.\nCreate epics, features and user stories with clear acceptance criteria for execution and delivery by the data engineering team.\nCreate scalable and robust data solution designs that incorporate governance, security and compliance aspects.\nDevelop and maintain logical and physical data models and work closely with data engineers, data analysts and data testers for successful implementation of them.\nAnalyze, assess and design data integration strategies across various sources and platforms.\nCreate project plans and timelines while monitoring and mitigating risks and controlling progress of the project.\nConduct daily scrum with the team with a clear focus on meeting sprint goals and timely resolution of impediments.\nAct as a liaison between technical teams and business stakeholders and ensure.\nGuide and mentor the team for best practices on Data solutions and delivery frameworks.\nActively work, facilitate and support the stakeholders/ clients to complete User Acceptance Testing ensure there is strong adoption of the data products after the launch.\nDefining and measuring KPIs/KRA for feature(s) and ensuring the Data roadmap is verified through measurable outcomes\n\nPrerequisites\n5 to 8 years of professional, hands on experience building end to end Data Solution on Cloud based Data Platforms including 2+ years working in a Data Architect role.\nProven hands on experience in building pipelines for Data Lakes, Data Lake Houses, Data Warehouses and Data Visualization solutions\nSound understanding of modern Data technologies like Databricks, Snowflake, Data Mesh and Data Fabric.\nExperience in managing Data Life Cycle in a fast-paced, Agile / Scrum environment.\nExcellent spoken and written communication, receptive listening skills, and ability to convey complex ideas in a clear, concise fashion to technical and non-technical audiences\nAbility to collaborate and work effectively with cross functional teams, project stakeholders and end users for quality deliverables withing stipulated timelines\nAbility to manage, coach and mentor a team of Data Engineers, Data Testers and Data Analysts. Strong process driver with expertise in Agile/Scrum framework on tools like Azure DevOps, Jira or Confluence\nExposure to Machine Learning, Gen AI and modern AI based solutions.\n\nExperience\nTechnical Lead Data Analytics with 6+ years of overall experience out of which 2+ years is on Data architecture.\n\nEducation\nEngineering degree from a Tier 1 institute preferred.\n\nCompensation\nThe compensation structure will be as per industry standards",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'Data Bricks', 'Data Lake', 'Data Warehousing', 'Python', 'Business Intelligence', 'Databricks Engineer', 'Machine Learning', 'Redshift Aws', 'Snowflake', 'Data Visualization', 'ETL', 'Data Mesh']",2025-06-10 14:22:05
Snowflake Architect,Blend360 India,8 - 13 years,Not Disclosed,['Hyderabad'],"Job Description\nWe are looking for an experienced and hands-on Snowflake Architect to lead and expand our data engineering capabilities. This role is ideal for a technically strong leader who can design scalable data architectures, manage a high-performing team, and collaborate cross-functionally to deliver reliable and secure data solutions.\nResponsibilities:\nDesign and implement robust Snowflake data warehouse architectures and ETL pipelines to support business intelligence and advanced analytics use cases.\nLead and mentor a team of data engineers, ensuring high-quality and timely delivery of projects.\nCollaborate closely with data analysts, data scientists, and business stakeholders to understand data needs and design effective data models.\nDevelop, document, and enforce best practices for Snowflake architecture, data modeling, performance optimization, and ETL processes.\nOwn the optimization of Snowflake environments to ensure low-latency and high-availability data access.\nDrive process improvements, evaluate emerging tools, and continuously enhance our data engineering infrastructure.\nEnsure data pipelines are built with high levels of accuracy, completeness, and security, in compliance with data privacy regulations (GDPR, CCPA, etc.).\nPartner with cloud engineering and DevOps teams to integrate data solutions seamlessly within the AWS ecosystem.\nParticipate in capacity planning, budgeting, and resource allocation for the data engineering function.\n\n\nQualifications\nBachelor s degree in Computer Science, Information Technology, or a related field.\n9+ years of overall experience in cloud-based data engineering, with at least 4 years of hands-on",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Architect', 'Data modeling', 'data security', 'data governance', 'Business intelligence', 'Information technology', 'SQL', 'Python', 'Capacity planning']",2025-06-10 14:22:07
Databricks Administrator - AWS /Azure,Numeric Technologies India,8 - 13 years,20-35 Lacs P.A.,[],"Databricks Administrator Azure/AWS | Remote | 6+ Years\n\nJob Description:\nWe are seeking an experienced Databricks Administrator with 6+ years of expertise in managing and optimizing Databricks environments. The ideal candidate should have hands-on experience with Azure/AWS Databricks, cluster management, security configurations, and performance optimization. This role requires close collaboration with data engineering and analytics teams to ensure smooth operations and scalability.\nKey Responsibilities:\nDeploy, configure, and manage Databricks workspaces, clusters, and jobs.\nMonitor and optimize Databricks performance, auto-scaling, and cost management.\nImplement security best practices, including role-based access control (RBAC) and encryption.\nManage Databricks integration with cloud storage (Azure Data Lake, S3, etc.) and other data services.\nAutomate infrastructure provisioning and management using Terraform, ARM templates, or CloudFormation.\nTroubleshoot Databricks runtime issues, job failures, and performance bottlenecks.\nSupport CI/CD pipelines for Databricks workloads and notebooks.\nCollaborate with data engineering teams to enhance ETL pipelines and data processing workflows.\nEnsure compliance with data governance policies and regulatory requirements.\nMaintain and upgrade Databricks versions and libraries as needed.\nRequired Skills & Qualifications:\n6+ years of experience as a Databricks Administrator or in a similar role.\nStrong knowledge of Azure/AWS Databricks and cloud computing platforms.\nHands-on experience with Databricks clusters, notebooks, libraries, and job scheduling.\nExpertise in Spark optimization, data caching, and performance tuning.\nProficiency in Python, Scala, or SQL for data processing.\nExperience with Terraform, ARM templates, or CloudFormation for infrastructure automation.\nFamiliarity with Git, DevOps, and CI/CD pipelines.\nStrong problem-solving skills and ability to troubleshoot Databricks-related issues.\nExcellent communication and stakeholder management skills.\nPreferred Qualifications:\nDatabricks certifications (e.g., Databricks Certified Associate/Professional).\nExperience in Delta Lake, Unity Catalog, and MLflow.\nKnowledge of Kubernetes, Docker, and containerized workloads.\nExperience with big data ecosystems (Hadoop, Apache Airflow, Kafka, etc.).\n\nEmail : Hrushikesh.akkala@numerictech.com\nPhone /Whatsapp : 9700111702\n\nFor immediate response and further opportunities, connect with me on LinkedIn: https://www.linkedin.com/in/hrushikesh-a-74a32126a/",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Databricks', 'S3', 'Ci Cd Pipeline', 'Aws Cloud', 'Notebook', 'scala', 'libraries', 'Azure Databricks', 'Arm Templates', 'SQL', 'Terraform', 'Job Scheduling', 'ARM', 'AWS', 'Python']",2025-06-10 14:22:09
Lead Consultant / Principal Consultant - Informatica MDM SaaS,Genpact,5 - 10 years,11-21 Lacs P.A.,"['Kolkata', 'Hyderabad', 'Bengaluru']","Ready to build the future with AI?\nAt Genpact, we dont just keep up with technology—we set the pace. AI and digital innovation are redefining industries, and we’re leading the charge. Genpact’s AI Gigafactory, our industry-first accelerator, is an example of how we’re scaling advanced technology solutions to help global enterprises work smarter, grow faster, and transform at scale. From large-scale models to agentic AI, our breakthrough solutions tackle companies’ most complex challenges.\nIf you thrive in a fast-moving, innovation-driven environment, love building and deploying cutting-edge AI solutions, and want to push the boundaries of what’s possible, this is your moment.",,,,"['Informatica MDM SaaS', 'IDMC', 'informatica MDM']",2025-06-10 14:22:12
AWS Data/API Gateway Pipeline Engineer,Kyndryl,6 - 10 years,Not Disclosed,['Bengaluru'],"Who We Are\nAt Kyndryl, we design, build, manage and modernize the mission-critical technology systems that the world depends on every day. So why work at Kyndryl? We are always moving forward – always pushing ourselves to go further in our efforts to build a more equitable, inclusive world for our employees, our customers and our communities.\n\nThe Role\nAre you ready to dive headfirst into the captivating world of data engineering at Kyndryl? As a Data Engineer, you'll be the visionary behind our data platforms, crafting them into powerful tools for decision-makers. Your role? Ensuring a treasure trove of pristine, harmonized data is at everyone's fingertips.",,,,"['continuous integration', 'kubernetes', 'glue', 'ci/cd', 'data pipeline', 'kinesis', 'redis', 'docker', 'sql', 'serverless architecture', 'java', 'git', 'postgresql', 'backend', 'aws api gateway', 'cd', 'rest', 'python', 'dynamo db', 'streams', 'rest api design', 'aws lambda', 'aws glue', 'lambda expressions', 'kafka', 'athena', 'design principles', 'aws']",2025-06-10 14:22:14
Digital Business Solution Architect,Illuminz,8 - 13 years,Not Disclosed,['Bengaluru'],"What if the work you did every day could impact the lives of people you know? Or all of humanity?\nAt Illumina, we are expanding access to genomic technology to realize health equity for billions of people around the world. Our efforts enable life-changing discoveries that are transforming human health through the early detection and diagnosis of diseases and new treatment options for patients.\nWorking at Illumina means being part of something bigger than yourself. Every person, in every role, has the opportunity to make a difference. Surrounded by extraordinary people, inspiring leaders, and world changing projects, you will do more and become more than you ever thought possible.\nPosition Summary:\nWe are looking for an experienced Business Solution Architect to drive the design and delivery of scalable, secure, and business-aligned solutions across our cloud ecosystem. This role bridges business strategy and technology execution, requiring deep technical proficiency in Angular, .NET Core, and NestJS, along with strong SQL and cloud platform experience.\nYou will work closely with stakeholders to understand business needs and translate them into effective architectural solutions. As a key technical leader, you will guide engineering teams, shape system architecture, and ensure our technology investments support long-term business success.\nPosition Responsibilities:\nLead the design and architecture of cloud-native solutions that align with business objectives and user needs.\nPartner with business units, product managers, UX teams, and engineering to define end-to-end solutions that balance functionality, scalability, performance, and cost-effectiveness.\nAct as the architectural owner across application layers UI, backend services, APIs, databases, and cloud infrastructure.\nGuide the integration of AI/ML, data engineering, and operational workflows into cohesive business solutions.\nProduce clear architectural documentation, solution proposals, and system blueprints to drive implementation.\nUphold enterprise standards for security, performance, and maintainability.\nProvide leadership during system troubleshooting and contribute to incident resolution with a holistic architectural perspective.\nSupport strategic planning, technical roadmaps, and continuous improvement initiatives in line with business growth.\nListed responsibilities are an essential, but not exhaustive list, of the usual duties associated with the position. Changes to individual responsibilities may occur due to business needs.\nPosition Requirements:\nProven experience aligning technical solutions with business strategies in a cross-functional environment.\nAbility to evaluate and recommend technologies that add business value and operational efficiency.\nStrong mentoring capabilities to upskill development teams and promote architectural best practices.\nExpertise in microservices, RESTful APIs, CI/CD, DevOps, and modern development frameworks.\nDeep hands-on experience with cloud platforms (Azure preferred), infrastructure design, and system integrations.\nExcellent problem-solving, documentation, and stakeholder communication skills.\nAll listed requirements are deemed as essential functions to this position; however, business conditions may require reasonable accommodation for additional task and responsibilities.\nPreferred Experience/Education/Skills:\nBachelor s or master s degree in computer science, Engineering, or related discipline.\n8+ years in project lifecycle, with at least 3 years focused on architecture or business solution design.\nStrong technical foundation in .NET Core, Angular, NestJS, and SQL.\nExperience with API data formats (XML/JSON), Git version control, and database design/optimization.\nFamiliarity with data architecture, quality, and governance best practices.\nUnderstanding of full SDLC, including testing, operations, and continuous delivery pipelines.\nRelevant certifications (e.g., Azure Solutions Architect Expert, AWS) are a plus.\n\nWe are a company deeply rooted in belonging, promoting an inclusive environment where employees feel valued and empowered to contribute to our mission. Built on a strong foundation, Illumina has always prioritized openness, collaboration, and seeking alternative perspectives to propel innovation in genomics. We are proud to confirm a zero-net gap in pay, regardless of gender, ethnicity, or race. We also have several Employee Resource Groups (ERG) that deliver career development experiences, increase cultural awareness, and offer opportunities to engage in social responsibility. We are proud to be an equal opportunity employer committed to providing employment opportunity regardless of sex, race, creed, color, gender, religion, marital status, domestic partner status, age, national origin or ancestry, physical or mental disability, medical condition, sexual orientation, pregnancy, military or veteran status, citizenship status, and genetic information. Illumina conducts background checks on applicants for whom a conditional offer of employment has been made. Qualified applicants with arrest or conviction records will be considered for employment in accordance with applicable local, state, and federal laws. Background check results may potentially result in the withdrawal of a conditional offer of employment. The background check process and any decisions made as a result shall be made in accordance with all applicable local, state, and federal laws. Illumina prohibits the use of generative artificial intelligence (AI) in the application and interview process. . To learn more, visit: https: / / www.dol.gov / ofccp / regs / compliance / posters / pdf / eeopost.pdf. The position will be posted until a final candidate is selected or the requisition has a sufficient number of qualified applicants. This role is not eligible for visa sponsorship.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'System architecture', 'Backend', 'Version control', 'Database design', 'XML', 'JSON', 'Business solutions', 'SDLC', 'SQL']",2025-06-10 14:22:16
Digital Business Solution Architect,Illumina,8 - 13 years,Not Disclosed,['Bengaluru'],"What if the work you did every day could impact the lives of people you know? Or all of humanity?\nPosition Summary:\nWe are looking for an experienced Business Solution Architect to drive the design and delivery of scalable, secure, and business-aligned solutions across our cloud ecosystem. This role bridges business strategy and technology execution, requiring deep technical proficiency in Angular, .NET Core, and NestJS, along with strong SQL and cloud platform experience.\nYou will work closely with stakeholders to understand business needs and translate them into effective architectural solutions. As a key technical leader, you will guide engineering teams, shape system architecture, and ensure our technology investments support long-term business success.\nPosition Responsibilities:\nLead the design and architecture of cloud-native solutions that align with business objectives and user needs.\nPartner with business units, product managers, UX teams, and engineering to define end-to-end solutions that balance functionality, scalability, performance, and cost-effectiveness.\nAct as the architectural owner across application layers UI, backend services, APIs, databases, and cloud infrastructure.\nGuide the integration of AI/ML, data engineering, and operational workflows into cohesive business solutions.\nProduce clear architectural documentation, solution proposals, and system blueprints to drive implementation.\nUphold enterprise standards for security, performance, and maintainability.\nProvide leadership during system troubleshooting and contribute to incident resolution with a holistic architectural perspective.\nSupport strategic planning, technical roadmaps, and continuous improvement initiatives in line with business growth.\nPosition Requirements:\nProven experience aligning technical solutions with business strategies in a cross-functional environment.\nAbility to evaluate and recommend technologies that add business value and operational efficiency.\nStrong mentoring capabilities to upskill development teams and promote architectural best practices.\nExpertise in microservices, RESTful APIs, CI/CD, DevOps, and modern development frameworks.\nDeep hands-on experience with cloud platforms (Azure preferred), infrastructure design, and system integrations.\nExcellent problem-solving, documentation, and stakeholder communication skills.\nAll listed requirements are deemed as essential functions to this position; however, business conditions may require reasonable accommodation for additional task and responsibilities.\nPreferred Experience/Education/Skills:\nBachelor s or master s degree in computer science, Engineering, or related discipline.\n8+ years in project lifecycle, with at least 3 years focused on architecture or business solution design.\nStrong technical foundation in .NET Core, Angular, NestJS, and SQL.\nExperience with API data formats (XML/JSON), Git version control, and database design/optimization.\nFamiliarity with data architecture, quality, and governance best practices.\nUnderstanding of full SDLC, including testing, operations, and continuous delivery pipelines.\nRelevant certifications (e.g., Azure Solutions Architect Expert, AWS) are a plus.\n\nWe are a company deeply rooted in belonging, promoting an inclusive environment where employees feel valued and empowered to contribute to our mission. Built on a strong foundation, Illumina has always prioritized openness, collaboration, and seeking alternative perspectives to propel innovation in genomics. We are proud to confirm a zero-net gap in pay, regardless of gender, ethnicity, or race. We also have several Employee Resource Groups (ERG) that deliver career development experiences, increase cultural awareness, and offer opportunities to engage in social responsibility. We are proud to be an equal opportunity employer committed to providing employment opportunity regardless of sex, race, creed, color, gender, religion, marital status, domestic partner status, age, national origin or ancestry, physical or mental disability, medical condition, sexual orientation, pregnancy, military or veteran status, citizenship status, and genetic information. Illumina conducts background checks on applicants for whom a conditional offer of employment has been made. Qualified applicants with arrest or conviction records will be considered for employment in accordance with applicable local, state, and federal laws. Background check results may potentially result in the withdrawal of a conditional offer of employment. The background check process and any decisions made as a result shall be made in accordance with all applicable local, state, and federal laws. Illumina prohibits the use of generative artificial intelligence (AI) in the application and interview process. . To learn more, visit: https: / / www.dol.gov / ofccp / regs / compliance / posters / pdf / eeopost.pdf. The position will be posted until a final candidate is selected or the requisition has a sufficient number of qualified applicants. This role is not eligible for visa sponsorship.",Industry Type: Biotechnology,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'System architecture', 'Backend', 'Version control', 'Database design', 'XML', 'JSON', 'Business solutions', 'SDLC', 'SQL']",2025-06-10 14:22:19
AI Engineer,Galaxy,3 - 5 years,Not Disclosed,['Navi Mumbai'],"Key Responsibilities\nDesign, develop, and deploy machine learning and deep learning models for real-world applications\nOptimize and fine-tune AI models for performance, scalability, and efficiency\nWork with large datasets, including data preprocessing, feature engineering, and model training\nImplement NLP, computer vision, reinforcement learning, or generative AI solutions based on business needs\nCollaborate with data engineers to build scalable ML pipelines and integrate AI models into production systems\nStay updated with the latest advancements in AI/ML research and apply them to solve business challenges\nConduct A/B testing, model validation, and performance monitoring to ensure robustness\nWork with cloud platforms (AWS, GCP, Azure) and AI/ML tools (TensorFlow, PyTorch, Hugging Face, LangChain, etc.)\nMentor junior engineers and contribute to technical decision-making\n\nSkills & Requirements\n\nMust Have Skills\nPython programming and AI/ML frameworks (TensorFlow, PyTorch, Scikit-learn)\nDeep learning architectures (CNNs, RNNs, Transformers, GANs)\nGenerative AI (Diffusion models, LLM fine-tuning, RAG)\nNLP (BERT, GPT, LLMs) or Computer Vision (OpenCV, YOLO)\n\nGood To Have Skills\nReinforcement learning or autonomous systems\nEdge AI or embedded ML deployments\nBig data technologies (Spark, Hadoop, SQL/NoSQL databases)\nCI/CD pipelines and version control (Git)\nMLOps tools (MLflow, Kubeflow, Docker, Kubernetes)\nCloud-based AI services (AWS SageMaker, GCP Vertex AI, Azure ML)\n\nSoft Skills\nProblem-solving skills\nAbility to translate business needs into AI solutions\nMentorship capabilities\n\nJob application: https://app.fabrichq.ai/jobs/f29e2342-c5c2-488a-93ca-bd0bbb62d369",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['AI/ML frameworks', 'Data Wrangling', 'Machine Learning', 'Deep Learning', 'Python', 'Pytorch', 'Tensorflow', 'LLM fine-tuning', 'Opencv', 'rag']",2025-06-10 14:22:21
Data Scientist Lead - L1,Wipro,5 - 8 years,Not Disclosed,['Gurugram'],"About The Role  \n\nRole Purpose\n\nThe purpose of the role is to define, architect and lead delivery of machine learning and AI solutions\n\n ? \n\nDo\n\n1. Demand generation through support in Solution development\n\na. Support Go-To-Market strategy\n\ni. Collaborate with sales, pre-sales &consulting team to assist in creating solutions and propositions for proactive demand generation\n\nii. Contribute to development solutions, proof of concepts aligned to key offerings to enable solution led sales\n\nb. Collaborate with different colleges and institutes for recruitment, joint research initiatives and provide data science courses\n\n2. Revenue generation through Building & operationalizing Machine Learning, Deep Learning solutions\n\na. Develop Machine Learning / Deep learning models for decision augmentation or for automation solutions\n\nb. Collaborate with ML Engineers, Data engineers and IT to evaluate ML deployment options\n\nc. Integrate model performance management tools into the current business infrastructure\n\n3. Team Management\n\na. Resourcing\n\ni. Support recruitment process to on-board right resources for the team\n\nb. Talent Management\n\ni. Support on boarding and training for the team members to enhance capability & effectiveness\n\nii. Manage team attrition\n\nc. Performance Management\n\ni. Conduct timely performance reviews and provide constructive feedback to own direct reports\n\nii. Be a role model to team for five habits\n\niii. Ensure that the Performance Nxt is followed for the entire team\n\nd. Employee Satisfaction and Engagement\n\ni. Lead and drive engagement initiatives for the team\n\n ? \n\nDeliver\n\n\nNo.\n\nPerformance Parameter\n\nMeasure 1. Demand generation Order booking 2. Revenue generation through delivery Timeliness, customer success stories, customer use cases 3. Capability Building & Team Management % trained on new skills, Team attrition %\n\n\n ? \n\n ? \nMandatory\n\nSkills:\nPython for Data Science.\n\nExperience5-8 Years.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'deep learning', 'data science', 'ml', 'data analytics', 'natural language processing', 'neural networks', 'predictive analytics', 'ml deployment', 'data engineering', 'artificial intelligence', 'sql', 'r', 'predictive modeling', 'statistical modeling', 'statistics']",2025-06-10 14:22:24
Walkin || Cognizant is hiring For Abinitio developer,Cognizant,6 - 9 years,Not Disclosed,['Kochi'],"Greetings from Cognizant!! #MegaWalkIn\n\nWe have an exciting opportunity for the #Abinitio Developer Role with Cognizant, join us if you are an aspirant for matching the below criteria!!\n\nPrimary Skill: Abinitio Developer\nExperience: 6-9 years\nJob Location: PAN India",,,,"['Abinitio Graphs', 'Initio', 'Ab Initio']",2025-06-10 14:22:27
Solution Architect - L1,Wipro,8 - 10 years,Not Disclosed,['Hyderabad'],"Role Purpose\nThe purpose of the role is to create exceptional architectural solution design and thought leadership and enable delivery teams to provide exceptional client engagement and satisfaction.\nDo\n1.Develop architectural solutions for the new deals/ major change requests in existing deals\nCreates an enterprise-wide architecture that ensures systems are scalable, reliable, and manageable.\nProvide solutioning of RFPs received from clients and ensure overall design assurance\nDevelop a direction to manage the portfolio of to-be-solutions including systems, shared infrastructure services, applications in order to better match business outcome objectives\nAnalyse technology environment, enterprise specifics, client requirements to set a collaboration solution design framework/ architecture\nProvide technical leadership to the design, development and implementation of custom solutions through thoughtful use of modern technology\nDefine and understand current state solutions and identify improvements, options & tradeoffs to define target state solutions\nClearly articulate, document and sell architectural targets, recommendations and reusable patterns and accordingly propose investment roadmaps\nEvaluate and recommend solutions to integrate with overall technology ecosystem\nWorks closely with various IT groups to transition tasks, ensure performance and manage issues through to resolution\nPerform detailed documentation (App view, multiple sections & views) of the architectural design and solution mentioning all the artefacts in detail\nValidate the solution/ prototype from technology, cost structure and customer differentiation point of view\nIdentify problem areas and perform root cause analysis of architectural design and solutions and provide relevant solutions to the problem\nCollaborating with sales, program/project, consulting teams to reconcile solutions to architecture\nTracks industry and application trends and relates these to planning current and future IT needs\nProvides technical and strategic input during the project planning phase in the form of technical architectural designs and recommendation\nCollaborates with all relevant parties in order to review the objectives and constraints of solutions and determine conformance with the Enterprise Architecture\nIdentifies implementation risks and potential impacts\n2.Enable Delivery Teams by providing optimal delivery solutions/ frameworks\nBuild and maintain relationships with executives, technical leaders, product owners, peer architects and other stakeholders to become a trusted advisor\nDevelops and establishes relevant technical, business process and overall support metrics (KPI/SLA) to drive results\nManages multiple projects and accurately reports the status of all major assignments while adhering to all project management standards\nIdentify technical, process, structural risks and prepare a risk mitigation plan for all the projects\nEnsure quality assurance of all the architecture or design decisions and provides technical mitigation support to the delivery teams\nRecommend tools for reuse, automation for improved productivity and reduced cycle times\nLeads the development and maintenance of enterprise framework and related artefacts\nDevelops trust and builds effective working relationships through respectful, collaborative engagement across individual product teams\nEnsures architecture principles and standards are consistently applied to all the projects\nEnsure optimal Client Engagement\nSupport pre-sales team while presenting the entire solution design and its principles to the client\nNegotiate, manage and coordinate with the client teams to ensure all requirements are met and create an impact of solution proposed\nDemonstrate thought leadership with strong technical capability in front of the client to win the confidence and act as a trusted advisor\n3.Competency Building and Branding\nEnsure completion of necessary trainings and certifications\nDevelop Proof of Concepts (POCs),case studies, demos etc. for new growth areas based on market and customer research\nDevelop and present a point of view of Wipro on solution design and architect by writing white papers, blogs etc.\nAttain market referencability and recognition through highest analyst rankings, client testimonials and partner credits\nBe the voice of Wipros Thought Leadership by speaking in forums (internal and external)\nMentor developers, designers and Junior architects in the project for their further career development and enhancement\nContribute to the architecture practice by conducting selection interviews etc\n4. Team Management\nResourcing\nAnticipating new talent requirements as per the market/ industry trends or client requirements\nHire adequate and right resources for the team\nTalent Management\nEnsure adequate onboarding and training for the team members to enhance capability & effectiveness\nBuild an internal talent pool and ensure their career progression within the organization\nManage team attrition\nDrive diversity in leadership positions\nPerformance Management\nSet goals for the team, conduct timely performance reviews and provide constructive feedback to own direct reports\nEnsure that the Performance Nxt is followed for the entire team\nEmployee Satisfaction and Engagement\nLead and drive engagement initiatives for the team\nTrack team satisfaction scores and identify initiatives to build engagement within the team\nMandatory Skills: DataBricks - Data Engineering.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['solution architecture', 'Data Engineering', 'Data Bricks', 'Software Development', 'automation', 'Performance Management', 'Talent Management', 'maintenance']",2025-06-10 14:22:29
Solution Architect - L1,Wipro,8 - 10 years,Not Disclosed,['Pune'],"Role Purpose\nThe purpose of the role is to create exceptional architectural solution design and thought leadership and enable delivery teams to provide exceptional client engagement and satisfaction.\nDo\n1.Develop architectural solutions for the new deals/ major change requests in existing deals\nCreates an enterprise-wide architecture that ensures systems are scalable, reliable, and manageable.\nProvide solutioning of RFPs received from clients and ensure overall design assurance\nDevelop a direction to manage the portfolio of to-be-solutions including systems, shared infrastructure services, applications in order to better match business outcome objectives\nAnalyse technology environment, enterprise specifics, client requirements to set a collaboration solution design framework/ architecture\nProvide technical leadership to the design, development and implementation of custom solutions through thoughtful use of modern technology\nDefine and understand current state solutions and identify improvements, options & tradeoffs to define target state solutions\nClearly articulate, document and sell architectural targets, recommendations and reusable patterns and accordingly propose investment roadmaps\nEvaluate and recommend solutions to integrate with overall technology ecosystem\nWorks closely with various IT groups to transition tasks, ensure performance and manage issues through to resolution\nPerform detailed documentation (App view, multiple sections & views) of the architectural design and solution mentioning all the artefacts in detail\nValidate the solution/ prototype from technology, cost structure and customer differentiation point of view\nIdentify problem areas and perform root cause analysis of architectural design and solutions and provide relevant solutions to the problem\nCollaborating with sales, program/project, consulting teams to reconcile solutions to architecture\nTracks industry and application trends and relates these to planning current and future IT needs\nProvides technical and strategic input during the project planning phase in the form of technical architectural designs and recommendation\nCollaborates with all relevant parties in order to review the objectives and constraints of solutions and determine conformance with the Enterprise Architecture\nIdentifies implementation risks and potential impacts\n2.Enable Delivery Teams by providing optimal delivery solutions/ frameworks\nBuild and maintain relationships with executives, technical leaders, product owners, peer architects and other stakeholders to become a trusted advisor\nDevelops and establishes relevant technical, business process and overall support metrics (KPI/SLA) to drive results\nManages multiple projects and accurately reports the status of all major assignments while adhering to all project management standards\nIdentify technical, process, structural risks and prepare a risk mitigation plan for all the projects\nEnsure quality assurance of all the architecture or design decisions and provides technical mitigation support to the delivery teams\nRecommend tools for reuse, automation for improved productivity and reduced cycle times\nLeads the development and maintenance of enterprise framework and related artefacts\nDevelops trust and builds effective working relationships through respectful, collaborative engagement across individual product teams\nEnsures architecture principles and standards are consistently applied to all the projects\nEnsure optimal Client Engagement\nSupport pre-sales team while presenting the entire solution design and its principles to the client\nNegotiate, manage and coordinate with the client teams to ensure all requirements are met and create an impact of solution proposed\nDemonstrate thought leadership with strong technical capability in front of the client to win the confidence and act as a trusted advisor\n3.Competency Building and Branding\nEnsure completion of necessary trainings and certifications\nDevelop Proof of Concepts (POCs),case studies, demos etc. for new growth areas based on market and customer research\nDevelop and present a point of view of Wipro on solution design and architect by writing white papers, blogs etc.\nAttain market referencability and recognition through highest analyst rankings, client testimonials and partner credits\nBe the voice of Wipros Thought Leadership by speaking in forums (internal and external)\nMentor developers, designers and Junior architects in the project for their further career development and enhancement\nContribute to the architecture practice by conducting selection interviews etc\n4. Team Management\nResourcing\nAnticipating new talent requirements as per the market/ industry trends or client requirements\nHire adequate and right resources for the team\nTalent Management\nEnsure adequate onboarding and training for the team members to enhance capability & effectiveness\nBuild an internal talent pool and ensure their career progression within the organization\nManage team attrition\nDrive diversity in leadership positions\nPerformance Management\nSet goals for the team, conduct timely performance reviews and provide constructive feedback to own direct reports\nEnsure that the Performance Nxt is followed for the entire team\nEmployee Satisfaction and Engagement\nLead and drive engagement initiatives for the team\nTrack team satisfaction scores and identify initiatives to build engagement within the team\nMandatory Skills: DataBricks - Data Engineering.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['solution architecture', 'Data Engineering', 'Data Bricks', 'Software Development', 'automation', 'Performance Management', 'Talent Management', 'maintenance']",2025-06-10 14:22:32
Regulatory Affairs Specialist,HCLTech,2 - 4 years,Not Disclosed,"['Madurai', 'Chennai']","Education: B.E Mechanical/Bio-Medical\nExperience: 2.5 to 5 yrs Knowledge on Medical device regulations Good Communication skills Ability to read through the Medical Device Documents Ability to work on Microsoft tools (Excel, Word and PowerPoint) Experience on Medical Device UDI Data management will be additional preference.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MDR', 'Regulatory Affairs', 'Medical Devices']",2025-06-10 14:22:34
Data / Analytics Engineer,Service based Top B2C/B2B MNC in Analyti...,6 - 10 years,Not Disclosed,"['Pune', 'Gurugram', 'Bengaluru']","Job Description:\nWe are looking for a skilled Data / Analytics Engineer with hands-on experience in vector databases and search optimization techniques. You will help build scalable, high-performance infrastructure to support AI-powered applications like semantic search, recommendation systems, and RAG pipelines.\nKey Responsibilities:\nOptimize vector search algorithms for performance and scalability.\nBuild pipelines to process high-dimensional embeddings (e.g., BERT, CLIP, OpenAI).\nImplement ANN indexing techniques like HNSW, IVF, PQ.\nIntegrate vector search with data platforms and APIs.\nCollaborate with cross-functional teams (data scientists, engineers, product).\nMonitor and resolve latency, throughput, and scaling issues.\nMust-Have Skills:\nPython\nAWS\nVector Databases (e.g., Elasticsearch, FAISS, Pinecone)\nVector Search / Similarity Search\nANN Search Algorithms HNSW, IVF, PQ\nSnowflake / Databricks\nEmbedding Models – BERT, CLIP, OpenAI\nKafka / Flink for real-time data pipelines\nREST APIs, GraphQL, or gRPC for integration\nGood to Have:\nKnowledge of semantic caching and hybrid retrieval\nExperience with distributed systems and high-performance computing\nFamiliarity with RAG (Retrieval-Augmented Generation) workflows\nApply Now if You:\nEnjoy solving performance bottlenecks in AI infrastructure\nLove working with cutting-edge ML models and search technologies\nThrive in collaborative, fast-paced environments",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Vector DB', 'AWS', 'Python', 'PQ', 'ANN Search', 'GraphQL', 'Kafka', 'HNSW', 'Flink', 'Vector Search', 'gRPC', 'CLIP', 'Snowflake', 'IVF', 'BERT', 'OpenAI', 'Databricks', 'REST APIs']",2025-06-10 14:22:36
Director Data Science,Astar Data,10 - 17 years,Not Disclosed,['Bengaluru'],"Sigmoid enables business transformation using data and analytics, leveraging real-time insights to make accurate and fast business decisions, by building modern data architectures using cloud and open source. Some of the worlds largest data producers engage with Sigmoid to solve complex business problems. Sigmoid brings deep expertise in data engineering, predictive analytics, artificial intelligence, and DataOps. Sigmoid has been recognized as one of the fastest growing technology companies in North America, 2021, by Financial Times, Inc. 5000, and Deloitte Technology Fast 500.\nOffices: New York | Dallas | San Francisco | Lima | Bengaluru\nThe below role is for our Bengaluru office.\n\nWhy Join Sigmoid?\n• Sigmoid provides the opportunity to push the boundaries of what is possible by seamlessly\ncombining technical expertise and creativity to tackle intrinsically complex business\nproblems and convert them into straight-forward data solutions.\n• Despite being continuously challenged, you are not alone. You will be part of a fast-paced\ndiverse environment as a member of a high-performing team that works together to\nenergize and inspire each other by challenging the status quo\n• Vibrant inclusive culture of mutual respect and fun through both work and play\nRoles and Responsibilities:\n• Convert broad vision and concepts into a structured data science roadmap, and guide a\nteam to successfully execute on it.\n• Handling end-to-end client AI & analytics programs in a fluid environment. Your role will be a\ncombination of hands-on contribution, technical team management, and client interaction.\n• Proven ability to discover solutions hidden in large datasets and to drive business results\nwith their data-based insights\n• Contribute to internal product development initiatives related to data science.\n• Drive excellent project management required to deliver complex projects, including\neffort/time estimation.\n• Be proactive, with full ownership of the engagement. Build scalable client engagement level\nprocesses for faster turnaround & higher accuracy\n• Define Technology/ Strategy and Roadmap for client accounts, and guides implementation\nof that strategy within projects\n• Manage the team-members, to ensure that the project plan is being adhered to over the\ncourse of the project\n• Build a trusted advisor relationship with the IT management at clients and internal accounts\nleadership.\nMandated Skills:\n• A B-Tech/M-Tech/MBA from a top tier Institutepreferably in a quantitativesubject\n• 10+ years of hands-onexperience in applied Machine Learning, AI and analytics\n• Experience of scientific programming in scripting languages like Python, R, SQL, NoSQL,\nSpark with ML tools & Cloud Technology (AWS, Azure, GCP)\n• Experience in Python libraries such as numpy, pandas, scikit-learn, tensor-flow, scrapy, BERT\netc. Strong grasp of depth and breadth of machine learning, deep learning, data mining, and\nstatistical concepts and experience in developing models and solutions in these areas\n• Expertise with client engagement, understanding complex problem statements, and offering\nsolutions in the domains of Supply Chain, Manufacturing, CPG, Marketing etc.\nDesired Skills:\nDeep understanding of ML algorithms for common use cases in both structured and\nunstructured data ecosystems.\nComfortable with large scale data processing and distributed computing\nProviding required inputs to sales, and pre-sales activities\nA self-starter who can work well with minimalguidance\nExcellent written and verbal communication skills",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Machine Learning', 'Algorithm Development', 'Pattern Recognition', 'Opencv', 'Image Processing', 'Artificial Intelligence', 'Natural Language Processing', 'Neural Networks', 'Computer Vision', 'Deep Learning']",2025-06-10 14:22:39
Solution Architect - L2,Wipro,10 years,Not Disclosed,['Hyderabad'],"Role Purpose\nThe purpose of the role is to create exceptional architectural solution design and thought leadership and enable delivery teams to provide exceptional client engagement and satisfaction.\nDo\n1.Develop architectural solutions for the new deals/ major change requests in existing deals\nCreates an enterprise-wide architecture that ensures systems are scalable, reliable, and manageable.\nProvide solutioning of RFPs received from clients and ensure overall design assurance\nDevelop a direction to manage the portfolio of to-be-solutions including systems, shared infrastructure services, applications in order to better match business outcome objectives\nAnalyse technology environment, enterprise specifics, client requirements to set a collaboration solution design framework/ architecture\nProvide technical leadership to the design, development and implementation of custom solutions through thoughtful use of modern technology\nDefine and understand current state solutions and identify improvements, options & tradeoffs to define target state solutions\nClearly articulate, document and sell architectural targets, recommendations and reusable patterns and accordingly propose investment roadmaps\nEvaluate and recommend solutions to integrate with overall technology ecosystem\nWorks closely with various IT groups to transition tasks, ensure performance and manage issues through to resolution\nPerform detailed documentation (App view, multiple sections & views) of the architectural design and solution mentioning all the artefacts in detail\nValidate the solution/ prototype from technology, cost structure and customer differentiation point of view\nIdentify problem areas and perform root cause analysis of architectural design and solutions and provide relevant solutions to the problem\nCollaborating with sales, program/project, consulting teams to reconcile solutions to architecture\nTracks industry and application trends and relates these to planning current and future IT needs\nProvides technical and strategic input during the project planning phase in the form of technical architectural designs and recommendation\nCollaborates with all relevant parties in order to review the objectives and constraints of solutions and determine conformance with the Enterprise Architecture\nIdentifies implementation risks and potential impacts\n2.Enable Delivery Teams by providing optimal delivery solutions/ frameworks\nBuild and maintain relationships with executives, technical leaders, product owners, peer architects and other stakeholders to become a trusted advisor\nDevelops and establishes relevant technical, business process and overall support metrics (KPI/SLA) to drive results\nManages multiple projects and accurately reports the status of all major assignments while adhering to all project management standards\nIdentify technical, process, structural risks and prepare a risk mitigation plan for all the projects\nEnsure quality assurance of all the architecture or design decisions and provides technical mitigation support to the delivery teams\nRecommend tools for reuse, automation for improved productivity and reduced cycle times\nLeads the development and maintenance of enterprise framework and related artefacts\nDevelops trust and builds effective working relationships through respectful, collaborative engagement across individual product teams\nEnsures architecture principles and standards are consistently applied to all the projects\nEnsure optimal Client Engagement\nSupport pre-sales team while presenting the entire solution design and its principles to the client\nNegotiate, manage and coordinate with the client teams to ensure all requirements are met and create an impact of solution proposed\nDemonstrate thought leadership with strong technical capability in front of the client to win the confidence and act as a trusted advisor\n3.Competency Building and Branding\nEnsure completion of necessary trainings and certifications\nDevelop Proof of Concepts (POCs),case studies, demos etc. for new growth areas based on market and customer research\nDevelop and present a point of view of Wipro on solution design and architect by writing white papers, blogs etc.\nAttain market referencability and recognition through highest analyst rankings, client testimonials and partner credits\nBe the voice of Wipros Thought Leadership by speaking in forums (internal and external)\nMentor developers, designers and Junior architects in the project for their further career development and enhancement\nContribute to the architecture practice by conducting selection interviews etc\n4. Team Management\nResourcing\nAnticipating new talent requirements as per the market/ industry trends or client requirements\nHire adequate and right resources for the team\nTalent Management\nEnsure adequate onboarding and training for the team members to enhance capability & effectiveness\nBuild an internal talent pool and ensure their career progression within the organization\nManage team attrition\nDrive diversity in leadership positions\nPerformance Management\nSet goals for the team, conduct timely performance reviews and provide constructive feedback to own direct reports\nEnsure that the Performance Nxt is followed for the entire team\nEmployee Satisfaction and Engagement\nLead and drive engagement initiatives for the team\nTrack team satisfaction scores and identify initiatives to build engagement within the team\nMandatory Skills: DataBricks - Data Engineering.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['solution architecture', 'Data Engineering', 'Data Bricks', 'Software Development', 'automation', 'Performance Management', 'Talent Management', 'maintenance']",2025-06-10 14:22:41
Data Validation and ETL Testing Engineer,Skills and Placement Services,7 - 10 years,11-20 Lacs P.A.,"['Hyderabad', 'Bengaluru']","Looking for a Business Analyst with strong SQL, ETL/data warehouse testing, insurance domain knowledge (GuideWire), and experience in defect tracking & issue management tools.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Business Analyst', 'SQL', 'Data Validation', 'Defect Tracking', 'Insurance Domain', 'UAT', 'Issue Management', 'ETL Testing', 'GuideWire', 'SDLC', 'QA Testing', 'Mappings', 'Data Pipeline', 'Agile', 'Data Warehousing']",2025-06-10 14:22:43
IT Recruiter / Senior IT Recruiter / Lead IT Recruiter,P99soft,1 - 8 years,Not Disclosed,['Bengaluru'],"Job Title: IT Recruiter / Senior IT Recruiter / Lead IT Recruiter\nExperience: 1-8 Years\nLocation: Hyderabad, Bangalore, Pune (Hybrid / 5 Days Office)\nEmployment Type: Full-Time\n\nAbout the Role:\nWe are looking for passionate and driven IT Recruiters across experience levels from junior to lead recruiters to join our growing team. Candidates must have strong communication skills and a good understanding of technical roles and current IT market trends. This role offers an opportunity to contribute to talent acquisition strategy while working in a fast-paced environment.\n\nKey Responsibilities (Based on Experience Level):\nFor Junior Recruiters (1-3 Years):\nSource and screen candidates using job portals, social platforms, and referrals.\nCoordinate interviews and manage candidate communications.\nAssist senior team members with bulk hiring and sourcing drives.\nMaintain candidate databases and trackers.\n\nFor Mid-Level to Lead Recruiters (3-8 Years):\nOwn end-to-end recruitment for mid to senior-level technical roles.\nEngage with hiring managers to understand hiring needs and define strategies.\nMentor junior recruiters and manage small teams (for Lead roles).\nDrive sourcing strategies including headhunting, LinkedIn, Boolean search, and employee referrals.\nManage candidate pipeline, reporting, and stakeholder communication.\n\nKey Skills & Requirements:\n1-8 years of experience in IT recruitment (in-house or agency).\nStrong understanding of technical roles : Developers, QA, DevOps, Cloud, Architects, Data Engineers, etc.\nExcellent communication and interpersonal skills.\nHands-on experience with sourcing tools and ATS.\nStrong coordination and stakeholder management skills.\nTeam mentoring/leadership experience for Lead-level roles.\n\nPreferred Qualifications:\nBachelors degree in HR, IT, or relevant field.\nExperience hiring across multiple locations and technologies.\nPrior experience working in a tech services or product environment is a plus.\n\nWork Model & Locations:\nWork Mode: Hybrid / 5 Days Office (depending on project needs)\nLocations: Hyderabad, Bangalore, Pune\n\nWhat We Offer:\nExposure to diverse technical hiring across industries\nGrowth path from recruiter to lead/managerial roles\nFast-paced, energetic, and collaborative culture\nOpportunity to work with a driven and passionate recruitment team",Industry Type: IT Services & Consulting,Department: Human Resources,"Employment Type: Full Time, Permanent","['QA', 'Interpersonal skills', 'Talent acquisition', 'devops', 'Cloud', 'HR', 'Management', 'Stakeholder management', 'IT recruitment', 'Recruitment']",2025-06-10 14:22:46
Product Owner,Digital Convergence Technologies,3 - 6 years,Not Disclosed,['Pune'],"What We're Looking For:\nA deep understanding of data software engineering or a minimum of 3 years of software development or data engineering experience.\nDomain expertise in shipping Software as a Service (SaaS) products.\nDeep domain expertise in cloud technologies, including AWS, Azure and/or GCP.\nSkilled in database concepts and tools (preferably on Databricks and Snowflake).\nDeep understanding of the data platform R&D processes and agile methodologies.\nExperience with big data, ETL and data pipelines, security and privacy, DevOps, or automation.\nBS or MS in Computer Science, or related fields, or equivalent experience in a technical product owner role or an adjacent position.\nProven ability to influence and coordinate cross-functional teams to execute plans in highly technical environments.\nExceptional communication skills critical for translating business requirements into technical specifications, and communicating with technical and non-technical audiences.\nWell-developed strategic-thinking skills, with the ability to inspire and lead others.\nA highly proactive get it done attitude and the skills to back it up.\nDemonstrated ability to thrive in a fast-paced, dynamic startup environment.\nBonus Points:\nUS Healthcare Payor or Provider industry experience.\nExperience with healthcare interoperability standards such as HL7 V2, FHIR and X12.\nExperience with integrating or implementing healthcare-related EMPI/MDM workflows.",Industry Type: Miscellaneous,Department: Product Management,"Employment Type: Full Time, Permanent","['EMPI', 'HL7', 'product management', 'MDM', 'US healthcare', 'Fhir']",2025-06-10 14:22:48
Big Data Lead,Persistent,6 - 10 years,Not Disclosed,['Hyderabad'],"About Persistent\nWe are a trusted Digital Engineering and Enterprise Modernization partner, combining deep technical expertise and industry experience to help our clients anticipate what’s next. Our offerings and proven solutions create a unique competitive advantage for our clients by giving them the power to see beyond and rise above. We work with many industry-leading organizations across the world including 12 of the 30 most innovative US companies, 80% of the largest banks in the US and India, and numerous innovators across the healthcare ecosystem.\nOur growth trajectory continues, as we reported $1,231M annual revenue (16% Y-o-Y). Along with our growth, we’ve onboarded over 4900 new employees in the past year, bringing our total employee count to over 23,500+ people located in 19 countries across the globe.",,,,"['big data technologies', 'hive', 'cloudera', 'scala', 'data warehousing', 'apache pig', 'sql', 'streaming', 'java', 'apache', 'data modeling', 'spark', 'design', 'flume', 'hadoop', 'big data', 'etl', 'big data hadoop', 'hbase', 'python', 'oracle', 'oozie', 'talend', 'data processing', 'machine learning', 'sql server', 'nosql', 'mapreduce', 'kafka', 'hdfs', 'sqoop', 'aws']",2025-06-10 14:22:50
Big Data Lead,Persistent,6 - 10 years,Not Disclosed,['Pune'],"About Persistent\nWe are a trusted Digital Engineering and Enterprise Modernization partner, combining deep technical expertise and industry experience to help our clients anticipate what’s next. Our offerings and proven solutions create a unique competitive advantage for our clients by giving them the power to see beyond and rise above. We work with many industry-leading organizations across the world including 12 of the 30 most innovative US companies, 80% of the largest banks in the US and India, and numerous innovators across the healthcare ecosystem.\nOur growth trajectory continues, as we reported $1,231M annual revenue (16% Y-o-Y). Along with our growth, we’ve onboarded over 4900 new employees in the past year, bringing our total employee count to over 23,500+ people located in 19 countries across the globe.",,,,"['big data technologies', 'hive', 'cloudera', 'scala', 'data warehousing', 'apache pig', 'sql', 'streaming', 'java', 'apache', 'data modeling', 'spark', 'design', 'flume', 'hadoop', 'big data', 'etl', 'big data hadoop', 'hbase', 'python', 'oracle', 'oozie', 'talend', 'data processing', 'machine learning', 'sql server', 'nosql', 'mapreduce', 'kafka', 'hdfs', 'sqoop', 'aws']",2025-06-10 14:22:53
Senior Data Analyst,Stack Digital,3 - 7 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Job Description\nRoles and Responsibilities:\nThe Senior Data Analyst is an active thought partner who shapes the Business demand and work closely with the Project / Product teams and stakeholders. The Senior Data Analyst gathers, analyses and models data and key performance indicators to develop quantitative and qualitative business insights. Develops processes and design reports to boost the business intelligence and is good at effectively processing large amounts of data into meaningful information. Key interface towards the Project / Product Managers, Design Architects, Data Engineers, Testers, End users etc. as a natural team to deliver the Business demands.\nKey Characteristics\nCollating Business requirements, Analyzing the value drivers and functional requirements, usability and supportability considerations.\nPerform root cause analysis on Data problems and translate Data requirements into functionality and assess the risks, feasibility, opportunities and various solution options.\nCreate/Update clear documentation to communicate requirements and related information.\nSupports in Creating acceptance criteria and validate that solution by testing and ensure it meet business needs.\nDescribe technology in terms easily understood by business customers and set realistic customer expectations for the project outcome.\nExcellent analytical & problem-solving skills, willingness to take ownership and resolve technical challenges.\nGenerate innovative approaches to existing problems or new opportunities",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Root cause analysis', 'Analytical', 'Senior Data Analyst', 'Manager Technology', 'Business intelligence', 'Testing']",2025-06-10 14:22:55
Snowflake Implementer,Delivery Centric Technologies,3 - 6 years,Not Disclosed,"['Chennai', 'Bengaluru', 'Mumbai (All Areas)']","Role: Snowflake Implementer\nExperience: 3 to 6 years\nLocation - Chennai, Pune, Mumbai, Bangalore\n\nJD:\nSnowflake Implementer : Designing, implementing, and managing Snowflake data warehouse solutions, ensuring data integrity, and optimizing performance for clients or internal teams.\n\nStrong SQL skills: Expertise in writing, optimizing, and troubleshooting SQL queries\nExperience with data warehousing: Understanding of data warehousing concepts, principles, and best practices\nKnowledge of ETL/ELT technologies: Experience with tools and techniques for data extraction, transformation, and loading\nExperience with data modeling: Ability to design and implement data models that meet business requirements.\nFamiliarity with cloud platforms: Experience with cloud platforms like AWS, Azure, or GCP (depending on the specific Snowflake environment).\nProblem-solving and analytical skills: Ability to identify, diagnose, and resolve technical issues.\nCommunication and collaboration skills: Ability to work effectively with cross-functional teams.\nExperience with Snowflake (preferred): Prior experience with Snowflake is highly desirable.\nCertifications (preferred): Snowflake certifications (e.g., Snowflake Data Engineer, Snowflake Database Administrator) can be a plus.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Cloud Platforms', 'SQL Queries', 'Data Modeling', 'ETL Tool', 'Sql Query Writing']",2025-06-10 14:22:57
DataOps Specialist - Azure - 5+ Years - Gurugam,Crescendo Global,5 - 10 years,Not Disclosed,['Gurugram'],"DataOps Specialist - Azure - 5+ Years - Gurugam\n\nAre you a data enthusiast with expertise in Azure and DataOps? Do you have experience working with data pipelines, data warehousing, and analytics? Our client, a leading organization in Gurugam, is looking for a DataOps Specialist with 5+ years of experience. If you are passionate about leveraging data to drive business insights and decisions, this role is for you!\n\nLocation : Gurugam\n\nYour Future Employer : Our client is a prominent player in the industry and is committed to creating an inclusive and diverse work environment. They offer ample opportunities for professional growth and development, along with a supportive and collaborative culture.\n\nResponsibilities\nDesign, build, and maintain data pipelines on Azure platform\nWork on data warehousing solutions and data modeling\nCollaborate with cross-functional teams to understand data requirements and provide solutions\nImplement and manage data governance and security practices\nTroubleshoot and optimize data processes for performance and reliability\nStay updated with the latest trends and technologies in DataOps and analytics\n\nRequirements\n5+ years of experience in data engineering, DataOps, or a related field\nProven expertise in working with Azure data services such as Azure Data Factory, Azure Synapse Analytics, etc.\nStrong understanding of data warehousing concepts and data modeling techniques\nProficiency in SQL, Python, or other scripting languages\nExperience with data governance, security, and compliance\nExcellent communication and collaboration skills\n\nWhat's in it for you : As a DataOps Specialist, you will have the opportunity to work on cutting-edge data technologies and make a significant impact on the organization's data initiatives. You will be part of a supportive team that values innovation and encourages continuous learning and development.\n\nReach us : If you feel this opportunity is well aligned with your career progression plans, please feel free to reach me with your updated profile at rohit.kumar@crescendogroup.in\n\nDisclaimer : Crescendo Global specializes in Senior to C-level niche recruitment. We are passionate about empowering job seekers and employers with an engaging memorable job search and leadership hiring experience. Crescendo Global does not discriminate on the basis of race, religion, color, origin, gender, sexual orientation, age, marital status, veteran status or disability status.\n\nNote : We receive a lot of applications on a daily basis so it becomes a bit difficult for us to get back to each candidate. Please assume that your profile has not been shortlisted in case you don't hear back from us in 1 week. Your patience is highly appreciated.\n\nScammers can misuse Crescendo Globals name for fake job offers. We never ask for money, purchases, or system upgrades. Verify all opportunities at www.crescendo-global.com and report fraud immediately. Stay alert!\n\nProfile keywords : DataOps, Azure, Data Engineering, Data Warehousing, Analytics, SQL, Python, Data Governance",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Terraform', 'DataOps', 'Azure Cloud', 'Synopsys', 'Data Bricks', 'Infrastructure', 'ARM', 'SQL']",2025-06-10 14:23:00
Lead Development Engineer INT,Tyson Foods India Innovation Center,5 - 10 years,Not Disclosed,['Bengaluru'],"Job Details:\nLead Development Engineers are senior-level professionals with advanced expertise in software development. They lead and oversee complex software development projects, provide technical guidance and mentorship to development teams, and drive the adoption of best practices and technologies. Lead Development Engineers also collaborate with stakeholders to define project requirements, manage project timelines and resources, and ensure the successful delivery of software products.Essential Duties and Responsibilities: Designs software architecture. Delivers high-complexity development tasks as part of a team. Leads and influences in collaborative meetings with peers and stakeholders. Leads interaction with stakeholders to create requirements and demonstrate work. Documents code. Establishes software product engineering best practices as it relates to software development. Reviews and corrects others. Works independently with limited supervision to perform work. Act as a knowledge resource within the team. Leads and defines priorities for projects or processes. Applies judgment and experience to identify resolution. Ability to make timely decisions and to take action. Expected to be a productive leader by example of a team that designs and improves processes, procedures, and platforms. Perform other assigned job-related duties that align with our organizations vision, mission, and values and fall within your scope of practice.Design and Development: Designing, coding, and debugging applications in various software languages or tools. Developing software solutions by studying information needs, conferring with users, and studying systems flow, data usage, and work processes.Testing and Maintenance: Performing unit testing and integration testing. Maintaining and improving existing codebases and peer review code changes. Troubleshooting and resolving software defects and issues.Collaboration: Collaborating with cross-functional teams, including product managers, designers, and other engineers, to define, design, and ship new features. Participating in code reviews and providing constructive feedback to peers.Documentation: Writing and maintaining technical documentation to describe program development, logic, coding, testing, changes, and corrections.Research and Development: Keeping up to date with the latest industry trends and technologies to ensure the company remains competitive. Conducting research to discover new technologies and tools to improve efficiency and performance.Project Management: Participating in project planning, including estimating timeframes and resource requirements. Managing tasks and deadlines to ensure timely delivery of projects.Security and Compliance: Ensuring software security and compliance with industry standards and regulations. Implementing data protection and security measures.Customer Interaction: Interacting with customers or end-users to gather requirements and provide support. Translating customer requirements into technical specifications.Continuous Improvement: Continuously improving development processes and methodologies. Engaging in continuous learning and professional development.Qualifications: Education: Bachelor s, Master s Degree, or a significant amount of relevant experience. Computer Science, Computer Engineering, Information Systems, Quantitative or Engineering Field preferred. Experience: 5+ years of relevant and practical experience.Special Skills: Programming (expert in multiple languages or software tools) Expert in multiple front-end and back-end frameworks Expert in Data Engineering Design patterns for Infrastructure as Code (IaC) Design patterns with Version Control, DevOps & CICD Proficient in Monitoring and Alerting Technical Writing and Diagramming Analysis (Technical, Business, or Data)Soft Skills: Communication: Leading and influencing in collaborative meetings. Leadership: Providing technical guidance and mentorship to development teams with best practices. Problem-solving: Designing software architecture and delivering high-complexity tasks. Decision-making: Applying judgment and experience to identify resolutions Emotional Intelligence: Understanding and motivating team members. Mentorship: Fostering the growth and development of the development engineering team. Change Management: Guiding the organization through development-related transformations. Strategic Vision: Aligning development initiatives with long-term business objectives.\nRelocation Assistance Eligible:\nNo\nWork Shift:\nTyson is an Equal Opportunity Employer. All qualified applicants will be considered without regard to race, national origin, color, religion, age, genetics, sex, sexual orientation, gender identity, disability or veteran status.",Industry Type: FMCG,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Change management', 'Front end', 'Coding', 'Technical writing', 'Project management', 'Debugging', 'Project planning', 'Unit testing', 'Troubleshooting', 'Monitoring']",2025-06-10 14:23:02
Software Engineer,Intelliswift software,3 - 8 years,Not Disclosed,['Bengaluru'],"Job Description:\nWe are looking for a skilled Palantir Foundry Developer with strong hands-on experience in data engineering using PySpark and SQL. The ideal candidate should be proficient in designing, building, and maintaining scalable data pipelines and integrating with Palantir Foundry environments.\nKey Skills:\nPalantir Foundry (Mandatory)\nPySpark,\nAdvanced SQL and Data Modelling\nData Pipeline Development and Optimization\nETL Processes, Data Transformation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Palantir Foundry', 'SQL']",2025-06-10 14:23:04
Software Development Engineer - Algorithms & Data Structures,Uplers,1 - 5 years,Not Disclosed,['Bengaluru'],"Software Development Engineer\n\nExperience: 1 - 5 Years Exp.\nSalary : Competitive\nPreferred Notice Period: Within 30 Days\nShift: 10:00AM to 7:00PM IST\nOpportunity Type: Onsite (Bengaluru)\nPlacement Type: Permanent\n\n(*Note: This is a requirement for one of Uplers' Clients)\n\nMust have skills required :\nDSA skills, Problem Solving, Coding\n\nInfibeam (One of Uplers' Clients) is Looking for:\nSoftware Development Engineer who is passionate about their work, eager to learn and grow, and who is committed to delivering exceptional results. If you are a team player, with a positive attitude and a desire to make a difference, then we want to hear from you.\n\nRole Overview Description\nSoftware Development Engineer\nInfibeam is seeking a talented Software Development Engineer who is passionate about technology, problem-solving, and creating reliable software solutions. The ideal candidate will demonstrate strong proficiency in data structures, algorithms, and coding, and will have the opportunity to work on exciting and challenging projects in a collaborative environment. With a focus on innovative software development, you will play a vital role in our dynamic team.\n\nResponsibilities\nDesign, develop and maintain high-quality software applications.\nCollaborate with product managers, designers, and other engineers to create functional, user-friendly software solutions.\nPerform unit testing and integration testing to ensure software quality.\nAnalyze requirements and troubleshoot complex issues to ensure system stability and enhance user experience.\nUtilize strong analytical skills and coding expertise to solve problems efficiently and effectively.\nConduct peer reviews of code to maintain high coding standards and foster a culture of continuous improvement.\nCollaborate with the engineering team to ensure best practices in software design and development.\nCreate and maintain documentation of software functionalities and technical specifications for future reference.\n\nQualifications\nBE/BTech in Computer Science/Information Technology\n1 to 5 years of professional software development experience.\nStrong understanding of data structures, algorithms, and problem-solving methodologies.\nProficiency in coding with a strong attention to detail and ability to write clean, maintainable code.\nFamiliarity with languages such as Java, Ruby on Rails is a plus.\nExcellent communication skills and the ability to work effectively in a team environment.\nStrong analytical and critical thinking skills.\n\nHow to apply for this opportunity:\nEasy 3-Step Process:\n1. Click On Apply! And Register or log in on our portal\n2. Upload updated Resume & Complete the Screening Form\n3. Increase your chances to get shortlisted & meet the client for the Interview!\n\nAbout Our Client:\nInfibeam Avenues Limited (IAL) is a leading digital payments and e-commerce technology platforms company in India and provides a comprehensive suite of web services spanning digital payment solutions, data centre infrastructure and software platforms. We provide solutions to merchants, enterprises, corporations and governments in both domestic as well as international markets to enable online commerce. Our digital technology facilitates businesses and governments to execute e-commerce transactions in a safe and secure manner.\n\nAbout Uplers:\nOur goal is to make hiring and getting hired reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant product and engineering job opportunities and progress in their career.\n\n(Note: There are many more opportunities apart from this on the portal.)\n\nSo, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you!",Industry Type: FinTech / Payments,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Coding', 'Data Structures', 'Problem Solving', 'DSA', 'Algorithms']",2025-06-10 14:23:06
QA Engineer ( Data and Functional Testing),worxogo,2 - 5 years,Not Disclosed,[],"Responsibilities:\nPerform manual testing of web and mobile applications to ensure quality and performance.\nConduct various types of testing such as Functional, Usability, Integration, and GUI testing.\nDesign, develop, and execute comprehensive test cases and test scenarios based on product requirements.\nValidate data accuracy and consistency through extensive MySQL queries and checks.\nWork with large datasets and perform data validation using MS Excel.Collaborate with developers, product managers, and other QA team members to ensure timely delivery of high-quality features.\nUnderstand and follow the complete software testing lifecycle, including test planning, test execution, defect tracking, and reporting.\nUtilize tools like JIRA and Confluence for test management and documentation.\nContribute to automation testing efforts using Python, including scripting and executing automated test cases.\nParticipate in team discussions to improve product quality and testing processes.\nMaintain clear and effective communication across technical and non-technical teams.\nRequirements:\nMinimum 2 years of experience in Data Testing and Manual Testing.\nStrong knowledge of Manual Testing methodologies, techniques, and best practices.\nHands-on experience with MySQL and writing complex queries.\nProficiency in MS Excel for data validation and analysis.\nGood understanding of testing documentation like test plans, test cases, and test scenarios.\nExperience working with JIRA and Confluence is a plus.\nAbility to work independently and in a team-oriented, collaborative environment.\nWorking knowledge of Python and experience in developing automation test scripts.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Testing', 'Functional Testing', 'Automation', 'GUI Testing', 'Usability Testing', 'MySQL', 'Integration Testing', 'Manual Testing', 'Test Cases', 'Python']",2025-06-10 14:23:08
SDE + Subject Matter Expert (DBMS),Newton School,0 - 4 years,Not Disclosed,['Pune'],"ABOUT NEWTON SCHOOL\nCome be part of a rocketship that s creating a massive impact in the world of education!\nOn one side you have over a million college graduates every year with barely 5% employability rates and on the other side, there are thousands of companies struggling to find talent. Newton School aims to bridge this massive gap through it s personalised learning platform. We are building an online Institute and solving the deep problem of employability of graduates.\nWehave a strong core team consisting of alumni from IITs and IIM s, having several years of industry experience in companies like Unacademy, Inmobi, Ola, Microsoft - among others. On this mission, we are backed by some of the most respected investors around the world, - RTP Global, Nexus Venture Partners and a slew of angel investors including CRED s Kunal Shah, Flipkart s Kalyan Krishnamoorthy, Unacademy and Razorpay founders, Udaan s Sujeet Kumar among others.\nAbout the Role:\nWe are looking for a highly skilled and experienced Database Management Systems (DBMS) SDE + Subject Matter Expert (DBMS) to join our team. This role is a perfect blend of technical leadership and mentoring. You ll be contributing to cutting-edge web development projects while guiding and inspiring the next generation of software engineers. If you re passionate about coding, solving complex problems, and helping others grow, this role is for you!\n\nKey Responsibilities:\nDesign and develop DBMS course content, lesson plans, and practical assignments.\nUpdated curriculum with the latest trends in database technologies.\nDeliver lectures and hands-on sessions on relational models, SQL, NoSQL, normalization, and database design.\nUse real-world examples to enhance student understanding of database concepts.\nTeach advanced topics like query optimization, database security, data warehousing, and cloud databases.\nCreate and evaluate tests, quizzes, and projects to monitor student progress.\nProvide constructive feedback and mentorship to support student growth.\nFoster an engaging and collaborative classroom environment.\nAssist students in resolving database-related issues during practical sessions.\nGuide students on career paths in database management and related fields.\nShare insights on industry tools such as MySQL, PostgreSQL, MongoDB, and Oracle.\nOrganize workshops, hackathons, and webinars for hands-on experience.\nCollaborate with instructors and departments to integrate DBMS into interdisciplinary projects.\nAdapt teaching strategies to accommodate various learning styles.\n\nQualifications & Experience:\nBachelor s or Master s degree in Computer Science, Information Technology, or a related field.\nMinimum of 0-4 years experience in data engineering or database management.\nCertifications such as Oracle DBA, Microsoft SQL Server, or AWS Certified Database Specialist are a plus.\nPrior experience as an instructor, trainer, or tutor is preferred.\n\nTechnical Skills required :\nStrong proficiency in relational databases (MySQL, PostgreSQL, Oracle) and NoSQL systems (MongoDB, Cassandra).\nSolid knowledge of SQL , PL/SQL , or T-SQL .\nSkilled in database design , normalization, indexing, and performance tuning .\nFamiliarity with cloud-based databases like AWS RDS, Azure SQL, or Google Cloud Spanner.\n\nPreferred Teaching Skills:\nExperience using e-learning platforms such as Moodle, Blackboard, or Zoom.\nStrong presentation and communication skills for simplifying complex concepts.\nPassion for teaching, mentoring, and facilitating student success.\n\nSoft Skills\nAbility to motivate and engage learners across different levels.\nStrong problem-solving and mentoring capabilities.\nCommitted to continuous learning and professional growth in the field of database management.\n\nWhy Join Us?\nWork with Newton School of Technology in collaboration with Ajeenkya DY Patil University and Rishihood University institutions at the forefront of reimagining tech education in India.\nBe part of an initiative thats shaping the next generation of tech leaders through industry-integrated, hands-on learning.\nStay engaged with cutting-edge technologies while making a meaningful impact by mentoring and educating future professionals.\nEnjoy a competitive salary and a comprehensive benefits package .\nThrive in a collaborative, innovative work culture based in Pune and Sonipat.\nABOUT NEWTON SCHOOL\nCome be part of a rocketship th\n...",Industry Type: Education / Training,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'E-learning', 'Coding', 'MySQL', 'Web development', 'Oracle DBA', 'DBMS', 'Oracle', 'Information technology', 'SQL']",2025-06-10 14:23:10
Vice President Engineering,Tech Tales Accelerantz,15 - 24 years,Not Disclosed,['Bengaluru'],"About the Role:\nWe are seeking a dynamic and strategic VP of Engineering to lead and manage Karyas engineering team. This executive will play a pivotal role in scaling our technology infrastructure by managing both the people and processes within the technology team.\n\nKey Responsibilities:\nServe as a hands-on technical leader overseeing the technical team and all engineering operations including architecture, design, software development processes and pipelines, and making individual contributions when necessary.\nDevelop and implement strategies to drive innovation, scale technology, and ensure product quality and performance.\nMonitor, analyse, and continuously improve engineering KPIs, including delivery predictability, sprint velocity, incident resolution time, and defect rates, to ensure high performance and alignment with organizational goals.\nCollaborate closely with product, design, and other cross-functional teams to define product roadmaps and ensure timely delivery of features.\nOwn the architecture and technical decision-making for major projects, ensuring scalability, reliability, and security of the technology stack.\nFoster a culture of technical excellence, continuous improvement, and agility within the engineering teams.\nAct as a key partner in strategic planning and growth initiatives, offering technical expertise and insights to shape the companys direction.\nEnsure the engineering team is adaptive and stays on course with industry trends, adopting new technologies and tools when appropriate.\nAssist the CTO in mentoring team leaders and individual contributors, providing guidance on career development and technical growth.\n\nIdeal Candidate\nProven experience as an engineering leader in a fast-paced, high-growth environment.\nStrong technical background with experience in software engineering, systems architecture, and technology operations.\nExperience with cloud platforms, distributed systems, and scaling technology solutions.\nExperience managing, mentoring, and developing high-performing engineering teams.\nDeep understanding of the software development lifecycle, from ideation through to deployment and support.\nExpertise in modern software engineering practices.\nStrong leadership, communication, and interpersonal skills, with the ability to work effectively with senior management and technical teams.\nCompetitive Salary:\nWe offer a competitive compensation package commensurate with experience, designed to attract top talent in the industry.\n\nBenefits & Growth:\nEnjoy flexible work options, comprehensive benefits, and ample opportunities for career growth. Be part of a mission-driven team with the chance to make a significant social impact while advancing your career.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Agile Leadership', 'Full-Stack Development', 'Cloud Platforms', 'Mobile Applications', 'Mobile-First Development', 'Technical Roadmapping', 'Data Engineering', 'Ethical AI', 'Indic Languages', 'Engineering Strategy', 'DevOps', 'CI/CD', 'Cross-functional Teams', 'AI Infrastructure', 'Scalable Architecture']",2025-06-10 14:23:13
"Lead Data Scientist, Operations || Mumbai || Max 38 LPA",Argus India Price Reporting Services,5 - 10 years,20-35 Lacs P.A.,"['Mumbai Suburban', 'Navi Mumbai', 'Mumbai (All Areas)']","Lead Data Scientist, Operations\nMumbai, India\n\nAbout Argus:\n\nArgus is the leading independent provider of market intelligence to the global energy and commodity markets. We offer essential price assessments, news, analytics, consulting services, data science tools and industry conferences to illuminate complex and opaque commodity markets.\nHeadquartered in London with 1,500 staff, Argus is an independent media organisation with 30 offices in the worlds principal commodity trading hubs.\nCompanies, trading firms and governments in 160 countries around the world trust Argus data to make decisions, analyse situations, manage risk, facilitate trading and for long-term planning. Argus prices are used as trusted benchmarks around the world for pricing transportation, commodities and energy.\nFounded in 1970, Argus remains a privately held UK-registered company owned by employee shareholders and global growth equity firm General Atlantic.\nWhat were looking for:\nJoin our Generative AI team to lead a new group in India, focused on creating and maintaining AI-ready data. As the point of contact in Mumbai, you will guide the local team and ensure seamless collaboration with our global counterparts. Your contributions will directly impact the development of innovative solutions used by industry leaders worldwide, supporting text and numerical data extraction, curation, and metadata enhancements to accelerate development and ensure rapid response times. You will play a pivotal role in transforming how our data are seamlessly integrated with AI systems, paving the way for the next generation of customer interactions.\n\nWhat will you be doing:\n\nLead and Develop the Team: Oversee a team of data scientists in Mumbai. Mentoring and guiding junior team members, fostering their professional growth and development.\nStrategic Planning: Develop and implement strategic plans for data science projects, ensuring alignment with the company's goals and objectives.\nAI-Ready Data Development: Design, develop, and maintain high-quality AI-ready datasets, ensuring data integrity, usability, and scalability to support advanced Generative AI models.\nAdvanced Data Processing: Drive hands-on efforts in complex data extraction, cleansing, and curation for diverse text and numerical datasets. Implement sophisticated metadata enrichment strategies to enhance data utility and accessibility for AI systems.\nAlgorithm Implementation & Optimization: Implement and optimize state-of-the-art algorithms and pipelines for efficient data processing, feature engineering, and data transformation tailored for LLM and GenAI applications.\nGenAI Application Development: Apply and integrate frameworks like LangChain and Hugging Face Transformers to build modular, scalable, and robust Generative AI data pipelines and applications.\nPrompt Engineering Application: Apply advanced prompt engineering techniques to optimize LLM performance for specific data extraction, summarization, and generation tasks, working closely with the Lead's guidance.\nLLM Evaluation Support: Contribute to the systematic evaluation of Large Language Models (LLMs) outputs, analysing quality, relevance, and accuracy, and supporting the implementation of LLM-as-a-judge frameworks.\nRetrieval-Augmented Generation (RAG) Contribution: Actively contribute to the implementation and optimization of RAG systems, including working with embedding models, vector databases, and, where applicable, knowledge graphs, to enhance data retrieval for GenAI.\nTechnical Leadership: Act as a technical leader and subject matter expert for junior data scientists, providing guidance on best practices in coding and PR reviews, data handling, and GenAI methodologies.\nCross-Functional Collaboration: Collaborate effectively with global data science teams, engineering, and product stakeholders to integrate data solutions and ensure alignment with broader company objectives.\nOperational Excellence: Troubleshoot and resolve data-related issues promptly to minimize potential disruptions, ensuring high operational efficiency and responsiveness.\nDocumentation & Code Quality: Produce clean, well-documented, production-grade code, adhering to best practices for version control and software engineering.\n\nSkills and Experience:\n\nLeadership Experience: Proven track record in leading and mentoring data science teams, with a focus on strategic planning and operational excellence.\nAcademic Background: Advanced degree in AI, statistics, mathematics, computer science, or a related field.\nProgramming and Frameworks: 5+ years of hands-on experience with Python, TensorFlow or PyTorch, and NLP libraries such as spaCy and Hugging Face.\nGenAI Tools: 2+ years of Practical experience with LangChain, Hugging Face Transformers, and embedding models for building GenAI applications.\nPrompt Engineering: Deep expertise in prompt engineering, including prompt tuning, chaining, and optimization techniques.\nLLM Evaluation: Experience evaluating LLM outputs, including using LLM-as-a-judge methodologies to assess quality and alignment.\nRAG and Knowledge Graphs: Practical understanding and experience using vector databases. In addition, familiarity with graph-based RAG architectures and the use of knowledge graphs to enhance retrieval and reasoning would be a strong plus.\nCloud: 2+ years of experience with Gemini/OpenAI models and cloud platforms such as AWS, Google Cloud, or Azure. Proficient with Docker for containerization.\nData Engineering: Strong understanding of data extraction, curation, metadata enrichment, and AI-ready dataset creation.\nCollaboration and Communication: Excellent communication skills and a collaborative mindset, with experience working across global teams.\n\nWhats in it for you:\n\nCompetitive salary\nHybrid Working Policy (3 days in Mumbai office/ 2 days WFH once fully inducted)\nGroup healthcare scheme\n18 days annual leave\n8 days of casual leave\nExtensive internal and external training\n\nHours:\n\nThis is a full-time position operating under a hybrid model, with three days in the office and up to two days working remotely.\nThe team supports Argus key business processes every day, as such you will be required to work on a shift-based rota with other members of the team supporting the business until 8pm. Typically support hours run from 11am to 8pm with each member of the team participating up to 2/3 times a week.\n\nFor more details about the company and to apply please make sure you send your CV and cover letter via our website: www.argusmedia.com/en/careers/open-positions\nBy submitting your job application, you automatically acknowledge and consent to the collection, use and/or disclosure of your personal data to the Company. Argus is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, sexual orientation, gender identity, disability or veteran status.",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Huggingface', 'Langchain', 'Spacy', 'Python', 'TensorFlow', 'Pytorch']",2025-06-10 14:23:15
Azure Solution Architect,Tanisha Systems,12 - 18 years,40-45 Lacs P.A.,"['Pune', 'Gurugram', 'Bengaluru']","Role & responsibilities\nWe are looking for Azure Solution Architect for MNC company permanent position for Remote.\n\nPreferred candidate profile\nIndustry: Financial Services / Banking\nKey Responsibilities\n\nArchitect, design, and lead the migration of Sybase ASE/IQ databases to Azure SQL\nConduct in-depth assessments of existing on-prem Sybase infrastructure and define the cloud migration blueprint\nDefine and implement scalable, secure, and high-performing Azure SQL architectures tailored to financial workloads\nCollaborate with cloud engineering, security, infrastructure, and business teams to ensure alignment\nLead end-to-end migration execution, including schema conversion, data movement, validation, and cutover\nEstablish automated ETL pipelines, backup/recovery strategies, and rollback mechanisms in Azure\nEnsure compliance with financial data regulations (SOX, GDPR, PCI-DSS, etc.)\nOptimize workloads post-migration for performance, cost efficiency, and maintainability\nLead governance reviews, technical workshops, and create documentation and runbooks\n\n\nRequired Skills & Experience\n\n12+ years of overall experience in enterprise data architecture and cloud solutions\nMandatory experience in migrating Sybase ASE/IQ to Azure SQL in large-scale environments\nDeep understanding of Azure SQL, Azure Data Factory, Azure Migrate, BACPAC, and SSMA\nExperience with cloud-native data tools for transformation, validation, and monitoring\nProficiency in scripting (PowerShell, Python) for automation and orchestration\nStrong understanding of data lineage, data masking, encryption, and compliance frameworks\nDemonstrated success in leading multi-terabyte data migrations in financial or regulated environments\nExcellent communication skills with experience in dealing with onsite and offshore teams\n\n\nPreferred Qualifications\n\nMicrosoft Certified: Azure Solutions Architect Expert or Data Engineer Associate\nFamiliarity with performance tuning and resource optimization in Azure SQL\nExperience with application re-platforming or modernization post-migration",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure data migration', 'financial domain', 'Azure services', 'Sybase', 'Architect', 'Azure Migrate', 'Azure', 'Power shell', 'Azure Certified', 'Azure SQL', 'SQL', 'Azure Data Factory', 'scripting', 'SSMA', 'ETL', 'BACPAC', 'Python']",2025-06-10 14:23:18
Data Architect,CGI,8 - 12 years,15-30 Lacs P.A.,['Hyderabad'],"Job Title: Data Architect / Data Modeler\nExperience Level: 8+ Years\nLocation: Hyderabad\nJob Summary\nWe are seeking a highly experienced Data Architect to join our growing Data & Analytics team. This role demands a strategic thinker and technical expert who can design and build robust, scalable, and efficient data solutions. You will play a critical role in architecting end-to-end data pipelines, designing optimized data models, and delivering business-centric data infrastructure using cutting-edge technologies such as Python, PySpark, SQL, Snowflake, and/or Databricks.\nThe ideal candidate will have a deep understanding of data engineering best practices and a proven track record of enabling data-driven decision-making through innovative and scalable data solutions.\nKey Responsibilities\nArchitect & Design Scalable Data Pipelines\nLead the design and implementation of high-performance, scalable, and maintainable data pipelines that support batch and real-time processing.\nData Modeling & Data Architecture\nDesign and implement optimized data models and database schemas to support analytics, reporting, and machine learning use cases.\nCloud Data Platforms\nDevelop and manage modern cloud-based data architectures using platforms like Snowflake or Databricks, ensuring performance, security, and cost-efficiency.\nData Integration & ETL Development\nBuild robust ETL/ELT workflows to ingest, transform, and provision data from a variety of internal and external sources.\nCollaboration with Stakeholders\nWork closely with data analysts, data scientists, product managers, and business leaders to translate business requirements into technical specifications and data solutions.\nData Quality & Governance\nImplement and advocate for best practices in data quality, security, compliance, lineage, and governance.\nPerformance Optimization\nOptimize data storage and query performance using advanced SQL, partitioning, indexing, caching strategies, and compute resource tuning.\nMentorship & Best Practices\nProvide mentorship to junior engineers, establish coding standards, and contribute to the growth and maturity of the data engineering practice.\nRequired Qualifications\nBachelors or Master’s degree in Computer Science, Engineering, Data Science, or a related field.\n8+ years of experience in data engineering or related roles.\nStrong expertise in Python and PySpark for data processing and transformation.\nProficient in advanced SQL with a deep understanding of query optimization and performance tuning.\nHands-on experience with Snowflake and/or Databricks in a production environment.\nExperience in designing and implementing data warehouses and data lakes.\nSolid understanding of distributed computing frameworks, big data ecosystems, and modern data architecture patterns.\nExperience with CI/CD, version control systems (e.g., Git), and workflow orchestration tools (e.g., Airflow, dbt, etc.).\nStrong communication skills with the ability to clearly articulate technical concepts to non-technical stakeholders.Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'pyspark', 'sql', 'snowflake', 'Data Architecture']",2025-06-10 14:23:20
Artificial Intelligence Architect (AI Architect),Team Computers,10 - 15 years,Not Disclosed,['Noida'],"We are seeking a highly skilled AI Architect to lead our team in developing cutting-edge generative AI solutions. \nThis role requires a deep understanding of AI/ML concepts, strong technical expertise, and exceptional leadership abilities. \nAs the Lead AI Architect, you will be the technical authority responsible for defining the strategy, architecture, and execution of cutting-edge generative AI initiatives.\n You will lead a talented team, driving the development of innovative solutions from concept through production deployment.",,,,"['Generative Ai', 'Natural Language Processing', 'Machine Learning']",2025-06-10 14:23:23
"Delivery Head - Infrastructure Engineering, Data Center",Bajaj Allianz General Insurance,8 - 13 years,Not Disclosed,['Pune'],"The role requires strong leadership, strategic thinking, and the ability to drive innovation and efficiency within the technology department. It demands extensive experience in leading complex data center infrastructures, focusing on servers, SAN storage, high availability, disaster recovery, and hybrid environments, including data center operations and physical servers (blade and rack). Responsibilities include designing and testing backup strategies, maintaining documentation, ensuring compliance with regulations, and conducting product and vendor evaluations. Collaboration with various IT teams, the security team, and business stakeholders is essential\nThe role includes setting up processes, prioritizing tasks, and defining KPIs for on role and offrole team members must have experience in managing large technical teams, coaching and motivating them. Required to lead IT infrastructure audits, oversee IT projects, and implement ITIL service management solutions. Managing vendor relationships and handling budgets (CAPEX and OPEX) are crucial. The focus is on saving costs and using technology and personnel efficiently.",,,,"['Service management', 'VMware', 'SAN', 'Automation', 'Data analysis', 'Data center operations', 'Disaster recovery', 'Manager Technology', 'Virtualization', 'Monitoring']",2025-06-10 14:23:25
Software Engineering Associate Advisor,ManipalCigna Health Insurance,1 - 5 years,Not Disclosed,['Hyderabad'],"About Evernorth:\nEvernorth Health Services, a division of The Cigna Group (NYSE: CI), creates pharmacy, care, and benefits solutions to improve health and increase vitality. We relentlessly innovate to make the prediction, prevention, and treatment of illness and disease more accessible to millions of people.\nPosition Overview:\nThe Full-stack Data Engineer is responsible for the delivery of a business need end-to-end starting from understanding the requirements to deploying the software into production. This role requires you to be fluent in some of the critical technologies with proficiency in others and have a hunger to learn on the job and add value to the business. Critical attributes of being a full-stack engineer among others is Ownership & Accountability.\nIn addition to Delivery, the full-stack engineer should have an automation first and continuous improvement mindset. He/She should drive the adoption of CI/CD tools and support the improvement of the tools sets/processes.\nFull stack engineers are able to articulate clear business objectives aligned to technical specifications and work in an iterative, agile pattern daily. They have ownership over their work tasks, and embrace interacting with all levels of the team and raise challenges when necessary. We aim to be cutting-edge engineers - not institutionalized developers.\nRoles & Responsibilities:\nMinimize ""meetings"" to get requirements and have direct business interactions\nWrite referenceable & modular code\nDesign and architect the solution independently\nBe fluent in particular areas and have proficiency in many areas\nHave a passion to learn\nTake ownership and accountability\nUnderstands when to automate and when not to\nHave a desire to simplify\nBe entrepreneurial / business minded\nHave a quality mindset, not just code quality but also to ensure ongoing data quality by monitoring data to identify problems before they have a business impact\nTake risks and champion new ideas\nQualifications\nPrimary Skills:\nHands on with Python / PySpark programimng - 5yrs+\nSQL exp - 4yr+ (NoSQL can also work, but should have SQL 3yrs atleast)\nBig data technologies such as Databricks Or Snowflake - 1yr+ (strong on theory)\nExp working with Cloud Tech - 3yrs+ - Any (AWS preferred)\nDevOps practices - 1yrs+\nExperience Desired:\nExperience with Git/SVN\nExperience with scripting (JavaScript, Python, R, Ruby, Perl, etc.)\nExperience being part of Agile teams - Scrum or Kanban.\nAirflow\nDatabricks / Cloud Certifications\nAdditional Skills:\nExcellent troubleshooting skills\nStrong communication skills\nFluent in BDD and TDD development methodologies\nWork in an agile CI/CD environment (Jenkins experience a plus)\nKnowledge and/or experience with Health care information domains is a plus",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Javascript', 'Agile', 'Perl', 'Scrum', 'Troubleshooting', 'Ruby', 'Monitoring', 'SQL', 'Python']",2025-06-10 14:23:28
"Sustainable, Client and Regulatory Reporting Data Product Owner",Capital Markets,15 - 20 years,Not Disclosed,['Bengaluru'],"Hiring, Sustainable, Client and Regulatory Reporting Data Product Owner - ISS Data (Associate Director)\nAbout your team\n\nThe Technology function provides IT services that are integral to running an efficient run-the business operating model and providing change-driven solutions to meet outcomes that deliver on our business strategy. These include the development and support of business applications that underpin our revenue, operational, compliance, finance, legal, marketing and customer service functions. The broader organisation incorporates Infrastructure services that the firm relies on to operate on a day-to-day basis including data centre, networks, proximity services, security, voice, incident management and remediation.\nThe Technology group is responsible for providing Technology solutions to the Investment Solutions & Services business (which covers Investment Management, Asset Management Operations & Distribution business units globally)\n\nThe Technology team supports and enhances existing applications as well as designs, builds and procures new solutions to meet requirements and enable the evolving business strategy.\nAs part of this group, a dedicated Data Programme team has been mobilised as a key foundational programme to support the execution of the overarching Investment Solutions and Service strategy.\n\nAbout your role\nThe Investment Reporting Data Product Owner role is instrumental in the creation and execution of a future state data reporting product to enable Regulatory, Client, Vendor, Internal & MI reporting and analytics. The successful candidate will have an in- depth knowledge of all data domains that represent institutional clients , the investment life cycle , regulatory and client reporting data requirements.\nThe role will sit within the ISS Delivery Data Analysis chapter and fully aligned with our cross functional ISS Data Programme in Technology, and the candidate will leverage their extensive industry knowledge to build a future state platform in collaboration with Business Architecture, Data Architecture, and business stakeholders.\nThe role is to maintain strong relationships with the various business contacts to ensure a superior service to our internal business stakeholders and our clients.\n\nKey Responsibilities\n\nLeadership and Management:\nLead the ISS distribution, Client Propositions, Sustainable Investing and Regulatory reporting data outcomes defining the data roadmap and capabilities and supporting the execution and delivery of the data solutions as a Data Product lead within the ISS Data Programme.\nLine management responsibilities for junior data analysts within the chapter, coaching, influencing and motivating them for high performance.\nDefine the data product vision and strategy with end-to-end thought leadership.\nLead and define the data product backlog , documentation, enable peer-reviews, analysis effort estimation, maintain backlog, and support end to end planning.\nBe a catalyst of change for driving efficiencies, scale and innovation.\n\nData Quality and Integrity:\nDefine data quality use cases for all the required data sets and contribute to the technical frameworks of data quality.\nAlign the functional solution with the best practice data architecture & engineering.\n\nCoordination and Communication:\nSenior management level communication to influence senior tech and business stakeholders globally, get alignment on the roadmaps.\nCoordinate with internal and external teams to communicate with those impacted by data flows.\nAn advocate for the ISS Data Programme.\nCollaborate closely with Data Governance, Business Architecture, and Data owners etc.\nConduct workshops within the scrum teams and across business teams, effectively document the minutes and drive the actions.\n\nAbout you\nThe Investment Reporting Data Product Owner role is instrumental in the creation and execution of a future state data reporting product to enable Regulatory, Client, Vendor, Internal & MI reporting and analytics. The successful candidate will have an in- depth knowledge of all data domains that represent institutional clients , the investment life cycle , regulatory and client reporting data requirements.\nThe role will sit within the ISS Delivery Data Analysis chapter and fully aligned with cross functional ISS Data Programme in Technology, and the candidate will leverage their extensive industry knowledge to build a future state platform in collaboration with Business Architecture, Data Architecture, and business stakeholders.\nThe role is to maintain strong relationships with the various business contacts to ensure a superior service to our internal business stakeholders and our clients.\n\nKey Responsibilities\n\nLeadership and Management:\nLead the ISS distribution, Client Propositions, Sustainable Investing and Regulatory reporting data outcomes defining the data roadmap and capabilities and supporting the execution and delivery of the data solutions as a Data Product lead within the ISS Data Programme.\nLine management responsibilities for junior data analysts within the chapter, coaching, influencing and motivating them for high performance.\nDefine the data product vision and strategy with end-to-end thought leadership.\nLead and define the data product backlog , documentation, enable peer-reviews, analysis effort estimation, maintain backlog, and support end to end planning.\nBe a catalyst of change for driving efficiencies, scale and innovation.\n\nData Quality and Integrity:\nDefine data quality use cases for all the required data sets and contribute to the technical frameworks of data quality.\nAlign the functional solution with the best practice data architecture & engineering.\n\nCoordination and Communication:\nSenior management level communication to influence senior tech and business stakeholders globally, get alignment on the roadmaps.\nCoordinate with internal and external teams to communicate with those impacted by data flows.\nAn advocate for the ISS Data Programme.\nCollaborate closely with Data Governance, Business Architecture, and Data owners etc.\nConduct workshops within the scrum teams and across business teams, effectively document the minutes and drive the actions.\n\nYour Skills and Experience\n\nStrong leadership and senior management level communication, internal and external client management and influencing skills.\nAt least 15 years of proven experience as a senior business/technical/data analyst within technology and/or business change delivering data led business outcomes within the financial services/asset management industry.\n5-10 years as a data product owner adhering to agile methodology, delivering data solutions using industry leading data platforms such as Snowflake, State Street Alpha Data, Refinitiv Eikon, SimCorp Dimension, BlackRock Aladdin, FactSet etc.\nOutstanding knowledge of Client life cycle covering institutional & wholesale with a focus on CRM data, Transfer agency data.\nVery good understanding of the data generated by investment management processes and how that is leveraged in Go-to market capabilities such as client reporting, Sales, Marketing.\nExcellent knowledge of regulatory environment with a focus on European regulations and ESG specific ones such as MIFID II, EMIR, SFDR.\nWork effortlessly in different operating models such as insourcing, outsourcing and hybrid models.\nAutomation mindset that can drive efficiencies and quality in the reporting landscape.\nKnowledge of industry standard data calcs for fund factsheets, Institutional admin and investment reports would be an added advantage.\nIn Depth expertise in data and calculations across the investment industry covering the below.\nClient Specific data: This includes institutional and wholesale client, account and channels data, client preferences and data sets needed for client analytics. Knowledge of Salesforce desirable.\nTransfer Agency & Platform data: This includes granular client holdings at various levels, client transactions and relevant ref data. Knowledge of role of TPAs as TA and integrating external feeds/products with strategic inhouse data platforms.\nInvestment data: This includes investment life cycle data covering data domains such as trading, ABOR, IBOR, Security and fund reference.\nShould possess Problem Solving, Attention to detail, Critical thinking.\nTechnical Skills: Hands on SQL, Advanced Excel, Python, ML (optional) and knowledge of end-to-end tech solutions involving data platforms.\nKnowledge of data management, data governance, and data engineering practices\nHands on experience with data modelling techniques such as dimensional, data vault.\nWillingness to own and drive things, collaboration across business and tech stakeholders.",Industry Type: Investment Banking / Venture Capital / Private Equity,Department: Product Management,"Employment Type: Full Time, Permanent","['Data Transformation', 'ESG Framework', 'Snowflake', 'Asset Management', 'Product Owner', 'Product Manager', 'MIFID II', 'alphastate street', 'SQL', 'EMIR', 'Data Quality', 'Data Analysis', 'charles river', 'Agile', 'UK Regulatory Reporting', 'data roadmap', 'Capital Market Operations', 'Aladdin', 'SFDR.', 'Python', 'ML']",2025-06-10 14:23:30
Sr. Data Scientist-Stratup-Mid-Size companies Exp.@ Bangalore_Urgent,"A leader in this space, we deliver world...",8 - 13 years,Not Disclosed,['Bengaluru'],"Senior Data Scientist\n\nLocation: Onsite Bangalore\nExperience: 8+ years\n\nRole Overview\n\nWe are seeking a Senior Data Scientist with a strong foundation in machine learning, deep learning, and statistical modeling, with the ability to translate complex operational problems into scalable AI/ML solutions. In addition to core data science responsibilities, the role involves building production-ready backends in Python and contributing to end-to-end model lifecycle management. Exposure to computer vision is a plus, especially for industrial use cases like identification, intrusion detection, and anomaly detection.\n\nKey Responsibilities\n\nDevelop, validate, and deploy machine learning and deep learning models for forecasting, classification, anomaly detection, and operational optimization\nBuild backend APIs using Python (FastAPI, Flask) to serve ML/DL models in production environments\nApply advanced computer vision models (e.g., YOLO, Faster R-CNN) to object detection, intrusion detection, and visual monitoring tasks\nTranslate business problems into analytical frameworks and data science solutions\nWork with data engineering and DevOps teams to operationalize and monitor models at scale\nCollaborate with product, domain experts, and engineering teams to iterate on solution design\nContribute to technical documentation, model explainability, and reproducibility practices\n\n\nRequired Skills\n\nStrong proficiency in Python for data science and backend development\nExperience with ML/DL libraries such as scikit-learn, TensorFlow, or PyTorch\nSolid knowledge of time-series modeling, forecasting techniques, and anomaly detection\nExperience building and deploying APIs for model serving (FastAPI, Flask)\nFamiliarity with real-time data pipelines using Kafka, Spark, or similar tools\nStrong understanding of model validation, feature engineering, and performance tuning\nAbility to work with SQL and NoSQL databases, and large-scale datasets\nGood communication skills and stakeholder engagement experience\n\n\nGood to Have\n\nExperience with ML model deployment tools (MLflow, Docker, Airflow)\nUnderstanding of MLOps and continuous model delivery practices\nBackground in aviation, logistics, manufacturing, or other industrial domains\nFamiliarity with edge deployment and optimization of vision models\n\n\nQualifications\n\nMasters or PhD in Data Science, Computer Science, Applied Mathematics, or related field\n7+ years of experience in machine learning and data science, including end-to-end deployment of models in production",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scikit-learn', 'time-series modeling', 'ML/DL libraries', 'data science', 'Python', 'Airflow', 'Kafka', 'MLflow', 'logistics', 'anomaly detection', 'aviation', 'SQL', 'PyTorch', 'NoSQL', 'MLOps', 'forecasting techniques', 'Docker', 'manufacturing', 'FastAPI', 'Spark', 'TensorFlow', 'Flask']",2025-06-10 14:23:33
Software Engineering Advisor,ManipalCigna Health Insurance,2 - 5 years,Not Disclosed,['Hyderabad'],"Software Engineering Advisor- HIH - Evernorth\nAbout Evernorth:\nEvernorth Health Services, a division of The Cigna Group (NYSE: CI), creates pharmacy, care, and benefits solutions to improve health and increase vitality. We relentlessly innovate to make the prediction, prevention, and treatment of illness and disease more accessible to millions of people.\nPosition Overview:\nThe Full-stack Data Engineer is responsible for the delivery of a business need end-to-end starting from understanding the requirements to deploying the software into production. This role requires you to be fluent in some of the critical technologies with proficiency in others and have a hunger to learn on the job and add value to the business. Critical attributes of being a full-stack engineer among others is Ownership & Accountability.\nIn addition to Delivery, the full-stack engineer should have an automation first and continuous improvement mindset. He/She should drive the adoption of CI/CD tools and support the improvement of the tools sets/processes.\nFull stack engineers are able to articulate clear business objectives aligned to technical specifications and work in an iterative, agile pattern daily. They have ownership over their work tasks, and embrace interacting with all levels of the team and raise challenges when necessary. We aim to be cutting-edge engineers - not institutionalized developers.\nRoles & Responsibilities:\nMinimize ""meetings"" to get requirements and have direct business interactions\nWrite referenceable & modular code\nDesign and architect the solution independently\nBe fluent in particular areas and have proficiency in many areas\nHave a passion to learn\nTake ownership and accountability\nUnderstands when to automate and when not to\nHave a desire to simplify\nBe entrepreneurial / business minded\nHave a quality mindset, not just code quality but also to ensure ongoing data quality by monitoring data to identify problems before they have a business impact\nTake risks and champion new ideas\nQualifications\nPrimary Skills:\nHands on with Python / PySpark programimng - 8yrs+\nSQL exp - 8yr+ (NoSQL can also work, but should have SQL 3yrs atleast)\nBig data technologies such as Databricks Or Snowflake - 1yr+ (strong on theory)\nExp working with Cloud Tech - 3yrs+ - Any (AWS preferred)\nDevOps practices - 2yrs+Python\nExperience Desired:\nExperience with Git/SVN\nExperience with scripting (JavaScript, Python, R, Ruby, Perl, etc.)\nExperience being part of Agile teams - Scrum or Kanban.\nAirflow\nDatabricks / Cloud Certifications\nAdditional Skills:\nExcellent troubleshooting skills\nStrong communication skills\nFluent in BDD and TDD development methodologies\nWork in an agile CI/CD environment (Jenkins experience a plus",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Javascript', 'Agile', 'Perl', 'Scrum', 'Troubleshooting', 'Ruby', 'Monitoring', 'SQL', 'Python']",2025-06-10 14:23:36
Senior DevOps Engineer,Leading Client,3 - 5 years,Not Disclosed,['Hyderabad'],"Roles & Responsibilities:\n3+ years of working experience in data engineering.\nHands-on keyboard' AWS implementation experience across a broad range of AWS services.\nMust have in depth AWS development experience (Containerization - Docker, Amazon EKS, Lambda, EC2, S3, Amazon DocumentDB, PostgreSQL)\nStrong knowledge of DevOps and CI/CD pipeline (GitHub, Jenkins, Artifactory)\nScripting capability and the ability to develop AWS environments as code\nHands-on AWS experience with at least 1 implementation (preferred in an Enterprise scale environment)\nExperience with core AWS platform architecture, including areas such asOrganizations, Account Design, VPC, Subnet, segmentation strategies.\nBackup and Disaster Recovery approach and design\nEnvironment and application automation\nCloudFormation and third-party automation approach/strategy\nNetwork connectivity, Direct Connect and VPN\nAWS Cost Management and Optimization\nSkilled experience in Python libraries (NumPy, Pandas dataframe)",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['DevOps', 'artifactory', 'kubernetes', 'continuous integration', 'python', 'github', 'ci/cd', 'scale', 'disaster recovery', 'numpy', 'eks', 'data engineering', 'docker', 'ansible', 'pandas', 'containerization', 'puppet', 'amazon ec2', 'lambda expressions', 'postgresql', 'linux', 'jenkins', 'aws', 'ci cd pipeline']",2025-06-10 14:23:39
Sr. Software Engineer_Java_SpringBoot,Lowes Services India Private limited,5 - 7 years,Not Disclosed,['Bengaluru'],"Executes the development, maintenance, and enhancements of integrations of varying complexity levels across various integration platforms on cloud and on-prem infrastructure\nTranslates business requirements and specifications into Integration hub related solutions, partners with Product Manager to understand business needs and functional specifications\nEvaluates project deliverables to ensure they meet specifications and architectural standards\nProvide technical support for the integration hub; solutions are extensible; works to simplify, optimize, remove bottlenecks, etc\nHandles data manipulation (extract, load, transform) and administration of data and systems securely and in accordance with enterprise data governance standards\nMaintains the health and monitoring of the Integration hub and related activities; ensures high availability of the platform; monitors workload demands\nParticipates and coaches others in end-to-end testing by applying and sharing an understanding of complex company and industry methodologies, policies, standards, and controls\nUnderstands Computer Science and/or Computer Engineering fundamentals\nParticipates in continuous improvement activities including training opportunities; continuously strives to learn data engineering best practices and apply them to daily activities\nWrite and review technical documents, including design, development, and revision documents.\nQualifications\n  Minimum Qualifications :\nBachelors Degree in Engineering, Computer Science, CIS, or related field (or equivalent work experience in a related field)\nMust have:\n5 to 7 years of software development experience in Java and J2EE technologies\nExtensive technical & development experience in Java, J2EE, REST API, Spring Framework\nWorked on build tools like Maven/Gradle\nBuilding scalable and fault tolerant applications using best practices and principles (SOLID, Clean Code, Design Patterns)\nStrong hands-on experience in developing micro service using Spring Boot, Spring Batch.\nStrong experience in Spring Framework such as Spring MVC, IOC, AOP and Spring JDBC.\nStrong experience in ORM (eg JPA, Hibernate), NoSQL and developing REST APIs using Jersey framework.\nExperience in core Java-Multithreading, collections, Servlet and JDBC.\nExperience in designing front end interfaces using HTML5, CSS3, JavaScript, jQuery, Ajax and AngularJS.\nHands on experience with build and deployment tools such as Ant or Maven or Gradle.\nUnit and integration testing using JUnit.\nFamiliar DevOps tools and experience in CI/CD software.\nWorked with postgres DB and other non-SQL DB.\nFamiliar with Jenkins\nAble to write unit test cases and functional test cases.\nWorked closely with the architecture group to validate overall design decisions\nWorked independently and handle product responsibilities\nQuick learner, with strong analytical (problem solving) & debugging skills\nGood understanding of performance and security aspects of software development\nStrong knowledge of Object-oriented programming\nStrong design skills, also experience working with/applying Design patterns to solve problems.\nExperience in docker and Kubernetes.\nNice to have:\nExperience in Agile methodology.\nFamiliar with Oracle, GCP Cloud Platform.\nIntegration tools experience Spark, PySpark, Talend.",Industry Type: Retail,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hibernate', 'Core Java', 'jQuery', 'Multithreading', 'Javascript', 'JDBC', 'Oracle', 'Technical support', 'SQL', 'Ajax']",2025-06-10 14:23:42
Software Engineering Advisor,ManipalCigna Health Insurance,10 - 13 years,Not Disclosed,['Hyderabad'],"Position Summary:\nFull Stack Engineer: Data Engineering- Job Description Cigna, a leading Health Services company, is looking for an exceptional engineer in our Data & Analytics Engineering organization. The Full Stack Engineer is responsible for the delivery of a business need starting from understanding the requirements to deploying the software into production. This role requires you to be fluent in some of the critical technologies with proficiency in others and have a hunger to learn on the job and add value to the business. Critical attributes of being a Full Stack Engineer, among others, is ownership, eagerness to learn & an open mindset. In addition to Delivery, the Full Stack Engineer should have an automation first and continuous improvement mindset. Person should drive the adoption of CI/CD tools and support the improvement of the tools sets/processes.\nJob Description & Responsibilities :\nBehaviors of a Full Stack Engineer: Full Stack Engineers are able to articulate clear business objectives aligned to technical specifications and work in an iterative, agile pattern daily. They have ownership over their work tasks, and embrace interacting with all levels of the team and raise challenges when necessary. We aim to be cutting-edge engineers - not institutionalized developers.\nExperience Required:\n11 - 13 years of experience in Python.\n11 - 13 years of experience in Data Management & SQL expertise.\n5+ years in Spark and AWS.\n5+ years in Databrick.\nExperience with working in agile CI/CD environments.\nExperience Desired:\nGit ,Teradata & Snowflake experience.\nExperience working on Analytical Models and their deployment / production enable? ment via data & analytical pipelines.\nExpertise with big data technologies - Hadoop, HiveQL, (Scala/Python) Expertise on Cloud technologies - (S3, Glue, Terraform, Lambda, Aurora, Redshift, EMR).\nExperience with BDD and TDD development methodologies.\nHealth care information domains preferred.\nEducation and Training Required:\nBachelor s degree (or equivalent) required.\nPrimary Skills:\nPython, AWS and Spark.\nCI/CD , Databrick.\nData management and SQL.\nAdditional Skills:\nStrong communication skills.\nTake ownership and accountability.\nWrite referenceable & modular code Be fluent areas and have proficiency in many areas Have a passion to learn.\nHave a quality mindset, not just code quality but also to ensure ongoing data quality by monitoring data to identify problems before they have business impact.\nTake risks and champion new ideas.\nAbout Evernorth Health Services",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'GIT', 'Data management', 'TDD', 'Agile', 'Data quality', 'Teradata', 'Monitoring', 'SQL', 'Python']",2025-06-10 14:23:45
Data Engineering_PLM Consulting,Capgemini,5 - 8 years,Not Disclosed,['Pune'],"Capgemini Invent \n\nCapgemini Invent is the digital innovation, consulting and transformation brand of the Capgemini Group, a global business line that combines market leading expertise in strategy, technology, data science and creative design, to help CxOs envision and build whats next for their businesses.\n\n What you will love about working here \nWe recognize the significance of flexible work arrangements to provide support. Be it remote work, or flexible work hours, you will get an environment to maintain healthy work life balance.\nAt the heart of our mission is your career growth. Our array of career growth programs and diverse professions are crafted to support you in exploring a world of opportunities.\nEquip yourself with valuable certifications in the latest technologies such as Generative AI.\n\n\n Your role \nBusiness consulting\nExperience in Discrete Manufacturing /R&D/Product Development\nEngineering Transformation on either of the below areas :\n- Process/project transformation\n- manufacturing transformation\n- servitization/service transformations\nRobust Product development and R&D experience\nDesign for X (Value, Six Sigma, Cost)\nLean Manufacturing (waste reduction, DFSS, DMAIC, Process Optimization)\nProcess mapping, Value-stream mapping\nWell versed in Agile/Scrum and SDLC processes\n\n\n Your Profile \nSupporting our clients in Digital continuity strategy, R&D transformation, New product introduction (NPI) process management, IT-Architecture & -specification, project management, change management and rollout.\n5-8 years of overall experience with 1+ years into consulting\nExperience in requirements gathering, design, optimization and / or implementation of business processes in product development or R&D within Aerospace & Defense Domain.\nBackground in analyzing As-Is Engineering Processes, methods and tools for modernization and efficiency improvement.\nExperience/Familiarity in PLM Tools Such as 3DX/Siemens Teamcenter/PTC Windchill etc.\nExperience in implementing large scale engineering transformations programs in the field of Process, Manufacturing, Service, R&D\nFamiliar with business process mapping techniques and use of tools such as Visio, SAP Signavio, Celonis.\nPossesses combination of technological understanding, strategic analysis, and implementation aspects as well as the talent to think conceptually and analytically.\nExperience eliciting high-level business requirements, documentation of user stories, performing fit gap analysis of requirement vs OOTB functionalities, creation of roadmaps, milestones, delivery timelines etc\nExperience in creating detailed functional specifications and other documentation, such as requirement traceability matrices, work-flow diagrams and use-cases.\nDeveloped POCs, and/or contributed in solution development for Industry.\n\n\n About Capgemini \n\nCapgemini is a global business and technology transformation partner, helping organizations to accelerate their dual transition to a digital and sustainable world, while creating tangible impact for enterprises and society. It is a responsible and diverse group of 340,000 team members in more than 50 countries. With its strong over 55-year heritage, Capgemini is trusted by its clients to unlock the value of technology to address the entire breadth of their business needs. It delivers end-to-end services and solutions leveraging strengths from strategy and design to engineering, all fueled by its market leading capabilities in AI, cloud and data, combined with its deep industry expertise and partner ecosystem. The Group reported 2023 global revenues of 22.5 billion.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['visio', 'scrum', 'agile', 'sdlc', 'sdlc process', 'discrete manufacturing', 'sap', 'business process mapping', 'process optimization', 'solution development', 'signavio', 'data engineering', 'plm', 'business consulting', 'siemens teamcenter', 'product development', 'new product introduction']",2025-06-10 14:23:48
Pyspark Developer,Optimum Solutions,4 - 9 years,Not Disclosed,"['Chennai', 'Bengaluru']","Minimum 4 years of experience in build & deployment of Bigdata applications using PySpark\n2+ years of Experience with AWS Cloud on data integration with Spark & AWS Glue/EMR\nIn-depth understanding of Spark architecture & distributed systems\nGood exposure to Spark job optimizations\nExpertise in handling complex large-scale Big Data environments\nAble to design, develop, test, deploy, maintain, and improve data integration pipeline\nMandatory Skills :\n4+ years of exp in PySpark\n2+ years of exp in AWS Glue/EMR\nStrong knowledge on SQL is required\nExcellent written & spoken communication skills, and time management skills.\nNice-to-Have\nAny cloud skills\nAny ETL knowledge,",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'AWS', 'SQL']",2025-06-10 14:23:49
Design Engineer (Autodesk Inventor),Hitech Digital Solutions,2 - 7 years,Not Disclosed,"['Ahmedabad( Prahlad Nagar )', 'Anand', 'Vadodara']","Job Title: Sr. Designer Autodesk Inventor\n\nLocation: Office-Based (Ahmedabad, India)\n\nAbout Us\n\nHitech is a leading provider of Data, Engineering Services, and Business Process Solutions. We specialize in delivering comprehensive engineering design services to clients across industries, including manufacturing, heavy engineering, and fabrication. With strong delivery teams based in India and global sales offices, we are committed to offering reliable, innovative, and scalable design support.",,,,"['Autodesk Inventor', '3D', 'Assembly Design', 'Sheet Metal Design', 'Autodesk', 'Sheet Metal', '3D Modeling', 'Solid Works', 'Inventor']",2025-06-10 14:23:52
AI Ml Engineer,Fulcrum Worldwide Software,4 - 8 years,15-30 Lacs P.A.,['Pune'],"The Role\nMachine learning and data science are interdisciplinary fields that require a combination of skills in mathematics, statistics, programming, and domain-specific knowledge to extract meaningful insights from data and develop predictive models.\nSkills Requirements\nMandatory Skillset - ML Techniques, any ML Framework, Gen AI COncepts, Azure, Python\nSecondary Skillset - AWS, or Google Cloud\n\nRequirements\nWork closely with cross-functional teams to understand business requirements and translate them into machine learning solutions.\nCollect, clean, and pre-process large datasets to prepare them for machine learning models.\nDevelop and implement machine learning algorithms and models for solving specific business problems.\nCollaborate with data engineers to ensure seamless integration of machine learning models into production systems.\nFine-tune models for optimal performance and conduct thorough testing and validation.\nStay updated on the latest advancements in machine learning and artificial intelligence and assess their potential impact on our projects.\nEffectively communicate complex technical concepts and findings to non-technical stakeholders.\nMonitor and maintain deployed models and update them as needed to adapt to changes in data or business requirements.\nAdhere to ethical standards and ensure that machine learning solutions comply with relevant regulations.\nProficiency in Python with hands-on experience in cloud platforms including GCP, AWS, and Azure; strong skills in API integration.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['NLP', 'Generative ai', 'Machine Learning', 'Deep Learning']",2025-06-10 14:23:54
Data Software Quality Engineer 36Lakhs CTC|| Kandi Srinivasa Reddy,Integra Technologies,8 - 11 years,35-37.5 Lacs P.A.,"['Kolkata', 'Ahmedabad', 'Bengaluru']","Dear Candidate,\nWe are looking for a Software Quality Engineer to ensure software products meet quality standards through test planning, automation, and defect analysis.\n\nKey Responsibilities:\nDevelop and execute manual and automated test cases.\nDesign test strategies for functional, regression, and performance testing.\nCollaborate with development and product teams to identify requirements.\nLog defects and track resolutions using tools like JIRA.\nContribute to continuous improvement of QA processes.\nRequired Skills & Qualifications:\nExperience with testing frameworks (Selenium, JUnit, TestNG, Cypress).\nProficient in scripting or programming (Python, Java, or JavaScript).\nFamiliarity with CI/CD tools (Jenkins, GitHub Actions).\nStrong understanding of SDLC, Agile, and QA methodologies.\nExperience with API testing (Postman, REST-assured) is a plus.\nSoft Skills:\nStrong troubleshooting and problem-solving skills.\nAbility to work independently and in a team.\nExcellent communication and documentation skills.\n\nNote: If interested, please share your updated resume and preferred time for a discussion. If shortlisted, our HR team will contact you.\n\nKandi Srinivasa Reddy\nDelivery Manager\nIntegra Technologies",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automotive Spice', 'Software Quality Assurance', 'Software Quality', 'Aspice', 'V-Model', 'SQA', 'Software QA', 'ISO', 'Spice', 'Process Definition', 'Software Development Life Cycle', 'Internal Quality Auditor', 'Metrics Analysis', 'Quality Process', 'Process Compliance', 'CMMI']",2025-06-10 14:23:57
DS / ML / Time-Series Engineer,Biz-metric India,2 - 5 years,Not Disclosed,"['Mumbai', 'Gurugram', 'Bengaluru']","BIZMETRIC INDIA PRIVATE LIMITED is looking for DS / ML / Time-Series Engineer to join our dynamic team and embark on a rewarding career journey\nData Exploration and Preparation:Explore and analyze large datasets to understand patterns and trends\nPrepare and clean datasets for analysis and model development\nFeature Engineering:Engineer features from raw data to enhance the performance of machine learning models\nCollaborate with data scientists to identify relevant features for model training\nModel Development:Design and implement machine learning models to solve business problems\nWork on both traditional statistical models and modern machine learning algorithms\nScalable Data Pipelines:Develop scalable and efficient data pipelines for processing and transforming data\nUtilize technologies like Apache Spark for large-scale data processing\nModel Deployment:Deploy machine learning models into production environments\nCollaborate with DevOps teams to integrate models into existing systems\nPerformance Optimization:Optimize the performance of data pipelines and machine learning models\nFine-tune models for accuracy, efficiency, and scalability\nCollaboration:Collaborate with cross-functional teams, including data scientists, software engineers, and business stakeholders\nCommunicate technical concepts and findings to non-technical audiences\nContinuous Learning:Stay current with advancements in data science and engineering\nImplement new technologies and methodologies to improve data engineering processes\n\n\nLearning Certification Opportunities: Enhance your professional growth.\nComprehensive Medical Coverage and Life Insurance: For your we'll-being.\nFlexible Work Environment: Enjoy a 5-day work week.\nCollaborative Culture: Be part of a fun, innovative workplace.\nJob Description:\nPython, Data Science, Azure Databricks, Machine Learning",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['algorithms', 'python', 'natural language processing', 'neural networks', 'predictive analytics', 'data pipeline', 'machine learning', 'data engineering', 'artificial intelligence', 'sql', 'deep learning', 'data science', 'spark', 'predictive modeling', 'machine learning algorithms', 'ml']",2025-06-10 14:24:00
Internal Audit-Data Strategy - Analyst,Goldman Sachs,1 - 3 years,Not Disclosed,['Hyderabad'],"What We Do\nInternal Audit s mission is to independently assess the firm s internal control structure, including the\nfirm s governance processes and controls, risk management, capital and anti-financial crime\nframework. In addition, it is also to raise awareness of control risk and monitor the implementation of\nmanagement s control measures.\nIn doing so, internal Audit:\nCommunicates and reports on the effectiveness of the firm s governance, risk management and controls that mitigate current and evolving risk\nRaise awareness of control risk\nAssesses the firm s control culture and conduct risks; and\nMonitors management s implementation of control measures\nGoldman Sachs Internal Audit comprises individuals from diverse backgrounds including chartered\naccountants, developers, risk management professionals, cybersecurity professionals, and data\nscientists. We are organized into global teams comprising business and technology auditors to cover\nall the firm s businesses and functions, including securities, investment banking, consumer and\ninvestment management, risk management, finance, cyber-security and technology risk, and\nengineering.\nWho We Look For\nGoldman Sachs Internal Auditors demonstrate a strong risk, control and analytical mindset, exercise\nprofessional skepticism and challenge status quo on risks and control measures effectively with\nmanagement. We look for individuals who enjoy learning about audit, businesses, and processes,\nhave innovative and creative mindset in adapting analytical techniques to enhance audit function,\ndevelop teamwork and build relationships and are able to evolve and thrive in a fast-paced global\nenvironment.\nEmbedded - Data Analytics\nIn Internal Audit, we ensure that Goldman Sachs maintains effective controls by assessing the\nreliability of financial reports, monitoring the firm s compliance with laws and regulations, and\nadvising management on developing smart control solutions. Embed Data Analytics team leverages\nits programming and analytical capabilities to build innovative data driven solutions. The team works\nclosely with auditors to understand their pain points and develop data-centric solutions to address\nthe same.\nYour Impact\nAs part of the third line of defense, you will be involved in independently assessing the firm s overall\ncontrol environment and its effectiveness as it relates to current and emerging risks and\ncommunicating the results to local/ global management. In doing so, you will be supporting the\nprovision of independent, objective and timely assurance around the firm s internal control structure,\nthereby supporting the Audit Committee, Board of Directors and Risk Committee in fulfilling their\noversight responsibilities.\nWe are looking for a strong data scientist, passionate about using data to challenge the norm, to join\nour Embed Data Analytics team. The candidate will work closely with the audit teams to build\ninnovative and reusable analytical tools that will help make audit testing more efficient and provide\nmeaningful insights into firm s control environment.\nResponsibilities\nExecute on DA strategy developed by IA management within the context of audit responsibilities, such as risk assessment, audit planning, creation of reusable tools and providing innovative solutions to complex problems\nPartner with audit teams to help identify risks associated with businesses and facilitate strategic data sourcing and develop innovative solutions to increase efficiency and effectiveness of audit testing\nBuild production ready analytical tools to automate repeatable and reusable processes within IA\nBuild and manage relationships and communications with Audit team members\nBasic Qualifications\n1-3 years of experience with a minimum of Bachelor s in Computer Science, Math, or Statistics\nExperience with RDBMS/ SQL\nProficiency in programming languages, such as Python, Java, or C++\nKnowledge of basic statistics, including descriptive statistics, data distribution models,\nTime Series Analysis, correlation, and regression, and its application to data\nStrong team player with excellent communication skills (written and oral). Ability to communicate what is relevant and important in a clear and concise manner and ability to handle multiple tasks\nStrong contributing member of Data Science team and help build analytical capabilities for Internal Audit Division\nDriven and motivated and constantly taking initiative to improve performance\nPreferred Qualifications\nExperience with advanced data analytics tools and techniques\nStrong experience in RDBMS/ SQL and Data Warehousing.\nExposure to ETL Processes and Data Engineering.\nExperience in implementing Data Quality measures and entitlement models\nFamiliarity in programming languages such as Python.\nStrong team player with excellent communication skills (written and oral). Ability to communicate what is relevant and important in a clear and concise manner and ability to handle multiple tasks\nSelf-driven and motivated to take up initiatives to improve our processes.\nWe re committed to finding reasonable accommodations for candidates with special needs or disabilities during our recruiting process. Learn more: https: / / www.goldmansachs.com / careers / footer / disability-statement.html",Industry Type: Banking,Department: Risk Management & Compliance,"Employment Type: Full Time, Permanent","['C++', 'Manager Internal Audit', 'Assurance', 'Analytical', 'Data analytics', 'HTML', 'Data quality', 'Investment banking', 'Risk management', 'SQL']",2025-06-10 14:24:02
Staff Quality Engineer,Fanatics,9 - 14 years,40-50 Lacs P.A.,['Hyderabad'],"As a Staff Quality Engineer at Fanatics, you will specialize in building automated testing including test suites, reporting capabilities and continuous integration to ensure the accuracy and consistency of the systems driving our inventory lifecycle, planning and product performance. Youll be part of a multi-disciplinary Quality Engineering team while also working with our data engineers to ensure we have tools and processes that warrant the correctness of the functionality and data and quickly identify any problems. The teams within our inventory organization own the source of truth for all inventory position related info, develop the systems that handle the inventory lifecycle of our products and the integrations that keep our partners updated with an accurate picture of what we can sell to our fans and what they demand the most. Our teams also work on models to drive efficiency of our inventory management. Multiple teams across Fanatics Commerce, including Fulfillment, Merchandising and Planning, Finance, Business Intelligence and Analytics rely on these datasets and several KPIs dashboards used by our Senior Leadership aid data-driven business decisions. The team takes pride in ensuring that these datasets and the processes that generate the data are accurate, reliable and available on time by building and operating near real-time & batch data pipelines, exploring emerging technologies in this area, and helping other teams consume and realize the value of the data. \nIf you are an experienced Quality Engineer with a passion for delivering quality products and are curious and eager to keep learning and discovering new tools to improve the efficiency of our quality cycle this opportunity is for you. \n\nResponsibilities:\nNurture a culture of quality through collaboration with teammates across the engineering function to make sure quality is embedded in both processes and technology. \nDevelop quality focused test strategies that help the team deliver software that provides maximum quality without sacrificing business value. \nDefine quality metrics and build quality monitoring solutions and dashboards. \nMentor/coach team members to ensure appropriate testing coverage within the team with a focus on continuous testing and a shift-left approach. \nContribute to process improvements and refinement of Quality Engineering practices across the QE function. \nRecommend best test practices and evangelize a testing early culture to empower the team to make sound engineering decisions. \nWork with multiple teams of engineers and product managers to gather requirements and data sources to design and build test plans. \nParticipate in projects developed with agile methodology. \nDesign and document test cases and test data to ensure proper coverage. \nPerform exploratory/manual tests as needed to ensure quality of the data across all data platform components. \nCollaborates with data partners to triage issues and works to ensure the validity, timeliness, consistency, completeness and accuracy of our data across all data platform components. \nWrite, execute, and monitor automated test suites for integration and regression testing. \nIntegrate tests as part of continuous delivery pipelines. \nResearch new tools that can improve our quality process and implement them. \n\nA suitable candidate would have:\nMinimum 8 years of testing experience certifying the quality of with applications developed in languages like Python, Golang, Java, C#. \nExperience in a programming language like Python, Java, Golang, Node,js using it for automation and data validation. \nMust understand databases and ORMs, experienced with at least one RDBMS and DB Query language. \nAbility to define technical standards and guidelines for test framework development. \nExperience on modern Quality Engineering principles such as Continuous Testing and Shift Left. \nClear understanding of different testing principles like data driven testing, behavior driven development, etc. \nSolid experience in writing clear, concise, and comprehensive test plans and test cases. \nExperience in building automated test suites for API's REST and gRPC with focus on data validation. \nExtensive experience with OpenAPI Specifications and tools like Swagger for designing, documenting, and validating APIs. \nProficiency in using schema validation tools for ensuring compliance with API contracts. \nStrong understanding of RESTful principles, HTTP protocols, and JSON schema validation. \nProficiency in managing test data and mocking/stubbing for complex test scenarios. \nExperience testing applications using producer/consumer models and event streaming like Kafka, RabbitMQ, Pulsar is a plus. \nProficiency in testing tools such as Postman, K6 and JMeter for API performance and functional testing. \nHands-on experience with test automation tools like Playwright, Selenium, and other modern test frameworks. \nGood understanding of service oriented and microservices architecture. \nExperience with cloud environments like AWS, GCP, source control tools like Github and continuous integration and delivery software, preferably, Gitlab. \nExperience with AWS services like S3, SQS, RDS, Elasticache, EMR \nProven track record of mentoring team members on automation best practices, framework design, and testing methodologies \nAbility to lead technical discussions and drive decisions on test automation strategies and implementation. \nStrong debugging, root cause analysis, and problem-solving skills. \nAbility to troubleshoot issues in distributed systems, APIs, and automation tools. \nGreat communication skills \nExperience on inventory management and fulfillment processes and workflow orchestration tools like Maestro, Temporal, etc. are a plus.",Industry Type: Internet (E-Commerce),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Playwright', 'Data Testing', 'Automation Testing', 'Selenium', 'Python', 'java', 'API Testing', 'Functional Testing', 'SQL']",2025-06-10 14:24:04
Senior Data Scientist,Tractable,5 - 10 years,Not Disclosed,['Noida'],"Who we are\nTractable is an Artificial Intelligence company bringing the speed and insight of Applied AI to visual assessment. Trained on millions of data points, our AI-powered solutions connect everyone involved in insurance, repairs, and sales of homes and cars - helping people work faster and smarter, while reducing friction and waste.\nFounded in 2014, Tractable is now the AI tool of choice for world-leading insurance and automotive companies. Our solutions unlock the potential of Applied AI to transform the whole recovery ecosystem, from assessing damage and accelerating claims and repairs to recycling parts. They help make response to recovery up to ten times faster - even after full-scale disasters like floods and hurricanes.\nTractable has a world-class culture, backed up by our team, making us a global employer of choice!\nWere a diverse team, uniting individuals of over 40 different nationalities and from varied backgrounds, with machine learning researchers and motor engineers collaborating together on a daily basis. We empower each team member to have tangible impact and grow their own scope by intentionally building a culture centred around collaboration, transparency, autonomy and continuous learning.\nWere seeking a Senior Data Scientist to lead the delivery of real-world AI solutions across industries like insurance and automotive. Youll drive end-to-end ML development from scalable pipelines to production deployment while mentoring teammates and collaborating with enterprise partners to deliver business impact.\nYour impact\nArchitect Solutions: Design and optimise scalable ML systems, focusing on computer vision and NLP/LLM applications\nBuild & Deploy Models: Develop and productionise deep learning models using Python and modern ML frameworks\nLead Cross-Functionally: Align research, engineering, and product goals through hands-on leadership\nMentor & Guide: Support junior team members and promote best practices in ML development\nEngage Customers: Collaborate directly with enterprise clients to ensure seamless integration of AI solutions\nDrive Innovation: Evaluate and implement new AI tools and methodologies to enhance performance and scalability\nWhat youll need to be successful\nApplied AI Expertise: 5+ years building ML systems, especially in computer vision or NLP/LLM\nStrong Python & ML Skills: Proficiency in Python and ML libraries (e.g. PyTorch, TensorFlow)\nProduction Experience: Hands-on experience deploying ML models in production environments\nCloud & Data Engineering: Knowledge of cloud platforms (ideally AWS) and scalable data pipelines\nCollaboration & Communication: Ability to work across teams and communicate complex ideas clearly\nProblem-Solving Mindset: Analytical thinking and a focus on continuous improvement\nPreferred experience\nIndustry Impact: Experience applying AI in insurance, automotive, or similar sectors\nLLM/NLP Expertise: Background in large language models or conversational AI\nScalable Product Delivery: Proven success scaling ML solutions in production\nLeadership: Track record of mentorship and technical leadership in data science teams\nDiversity commitment\nAt Tractable, we are committed to building a diverse team and inclusive workplace where people s varied backgrounds and experiences are valued and recognised.\nWe encourage applications from candidates of all backgrounds and offer equal opportunities without discrimination.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer vision', 'Claims', 'Analytical', 'Artificial Intelligence', 'Machine learning', 'Technical leadership', 'Mentor', 'Continuous improvement', 'Automotive', 'Python']",2025-06-10 14:24:06
"Associate Director, Data and Analytics",Hsbc,10 - 15 years,Not Disclosed,['Pune'],"Engineer the data transformations and analysis for the Cash Equities Trading platform.\nTechnology SME on the real-time stream processing paradigm.\nBring your experience in Low latency, High through-put, auto scaling platform design and implementation.\nImplementing an end-to-end platform service, assessing the operations and non-functional needs clearly.\nDrive and document technical and functional decisions with appropriate diligence.\nProvide operational support and manage incidents.\nRequirements\nTo be successful in this role, you should meet the following requirements:\n10+ years of experience in data engineering technology and tools.\nPreferred having experience with Java / Scala based implementations for enterprise-wide platforms.\nExperience with Apache Beam, Google Dataflow, Apache Kafka for real-time steam processing technology stack.\nComplex state-full processing of events with partitioning for higher throughputs.\nHave dealt with fine-tuning the through-puts and improving the performance aspects on data pipelines.\nExperience with analytical data store optimizations, querying and managing them.\nExperience with alternate data engineering tools (Apache Flink, Apache Spark etc)\nAutomated CI/CD or operations concerns on the engineering platforms.\nInterpreting problems from functional context and transforming them into technology solutions.",Industry Type: Consumer Electronics & Appliances,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['operational support', 'Usage', 'spark', 'Analytical', 'SCALA', 'Banking', 'Technology solutions', 'Associate Director', 'Analytics', 'Financial services']",2025-06-10 14:24:09
"Data Architect (Exp in Azure Databricks, Pyspark, SQL)",Adecco India,10 - 15 years,30-40 Lacs P.A.,"['Pune', 'Bengaluru']","Job Role & responsibilities:-\n\nUnderstanding operational needs by collaborating with specialized teams\nSupporting key business operations. This involves architecture designing, building and deploying data systems, pipelines etc\nDesigning and implementing agile, scalable, and cost efficiency solution on cloud data services.\nLead a team of developers, implement Sprint planning and executions to ensure timely deliveries\n\nTechnical Skill, Qualification & experience required:-\n\n9-11 years of experience in Cloud Data Engineering.\nExperience in Azure Cloud Data Engineering, Azure Databricks, datafactory , Pyspark, SQL,Python\nHands on experience in Data Engineer, Azure Databricks, Data factory, Pyspark, SQL\nProficient in Cloud Services Azure\nArchitect and implement ETL and data movement solutions.\nBachelors/Master's Degree in Computer Science or related field\nDesign and implement data solutions using medallion architecture, ensuring effective organization and flow of data through bronze, silver, and gold layers.\nOptimize data storage and processing strategies to enhance performance and data accessibility across various stages of the medallion architecture.\nCollaborate with data engineers and analysts to define data access patterns and establish efficient data pipelines.\nDevelop and oversee data flow strategies to ensure seamless data movement and transformation across different environments and stages of the data lifecycle.\nMigrate data from traditional database systems to Cloud environment\nStrong hands-on experience for working with Streaming dataset\nBuilding Complex Notebook in Databricks to achieve business Transformations.\nHands-on Expertise in Data Refinement using Pyspark and Spark SQL\nFamiliarity with building dataset using Scala.\nFamiliarity with tools such as Jira and GitHub\nExperience leading agile scrum, sprint planning and review sessions\nGood communication and interpersonal skills\n\n* Immediate Joiners will be preferred only",Industry Type: Insurance,Department: Other,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Azure data Architect', 'Data Architect', 'Azure Databricks', 'Data Architecture', 'Data Bricks', 'SQL', 'Python']",2025-06-10 14:24:11
ML Platform Specialist- 5+ Years- Gurgaon(Hybrid),Crescendo Global,5 - 10 years,Not Disclosed,['Gurugram'],"Exciting opportunity for an ML Platform Specialist to join a leading technology-driven firm. You will be designing, deploying, and maintaining scalable machine learning infrastructure with a strong focus on Databricks, model lifecycle, and MLOps practices.\n\nLocation: Gurugram (Hybrid)\n\nYour Future Employer\nOur client is a leading digital transformation partner driving innovation across industries. With a strong focus on data-driven solutions and cutting-edge technologies, they are committed to fostering a collaborative and growth-focused environment.\n\nResponsibilities\nDesigning and implementing scalable ML infrastructure on Databricks Lakehouse\nBuilding CI/CD pipelines and workflows for machine learning lifecycle\nManaging model monitoring, versioning, and registry using MLflow and Databricks\nCollaborating with cross-functional teams to optimize machine learning workflows\nDriving continuous improvement in MLOps and automation strategies\n\nRequirements\nBachelors or Masters in Computer Science, ML, Data Engineering, or related field\n3-5 years of experience in MLOps, with strong expertise in Databricks and Azure ML\nProficient in Python, PySpark, MLflow, Delta Lake, and Databricks Feature Store\nHands-on experience with cloud platforms (Azure/AWS/GCP), CI/CD, Git\nKnowledge of Terraform, Kubernetes, Azure DevOps, and distributed computing is a plus\n\nWhats in it for you\nCompetitive compensation with performance-driven growth opportunities\nWork on cutting-edge MLOps infrastructure and enterprise-scale ML solutions\nCollaborative, diverse, and innovation-driven work culture\nContinuous learning, upskilling, and career development support",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ML OPS', 'Ci/Cd', 'Machine Learning', 'Data Bricks', 'Python', 'Pyspark', 'Azure Cloud', 'GCP', 'ML flow', 'Azure Machine Learning', 'Git Version Control', 'AWS']",2025-06-10 14:24:14
Big Data Developer,Persistent,4 - 8 years,Not Disclosed,['Pune'],"About Persistent\nWe are a trusted Digital Engineering and Enterprise Modernization partner, combining deep technical expertise and industry experience to help our clients anticipate what’s next. Our offerings and proven solutions create a unique competitive advantage for our clients by giving them the power to see beyond and rise above. We work with many industry-leading organizations across the world including 12 of the 30 most innovative US companies, 80% of the largest banks in the US and India, and numerous innovators across the healthcare ecosystem.\nOur growth trajectory continues, as we reported $1,231M annual revenue (16% Y-o-Y). Along with our growth, we’ve onboarded over 4900 new employees in the past year, bringing our total employee count to over 23,500+ people located in 19 countries across the globe.",,,,"['big data administration', 'hive', 'cloudera', 'data', 'business requirements', 'scala', 'big data technologies', 'data architecture', 'coding', 'java', 'spark', 'hadoop', 'big data', 'hbase', 'programming', 'python', 'software development', 'software testing', 'performance tuning', 'engineering', 'angular', 'hdfs', 'agile', 'sdlc', 'informatica']",2025-06-10 14:24:18
Big Data Developer,Persistent,4 - 8 years,Not Disclosed,['Hyderabad'],"About Persistent\nWe are a trusted Digital Engineering and Enterprise Modernization partner, combining deep technical expertise and industry experience to help our clients anticipate what’s next. Our offerings and proven solutions create a unique competitive advantage for our clients by giving them the power to see beyond and rise above. We work with many industry-leading organizations across the world including 12 of the 30 most innovative US companies, 80% of the largest banks in the US and India, and numerous innovators across the healthcare ecosystem.\nOur growth trajectory continues, as we reported $1,231M annual revenue (16% Y-o-Y). Along with our growth, we’ve onboarded over 4900 new employees in the past year, bringing our total employee count to over 23,500+ people located in 19 countries across the globe.",,,,"['big data administration', 'cloudera', 'hive', 'data', 'business requirements', 'scala', 'big data technologies', 'data architecture', 'coding', 'java', 'spark', 'hadoop', 'big data', 'hbase', 'programming', 'python', 'software development', 'software testing', 'performance tuning', 'engineering', 'angular', 'hdfs', 'agile', 'sdlc', 'informatica']",2025-06-10 14:24:21
Snowflake Administrator,Clifyx Technology,5 - 10 years,Not Disclosed,['Bengaluru'],"ECMS #\n528403\nNumber of Openings\n1\nDuration of project\n5 years\nNo of years experience\n3-5 years\nDetailed job description - Skill Set:\nScroll below\nMandatory Skills\nSnowflake Administrator\nVendor Billing range (local currency /Day)\nUpto 8000 inr/day\nWork Location\nPune only\nHybrid/Remote/WFO\nHybrid\nBGV Pre/Post onboarding\nPost Onboarding\nAny particular shift timings\nEU and US shift\nJD : Below are the details about required skills.\nAdminister and manage Snowflake environments: Oversee user access, security, and performance tuning.\nDevelop and optimize SQL queries: Create and refine complex SQL queries for data extraction, transformation, and loading (ETL) processes.\nImplement and maintain data pipelines: Use Python and integrate them with Snowflake.\nMonitor and troubleshoot: Ensure the smooth operation of Snowflake environments, identifying and resolving issues promptly.\nCollaborate with data engineers: Work closely with data engineers to provide optimized solutions and best practices.\nReview roles hierarchy: Provide recommendations for best practices in role hierarchy and security.\nExperience: Minimum of 3 years as a Snowflake Administrator, with a total of 5+ years in database administration or data engineering.\nTechnical Skills: Proficiency in SQL, Python, and experience with performance tuning and optimization.\nCloud Services: Experience with cloud platforms such as Azure.\nData Warehousing: Strong understanding of data warehousing concepts and ETL processes.\nProblem-Solving: Excellent analytical and problem-solving skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Administration', 'Performance tuning', 'SQL queries', 'Analytical', 'Billing', 'Database administration', 'Data warehousing', 'SQL', 'Python', 'Data extraction']",2025-06-10 14:24:24
Site Reliability Engineer - Data Center storage | Aziro,Aziro,5 - 9 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","Site Reliability Engineer - Data Center Storage\n--------------------------------------------------------------------\nAs a Site Reliability Engineer - Data Center Storage, you will be responsible for:\nMust haves:\nHands-on working knowledge of the command line in Linux systems\nUnderstanding of networking, data center infrastructure, and server provisioning and\nbooting",,,,"['Linux', 'Site Reliability Engineering', 'Data Center Infrastructure', 'Devops', 'Jenkins', 'GIT', 'shell', 'Booting', 'Ansible', 'Sre', 'Puppet', 'Python']",2025-06-10 14:24:27
Quality Engineer,Corelogic,2 - 5 years,Not Disclosed,['Noida'],"In India, we operate as Next Gear India Private Limited, a fully-owned subsidiary of Cotality with offices in Kolkata, West Bengal, and Noida, Uttar Pradesh. Next Gear India Private Limited plays a vital role in Cotalitys Product Development capabilities, focusing on creating and delivering innovative solutions for the Property & Casualty (P&C) Insurance and Property Restoration industries.\nWhile Next Gear India Private Limited operates under its own registered name in India, we are seamlessly integrated into the Cotality family, sharing the same commitment to innovation, quality, and client success.\nWhen you join Next Gear India Private Limited, you become part of the global Cotality team. Together, we shape the future of property insights and analytics, contributing to a smarter and more resilient property ecosystem through cutting-edge technology and insights.\nQA Automation Engineer\nAs a QA Automation Engineer specializing in Data Warehousing, you will play a critical role in ensuring that our data solutions are of the highest quality. You will work closely with data engineers and analysts to develop, implement, and maintain automated testing frameworks for data validation, ETL processes, data quality, and integration. Your work will ensure that data is accurate, consistent, and performs optimally across our data warehouse systems.\nResponsibilities\nDevelop and Implement Automation Frameworks : Design, build, and maintain scalable test automation frameworks tailored for data warehousing environments.\nTest Strategy and Execution : Define and execute automated test strategies for ETL processes, data pipelines, and database integration across a variety of data sources.\nData Validation : Implement automated tests to validate data consistency, accuracy, completeness, and transformation logic.\nPerformance Testing : Ensure that the data warehouse systems meet performance benchmarks through automation tools and load testing strategies.\nCollaborate with Teams : Work closely with data engineers, software developers, and data analysts to understand business requirements and design tests accordingly.\nContinuous Integration : Integrate automated tests into the CI/CD pipelines, ensuring that testing is part of the deployment process.\nDefect Tracking and Reporting : Use defect-tracking tools (e.g., JIRA) to log and track issues found during automated testing, ensuring that defects are resolved in a timely manner.\nTest Data Management : Develop strategies for handling large volumes of test data while maintaining data security and privacy.\nTool and Technology Evaluation : Stay current with emerging trends in automation testing for data warehousing and recommend tools, frameworks, and best practices.\nJob Qualifications:\nRequirements and skills\nAt Least 4+ Years Experience\nSolid understanding of data warehousing concepts (ETL, OLAP, data marts, data vault,star/snowflake schemas, etc.).\nProven experience in building and maintaining automation frameworks using tools like Python, Java, or similar, with a focus on database and ETL testing.\nStrong knowledge of SQL for writing complex queries to validate data, test data pipelines, and check transformations.\nExperience with ETL tools (e.g., Matillion, Qlik Replicate) and their testing processes.\nPerformance Testing\nExperience with version control systems like Git\nStrong analytical and problem-solving skills, with the ability to troubleshoot complex data issues.\nStrong communication and collaboration skills.\nAttention to detail and a passion for delivering high-quality solutions.\nAbility to work in a fast-paced environment and manage multiple priorities.\nEnthusiastic about learning new technologies and frameworks.\nExperience with the following tools and technologies are desired.\nQLIK Replicate\nMatillion ETL\nSnowflake\nData Vault Warehouse Design\nPower BI\nAzure Cloud - Including Logic App, Azure Functions, ADF",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Data management', 'Social media', 'Performance testing', 'SMS', 'OLAP', 'JIRA', 'Analytics', 'SQL', 'Python']",2025-06-10 14:24:30
Data Analytics Engineer,Automotive Industry,5 - 6 years,6-11 Lacs P.A.,['Chennai'],"Position: Data Analytics Engineer\nExp: 5 -6 years\nNP: Immediate - 30 days\nQualification: B.tech\nLocation: Chennai Hybrid\nPrimary: Google Cloud Platform ,Python skills and Big Data pipeline\nSecondary: Big Query SQ, coding, testing, implementing, debugging workflows and apps\nKindly share your updated resume to aishwarya_s@onwardgroup.com\nKindly fill the below details\nTotal Exp:\nRelevant Exp:\nNotice Period: CTC:\nECTC:\nIf servicing NP, Last working Day, offered location & CTC:\nAvailable for Video modes interview on Weekdays (Y/N) :\nPAN Number:\nName as Per PAN Card:\nDate of Birth:\nAlternative Contact No:\nReason for Job Change:",Industry Type: Automobile,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['GCP', 'Bigquery', 'Google Cloud Platform', 'Query SQ', 'Python']",2025-06-10 14:24:32
Power Bi Developer,Mindsprint,3 - 6 years,Not Disclosed,"['Chennai', 'Bengaluru']","Job Description: An experienced and skilled BI engineer with designing, developing, and deploying business intelligence solutions using Microsoft Power BI\n\nMandatory Skills\n3+ years of experience in Power BI\nStrong knowledge in data transformation using Power Query.\nAbility to write complex DAX formula for data aggregation, filtering, ranking etc.",,,,"['Power Bi', 'Microsoft Fabric', 'Dax', 'Power Query', 'Azure Devops', 'RLS', 'SQL']",2025-06-10 14:24:35
Senior Snowflake Developer,Blend360 India,7 - 11 years,Not Disclosed,['Hyderabad'],"We are seeking a highly skilled and motivated Senior Snowflake Developer to join our growing data engineering team. In this role, you will be responsible for building scalable and secure data pipelines and Snowflake-based architectures that power data analytics across the organization. You ll collaborate with business and technical stakeholders to design robust solutions in an AWS environment and play a key role in driving our data strategy forward.\nResponsibilities\nDesign, develop, and maintain efficient and scalable Snowflake data warehouse solutions on AWS.\nBuild robust ETL/ELT pipelines using SQL, Python, and AWS services (e.g., Glue, Lambda, S3).\nCollaborate with data analysts, engineers, and business teams to gather requirements and design data models aligned with business needs.\nOptimize Snowflake performance through best practices in clustering, partitioning, caching, and query tuning.\nEnsure data quality, accuracy, and completeness across data pipelines and warehouse processes.\nMaintain documentation and enforce best practices for data architecture, governance, and security.\nContinuously evaluate tools, technologies, and processes to improve system reliability, scalability, and performance.\nEnsure compliance with relevant data privacy and security regulations (e.g., GDPR, CCPA).\n\n\nBachelor s degree in Computer Science, Information Technology, or a related field.\nMinimum 5 years of experience in data engineering, with at least 3 years of hands-on experience with Snowflake.",Industry Type: Industrial Automation,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Performance tuning', 'Data modeling', 'Schema', 'Data quality', 'data privacy', 'Information technology', 'SQL', 'Python', 'Data architecture']",2025-06-10 14:24:37
Lead Data Scientist - Media Mix Modelling (MMM),Blend360 India,7 - 10 years,Not Disclosed,['Hyderabad'],"Job Description\nWe are seeking a highly skilled Data Scientist/Analyst with expertise in Media Mix Modeling (MMM) to join our dynamic analytics team. The ideal candidate will play a critical role in providing actionable insights and optimizing marketing spend across various channels by leveraging statistical models and data-driven techniques.\nKey Responsibilities:\nDevelop and implement Media Mix Models to optimize marketing spend across different channels (e.g., TV, digital, radio, print, etc.).\nAnalyze historical data to understand the impact of marketing efforts and determine the effectiveness of different media channels.\nCollaborate with marketing and business teams to translate business objectives into quantitative analyses and actionable insights.\nBuild predictive models to forecast the impact of future marketing activities and recommend budget allocation.\nPresent and communicate complex findings in a clear, concise, and actionable manner to both technical and non-technical stakeholders.\nPerform deep-dive analyses of marketing campaigns and customer data to identify trends, opportunities, and areas for improvement.\nEnsure data integrity, accuracy, and consistency in all analyses and models.\nStay up-to-date with the latest trends and advancements in media mix modeling, marketing analytics, and data science.\nCollaborate with cross-functional teams including Data Engineering, Marketing, and Business Intelligence to ensure seamless data flow and integration.\nCreate and maintain documentation for all models, methodologies, and analysis processes.\n\n\nQualifications\nBachelor s or Master s degree in Data Science, Statistics, Economics, Mathematics, or a related field.\nProven experience (6+ years) working in Media Mix Modeling (MMM) and/or marketing analytics.",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Analytical', 'Social media', 'Consulting', 'data integrity', 'Regression analysis', 'Business intelligence', 'CRM', 'SQL', 'Python']",2025-06-10 14:24:40
Senior Software Developer / Full Stack Developer,Sadhana It Solutions,3 - 8 years,Not Disclosed,[],"We are looking for engineers with 3+ years of experience in frontend and backend technologies.\nYou will enjoy this role if you\nDesire to continuously learn, problem-solve and acquire new skills with cutting-edge technology.\nWork collaboratively as part of a close-knit team of geeks, architects and leads.\nDesired Skills Experience\n3+ years of frontend and/or backend engineering experience. Ideally 1+ years in technical leadership or software architect role.\nExperience in one or more languages/web frameworks\nHand on experience on any of frontend libraries Angular, ReactJS, React-Native, VueJS, Flutter, NextJS etc.\nBackend Javascript frameworks (React, React-Native, NodeJS, NestJS, etc.),Python frameworks (Django, Flask, Fast API etc.), Java/Spring, Golang etc.\nExperience developing RESTful APIs, GraphQL and micro-services.\nStrong database fundamentals and schema design skills. Experience with SQL NoSQL databases (PostgreSQL, MySQL, MongoDB, Redis, Cassandra, Neo4j)\nExperience with pipeline-based development and CI/CD pipeline implementation for frequent deployment using Jenkins, Github Actions, Gitlab etc.\nExperience in at least one cloud platform (Amazon Web Service or Google Cloud Platform)\nExperience with agile methodologies, software development lifecycles, design patterns.\nCommunication: You like discussing a plan upfront, welcome collaboration, and are an excellent verbal and written communicator.\nBachelors degree in Computer Science or equivalent experience.\nBonus points if you\nExperience in building multi-tier micro-service applications with exposure to caching, pub-sub, data security, authentication, messaging technologies\nHave worked in startup environments developing products from scratch (right from idea stage)\nKnowledge of container and container orchestration technologies (Docker, Kubernetes, GKE, EKS, ECS)\nExperience with data engineering, ETL and analytics (Airflow, Snowflake, Redshift, dbt)\nRoles Responsibilities\nReview requirements and lead the design, product architecture, and implementation of highly scalable cloud-based SaaS applications using modern technology stacks.\nRegularly review the teams skill set and available resources and communicate if there are any gaps or areas of opportunity.\nChampion test-driven development and participate in code reviews. Mentor and coach your team members.\nBe a steward of code quality, scalability, security, and performance.\nBuild and nurture Velotios engineering culture. Write blogs, conduct technical workshops/talks and attend conferences.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['nextjs', 'ci/cd', 'vue.js', 'redis', 'sql', 'database design', 'spring', 'react.js', 'java', 'postgresql', 'gcp', 'writing', 'jenkins', 'mysql', 'nestjs', 'graphql', 'mongodb', 'communication skills', 'front end', 'rest', 'github', 'python', 'engineering', 'angular', 'node.js', 'django', 'neo4j', 'cassandra', 'gitlab', 'aws', 'flask']",2025-06-10 14:24:43
UiPath Tester with R12 SCM Functional (Pune/Hyd/Bangalore) -3+ Yrs,Databuzz ltd,3 - 5 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Databuzz is Hiring for UiPath Tester with R12 SCM Functional (Pune/Hyd/Bangalore) -3+ Yrs -Hybrid- Immediate joiners\n\nPlease mail your profile to alekya.chebrolu@databuzzltd.com with the below details, If\nyou are Interested.\n\nAbout DatabuzzLTD:\n\nDatabuzz is One stop shop for data analytics specialized in Data Science, Big Data, Data Engineering, AI & ML, Cloud Infrastructure and Devops. We are an MNC based in both UK and INDIA. We are a ISO 27001 & GDPR complaint company.\n\nCTC -\nECTC -\nNotice Period/LWD - (Candidate serving notice period will be preferred)\n\nPosition: UiPath Tester with R12 SCM Functional (Pune/Hyd/Bangalore)\nExp -3+ yrs\n\nMandatory Skills:\nShould have experience in UiPath SCM\nShould have 3-4 years of UiPath test automation experience\nHands-on experience in building automated scripts using UiPath\nExperience leveraging opensource test automation tech stack eg selenium robot framework java, JavaScript etc\nExperience in any of the programming scripting languages like Java JS\nExperience on Testing Web Service API\nExperience building data driven tests\nShould have 2 years of Oracle Applications R12x Functional experience in SCM Mfg Modules Order Management Purchasing Inventory WIP BOM with focus on Order Management\nExperience with multiple Technologies such as SQL,PLSQL,Alerts ADI Data Loader\n\n\nRegards,\nAlekya\nTalent Acquisition Specialist\nalekya.chebrolu@databuzzltd.com",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['selenium/java/java script', 'UiPath SCM', 'Web Service API', 'UiPath test automation']",2025-06-10 14:24:45
Data Scientist,Top Rated Firm in IT Services Domain,3 - 8 years,5-7 Lacs P.A.,['Hyderabad( Nanakramguda )'],"Key Responsibilities:\nDesign and develop machine learning models and algorithms to solve business problems\nWrite clean, efficient, and reusable Python code for data processing and model deployment\nCollaborate with data engineers and product teams to integrate models into production systems\nAnalyze large datasets to derive insights, trends, and patterns\nEvaluate model performance and continuously improve through retraining and tuning\nCreate dashboards, reports, and data visualizations as needed\nMaintain documentation and ensure code quality and version control\n\nPreference\nMust have hands-on experience in building, training, and deploying AI/ML models using relevant frameworks and tools within a Linux environment.\nStrong proficiency in Python with hands-on experience in data science libraries (NumPy, Pandas, Scikit-learn, TensorFlow/PyTorch, etc.)\nExperience working with Hugging Face Transformers, spaCy, ChatGPT (OpenAI APIs), and DeepSeek LLMs for building NLP or generative AI solutions\nSolid understanding of machine learning, statistics, and data modeling\nExperience with data preprocessing, feature engineering, and model evaluation\nFamiliarity with SQL and working with structured/unstructured data\nKnowledge of APIs, data pipelines, and cloud platforms (AWS, GCP, or Azure) is a plus",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Python', 'Tensorflow', 'Pyspark', 'Artificial Intelligence', 'Natural Language Processing', 'Jupyter Notebook', 'Machine Learning', 'Scikit-Learn', 'Numpy', 'SQL', 'Pytorch', 'Pandas', 'AWS']",2025-06-10 14:24:48
Ml Engineer,Sightspectrum,5 - 10 years,1-6 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Description:\n\n\n\nStrong experience in ETL development, data modeling, and managing data in large-scale environments. - Proficient in AWS services including SageMaker, S3, Glue, Lambda, and CloudFormation/Terraform. - Hands-on expertise with MLOps best practices, including model versioning, monitoring, and CI/CD for ML pipelines.\n\n- Proficiency in Python and SQL; experience with Java is a plus for streaming jobs.\n- Deep understanding of cloud infrastructure automation using Terraform or similar IaC tools. - Excellent problem-solving skills with the ability to troubleshoot data processing and deployment issues.\n- Experience in fast-paced, agile development environments with frequent delivery cycles.\n- Strong communication and collaboration skills to work effectively across cross-functional team Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MLOps', 'Aws Sagemaker', 'Rasa', 'Serverless', 'Ml']",2025-06-10 14:24:50
Junior Fullstack Engineer,Data Core Systems,1 - 2 years,Not Disclosed,['Kolkata'],Required\n• Degree in Computer Science or equivalent practical experience\n• 1+ year of experience developing with Python and React/TypeScript\n• Experience working in a team development environment (not just personal projects)\n• Basic understanding of web development\n\nPreferred\n• Experience working in a startup environment\n• Understanding of the full product development lifecycle\n• Ability to read technical documents in English\n• Contributions to open source projects,Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['React', 'Web Development', 'AWS', 'Python', 'Rest Web Api', 'HTML']",2025-06-10 14:24:52
"Manager, BI Analytics & Governance",iCIMS Inc,7 - 12 years,Not Disclosed,['Hyderabad'],"Job Overview We are seeking a strategic and hands-on Manager of Business Intelligence (BI) and Data Governance to lead the development and execution of our enterprise-wide data strategy.\nThis role will oversee data governance frameworks, manage modern BI platforms, and ensure the integrity, availability, and usability of business-critical data.\nReporting into senior leadership, this role plays a pivotal part in shaping data-informed decision-making across functions including Finance, Revenue Operations, Product, and more.\nThe ideal candidate is a technically proficient and people-oriented leader with a deep understanding of data governance, cloud data architecture, and SaaS KPIs.\nThey will drive stakeholder engagement, enablement, and adoption of data tools and insights, with a focus on building scalable, trusted, and observable data systems.\nAbout Us When you join iCIMS, you join the team helping global companies transform business and the world through the power of talent.\nOur customers do amazing things: design rocket ships, create vaccines, deliver consumer goods globally, overnight, with a smile.\nAs the Talent Cloud company, we empower these organizations to attract, engage, hire, and advance the right talent.\nWe re passionate about helping companies build a diverse, winning workforce and about building our home team.\nWere dedicated to fostering an inclusive, purpose-driven, and innovative work environment where everyone belongs.\nResponsibilities Data Governance Leadership: Establish and maintain a comprehensive data governance framework that includes data quality standards, ownership models, data stewardship processes, and compliance alignment with regulations such as GDPR and SOC 2.\nEnterprise Data Architecture: Oversee data orchestration across Salesforce (SFDC), cloud-based data warehouses (e.g., Databricks, Snowflake, or equivalent), and internal systems.\nCross collaborate with data engineering team for the development and optimization of ETL pipelines to ensure data reliability and performance at scale.\nTeam Management & Enablement: Lead and mentor a team of BI analysts, and governance specialists.\nFoster a culture of collaboration, continuous learning, and stakeholder enablement to increase data adoption across the organization.\nBI Strategy & Tools Management: Own the BI toolset (with a strong emphasis on Tableau), and define standards for scalable dashboard design, self-service reporting, and analytics enablement.\nEvaluate and incorporate additional platforms (e.g., Power BI, Looker) as needed.\nStakeholder Engagement & Strategic Alignment: Partner with leaders in Finance, RevOps, Product, and other departments to align reporting and data strategy with business objectives.\nTranslate business needs into scalable reporting solutions and drive enterprise-wide adoption through clear communication and training.\nData Quality & Observability: Implement data quality monitoring, lineage tracking, and observability tools to proactively detect issues and ensure data reliability and trustworthiness.\nDocumentation & Transparency: Create and maintain robust documentation for data processes, pipeline architecture, code repositories (via GitHub), and business definitions to support transparency and auditability for technical and non-technical users.\nExecutive-Level Reporting & Insight: Design and maintain strategic dashboards that surface key SaaS performance indicators to senior leadership and the board.\nDeliver actionable insights to support company-wide strategic decisions.\nContinuous Improvement & Innovation: Stay current with trends in data governance, BI technologies, and AI.\nProactively recommend and implement enhancements to tools, processes, and governance maturity.\nQualifications Data Governance Expertise: Proven experience implementing data governance frameworks, compliance standards, and ownership models across cross-functional teams.\nSQL Expertise: Advanced SQL skills with a strong background in ETL/data pipeline development across systems like Salesforce and enterprise data warehouses.\nBI Tools Mastery: Expertise in Tableau for developing reports and dashboards.\nExperience driving adoption of BI best practices across a diverse user base.\nSalesforce Data Proficiency: Deep understanding of SFDC data structure, reporting, and integration with downstream systems.\nVersion Control & Documentation: Hands-on experience with GitHub and best practices in code versioning and documentation of data pipelines.\nLeadership & Stakeholder Communication: 3+ years of people management experience with a track record of team development and stakeholder engagement.\nAnalytics Experience: 8+ years of experience in analytics roles, working with large datasets to derive insights and support executive-level decision-making.\nProgramming Knowledge: Proficiency in Python for automation, data manipulation, and integration tasks.\nSaaS Environment Acumen: Deep understanding of SaaS metrics, business models, and executive reporting needs.\nCross-functional Collaboration: Demonstrated success in partnering with teams like Finance, Product, and RevOps to meet enterprise reporting and insight goals.\nEEO Statement iCIMS is a place where everyone belongs.\nWe celebrate diversity and are committed to creating an inclusive environment for all employees.\nOur approach helps us to build a winning team that represents a variety of backgrounds, perspectives, and abilities.\nSo, regardless of how your diversity expresses itself, you can find a home here at iCIMS.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'Team management', 'Version control', 'SOC', 'Data quality', 'Business intelligence', 'Analytics', 'SQL', 'Python', 'Salesforce']",2025-06-10 14:24:55
Software Engineer,Sadhana It Solutions,2 - 4 years,Not Disclosed,[],"Quickly learn the latest technologies across Web/Mobile Apps, Cloud/DevOps, Data science, and Data engineering.\nWork with some awesome mentors and peers to become an expert in your area of work.\nLearn the best engineering practices.\nWork on exciting problems startups and enterprises and fast track your career.\nRoles Responsibilities:\nBuild highly scalable, high performance, responsive web applications.\nBe responsible for building, shipping, and maintaining core product features, end to end.\nHelp out in building the back-end front-end infrastructure.\nTranslation of requirements, designs, and wireframes into high quality code.\nCollaborate closely with designers, engineers, founders, and product managers.\nDesired Skills Experience:\nProjects/Internships with coding experience in either of JavaScript, Python, Golang, Java etc.\nWeb development experience with Angular, React/Node.js or similar frameworks will be a bonus.\nBasic understanding of Computer Science fundamentals Databases, Web Application Architecture etc.\nGood understanding of data structures, algorithms etc.\nBonus points if you have contributed to open source projects, participated in competitive coding platforms like HackerEarth, Codeforces, SPOJ, etc.\nBonus points if you have experience/knowledge with cloud service providers like AWS, GCP or Azure and/or exposure to DevOps.\nCommunication:\nYou like discussing a plan upfront, welcome collaboration, and are an excellent verbal and written communicator.\nQualification:\n  MCA/B.E/B.Tech/M.Tech or equivalent experience.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['algorithms', 'python', 'golang', 'microsoft azure', 'engineering', 'javascript', 'angular', 'react.js', 'node', 'coding', 'node.js', 'open source', 'java', 'gcp', 'devops', 'writing', 'data structures', 'web development', 'software engineering', 'aws', 'communication skills', 'architecture']",2025-06-10 14:24:58
Job opportunity For MLOPS,Sightspectrum,5 - 10 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Greetings from Sight Spectrum Technologies!!!\n\n\nWe would like to ensure that you are interested in this position.\n\nCompany: Sight Spectrum Technologies(https://sightspectrum.com/)\n\nExperience :  5+Years \nLocation: Chennai, Bangalore, hyderabad, Coimbatore\nDescription:\n\n\nStrong experience in ETL development, data modeling, and managing data in large-scale environments.\n- Proficient in AWS services including SageMaker, S3, Glue, Lambda, and CloudFormation/Terraform. - Hands-on expertise with MLOps best practices, including model versioning, monitoring, and CI/CD for ML pipelines.\n- Proficiency in Python and SQL; experience with Java is a plus for streaming jobs.\n- Deep understanding of cloud infrastructure automation using Terraform or similar IaC tools.\n- Excellent problem-solving skills with the ability to troubleshoot data processing and deployment issues.\n- Experience in fast-paced, agile development environments with frequent delivery cycles.- Strong communication and collaboration skills to work effectively across cross-functional teams\n\nPlease fill the below details for reference.\nTotal Experience:\nRelevant Experience:\nCurrent CTC:\nExpected CTC:\nNotice Period (LWD):\nCurrent Location:\nPreferred Location:\nPayroll Company:\nReason for change:\nClient Company:\nOffer Details:\nPF(Yes/No):\nUAN No:\nLinkdln Id:\n\n\nIf interested kindly share your resume to roopavahini@sightspectrum.in",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MLOPS', 'Aws Sagemaker', 'ETL', 'AWS']",2025-06-10 14:25:00
Technical Recruiter,Rstar Technologies,5 - 8 years,Not Disclosed,[],"Role Overview\nWe are looking for a passionate and self-driven Technical Recruiter to join our talent acquisition team. You will play a crucial role in identifying, attracting, and hiring top technical talent across a wide range of roles such as Salesforce Developers, Full Stack Engineers, Cloud Architects, DevOps Engineers, Data Engineers, and AI/ML specialists, Solution Architects.\n\nThe ideal candidate is someone who thrives in a fast-paced environment, understands complex technical requirements, and can deliver high-quality candidates within set timelines.\n\nKey Responsibilities\nPartner with hiring managers to understand job roles and craft effective hiring strategies\nSource candidates using advanced search methods (Boolean, LinkedIn, GitHub, Naukri, Hirist, etc.)\nScreen and interview candidates for technical roles to evaluate technical and cultural fit\nManage the complete recruitment life cyclefrom sourcing to offer negotiation and onboarding\nBuild a strong pipeline for roles in Salesforce (Admin/Developer/Architect), Full Stack Development (Java, .NET, Node.js, React, Angular), Cloud (AWS/Azure/GCP), and emerging tech (AI/ML, Data Science, etc.)\nKeep up with market trends and salary benchmarks for technical roles\nTrack and report recruitment metrics such as TAT, quality of hire, and source effectiveness\nRequired Skills & Experience\n5-8 years of experience as a Technical Recruiter (in-house or agency)\nDeep understanding of technical stacks:\nSalesforce (Admin, Developer, CPQ, Marketing Cloud, Service Cloud, etc.)\nFull Stack Developers (Java, Python, Node.js, React, Angular, .NET)\nCloud Experts (AWS, Azure, GCP)\nDevOps, SRE, Data Engineers, AI/ML Specialists\nProficiency with ATS platforms and sourcing tools\nStrong communication and negotiation skills",Industry Type: IT Services & Consulting,Department: Human Resources,"Employment Type: Full Time, Permanent","['Technical Recruitment', 'Technical Hiring', 'IT Recruitment', 'recruitment', 'End To End Recruitment', 'Leadership Hiring', 'Corporate Recruitment']",2025-06-10 14:25:02
Chennai Career Event- Applications invited For Solution Analyst,ExxonMobil,3 - 6 years,Not Disclosed,['Bengaluru'],"Job Title: Chennai Career Event- Applications invited for Solution Analyst\n\nAbout us\nAt ExxonMobil, our vision is to lead in energy innovations that advance modern living and a net-zero future. As one of the worlds largest publicly traded energy and chemical companies, we are powered by a unique and diverse workforce fueled by the pride in what we do and what we stand for.\nThe success of our Upstream, Product Solutions and Low Carbon Solutions businesses is the result of the talent, curiosity and drive of our people. They bring solutions every day to optimize our strategy in energy, chemicals, lubricants and lower-emissions technologies.",,,,"['Power Bi', 'Snowflake', 'Tableau', 'S4 Hana', 'procurement process']",2025-06-10 14:25:05
Data Tester,Creditsafe,3 - 6 years,Not Disclosed,['Hyderabad'],"Role\nWe are looking for a Test Engineer who will become part of our team building and testing the Creditsafe data. You will be working closely with the database teams and data engineering to build specific systems facilitating the extraction and\ntransformation of Creditsafe data. Based on the test strategy and approach you will develop, enhance and execute tests\nthat add value to Creditsafe data. You will act as a primary source of guidance to Junior Test Engineers and Test\nEngineers in all areas of data quality. You will contribute to the team using data quality best practices and techniques.\nYou can confidently communicate test results with your team members and stakeholders using evidence and reports. You\nact as a mentor and coach to the less experienced members of the test team. You will promote and coach leading practices in data test management, design, and implementation. You will be part of an Agile team and will effectively contribute to the ceremonies, acting as the quality specialist within that team. You are an influencer and will provide leadership in defining and implementing agreed standards and will actively promote this within your team and the wider\ndevelopment community.\nThe ideal candidate has extensive experience in mentorship and leading by example and is able to communicate values consistentwith the Creditsafe philosophy of engagement. You have critical thinking skills and can diplomatically communicate within, and outside their areas of responsibility, challenging assumptions where required. Required Skills\nProven working experience as a data test engineer or business data analyst or ETL tester.\nTechnical expertise regarding data models, database design development, data mining and segmentation\ntechniques\nStrong knowledge of and experience with SQL databases\nHands on experience of best engineering practices (handling and logging errors, system monitoring and building\nhuman-fault-tolerant applications)\nKnowledge of statistics and experience using statistical packages for analysing datasets (Excel, SPSS, SAS etc.) is an advantage.\nComfortable working with relational databases such as Redshift, Oracle, PostgreSQL, MySQL, and MariaDB (PostgreSQL preferred)\nRequired Skills\nProven working experience as a data test engineer or business data analyst or ETL tester.\nTechnical expertise regarding data models, database design development, data mining and segmentationtechniques\nStrong knowledge of and experience with SQL databases\nHands on experience of best engineering practices (handling and logging errors, system monitoring and buildinghuman-fault-tolerant applications)\nKnowledge of statistics and experience using statistical packages for analysing datasets (Excel, SPSS, SAS etc.) isan advantage.\nComfortable working with relational databases such as Redshift, Oracle, PostgreSQL, MySQL, and MariaDB(PostgreSQL preferred)Required Skills\nProven working experience as a data test engineer or business data analyst or ETL tester.\nTechnical expertise regarding data models, database design development, data mining and segmentationtechniques\nStrong knowledge of and experience with SQL databases\nHands on experience of best engineering practices (handling and logging errors, system monitoring and buildinghuman-fault-tolerant applications)\nKnowledge of statistics and experience using statistical packages for analysing datasets (Excel, SPSS, SAS etc.) isan advantage.\nComfortable working with relational databases such as Redshift, Oracle, PostgreSQL, MySQL, and MariaDB(PostgreSQL preferred)Strong analytical skills with the ability to collect, organise, analyse, and disseminate significant amounts ofinformation with attention to detail and accuracy\nAdept at queries, report writing and presenting findings.\nBS in Mathematics, Economics, Computer Science, Information Management or Statistics is desirable but notessential\nA good understanding of cloud technology, preferably AWS and/or Azure DevOps\nA practical understanding of programming JavaScript, Python\nExcellent communication skills\nPractical experience of testing in an Agile approachDesirable Skills\nAn understanding of version control systems\nPractical experience of conducting code reviews\nPractical experience of pair testing and pair programmingStrong analytical skills with the ability to collect, organise, analyse, and disseminate significant amounts ofinformation with attention to detail and accuracy\nAdept at queries, report writing and presenting findings.\nBS in Mathematics, Economics, Computer Science, Information Management or Statistics is desirable but notessential\nA good understanding of cloud technology, preferably AWS and/or Azure DevOps\nA practical understanding of programming JavaScript, Python\nExcellent communication skills\nPractical experience of testing in an Agile approachDesirable Skills\nAn understanding of version control systems\nPractical experience of conducting code reviews\nPractical experience of pair testing and pair programming",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['JavaScript', 'PostgreSQL', 'MySQL', 'MariaDB', 'Oracle', 'Redshift', 'Python']",2025-06-10 14:25:07
AI Solution Architect,L&T Technology Services (LTTS),6 - 11 years,Not Disclosed,['Mumbai'],"Job Summary:\n\nWe are seeking an experienced AI Solution Architect to lead the design and implementation of scalable AI/ML systems and solutions. The ideal candidate will bridge the gap between business needs and technical execution, translating complex problems into AI-powered solutions that drive strategic value.\n\n\nKey Responsibilities:\nSolution Design & Architecture\nDesign end-to-end AI/ML architectures, integrating with existing enterprise systems and cloud infrastructure.\nLead technical planning, proof-of-concepts (POCs), and solution prototyping for AI use cases.\nAI/ML System Development\nCollaborate with data scientists, engineers, and stakeholders to define solution requirements.\nGuide the selection and use of machine learning frameworks (e.g., TensorFlow, PyTorch, Hugging Face).\nEvaluate and incorporate LLMs, computer vision, NLP, and generative AI models as appropriate.\nTechnical Leadership\nProvide architectural oversight and code reviews to ensure high-quality delivery.\nEstablish best practices for AI model lifecycle management (training, deployment, monitoring).\nAdvocate for ethical and responsible AI practices including bias mitigation and model transparency.\nStakeholder Engagement\nPartner with business units to understand goals and align AI solutions accordingly.\nCommunicate complex AI concepts to non-technical audiences and influence executive stakeholders.\nInnovation & Strategy\nStay updated on emerging AI technologies, trends, and industry standards.\nDrive innovation initiatives to create competitive advantage through AI.\n\n\nRequired Qualifications:\nBachelor s or Master s in Computer Science, Engineering, Data Science, or related field.\n6 years of experience in software development or data engineering roles.\n3+ years of experience designing and delivering AI/ML solutions.\nProficiency in cloud platforms (AWS, Azure, GCP) and container orchestration (Kubernetes, Docker).\nHands-on experience with MLOps tools (e.g., MLflow, Kubeflow, SageMaker, Vertex AI).\nStrong programming skills in Python and familiarity with CI/CD pipelines.\n\n\nPreferred Qualifications:\nExperience with generative AI, foundation models, or large language models (LLMs).\nKnowledge of data privacy regulations (e.g., GDPR, HIPAA) and AI compliance frameworks.\nCertifications in cloud architecture or AI (e.g., AWS Certified Machine Learning, Google Professional ML Engineer).\nStrong analytical, communication, and project management skills.",Industry Type: Electronic Components / Semiconductors,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Architecture', 'Project management', 'Analytical', 'Machine learning', 'Cloud', 'Technical leadership', 'AWS', 'Monitoring', 'Solution Architect', 'Python']",2025-06-10 14:25:10
"SDE II - NodeJS, Newton",Affle,3 - 6 years,Not Disclosed,['Gurugram'],"Were looking for a skilled Node.js Developer with a strong foundation in data engineering to join our engineering team. Youll be responsible for building scalable backend systems using modern Node.js frameworks and tools, while also designing and maintaining robust data pipelines and integrations.\nPrimary Responsibilities:\nBuild and maintain performant APIs and backend services using Node.js and frameworks like Express.js, NestJS, or Fastify.\nDevelop and manage ETL/ELT pipelines, data models, schemas, and data transformation logic for analytics and operational use.\nEnsure data quality, integrity, and consistency through validation, monitoring, and logging.\nWork with database technologies (MySQL, PostgreSQL, MongoDB, Redis) to store and manage application and analytical data.\nImplement integrations with third-party APIs and internal microservices.\nUse ORMs like Sequelize, TypeORM, or Prisma for data modeling and interaction.\nWrite unit, integration, and E2E tests using frameworks such as Jest, Mocha, or Supertest.\nCollaborate with frontend, DevOps, and data engineering teams to ship end-to-end features.\nMonitor and optimize system performance, logging (e.g., Winston, Pino), and error handling.\nContribute to CI/CD workflows and infrastructure automation using tools like PM2, Docker and Jenkins.\nRequired Skills:\n3+ years of experience in backend development using Node.js.\nHands-on experience with Express.js, NestJS, or other Node.js frameworks.\nUnderstanding of data modelling, partitioning, indexing, and query optimization.\nExperience in building and maintaining data pipelines, preferably using custom Node.js scripts.\nFamiliarity with stream processing and messaging systems (e.g., Kafka, RabbitMQ, or Redis Streams).\nSolid understanding of SQL and NoSQL data stores and schema design.\nStrong knowledge of JavaScript and preferably TypeScript.\nFamiliarity with cloud platforms (AWS/GCP/Azure) and services like S3, Lambda, or Cloud Functions.\nExperience with containerized environments (Docker) and CI/CD.\nExperience with data warehouses (e.g., BigQuery, Snowflake, Redshift).\nNice To Have:\nCloud Certification in AWS or GCP.\nExperience with distributed processing tools (eg. Spark, Trino/Presto)\nExperience with Data Transformation tool (ex. DBT, SQLMesh) and Data Orchestration (ex. Apache Airflow, Kestra etc)\nFamiliarity with Serverless architectures and tools like Vercel/Netlify for deployment",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'mocha', 'ci/cd', 'redis', 'sql', 'docker', 'database design', 'data modeling', 'postgresql', 'jenkins', 'mysql', 'nestjs', 'bigquery', 'etl', 'mongodb', 'cd', 'snowflake', 'microsoft azure', 'backend development', 'data engineering', 'jest', 'javascript', 'node.js', 'nosql data stores', 'pm']",2025-06-10 14:25:13
Pyspark Developer,perminent job,6 - 11 years,Not Disclosed,"['Chennai', 'Bengaluru']","• Experience with big data technologies (Hadoop, Spark, Hive)\n• Proven experience as a development data engineer or similar role, with ETL\nbackground.\n• Experience with data integration / ETL best practices and data quality principles.\n• Play a crucial role in ensuring the quality and reliability of the data by designing,\nimplementing, and executing comprehensive testing.\n• By going over the User Stories build the comprehensive code base and business rules\nfor testing and validation of the data.\n• Knowledge of continuous integration and continuous deployment (CI/CD) pipelines.\n• Familiarity with Agile/Scrum development methodologies.\n• Excellent analytical and problem-solving skills.\n• Strong communication and collaboration skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Spark', 'Python', 'SQL', 'Hive', 'Hadoop', 'Unix Shell Scripting']",2025-06-10 14:25:15
AI/ML Engineer,Leading Client,5 - 8 years,Not Disclosed,['Chennai'],"Experience in CI/CD pipelines, scripting languages, and a deep understanding of version control systems (e.g. Git), containerization (e.g. Docker), and continuous integration/deployment tools (e.g. Jenkins) third party integration is a plus, cloud computing platforms (e.g. AWS, GCP, Azure), Kubernetes and Kafka.\n\nExperience in 4+ years of experience building production-grade ML pipelines.\n\nProficient in Python and frameworks like Tensorflow, Keras, or PyTorch.\n\nExperience with cloud build, deployment, and orchestration tools\n\nExperience with MLOps tools such as MLFlow, Kubeflow, Weights & Biases, AWS Sagemaker, Vertex AI, DVC, Airflow, Prefect, etc.,\n\nExperience in statistical modeling, machine learning, data mining, and unstructured data analytics.\n\nUnderstanding of ML Lifecycle, MLOps & Hands on experience to Productionize the ML Model\n\nDetail-oriented, with the ability to work both independently and collaboratively.\n\nAbility to work successfully with multi-functional teams, principals, and architects, across organizational boundaries and geographies.\n\nEqual comfort driving low-level technical implementation and high-level architecture evolution\n\nExperience working with data engineering pipelines.",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['AI/ML Engineering', 'continuous integration', 'kubernetes', 'python', 'vertex', 'production', 'data mining', 'ci/cd', 'aws sagemaker', 'microsoft azure', 'machine learning', 'artificial intelligence', 'docker', 'tensorflow', 'git', 'gcp', 'kafka', 'pytorch', 'jenkins', 'keras', 'statistical modeling', 'aws', 'ml']",2025-06-10 14:25:18
Solution Architect,Horizon Therapeutics,6 - 12 years,Not Disclosed,['Hyderabad'],"Career Category Information Systems Job Description\nABOUT AMGEN\nAmgen harnesses the best of biology and technology to fight the world s toughest diseases, and make people s lives easier, fuller and longer. We discover, develop, manufacture and deliver innovative medicines to help millions of patients. Amgen helped establish the biotechnology industry more than 40 years ago and remains on the cutting-edge of innovation, using technology and human genetic data to push beyond what s known today.\nABOUT THE ROLE\nRole Description:\nWe are seeking a seasoned Solution Architect to drive the architecture, development and implementation of data solutions to Amgen functional groups. The ideal candidate able to work in large scale Data Analytic initiatives, engage and work along with Business, Program Management, Data Engineering and Analytic Engineering teams. Be champions of enterprise data analytic strategy, data architecture blueprints and architectural guidelines. As a Solution Architect, you will play a crucial role in designing, building, and optimizing data solutions to Amgen functional groups such as RD, Operations and GCO.\nRoles Responsibilities:\nImplement and manage large scale data analytic solutions to Amgen functional groups that align with the Amgen Data strategy\nCollaborate with Business, Program Management, Data Engineering and Analytic Engineering teams to deliver data solutions\nResponsible for design, develop, optimize, delivery and support of Data solutions on AWS and Databricks architecture\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nProvide expert guidance and mentorship to the team members, fostering a culture of innovation and best practices.\nBe passionate and hands-on to quickly experiment with new data related technologies\nDefine guidelines, standards, strategies, security policies and change management policies to support the Enterprise Data platform.\nCollaborate and align with EARB, Cloud Infrastructure, Security and other technology leaders on Enterprise Data Architecture changes\nWork with different project and application groups to drive growth of the Enterprise Data Platform using effective written/verbal communication skills, and lead demos at different roadmap sessions\nOverall management of the Enterprise Data Platform on AWS environment to ensure that the service delivery is cost effective and business SLAs around uptime, performance and capacity are met\nEnsure scalability, reliability, and performance of data platforms by implementing best practices for architecture, cloud resource optimization, and system tuning.\nCollaboration with RunOps engineers to continuously increase our ability to push changes into production with as little manual overhead and as much speed as possible.\nMaintain knowledge of market trends and developments in data integration, data management and analytics software/tools\nWork as part of team in a SAFe Agile/Scrum model\nBasic Qualifications and Experience:\nMaster s degree with 6 - 8 years of experience in Computer Science, IT or related field OR\nBachelor s degree with 9 - 12 years of experience in Computer Science, IT or related field OR\nFunctional Skills:\nMust-Have Skills:\n7+ years of hands-on experience in Data integrations, Data Management and BI technology stack.\nStrong experience with one or more Data Management tools such as AWS data lake, Snowflake or Azure Data Fabric\nExpert-level proficiency with Databricks and experience in optimizing data pipelines and workflows in Databricks environments.\nStrong experience with Python, PySpark, and SQL for building scalable data workflows and pipelines.\nExperience with Apache Spark, Delta Lake, and other relevant technologies for large-scale data processing.\nFamiliarity with BI tools including Tableau and PowerBI\nDemonstrated ability to enhance cost-efficiency, scalability, and performance for data solutions\nStrong analytical and problem-solving skills to address complex data solutions\nGood-to-Have Skills:\nPreferred to have experience in life science or tech or consultative solution architecture roles\nExperience working with agile development methodologies such as Scaled Agile.\nProfessional Certifications\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.\nEQUAL OPPORTUNITY STATEMENT\nAmgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.\nWe will ensure that individuals with disabilities are provided with reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request an accommodation.\n.",Industry Type: Biotechnology,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Solution architecture', 'Change management', 'Data management', 'Analytical', 'Troubleshooting', 'infrastructure security', 'SQL', 'Python', 'Data architecture']",2025-06-10 14:25:20
Data Engineer_Low_T&M_Delivery,Ducen,8 - 13 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Enter job post details\nThis, is for Noida Location, work from office 5 days from . Shift is 12 noon to 9 pm\nQualification\n8+ years of DBA experience, generally with Database technologies.\nAt least 5 years experience in PostgreSQL and good understanding of change control management in cloud database services.\nExperience with automation platform tools like Ansible, Github Actions and experience in any of the Public Cloud solutions\nProficiency in writing automation procedures, functions and packages for administration and application support.\nShould have strong Linux platform skills and understanding of network, storage, tiered application environments and security.\nCloud deployment experience would be a big plus.\nExperience with multi-tenant database design would be a plus\nJob Description\nDatabase Planning & Monitoring: Monitor, plan, and coordinate secure data solutions in alignment with data requirements; Design and build data models and schemas to support application requirements; Create and maintain databases, tables, views, stored procedures, and other database objects; Create database metric alerts; Plan back-ups, test recovery procedures, and clone database.\nDatabase Performance & Optimization: Performance-tune databases, manage storage space, and administer database instances; Create and manage keys and indexes; Work closely with the application development teams to resolve any performance-related issues and provide application support.\nDocumentation & Process: Produce and maintain documentation on database structure, procedures, and data standards.\nCandidate Privacy Policy\nOrion Systems Integrators, LLC and its subsidiaries and its affiliates (collectively, Orion, we or us ) are committed to protecting your privacy. This (orioninc.com) ( Notice ) explains:\nWhat information we collect during our application and recruitment process and why we collect it;\nHow we handle that information; and\nHow to access and update that information.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Application support', 'Linux', 'Database design', 'Postgresql', 'Application development', 'Life sciences', 'Stored procedures', 'Automotive', 'Financial services']",2025-06-10 14:25:22
Autosys Administrator,E2techspace,5 - 10 years,Not Disclosed,[],"Immediate Joiners only. This is a 100% remote role but strict background checks will be conducted on candidates to prevent any possibility of moonlighting. The client is based out of New York so candidate will be expected to work in EST hours.\n\nAbout the Company:\n\nOur client is a leading hedge fund with a global footprint, managing multi-billion-dollar portfolios across a diverse range of asset classes. Technology is a cornerstone of the funds investment strategy, operational infrastructure, and risk management framework. The firm is seeking an experienced Autosys Specialist to manage and optimize enterprise-level batch job scheduling critical to front-, middle-, and back-office operations.\n\nPosition Overview:\n\nAs an Autosys Specialist, you will be responsible for the design, implementation, monitoring, and maintenance of the Autosys job scheduling environment. You will work closely with infrastructure, DevOps, application support, and business stakeholders to ensure the timely and reliable execution of jobs that support trading, risk, reporting, and operational workflows.\n\nKey Responsibilities:\n\nAdminister and support the Autosys Workload Automation environment.\nDevelop and maintain job schedules, calendars, dependencies, and event-driven workflows.\nMonitor job performance and proactively resolve job failures or delays with minimal disruption to critical processes.\nWork with developers, data engineers, and business analysts to onboard and optimize batch jobs.\nProvide incident management and root cause analysis (RCA) for failed or delayed jobs.\nPerform patching, version upgrades, and configuration management of the Autosys environment.\nMaintain compliance with change management and audit processes.\nDevelop operational documentation and automate repetitive tasks using scripting languages (Shell, Python, Perl, etc.).\nImplement job control and alerting integration with monitoring systems (e.g., Splunk, AppDynamics, or similar).\nSupport DR (Disaster Recovery) testing and high availability configuration of Autosys infrastructure.\n\nRequired Qualifications:\n\n5+ years of experience as an Autosys Administrator/Scheduler in an enterprise environment.\nStrong understanding of job scheduling best practices, dependencies, and SLA management.\nExperience with UNIX/Linux environments, scripting (Shell, Bash, Python), and SQL.\nSolid knowledge of event-based and time-based scheduling models.\nExperience with job onboarding automation and REST APIs related to Autosys (e.g., WCC APIs).\nStrong problem-solving and debugging skills.\n\nPreferred Qualifications:\n\nExperience with cloud environments (AWS, Azure) and hybrid scheduling setups.\nExperience in financial services, preferably within hedge funds, investment banking, or asset management.\nFamiliarity with front- and middle-office workflows (e.g., trading, risk, compliance reporting).\nKnowledge of other schedulers (Control-M, UC4, Tidal) is a plus.\nExposure to CI/CD tools like Jenkins, GitLab, or similar pipelines.\nKnowledge of monitoring/logging tools (e.g., Splunk, Grafana).\nITIL Foundation certification or similar operational frameworks.",Industry Type: Financial Services (Asset Management),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AutoSys', 'Autosys Scheduler', 'Autosys R5']",2025-06-10 14:25:24
MDM Data Platform Engineer,TechAffinity,5 - 8 years,Not Disclosed,['Chennai'],"Key Responsibilities:\nTechnical Skills:\nStrong proficiency in SQL for data manipulation and querying.\nKnowledge of Python scripting for data processing and automation.\nExperience in Reltio Integration Hub (RIH) and handling API-based integrations.\nFamiliarity with Data Modelling Matching, Survivorship concepts and methodologies.\nExperience with D&B, ZoomInfo, and Salesforce connectors for data enrichment.\nUnderstanding of MDM workflow configurations and role-based data governance\nExperience with AWS Databricks, Data Lake and Warehouse\nImplement and configure MDM solutions using Reltio while ensuring alignment with business requirements and best practices.\nDevelop and maintain data models, workflows, and business rules within the MDM platform.\nWork on Reltio Workflow (DCR Workflow & Custom Workflow) to manage data approvals and role-based assignments.\nSupport data integration efforts using Reltio Integration Hub (RIH) to facilitate data movement across multiple systems.\nDevelop ETL pipelines using SQL, Python, and integration tools to extract, transform, and load (ETL) data.\nWork with D&B, ZoomInfo, and Salesforce connectors for data enrichment and integration.\nPerform data analysis and profiling to identify data quality issues and recommend solutions for data cleansing and enrichment.\nCollaborate with stakeholders to define and document data governance policies, procedures, and standards.\nOptimize MDM workflows to enhance data stewardship and governance.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Master Data Management', 'Data Lake', 'Aws Databricks', 'SQL', 'Python', 'Reltio Integration Hub', 'MDM', 'Data Warehousing', 'Data Governance', 'Data Modelling Matching']",2025-06-10 14:25:26
Data & AI/ML Architect,"Milestone Technologies, Inc",14 - 19 years,Not Disclosed,['Kochi'],"Job Summary:\nWe are seeking a highly experienced and visionary Databricks Data Architect with over 14 years in data engineering and architecture, including deep hands-on experience in designing and scaling Lakehouse architectures using Databricks . The ideal candidate will possess deep expertise across data modeling, data governance, real-time and batch processing, and cloud-native analytics using the Databricks platform. You will lead the strategy, design, and implementation of modern data architecture to drive enterprise-wide data initiatives and maximize the value from the Databricks platform.\nKey Responsibilities:\nLead the architecture, design, and implementation of scalable and secure Lakehouse solutions using Databricks and Delta Lake .\nDefine and implement data modeling best practices , including medallion architecture (bronze/silver/gold layers).\nChampion data quality and governance frameworks leveraging Databricks Unity Catalog for metadata, lineage, access control, and auditing.\nArchitect real-time and batch data ingestion pipelines using Apache Spark Structured Streaming , Auto Loader , and Delta Live Tables (DLT) .\nDevelop reusable templates, workflows, and libraries for data ingestion, transformation, and consumption across various domains.\nCollaborate with enterprise data governance and security teams to ensure compliance with regulatory and organizational data standards.\nPromote self-service analytics and data democratization by enabling business users through Databricks SQL and Power BI/Tableau integrations .\nPartner with Data Scientists and ML Engineers to enable ML workflows using MLflow , Feature Store , and Databricks Model Serving .\nProvide architectural leadership for enterprise data platforms, including performance optimization , cost governance , and CI/CD automation in Databricks.\nDefine and drive the adoption of DevOps/MLOps best practices on Databricks using Databricks Repos , Git , Jobs , and Terraform .\nMentor and lead engineering teams on modern data platform practices, Spark performance tuning , and efficient Delta Lake optimizations (Z-ordering, OPTIMIZE, VACUUM, etc.) .\nTechnical Skills:\n10+ years in Data Warehousing, Data Architecture, and Enterprise ETL design .\n5+ years hands-on experience with Databricks on Azure/AWS/GCP , including advanced Apache Spark and Delta Lake .\nStrong command of SQL, PySpark, and Spark SQL for large-scale data transformation.\nProficiency with Databricks Unity Catalog , Delta Live Tables , Autoloader , DBFS , Jobs , and Workflows .\nHands-on experience with Databricks SQL and integration with BI tools (Power BI, Tableau, etc.).\nExperience implementing CI/CD on Databricks , using tools like Git , Azure DevOps , Terraform , and Databricks Repos .\nProficient with streaming architecture using Spark Structured Streaming , Kafka , or Event Hubs/Kinesis .\nUnderstanding of ML lifecycle management with MLflow , and experience in deploying MLOps solutions on Databricks.\nFamiliarity with cloud object stores (e.g., AWS S3, Azure Data Lake Gen2) and data lake architectures .\nExposure to data cataloging and metadata management using Unity Catalog or third-party tools.\nKnowledge of orchestration tools like Airflow , Databricks Workflows , or Azure Data Factory .\nExperience with Docker/Kubernetes for containerization (optional, for cross-platform knowledge).\nPreferred Certifications (a plus):\nDatabricks Certified Data Engineer Associate/Professional\nDatabricks Certified Lakehouse Architect\nMicrosoft Certified: Azure Data Engineer / Azure Solutions Architect\nAWS Certified Data Analytics - Specialty\nGoogle Professional Data Engineer",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Automation', 'metadata', 'GIT', 'Architecture', 'Data modeling', 'Data quality', 'microsoft', 'SQL', 'Data architecture']",2025-06-10 14:25:29
Senior Workato Developer,Foundever,5 - 9 years,9.5-19.5 Lacs P.A.,['Chennai'],"About Us: At Foundever, we are dedicated to harnessing the power of data to drive business transformation and deliver exceptional results for our clients. We are seeking a detail-oriented and analytical Data Engineer to join our team. The ideal candidate will have strong ETL skills and experience in data management within a BPO environment.\n\nResponsibilities\nSenior Workato Developer",,,,"['Workato', 'integration development', 'SQL', 'Python']",2025-06-10 14:25:32
"Data Migration, CBS (Finacle), Senior Manager",Wikilabs India,6 - 9 years,Not Disclosed,['Navi Mumbai'],"Job Title: Senior Manager - Data Migration(Legacy to Finacle Application )\nLocation: Seawoods , Navi Mumbai\nShift: General.\nRelocation is not allowed; only candidates from Mumbai/Navi Mumbai are preferred.\n\nJob Summary:\nWe are seeking a highly experienced and hands-on Senior Manager, Finacle Data Migration, to lead and execute complex data migration projects for banking . The ideal candidate will have a proven track record of migrating legacy banking systems to Finacle 11x, with deep expertise across key modules including Assets, Liabilities, Customer Data Hub, and Trade Finance. This role requires a strong technical background in ETL processes, database management, and reporting tools, coupled with the ability to independently manage and deliver end-to-end data migration cycles.\n\nKey Responsibilities:\nLead and execute end-to-end data migration projects from legacy systems to Finacle 11x, specifically for modules such as Assets, Liabilities, Customer Data Hub, and Trade Finance (TF).\nDevelop, test, and maintain robust data migration scripts, data quality rules, and reconciliation scripts.\nConduct detailed data mapping activities from various legacy formats to Finacle 11x standards. Perform all necessary data extraction requirements from surrounding systems, ensuring data integrity and completeness.\nExecute multiple mock migrations, continuously updating and refining scripts based on findings and business requirements.\nConduct thorough data validation and correction of uploaded data to ensure accuracy and consistency post-migration.\nOversee and manage pre-migration preparation and post-migration activities, including cutover planning and post-go-live support.\nDevelop and maintain reports using SSRS/Power BI to support migration activities, reconciliation, and post-migration data analysis.\nProactively track project progress, identify potential risks, and report status to stakeholders effectively and transparently.\nCollaborate closely with business users, IT teams, and vendors to ensure successful project delivery.\n\nRequired Skills & Experience:\nMinimum of 6 years of experience in IT, with a significant focus on data migration within the banking or financial services sector.\nMust have hands-on experience working on migrating data to Finacle 11x versions.\nDemonstrable experience with Finacle modules including Assets, Liabilities, Customer Data Hub, and Trade Finance (TF).\nStrong background in ETL (Extract, Transform, Load) processes and tools.\nProven hands-on experience in Finacle data migration projects, having successfully executed at least 1-2 full-cycle Finacle data migration projects from legacy to Finacle 11x.\nProficiency in database management systems, specifically MS SQL and/or Oracle.\nExpert-level knowledge and hands-on experience with SQL, PL/SQL, and SQL Uploader.\nHands-on experience in report development using SSRS and/or Power BI.\nExcellent understanding of data validation techniques, data quality principles, and data reconciliation processes.\nAbility to independently manage tasks, create technical deliverables, and troubleshoot complex data issues.\nStrong analytical and problem-solving skills with meticulous attention to detail.\nExcellent communication and interpersonal skills to effectively collaborate with various teams and stakeholders.\n\nEducation:\nBachelor's or Master's degree in Computer Science, Information Technology, or a related field.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Migration', 'legacy Migration', 'Legacy to Finacle Application Migration', 'Core banking']",2025-06-10 14:25:34
Associate Threat Analyst,Barracuda Networks,0 - 1 years,Not Disclosed,['Bengaluru'],"We at Barracuda are at the forefront of protecting our customers from email-borne threats and data leaks. As an Analyst you will be having an opportunity to work with a core team of Threat Analysts who are specialized in stopping malicious traffic and content from reaching our customers.\n  What you'll be working on:\nAnalyze attack patterns and data trends, identifying drift and anomalies.\nConduct root cause analysis and develop hypotheses for missed attacks.\nReport findings to the ML team with impact assessments.\nEvaluate feature importance and model effectiveness.\nBuild reports, queries, and dashboards for insights.\nWhat you bring to the role:\n0-1 year experience in Python, Scala, SQL, Databricks, and Spark.\nHands-on experience in ML model development (MLLib, TensorFlow, PyTorch, Scikit).\nUnderstanding of email security (spam/phishing) is a plus.\nStrong analytical, communication, and adaptability skills.\nWillingness to work in a fast-paced, shift-based environment.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Training', 'Root cause analysis', 'Analyst', 'spark', 'Analytical', 'SCALA', 'Equity', 'model development', 'SQL', 'Python']",2025-06-10 14:25:36
Associate-DIG,Tresvista Financial Services,3 - 7 years,Not Disclosed,"['Mumbai', 'Pune', 'Bengaluru']","Roles and Responsibilities:\nExpertise and experience with Analytics, SQL, Python and handling large datasets is mandatory. • Use technical skills to ensure the accuracy of large analytical data sets, automate processes with scripts and macros and efficiently query information from a vast database.\nHave a good understanding of Fixed Income, Equity, Derivatives and Alternatives products and how they are modeled and traded\nExhibit attention to detail when quality checking Package analytics and be accountable of timely delivery of reports to clients in accordance with Service Level Agreements.\nEngage in meetings with end-users of the Package product from all levels within the company from Portfolio and Risk Managers to Operations teams and also with our external Clients.\nSupport client requests related to the Package analytics.\nProject work: engaging with other internal teams to think creatively and deliver innovative solutions to our complex client demands.\n\n\nPrerequisites\n• Excellent problem-solving and critical-thinking skills and an ability to identify problems, design and articulate solutions and implement change.\nExpertise and experience with Analytics, SQL, Python and handling large datasets is mandatory.\nExperience with UNIX, PERL and developing an Asset Management platform is highly desirable.\nKnowledge and understanding of Fixed Income, Equity, Derivatives and Alternatives products is preferred.\nExperience with Risk analytics such as Durations, Spreads, Beta and VaR is preferable.\nMust possess strong verbal and written communication skills and be able to develop good working relationships with stakeholders.\nMust be detail orientated, possess initiative and work well under pressure.\nWork experience with BFSI will be an added advantage.\nA Degree in Engineering or Technology is required.\n\n\nExperience\n3-7 years\n\nEducation\nB.Tech/B.E/MCA\n\nCompensation\nThe compensation structure will be as per industry standard",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Unix', 'Data Engineering', 'Analytics', 'SQL', 'Python', 'Large Databases', 'VAR', 'Perl', 'Data Analyst', 'Fixed Income', 'Equity Derivatives', 'Beta', 'Spread', 'Risk Analysis', 'Data Analysis']",2025-06-10 14:25:37
Power Bi Developer,IntelERA Inc,7 - 12 years,20-35 Lacs P.A.,[],"Job Title: Power BI Engineer\nLocation: Fully Remote (Preferred candidates in Pune, but open to all)\nWork Hours: 4:30 PM to 1:30 AM IST\nExperience Required: 7+ Years\n\nWe are seeking an experienced Power BI Engineer to join our team. The ideal candidate has a strong background in data analytics, visualization, and business intelligence, with a proven track record of designing and implementing impactful dashboards and reports. This role requires advanced skills in Power BI, data modeling, DAX, and ETL processes, as well as experience with large datasets and complex data transformations.\n\n\nKey Responsibilities:\n\nDesign, develop, and deploy Power BI dashboards and reports to provide insights that drive business decisions.\nWork closely with business stakeholders to gather and translate requirements into scalable, impactful data visualizations.\nImplement and optimize data models, using DAX to create robust calculations and complex measures.\nIntegrate and manage diverse data sources, including SQL databases, cloud services (e.g., Azure), APIs, and flat files.\nConduct data analysis and quality assurance to ensure data accuracy and report integrity.\nDevelop ETL processes using Power Query, SQL, and Azure Data Factory to extract, transform, and load data.\nImplement row-level security (RLS) and ensure adherence to data governance and security policies.\nPerform performance tuning on Power BI dashboards, ensuring efficient load times and smooth user experience.\nCollaborate with data engineers, data scientists, and other stakeholders to integrate advanced analytics and predictive insights.\nStay up-to-date with the latest Power BI features, best practices, and trends to continuously enhance BI solutions.\n\nRequired Skills and Qualifications:\n\n7+ years of experience in business intelligence and data analytics, with a strong focus on Power BI.\nProficient in Power BI Desktop, Power BI Service, and Power BI Report Server.\nExpertise in data modeling, DAX (Data Analysis Expressions), and M language.\nSolid understanding of relational databases and SQL, with experience in database management (SQL Server, Oracle, etc.).\nHands-on experience with ETL tools, such as Power Query, SQL Server Integration Services (SSIS), or Azure Data Factory.\nFamiliarity with cloud platforms, particularly Azure services like Azure SQL Database, Azure Data Lake, and Azure Analysis Services.\nStrong analytical skills, with a keen eye for detail and a commitment to data quality.\nExperience in implementing row-level security (RLS) and maintaining compliance with data governance standards.\nFamiliarity with scripting languages like Python or PowerShell for automation is a plus.\nExcellent communication and interpersonal skills, with the ability to effectively collaborate with both technical and non-technical stakeholders.\n\nSoft Skills:\n\nTakes ownership, is a proactive problem-solver with a positive, can-do attitude.\nPassionate about development.\nExcellent communication and teamwork skills.\nAbility to work effectively in a remote environment.\n\nNice to Have:\n\nKnowledge of Agile methodologies and familiarity with tools like Azure DevOps or JIRA.\nCertifications in Power BI, Microsoft Azure, or data engineering are advantageous.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power Bi', 'Data Visualization', 'Dax', 'Data Modeling', 'Data Analytics', 'Azure Data Factory', 'Power Bi Dashboards', 'SSIS', 'Power Query', 'SQL']",2025-06-10 14:25:39
Business Data Analyst,CGI,5 - 8 years,5-15 Lacs P.A.,['Hyderabad'],"Job Summary\nWe are seeking an experienced and results-driven Business Data Analyst with 5+ years of hands-on experience in data analytics, visualization, and business insight generation. This role is ideal for someone who thrives at the intersection of business and datatranslating complex data sets into compelling insights, dashboards, and strategies that support decision-making across the organization.\nYou will collaborate closely with stakeholders across departments to identify business needs, design and build analytical solutions, and tell compelling data stories using advanced visualization tools.\nKey Responsibilities\nData Analytics & Insights\nAnalyze large and complex data sets to identify trends, anomalies, and opportunities that help drive business strategy and operational efficiency.\nDashboard Development & Data Visualization\nDesign, develop, and maintain interactive dashboards and visual reports using tools like Power BI, Tableau, or Looker to enable data-driven decisions.\nBusiness Stakeholder Engagement\nCollaborate with cross-functional teams to understand business goals, define metrics, and convert ambiguous requirements into concrete analytical deliverables.\nKPI Definition & Performance Monitoring\nDefine, track, and report key performance indicators (KPIs), ensuring alignment with business objectives and consistent measurement across teams.\nData Modeling & Reporting Automation\nWork with data engineering and BI teams to create scalable, reusable data models and automate recurring reports and analysis processes.\nStorytelling with Data\nCommunicate findings through clear narratives supported by data visualizations and actionable recommendations to both technical and non-technical audiences.\nData Quality & Governance\nEnsure accuracy, consistency, and integrity of data through validation, testing, and documentation practices.\nRequired Qualifications\nBachelor’s or Master’s degree in Business, Economics, Statistics, Computer Science, Information Systems, or a related field.\n5+ years of professional experience in a data analyst or business analyst role with a focus on data visualization and analytics.\nProficiency in data visualization tools: Power BI, Tableau, Looker (at least one).\nStrong experience in SQL and working with relational databases to extract, manipulate, and analyze data.\nDeep understanding of business processes, KPIs, and analytical methods.\nExcellent problem-solving skills with attention to detail and accuracy.\nStrong communication and stakeholder management skills with the ability to explain technical concepts in a clear and business-friendly manner.\nExperience working in Agile or fast-paced environments.\nPreferred Qualifications\nExperience working with cloud data platforms (e.g., Snowflake, BigQuery, Redshift).\nExposure to Python or R for data manipulation and statistical analysis.\nKnowledge of data warehousing, dimensional modeling, or ELT/ETL processes.\nDomain experience in Healthcare is a plus.",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power BI', 'Data Visualization', 'Data Analytics', 'Insights', 'KPI', 'business insight generation', 'Tableau', 'Data Modeling', 'Looker', 'SQL', 'Performance Monitoring']",2025-06-10 14:25:41
Senior Data Operations Eng. For Product Based MNC-Pune,A client of Seventh contact,5 - 9 years,22.5-25 Lacs P.A.,[],"Monitor and maintain data pipelines and ETL processes to ensure reliability and performance.\nAutomate routine data operations tasks and optimize workflows for scalability and efficiency.\n\nRequired Candidate profile\nData operations, data engineering, or a related role.\nStrong SQL skills and experience with relational databases (e.g., PostgreSQL, MySQL).\nProficiency with data pipeline tools (e.g., Apache Airflow",Industry Type: Software Product,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Database Queries', 'SQL Queries', 'SQL']",2025-06-10 14:25:43
AI Engineer / Data Analyst,Meru Software,2 - 5 years,Not Disclosed,['Hyderabad'],Meru Software Pvt Ltd is looking for AI Engineer / Data Analyst to join our dynamic team and embark on a rewarding career journey.\n\nDesigning and developing AI algorithms and models to solve specific business problems. Creating and maintaining databases for storing and processing large amounts of data. Developing and deploying machine learning and deep learning models. Implementing and integrating AI solutions with existing systems and software. Analyzing and interpreting complex data sets to extract insights and drive decision - making. Collaborating with cross - functional teams to develop and deploy AI applications. Ensuring the security and privacy of data used in AI applications. Communicating and presenting technical information to non - technical stakeholders. Excellent communication skills & attention to detail.,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine learning', 'Data Analyst', 'SQL', 'Python']",2025-06-10 14:25:45
Data Lead,Multiverse Solutions,8 - 10 years,10-20 Lacs P.A.,[],"Job Summary:\nWe are seeking an experienced and hands-on Data Lead with deep expertise in Microsoft Azure Data Analytics ecosystem. The ideal candidate will lead the design, development, and implementation of scalable data pipelines and analytics solutions using Azure Data Factory (ADF), Synapse Analytics, Microsoft Fabric, Apache Spark, and modern data modeling techniques. A strong grasp of CDC mechanisms, performance tuning, and cloud-native architecture is essential.\nKey Responsibilities:\nLead the architecture and implementation of scalable data integration and analytics solutions in Azure.\nDesign and build end-to-end data pipelines using ADF, Azure Synapse Analytics, Azure Data Lake, and Microsoft Fabric.\nImplement and manage large-scale data processing using Apache Spark within Synapse or Fabric.\nDevelop and maintain data models using Star and Snowflake schema for optimal reporting and analytics performance.\nImplement Change Data Capture (CDC) strategies to ensure near real-time or incremental data processing.\nCollaborate with stakeholders to translate business requirements into technical data solutions.\nManage and mentor a team of data engineers and analysts.\nMonitor, troubleshoot, and optimize performance of data workflows and queries.\nEnsure best practices in data governance, security, lineage, and documentation.\nStay updated with the latest developments in the Azure data ecosystem and recommend enhancements.\nRequired Skills and Qualifications:\n810 years of overall experience in data engineering and analytics, with at least 3+ years in a data lead role.\nStrong expertise in Azure Data Factory, Azure Synapse, and Microsoft Fabric.\nHands-on experience with Apache Spark for large-scale data processing.\nProficient in SQL, Python, or PySpark for data transformation and automation.\nSolid experience with CDC patterns (e.g., using ADF, or SQL-based approaches).\nIn-depth understanding of data warehousing concepts and data modeling (Star, Snowflake).\nKnowledge of Power BI integration with Synapse/Fabric is a plus.\nFamiliarity with DevOps for data pipelines, version control (Git), and CI/CD for data solutions.\nStrong problem-solving skills and ability to lead architecture discussions and POCs.\nExcellent communication and stakeholder management skills.\nPreferred Qualifications:\nMicrosoft Certifications in Azure Data Engineering or Analytics.\nExperience with Delta Lake, Databricks, or Snowflake (as source/target).\nKnowledge of data privacy and compliance standards like GDPR, HIPAA.\nWhat We Offer:\nOpportunity to lead strategic data initiatives on the latest Azure stack.\nA dynamic and collaborative work environment.\nAccess to continuous learning, certifications, and upskilling programs.\nCompetitive compensation and benefits package.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Synapse', 'Cdc', 'Architecture', 'Microsoft Fabric', 'Pyspark', 'Azure Data Lake', 'Data Lake', 'Data Modeling', 'Data Bricks', 'SQL', 'Python']",2025-06-10 14:25:48
Tableau Developer,Viaprom Technologies,4 - 5 years,Not Disclosed,['Bengaluru'],"Job Description:\nWe at VIAPROM are seeking a skilled and experienced Tableau Developer to join our team within 15-20 days in Bengaluru. Candidates who are based out of Bengaluru will be preferred. The ideal candidate should have a strong background in Visualization and data storytelling.\n\nKey Responsibilities:\n1. Collaborate with business stakeholders to understand their data analysis and reporting requirements.\n2. Design and develop interactive and visually appealing dashboards, reports, and data visualizations using Tableau Desktop.\n3. Utilize Tableau's features, including calculated fields, parameters, data blending, and advanced analytics, to create insightful and actionable visualizations.\n4. Transform complex data sets into intuitive visualizations that effectively communicate key insights and trends.\n5. Work closely with data engineers to extract, transform, and load data from various sources into Tableau.\n6. Optimize the performance and efficiency of Tableau dashboards and reports by implementing best practices and data optimization techniques.\n7. Conduct data analysis to identify trends, patterns, and anomalies within the data.\n8. Collaborate with cross-functional teams to integrate Tableau dashboards and visualizations into business applications or portals.\n9. Stay up-to-date with the latest Tableau features, trends, and best practices to continually enhance data visualization capabilities.\n10. Provide support and training to end-users on Tableau functionality and usage.\n\nDesired Candidate Skills:\n1. Proven experience as a Tableau Developer with atleast 4-5 years of hands-on experience.\n2. In-depth knowledge of Tableau Desktop and Tableau Server, including data preparation, visualization design, and dashboard development.\n3. Strong understanding of data visualization best practices, data analysis, and data storytelling.\n4. Proficiency in SQL and experience with data modeling and database design.\n5. Ability to gather and translate business requirements into effective Tableau visualizations.\n6. Excellent communication and collaboration skills, with the ability to work effectively with cross-functional teams and stakeholders.\n\nEducational Qualifications\n1. Bachelor's degree in Computer Science, Information Systems, Data Science, or a related field.\n2. Candidates with Tableau Desktop and/or Tableau Server certification is preferable.\n3. Familiarity with statistical analysis and data mining techniques.\n4. Understanding of data governance and data security best practices.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Tableau', 'database design', 'data analysis', 'data modeling', 'data mining', 'Advanced Excel', 'statistical analysis', 'SQL']",2025-06-10 14:25:50
Software Engineering Lead Analyst,ManipalCigna Health Insurance,3 - 6 years,Not Disclosed,['Hyderabad'],"The Full-stack Data Engineer is responsible for the delivery of a business need end-to-end starting from understanding the requirements to deploying the software into production. This role requires you to be fluent in some of the critical technologies with proficiency in others and have a hunger to learn on the job and add value to the business. Critical attributes of being a full-stack engineer among others is Ownership & Accountability.\nIn addition to Delivery, the full-stack engineer should have an automation first and continuous improvement mindset. He/She should drive the adoption of CI/CD tools and support the improvement of the tools sets/processes.\nFull stack engineers are able to articulate clear business objectives aligned to technical specifications and work in an iterative, agile pattern daily. They have ownership over their work tasks, and embrace interacting with all levels of the team and raise challenges when necessary. We aim to be cutting-edge engineers - not institutionalized developers.\nRoles & Responsibilities:\nMinimize ""meetings"" to get requirements and have direct business interactions\nWrite referenceable & modular code\nDesign and architect the solution independently\nBe fluent in particular areas and have proficiency in many areas\nHave a passion to learn\nTake ownership and accountability\nUnderstands when to automate and when not to\nHave a desire to simplify\nBe entrepreneurial / business minded\nHave a quality mindset, not just code quality but also to ensure ongoing data quality by monitoring data to identify problems before they have a business impact\nTake risks and champion new ideas\nQualifications\nPrimary Skills:\nHands on with Python / PySpark programimng - 3yrs+\nSQL exp - 2yr+ (NoSQL can also work, but should have SQL 1yrs atleast)\nExp working with Cloud Tech - 1yrs+ - Any (AWS preferred)\nDevOps practices\nExperience Desired:\nExperience with Git/SVN\nExperience with scripting (JavaScript, Python, R, Ruby, Perl, etc.)\nExperience being part of Agile teams - Scrum or Kanban.\nAirflow\nDatabricks / Cloud Certifications\nAdditional Skills:\nExcellent troubleshooting skills\nStrong communication skills\nFluent in BDD and TDD development methodologies\nWork in an agile CI/CD environment (Jenkins experience a plus)\nKnowledge and/or experience with Health care information domains is a plus",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Javascript', 'Agile', 'Perl', 'Scrum', 'Troubleshooting', 'Ruby', 'Lead Analyst', 'SQL', 'Python']",2025-06-10 14:25:52
Data Center Operations Engineer,Locuz,3 - 8 years,4-7.5 Lacs P.A.,"['Thane', 'Vikhroli - Mumbai']","Job Description: Data Center Operations (3-8 years experience)\nLocation: Thane, Maharashtra\nRole Overview:\nWe are seeking an experienced Data Center Operations professional to manage and oversee the day-to-day operations of our data center facilities. The ideal candidate should have 3-8 years of hands-on experience in data center management, ensuring high availability, performance, and security of critical infrastructure.\nKey Responsibilities:\nOversee the operations, maintenance, and monitoring of data centre infrastructure.\nManage server hardware, networking equipment, and storage systems.\nEnsure uptime and availability of IT services by conducting proactive maintenance and troubleshooting.\nLead and coordinate teams for incident management and disaster recovery processes.\nImplement and maintain data centre best practices and compliance standards.\nWork with vendors for hardware procurement and software upgrades.\nManage capacity planning and resource optimization for the data centre.\nSkills & Qualifications:\n3-8 years of experience in data center operations or related fields.\nIn-depth knowledge of data center infrastructure, networking, and virtualization.\nExperience with server management, backup solutions, and monitoring tools.\nStrong understanding of security and disaster recovery processes.\nExcellent communication, leadership, and problem-solving skills.\nAbility to work in a fast-paced and high-pressure environment.\n\nInterested candidates share your resume to supriya.battula@locuz.com\nImmediate joiners are preferred.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Center Operations', 'Networking', 'Server', 'Physical Security', 'RAID']",2025-06-10 14:25:54
Aws Architect,Busybrains,9 - 12 years,35-40 Lacs P.A.,['Bengaluru'],"We are seeking an experienced AWS Architect with a strong background in designing and implementing cloud-native data platforms. The ideal candidate should possess deep expertise in AWS services such as S3, Redshift, Aurora, Glue, and Lambda, along with hands-on experience in data engineering and orchestration tools. Strong communication and stakeholder management skills are essential for this role.\n\nKey Responsibilities\nDesign and implement end-to-end data platforms leveraging AWS services.\nLead architecture discussions and ensure scalability, reliability, and cost-effectiveness.\nDevelop and optimize solutions using Redshift, including stored procedures, federated queries, and Redshift Data API.\nUtilize AWS Glue and Lambda functions to build ETL/ELT pipelines.\nWrite efficient Python code and data frame transformations, along with unit testing.\nManage orchestration tools such as AWS Step Functions and Airflow.\nPerform Redshift performance tuning to ensure optimal query execution.\nCollaborate with stakeholders to understand requirements and communicate technical solutions clearly.\nRequired Skills & Qualifications\nMinimum 9 years of IT experience with proven AWS expertise.\nHands-on experience with AWS services: S3, Redshift, Aurora, Glue, and Lambda.\nMandatory experience working with AWS Redshift, including stored procedures and performance tuning.\nExperience building end-to-end data platforms on AWS.\nProficiency in Python, especially working with data frames and writing testable, production-grade code.\nFamiliarity with orchestration tools like Airflow or AWS Step Functions.\nExcellent problem-solving skills and a collaborative mindset.\nStrong verbal and written communication and stakeholder management abilities.\nNice to Have\nExperience with CI/CD for data pipelines.\nKnowledge of AWS Lake Formation and Data Governance practices.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Aws Architecture', 'AWS', 'S3', 'Airflow', 'Pyspark', 'Data Frames', 'Redshift', 'SQL', 'Aurora', 'Glue', 'Redshift Aws', 'API', 'Aura', 'Lambda', 'Python', 'Step functions']",2025-06-10 14:25:57
Bigdata Administrator,Kryon Knowledge Works,5 - 8 years,Not Disclosed,['Hyderabad'],"Location: Hyderabad (Hybrid)\nPlease share your resume with +91 9361912009\n\nRoles and Responsibilities\n\nDeep understanding of Linux, networking and security fundamentals.\nExperience working with AWS cloud platform and infrastructure.\nExperience working with infrastructure as code with Terraform or Ansible tools.\nExperience managing large BigData clusters in production (at least one of -- Cloudera, Hortonworks, EMR).\nExcellent knowledge and solid work experience providing observability for BigData platforms using tools like Prometheus, InfluxDB, Dynatrace, Grafana, Splunk etc.\nExpert knowledge on Hadoop Distributed File System (HDFS) and Hadoop YARN.\nDecent knowledge of various Hadoop file formats like ORC, Parquet, Avro etc.\nDeep understanding of Hive (Tez), Hive LLAP, Presto and Spark compute engines.\nAbility to understand query plans and optimize performance for complex SQL queries on Hive and Spark.\nExperience supporting Spark with Python (PySpark) and R (SparklyR, SparkR) languages\nSolid professional coding experience with at least one scripting language - Shell, Python etc.\nExperience working with Data Analysts, Data Scientists and at least one of these related analytical applications like SAS, R-Studio, JupyterHub, H2O etc.\nAble to read and understand code (Java, Python, R, Scala), but expertise in at least one scripting languages like Python or Shell.\n\nNice to have skills:\n\nExperience with workflow management tools like Airflow, Oozie etc.\nKnowledge in analytical libraries like Pandas, Numpy, Scipy, PyTorch etc.\nImplementation history of Packer, Chef, Jenkins or any other similar tooling.\nPrior working knowledge of Active Directory and Windows OS based VDI platforms like Citrix, AWS Workspaces etc.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ORC', 'Linux', 'Terraform', 'AWS', 'Active Directory', 'Parquet', 'PySpark', 'Hadoop', 'Avro']",2025-06-10 14:26:00
Data Center Ops Engineer,Clover Infotech,2 - 5 years,Not Disclosed,"['Mumbai', 'Navi Mumbai']","Job Role : Data Center Operations Engineer\nExperience : 2+yrs\nLocation : Navi Mumbai\nJob Type - Permanent Work from Office\nRoles & Responsibilities :\nHardware Monitoring & Inventory Management\nMin exp in DC environment activities 2-3 years\nDC monitoring and Operations support\nKnowledge of mounting of Servers and Network devices in RACK\nKnowledge on MUX, Primary & Secondary Power, UPS, ATS, STS\nCrimping of Network cable\nGood in communication\nManaging the DC L1 Support for day-to-day operations\nMaintaining DC Environment & Data center Hygiene.\nEscorting visitors when required & follow escalation matrix",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Inventory Management', 'Data Center Operations', 'Mounting', 'Crimping', 'Vendor Coordination', 'Device Management', 'Server Monitoring']",2025-06-10 14:26:04
PLSQL Developer,Emagia,4 - 7 years,8-13 Lacs P.A.,['Hyderabad'],"We are looking for a highly skilled Sr. PL/SQL Developer / Data Engineer with 5+ years of experience in database development, performance optimization, and data engineering tasks, primarily in the healthcare domain. The candidate should possess hands-on expertise in PL/SQL development, SQL query optimization, database object creation, and dashboard reporting tools, with working knowledge of Agile methodologies and tools like JIRA, Bitbucket, and PLX Dashboards.\nKey Responsibilities\nDesign, develop, and optimize complex PL/SQL procedures, functions, triggers, and packages.\nCreate and maintain database objects such as tables, views, indexes, and packages using Oracle best practices.\nAnalyze and interpret Product Requirement Documents (PRDs) and Upgrade Documents to implement business logic in backend systems.\nWrite, debug, and tune SQL queries for large datasets to ensure high performance and reliability.\nCollaborate with QA, analysts, and cross-functional teams to ensure data accuracy and requirement clarity.\nCreate complex data extraction and reporting queries for dashboards and business analysis.\nPerform unit testing, performance tuning, and provide support for production deployments.\nWork with tools like JIRA, Buganizer, Gitbash, Bitbucket, and Toad for issue tracking and version control.\nParticipate in code reviews, mentor junior developers, and follow coding and documentation standards.\nEnsure database solutions comply with security and compliance standards.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['PLSQL', 'Procedures', 'Triggers', 'SQL', 'Indexing']",2025-06-10 14:26:05
ETL_GCP_Bigquery,Tekskillsindia,6 - 8 years,Not Disclosed,"['Hyderabad', 'Pune']","Develop, implement, and optimize ETL/ELT pipelines for processing large datasets efficiently.\n• Work extensively with BigQuery for data processing, querying, and optimization.\n• Utilize Cloud Storage, Cloud Logging, Dataproc, and Pub/Sub for data ingestion, storage, and event-driven processing.\n• Perform performance tuning and testing of the ELT platform to ensure high efficiency and scalability.\n• Debug technical issues, perform root cause analysis, and provide solutions for production incidents.\n• Ensure data quality, accuracy, and integrity across data pipelines.\n• Collaborate with cross-functional teams to define technical requirements and deliver solutions.\n• Work independently on assigned tasks while maintaining high levels of productivity and efficiency.\nSkills Required:\n• Proficiency in SQL and PL/SQL for querying and manipulating data.\n• Experience in Python for data processing and automation.\n• Hands-on experience with Google Cloud Platform (GCP), particularly:\no BigQuery (must-have)\no Cloud Storage\no Cloud Logging\no Dataproc\no Pub/Sub\n• Experience with GitHub and CI/CD pipelines for automation and deployment.\n• Performance tuning and performance testing of ELT processes.\n• Strong analytical and debugging skills to resolve data and pipeline issues efficiently.\n• Self-motivated and able to work independently as an individual contributor.\n• Good understanding of data modeling, database design, and data warehousing concepts.Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GCP', 'Bigquery', 'ETL']",2025-06-10 14:26:07
Business Analyst- Cloud Data Platform,Rarr Technologies,4 - 6 years,Not Disclosed,[],"Job Title: Business Analyst (BA) Cloud Data Platform\n\nPrimary Skillset:\nBusiness Analysis expertise in cloud-based data platforms\nSQL\nWork closely with stakeholders to gather, analyze, and document business and technical requirements\nCollaborate with Azure Data Engineers and architects to translate business needs into scalable cloud-based data solutions\nConduct detailed analysis and support design decisions across data integration, transformation, and storage layers\nAssist in defining and validating data quality rules and governance processes\nDrive requirement sessions and ensure traceability throughout the project lifecycle\nManage timelines, risks, and dependencies across multiple priorities and projects\nRequired Experience and Qualifications:\nMinimum 4 years of experience as a Business Analyst, with at least 2+ years in Azure Data Engineering projects\nGood to have knowledge of:\nAzure Data Factory\nAzure Databricks\nAzure SQL Database\nOther Azure data services\nStrong analytical and problem-solving skills with the ability to work in fast-paced, dynamic environments\nFamiliarity with ETL pipelines, data modeling, and data warehousing concepts\nExcellent verbal and written communication skills\nDemonstrated ability to collaborate across business and technical teams\nUnderstanding of data governance and data quality best practices\nPreferred Skills:\nExposure to other cloud platforms\nFamiliarity with modern data engineering tools and frameworks\nSQL\nBusiness Analyst",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data services', 'Business Analyst', 'Business analysis', 'Data modeling', 'Analytical', 'Cloud', 'data governance', 'Data quality', 'Data warehousing', 'SQL']",2025-06-10 14:26:09
DevOps Engineer - Lead,Blend360 India,7 - 12 years,Not Disclosed,['Hyderabad'],"We are seeking a highly skilled Lead Azure DevOps Engineer to join our team and drive the end-to-end deployment, scalability, and operationalization of machine learning models in production. You will collaborate closely with data scientists, data engineers, and DevOps teams to ensure seamless CI/CD, reproducibility, monitoring, and governance of ML pipelines\nKey Responsibilities\nDesign, implement, and maintain CI/CD pipelines for deploying and monitoring microservices efficiently.\nManage infrastructure as code using Terraform for repeatable and scalable provisioning.\nDeploy and optimize containerized applications using Docker and Azure services (Container Apps, Container Registry, Key Vault, Service Bus, Blob Storage).\nApply best practices for securing Docker images, including vulnerability scanning, reducing image size, and optimizing build efficiency.\nImplement and maintain Azure Monitor for logging, monitoring, and alerting to ensure system reliability.\nEnsure security best practices across cloud environments, including secrets management, access control, and compliance.\n(Nice to have) Design and manage multi-client architectures within shared pipelines and storage accounts in Azure Blob Storage\n\n\n6+ years of experience in DevOps or MLOps with a strong focus on production-grade ML solutions.\nStrong expertise in Azure, particularly with CI/CD, container orchestration, and cloud security. Proficiency in Terraform for infrastructu",Industry Type: Industrial Automation,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'orchestration', 'cloud security', 'devops', 'Machine learning', 'Infrastructure', 'Vulnerability', 'Management', 'Monitoring', 'microservices']",2025-06-10 14:26:11
HR Recruiter cum HRBP,Anarock Property Consultants,4 - 8 years,6-7.5 Lacs P.A.,"['Bengaluru( Palace Road, HSR Layout )']","Responsibilities:\nPartner with hiring managers to understand technical hiring needs and develop sourcing strategies.\nManage end-to-end recruitment life cycle for tech roles (SDEs, QA, DevOps, Data Engineers, etc.).\nSource, screen, and interview candidates using job portals, Naukri, IIM Jobs, LinkedIn, employee referrals, and other channels.",,,,"['Technical Hiring', 'Negotiation', 'Sales Hiring', 'Business Partnering', 'Recruitment', 'Hiring', 'Employee Engagement', 'Campus Hiring', 'Talent Sourcing']",2025-06-10 14:26:14
Scala developer (with Spark),Fortune India 500 IT Services Firm,5 - 8 years,Not Disclosed,"['Hyderabad', 'Pune']","We are looking for a highly skilled Scala Developer with solid experience in Apache Spark to join our data engineering team.\n\nExperience- 5 to 8yrs\nLocation- Pune, Hyderabad\nMandatory skills- Scala development, Spark\nKey Responsibilities:\nDesign, develop, and optimize batch and streaming data pipelines using Scala and Apache Spark.\nWrite efficient, reusable, and testable code following functional programming best practices.\nWork with large-scale datasets from a variety of sources (e.g., Kafka, Hive, S3, Parquet).\nCollaborate with data scientists, data analysts, and DevOps to ensure robust and scalable pipelines.\nTune Spark jobs for performance and resource efficiency.\nImplement data quality checks, logging, and error-handling mechanisms.\n\n\n\nInterested candidates share your CV at himani.girnar@alikethoughts.com with below details\n\nCandidate's name-\nEmail and Alternate Email ID-\nContact and Alternate Contact no-\nTotal exp-\nRelevant experience-\nCurrent Org-\nNotice period-\nCCTC-\nECTC-\nCurrent Location-\nPreferred Location-\nPancard No-",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Scala Programming', 'Spark', 'SCALA', 'Big Data Technologies', 'SQL']",2025-06-10 14:26:16
Kafka Engineer DevOps/SRE Technology Experts,Hireflex247,5 - 10 years,Not Disclosed,['Bengaluru'],"5+ years of hands-on experience in Kafka administration , DevOps , and SRE practices .\nDeep understanding of Kafka architecture , internals, and performance tuning.\nPractical experience with Kubernetes (EKS, AKS, or GKE) and Kafka operators like Strimzi .\nProficiency in Terraform , Helm , and setting up/managing CI/CD pipelines .\nStrong scripting abilities using Python , Bash , or Golang for automation tasks.\nFamiliarity with monitoring and logging tools such as Prometheus , Grafana , Loki , ELK stack , and OpenTelemetry .\nExposure to the broader Kafka ecosystem , including MirrorMaker 2.0 , Kafka Streams , ksqlDB , and Schema Registry .\nSound knowledge of networking , DNS , service meshes , and Kubernetes security best practices .\nSoft Skills & Kafka Evangelism:\nDemonstrated passion for teaching, mentoring , and knowledge sharing across engineering teams.\nStrong interpersonal and communication skills able to simplify and explain Kafka concepts to a broad audience, including non-technical stakeholders.\nComfortable leading internal Kafka meetups , contributing to technical blogs , and creating educational materials to promote best practices.\nA proactive problem-solver and influencer who can champion Kafka adoption across the company.\nStrong collaboration skills able to work effectively with developers , data engineers , and leadership teams .\nJob Type: Job Location: Apply for this position Are you willing to work & report at Flexible hours? * Allowed Type(s): .pdf, .doc, .docx Years of Experience\nLinkedIn Profile Link *\nBy using this form you agree with the storage and handling of your data by this website. *",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Automation', 'PDF', 'Networking', 'devops', 'Schema', 'DNS', 'Manager Technology', 'Monitoring', 'Python']",2025-06-10 14:26:18
Software Engineer (Contract To Hire),Infosys,8 - 13 years,9-19 Lacs P.A.,['Pune'],"** Mandate Skills : API development, Java, Databricks and AWS **\nTechnical\nTwo or more years of API Development experience (specifically Rest APIs using Java, Spring boot, Hibernate)\nTwo or more years of Data Engineering and the respective tools and technologies (e.g., Apache Spark, Databricks, SQL DB, NoSQL DB, Data Lake concepts)\nWorking knowledge of Test-driven development\nWorking knowledge of experience leveraging DevOps and lean development principles such as Continuous Integration, Continuous Delivery/Deployment using tools like Git\nWorking knowledge of ETL, Data Modeling, Data Warehousing, and working with large-scale datasets \nWorking Knowledge of AWS services such as Lambda, RDS, ECS, DynamoDB, API Gateway, S3 etc.\nGood to have:\nAWS Developer certification or working experience in AWS cloud or other cloud technologies\nPassionate, creative and have the desire to learn new complex technical areas\nAccountable, curious, and collaborative with an intense focus on product quality\nSkilled in interpersonal communications and ability to communicate complex topics to non-technical audiences\nExperience working in an agile team environment",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Java', 'API', 'Databricks', 'AWS']",2025-06-10 14:26:19
F2F Interview | Chennai | Snowflake Developer | MNC !!,India's Largest IT Service Provider,5 - 10 years,15-25 Lacs P.A.,['Chennai'],"Warm welcome from SP Staffing Services!\n\nReaching out to you regarding permanent opportunity !!\nJob Description:\nExp: 5-12 yrs\nLocation: Chennai\nSkill: Snowflake Developer\n\nDesired Skill Sets:\nShould have strong experience in Snwoflake\nStrong Experience in AWS and Python\nExperience in ETL tools like Abinitio, Teradata\nInterested can share your resume to sangeetha.spstaffing@gmail.com with below inline details.\n\nFull Name as per PAN:\nMobile No:\nAlt No/ Whatsapp No:\nTotal Exp:\nRelevant Exp in Snowflake Development:\nRel Exp in AWS:\nRel Exp in Python/Abinitio/Teradata:\nCurrent CTC:\nExpected CTC:\nNotice Period (Official):\nNotice Period (Negotiable)/Reason:\nDate of Birth:\nPAN number:\nReason for Job Change:\nOffer in Pipeline (Current Status):\nAvailability for F2F interview on 14th June, Saturday between 9 AM- 12 PM(plz mention time):\nCurrent Res Location:\nPreferred Job Location:\nWhether educational % in 10th std, 12th std, UG is all above 50%?\nDo you have any gaps in between your education or Career?\nIf having gap, please mention the duration in months/year:",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'AWS', 'Snowflake Sql', 'Snowsql', 'Snowflake Modeling', 'Snowflake Db', 'Snowpipe']",2025-06-10 14:26:21
Data Governance - Engineer,Apidel Technologies,1 - 2 years,Not Disclosed,['Mumbai( Vikhroli )'],Strong SQL expertise\nHands-on experience with Advent Geneva and dataset setup\nExcellent communication skills to work across teams and functions.,Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['SQL Queries', 'Geneva', 'advent geneva', 'Complex Queries', 'SQL']",2025-06-10 14:26:24
Data Scientist,Synergy Maritime,3 - 5 years,Not Disclosed,['Chennai'],Role & responsibilities\n\nDesign ML design and Ops stack considering the various trade-offs.\nStatistical Analysis and fundamentals\nMLOPS frameworks design and implementation \nModel Evaluation best practices -Train and retrain systems when necessary.\nExtend existing ML libraries and frameworks -Keep abreast of developments in the field.,,,,"['Data Science', 'Docker', 'Artificial Intelligence', 'Hadoop', 'Big Data Technologies', 'Machine Learning', 'Deep Learning', 'Kubernetes', 'SQL']",2025-06-10 14:26:27
Backend Engineer,Fusion Plus Solutions,1 - 3 years,Not Disclosed,['Hyderabad'],"Fusion Plus Solutions Inc is looking for Backend Engineer to join our dynamic team and embark on a rewarding career journey.\nA Backend Engineer is responsible for designing, developing, and maintaining the server-side components of a software application or system\nThey collaborate with cross-functional teams, including front-end developers, product managers, and data engineers, to ensure the seamless integration and functionality of the software\nBackend Engineers work with programming languages, databases, and frameworks to create robust and scalable server-side solutions\nKey Responsibilities:Collaborate with cross-functional teams to understand software requirements and translate them into technical specifications\nDesign and develop server-side software components, APIs, and services to support the functionality of the software application\nWrite clean, efficient, and maintainable code using programming languages such as Python, Java, or Node\njs\nImplement and maintain data models, databases, and data access layers for efficient data storage and retrieval\nEnsure the security, performance, and scalability of backend systems through proper design and optimization techniques\nIntegrate third-party services, APIs, and libraries into the backend infrastructure to enhance functionality\nImplement and maintain unit tests, automated tests, and continuous integration/continuous deployment (CI/CD) pipelines to ensure code quality and reliability\nCollaborate with front-end developers to design and implement API endpoints and data formats for efficient client-server communication\nMonitor and troubleshoot production systems to identify and resolve backend-related issues and performance bottlenecks",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['interviewing', 'continuous integration', 'head hunting', 'python', 'screening', 'software testing', 'hiring', 'job portals', 'hrsd', 'scalability', 'javascript', 'sourcing', 'staffing', 'talent acquisition', 'node.js', 'it recruitment', 'java', 'recruitment', 'onboarding', 'mysql', 'api', 'aws', 'mongodb', 'jira']",2025-06-10 14:26:29
Lead Data Scientist,Tothr,8 - 12 years,20-22.5 Lacs P.A.,"['Chennai', 'Bengaluru']","Experience working closely with other data scientists, data engineers' software engineers, data managers and business partners.\n7+ years in designing, planning, prototyping, productionizing, maintaining\nknowledge in Python, Go, Java,\nSQL knowledge",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Statistical Modeling', 'Python Development', 'Machine Learning', 'Deep Learning', 'Generative Ai', 'Advance Sql']",2025-06-10 14:26:31
Machine Learning Engineer,Bay Area Tek Solutions LLC,2 - 5 years,Not Disclosed,['Bengaluru'],"Must have: Strong on programming languages like Python, Java One cloud hands-on experience (GCP preferred) Experience working with Dockers Environments managing (e.g venv, pip, poetry, etc.) Experience with orchestrators like Vertex AI pipelines, Airflow, etc Understanding of full ML Cycle end-to-end Data engineering, Feature Engineering techniques Experience with ML modelling and evaluation metrics Experience with Tensorflow, Pytorch or another framework Experience with Models monitoring Advance SQL knowledge Aware of Streaming concepts like Windowing , Late arrival , Triggers etc",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['GCP', 'Machine learning', 'Cloud', 'Programming', 'Management', 'Monitoring', 'SQL', 'Python']",2025-06-10 14:26:33
Devops AWS DATA Engineeer|| Technical Analyst || 12Lakhs CTC,Robotics Technologies,6 - 10 years,11-12 Lacs P.A.,['Hyderabad( Banjara hills )'],"We are seeking a highly skilled Devops Engineer to join our dynamic development team. In this role, you will be responsible for designing, developing, and maintaining both frontend and backend components of our applications using Devops and associated technologies.\nYou will collaborate with cross-functional teams to deliver robust, scalable, and high-performing software solutions that meet our business needs. The ideal candidate will have a strong background in devops, experience with modern frontend frameworks, and a passion for full-stack development.\n\nRequirements:\nBachelor's degree in Computer Science Engineering, or a related field.\n6 to 10+ years of experience in full-stack development, with a strong focus on DevOps.\n\nDevOps with AWS Data Engineer - Roles & Responsibilities:\nUse AWS services like EC2, VPC, S3, IAM, RDS, and Route 53.\nAutomate infrastructure using Infrastructure as Code (IaC) tools like Terraform or AWS CloudFormation.\nBuild and maintain CI/CD pipelines using tools AWS CodePipeline, Jenkins,GitLab CI/CD.\nCross-Functional Collaboration\nAutomate build, test, and deployment processes for Java applications.\nUse Ansible, Chef, or AWS Systems Manager for managing configurations across environments.\nContainerize Java apps using Docker.\nDeploy and manage containers using Amazon ECS, EKS (Kubernetes), or Fargate.\nMonitoring & Logging using Amazon CloudWatch,Prometheus + Grafana,E\nStack (Elasticsearch, Logstash, Kibana),AWS X-Ray for distributed tracing manage access with IAM roles/policies.\nUse AWS Secrets Manager / Parameter Store for managing credentials.\nEnforce security best practices, encryption, and audits.\nAutomate backups for databases and services using AWS Backup, RDS Snapshots, and S3 lifecycle rules.\nImplement Disaster Recovery (DR) strategies.\nWork closely with development teams to integrate DevOps practices.\nDocument pipelines, architecture, and troubleshooting runbooks.\nMonitor and optimize AWS resource usage.\nUse AWS Cost Explorer, Budgets, and Savings Plans.\n\nMust-Have Skills:\nExperience working on Linux-based infrastructure.\nExcellent understanding of Ruby, Python, Perl, and Java.\nConfiguration and managing databases such as MySQL, Mongo.\nExcellent troubleshooting.\nSelecting and deploying appropriate CI/CD tools\nWorking knowledge of various tools, open-source technologies, and cloud services.\nAwareness of critical concepts in DevOps and Agile principles.\nManaging stakeholders and external interfaces.\nSetting up tools and required infrastructure.\nDefining and setting development, testing, release, update, and support processes for DevOps operation.\nHave the technical skills to review, verify, and validate the software code developed in the project.\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Iac', 'Devops', 'Jenkins', 'AWS', 'Kubernetes', 'RDS', 'Aws Cloudformation', 'Amazon Cloudwatch', 'Prometheus', 'Ci/Cd', 'Grafana', 'DR', 'Cloud Trail', 'Docker', 'IAM', 'Ansible / Chef', 'fargate', 'Gitlab', 'Monitoring', 'Python']",2025-06-10 14:26:35
Devops AWS DATA Engineeer|| Technical Analyst || 12Lakhs CTC,Robotics Technologies,5 - 9 years,11-12 Lacs P.A.,['Hyderabad( Banjara hills )'],"We are seeking a highly skilled Devops Engineer to join our dynamic development team. In this role, you will be responsible for designing, developing, and maintaining both frontend and backend components of our applications using Devops and associated technologies.\nYou will collaborate with cross-functional teams to deliver robust, scalable, and high-performing software solutions that meet our business needs. The ideal candidate will have a strong background in devops, experience with modern frontend frameworks, and a passion for full-stack development.\n\nRequirements:\nBachelor's degree in Computer Science Engineering, or a related field.\n5 to 9+ years of experience in full-stack development, with a strong focus on DevOps.\n\nDevOps with AWS Data Engineer - Roles & Responsibilities:\nUse AWS services like EC2, VPC, S3, IAM, RDS, and Route 53.\nAutomate infrastructure using Infrastructure as Code (IaC) tools like Terraform or AWS CloudFormation.\nBuild and maintain CI/CD pipelines using tools AWS CodePipeline, Jenkins,GitLab CI/CD.\nCross-Functional Collaboration\nAutomate build, test, and deployment processes for Java applications.\nUse Ansible, Chef, or AWS Systems Manager for managing configurations across environments.\nContainerize Java apps using Docker.\nDeploy and manage containers using Amazon ECS, EKS (Kubernetes), or Fargate.\nMonitoring & Logging using Amazon CloudWatch,Prometheus + Grafana,E\nStack (Elasticsearch, Logstash, Kibana),AWS X-Ray for distributed tracing manage access with IAM roles/policies.\nUse AWS Secrets Manager / Parameter Store for managing credentials.\nEnforce security best practices, encryption, and audits.\nAutomate backups for databases and services using AWS Backup, RDS Snapshots, and S3 lifecycle rules.\nImplement Disaster Recovery (DR) strategies.\nWork closely with development teams to integrate DevOps practices.\nDocument pipelines, architecture, and troubleshooting runbooks.\nMonitor and optimize AWS resource usage.\nUse AWS Cost Explorer, Budgets, and Savings Plans.\n\nMust-Have Skills:\nExperience working on Linux-based infrastructure.\nExcellent understanding of Ruby, Python, Perl, and Java.\nConfiguration and managing databases such as MySQL, Mongo.\nExcellent troubleshooting.\nSelecting and deploying appropriate CI/CD tools\nWorking knowledge of various tools, open-source technologies, and cloud services.\nAwareness of critical concepts in DevOps and Agile principles.\nManaging stakeholders and external interfaces.\nSetting up tools and required infrastructure.\nDefining and setting development, testing, release, update, and support processes for DevOps operation.\nHave the technical skills to review, verify, and validate the software code developed in the project.\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Iac', 'Devops', 'Jenkins', 'AWS', 'Kubernetes', 'RDS', 'Aws Cloudformation', 'Amazon Cloudwatch', 'Prometheus', 'Ci/Cd', 'Grafana', 'DR', 'Cloud Trail', 'Docker', 'IAM', 'Ansible / Chef', 'fargate', 'Gitlab', 'Monitoring', 'Python']",2025-06-10 14:26:38
Consultant - Data Governance (Immediate joiner),A leading Insurance Consulting Client,4 - 7 years,Not Disclosed,"['Pune', 'Bengaluru']","We are hiring for a leading Insurance Consulting organisation for Data Strategy & Governance role\n\nExp: 4 to 6 yyrs\n\nLocation : BangalorePune\n\nResponsibility:\nDevelop and Drive Data Capability Maturity Assessment, Data & Analytics Operating Model & Data Governance exercises for clients\nManaging Critical Data Elements (CDEs) and coordinating with Finance Data Stewards\nOverseeing data quality standards and governance implementation\nEstablish processes around effective data management ensuring Data Quality & Governance standards as well as roles for Data Stewards",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Governance', 'Data Transformation', 'Data Strategy', 'Metadata Management', 'Data Lineage', 'Data Engineering', 'Data Management', 'Data Quality Management', 'Data Integration', 'Data Quality', 'Data Pipeline', 'Data Stewardship', 'Master Data Management', 'Data Architecture']",2025-06-10 14:26:40
Data Analyst-Having Stratup-Mid-Size companies Exp.@ Bangalore_Urgent,"A leader in this space, we deliver world...",8 - 13 years,Not Disclosed,['Bengaluru'],"Data Analyst\n\nLocation: Bangalore\nExperience: 8 - 15 Yrs\nType: Full-time\n\nRole Overview\n\nWe are seeking a skilled Data Analyst to support our platform powering operational intelligence across airports and similar sectors. The ideal candidate will have experience working with time-series datasets and operational information to uncover trends, anomalies, and actionable insights. This role will work closely with data engineers, ML teams, and domain experts to turn raw data into meaningful intelligence for business and operations stakeholders.\n\nKey Responsibilities\n\nAnalyze time-series and sensor data from various sources\nDevelop and maintain dashboards, reports, and visualizations to communicate key metrics and trends.\nCorrelate data from multiple systems (vision, weather, flight schedules, etc) to provide holistic insights.\nCollaborate with AI/ML teams to support model validation and interpret AI-driven alerts (e.g., anomalies, intrusion detection).\nPrepare and clean datasets for analysis and modeling; ensure data quality and consistency.\nWork with stakeholders to understand reporting needs and deliver business-oriented outputs.\n\n\nQualifications & Required Skills\n\nBachelors or Masters degree in Data Science, Statistics, Computer Science, Engineering, or a related field.\n5+ years of experience in a data analyst role, ideally in a technical/industrial domain.\nStrong SQL skills and proficiency with BI/reporting tools (e.g., Power BI, Tableau, Grafana).\nHands-on experience analyzing structured and semi-structured data (JSON, CSV, time-series).\nProficiency in Python or R for data manipulation and exploratory analysis.\nUnderstanding of time-series databases or streaming data (e.g., InfluxDB, Kafka, Kinesis).\nSolid grasp of statistical analysis and anomaly detection methods.\nExperience working with data from industrial systems or large-scale physical infrastructure.\n\n\nGood-to-Have Skills\n\nDomain experience in airports, smart infrastructure, transportation, or logistics.\nFamiliarity with data platforms (Snowflake, BigQuery, Custom-built using open-source).\nExposure to tools like Airflow, Jupyter Notebooks and data quality frameworks.\nBasic understanding of AI/ML workflows and data preparation requirements.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Kafka', 'SQL', 'airports', 'InfluxDB', 'Airflow', 'structured Data', 'time-series', 'JSON', 'Tableau', 'Grafana', 'R', 'AI/ML', 'Kinesis', 'Snowflake', 'time-series databases', 'Data Preparation', 'Python', 'smart infrastructure', 'BigQuery', 'streaming data', 'Power BI', 'CSV', 'transportation', 'logistic', 'reporting tools']",2025-06-10 14:26:43
Snowflake Architect,Blend360 India,8 - 13 years,Not Disclosed,['Hyderabad'],"We are looking for an experienced and hands-on Snowflake Architect to lead and expand our data engineering capabilities. This role is ideal for a technically strong leader who can design scalable data architectures, manage a high-performing team, and collaborate cross-functionally to deliver reliable and secure data solutions.\nResponsibilities:\nDesign and implement robust Snowflake data warehouse architectures and ETL pipelines to support business intelligence and advanced analytics use cases.\nLead and mentor a team of data engineers, ensuring high-quality and timely delivery of projects.\nCollaborate closely with data analysts, data scientists, and business stakeholders to understand data needs and design effective data models.\nDevelop, document, and enforce best practices for Snowflake architecture, data modeling, performance optimization, and ETL processes.\nOwn the optimization of Snowflake environments to ensure low-latency and high-availability data access.\nDrive process improvements, evaluate emerging tools, and continuously enhance our data engineering infrastructure.\nEnsure data pipelines are built with high levels of accuracy, completeness, and security, in compliance with data privacy regulations (GDPR, CCPA, etc.).\nPartner with cloud engineering and DevOps teams to integrate data solutions seamlessly within the AWS ecosystem.\nParticipate in capacity planning, budgeting, and resource allocation for the data engineering function.\n\n\nBachelor s degree in Computer Science, Information Technology, or a related field.\n9+ years of overall experience in cloud-based data engineering, with at least 4 years of hands-on experience in Snowflake.",Industry Type: Industrial Automation,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Architect', 'Data modeling', 'data security', 'data governance', 'Business intelligence', 'Information technology', 'SQL', 'Python', 'Capacity planning']",2025-06-10 14:26:46
Data Loss Prevention Engineer,Theorynine Technologies,2 - 5 years,3-8 Lacs P.A.,"['Thane', 'Goregaon']","We are hiring a DLP Specialist with 3 to 6 years of experience in managing\nendpoint security technologies. The ideal candidate will play a critical role in\ndeploying, maintaining, and optimizing Data Loss Prevention.\nForecepoint DLP is Compulsory\n\nRequired Candidate profile\nImplement and manage endpoint security tools, including DLP, XDR, and\nencryption solutions. Investigate and analyze DLP alerts and incidents, ensuring swift and\neffective response.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Loss Prevention', 'Forcepoint', 'xdr', 'Dlp']",2025-06-10 14:26:49
Principal Software Engineer,Nomura,5 - 10 years,Not Disclosed,['Mumbai'],"Job Title: Principal Software Engineer\nJob Code: 10011\nCountry: IN\nCity: Mumbai\nSkill Category: IT\\Technology\nDescription:\nRole Description\nYou are a confident and selfmotivating developer with a solid understanding of python. You are client focused and are comfortable communicating with stakeholders across the bank. You are at home developing in a small selforganising team and value user feedback within an agile development process. You can manage expectations effectively and have a demonstrable track record of owning delivery of a high profile product to demanding business users. You are familiar with the Fixed Income markets, both cash and derivatives, and understand the value of an intuitive, robust and reliable research platform for ETrading.\nSummary:\nWe are seeking an innovative and experienced Python Developer to join our dynamic team. The ideal candidate will be responsible for building out a best in class Python research platform to support the development, and optimization of electronic trading strategies across various asset classes and markets.\nKey Responsibilities:\nDesign and develop a sophisticated research platform to support the build of algorithmic trading strategies using quantitative methods and statistical analysis\nBuild tools to analyse market microstructure and develop models to predict shortterm price movements\nCollaborate with quants, developers, and traders to translate trading ideas into productionready code\nBuild tools to monitor and evaluate the performance of trading strategies, making realtime adjustments as necessary\nDesign and develop platform to conduct research on new trading opportunities and market inefficiencies\nContribute to the firms overall electronic trading infrastructure and technology stack\nRequired Experience:\nDegree in Computer Science, Mathematics, Physics, or a related quantitative field\nMinimum of 5 years of experience in Python development\nUnderstanding of market microstructure, order types, and exchange connectivity\nExcellent problemsolving skills and ability to work under pressure in a fastpaced environment\nStrong proficiency in Python and its core libraries (NumPy, Pandas, SciPy)\nExperience with financial data analysis libraries (yfinance, pandasdatareader, talib)\nFamiliarity with data visualization tools (Matplotlib, Seaborn, Plotly)\nUnderstanding of SQL and database management\nExperience with API integration (REST APIs, WebSocket)\nVersion control proficiency (Git)\nPreferred Experience:\nExperience with multiple asset classes (Rates, equities, futures, options, FX)\nKnowledge of marketmaking strategies and risk management techniques\nKnowledge of machine learning libraries (scikitlearn, TensorFlow, PyTorch) for predictive modeling\nKnowledge of algorithmic trading concepts and frameworks (Backtrader, Zipline)\nKDB or other time series database\nKey Competencies:\nStrong analytical and quantitative skills\nCreativity in developing novel trading strategies\nExcellent communication skills to explain complex concepts to both technical and nontechnical audiences\nAbility to work collaboratively in a team environment\nAdaptability to rapidly changing market conditions and technologies\nStrong ethical standards and commitment to market integrity\nWe are committed to providing equal opportunities throughout employment including in the recruitment, training and development of employees. We prohibit discrimination in the workplace whether on grounds of gender, marital or domestic partnership status, pregnancy, carer s responsibilities, sexual orientation, gender identity, gender expression, race, color, national or ethnic origins, religious belief, disability or age.\n*Applying for this role does not amount to a job offer or create an obligation on Nomura to provide a job offer. The expression ""Nomura"" refers to Nomura Services India Private Limited together with its affiliates.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'electronic trading', 'Analytical', 'data visualization', 'Research', 'Training and Development', 'Risk management', 'SQL', 'Recruitment', 'Python']",2025-06-10 14:26:52
Azure Data lead,Infyjob Technology Services,7 - 12 years,Not Disclosed,['Chennai'],"candidate must have 7 year relevant experience in this following key skills\nazure data bricks\nAzure data factory\npython\nSQL\nonly we looking immediate joiners, even only give first priority for tier one companies\n\nonly chennai loaction available\n\nRequired Candidate profile\nAzure Data Engineer, Azure Solutions Architect , azure data factory",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Databricks', 'SQL', 'Python']",2025-06-10 14:26:55
Lead Python Engineer,Creditsafe,4 - 8 years,Not Disclosed,[],"As a Lead Engineer, you will play a critical role in shaping the technical direction of our projects. You will be responsible for leading a team of developers undertaking Creditsafe s digital transformation to our cloud infrastructure on AWS. Your expertise in Data Engineering, Python and AWS will be crucial in building and maintaining high-performance, scalable, and reliable systems.\nKey Responsibilities:\nTechnical Leadership: Lead and mentor a team of engineers, providing guidance and support to ensure high-quality code and efficient project delivery.\nSoftware Design and Development: Collaborate with cross-functional teams to design and\ndevelop data-centric applications, microservices, and APIs that meet project requirements.\nAWS Infrastructure: Design, configure, and manage cloud infrastructure on AWS, including\nservices like EC2, S3, Lambda, and RDS.\nPerformance Optimization: Identify and resolve performance bottlenecks, optimize code and AWS resources to ensure scalability and reliability.\nCode Review: Conduct code reviews to ensure code quality, consistency, and adherence to\nbest practices.\nSecurity: Implement and maintain security best practices within the codebase and cloud\ninfrastructure.\nDocumentation: Create and maintain technical documentation to facilitate knowledge\nsharing and onboarding of team members.\nCollaboration: Collaborate with product managers, architects, and other stakeholders to\ndeliver high-impact software solutions.\nResearch and Innovation: Stay up to date with the latest Python, Data Engineering and AWS technologies, and propose innovative solutions that can enhance our systems.\nTroubleshooting: Investigate and resolve technical issues and outages as they arise.\nQualifications:\nBachelor's or higher degree in Computer Science, Software Engineering, or a related field.\nProven experience as a Data Engineer with a strong focus on AWS services.\nSolid experience in leading technical teams and project management.\nProficiency in Python, including deep knowledge of data engineering implementation\npatterns.\nStrong expertise in AWS services and infrastructure setup.\nFamiliarity with containerization and orchestration tools (e.g., Docker, Kubernetes) is a plus.\nExcellent problem-solving skills and the ability to troubleshoot complex technical issues.\nStrong communication and teamwork skills.\nA passion for staying updated with the latest industry trends and technologies.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'S3', 'RDS', 'project management', 'Docker', 'EC2', 'data engineering', 'AWS', 'Lambda', 'Kubernetes']",2025-06-10 14:26:58
Snowflake Architect,Blend360 India,8 - 13 years,Not Disclosed,['Hyderabad'],"We are looking for an experienced and hands-on Manager - Snowflake Data Engineering to lead and expand our data engineering capabilities. This role is ideal for a technically strong leader who can design scalable data architectures, manage a high-performing team, and collaborate cross-functionally to deliver reliable and secure data solutions.\nResponsibilities:\nDesign and implement robust Snowflake data warehouse architectures and ETL pipelines to support business intelligence and advanced analytics use cases.\nLead and mentor a team of data engineers, ensuring high-quality and timely delivery of projects.\nCollaborate closely with data analysts, data scientists, and business stakeholders to understand data needs and design effective data models.\nDevelop, document, and enforce best practices for Snowflake architecture, data modeling, performance optimization, and ETL processes.\nOwn the optimization of Snowflake environments to ensure low-latency and high-availability data access.\nDrive process improvements, evaluate emerging tools, and continuously enhance our data engineering infrastructure.\nEnsure data pipelines are built with high levels of accuracy, completeness, and security, in compliance with data privacy regulations (GDPR, CCPA, etc.).\nPartner with cloud engineering and DevOps teams to integrate data solutions seamlessly within the AWS ecosystem.\nParticipate in capacity planning, budgeting, and resource allocation for the data engineering function.\n\n\nBachelor s degree in Computer Science, Information Technology, or a related field.\n9+ years of overall experience in cloud-based data engineering, with at least 4 years of hands-on experience in Snowflake.",Industry Type: Industrial Automation,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data modeling', 'data security', 'data governance', 'Engineering Manager', 'Business intelligence', 'Information technology', 'SQL', 'Python', 'Capacity planning']",2025-06-10 14:27:00
SDE II - Full stack,Affle,2 - 6 years,Not Disclosed,"['Mumbai', 'Gurugram', 'Bengaluru']","As a Software Engineer at RevX, you will take part in the implementation and delivery of robust features/products. You will work closely with software engineers, data engineers, engineering managers and product managers to build and ship new features that optimize customer engagement, and operational efficiency, and drive growth.\nRequired Experience/Skills:\nRelevant work experience of 4+ years of software development role\nStrong in Data Structures, Algorithms, and Problem-Solving Skills\nExperience working on Java, J2EE, and Spring Boot applications.\nExperience in programming with Messaging (Kafka, RabbitMQ), Caching (Ehcache, Radis)\nExperience in programming with Mysql, Elastic search and Bigquery\nExperience working knowledge of Angular8+ (TypeScript)\nExperience with HTML5, CSS3, JavaScript ECMA5/6, and UI Frameworks.\nWorking Knowledge of Unix environment (shell, scripting)\nStrong experience in Agile and Test-Driven Methodologies\n""Self-starter"" attitude and the ability to make decisions independently\nProficient understanding of deployment process of client-side application\nProficient understanding of code versioning tools, such as Git or SVN\nHands-on experience in responsive design\nMajor Responsibilities:\nDesign and build complex systems that can scale rapidly with little maintenance.\nDesign and implement effective service/product interfaces.\nAble to lead and successfully complete software projects without major guidance from a manager/lead.\nProvide technical support for many applications within the technology portfolio.\nRespond to and troubleshoot complex problems quickly, efficiently, and effectively.\nHandle multiple competing priorities in an agile, fast-paced environment.\nCreate and maintain documentation for your projects.\nEducation:\nBachelor of Engineering or similar degree from any reputed University.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['algorithms', 'css', 'deployment process', 'code versioning tools', 'ehcache', 'elastic search', 'git', 'java', 'ui', 'j2ee', 'data structures', 'html', 'shell scripting', 'mysql', 'bigquery', 'typescript', 'software testing', 'svn', 'rabbitmq', 'javascript', 'driving', 'spring boot', 'kafka', 'full stack', 'agile', 'unix']",2025-06-10 14:27:03
Data QA - Python SQL Automation,Rarr Technologies,3 - 8 years,Not Disclosed,[],"Strong quality assurance experience with hands on experience in SQL, Python (postgres experience preferred) - Min 6-10 years\nStrong experience in performing manual and automation testing in validating data sources\nStrong ETL experience in any tool\nStrong demonstrated experience to work closely with data engineering functions to ensure a sustainable test approach\nStrong communication skills\nPython, Sql, Api, Automation, Selenium, Testing",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['QA', 'Automation', 'Automation testing', 'Manager Quality Assurance', 'selenium testing', 'MIN', 'Manual', 'SQL', 'Python', 'Testing']",2025-06-10 14:27:05
Power BI Reporting/ Analytics - AEG,Sutherland Global Services Inc,10 - 15 years,Not Disclosed,['Hyderabad'],"Role Overview:\nWe are looking for a motivated Power BI Reporting / Analytics Specialist to join our team and help transform raw data into actionable insights within the context of SAP implementations. The ideal candidate will have 10+ years of experience in working with SAP, as well as hands-on experience in creating reports, dashboards, and analytics using Power BI. In this role, you will collaborate with SAP functional teams to gather data from various SAP modules and develop business intelligence solutions that empower data-driven decision-making.\nKey Responsibilities:",,,,"['Performance tuning', 'Data analysis', 'Data structures', 'Agile methodology', 'Business intelligence', 'Analytics', 'Monitoring', 'Reporting tools', 'SQL', 'Data extraction']",2025-06-10 14:27:08
AI INTERNSHIP Developer,ATS Services,3 months duration,Unpaid,['Faridabad'],"We are seeking a motivated and analytical AI Intern (Developer) to join our team for a 6-month internship. This is a unique opportunity to gain hands-on experience working on real-world AI solutions in fintech and insurance two of the most data-rich and fast-evolving sectors. The ideal candidate should have strong expertise in Postgres database/Oracle, Express.js, React.js, and Node.js , with a focus on both front-end and back-end development.\nResponsibilities:\nAssist in developing machine learning models for use cases.\nPerform data cleaning, analysis, and feature engineering.\nWork with structured and unstructured financial/insurance, etc. data\nCollaborate with data engineers and domain experts to deploy AI models.\nResearch industry trends in AI for fintech and insurance tech\nDocument experiments, model results, and insights\nConduct unit and integration testing to maintain code quality.\nCollaborate with cross-functional teams to deliver seamless user experiences.\nTroubleshoot and resolve technical issues.\nManage and guide the development team effectively.\nRequirements:\nStrong Python programming skills\nFamiliarity with ML libraries: scikit-learn, XGBoost, TensorFlow/PyTorch\nUnderstanding of supervised and unsupervised learning techniques\nGood with data wrangling using Pandas, NumPy, SQL\nExperience with NLP or OCR tools\nStrong analytical and problem-solving skills\nStrong proficiency in MERN Stack (MongoDB, Express.js, React.js, Node.js) .\nExperience in REST API development and integration .\nAbility to manage time efficiently and meet deadlines.\nTeam player with excellent communication skills .\nDelivery-focused mindset with the ability to work under minimal supervision.\nCompany: ATS Services is a leading service provider in the Indian BFSI Industry. We inculcate a work environment that promotes diversity, embraces change and provides leadership opportunities. We provide consulting, services and IT solutions to our customers in the areas of credit & risk, risk analytics, development of scorecards and segmentation models, collections for retail and B2B Customers, Insurance retention and a variety of IT solutions in the areas of credit MIS, business intelligence and business process consulting and automation.\nQualification:\nTech Computer/IT Applications\nExperience Fresher\nProficiency in Languages: English and Hindi\nDuration\n6 months\nBenefits:\nStipend +PF+ Insurance.\n5 Days working\nDay Shift\nLocation Faridabad work from office\nIf you are interested in above position, you can apply and appear for the interview, or you may get in touch with us as undersigned,\n\n\nOur Location\n7th Floor, Tower B, Vatika Mindscapes,\nSector 27-D, Faridabad, Haryana - 121003\n+91-129-6612001\ncontactus@servicesats.com\n\n\nDrop Us A Line\nYour name\nYour email\nSubject\nYour message (optional)",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Front end', 'Bfsi', 'Risk analytics', 'MIS', 'Analytical', 'Business intelligence', 'Internship', 'SQL', 'Python']",2025-06-10 14:27:09
Technical Lead,WebMD,9 - 14 years,Not Disclosed,['Navi Mumbai'],"Position: Tech Lead No. of Positions: 1\nAbout Company:\nHeadquartered in El Segundo, Calif., Internet Brands is a fully integrated online media and software services organization focused on four high-value vertical categories: Health, Automotive, Legal, and Home/Travel. The company's award- winning consumer websites lead their categories and serve more than 250 million monthly visitors, while a full range of web presence offerings has established deep, long-term relationships with SMB and enterprise clients. Internet Brands' powerful, proprietary operating platform provides the flexibility and scalability to fuel the company's continued growth. Internet Brands is a portfolio company of KKR and Temasek.\nWebMD Health Corp., an Internet Brands Company, is the leading provider of health information services, serving patients, physicians, health care professionals, employers, and health plans through our public and private online portals, mobile platforms, and health-focused publications. The WebMD Health Network includes WebMD Health, Medscape, Jobson Healthcare Information, prIME Oncology, MediQuality, Frontline, QxMD, Vitals Consumer Services, MedicineNet, eMedicineHealth, RxList, OnHealth, Medscape Education, and other owned WebMD sites. WebMD, Medscape, CME Circle®, Medpulse®, eMedicine®, MedicineNet®, ®, and RxList® are among the trademarks of WebMD Health Corp. or its subsidiaries.\n\n\n\nFor Company details, visit our website: /\nB.E. Computer Science/IT degree (or any other engineering discipline)\n2 PM to 11 PM IST\n8+ years",,,,"['Data Warehousing', 'ETL', 'SQL']",2025-06-10 14:27:12
F2F Interview | Chennai | Teradata | MNC !!,India's Largest IT service Provider,5 - 10 years,15-25 Lacs P.A.,['Chennai'],"Warm welcome from SP Staffing Services!\n\nReaching out to you regarding permanent opportunity !!\nJob Description:\nExp: 5-12 yrs\nLocation: Chennai\nSkill: Teradata Developer\n\nDesired Skill Sets:\n\nExperience in developing data models and data pipelines in Teradata\nAdvanced SQL skills, including use of derived tables, unions, multi-table inner/outer joins\nHands on with Teradata Sql and utilities like Bteq/Fastload/Multiload/TPT etc.\nWorked with front end reporting teams in defining reporting requirements, develop complex summarized views, harmonized tables to be used for reporting\nGood understanding of Teradata architecture (Staging, foundation and reporting)\nIdentifying and eliminating spool spaces, skewness issues\nCarry out monitoring, tuning, and database performance analysis\nUnderstanding various indexes and using appropriately in the queries\nExperience in end to end implementation of data-warehousing projects\nExperienced in UNIX shell scripts to write wrapper scripts to call ETL Jobs\n\nInterested can share your resume to sangeetha.spstaffing@gmail.com with below inline details.\n\nFull Name as per PAN:\nMobile No:\nAlt No/ Whatsapp No:\nTotal Exp:\nRelevant Exp in Teradata Development:\nRel Exp in Bteq/Fastload/Multiload/TPT:\nRel Exp in Unix:\nCurrent CTC:\nExpected CTC:\nNotice Period (Official):\nNotice Period (Negotiable)/Reason:\nDate of Birth:\nPAN number:\nReason for Job Change:\nOffer in Pipeline (Current Status):\nAvailability for F2F interview on 14th June, Saturday between 9 AM- 12 PM(plz mention time):\nCurrent Res Location:\nPreferred Job Location:\nWhether educational % in 10th std, 12th std, UG is all above 50%?\nDo you have any gaps in between your education or Career?\nIf having gap, please mention the duration in months/year:",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Bteq', 'Teradata', 'Multiload', 'MLOAD', 'Fastload', 'Fastexport', 'Tpt', 'TPUMP', 'Teradata SQL']",2025-06-10 14:27:15
Senior Data Engineer - Contract - Onsite,Analyticorex,4 - 9 years,15-25 Lacs P.A.,['Hyderabad'],"We are seeking a Senior Data Engineer to support a high-impact data platform engagement for a leading utility company in the USA. This 5-day on-site contract role offers an excellent growth opportunity for professionals experienced in developing scalable data pipelines and distributed data systems. The ideal candidate brings deep proficiency in Python, SQL, and PySpark, along with proven experience in ETL development, data lakes, and big data frameworks.\nKey Responsibilities\nDesign, build, and maintain robust and scalable data pipelines for ingesting and transforming large volumes of structured and semi-structured data.\nEnhance platform automation, orchestration, monitoring, and alerting to improve system reliability and developer efficiency.\nImplement end-to-end ETL workflows and support data lake architecture and big data processing.\nPartner with engineering, product, and business teams to deliver high-quality, production-ready datasets and services.\nContinuously identify and implement improvements for pipeline efficiency, automation, and usability.\nSupport platform users by providing technical assistance and clear documentation.\nDefine and maintain best practices for data quality, observability, and operational excellence.\nRequired Qualifications\n4+ years of professional experience in data engineering.\nExpert-level knowledge of Python, SQL, and PySpark.\nSolid experience in building data pipelines, ETL development, and working with data lakes and big data frameworks.\nDemonstrated ability to collaborate with cross-functional teams in a distributed engineering environment.\nHands-on experience with version control and CI/CD workflows (Git, GitLab, Bitbucket, etc.).\nFamiliarity with platform monitoring and alerting best practices.\nNice to Have\nExperience with Palantir Foundry or similar enterprise data platforms.\nKnowledge of cloud-based data architectures and infrastructure-as-code.\nWhy Join\nContribute to a critical data transformation effort for a major US utility provider.\nWork in a fast-paced environment with opportunities for career growth and development.\nEngage in meaningful, hands-on engineering with modern data infrastructure and tooling.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Temporary/Contractual","['Pyspark', 'python', 'Sql', 'Spark']",2025-06-10 14:27:17
Business_Analyst_DM,Renew,7 - 11 years,Not Disclosed,['Haryana'],"About Company\nJob Description\n  Manage and guide Business Analysts and cross functional teams\nWeekly cadenace with Business for requirement gathering and project updates\nUnderstand Business problems and identify constraints\nDesign digital and advance analytics solutions\nImplement solution with understanding of end-to-end architecture\nIdentify opportunities for implementation of new use cases\nEnsure ReD targets are met and delivered on time\nEnsure documentation of Use Cases\nQualification:\nEducation - Engineering (Electrical/Electronics) + MBA\n6 to 10 yrs relevant exp\nExp in Power Sector/Renewable energy/ Storage/Hydro/RTC power/Power Trading\nAnalytical approach with focus on solution delivery\nHandle multiple projects (intra and inter-department)\nKnowledge of Power markets is a must\nParticipated in some Digital transformation/enablement exercise in organization\nBasic understanding of Data Scientists & Data engineering roles\nExp with PowerBI, Tableau, JIRA would be a plus",Industry Type: Power,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['digital', 'project management', 'python', 'analytical', 'documentation', 'program management', 'power bi', 'business analysis', 'solution delivery', 'data warehousing', 'transformation', 'data engineering', 'sql', 'analytics', 'tableau', 'requirement gathering', 'pmp', 'use cases', 'agile', 'digital transformation', 'jira']",2025-06-10 14:27:20
Scrum Master,BriskWin IT (BWIT),7 - 12 years,Not Disclosed,['Hyderabad'],"Act as Scrum Master for Agile teams delivering data and analytics solutions for manufacturing and supply chain operations.\nWork closely with Product Owners to align on business priorities, maintain a clear and actionable backlog, and ensure stakeholder needs are met.\nFacilitate core Agile ceremonies: Sprint Planning, Daily Standups, Backlog Refinement, Reviews, and Retrospectives.\nGuide the team through data-focused sprints, including work on ingestion, transformation, integration, and reporting.\nTrack progress, remove blockers, and drive continuous improvement in team performance and delivery.\nCollaborate with data engineers, analysts, architects, and business teams to ensure high-quality, end-to-end solutions.\nPromote Agile best practices across platforms like SAP ECC, IBP, HANA, BOBJ, Databricks, and Tableau.\nMonitor and share Agile metrics (e. g. , velocity, burn-down) to keep teams and stakeholders aligned.\nSupport team capacity planning, identify bottlenecks early, and help the team stay focused and accountable.\nFoster a culture of collaboration, adaptability, and frequent customer feedback to ensure business value is delivered in every sprint.\nGuide the team to continuously break down efforts to smaller components. Smaller workpieces result in better flow. Having 8 stories/tasks of day each is better than having 1 story/task of 4 days.\nGuide the team to always provide clarity on the stories/tasks by using detailed descriptions and explicit acceptance criteria.\nBring the team s focus in the daily standup meetings to completing things instead of working on things.\nJob Location: Remote",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['SAP ECC', 'PDF', 'Focus', 'Supply chain operations', 'Agile', 'Scrum', 'Continuous improvement', 'Analytics', 'Monitoring', 'Capacity planning']",2025-06-10 14:27:23
Data Analyst,Talent Hire It Solutions,5 - 10 years,Not Disclosed,"['Kochi', 'Thiruvananthapuram']","Collaborate with business stakeholders to understand data needs and translate them into analytical\nrequirements.\nAnalyze large datasets to uncover trends, patterns, and actionable insights.\nDesign and build dashboards and reports using Power BI.\nPerform ad-hoc analysis and develop data-driven narratives to support decision-making.\nEnsure data accuracy, consistency, and integrity through data validation and quality checks.\nBuild and maintain SQL queries, views, and data models for reporting purposes.\nCommunicate findings clearly through presentations, visualizations, and written summaries.\nPartner with data engineers and architects to improve data pipelines and architecture.\nContribute to the definition of KPIs, metrics, and data governance standards.\n\nJob Specification / Skills and Competencies\nBachelors or Master’s degree in Statistics, Mathematics, Computer Science,\nEconomics, or a related field.\n5+ years of experience in a data analyst or business intelligence role.\nAdvanced proficiency in SQL and experience working with relational databases (e.g.,\nSQL Server, Redshift, Snowflake).\n\nJob Description\n\n2\n\nHands-on experience in Power BI.\nProficiency in Python, Excel and data storytelling.\nUnderstanding of data modelling, ETL concepts, and basic data architecture.\nStrong analytical thinking and problem-solving skills.\nExcellent communication and stakeholder management skills\nTo adhere to the Information Security Management policies and procedures.\n\nSoft Skills Required\nMust be a good team player with good communication skills\nMust have good presentation skills\nMust be a pro-active problem solver and a leader by self\nManage & nurture a team of data engineersRole & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SQL', 'Power BI', 'Amazon Athena', 'Python']",2025-06-10 14:27:25
Vertex AI Developer,Swits Digital,3 - 5 years,Not Disclosed,['Chennai'],"Job Title: Vertex AI Developer\nExperience: 3 - 5 Years\nLocation: Chennai / Hyderabad\nNotice Period: Immediate Joiners Preferred\nEmployment Type: Full-Time\nJob Description:\nWe are looking for a passionate and skilled Vertex AI Developer with hands-on experience in Google Cloud s Vertex AI , Python , Machine Learning (ML) , and Generative AI . The ideal candidate will play a key role in designing, developing, and deploying scalable ML/GenAI models and workflows using GCP Vertex AI services.\nKey Responsibilities:\nDevelop, deploy, and manage ML/GenAI models using Vertex AI on Google Cloud Platform (GCP) .\nWork with structured and unstructured data to create and train predictive and generative models.\nIntegrate AI models into scalable applications using Python APIs and GCP components.\nCollaborate with data scientists, ML engineers, and DevOps teams to implement end-to-end ML pipelines.\nMonitor model performance and iterate on improvements as necessary.\nDocument solutions, best practices, and technical decisions.\nMandatory Skills:\n3 to 5 years of experience in Machine Learning/AI Development .\nStrong proficiency in Python and ML libraries such as TensorFlow, PyTorch, Scikit-learn .\nHands-on experience with Vertex AI including AutoML, Pipelines, Model Deployment, and Monitoring.\nExperience in GenAI frameworks (e.g., PaLM, LangChain, LLMOps).\nProficiency in using Google Cloud Platform tools and services.\nStrong understanding of MLOps, CI/CD, and model lifecycle management.\nPreferred Skills:\nExperience with containerization tools like Docker , orchestration tools like Kubernetes .\nExposure to Natural Language Processing (NLP) and LLMs .\nFamiliarity with data engineering concepts (BigQuery, Dataflow).",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['orchestration', 'GCP', 'devops', 'Machine learning', 'Cloud', 'Deployment', 'Natural language processing', 'Management', 'Monitoring', 'Python']",2025-06-10 14:27:27
Master Data Analyst - Reporting,Heinz,3 - 5 years,Not Disclosed,['Ahmedabad'],"Responsibilities\nBe a team player\nDedicated to Source (Catalyst & Keystone) to GFIN (Target) Reconciliations, GFIN vs HFM Reconciliations, and research discrepancies between the systems\nWork with Data Engineering and Power BI developers to create recon dashboards and general maintenance of dashboards\nCreate and update process documentation\nSupport Internal Audit requests\nParticipate in ad hoc project requests\nManagement of Change Champions for MDG - to ensure the smooth flowing of new Data Objects via the MDG process\nGovernance of new Data Objects, and Values\nManagement of effective input capture and central overview data files\nExperience\n3 to 5 years experience\nProactive, self-starter - driving agenda and opportunities for improvement\nSimplification in comms and actions necessary to take to solve issues\nQuestion and call out when data look unusual\nAutonomy to review and research on their own and support findings\nExcel savvy - experience working with frequent and large data sets\nSome systems experience preferred (SAP, HFM, Oracle, Snowflake, Power BI)\nExperience of Data Governance and systems related to DG\nHas proven Networks in KH - knows who to speak to\nWorking knowledge of products, and how these fit together with supply elements\nUnderstanding of data rules by functional areas\nConfidence in using applications, some systems experience preferred - SAP, HFM, Oracle, Snowflake, Power BI\nHours - Hybrid\nUS hours - during close week\nHybrid hours - same schedule as current team during non close week\nLocation(s) Ahmedabad - Mondeal Heights - GBS Center",Industry Type: Food Processing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Manager Internal Audit', 'SAP', 'Excel', 'Process documentation', 'data governance', 'power bi', 'Data Analyst', 'Research', 'Management', 'Oracle']",2025-06-10 14:27:29
Lead Data Scientist - Media Mix Modelling (MMM),Blend360 India,7 - 10 years,Not Disclosed,['Hyderabad'],"We are seeking a highly skilled Data Scientist/Analyst with expertise in Media Mix Modeling (MMM) to join our dynamic analytics team. The ideal candidate will play a critical role in providing actionable insights and optimizing marketing spend across various channels by leveraging statistical models and data-driven techniques.\nKey Responsibilities:\nDevelop and implement Media Mix Models to optimize marketing spend across different channels (e.g., TV, digital, radio, print, etc.).\nAnalyze historical data to understand the impact of marketing efforts and determine the effectiveness of different media channels.\nCollaborate with marketing and business teams to translate business objectives into quantitative analyses and actionable insights.\nBuild predictive models to forecast the impact of future marketing activities and recommend budget allocation.\nPresent and communicate complex findings in a clear, concise, and actionable manner to both technical and non-technical stakeholders.\nPerform deep-dive analyses of marketing campaigns and customer data to identify trends, opportunities, and areas for improvement.\nEnsure data integrity, accuracy, and consistency in all analyses and models.\nStay up-to-date with the latest trends and advancements in media mix modeling, marketing analytics, and data science.\nCollaborate with cross-functional teams including Data Engineering, Marketing, and Business Intelligence to ensure seamless data flow and integration.\nCreate and maintain documentation for all models, methodologies, and analysis processes.\n\n\nBachelor s or Master s degree in Data Science, Statistics, Economics, Mathematics, or a related field.\nProven experience (6+ years) working in Media Mix Modeling (MMM) and/or marketing analytics.\nStrong proficiency in stati",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Analytical', 'Social media', 'Consulting', 'data integrity', 'Regression analysis', 'Business intelligence', 'CRM', 'SQL', 'Python']",2025-06-10 14:27:31
Associate Lead - Marketing,Ignitho,2 - 3 years,Not Disclosed,['Chennai'],"Position Details: Associate Lead Marketing (277) Location: Chennai, Tamil nadu Openings: 1 Salary Range:\nDescription:\nJob Title: Associate Lead - Marketing\nLocation: Chennai\nExperience Level: 2-3 Years\nJob Type: Full-Time\nAbout the Company\nIgnitho Inc. is a fast-growing AI and data engineering services company with a global footprint, including offices in the US, UK, India, and Costa Rica. We partner with leading enterprises and Fortune 500 companies to deliver high-impact Data and AI solutions.\nWe are a certified partner of Microsoft, DOMO, and H2O.ai, and have earned analyst mentions from ISG, HFS, and TechMarketView a testament to our market credibility and innovation.\nKey Responsibilities\nDevelop and execute data-driven marketing strategies aligned with business goals in the B2B IT and AI services domain\nOwn and optimize go-to-market (GTM) plans for service offerings and campaigns\nCraft messaging, positioning, and value propositions that resonate with CIOs, CDOs, and enterprise decision-makers\nDrive lead generation through performance marketing, email campaigns, LinkedIn outreach, webinars, and events\nLeverage strategic partnerships with Microsoft, DOMO, and H2O.ai for co-branded campaigns and thought leadership\nBuild brand credibility using analyst mentions from ISG, HFS & TechMarketView in outreach and positioning\nCollaborate with Sales to ensure marketing-qualified leads (MQLs) align with business goals\nMeasure and report on campaign performance and ROI, using insights to improve strategy\nConduct market and competitor analysis to spot trends, opportunities, and differentiation tactics\nRequirements\nRequirements\n2 3 years of B2B marketing experience, preferably in IT Services, SaaS, or AI/Data domains\nProven track record in digital marketing, lead generation, and campaign execution with measurable impact\nStrong communication and storytelling abilities; hands-on experience with tools like HubSpot, LinkedIn Ads, and Google Analytics\nNice to Have\nExposure to global markets such as the US or UK, or experience with enterprise-focused marketing\nFamiliarity with CRM and marketing automation tools\nBasic understanding of AI/ML and Data Engineering concepts",Industry Type: Analytics / KPO / Research,Department: Marketing & Communication,"Employment Type: Full Time, Permanent","['IT services', 'Engineering services', 'Lead generation', 'Google Analytics', 'Strategic partnerships', 'digital marketing lead', 'Automation tools', 'microsoft', 'CRM', 'marketing automation']",2025-06-10 14:27:33
Power BI Developer,Leading Client,4 - 6 years,Not Disclosed,['Pune'],"Job Summary :\n\nWe are seeking a Power BI Developer with 4+ years of experience, who has strong skills in SQL development, data modeling, and dashboard/report creation.\n\nThis role is onsite in Pune and will require close collaboration with data engineers, business analysts, and internal stakeholders to deliver business-critical insights and optimized reports.\n\nKey Responsibilities :\n\n- Design, develop, and maintain interactive Power BI dashboards and reports using business and user requirements.\n\n- Build optimized SQL queries, views, and stored procedures to support reporting needs.\n\n- Develop and maintain efficient data models, DAX measures, and Power Query transformations.\n\n- Connect Power BI to diverse data sources such as SQL Server, Excel, SharePoint, APIs, and ensure high performance and reliability.\n\n- Create custom visualizations and drill-downs to aid user decision-making.\n\n- Ensure data quality, consistency, and security in all Power BI deliverables.\n\n- Implement Row-Level Security (RLS) and manage workspace roles.\n\n- Coordinate with business users to gather requirements, perform UAT, and provide production support.\n\n- Collaborate with IT to manage deployment and scheduling of reports via Power BI Service.\n\nRequired Skills :\n\n- 4+ years of experience in Power BI Development.\n\n- Expert in SQL - writing complex queries, joins, views, and stored procedures.\n\n- Strong understanding of data modeling concepts (star/snowflake schema).\n\n- Hands-on experience with DAX, Power Query (M Language).\n\n- Experience in integrating Power BI with SQL Server, Excel, and SharePoint.\n\n- Good understanding of ETL/ELT processes and data transformation logic.\n\n- Experience working with Power BI Service - including workspace management, gateways, RLS.\n\n- Excellent communication and stakeholder interaction skills.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power BI', 'Reporting Analytics', 'DAX Power BI', 'Dashboard Design', 'Data Modeling', 'ETL', 'SQL', 'Power Query M']",2025-06-10 14:27:35
Data Architect- Snowflake & DBT,InfoCepts,3 - 5 years,Not Disclosed,['Chennai'],"InfoCepts is looking for Data Architect- Snowflake & DBT to join our dynamic team and embark on a rewarding career journey\nDesign and Development: Create and implement data warehouse solutions using Snowflake, including data modeling, schema design, and ETL (Extract, Transform, Load) processes\nPerformance Optimization: Optimize queries, performance-tune databases, and ensure efficient use of Snowflake resources for faster data retrieval and processing\nData Integration: Integrate data from various sources, ensuring compatibility, consistency, and accuracy",,,,"['snowflake', 'python', 'hipaa', 'data security', 'data warehousing', 'data architecture', 'sql querying', 'relational databases', 'sql', 'gdpr', 'database design', 'database management', 'data modeling', 'etl tool', 'data governance', 'data warehousing concepts', 'etl', 'communication skills']",2025-06-10 14:27:38
Neo4j GraphDB Developer,Leading Client,1 - 4 years,Not Disclosed,"['Mumbai', 'Pune', 'Chennai']","Graph data Engineer required for a complex Supplier Chain Project.\n\nKey required Skills\nGraph data modelling (Experience with graph data models (LPG, RDF) and graph language (Cypher), exposure to various graph data modelling techniques)\nExperience with neo4j Aura, Optimizing complex queries.\nExperience with GCP stacks like BigQuery, GCS, Dataproc. Experience in PySpark, SparkSQL is desirable.\nExperience in exposing Graph data to visualisation tools such as Neo Dash, Tableau and PowerBI\nThe Expertise You Have:\nBachelors or Masters Degree in a technology related field (e.g. Engineering, Computer Science, etc.).\nDemonstrable experience in implementing data solutions in Graph DB space.\nHands-on experience with graph databases (Neo4j(Preferred), or any other).\nExperience Tuning Graph databases.\nUnderstanding of graph data model paradigms (LPG, RDF) and graph language, hands-on experience with Cypher is required.\nSolid understanding of graph data modelling, graph schema development, graph data design.\nRelational databases experience, hands-on SQL experience is required.Desirable (Optional) skills:\nData ingestion technologies (ETL/ELT), Messaging/Streaming Technologies (GCP data fusion, Kinesis/Kafka), API and in-memory technologies.\nUnderstanding of developing highly scalable distributed systems using Open-source technologies.\nExperience in Supply Chain Data is desirable but not essential.\nLocation:Pune, Mumbai, Chennai, Bangalore, Hyderabad",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GraphDB', 'schema', 'hive', 'css', 'pyspark', 'kinesis', 'jquery', 'sql', 'gen', 'java', 'computer science', 'gcp', 'spark', 'data ingestion', 'html', 'bigquery', 'api', 'hadoop', 'etl', 'lpg', 'supply chain', 'power bi', 'relational databases', 'elt', 'javascript', 'dataproc', 'fusion', 'tableau', 'neo4j', 'rdf', 'kafka', 'dash', 'db', 'graph databases']",2025-06-10 14:27:41
Data Modeler - Banking Domain,Emperen Technologies,7 - 9 years,Not Disclosed,['Delhi / NCR'],"Key Responsibilities :\n\nDesign and develop data models to support business intelligence and analytics solutions.\nWork with Erwin or Erwin Studio to create, manage, and optimize conceptual, logical, and physical data models.\nImplement Dimensional Modelling techniques for data warehousing and reporting.\nCollaborate with business analysts, data engineers, and stakeholders to gather and understand data requirements.\nEnsure data integrity, consistency, and compliance with Banking domain standards.\nWork with Snowflake to develop and optimize cloud-based data models.\nWrite and execute complex SQL queries for data analysis and validation.\nIdentify and resolve data quality issues and inconsistencies.\nRequired Skills & Qualifications :\n7+ years of experience in Data Modelling and Data Analysis.\nStrong expertise in Erwin or Erwin Studio for data modeling.\nExperience with Dimensional Modelling and Data Warehousing (DWH) concepts.\nProficiency in Snowflake and SQL for data management and querying.\nPrevious experience in the Banking domain is mandatory.\nStrong analytical and problem-solving skills.\nAbility to work independently in a remote environment.\nExcellent verbal and written communication skills.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Modeling', 'Business Intelligence', 'Dimensional Modeling', 'Snowflake DB', 'Data Warehousing', 'Data Analytics', 'Erwin', 'SQL']",2025-06-10 14:27:42
Data Modeler - Banking Domain,Emperen Technologies,7 - 9 years,Not Disclosed,['Surat'],"Key Responsibilities :\n\nDesign and develop data models to support business intelligence and analytics solutions.\nWork with Erwin or Erwin Studio to create, manage, and optimize conceptual, logical, and physical data models.\nImplement Dimensional Modelling techniques for data warehousing and reporting.\nCollaborate with business analysts, data engineers, and stakeholders to gather and understand data requirements.\nEnsure data integrity, consistency, and compliance with Banking domain standards.\nWork with Snowflake to develop and optimize cloud-based data models.\nWrite and execute complex SQL queries for data analysis and validation.\nIdentify and resolve data quality issues and inconsistencies.\nRequired Skills & Qualifications :\n7+ years of experience in Data Modelling and Data Analysis.\nStrong expertise in Erwin or Erwin Studio for data modeling.\nExperience with Dimensional Modelling and Data Warehousing (DWH) concepts.\nProficiency in Snowflake and SQL for data management and querying.\nPrevious experience in the Banking domain is mandatory.\nStrong analytical and problem-solving skills.\nAbility to work independently in a remote environment.\nExcellent verbal and written communication skills.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Modeling', 'Business Intelligence', 'Dimensional Modeling', 'Snowflake DB', 'Data Warehousing', 'Data Analytics', 'Erwin', 'SQL']",2025-06-10 14:27:44
Data Modeler - Banking Domain,Emperen Technologies,7 - 9 years,Not Disclosed,['Bhopal'],"Key Responsibilities :\n\nDesign and develop data models to support business intelligence and analytics solutions.\nWork with Erwin or Erwin Studio to create, manage, and optimize conceptual, logical, and physical data models.\nImplement Dimensional Modelling techniques for data warehousing and reporting.\nCollaborate with business analysts, data engineers, and stakeholders to gather and understand data requirements.\nEnsure data integrity, consistency, and compliance with Banking domain standards.\nWork with Snowflake to develop and optimize cloud-based data models.\nWrite and execute complex SQL queries for data analysis and validation.\nIdentify and resolve data quality issues and inconsistencies.\nRequired Skills & Qualifications :\n7+ years of experience in Data Modelling and Data Analysis.\nStrong expertise in Erwin or Erwin Studio for data modeling.\nExperience with Dimensional Modelling and Data Warehousing (DWH) concepts.\nProficiency in Snowflake and SQL for data management and querying.\nPrevious experience in the Banking domain is mandatory.\nStrong analytical and problem-solving skills.\nAbility to work independently in a remote environment.\nExcellent verbal and written communication skills.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Modeling', 'Business Intelligence', 'Dimensional Modeling', 'Snowflake DB', 'Data Warehousing', 'Data Analytics', 'Erwin', 'SQL']",2025-06-10 14:27:46
