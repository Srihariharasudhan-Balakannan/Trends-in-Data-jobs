job_title,company,experience,salary,locations,description,industry,department,employment_type,skills,scraped_at
Data Engineer - Python/SQL,Wipro,3 - 5 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\n\n\n\nDo\n1. Instrumental in understanding the requirements and design of the product/ software\nDevelop software solutions by studying information needs, studying systems flow, data usage and work processes\nInvestigating problem areas followed by the software development life cycle\nFacilitate root cause analysis of the system issues and problem statement\nIdentify ideas to improve system performance and impact availability\nAnalyze client requirements and convert requirements to feasible design\nCollaborate with functional teams or systems analysts who carry out the detailed investigation into software requirements\nConferring with project managers to obtain information on software capabilities\n\n\n\n2. Perform coding and ensure optimal software/ module development\nDetermine operational feasibility by evaluating analysis, problem definition, requirements, software development and proposed software\nDevelop and automate processes for software validation by setting up and designing test cases/scenarios/usage cases, and executing these cases\nModifying software to fix errors, adapt it to new hardware, improve its performance, or upgrade interfaces.\nAnalyzing information to recommend and plan the installation of new systems or modifications of an existing system\nEnsuring that code is error free or has no bugs and test failure\nPreparing reports on programming project specifications, activities and status\nEnsure all the codes are raised as per the norm defined for project / program / account with clear description and replication patterns\nCompile timely, comprehensive and accurate documentation and reports as requested\nCoordinating with the team on daily project status and progress and documenting it\nProviding feedback on usability and serviceability, trace the result to quality risk and report it to concerned stakeholders\n\n\n\n3. Status Reporting and Customer Focus on an ongoing basis with respect to project and its execution\nCapturing all the requirements and clarifications from the client for better quality work\nTaking feedback on the regular basis to ensure smooth and on time delivery\nParticipating in continuing education and training to remain current on best practices, learn new programming languages, and better assist other team members.\nConsulting with engineering staff to evaluate software-hardware interfaces and develop specifications and performance requirements\nDocument and demonstrate solutions by developing documentation, flowcharts, layouts, diagrams, charts, code comments and clear code\nDocumenting very necessary details and reports in a formal way for proper understanding of software from client proposal to implementation\nEnsure good quality of interaction with customer w.r.t. e-mail content, fault report tracking, voice calls, business etiquette etc\nTimely Response to customer requests and no instances of complaints either internally or externally\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure\n1.Continuous Integration, Deployment & Monitoring of Software100% error free on boarding & implementation, throughput %, Adherence to the schedule/ release plan\n2.Quality & CSATOn-Time Delivery, Manage software, Troubleshoot queries,Customer experience, completion of assigned certifications for skill upgradation\n3.MIS & Reporting100% on time MIS & report generation\nMandatory Skills: Python for Insights.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'module development', 'Data Engineering', 'software development', 'software programs', 'SQL']",2025-06-11 05:20:54
Data Engineer,Visa,3 - 8 years,Not Disclosed,['Bengaluru'],"We are seeking a Data Engineer with a strong background in data engineering. This role involves managing system requirements, design, development, integration, quality assurance, implementation, and maintenance of corporate applications.\nWork with product owners, business stakeholders and internal teams to understand business requirements and desired business outcomes.\nAssist in scoping and designing analytic data assets, implementing modelled attributes and contributing to brainstorming sessions.\nBuild and maintain a robust data engineering process to develop and implement self-serve data and tools for Visa s product management teams and data scientists.\nFind opportunities to create, automate and scale repeatable analyses or build self-service tools for business users.\nExecute data engineering projects ranging from small to large either individually or as part of a project team.\nSet the benchmark in the team for good data engineering practices and assist leads and architects in solution design.\nExhibit a passion for optimizing existing solutions and making incremental improvements.\nThis is a hybrid position. Expectation of days in office will be confirmed by your hiring manager.\n\n\nBasic Qualification\n-Bachelors degree, OR 3+ years of relevant work experience\n\nPreferred Qualification\n-Minimum of 1 years experience in building data engineering pipelines.\n-Design and coding skills with Big Data technologies like Hadoop, Spark, Hive and Map reduce.\n-Mastery in Pyspark or Scala.\n-Expertise in any programming like Java or Python. Knowing OOP concepts like inheritance, polymorphism and implementing Design Patterns in programming is needed.\n-Experience with cloud platforms like AWS, GCP, or Azure is good to have.\n-Excellent problem-solving skills and ability to think critically.\n-Experience with any one ETL tool like Informatica, SSIS, Pentaho or Azure Data Factory.\n-Knowledge of successful design, and development of data driven real time and batch systems.\n-Experience in data warehousing and an expert in any one of the RDBMS like SQL Server, Oracle, etc.\n-Nice to have reporting skills on PowerBI/Tableau/QlikView.\n-Strong understanding of cloud architecture and service offerings including compute, storage, databases, networking, AI, and ML.\n-Passionate about delivering zero defect code that meets or exceeds the proposed defect SLA and have high sense of accountability for quality and timelines on deliverables.\n-Experience developing as part of Agile/Scrum team is preferred and hands on with Jira.\n-Understanding basic CI/ CD functionality and Git concepts is must.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Manager Quality Assurance', 'Networking', 'RDBMS', 'Coding', 'Informatica', 'Oracle', 'SSIS', 'SQL', 'Python']",2025-06-11 05:20:56
Data Engineer,PwC India,4 - 8 years,Not Disclosed,['Bengaluru'],"If Interested please fill the below application link : https://forms.office.com/r/Zc8wDfEGEH\n\n\nResponsibilities:\nDeliver projects integrating data flows within and across technology systems.\nLead data modeling sessions with end user groups, project stakeholders, and technology teams to produce logical and physical data models.",,,,"['Pyspark', 'Aws Cloud', 'Azure Cloud', 'Python']",2025-06-11 05:20:57
Data Engineer-Data Modeling,IBM,4 - 9 years,Not Disclosed,['Mumbai'],"As an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n4+ years of experience in data modelling, data architecture.\nProficiency in data modelling tools ERwin, IBM Infosphere Data Architect and database management systems\nFamiliarity with different data models like relational, dimensional and NoSQl databases.\nUnderstanding of business processes and how data supports business decision making.\nStrong understanding of database design principles, data warehousing concepts, and data governance practices\n\n\nPreferred technical and professional experience\nExcellent analytical and problem-solving skills with a keen attention to detail.\nAbility to work collaboratively in a team environment and manage multiple projects simultaneously.\nKnowledge of programming languages such as SQL",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['database design', 'data modeling', 'data governance', 'data warehousing concepts', 'design principles', 'ibm infosphere', 'data warehousing', 'data architecture', 'erwin', 'machine learning', 'data engineering', 'sql', 'nosql', 'database management', 'elastic search', 'splunk', 'big data']",2025-06-11 05:20:59
Data Engineer/Consultant Specialist,Hsbc,7 - 8 years,Not Disclosed,['Hyderabad'],"Some careers shine brighter than others.\nIf you re looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further.\nHSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions.\nWe are currently seeking an experienced professional to join our team in the role of Marketing Title.\nIn this role, you will:\nThis role will be responsible for:\nDevelop and support new feeds ingestion / understand the existing framework and do the development as per the business rules and requirements.\nDevelopment and maintenance of new changes / enhancements in Data Ingestion / Juniper and promoting and supporting those in the production environment within the stipulated timelines.\nNeed to get familiar with the Data Ingestion / Data Refinery / Common Data Model / Compdata frameworks quickly and contribute to the application development as soon as possible.\nMethodical and measured approach with a keen eye for attention to detail;\nAbility to work under pressure and remain calm in the face of adversity;\nAbility to collaborate, interact and engage with different business, technical and subject matter experts;\nGood, concise, written and verbal communication\nAbility to manage workload from multiple requests and to balance priorities;\nPro-active, a can do mind-set and attitude;\nGood documentation skills\n\n\n\n\n\n\n\n\n\n\nRequirements\n\n\n\nTo be successful in this role, you should meet the following requirements:\nExperience (1 = essential, 2 = very useful, 3 = nice to have):\n1. Hadoop / Hive / GCP\n2. Agile / Scrum\n3. LINUX\nTechnical skills (1 = essential, 2 = useful, 3 = nice to have):\n1. Any ETL tool\n1. Analytical trouble shooting.\n2. Hive QL\n1. On-Prem / Cloud infra knowledgeYou ll achieve more when you join HSBC.",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'Linux', 'GCP', 'Analytical', 'business rules', 'Application development', 'Refinery', 'Troubleshooting', 'ETL tool', 'Financial services']",2025-06-11 05:21:01
Data Engineer-Data Modeling,IBM,4 - 9 years,Not Disclosed,['Pune'],"As an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n4+ years of experience in data modelling, data architecture.\nProficiency in data modelling tools ERwin, IBM Infosphere Data Architect and database management systems\nFamiliarity with different data models like relational, dimensional and NoSQl databases.\nUnderstanding of business processes and how data supports business decision making.\nStrong understanding of database design principles, data warehousing concepts, and data governance practices\n\n\nPreferred technical and professional experience\nExcellent analytical and problem-solving skills with a keen attention to detail.\nAbility to work collaboratively in a team environment and manage multiple projects simultaneously.\nKnowledge of programming languages such as SQL",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['database design', 'data modeling', 'data governance', 'data warehousing concepts', 'design principles', 'ibm infosphere', 'data warehousing', 'data architecture', 'erwin', 'machine learning', 'data engineering', 'sql', 'nosql', 'database management', 'elastic search', 'splunk', 'big data']",2025-06-11 05:21:02
Data Engineer,Capgemini,3 - 5 years,Not Disclosed,['Pune'],"Capgemini Invent\n\nCapgemini Invent is the digital innovation, consulting and transformation brand of the Capgemini Group, a global business line that combines market leading expertise in strategy, technology, data science and creative design, to help CxOs envision and build whats next for their businesses.\n\nYour Role\nHas data pipeline implementation experience with any of these cloud providers - AWS, Azure, GCP.\nExperience with cloud storage, cloud database, cloud data warehousing and Data Lake solutions like Snowflake, Big query, AWS Redshift, ADLS, S3.\nHas good knowledge of cloud compute services and load balancing.\nHas good knowledge of cloud identity management, authentication and authorization.\nProficiency in using cloud utility functions such as AWS lambda, AWS step functions, Cloud Run, Cloud functions, Azure functions.\nExperience in using cloud data integration services for structured, semi structured and unstructured data such as Azure Databricks, Azure Data Factory, Azure Synapse Analytics, AWS Glue, AWS EMR, Dataflow, Dataproc.\nYour Profile\nGood knowledge of Infra capacity sizing, costing of cloud services to drive optimized solution architecture, leading to optimal infra investment vs performance and scaling.\nAble to contribute to making architectural choices using various cloud services and solution methodologies.\nExpertise in programming using python.\nVery good knowledge of cloud Dev-ops practices such as infrastructure as code, CI/CD components, and automated deployments on cloud.\nMust understand networking, security, design principles and best practices in cloud.\nWhat you will love about working here\nWe recognize the significance of flexible work arrangements to provide support. Be it remote work, or flexible work hours, you will get an environment to maintain healthy work life balance.\nAt the heart of our mission is your career growth. Our array of career growth programs and diverse professions are crafted to support you in exploring a world of opportunities.\nEquip yourself with valuable certifications in the latest technologies such as Generative AI.\nAbout Capgemini\n\nCapgemini is a global business and technology transformation partner, helping organizations to accelerate their dual transition to a digital and sustainable world, while creating tangible impact for enterprises and society. It is a responsible and diverse group of 340,000 team members in more than 50 countries. With its strong over 55-year heritage, Capgemini is trusted by its clients to unlock the value of technology to address the entire breadth of their business needs. It delivers end-to-end services and solutions leveraging strengths from strategy and design to engineering, all fueled by its market leading capabilities in AI, cloud and data, combined with its deep industry expertise and partner ecosystem. The Group reported 2023 global revenues of 22.5 billion.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'networking', 'ops', 'design principles', 'aws', 'azure databricks', 'snowflake', 'azure data lake', 'glue', 'amazon redshift', 'azure synapse', 'microsoft azure', 'data warehousing', 'azure data factory', 'emr', 'aws lambda', 'data engineering', 'dataproc', 'aws glue', 'gcp', 'bigquery', 'data flow']",2025-06-11 05:21:04
Data Engineer - Hadoop,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Wipro Limited (NYSEWIT, BSE507685, NSEWIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients’ most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.\n About The Role  _x000D_\n\n \n\nRole Purpose  \nThe purpose of the role is to support process delivery by ensuring daily performance of the Production Specialists, resolve technical escalations and develop technical capability within the Production Specialists.\n\n\n ? _x000D_\n\n \n\nDo  \n\n\nOversee and support process by reviewing daily transactions on performance parameters\nReview performance dashboard and the scores for the team\nSupport the team in improving performance parameters by providing technical support and process guidance\nRecord, track, and document all queries received, problem-solving steps taken and total successful and unsuccessful resolutions\nEnsure standard processes and procedures are followed to resolve all client queries\nResolve client queries as per the SLA’s defined in the contract\nDevelop understanding of process/ product for the team members to facilitate better client interaction and troubleshooting\nDocument and analyze call logs to spot most occurring trends to prevent future problems\nIdentify red flags and escalate serious client issues to Team leader in cases of untimely resolution\nEnsure all product information and disclosures are given to clients before and after the call/email requests\nAvoids legal challenges by monitoring compliance with service agreements\n\n\n ? _x000D_\n\n\nHandle technical escalations through effective diagnosis and troubleshooting of client queries\nManage and resolve technical roadblocks/ escalations as per SLA and quality requirements\nIf unable to resolve the issues, timely escalate the issues to TA & SES\nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers’ and clients’ business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract SLA’s\n\n\n ? _x000D_\n\n\nBuild people capability to ensure operational excellence and maintain superior customer service levels of the existing account/client\nMentor and guide Production Specialists on improving technical knowledge\nCollate trainings to be conducted as triage to bridge the skill gaps identified through interviews with the Production Specialist\nDevelop and conduct trainings (Triages) within products for production specialist as per target\nInform client about the triages being conducted\nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nIdentify and document most common problems and recommend appropriate resolutions to the team\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\n\n ? _x000D_\n\nDeliver\nNoPerformance ParameterMeasure1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback, NSAT/ ESAT2Team ManagementProductivity, efficiency, absenteeism3Capability developmentTriages completed, Technical Test performance\n\nMandatory\n\nSkills:\nHadoop_x000D_.\n\nExperience5-8 Years_x000D_.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'data engineering', 'spark', 'troubleshooting', 'hadoop', 'cloudera', 'python', 'scala', 'big data analytics', 'oozie', 'airflow', 'pyspark', 'data warehousing', 'apache pig', 'machine learning', 'sql', 'mapreduce', 'sqoop', 'big data', 'aws', 'etl', 'hbase']",2025-06-11 05:21:06
Data Engineer-Data Modelling,IBM,2 - 5 years,Not Disclosed,['Pune'],"Translates business needs into a data model, providing expertise on data modeling tools and techniques for designing data models for applications and related systems.\nSkills include logical and physical data modeling, and knowledge of ERWin, MDM, and/or ETL.\nData modeling is a process used to define and analyze data requirements needed to support the business processes within the scope of corresponding information systems in organizations\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nTranslates business needs into a data model, providing expertise on data modeling tools and techniques for designing data models for applications and related systems.\nSkills include logical and physical data modeling, and knowledge of ERWin, MDM, and/or ETL.\nData modeling is a process used to define and analyze data requirements needed to support the business processes within the scope of corresponding information systems in organizations.\nTherefore, the process of data modeling involves professional data modelers working closely with business stakeholders, as well as potential users of the information system.\nThere are three different types of data models produced while progressing from requirements to the actual database to be used for the information system.\n\n\nPreferred technical and professional experience\nTranslates business needs into a data model, providing expertise on data modeling tools and techniques for designing data models for applications and related systems.\nSkills include logical and physical data modeling, and knowledge of ERWin, MDM, and/or ETL.\nData modeling is a process used to define and analyze data requirements needed to support the business processes within the scope of corresponding information systems in organizations.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['erwin', 'data engineering', 'data modeling', 'mdm', 'etl', 'hive', 'python', 'oracle', 'data analysis', 'data warehousing', 'power bi', 'microsoft azure', 'business intelligence', 'sql server', 'sql', 'plsql', 'tableau', 'unix shell scripting', 'spark', 'hadoop', 'big data', 'aws', 'informatica', 'unix']",2025-06-11 05:21:08
Data Engineer,"NTT DATA, Inc.",1 - 4 years,Not Disclosed,['Bengaluru'],"Req ID: 321498\n\nWe are currently seeking a Data Engineer to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\nJob Duties""¢ Work closely with Lead Data Engineer to understand business requirements, analyse and translate these requirements into technical specifications and solution design.\n""¢ Work closely with Data modeller to ensure data models support the solution design\n""¢ Develop , test and fix ETL code using Snowflake, Fivetran, SQL, Stored proc.\n""¢ Analysis of the data and ETL for defects/service tickets (for solution in production ) raised and service tickets.\n""¢ Develop documentation and artefacts to support projects\n\nMinimum Skills Required""¢ ADF\n""¢ Fivetran (orchestration & integration)\n""¢ SQL\n""¢ Snowflake DWH",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'data engineering', 'sql', 'oracle adf', 'etl', 'hive', 'python', 'data analysis', 'oracle', 'informatica powercenter', 'amazon redshift', 'talend', 'data warehousing', 'power bi', 'plsql', 'tableau', 'data modeling', 'spark', 'etl tool', 'technical specifications', 'hadoop', 'informatica', 'unix']",2025-06-11 05:21:10
Data Engineer,IBM,2 - 4 years,Not Disclosed,['Bengaluru'],"Ingest new data from relational and non-relational source database systems into our warehouse. Connect data from various sources.\n\nIntegrate data from external sources to warehouse by building facts and dimensions based on the EPM data model requirements.\n\n\n\nAutomate data exchange and processing through serverless data pipelines.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience in data analysis and integration.\nExperience in data building and consuming fact and dimension tables.\nExperience in automating data integration through data pipelines.\nExperience with object-oriented programing languages such as Python.\nExperience with structured data processing languages such as SQL and Spark SQL.\nExperience with REST APIs and JSON\nExperience in IBM Cloud data processing services such as IBM Code Engine, IBM Event Streams (Apache Kafka).\nStrong understanding of Datawarehouse concepts and various data warehouse architectures\n\n\nPreferred technical and professional experience\nExperience with IBM Cloud architecture\nExperience with DevOps.\nKnowledge of Agile development methodologies\nExperience with building containerized applications and running them in serverless environments on the Cloud such as IBM Code Engine, Kubernetes, or Satellite.\nExperience with IBM Cognitive Enterprise Data Platform and CodeHub.\nExperience with data integration tools such as IBM DataStage or Informatica",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data warehousing', 'sql', 'spark', 'data warehousing concepts', 'hive', 'kubernetes', 'rest', 'data analysis', 'ibm cloud', 'datastage', 'data processing', 'data engineering', 'apache', 'cloud architecture', 'cognitive', 'devops', 'ibm datastage', 'kafka', 'json', 'agile', 'hadoop', 'informatica']",2025-06-11 05:21:11
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Hyderabad'],"Project Role :Data Engineer\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\nMust have skills :Data Engineering\n\n\nGood to have skills :NAMinimum\n\n12 year(s) of experience is required\n\n\nEducational Qualification :15 years full time education\nSummary:seeking a hands-on Senior Engineering Manager of Data Platform to spearhead the development of capabilities that power Vertex products while providing a connected experience for our customers. This role demands a deep engineering background with hands-on experience in building and scaling production-level systems. The ideal candidate will excel in leading teams to deliver high-quality data products and will provide mentorship, guidance, and leadership.In this role, you will work to increase the domain data coverage and adoption of the Data Platform by promoting a connected user experience through data. You will increase data literacy and trust by leading our Data Governance and Master Data Management initiatives. You will contribute to the vision and roadmap of self-serve capabilities through the Data Platform.\nRoles & Responsibilities:Be hands-on in leading the development of features that enhance self-service capabilities of our data platform, ensuring the platform is scalable, reliable, and fully aligned with business objectives, and defining and implementing best practices in data architecture, data modeling, and data governance.Work closely with Product, Engineering, and other departments to ensure the data platform meets business requirements.Influence cross-functional initiatives related to data tools, governance, and cross-domain data sharing. Ensure technical designs are thoroughly evaluated and aligned with business objectives.Determine appropriate recruiting of staff to achieve goals and objectives. Interview, recruit, develop and retain top talent.Manage and mentor a team of engineers, fostering a collaborative and high-performance culture, and encouraging a growth mindset and accountability for outcomes. Interpret how the business strategy links to individual roles and responsibilities.Provide career development opportunities and establish processes and practices for knowledge sharing and communication.Partner with external vendors to address issues, and technical challenges.Stay current with emerging technologies and industry trends in field to ensure the platform remains cutting-edge.\nProfessional & Technical\n\n\nSkills:\n12+ years of hands-on experience in software development (preferably in the data space), with 3+ years of people management experience, demonstrating success in building, growing, and managing multiple teams.Extensive experience in architecting and building complex data platforms and products. In-depth knowledge of cloud-based services and data tools such as Snowflake, AWS, Azure, with expertise in data ingestion, normalization, and modeling.Strong experience in building and scaling production-level cloud-based data systems utilizing data ingestion tools like Fivetran, Data Quality and Observability tools like Monte Carlo, Data Catalog like Atlan and Master Data tools like Reltio or Informatica.Thorough understanding of best practices regarding agile software development and software testing.Experience of deploying cloud-based applications using automated CI/CD processes and container technologies.Understanding of security best practices when architecting SaaS applications on cloud Infrastructure.Ability to understand complex business systems and a willingness to learn and apply new technologies as needed.Proven ability to influence and deliver high-impact initiatives. Forward-thinking mindset with the ability to define and drive the teams mission, vision, and long-term strategies.Excellent leadership skills with a track record of managing teams and collaborating effectively across departments. Strong written and communication skills.Proven ability to work with and lead remote teams to achieve sustainable long-term success.Work together and Get Stuff Done attitude without losing sight of quality, and a sense of responsibility to customers and the team.\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Data Engineering.- This position is based at our Hyderabad office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['architecting', 'software testing', 'data engineering', 'data ingestion', 'agile software development', 'continuous integration', 'snowflake', 'software development', 'microsoft azure', 'reltio', 'data architecture', 'data quality', 'data modeling', 'data governance', 'aws', 'etl', 'informatica']",2025-06-11 05:21:13
Data Engineer-Hive/Impala/Kudo,IBM,15 - 20 years,Not Disclosed,['Mumbai'],"Location Mumbai\n\n Role Overview :\nAs a Big Data Engineer, you'll design and build robust data pipelines on Cloudera using Spark (Scala/PySpark) for ingestion, transformation, and processing of high-volume data from banking systems.\n\n Key Responsibilities :\nBuild scalable batch and real-time ETL pipelines using Spark and Hive\nIntegrate structured and unstructured data sources\nPerform performance tuning and code optimization\nSupport orchestration and job scheduling (NiFi, Airflow)\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience3–15 years\nProficiency in PySpark/Scala with Hive/Impala\nExperience with data partitioning, bucketing, and optimization\nFamiliarity with Kafka, Iceberg, NiFi is a must\nKnowledge of banking or financial datasets is a plus",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'scala', 'impala', 'apache nifi', 'kafka', 'cloudera', 'amazon redshift', 'pyspark', 'data warehousing', 'sql', 'etl pipelines', 'apache', 'spark', 'linux', 'hadoop', 'big data', 'etl', 'hbase', 'python', 'performance tuning', 'oozie', 'airflow', 'data engineering', 'nosql', 'mapreduce', 'sqoop', 'aws', 'unix']",2025-06-11 05:21:15
Data Engineer,Global Banking Organization,5 - 10 years,Not Disclosed,['Hyderabad'],"Key Skills: Data Engineer, AI (Artificial intelligence), SQL, Python, Java.\nRoles and Responsibilities:\nArchitect and implement modern, scalable data solutions on cloud platforms, specifically Google Cloud Platform (GCP).\nCollaborate with cross-functional teams to assess, redesign, and modernize legacy data systems.\nDesign and develop efficient ETL pipelines for data extraction, transformation, and loading to support analytics and ML models.\nEnsure robust data governance by maintaining high standards of data security, integrity, and compliance with regulatory requirements.\nMonitor, troubleshoot, and optimize data workflows and pipelines for enhanced system performance and scalability.\nProvide hands-on technical expertise and guidance across data engineering projects, with a focus on cloud adoption and automation.\nWork in an agile environment and contribute to continuous delivery and improvement initiatives.\nExperience Requirements:\n5-10 years experience in designing and implementing data engineering solutions in GCP or other leading cloud platforms.\nSolid understanding of legacy data infrastructure with demonstrated success in modernization and migration projects.\nProficiency in programming languages such as Python and Java for building data solutions and automation scripts.\nStrong SQL skills, with experience in working with both relational (SQL) and non-relational (NoSQL) databases.\nFamiliarity with data warehousing concepts, tools, and practices.\nHands-on experience with data integration tools and frameworks.\nExcellent analytical, problem-solving, and communication skills.\nExperience working in fast-paced agile environments and collaborating with multi-disciplinary teams.\nEducation: B.tech, M.tech, B.com, M.com, MBA, any PG.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Data Engineer', 'Python']",2025-06-11 05:21:16
Data Engineer,"NTT DATA, Inc.",4 - 9 years,Not Disclosed,['Chennai'],"Req ID: 324631\n\nWe are currently seeking a Data Engineer to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\n\nKey Responsibilities:\n\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\n\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\n\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\n\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\n\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\n\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\n\nAdhere to Agile practices throughout the solution development process.\n\nDesign, build, and deploy databases and data stores to support organizational requirements.\n\nBasic Qualifications:\n\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\n\nExperience with Informatica, Python, Databricks, Azure Data Engineer\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\n\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\n\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\n\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\n\nUndergraduate or Graduate degree preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'nosql', 'elastic search', 'cassandra', 'solr', 'core data', 'cloudera', 'python', 'data analytics', 'scala', 'kudu', 'microsoft azure', 'data engineering', 'data bricks', 'gen', 'java', 'apache nifi', 'spark', 'gcp', 'kafka', 'software engineering', 'hadoop', 'aws', 'data integration']",2025-06-11 05:21:18
Data Engineer,"NTT DATA, Inc.",4 - 9 years,Not Disclosed,['Chennai'],"Req ID: 324632\n\nWe are currently seeking a Data Engineer to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\n\nKey Responsibilities:\n\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\n\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\n\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\n\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\n\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\n\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\n\nAdhere to Agile practices throughout the solution development process.\n\nDesign, build, and deploy databases and data stores to support organizational requirements.\n\nBasic Qualifications:\n\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\n\nExperience with Informatica, Python, Databricks, Azure Data Engineer\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\n\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\n\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\n\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\n\nUndergraduate or Graduate degree preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'nosql', 'elastic search', 'cassandra', 'solr', 'core data', 'cloudera', 'python', 'data analytics', 'scala', 'kudu', 'microsoft azure', 'data engineering', 'data bricks', 'gen', 'java', 'apache nifi', 'spark', 'gcp', 'kafka', 'software engineering', 'hadoop', 'aws', 'data integration']",2025-06-11 05:21:20
Starburst Data Engineer/ Architect,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Starburst Data Engineer/Architect \nExpertise in Starburst and policy management like Ranger or equivalent.\nIn-depth knowledge of data modelling principles and techniques, including relational and dimensional.\nExcellent problem solving skills and the ability to troubleshoot and debug complex data related issues.\nStrong awareness of data tools and platforms like: Starburst, Snowflakes, Databricks and programming languages like SQL.\nIn-depth knowledge of data management principles, methodologies, and best practices with excellent analytical, problem-solving and decision making skills.\nDevelop, implement and maintain database systems using SQL.\nWrite complex SQL queries for integration with applications.\nDevelop and maintain data models (Conceptual, physical and logical) to meet organisational needs.\n\n\n ? \n\nDo\n\n1. Managing the technical scope of the project in line with the requirements at all stages\n\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\n\nb. Develop record management process and policies\n\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\n\nd. Providing sales data, proposals, data insights and account reviews to the client base\n\ne. Identify areas to increase efficiency and automation of processes\n\nf. Set up and maintain automated data processes\n\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\n\nh. Produce and track key performance indicators\n\n\n\n2. Analyze the data sets and provide adequate information\n\na. Liaise with internal and external clients to fully understand data content\n\nb. Design and carry out surveys and analyze survey data as per the customer requirement\n\nc. Analyze and interpret complex data sets relating to customer??s business and prepare reports for internal and external audiences using business analytics reporting tools\n\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\n\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\n\nf. Develop predictive models and share insights with the clients as per their requirement\n\n ? \n\nDeliver\nNoPerformance ParameterMeasure1.Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\n\n ? \n\n ? \nMandatory\n\nSkills:\nStartburst.\n\nExperience5-8 Years.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data management', 'sql', 'data bricks', 'data modeling', 'policy management', 'hive', 'snowflake', 'python', 'data mining', 'data warehousing', 'power bi', 'dbms', 'data architecture', 'sql server', 'plsql', 'tableau', 'unix shell scripting', 'spark', 'hadoop', 'etl', 'ssis', 'data integration', 'informatica']",2025-06-11 05:21:21
Data Engineer,"NTT DATA, Inc.",4 - 9 years,Not Disclosed,['Pune'],"Req ID: 324653\n\nWe are currently seeking a Data Engineer to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nKey Responsibilities:\n\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\n\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\n\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\n\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\n\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\n\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\n\nAdhere to Agile practices throughout the solution development process.\n\nDesign, build, and deploy databases and data stores to support organizational requirements.\n\nBasic Qualifications:\n\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\n\nExperience with Informatica, Python, Databricks, Azure Data Engineer\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\n\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\n\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\n\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\n\nUndergraduate or Graduate degree preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'nosql', 'elastic search', 'cassandra', 'solr', 'core data', 'cloudera', 'python', 'data analytics', 'scala', 'kudu', 'microsoft azure', 'data engineering', 'data bricks', 'gen', 'java', 'apache nifi', 'spark', 'gcp', 'kafka', 'software engineering', 'hadoop', 'aws', 'data integration']",2025-06-11 05:21:23
Data Engineer,"NTT DATA, Inc.",4 - 9 years,Not Disclosed,['Pune'],"Req ID: 324609\n\nWe are currently seeking a Data Engineer to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nKey Responsibilities:\n\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\n\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\n\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\n\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\n\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\n\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\n\nAdhere to Agile practices throughout the solution development process.\n\nDesign, build, and deploy databases and data stores to support organizational requirements.\n\nBasic Qualifications:\n\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\n\nExperience with Informatica, Python, Databricks, Azure Data Engineer\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\n\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\n\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\n\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\n\nUndergraduate or Graduate degree preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'nosql', 'elastic search', 'cassandra', 'solr', 'core data', 'cloudera', 'python', 'data analytics', 'scala', 'kudu', 'microsoft azure', 'data engineering', 'data bricks', 'gen', 'java', 'apache nifi', 'spark', 'gcp', 'kafka', 'software engineering', 'hadoop', 'aws', 'data integration']",2025-06-11 05:21:25
Data Engineer,Blend360 India,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Description\nWe are looking for an experienced Senior Data Engineer with a strong foundation in Python, SQL, and Spark , and hands-on expertise in AWS, Databricks . In this role, you will build and maintain scalable data pipelines and architecture to support analytics, data science, and business intelligence initiatives. You ll work closely with cross-functional teams to drive data reliability, quality, and performance.\nResponsibilities:\nDesign, develop, and optimize scalable data pipelines using Databricks in AWS such as Glue, S3, Lambda, EMR, Databricks notebooks, workflows and jobs.\nBuilding data lake in WS Databricks.\nBuild and maintain robust ETL/ELT workflows using Python and SQL to handle structured and semi-structured data.\nDevelop distributed data processing solutions using Apache Spark or PySpark .\nPartner with data scientists and analysts to provide high-quality, accessible, and well-structured data.\nEnsure data quality, governance, security, and compliance across pipelines and data stores.\nMonitor, troubleshoot, and improve the performance of data systems and pipelines.\nParticipate in code reviews and help establish engineering best practices.\nMentor junior data engineers and support their technical development.\n\n\nQualifications\nRequirements\nBachelors or masters degree in computer science, Engineering, or a related field.\n5+ years of hands-on experience in data",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Version control', 'GIT', 'Workflow', 'Data quality', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-11 05:21:27
Data Engineer/ Consultant Specialist,Hsbc,7 - 8 years,Not Disclosed,['Pune'],"Some careers shine brighter than others.\nIf you re looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further.\nHSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions.\nWe are currently seeking an experienced professional to join our team in the role of Marketing Title.\nIn this role, you will:\nThis role will be responsible for:\nDevelop and support new feeds ingestion / understand the existing framework and do the development as per the business rules and requirements.\nDevelopment and maintenance of new changes / enhancements in Data Ingestion / Juniper and promoting and supporting those in the production environment within the stipulated timelines.\nNeed to get familiar with the Data Ingestion / Data Refinery / Common Data Model / Compdata frameworks quickly and contribute to the application development as soon as possible.\nMethodical and measured approach with a keen eye for attention to detail;\nAbility to work under pressure and remain calm in the face of adversity;\nAbility to collaborate, interact and engage with different business, technical and subject matter experts;\nGood, concise, written and verbal communication\nAbility to manage workload from multiple requests and to balance priorities;\nPro-active, a can do mind-set and attitude;\nGood documentation skills\n\n\n\n\n\n\n\n\n\n\nRequirements\n\n\n\nTo be successful in this role, you should meet the following requirements:\nExperience (1 = essential, 2 = very useful, 3 = nice to have):\n1. Hadoop / Hive / GCP\n2. Agile / Scrum\n3. LINUX\nTechnical skills (1 = essential, 2 = useful, 3 = nice to have):\n1. Any ETL tool\n1. Analytical trouble shooting.\n2. Hive QL\n1. On-Prem / Cloud infra knowledge",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'Linux', 'GCP', 'Analytical', 'business rules', 'Application development', 'Refinery', 'Troubleshooting', 'ETL tool', 'Financial services']",2025-06-11 05:21:28
Data Engineer,Grid Dynamics,7 - 12 years,15-25 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","We are seeking a highly motivated and technically proficient Big Data Engineer to join our innovative team and contribute to the development of a next-generation Big Data platform. This is an exciting opportunity to work on cutting-edge solutions handling petabyte-scale datasets for one of the worlds largest technology companies headquartered in Silicon Valley.\nKey Responsibilities\nDesign and develop scalable Big Data analytical applications.\nBuild and optimize complex ETL pipelines and data processing frameworks.\nImplement large-scale, near real-time streaming data processing systems.\nContinuously enhance and support the project’s codebase, CI/CD pipelines, and deployment infrastructure.\nCollaborate with a team of top-tier engineers to build highly performant and resilient data systems using the latest Big Data technologies.\nRequired Qualifications\nStrong programming skills in Scala, Java, or Python (Scala preferred).\nHands-on experience with Apache Spark, Hadoop, and Hive.\nProficiency in stream processing technologies such as Kafka, Spark Streaming, or Akka Streams.\nSolid understanding of data quality, validation, and quality engineering practices.\nExperience with Git and version control best practices.\nAbility to rapidly learn and apply new tools, frameworks, and technologies.\nPreferred Qualifications\nStrong experience with AWS Cloud services (e.g., EMR, S3, Lambda, Glue, Redshift).\nFamiliarity with Unix-based operating systems and shell scripting (bash, ssh, grep, etc.).\nExperience with GitHub-based development workflows and pull request processes.\nKnowledge of JVM-based build tools such as SBT, Maven, or Gradle.\nWhat We Offer\nOpportunity to work on bleeding-edge Big Data projects with global impact.\nA collaborative and intellectually stimulating environment.\nCompetitive compensation and performance-based incentives.\nFlexible work schedule.\nComprehensive benefits package including medical insurance and fitness programs.\nRegular corporate social events and team-building activities.\nOngoing professional growth and career development opportunities.\nAccess to a modern and well-equipped office space.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SCALA', 'Hadoop', 'Big Data', 'Spark', 'AWS', 'Hive']",2025-06-11 05:21:30
Data Engineer,R Systems International,4 - 6 years,15-25 Lacs P.A.,['Noida'],"We are looking for a highly experienced Senior Data Engineer with deep expertise in Snowflake to lead efforts in optimizing the performance of our data warehouse to enable faster, more reliable reporting. You will be responsible for improving query efficiency, data pipeline performance, and overall reporting speed by tuning Snowflake environments, optimizing data models, and collaborating with Application development teams.\n\nRoles and Responsibilities",,,,"['hive', 'analytical', 'report generation', 'bi', 'data warehousing', 'data pipeline', 'sql', 'plsql', 'star schema', 'optimization', 'spark', 'data ingestion', 'hadoop', 'etl', 'big data', 'reporting', 'snow flake schema', 'snowflake', 'python', 'sql queries', 'performance tuning', 'elt', 'data engineering', 'kafka', 'clustering', 'informatica']",2025-06-11 05:21:32
Data Engineer,7dxperts,5 - 8 years,15-20 Lacs P.A.,['Bengaluru'],"Role & responsibilities\n3+ years of experience in Spark, Databricks, Hadoop, Data and ML Engineering.\n3+ Years on experience in designing architectures using AWS cloud services & Databricks.\nArchitecture, design and build Big Data Platform (Data Lake / Data Warehouse / Lake house) using Databricks services and integrating with wider AWS cloud services.\nKnowledge & experience in infrastructure as code and CI/CD pipeline to build and deploy data platform tech stack and solution.\nHands-on spark experience in supporting and developing Data Engineering (ETL/ELT) and Machine learning (ML) solutions using Python, Spark, Scala or R languages.\nDistributed system fundamentals and optimising Spark distributed computing.\nExperience in setting up batch and streams data pipeline using Databricks DLT, jobs and streams.\nUnderstand the concepts and principles of data modelling, Database, tables and can produce, maintain, and update relevant data models across multiple subject areas.\nDesign, build and test medium to complex or large-scale data pipelines (ETL/ELT) based on feeds from multiple systems using a range of different storage technologies and/or access methods, implement data quality validation and to create repeatable and reusable pipelines\nExperience in designing metadata repositories, understanding range of metadata tools and technologies to implement metadata repositories and working with metadata.\nUnderstand the concepts of build automation, implementing automation pipelines to build, test and deploy changes to higher environments.\nDefine and execute test cases, scripts and understand the role of testing and how it works.\n\nPreferred candidate profile\nBig Data technologies Databricks, Spark, Hadoop, EMR or Hortonworks.\nSolid hands-on experience in programming languages Python, Spark, SQL, Spark SQL, Spark Streaming, Hive and Presto\nExperience in different Databricks components and API like notebooks, jobs, DLT, interactive and jobs cluster, SQL warehouse, policies, secrets, dbfs, Hive Metastore, Glue Metastore, Unity Catalog and ML Flow.\nKnowledge and experience in AWS Lambda, VPC, S3, EC2, API Gateway, IAM users, roles & policies, Cognito, Application Load Balancer, Glue, Redshift, Spectrum, Athena and Kinesis.\nExperience in using source control tools like git, bit bucket or AWS code commit and automation tools like Jenkins, AWS Code build and Code deploy.\nHands-on experience in terraform and Databricks API to automate infrastructure stack.\nExperience in implementing CI/CD pipeline and ML Ops pipeline using Git, Git actions or Jenkins.\nExperience in delivering project artifacts like design documents, test cases, traceability matrix and low-level design documents.\nBuild references architectures, how-tos, and demo applications for customers.\nReady to complete certifications",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'Data Bricks', 'Python', 'ML', 'ML Engineering', 'Pyspark', 'MLops', 'Ci Cd Pipeline', 'GIT', 'Machine Learning', 'SQL']",2025-06-11 05:21:34
Data Engineer - Streamsets,Wipro,4 - 6 years,Not Disclosed,['Pune'],"Role Purpose\n\nConsultants are expected to complete specific tasks as part of a consulting project with minimal supervision. They will start to build a core areas of expertise and will contribute to client projects typically involving in-depth analysis, research, supporting solution development and being a successful communicator. The Consultant must achieve high personal billability.\n\n\n\nResponsibilities\n\nAs aDeveloper, Analyze, design and develop components, tools and custom features using Databricks and streamsets as per business needs.\n\nAnalyze, create and develop technical design to determine business functional and non-functional requirements & processes and review them with the technology leads and architects.\n\nWork collaboratively with all the teams as required to build a data model and setup things in Databricks andstreamsets as appropriate to transform the data and transfer it as appropriate.\n\nDevelop solutions to publish/subscribe Kafka topics.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'data bricks', 'web services', 'asp.net', 'api', 'requirement analysis']",2025-06-11 05:21:35
Data Engineer,Grid Dynamics,4 - 6 years,Not Disclosed,['Bengaluru'],"We are seeking a skilled Data Engineer & Data Analyst with over 4 years of experience to design, build, and maintain scalable data pipelines and perform advanced data analysis to support business intelligence and data-driven decision-making. The ideal candidate will have a strong foundation in computer science principles, extensive experience with SQL and big data tools, and proficiency in cloud platforms and data visualization tools.\nKey Responsibilities:\nDesign, develop, and maintain robust, scalable ETL pipelines using Apache Airflow, DBT, Composer (GCP), Control-M, Cron, Luigi, and similar tools.\nBuild and optimize data architectures including data lakes and data warehouses.\nIntegrate data from multiple sources ensuring data quality and consistency.\nCollaborate with data scientists, analysts, and stakeholders to translate business requirements into technical solutions.\nAnalyze complex datasets to identify trends, generate actionable insights, and support decision-making.\nDevelop and maintain dashboards and reports using Tableau, Power BI, and Jupyter Notebooks for visualization and pipeline validation.\nManage and optimize relational and NoSQL databases such as MySQL, PostgreSQL, Oracle, MongoDB, and DynamoDB.\nWork with big data tools and frameworks including Hadoop, Spark, Hive, Kafka, Informatica, Talend, SSIS, and Dataflow.\nUtilize cloud data services and warehouses like AWS Glue, GCP Dataflow, Azure Data Factory, Snowflake, Redshift, and BigQuery.\nSupport CI/CD pipelines and DevOps workflows using Git, Docker, Terraform, and related tools.\nEnsure data governance, security, and compliance standards are met.\nParticipate in Agile and DevOps processes to enhance data engineering workflows.\n\nRequired Qualifications:\n4+ years of professional experience in data engineering and data analysis roles.\nStrong proficiency in SQL and experience with database management systems such as MySQL, PostgreSQL, Oracle, and MongoDB.\nHands-on experience with big data tools like Hadoop and Apache Spark.\nProficient in Python programming.\nExperience with data visualization tools such as Tableau, Power BI, and Jupyter Notebooks.\nProven ability to design, build, and maintain scalable ETL pipelines using tools like Apache Airflow, DBT, Composer (GCP), Control-M, Cron, and Luigi.\nFamiliarity with data engineering tools including Hive, Kafka, Informatica, Talend, SSIS, and Dataflow.\nExperience working with cloud data warehouses and services (Snowflake, Redshift, BigQuery, AWS Glue, GCP Dataflow, Azure Data Factory).\nUnderstanding of data modeling concepts and data lake/data warehouse architectures.\nExperience supporting CI/CD practices with Git, Docker, Terraform, and DevOps workflows.\nKnowledge of both relational and NoSQL databases, including PostgreSQL, BigQuery, MongoDB, and DynamoDB.\nExposure to Agile and DevOps methodologies.\nExperience with at least one cloud platform:\nGoogle Cloud Platform (BigQuery, Dataflow, Composer, Cloud Storage, Pub/Sub)\nAmazon Web Services (S3, Glue, Redshift, Lambda, Athena)\nMicrosoft Azure (Data Factory, Synapse Analytics, Blob Storage)\n\nPreferred Skills:\nStrong problem-solving and communication skills.\nAbility to work independently and collaboratively in a team environment.\nExperience with service development, REST APIs, and automation testing is a plus.\nFamiliarity with version control systems and workflow automation.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['SQL', 'Pyspark', 'Azure', 'Data Engineering', 'Amazon Web Services', 'GCP', 'Hadoop', 'Spark', 'Google Cloud Platforms', 'AWS', 'Python']",2025-06-11 05:21:37
Data Engineer,Talent Aspire,2 - 7 years,Not Disclosed,"['Chandigarh', 'Bengaluru', 'Remote']","As the Data Engineer, you will play a pivotal role in shaping our data infrastructure and\nexecuting against our strategy. You will ideate alongside engineering, data and our clients to\ndeploy data products with an innovative and meaningful impact to clients. You will design, build,\nand maintain scalable data pipelines and workflows on AWS. Additionally, your expertise in AI\nand machine learning will enhance our ability to deliver smarter, more predictive solutions.\nKey Responsibilities\nCollaborate with other engineers, customers to brainstorm and develop impactful data\nproducts tailored to our clients.\nLeverage AI and machine learning techniques to integrate intelligent features into our\nofferings.\nDevelop, and optimize end-to-end data pipelines on AWS\nFollow best practices in software architecture and development.\nImplement effective cost management and performance optimization strategies.\nDevelop and maintain systems using Python, SQL, PySpark, and Django for front-end\ndevelopment.\nWork directly with clients and end-users and address their data needs\nUtilize databases and tools including and not limited to, Postgres, Redshift, Airflow, and\nMongoDB to support our data ecosystem.\nLeverage AI frameworks and libraries to integrate advanced analytics into our solutions.\nQualifications\n\nExperience:\nMinimum of 3 years of experience in data engineering, software development, or\nrelated roles.\nProven track record in designing and deploying AWS cloud infrastructure\nsolutions\nAt least 2 years in data analysis and mining techniques to aid in descriptive and\ndiagnostic insights\nExtensive hands-on experience with Postgres, Redshift, Airflow, MongoDB, and\nreal-time data workflows.\n\nTechnical Skills:\nExpertise in Python, SQL, and PySpark\nStrong background in software architecture and scalable development practices.\nTableau, Metabase or similar viz tools experience\nWorking knowledge of AI frameworks and libraries is a plus.\nLeadership & Communication:\nDemonstrates ownership and accountability for delivery with a strong\ncommitment to quality.\nExcellent communication skills with a history of effective client and end-user\nengagement.\nStartup & Fintech Mindset:\nAdaptability and agility to thrive in a fast-paced, early-stage startup environment.\nPassion for fintech innovation and a strong desire to make a meaningful impact\non the future of finance.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'and PySpark', 'Django', 'AI frameworks', 'Python', 'SQL']",2025-06-11 05:21:39
Data Engineer,Wissen Infotech,5 - 10 years,8-18 Lacs P.A.,"['Bengaluru', 'Mumbai (All Areas)']","Key Responsibilities\n• Design, develop, and optimize data pipelines using Python and AWS services such as Glue, Lambda, S3, EMR, Redshift, Athena, and Kinesis.\n• Implement ETL/ELT processes to extract, transform, and load data from various sources into centralized repositories (e.g., data lakes or data warehouses).\n• Collaborate with cross-functional teams to understand business requirements and translate them into scalable data solutions.\n• Monitor, troubleshoot, and enhance data workflows for performance and cost optimization.\n• Ensure data quality and consistency by implementing validation and governance practices.\n• Work on data security best practices in compliance with organizational policies and regulations.\n• Automate repetitive data engineering tasks using Python scripts and frameworks.\n• Leverage CI/CD pipelines for deployment of data workflows on AWS.\n\nRequired Skills and Qualifications\n\n• Professional Experience: 5+ years of experience in data engineering or a related field.\n• Programming: Strong proficiency in Python, with experience in libraries like pandas, pySpark, or boto3.\n• AWS Expertise: Hands-on experience with core AWS services for data engineering, such as AWS Glue for ETL/ELT, S3 for storage.\n• Redshift or Athena for data warehousing and querying.\n• Lambda for serverless compute.\n• Kinesis or SNS/SQS for data streaming.\n• IAM Roles for security. • Databases: Proficiency in SQL and experience with relational (e.g., PostgreSQL, MySQL) and NoSQL (e.g., DynamoDB) databases. • Data Processing: Knowledge of big data frameworks (e.g., Hadoop, Spark) is a plus. • DevOps: Familiarity with CI/CD pipelines and tools like Jenkins, Git, and CodePipeline. • Version Control: Proficient with Git-based workflows. • Problem Solving: Excellent analytical and debugging skills. Optional Skills • Knowledge of data modeling and data warehouse design principles. • Experience with data visualization tools (e.g., Tableau, Power BI). • Familiarity with containerization (e.g., Docker) and orchestration (e.g., Kubernetes).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'ETL', 'AWS', 'Python', 'SQL', 'Snowflake', 'Hadoop', 'SCALA', 'Big Data', 'Spark', 'Aws Glue']",2025-06-11 05:21:40
"Data Engineer - Hadoop, Spark, Python, Data bricks",Damco Solutions,4 - 9 years,Not Disclosed,['Coimbatore'],"Position Name: Data Engineer\nLocation: Coimbatore (Hybrid 3 days per week)\nWork Shift Timing: 1.30 pm to 10.30 pm (IST)\nMandatory Skills: Hadoop, Spark, Python, Data bricks\nGood to have: Java/Scala\n\nThe Role:\n• Designing and building optimized data pipelines using cutting-edge technologies in a cloud environment to drive analytical insights.\n• Constructing infrastructure for efficient ETL processes from various sources and storage systems.\n• Leading the implementation of algorithms and prototypes to transform raw data into useful information.\n• Architecting, designing, and maintaining database pipeline architectures, ensuring readiness for AI/ML transformations.\n• Creating innovative data validation methods and data analysis tools.\n• Ensuring compliance with data governance and security policies.\n• Interpreting data trends and patterns to establish operational alerts.\n• Developing analytical tools, programs, and reporting mechanisms.\n• Conducting complex data analysis and presenting results effectively.\n• Preparing data for prescriptive and predictive modeling.\n• Continuously exploring opportunities to enhance data quality and reliability.\n• Applying strong programming and problem-solving skills to develop scalable solutions.\n\nRequirements:\n• Experience in the Big Data technologies (Hadoop, Spark, Nifi, Impala).\n• Hands-on experience designing, building, deploying, testing, maintaining, monitoring, and owning scalable, resilient, and distributed data pipelines.\n• High proficiency in Scala/Java and Spark for applied large-scale data processing\n• Expertise with big data technologies, including Spark, Data Lake, and Hive.\n• Solid understanding of batch and streaming data processing techniques.\n• Proficient knowledge of the Data Lifecycle Management process, including data collection, access, use, storage, transfer, and deletion.\n• Expert-level ability to write complex, optimized SQL queries across extensive data volumes.\n• Experience on HDFS, Nifi, Kafka.\n• Experience on Apache Ozone, Delta Tables, Databricks, Axon(Kafka), Spring Batch, Oracle DB\n• Familiarity with Agile methodologies.\n• Obsession for service observability, instrumentation, monitoring, and alerting.\n• Knowledge or experience in architectural best practices for building data lakes\n\n\nInterested candidates can share their resume at Neesha1@damcogroup.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'Hadoop', 'Data bricks', 'Spark', 'Hdfs', 'Impala', 'Apache Nifi', 'date lake', 'Hive', 'java', 'SCALA', 'Big data', 'oracle DB']",2025-06-11 05:21:42
Data Engineer,Xyram Software Solution,5 - 9 years,Not Disclosed,['Bengaluru'],"Responsibilities\nDesign, develop, and maintain scalable data pipelines and ETL processes\nOptimize data flow and collection for cross-functional teams\nBuild infrastructure required for optimal extraction, transformation, and loading of data\nEnsure data quality, reliability, and integrity across all data systems\nCollaborate with data scientists and analysts to help implement models and algorithms\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, etc.\nCreate and maintain comprehensive technical documentation\nEvaluate and integrate new data management technologies and tools\n\nRequirements\n3-5 years of professional experience in data engineering roles\nBachelors degree in Computer Science, Engineering, or related field; Masters degree preferred Job Description\nExpert knowledge of SQL and experience with relational databases (e.g., PostgreSQL, Redshift, TIDB, MySQL, Oracle, Teradata)\nExtensive experience with big data technologies (e.g., Hadoop, Spark, Hive, Flink)\nProficiency in at least one programming language such as Python, Java, or Scala\nExperience with data modeling, data warehousing, and building ETL pipelines\nStrong knowledge of data pipeline and workflow management tools (e.g., Airflow, Luigi, NiFi)\nExperience with cloud platforms (AWS, Azure, or GCP) and their data services. AWS Preferred\nHands on Experience with building streaming pipelines with flink, Kafka, Kinesis. Flink\nUnderstanding of data governance and data security principles\nExperience with version control systems (e.g., Git) and CI/CD practices\n\nPreferred Skills\nExperience with containerization and orchestration tools (Docker, Kubernetes)\nBasic knowledge of machine learning workflows and MLOps\nExperience with NoSQL databases (MongoDB, Cassandra, etc.)\nFamiliarity with data visualization tools (Tableau, Power BI, etc.)\nExperience with real-time data processing\nKnowledge of data governance frameworks and compliance requirements (GDPR, CCPA, etc.)\nExperience with infrastructure-as-code tools (Terraform, CloudFormation)\n\nPersonal Qualities\nStrong problem-solving skills and attention to detail\nExcellent communication skills, both written and verbal\nAbility to work independently and as part of a team\nProactive approach to identifying and solving problems",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data management', 'Data modeling', 'Postgresql', 'MySQL', 'Oracle', 'Teradata', 'Technical documentation', 'SQL', 'Python']",2025-06-11 05:21:43
Data Engineer - Mixed media modelling,Damco Solutions,5 - 10 years,Not Disclosed,['Pune'],"Role: Data Engineer Mixed Media Model (MMM)\nExp: 4 years +\nLocation: Pune/ Remote\n\nJob Summary:\nThe ideal candidate will have strong experience building scalable ETL pipelines and working with both online and offline marketing data to support MMM, attribution, and ROI analysis.\nThe role requires close collaboration with data scientists and marketing teams to deliver clean, structured datasets for modeling.\n\nMandatory Skills:\nStrong proficiency in SQL and Python or Scala\nHands-on experience with cloud platforms (preferably GCP/BigQuery)\nProven experience with ETL tools like Apache Airflow or DBT\nExperience integrating data from multiple sources: digital platforms (Google Ads, Meta), CRM, POS, TV, Radio, etc.\nUnderstanding of Media Mix Modeling (MMM) and attribution methodologies\n\nGood to have skill:\n\nExperience with data visualization tools (Tableau, Looker, Power BI)\nExposure to statistical modeling techniques\n\n\nPlease share your resume at Neesha1@damcogroup.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Mixed media modelling', 'SQL', 'Python', 'Power Bi', 'GCP', 'Bigquery', 'Cloud', 'Apache airflow', 'Data Visualization', 'Tableau', 'ETL']",2025-06-11 05:21:45
DCT Data Engineer,Leading Client,1 - 3 years,Not Disclosed,"['Indore', 'Pune', 'Bengaluru']","LocationsPune, Bangalore, Indore\n\nWork modeWork from Office\n\nInformatica data quality - idq\nAzure databricks\nAzure data lake\nAzure Data Factory\nApi integration",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'azure databricks', 'hive', 'snowflake', 'python', 'azure data lake', 'api integration', 'informatica data quality', 'informatica powercenter', 'microsoft azure', 'pyspark', 'data warehousing', 'azure data factory', 'sql', 'spark', 'hadoop', 'etl', 'big data', 'aws', 'informatica']",2025-06-11 05:21:47
Data Engineer-Apache Airflow,IBM,8 - 13 years,Not Disclosed,['Mumbai'],"Role Overview :\nSeeking an experienced Apache Airflow specialist to design and manage data orchestration pipelines for batch/streaming workflows in a Cloudera environment.\n\n Key Responsibilities :\nDesign, schedule, and monitor DAGs for ETL/ELT pipelines\nIntegrate Airflow with Cloudera services and external APIs\nImplement retries, alerts, logging, and failure recovery\nCollaborate with data engineers and DevOps teams\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n Skills Required :\n Experience 3–8 years\nExpertise in Airflow 2.x, Python, Bash\nKnowledge of CI/CD for Airflow DAGs\nProven experience with Cloudera CDP, Spark/Hive-based data pipelines\nIntegration with Kafka, REST APIs, databases",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'ci/cd', 'spark', 'python', 'bash', 'hive', 'cloudera', 'scala', 'pyspark', 'dbms', 'apache pig', 'sql', 'apache', 'devops', 'linux', 'shell scripting', 'hadoop', 'etl', 'big data', 'hbase', 'rest', 'oozie', 'airflow', 'elt', 'data engineering', 'cdp', 'nosql', 'mapreduce', 'kafka', 'sqoop', 'aws', 'unix']",2025-06-11 05:21:49
Sr Eng Data Engineering,Johnson & Johnson,8 - 13 years,Not Disclosed,['Bengaluru'],"Johnson & Johnson MedTech is seeking a Sr Eng Data Engineering for Digital Surgery Platform (DSP) in Bangalore, India.\n\nJohnson & Johnson (J&J) stands as the world's leading manufacturer of healthcare products and a service provider in the pharmaceutical and medical device sectors. At Johnson & Johnson MedTech's Digital Surgery Platform, we are groundbreaking the future of healthcare by harnessing the power of people and technology, transitioning to a digital-first MedTech enterprise. With a focus on innovation and an ambitious strategic vision, we are integrating robotic-assisted surgery platforms, connected medical devices, surgical instruments, medical imaging, surgical efficiency solutions, and OR workflow into the next-generation MedTech platform. This initiative will also foster new surgical insights, improve supply chain innovation, use cloud infrastructure, incorporate cybersecurity, collaborate with hospital EMRs, and elevate our digital solutions.\nWe are a diverse and growing team, that nurture creativity, deep understanding of data processing techniques, and the use of sophisticated analytics technologies to deliver results.\n\nOverview\nAs a Sr Eng Data Engineering for J&J MedTech Digital Surgery Platform (DSP), you will play a pivotal role in building the modern cloud data platform by demonstrating your in-depth technical expertise and interpersonal skills. In this role, you will be required to focus on accelerating digital product development as part of the multifunctional and fast-paced DSP data platform team and will give to the digital transformation through innovative data solutions.\nOne of the key success criteria for this role is to ensure the quality of DSP software solutions and demonstrate the ability to collaborate effectively with the core infrastructure and other engineering teams and work closely with the DSP security and technical quality partners.\n\nResponsibilities\nWork with platform data engineering, core platform, security, and technical quality to design, implement and deploy data engineering solutions.\nDevelop pipelines for ingestion, transformation, orchestration, and consumption of various types of data.\nDesign and deploy data layering pipelines that use modern Spark based data processing technologies such as Databricks and Delta Live Table (DLT).\nIntegrate data engineering solutions with Azure data governance components not limited to Purview and Databricks Unity Catalog.\nImplement and support security monitoring solutions within Azure Databricks ecosystem.\nDesign, implement, and support data monitoring solutions in data analytical workspaces.\nConfigure and deploy Databricks Analytical workspaces in Azure with IaC (Terraform, Databricks API) with J&J DevOps automation tools within JPM/Xena framework.\nImplement automated CICD processes for data processing pipelines.\nSupport DataOps for the distributed DSP data architecture.\nFunction as a data engineering SME within the data platform.\nManage authoring and execution of automated test scripts.\nBuild effective partnerships with DSP architecture, core infrastructure and other domains to design and deploy data engineering solutions.\nWork closely with the DSP Product Managers to understand business needs, translate them to system requirements, demonstrate in-depth understanding of use cases for building prototypes and solutions for data processing pipelines.\nOperate in SAFe Agile DevOps principles and methodology in building quality DSP technical solutions. Author and implement automated test scripts as mandates DSP quality requirements.\n\nQualifications\nRequired\nBachelors degree or equivalent experience in software or computer science or data engineering.\n8+ years of overall IT experience.\n5-7 years of experience in cloud computing and data systems.\nAdvanced Python programming skills.\nExpert level in Azure Databricks Spark technology and data engineering (Python) including Delta Live Tables (DLT).\nExperience in design and implementation of secure Azure data solutions.\nIn-depth knowledge of the data architecture infrastructure, network components, data processing\nProficiency in building data pipelines in Azure Databricks.\nProficiency in configuration and administration of Azure Databricks workspaces and Databricks Unity Catalog.\nDeep understanding of principles of modern data Lakehouse.\nDeep understanding of Azure system capabilities, data services, and ability to implement security controls.\nProficiency with enterprise DevOps tools including Bitbucket, Jenkins, Artifactory.\nExperience with DataOps.\nExperience with quality software systems.\nDeep understanding of and experience in SAFe Agile.\nUnderstanding of SDLC.\n\nPreferred\nMaster’s degree or equivalent.\nProven healthcare experience.\nAzure Databricks certification.\nAbility to analyze use cases and translate them into system requirements, make data driven decisions\nDevOps's automation tools with JPM/Xena framework.\nExpertise in automated testing.\nExperience in AI and MLs.\nExcellent verbal and written communication skills.\nAbility to travel up to 10% of domestic required.\n\nJohnson & Johnson is an Affirmative Action and Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, or protected veteran status and will not be discriminated against on the basis of disability.",Industry Type: Pharmaceutical & Life Sciences,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Advanced Python', 'Spark', 'Devops', 'Data Bricks', 'Agile Safe', 'Cloud Computing', 'data systems']",2025-06-11 05:21:51
Data Engineer Senior Consultant,"NTT DATA, Inc.",8 - 13 years,Not Disclosed,['Bengaluru'],"Req ID: 327863\n\nWe are currently seeking a Data Engineer Senior Consultant to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\nJob DutiesResponsibilitiesLead the development of backend systems using Django. Design and implement scalable and secure APIs. Integrate Azure Cloud services for application deployment and management. Utilize Azure Databricks for big data processing and analytics. Implement data processing pipelines using PySpark. Collaborate with front-end developers, product managers, and other stakeholders to deliver comprehensive solutions. Conduct code reviews and ensure adherence to best practices. Mentor and guide junior developers. Optimize database performance and manage data storage solutions. Ensure high performance and security standards for applications. Participate in architecture design and technical decision-making. Minimum Skills RequiredQualificationsBachelor's degree in Computer Science, Information Technology, or a related field. 8+ years of experience in backend development. 8+ years of experience with Django. Proven experience with Azure Cloud services. Experience with Azure Databricks and PySpark. Strong understanding of RESTful APIs and web services. Excellent communication and problem-solving skills. Familiarity with Agile methodologies. Experience with database management (SQL and NoSQL).\n\nSkills:\nDjango, Python, Azure Cloud, Azure Databricks, Delta Lake and Delta tables, PySpark, SQL/NoSQL databases, RESTful APIs, Git, and Agile methodologies",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['web services', 'rest', 'python', 'backend development', 'django', 'azure cloud services', 'pyspark', 'jquery', 'sql', 'git', 'asp.net', 'ssrs', 'web api', 'wcf', 'big data', 'agile methodology', 'azure databricks', 'c#', 'ssas', 'data engineering', 'azure cloud', 'sql server', 'javascript', 'nosql', 'linq', 'ssis']",2025-06-11 05:21:53
Senior GenAI Data Engineer,"NTT DATA, Inc.",2 - 5 years,Not Disclosed,"['New Delhi', 'Chennai', 'Bengaluru']","Your day at NTT DATA\nSenior GenAI Data Engineer\nWe are seeking an experienced Senior Data Engineer to join our team in delivering cutting-edge Generative AI (GenAI) solutions to clients. The successful candidate will be responsible for designing, developing, and deploying data pipelines and architectures that support the training, fine-tuning, and deployment of LLMs for various industries. This role requires strong technical expertise in data engineering, problem-solving skills, and the ability to work effectively with clients and internal teams.\nWhat you'll be doing\nKey Responsibilities:\nDesign, develop, and manage data pipelines and architectures to support GenAI model training, fine-tuning, and deployment\nData Ingestion and Integration: Develop data ingestion frameworks to collect data from various sources, transform, and integrate it into a unified data platform for GenAI model training and deployment.\nGenAI Model Integration: Collaborate with data scientists to integrate GenAI models into production-ready applications, ensuring seamless model deployment, monitoring, and maintenance.\nCloud Infrastructure Management: Design, implement, and manage cloud-based data infrastructure (e.g., AWS, GCP, Azure) to support large-scale GenAI workloads, ensuring cost-effectiveness, security, and compliance.\nWrite scalable, readable, and maintainable code using object-oriented programming concepts in languages like Python, and utilize libraries like Hugging Face Transformers, PyTorch, or TensorFlow\nPerformance Optimization: Optimize data pipelines, GenAI model performance, and infrastructure for scalability, efficiency, and cost-effectiveness.\nData Security and Compliance: Ensure data security, privacy, and compliance with regulatory requirements (e.g., GDPR, HIPAA) across data pipelines and GenAI applications.\nClient Collaboration: Collaborate with clients to understand their GenAI needs, design solutions, and deliver high-quality data engineering services.\nInnovation and R&D: Stay up to date with the latest GenAI trends, technologies, and innovations, applying research and development skills to improve data engineering services.\nKnowledge Sharing: Share knowledge, best practices, and expertise with team members, contributing to the growth and development of the team.\nRequirements:\nBachelors degree in computer science, Engineering, or related fields (Master's recommended)\nExperience with vector databases (e.g., Pinecone, Weaviate, Faiss, Annoy) for efficient similarity search and storage of dense vectors in GenAI applications\n5+ years of experience in data engineering, with a strong emphasis on cloud environments (AWS, GCP, Azure, or Cloud Native platforms)\nProficiency in programming languages like SQL, Python, and PySpark\nStrong data architecture, data modeling, and data governance skills\nExperience with Big Data Platforms (Hadoop, Databricks, Hive, Kafka, Apache Iceberg), Data Warehouses (Teradata, Snowflake, BigQuery), and lakehouses (Delta Lake, Apache Hudi)\nKnowledge of DevOps practices, including Git workflows and CI/CD pipelines (Azure DevOps, Jenkins, GitHub Actions)\nExperience with GenAI frameworks and tools (e.g., TensorFlow, PyTorch, Keras)\nNice to have:\nExperience with containerization and orchestration tools like Docker and Kubernetes\nIntegrate vector databases and implement similarity search techniques, with a focus on GraphRAG is a plus\nFamiliarity with API gateway and service mesh architectures\nExperience with low latency/streaming, batch, and micro-batch processing\nFamiliarity with Linux-based operating systems and REST APIs\nLocation: Delhi or Bangalore\nWorkplace type:\nHybrid Working",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GenAI', 'hive', 'continuous integration', 'kubernetes', 'ci/cd', 'pyspark', 'data architecture', 'sql', 'docker', 'tensorflow', 'git', 'data modeling', 'gcp', 'devops', 'linux', 'jenkins', 'pytorch', 'keras', 'hadoop', 'bigquery', 'python', 'microsoft azure', 'data engineering', 'data bricks', 'data governance', 'aws']",2025-06-11 05:21:55
Big Data Engineer - Python+ PySpark + Spark,Hexaware Technologies,9 - 12 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","Experience - 9 years - 12 years\nLocation - Mumbai / Chennai / Bangalore / Pune\n\nDevelop and maintain scalable data pipelines using PySpark and Spark SQL for processing large datasets efficiently.\nWrite clean, reusable, and optimized code in Python for data manipulation, analysis, and automation tasks.",,,,"['PySpark', 'Spark', 'Python', 'SQL']",2025-06-11 05:21:56
Data Engineer,Kanini Software Solutions,12 - 20 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","We are looking for a skilled Data Engineer to join our growing data team. The ideal candidate will be responsible for designing, building, and maintaining data pipelines and infrastructure to support data analytics and business intelligence needs. A strong foundation in cloud data platforms, data transformation tools, and programming is essential.\nKey Responsibilities:\nDesign and implement scalable data pipelines using Azure Data Lake and dbt.\nIngest and transform data from various sources including databases, APIs, flat files, JSON, and XML.",,,,"['Pyspark', 'Azure', 'Snowflake']",2025-06-11 05:21:57
Data Engineer-Talend DQ,IBM,10 - 15 years,Not Disclosed,['Mumbai'],"Role Overview :\n We are hiring aTalend Data Quality Developerto design and implement robust data quality (DQ) frameworks in a Cloudera-based data lakehouse environment. The role focuses on building rule-driven validation and monitoring processes for migrated data pipelines, ensuring high levels of data trust and regulatory compliance across critical banking domains. \n\n Key Responsibilities :\nDesign and implement data quality rules using Talend DQ Studio , tailored to validate customer, account, transaction, and KYC datasets within the Cloudera Lakehouse.\nCreate reusable templates for profiling, validation, standardization, and exception handling.\nIntegrate DQ checks within PySpark-based ingestion and transformation pipelines targeting Apache Iceberg tables .\nEnsure compatibility with Cloudera components (HDFS, Hive, Iceberg, Ranger, Atlas) and job orchestration frameworks (Airflow/Oozie).\n\n\nPerform initial and ongoing data profiling on source and target systems to detect data anomalies and drive rule definitions.\nMonitor and report DQ metrics through dashboards and exception reports.\nWork closely with data governance, architecture, and business teams to align DQ rules with enterprise definitions and regulatory requirements.\nSupport lineage and metadata integration with tools like Apache Atlas or external catalogs.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n Experience 5–10 years in data management, with 3+ years in  Talend Data Quality  tools.\n\n Platforms Experience in  Cloudera Data Platform (CDP) , with understanding of  Iceberg ,  Hive ,  HDFS , and  Spark ecosystems.\n\n Languages/Tools Talend Studio (DQ module), SQL, Python (preferred), Bash scripting.\n\n Data Concepts Strong grasp of data quality dimensions—completeness, consistency, accuracy, timeliness, uniqueness.\n\n Banking Exposure Experience with financial services data (CIF, AML, KYC, product masters) is highly preferred.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data management', 'talend', 'data quality', 'spark', 'quality tools', 'hive', 'cloudera', 'python', 'metadata', 'data validation', 'oozie', 'airflow', 'financial services', 'data engineering', 'dashboards', 'sql', 'apache', 'data governance', 'apache atlas', 'bash', 'hadoop', 'bash scripting', 'data profiling']",2025-06-11 05:21:59
Data Engineer,Leading Client,4 - 9 years,Not Disclosed,['Bengaluru'],"Roles and Responsibilities:\n4+ years of experience as a data developer using Python\nKnowledge in Spark, PySpark preferable but not mandatory\nAzure Cloud experience (preferred) Alternate Cloud\nexperience is fine preferred experience in Azure platform including\nAzure data Lake, data Bricks, data Factory\nWorking Knowledge on different file formats such as JSON, Parquet, CSV, etc.\nFamiliarity with data encryption, data masking\nDatabase experience in SQL Server is preferable\npreferred experience in NoSQL databases like MongoDB\nTeam player, reliable, self-motivated, and self-disciplined",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'hive', 'scala', 'csv', 'datafactory', 'pyspark', 'dbms', 'apache pig', 'parquet', 'sql', 'spark', 'json', 'hadoop', 'big data', 'mongodb', 'hbase', 'python', 'azure data lake', 'oozie', 'microsoft azure', 'sql server', 'azure cloud', 'nosql', 'data bricks', 'mapreduce', 'kafka', 'sqoop', 'aws', 'nosql databases']",2025-06-11 05:22:01
Data/ML Ops Engineer,"NTT DATA, Inc.",2 - 5 years,Not Disclosed,['Bengaluru'],"Additional Career Level Description:\n\n\nKnowledge and application:\nSeasoned, experienced professional; has complete knowledge and understanding of area of specialization.\nUses evaluation, judgment, and interpretation to select right course of action.\n\n\n\nProblem solving:\nWorks on problems of diverse scope where analysis of information requires evaluation of identifiable factors.\nResolves and assesses a wide range of issues in creative ways and suggests variations in approach.\n\n\n\nInteraction:\nEnhances relationships and networks with senior internal/external partners who are not familiar with the subject matter often requiring persuasion.\nWorks with others outside of own area of expertise, with the ability to adapt style to differing audiences and often advises others on difficult matters.\n\n\n\nImpact:\nImpacts short to medium term goals through personal effort or influence over team members.\n\n\n\nAccountability:\nAccountable for own targets with work reviewed at critical points.\nWork is done independently and is reviewed at critical points.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ML Ops', 'python', 'spark', 'big data', 'data engineering', 'artificial intelligence', 'ml', 'sql']",2025-06-11 05:22:03
Data Engineer,sanas.ai,2 - 5 years,Not Disclosed,['Bengaluru'],"Sanas is revolutionizing the way we communicate with the world s first real-time algorithm, designed to modulate accents, eliminate background noises, and magnify speech clarity. Pioneered by seasoned startup founders with a proven track record of creating and steering multiple unicorn companies, our groundbreaking GDP-shifting technology sets a gold standard.\n\nSanas is a 200-strong team, established in 2020. In this short span, we ve successfully secured over $100 million in funding. Our innovation have been supported by the industry s leading investors, including Insight Partners, Google Ventures, Quadrille Capital, General Catalyst, Quiet Capital, and other influential investors. Our reputation is further solidified by collaborations with numerous Fortune 100 companies. With Sanas, you re not just adopting a product; you re investing in the future of communication.\n\nWe re looking for a sharp, hands-on Data Engineer to help us build and scale the data infrastructure that powers cutting-edge audio and speech AI products. You ll be responsible for designing robust pipelines, managing high-volume audio data, and enabling machine learning teams to access the right data fast.\n\nAs one of the first dedicated data engineers on the team, youll play a foundational role in shaping how we handle data end-to-end, from ingestion to training-ready features. Youll work closely with ML engineers, research scientists, and product teams to ensure data is clean, accessible, and structured for experimentation and production.\nKey Responsibilities :\nBuild scalable, fault-tolerant pipelines for ingesting, processing, and transforming large volumes of audio and metadata.\nDesign and maintain ETL workflows for training and evaluating ML models, using tools like Airflow or custom pipelines.\nCollaborate with ML research scientists to make raw and derived audio features (e.g., spectrograms, MFCCs) efficiently available for training and inference.\nManage and organize datasets, including labeling workflows, versioning, annotation pipelines, and compliance with privacy policies.\nImplement data quality, observability, and validation checks across critical data pipelines.\nHelp optimize data storage and compute strategies for large-scale training.\nQualifications :\n2-5 years of experience as a Data Engineer, Software Engineer, or similar role with a focus on data infrastructure.\nProficient in Python, SQL, and working with distributed data processing tools (e.g., Spark, Dask, Beam).\nExperience with cloud data infrastructure (AWS/GCP), object storage (e.g.,S3), and data orchestration tools.\nFamiliarity with audio data and its unique challenges (large file sizes, time-series features, metadata handling) is a strong plus.\nComfortable working in a fast-paced, iterative startup environment where systems are constantly evolving.\nStrong communication skills and a collaborative mindset you ll be working cross-functionally with ML, infra, and product teams.\nNice to Have :\nExperience with data for speech models like ASR, TTS, or speaker verification.\nKnowledge of real-time data processing (e.g., Kafka, WebSockets, or low-latency APIs).\nBackground in MLOps, feature engineering, or supporting model lifecycle workflows.\nExperience with labeling tools, audio annotation platforms, or human-in-the-loop systems.\nJoining us means contributing to the world s first real-time speech understanding platform revolutionizing Contact Centers and Enterprises alike.\n\n\nOur technology empowers agents, transforms customer experiences, and drives measurable growth. But this is just the beginning. Youll be part of a team exploring the vast potential of an increasingly sonic future",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ASR', 'Training', 'metadata', 'orchestration', 'GCP', 'Machine learning', 'Data processing', 'Data quality', 'SQL', 'Python']",2025-06-11 05:22:05
Data Engineer KL-BL,PureSoftware Pvt Ltd,7 - 12 years,Not Disclosed,"['Bengaluru', 'Malaysia']","Core Competences Required and Desired Attributes:\n  Bachelor's degree in computer science, Information Technology, or a related field.\nProficiency in Azure Data Factory, Azure Databricks and Unity Catalog, Azure SQL Database, and other Azure data services.\nStrong programming skills in SQL, Python and PySpark languages.\nExperience in the Asset Management domain would be preferable.\nStrong proficiency in data analysis and data modelling, with the ability to extract insights from complex data sets.\nHands-on experience in Power BI, including creating custom visuals, DAX expressions, and data modelling.\nFamiliarity with Azure Analysis Services, data modelling techniques, and optimization.\nExperience with data quality and data governance frameworks with an ability to debug, fine tune and optimise large scale data processing jobs.\nStrong analytical and problem-solving skills, with a keen eye for detail.\nExcellent communication and interpersonal skills, with the ability to work collaboratively in a team environment.\nProactive and self-motivated, with the ability to manage multiple tasks and deliver high-quality results within deadlines.\n\nRoles and Responsibilities\nCore Competences Required and Desired Attributes:\n  Bachelor's degree in computer science, Information Technology, or a related field.\nProficiency in Azure Data Factory, Azure Databricks and Unity Catalog, Azure SQL Database, and other Azure data services.\nStrong programming skills in SQL, Python and PySpark languages.\nExperience in the Asset Management domain would be preferable.\nStrong proficiency in data analysis and data modelling, with the ability to extract insights from complex data sets.\nHands-on experience in Power BI, including creating custom visuals, DAX expressions, and data modelling.\nFamiliarity with Azure Analysis Services, data modelling techniques, and optimization.\nExperience with data quality and data governance frameworks with an ability to debug, fine tune and optimise large scale data processing jobs.\nStrong analytical and problem-solving skills, with a keen eye for detail.\nExcellent communication and interpersonal skills, with the ability to work collaboratively in a team environment.\nProactive and self-motivated, with the ability to manage multiple tasks and deliver high-quality results within deadlines.",Industry Type: Not mentioned,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['azure databricks', 'python', 'data services', 'data analysis', 'modeling', 'analytical', 'languages', 'catalog', 'pyspark', 'datafactory', 'interpersonal skills', 'microsoft azure', 'power bi', 'azure data factory', 'data engineering', 'sql', 'sql azure', 'data modeling', 'azure analysis', 'programming', 'communication skills']",2025-06-11 05:22:06
Sr. Executive Data Engineering Analytics,IndiGo,5 - 10 years,Not Disclosed,['Gurugram'],"Role & responsibilities\nDevelop in Python, create responsive dashboards, and manage large datasets.\nDesign and deploy Power BI reports based on business needs.\nApply machine learning, deep learning, and statistical analysis (e.g., classification, regression, sentiment analysis, time series).\nTranslate technical concepts for non-technical stakeholders.\nDesign and implement Big Data platform components (batch/stream processing, memory cache, SQL query layer, rule engine).\nBuild scalable data solutions.\nConduct root cause analysis, troubleshoot applications, and support configurations.\nAutomate processes and reporting within the Operations Control Center (OCC).\nCollaborate with leadership to solve business problems and define objectives.\nRecommend and implement automated solutions.\nPerform data analysis and apply statistical methods for decision-making.\n\nPreferred candidate profile\n\nEducation: Bachelors or Master’s in Computer Science, Engineering, or related field. (Mathematics/Statistics)\nExperience: 5–10 years in data engineering & analytics.\nSkills:\nPython (hands-on)\nPower BI (dashboarding)\nSQL/SSMS (data storage and extraction)\nPower Automate / Power Apps (nice to have)",Industry Type: Travel & Tourism,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Python', 'SQL', 'SSMS', 'Power Bi', 'Power Automate']",2025-06-11 05:22:07
Data Engineer,Acenet,5 - 7 years,Not Disclosed,"['Pune', 'Gurugram', 'Bengaluru']","About Us:\nJob Summary :\nWe are seeking a highly skilled individual to join our team as a Data Engineering/Operations Specialist. This role will be responsible for maintaining and evolving data pipeline architecture, orchestrating new data sources for further processing, and ensuring the up-to-date documentation of pipelines and data feeds.\nKey Responsibilities:\n*Maintain, upgrade and evolve data pipeline architectures to ensure optimal performance and scalability.\n*Orchestrate the integration of new data sources into existing pipelines for further processing and analysis.\n*Keep documentation up to date for pipelines and data feeds to facilitate smooth operations and collaboration within the team.\n*Collaborate with cross-functional teams to understand data requirements and optimize pipeline performance accordingly.\n*Troubleshoot and resolve any issues related to pipeline architecture and data processing.\nRole Requirements and Qualifications:\n*Experience with Cloud platform for deployment and management of data pipelines.\n*Familiarity with AWS / Azure for efficient data processing workflows.\n*Experience with constructing FAIR data products is highly desirable.\n*Basic understanding of computational clusters to optimize pipeline performance.\n*Prior experience in data engineering or operations roles, preferably in a cloud-based environment.\n*Proven track record of successfully maintaining and evolving data pipeline architectures.\n*Strong problem-solving skills and ability to troubleshoot technical issues independently.\n*Excellent communication skills to collaborate effectively with cross-functional teams.\nWhy Join Us:\n*Opportunities to work on transformative projects, cutting-edge technology and innovative solutions with leading global firms across industry sectors.\n*Continuous investment in employee growth and professional development with a strong focus on up & re-skilling.\n*Competitive compensation & benefits, ESOPs and international assignments.\n*Supportive environment with healthy work-life balance and a focus on employee well-being.\n*Open culture that values diverse perspectives, encourages transparent communication and rewards contributions.\nHow to Apply:\nIf you are interested in joining our team and meet the qualifications listed above, please apply and submit your resume highlighting why you are the ideal candidate for this position.",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Supply chain', 'Architecture', 'Manager Technology', 'Healthcare', 'Data processing', 'Business strategy', 'Fund raising', 'Troubleshooting', 'Financial services', 'Logistics']",2025-06-11 05:22:09
Lead Data Engineer,Acuity Knowledge Partners,8 - 13 years,20-25 Lacs P.A.,"['Pune', 'Bangalore Rural', 'Gurugram']","Desired Skills and experience\n9+ years of experience in software development with a focus on data projects using Python, PySpark, and associated frameworks.\nProven experience as a Data Engineer with experience in Azure cloud.\nExperience implementing solutions using Azure cloud services, Azure Data Factory, Azure Lake Gen 2, Azure Databases, Azure Data Fabric, API Gateway management, Azure Functions.",,,,"['Data Engineering', 'Python', 'Pyspark', 'ETL', 'SQL']",2025-06-11 05:22:10
Data Engineer _Technology Lead,Broadridge,6 - 10 years,Not Disclosed,['Bengaluru'],"Key Responsibilities:\nAnalyzes and solve problems using technical experience, judgment and precedents\nProvides informal guidance to new team members\nExplains complex information to others in straightforward situations\n1. Data Engineering and Modelling:\nDesign & Develop Scalable Data Pipelines: Leverage AWS technologies to design, develop, and manage end-to-end data pipelines with services like .",,,,"['Star Schema', 'Snowflake', 'AWS', 'Apache Airflow']",2025-06-11 05:22:12
Data Engineer,Grid Dynamics,8 - 13 years,Not Disclosed,['Hyderabad'],"Role & responsibilities Details on tech stack\ndatabricks, python, pyspark, Snowflake, SQL\nMin requirements to the candidate\nAdvanced SQL queries, scripts, stored procedures, materialized views, and views\nFocus on ELT to load data into database and perform transformations in database\nAbility to use analytical SQL functions\nSnowflake experience \nCloud Data Warehouse solutions experience (Snowflake, Azure DW, or Redshift); data modeling, analysis, programming\nExperience with DevOps models utilizing a CI/CD tool\nWork in hands-on Cloud environment in Azure Cloud Platform (ADLS, Blob)\nAirflow\nGD Requirements \nGood interpersonal skills; comfort and competence in dealing with different teams within the organization.\nRequires an ability to interface with multiple constituent groups and build sustainable relationships.\nStrong and effective communication skills (verbal and written).\nStrong analytical, problem-solving skills.\nExperience of working in a matrix organization.\nProactive problem solver.\nAbility to prioritize and deliver.\nResults-oriented, flexible, adaptable.\nWork well independently and lead a team.\nVersatile, creative temperament, ability to think out-of-the box while defining sound and practical solutions.\nAbility to master new skills.\nFamiliar with Agile practices and methodologies\nProfessional data engineering experience focused on batch and real-time data pipelines using Spark, Python, SQL\nData warehouse (data modeling, programming)\nExperience working with Snowflake\nExperience working on a cloud environment, preferably, Microsoft Azure Cloud Data Warehouse solutions (Snowflake, Azure DW)\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Pyspark', 'Azure Data Engineering', 'Azure Databricks', 'Python', 'Airflow', 'snowflake', 'airflo', 'SQL']",2025-06-11 05:22:13
Data Engineer - Bangalore,Cygnus Ad Consulting,5 - 10 years,15-16 Lacs P.A.,"['Bangalore Rural', 'Bengaluru']","Experience in designing, building, and managing data solutions on Azure. Design, develop, and optimize big data pipelines and architectures on Azure. Implement ETL/ELT processes using Azure Data Factory, Databricks, and Spark.\n\nRequired Candidate profile\n5yrs of exp in data engineering and big data technologies.\nHands-on experience with Azure services (Azure Data Factory, Azure Synapse, Azure SQL, ADLS, etc.).\nDatabricks Certification (Mandatory).",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'Databricks Certification', 'Databricks', 'azure', 'Azure Databricks', 'Ci/Cd', 'Azure SQL', 'Devops', 'Azure Data Factory', 'Azure Synapse', 'ETL/ELT', 'ADLS', 'SCALA', 'Big Data Technologies', 'Spark', 'AWS', 'Python']",2025-06-11 05:22:15
Data Engineer,Bebo Technologies,4 - 9 years,Not Disclosed,['Chandigarh'],"Design, build, and maintain scalable and reliable data pipelines on Databricks, Snowflake, or equivalent cloud platforms.\nIngest and process structured, semi-structured, and unstructured data from a variety of sources including APIs, RDBMS, and file systems.\nPerform data wrangling, cleansing, transformation, and enrichment using PySpark, Pandas, NumPy, or similar libraries.\nOptimize and manage large-scale data workflows for performance, scalability, and cost-efficiency.\nWrite and optimize complex SQL queries for transformation, extraction, and reporting.\nDesign and implement efficient data models and database schemas with appropriate partitioning and indexing strategies for Data Warehouse or Data Mart.\nLeverage cloud services (e.g., AWS S3, Glue, Kinesis, Lambda) for storage, processing, and orchestration.\nUse orchestration tools like Airflow, Temporal, or AWS Step Functions to manage end-to-end workflows.\nBuild containerized solutions using Docker and manage deployment pipelines via CI/CD tools such as Azure DevOps, GitHub Actions, or Jenkins.\nCollaborate closely with data scientists, analysts, and business stakeholders to understand requirements and deliver data solutions.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'python', 'Snowflake', 'Data Bricks', 'sql']",2025-06-11 05:22:17
Senior Data Engineer,Paypal,8 - 13 years,Not Disclosed,['Bengaluru'],"PayPal Marketing Technology team is dedicated to creating a best-in-class platform. We are looking for highly talented, professional, and motivated engineers to join our team. As a Lead Data Engineer on our Marketing Technology Platform, you will be at the forefront of designing and developing backend data pipelines using GCP (BigQuery, Bigtable and Dataproc), Python programming. As part of your responsibilities, you need to provide technical leadership and contribute to the definition, development, integration, test, documentation, and support across multiple platforms including Unix, Python, GCP services.\n\nMeet our team\n\nAt PayPal Marketing Technology Platform, we are very supportive, forward-thinking community of customer-centric technologists. We celebrate our successes, learn from our challenges, and always keep pushing forward. Whether we're brainstorming the next big feature, tackling complex technical challenges, or sharing insights from our latest project, theres a shared sense of purpose and excitement for what we're building. Together, we share a common goal: to build seamless, secure, and scalable solutions that empower individuals and businesses around the globe.\n  Your way to impact\nYour work will directly contribute to PayPal s overarching mission of revolutionizing customer engagement globally. By building, enhancing, and scaling the back-end data pipelines that underpin our marketing technology experiences, you will be a key player in enabling seamless and innovative ways of engagement of our customers worldwide. Your efforts in developing high-quality, secure, and performant software solutions will not only improve user experiences but also drive inclusion and flexibility that is critical in todays digital economy. Your role goes beyond coding; its about making a tangible impact on the lives of millions.\n  Your day to day\nLead, develop, and grow a high performance, multi-function team of talented and passionate professionals, who are results driven to take the business forward and demonstrate superior leadership in line with the PayPal values.\nBuild scalable systems, lead technical discussions, participate in code reviews, and guide the team in engineering best practices.\nWrite quality code and build secure, highly available systems.\nProvide technical leadership and contribute to the definition, development, integration, test, documentation, and support across multiple platforms including Unix, Python, GCP services.\nManage your own project deliverables, timelines, and priorities, effectively balancing multiple tasks to meet project deadlines and performance targets.\nSharing your knowledge and experience to new members to help onboard them onto the team quickly and efficiently, fostering a culture of learning and continuous improvement.\nWhat do you need to bring-\nA bachelors degree in computer science or an equivalent combination of technical education and work experience.\nAt least 8+ years of ETL Expertise ie managing data extraction, transformation, and loading from various sources using advanced SQL and Jupyter Notebooks/Python.\nAt least 3+ years of experience in GCP Cloud services & Streaming Integrations (must).\nAt least 4+ year Experience in design and building highly scalable distributed applications capable of handling very high volume of data in GCP using BigQuery and python.\nStrong conceptual knowledge in Data warehouses, Data marts, distributed data platforms and data lakes, Data Modeling, Schema design and CI/CD\nProven experience in leading teams within a global, complex matrix organization.\nExperience working on SaaS platform(s): Adobe-RTCDP is a plus.\nExperience using Atlassian JIRA, Service Now, Atlassian Confluence tools.\nExperience in delivering projects using Agile Methodology.",Industry Type: FinTech / Payments,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Unix', 'Coding', 'Data modeling', 'Schema', 'Adobe', 'Continuous improvement', 'Customer engagement', 'SQL', 'Data extraction']",2025-06-11 05:22:18
Data Engineer,Grid Dynamics,8 - 13 years,Not Disclosed,['Hyderabad'],"Job Description:\nAdvanced SQL queries, scripts, stored procedures, materialized views, and views\nFocus on ELT to load data into database and perform transformations in database\nAbility to use analytical SQL functions\nSnowflake experience \nCloud Data Warehouse solutions experience (Snowflake, Azure DW, or Redshift); data modeling, analysis, programming\nExperience with DevOps models utilizing a CI/CD tool\nWork in hands-on Cloud environment in Azure Cloud Platform (ADLS, Blob)\nAirflow\n\nPreferred candidate profile\nGood interpersonal skills; comfort and competence in dealing with different teams within the organization.\nRequires an ability to interface with multiple constituent groups and build sustainable relationships.\nStrong and effective communication skills (verbal and written).\nStrong analytical, problem-solving skills.\nExperience of working in a matrix organization.\nProactive problem solver.\nAbility to prioritize and deliver.\nResults-oriented, flexible, adaptable.\nWork well independently and lead a team.\nVersatile, creative temperament, ability to think out-of-the box while defining sound and practical solutions.\nAbility to master new skills.\nFamiliar with Agile practices and methodologies\nProfessional data engineering experience focused on batch and real-time data pipelines using Spark, Python, SQL\nData warehouse (data modeling, programming)\nExperience working with Snowflake\nExperience working on a cloud environment, preferably, Microsoft Azure Cloud Data Warehouse solutions (Snowflake, Azure DW)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Airflow', 'Snowflake', 'Azure Databricks', 'Pyspark', 'Spark', 'Python']",2025-06-11 05:22:20
Data Engineer,Bay Area Tek Solutions LLC,5 - 8 years,Not Disclosed,['Bengaluru'],"Strong on programming languages like Python, Java Must have one cloud hands-on experience (GCP preferred) Must have: Experience working with Dockers Must have: Environments managing (e.g venv, pip, poetry, etc.) Must have: Experience with orchestrators like Vertex AI pipelines, Airflow, etc Must have: Data engineering, Feature Engineering techniques Proficient in either Apache Spark or Apache Beam or Apache Flink Must have: Advance SQL knowledge Must be aware of Streaming concepts like Windowing , Late arrival , Triggers etc",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Beam', 'GCP', 'spark', 'Cloud', 'Programming', 'Management', 'SQL', 'Python']",2025-06-11 05:22:21
Sr. AWS Databricks Data Engineer,Tata Consultancy Services,6 - 11 years,Not Disclosed,"['Kolkata', 'Pune', 'Chennai']","Role & responsibilities\n\nData Engineer, Expertise in AWS, Databricks and Pyspark",,,,"['AWS', 'Data Bricks', 'Pyspark', 'Data engineer', 'Aws Databricks']",2025-06-11 05:22:23
Lead Data Engineer,"NTT DATA, Inc.",5 - 10 years,Not Disclosed,['Bengaluru'],"Req ID: 306669\n\nWe are currently seeking a Lead Data Engineer to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\n Position Overview  We are seeking a highly skilled and experienced Lead Data/Product Engineer to join our dynamic team. The ideal candidate will have a strong background in streaming services and AWS cloud technology, leading teams and directing engineering workloads. This is an opportunity to work on the core systems supporting multiple secondary teams, so a history in software engineering and interface design would be an advantage.\n\n\n\n Key Responsibilities  \n\nLead and direct a small team of engineers engaged in\n\n- Engineering reuseable assets for the later build of data products\n\n- Building foundational integrations with Kafka, Confluent Cloud and AWS\n\n- Integrating with a large number of upstream and downstream technologies\n\n- Providing best in class documentation for downstream teams to develop, test and run data products built using our tools\n\n- Testing our tooling, and providing a framework for downstream teams to test their utilisation of our products\n\n- Helping to deliver CI, CD and IaC for both our own tooling, and as templates for downstream teams\n\n\n\n Required Skills and Qualifications  \n\n\n\n- Bachelor's degree in Computer Science, Engineering, or related field\n\n- 5+ years of experience in data engineering\n\n- 3+ years of experience with real time (or near real time) streaming systems\n\n- 2+ years of experience leading a team of data engineers\n\n- A willingness to independently learn a high number of new technologies and to lead a team in learning new technologies\n\n- Experience in AWS cloud services, particularly Lambda, SNS, S3, and EKS, API Gateway\n\n- Strong experience with Python\n\n- Strong experience in Kafka\n\n- Excellent understanding of data streaming architectures and best practices\n\n- Strong problem-solving skills and ability to think critically\n\n- Excellent communication skills to convey complex technical concepts both directly and through documentation\n\n- Strong use of version control and proven ability to govern a team in the best practice use of version control\n\n- Strong understanding of Agile and proven ability to govern a team in the best practice use of Agile methodologies\n\n\n\n Preferred Skills and Qualifications  \n\n   \n\n- An understanding of cloud networking patterns and practises\n\n- Experience with working on a library or other long term product\n\n- Knowledge of the Flink ecosystem\n\n- Experience with terraform\n\n- Experience with CI pipelines\n\n- Ability to code in a JVM language\n\n- Understanding of GDPR and the correct handling of PII\n\n- Knowledge of technical interface design\n\n- Basic use of Docker",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'version control', 'kafka', 'cloud formation aws', 'agile', 'jvm', 'cloud services', 'api gateway', 'eks', 'data engineering', 'docker', 'sql', 'java', 'lambda expressions', 'aws cloud', 'devops', 'real time pcr', 'sns', 'terraform', 'software engineering', 'aws', 'agile methodology', 'interface design']",2025-06-11 05:22:24
Senior Data Engineer - Azure,Blend360 India,6 - 11 years,Not Disclosed,['Hyderabad'],"Job Description\nAs a Sr Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\nQualifications\n7+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar fiel",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-11 05:22:26
Big Data Engineer,Grid Dynamics,6 - 11 years,Not Disclosed,['Bengaluru'],"NOTE: We are only looking for candidates who can join Immediately to available to join in 15 days\nExperience level- 6+ years\nLocation: Bangalore (Candidates who are currently in Bangalore can apply)\n\nQualifications we are looking for\nMaster/Bachelor degree in Computer Science, Electrical Engineering, Information Systems or other technical discipline; advanced degree preferred.\nMinimum of 7+ years of software development experience (with a concentration in data centric initiatives), with demonstrated expertise in leveraging standard development best practice methodologies.\nMinimum 4+ years of experience in Hadoop using Core Java Programming, Spark, Scala, Hive and Go lang\nExpertise in Object Oriented Programming Language Java\nExperience using CI/CD Process, version control and bug tracking tools.\nExperience in handling very large data volume in Real Time and batch mode.\nExperience with automation of job execution and validation\nStrong knowledge of Database concepts\nStrong team player.\nStrong communication skills with proven ability to present complex ideas and document in a clear and concise way.\nQuick learner; self-starter, detailed and in-depth.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Scala', 'Big data engineer', 'Spark', 'Java', 'Core Java', 'Hive', 'CI', 'Hadoop', 'Big Data', 'Java Development', 'Big Data Technologies', 'Ci/Cd']",2025-06-11 05:22:28
Data Engineer,wits inovation lab,15 - 20 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Work closely with the Technology Product Owner and Engineering Leads/SMEs to understand the business requirements and develop technology solutions.\nActively engage in the whole delivery lifecycle from inception, design, development, testing, deployment, operations, monitoring and continuous improvement of systems and services.\nAccountable for the technical excellence of the squad, including technical debt management, technical risk management and ensuring alignment to standards and frameworks. Recognised\nas a technical authority and escalation point for the team, guiding them on the HOW .\nBring a proven record in managing and mentoring teams, with a commitment to building team capabilities and fostering a positive, inclusive culture.\nStay abreast of the latest market developments in technologies and data engineering practices and recommend potential innovative technologies and tools to enhance the capabilities of the engineering team.\nContribute to engineering communities and provide ongoing support of platforms as required.\n\nWhat will you bring\nTo grow and be successful in this role, you will ideally bring the following:\nAt least 5 years of relevant work experience in application delivery with hands on experience with SQL Server Database and Microsoft MSBI Technology (SQL Server Integration Services and SQL Server Reporting Services)\nProven ability to develop and maintain complex SQL queries for data analysis and reporting.\nExtensive hands-on experience in data analysis, profiling and technical solution delivery in line with data warehousing principles in SQL Server.\nGood understanding of networking concepts (protocols, security, virtual network, monitoring and troubleshooting etc.)\nGood to have knowledge around scalable web tier architecture such as J2EE framework, common webserver configuration (JBOSS/IIS), security best practices\nGood experience with DevOps practices, including CI/CD pipelines and observability tools\nGood understanding of SDLC and Agile methodologies. Solid understanding of software engineering principles, patterns, and practices.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'data analysis', 'SQL Server Reporting Services', 'Agile methodologies', 'JBOSS', 'SQL Server Database', 'SDLC', 'software engineering principles', 'IIS', 'DevOps', 'J2EE framework', 'CI/CD', 'SQL Server Integration Services']",2025-06-11 05:22:29
Data engineer with Gen Ai,Leading Client,4 - 6 years,Not Disclosed,['Chennai'],"We are seeking a skilled Data Engineer who can function as a Data Architect, designing scalable data pipelines, table structures, and ETL workflows. The ideal candidate will be responsible for recommending cost-effective and high-performance data architecture solutions, collaborating with cross-functional teams to enable efficient analytics and data science initiatives.\n\nKey Responsibilities:\n\nDesign and implement ETL workflows, data pipelines, and table structures to support business analytics and data science.\n\nOptimize data storage, retrieval, and processing for cost-efficiency and high performance.\n\nCollaborate with Analytics and Data Science teams for feature engineering and KPI computations.\n\nDevelop and maintain data models for structured and unstructured data.\n\nEnsure data quality, integrity, and security across systems.\n\nWork with cloud platforms (AWS/ Azure/ GCP) to design and manage scalable data architectures.\n\nTechnical Skills Required:\n\nSQL & Python Strong proficiency in writing optimized queries and scripts.\n\nPySpark Hands-on experience with distributed data processing.\n\nCloud Technologies (AWS/ Azure/ GCP) Experience with cloud-based data solutions.\n\nSpark & Airflow Experience with big data frameworks and workflow orchestration.\n\nGen AI (Preferred) Exposure to generative AI applications is a plus.\n\nPreferred Qualifications:\n\nExperience in data modeling, ETL optimization, and performance tuning.\n\nStrong problem-solving skills and ability to work in a fast-paced environment.\n\nPrior experience working with large-scale data processing.",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Gen Ai', 'bigdata frameworks', 'python', 'performance tuning', 'data processing', 'airflow', 'microsoft azure', 'pyspark', 'business analytics', 'data architecture', 'data engineering', 'sql', 'gen', 'data quality', 'query optimization', 'data modeling', 'data science', 'spark', 'gcp', 'hadoop', 'aws', 'etl', 'big data']",2025-06-11 05:22:31
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Bengaluru'],"As a BigData Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\n\nAs a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nBig Data Developer, Hadoop, Hive, Spark, PySpark, Strong SQL.\nAbility to incorporate a variety of statistical and machine learning techniques. Basic understanding of Cloud (AWS,Azure, etc).\nAbility to use programming languages like Java, Python, Scala, etc., to build pipelines to extract and transform data from a repository to a data consumer\nAbility to use Extract, Transform, and Load (ETL) tools and/or data integration, or federation tools to prepare and transform data as needed.\nAbility to use leading edge tools such as Linux, SQL, Python, Spark, Hadoop and Java\n\n\nPreferred technical and professional experience\nBasic understanding or experience with predictive/prescriptive modeling skills\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['scala', 'sql', 'spark', 'hadoop', 'python', 'hive', 'data management', 'pyspark', 'data warehousing', 'apache pig', 'plsql', 'java', 'unix shell scripting', 'linux', 'big data', 'etl', 'hbase', 'data analysis', 'oracle', 'microsoft azure', 'machine learning', 'data engineering', 'aws', 'data integration', 'informatica']",2025-06-11 05:22:33
Data Engineer-Data Warehouse,IBM,2 - 5 years,Not Disclosed,['Bengaluru'],"As an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviour’s.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nGood Hands on experience in DBT is required.\nETL Datastage and snowflake - preferred.\nAbility to use programming languages like Java, Python, Scala, etc., to build pipelines to extract and transform data from a repository to a data consumer\nAbility to use Extract, Transform, and Load (ETL) tools and/or data integration, or federation tools to prepare and transform data as needed.\nAbility to use leading edge tools such as Linux, SQL, Python, Spark, Hadoop and Java\n\n\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['scala', 'java', 'spark', 'hadoop', 'python', 'hive', 'data management', 'autosys', 'data warehousing', 'sql', 'plsql', 'unix shell scripting', 'etl tool', 'linux', 'etl datastage', 'shell scripting', 'big data', 'etl', 'teradata sql', 'snowflake', 'oracle', 'datastage', 'warehouse', 'sql server', 'informatica', 'unix']",2025-06-11 05:22:34
Data Engineer,KC International School,8 - 13 years,Not Disclosed,['Chennai'],"KC International School is looking for Data Engineer to join our dynamic team and embark on a rewarding career journey\nLiaising with coworkers and clients to elucidate the requirements for each task.\nConceptualizing and generating infrastructure that allows big data to be accessed and analyzed.\nReformulating existing frameworks to optimize their functioning.\nTesting such structures to ensure that they are fit for use.\nPreparing raw data for manipulation by data scientists.\nDetecting and correcting errors in your work.\nEnsuring that your work remains backed up and readily accessible to relevant coworkers.\nRemaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.\n\n\nThe DE at KC will design, develop and maintain all school data infrastructure ensuring accurate and efficient data management.",Industry Type: Education / Training,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['hive', 'cloudera', 'python', 'data analysis', 'scala', 'oozie', 'airflow', 'data warehousing', 'pyspark', 'apache pig', 'machine learning', 'data engineering', 'sql', 'mapreduce', 'spark', 'hadoop', 'sqoop', 'big data', 'aws', 'etl', 'hbase']",2025-06-11 05:22:36
Data Engineer | Scala - Noida (WFO),CloudKeeper,3 - 6 years,Not Disclosed,['Noida'],"About CloudKeeper\nCloudKeeper is a cloud cost optimization partner that combines the power of\ngroup buying & commitments management, expert cloud consulting & support,\nand an enhanced visibility & analytics platform to reduce cloud cost & help\nbusinesses maximize the value from AWS, Microsoft Azure, & Google Cloud.\nA certified AWS Premier Partner, Azure Technology Consulting Partner,\nGoogle,Cloud Partner, and FinOps Foundation Premier Member, CloudKeeper\nhas helped 400+ global companies save an average of 20% on their cloud bills,\nmodernize their cloud set-up and maximize value all while maintaining\nflexibility and avoiding any long-term commitments or cost.\nCloudKeeper hived off from TO THE NEW, digital technology services company\nwith 2500+ employees and an 8-time GPTW winner.\nPosition Overview:\nWe are looking for an experienced and driven Data Engineer to join our team.\nThe ideal candidate will have a strong foundation in big data technologies,\nparticularly Spark, and a basic understanding of Scala to design and implement\nefficient data pipelines. As a Data Engineer at CloudKeeper, you will be\nresponsible for building and maintaining robust data infrastructure, integrating\nlarge datasets, and ensuring seamless data flow for analytical and operational\npurposes.\n\nKey Responsibilities:\nDesign, develop, and maintain scalable data pipelines and ETL processes to collect, process, and store data from various sources.\nWork with Apache Spark to process large datasets in a distributed environment, ensuring optimal performance and scalability.\nDevelop and optimize Spark jobs and data transformations using Scala for large-scale data processing.\nCollaborate with data analysts and other stakeholders to ensure data pipelines meet business and technical requirements.\nIntegrate data from different sources (databases, APIs, cloud storage, etc.) into a unified data platform.\nEnsure data quality, consistency, and accuracy by building robust data validation and cleansing mechanisms.\nUse cloud platforms (AWS, Azure, or GCP) to deploy and manage data processing and storage solutions.\nAutomate data workflows and tasks using appropriate tools and frameworks.\nMonitor and troubleshoot data pipeline performance, optimizing for efficiency and cost-effectiveness.\nImplement data security best practices, ensuring data privacy and compliance with industry standards.\nRequired Qualifications:\n4- 6 years of experience required as a Data Engineer or an equivalent role\nStrong experience working with Apache Spark with Scala for distributed data processing and big data handling.\nBasic knowledge of Python and its application in Spark for writing efficient data transformations and processing jobs.\nProficiency in SQL for querying and manipulating large datasets.ing technologies.\nExperience with cloud data platforms, preferably AWS (e.g., S3, EC2, EMR, Redshift) or other cloud-based solutions.\nStrong knowledge of data modeling, ETL processes, and data pipeline orchestration.\nFamiliarity with containerization (Docker) and cloud-native tools for deploying data solutions.\nKnowledge of data warehousing concepts and experience with tools like AWS Redshift, Google BigQuery, or Snowflake is a plus.\nExperience with version control systems such as Git.\nStrong problem-solving abilities and a proactive approach to resolving technical challenges.\nExcellent communication skills and the ability to work collaboratively within cross-functional teams.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Temporary/Contractual","['Data Engineering', 'SCALA', 'Pyspark', 'Scala Programming', 'Python Framework', 'SQL Queries', 'Spark', 'Python', 'SQL']",2025-06-11 05:22:38
Data Engineer,Emiza Supply Chain Services,2 - 6 years,Not Disclosed,['Mumbai (All Areas)( Vidya Vihar West )'],"Role & responsibilities\n\nKey Responsibilities\n\nDesign, build, and maintain scalable data pipelines and ETL/ELT processes.\nIntegrate data from various internal and external sources (e.g., ERP, WMS, APIs).\nOptimize and monitor data flows for performance and reliability.\nCollaborate with data analysts, software developers, and business teams to understand data requirements.\nEnsure data quality, consistency, and security across the data lifecycle.\nSupport reporting, dashboarding, and data science initiatives with clean and structured data.\nMaintain data documentation and metadata repositories.\n\nPreferred candidate profile\n\nBachelor's or Masters degree in Computer Science, Engineering, or a related field.\n2+ years of experience in a data engineering or similar role.\nStrong proficiency in SQL and working with relational databases (e.g., PostgreSQL, MySQL).\nExperience with big data technologies like Spark, Hadoop, or similar is a plus.\nHands-on experience with ETL tools (e.g., Apache Airflow, Talend, DBT).\nProficiency in Python or Scala for data processing.\nFamiliarity with cloud platforms (AWS, GCP, or Azure), especially with data services like S3, Redshift, BigQuery, etc.\nKnowledge of APIs and data integration concepts.",Industry Type: Courier / Logistics,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Etl Pipelines', 'Data Engineering', 'Hadoop', 'Cloud Platform', 'Elt', 'Data Pipeline', 'Spark', 'AWS', 'Python', 'SQL']",2025-06-11 05:22:39
Data Engineer,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will responsible for designing, building, maintaining, analyzing, and interpreting data to provide actionable insights that drive business decisions. This role involves working with large datasets, developing reports, supporting and executing data governance initiatives, and visualizing data to ensure data is accessible, reliable, and efficiently managed. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture and ETL processes.\nRoles & Responsibilities:\nDesign, develop, and maintain data solutions for data generation, collection, and processing.\nBe a key team member that assists in the design and development of the data pipeline.\nCreate data pipelines and ensure data quality by implementing ETL processes to migrate and deploy data across systems.\nContribute to the design, development, and implementation of data pipelines, ETL/ELT processes, and data integration solutions.\nTake ownership of data pipeline projects from inception to deployment, manage scope, timelines, and risks.\nCollaborate with cross-functional teams to understand data requirements and design solutions that meet business needs.\nDevelop and maintain data models, data dictionaries, and other documentation to ensure data accuracy and consistency.\nImplement data security and privacy measures to protect sensitive data.\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nCollaborate and communicate effectively with product teams.\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients.\nBasic Qualifications and Experience\nMasters degree and 1 to 3 years of experience in Computer Science, IT, or related field OR\nBachelors degree and 3 to 5 years of experience in Computer Science, IT, or related field OR\nDiploma and 7 to 9 years of experience in Computer Science, IT, or related field\nMust-Have Skills:\nHands-on experience with big data technologies and platforms, such as Databricks, Apache Spark (PySpark, SparkSQL), workflow orchestration, performance tuning on big data processing.\nProficiency in data analysis tools (e.g., SQL) and experience with data visualization tools.\nExcellent problem-solving skills and the ability to work with large, complex datasets.\nPreferred Qualifications:\nGood-to-Have Skills:\nExperience with ETL tools such as Apache Spark, and various Python packages related to data processing, machine learning model development.\nStrong understanding of data modeling, data warehousing, and data integration concepts.\nKnowledge of Python/R, Databricks, SageMaker, cloud data platforms.\nProfessional Certifications:\nCertified Data Engineer / Data Analyst (preferred on Databricks or cloud environments).\nCertified Data Scientist (preferred on Databricks or Cloud environments).\nMachine Learning Certification (preferred on Databricks or Cloud environments).\nSoft Skills:\nExcellent critical-thinking and problem-solving skills.\nStrong communication and collaboration skills.\nDemonstrated awareness of how to function in a team setting.\nDemonstrated presentation skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'SageMaker', 'R', 'data modeling', 'data warehousing', 'cloud data platforms', 'Databricks', 'ETL', 'data integration', 'Python']",2025-06-11 05:22:41
Associate Data Engineer,"NTT DATA, Inc.",1 - 3 years,Not Disclosed,"['New Delhi', 'Chennai', 'Bengaluru']","Your day at NTT DATA\nWe are seeking an experienced Data Engineer to join our team in delivering cutting-edge Generative AI (GenAI) solutions to clients. The successful candidate will be responsible for designing, developing, and deploying data pipelines and architectures that support the training, fine-tuning, and deployment of LLMs for various industries. This role requires strong technical expertise in data engineering, problem-solving skills, and the ability to work effectively with clients and internal teams.\n\nWhat youll be doing\n\nKey Responsibilities:\nDesign, develop, and manage data pipelines and architectures to support GenAI model training, fine-tuning, and deployment\nData Ingestion and Integration: Develop data ingestion frameworks to collect data from various sources, transform, and integrate it into a unified data platform for GenAI model training and deployment.\nGenAI Model Integration: Collaborate with data scientists to integrate GenAI models into production-ready applications, ensuring seamless model deployment, monitoring, and maintenance.\nCloud Infrastructure Management: Design, implement, and manage cloud-based data infrastructure (e.g., AWS, GCP, Azure) to support large-scale GenAI workloads, ensuring cost-effectiveness, security, and compliance.\nWrite scalable, readable, and maintainable code using object-oriented programming concepts in languages like Python, and utilize libraries like Hugging Face Transformers, PyTorch, or TensorFlow\nPerformance Optimization: Optimize data pipelines, GenAI model performance, and infrastructure for scalability, efficiency, and cost-effectiveness.\nData Security and Compliance: Ensure data security, privacy, and compliance with regulatory requirements (e.g., GDPR, HIPAA) across data pipelines and GenAI applications.\nClient Collaboration: Collaborate with clients to understand their GenAI needs, design solutions, and deliver high-quality data engineering services.\nInnovation and R&D: Stay up to date with the latest GenAI trends, technologies, and innovations, applying research and development skills to improve data engineering services.\nKnowledge Sharing: Share knowledge, best practices, and expertise with team members, contributing to the growth and development of the team.\n\nBachelors degree in computer science, Engineering, or related fields (Masters recommended)\nExperience with vector databases (e.g., Pinecone, Weaviate, Faiss, Annoy) for efficient similarity search and storage of dense vectors in GenAI applications\n5+ years of experience in data engineering, with a strong emphasis on cloud environments (AWS, GCP, Azure, or Cloud Native platforms)\nProficiency in programming languages like SQL, Python, and PySpark\nStrong data architecture, data modeling, and data governance skills\nExperience with Big Data Platforms (Hadoop, Databricks, Hive, Kafka, Apache Iceberg), Data Warehouses (Teradata, Snowflake, BigQuery), and lakehouses (Delta Lake, Apache Hudi)\nKnowledge of DevOps practices, including Git workflows and CI/CD pipelines (Azure DevOps, Jenkins, GitHub Actions)\nExperience with GenAI frameworks and tools (e.g., TensorFlow, PyTorch, Keras)\nNice to have:\nExperience with containerization and orchestration tools like Docker and Kubernetes\nIntegrate vector databases and implement similarity search techniques, with a focus on GraphRAG is a plus\nFamiliarity with API gateway and service mesh architectures\nExperience with low latency/streaming, batch, and micro-batch processing\nFamiliarity with Linux-based operating systems and REST APIs",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Apache Iceberg', 'Faiss', 'PySpark', 'Kafka', 'Pinecone', 'GitHub Actions', 'Snowflake', 'Apache Hudi', 'AWS', 'Azure DevOps', 'Python', 'Azure', 'BigQuery', 'Hadoop', 'Annoy', 'Teradata', 'SQL', 'Jenkins', 'Hive', 'Cloud Native platforms', 'GCP', 'Delta Lake', 'Databricks', 'Weaviate']",2025-06-11 05:22:42
Sr Manager of Data Engineering,JPMorgan Chase Bank,5 - 10 years,Not Disclosed,['Hyderabad'],"You have the opportunity to unleash your full potential at a world-renowned company and take the lead in shaping the future of technology.\nAs a Senior Manager of Data Engineering at JPMorgan Chase within the CCB, you serve in a leadership role by providing technical coaching and advisory for multiple technical teams, as we'll as anticipate the needs and potential dependencies of other data users within the firm. As an expert in your field, your insights influence budget and technical considerations to advance operational efficiencies and functionalities.\n  Job responsibilities\nArchitect and oversee the design of complex data solutions that meet diverse business needs and customer requirements.\nGuide the evolution of logical and physical data models to support emerging business use cases and technological advancements.\nBuild and manage end-to-end cloud-native data pipelines in AWS, leveraging your hands-on expertise with AWS components.\nBuild analytical systems from the ground up, providing architectural direction, translating business issues into specific requirements, and identifying appropriate data to support solutions.\nWork across the Service Delivery Lifecycle on engineering major/minor enhancements and ongoing maintenance of existing applications.\nHelp others build code to extract raw data, coach the team on techniques to validate its quality, and apply your deep data knowledge to ensure the correct data is ingested across the pipeline.\nGuide the development of data tools used to transform, manage, and access data, and advise the team on writing and validating code to test the storage and availability of data platforms for resilience.\nOversee the implementation of performance monitoring protocols across data pipelines, coaching the team on building visualizations and aggregations to monitor pipeline health.\nCoach others on implementing solutions and self-healing processes that minimize points of failure across multiple product features.\nAdds to team culture of diversity, equity, inclusion, and respect\nRequired qualifications, capabilities, and skills\nFormal training or certification on software engineering concepts and 5+ years applied experience\nExtensive experience in managing the full lifecycle of data, from collection and storage to analysis and reporting.\nProficiency in one or more large-scale data processing distributions such as JavaSpark along with knowledge on Data Pipeline (DPL), Data Modeling, Data warehouse, Data Migration and so-on.\nHands-on practical experience in system design, application development, testing, and operational stability\nProficient in coding in one or more modern programming languages\nShould have good hands-on experience with AWS services and its components along with good understanding on Kubernetes.\nExperience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages.\nStrong understanding of domain driven design, micro-services patterns, and architecture\nOverall knowledge of the Software Development Life Cycle along with experience with IBM MQ, Apache Kafka\nSolid understanding of agile methodologies such as CI/CD, Application Resiliency, and Security\nDemonstrated knowledge of software applications and technical processes within a technical discipline (eg, cloud, LLMs etc)\nPreferred qualifications, capabilities, and skills\nFamiliarity with modern front-end technologies\nExperience designing and building REST API services using Java\nExposure to cloud technologies - knowledge on Hybrid cloud architectures is highly desirable.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data migration', 'Front end', 'Data modeling', 'Coding', 'Analytical', 'Debugging', 'Agile', 'System design', 'Application development', 'Apache']",2025-06-11 05:22:44
Consultant-Data Engineer (Only from Pharma/Lifescience/Biotech domain),Chryselys,6 - 11 years,Not Disclosed,['Hyderabad'],"Job Description for Consultant - Data Engineer\nAbout Us:\nChryselys is a Pharma Analytics & Business consulting company that delivers data-driven insights leveraging AI-powered, cloud-native platforms to achieve high-impact transformations.\nWe specialize in digital technologies and advanced data science techniques that provide strategic and operational insights.\nWho we are:\nPeople - Our team of industry veterans, advisors and senior strategists have diverse backgrounds and have worked at top tier companies.\nQuality - Our goal is to deliver the value of a big five consulting company without the big five cost.\nTechnology - Our solutions are Business centric built on cloud native technologies.\nKey Responsibilities and Core Competencies:\n•      You will be responsible for managing and delivering multiple Pharma projects.\n•      Leading a team of atleast 8 members, resolving their technical and business related problems and other queries.\n•      Responsible for client interaction; requirements gathering, creating required documents, development, quality assurance of the deliverables.\n•      Good collaboration with onshore and Senior folks.\n•      Should have fair understanding of Data Capabilities (Data Management, Data Quality, Master and Reference Data).\n•      Exposure to Project management methodologies including Agile and Waterfall.\n•      Experience working in RFPs would be a plus.\nRequired Technical Skills:\n•      Proficient in Python, Pyspark, SQL\n•      Extensive hands-on experience in big data processing and cloud technologies like AWS and Azure services, Databricks etc.\n•      Strong experience working with cloud data warehouses like Snowflake, Redshift, Azure etc.\n•      Good experience in ETL, Data Modelling, building ETL Pipelines.\n•      Conceptual knowledge of Relational database technologies, Data Lake, Lake Houses etc.\n•      Sound knowledge in Data operations, quality and data governance.\nPreferred Qualifications:\n•      Bachelors or master’s Engineering/ MCA or equivalent degree.\n•      6-13 years of experience as Data Engineer, with atleast 2 years in managing medium to large scale programs.\n•      Minimum 5 years of Pharma and Life Science domain exposure in IQVIA, Veeva, Symphony, IMS etc.\n•      High motivation, good work ethic, maturity, self-organized and personal initiative.\n•      Ability to work collaboratively and providing the support to the team.\n•      Excellent written and verbal communication skills.\n•      Strong analytical and problem-solving skills.\nLocation\n•      Preferably Hyderabad, India",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'AWS', 'Data Bricks', 'Python', 'SQL', 'Data Engineering', 'Microsoft Azure', 'Data Lake', 'Data Warehousing']",2025-06-11 05:22:46
Data Engineer,TOP OEM,5 - 10 years,Not Disclosed,['Faridabad'],"Role & responsibilities\nAs a Data Engineer dedicated to projects, you will play a crucial role in designing and maintaining robust data architectures. This position requires 100% dedication to projects and Data Architecture Design: Design and implement scalable, reliable, and maintainable data architectures.\nData Integration: Develop ETL processes to integrate data from various sources into a centralized data warehouse. Ensure data quality and integrity throughout the integration process.\nDatabase Management: Administer and optimize databases for high performance and availability. Implement security measures to safeguard data against unauthorized access.\nData Modelling: Create and maintain data models for efficient storage and retrieval of information. Collaborate with data scientists and analysts to translate data needs into effective structures.\nCoding and Scripting: Utilize programming languages (e.g., Python, SQL) for developing and maintaining data pipelines.\nPerformance Monitoring: Monitor data processing systems to identify and resolve performance bottlenecks.\n\nGood to have Skills:\n1. Capability to create data pipelines for KPI Dashboards.\n2. Optimization of databases (Cloud) for efficient resource utilization.\n3. Expertise in various database technologies with the ability to apply technology to use cases.\n4. Experience with database caching, decoupling the database from reports.\n5. Proficient in using microservices for data ingestion.",Industry Type: Automobile,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'Data Engineering', 'Big Data', 'SQL']",2025-06-11 05:22:47
Data Engineer,DATA ENGINEER,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Title: Data Engineer\nExperience: 5+ Years\nLocation: Hyderabad (Onsite)\nAvailability: Immediate Joiners Preferred\nJob Description:\nWe are seeking an experienced Data Engineer with a strong background in Java, Spark, and Scala to join our dynamic team in Hyderabad. The ideal candidate will be responsible for building scalable data pipelines, optimizing data processing workflows, and supporting data-driven solutions for enterprise-grade applications. This is a full-time onsite role.\nKey Responsibilities:\nDesign, develop, and maintain robust and scalable data processing pipelines.\nWork with large-scale data using distributed computing technologies like Apache Spark.\nDevelop applications and data integration workflows using Java and Scala.\nCollaborate with cross-functional teams including Data Scientists, Analysts, and Product Managers.\nEnsure data quality, integrity, and security in all data engineering solutions.\nMonitor and troubleshoot performance and data issues in production systems.\nMust-Have Skills:\nStrong hands-on experience with Java, Apache Spark, and Scala.\nProven experience working on large-scale data processing systems.\nSolid understanding of distributed systems and performance tuning.\nGood-to-Have Skills:\nExperience with Hadoop, Hive, and HDFS.\nFamiliarity with data warehousing concepts and ETL processes.\nExposure to cloud data platforms is a plus.\nDesired Candidate Profile:\n5+ years of relevant experience in data engineering or big data technologies.\nStrong problem-solving and analytical skills.\nExcellent communication and collaboration skills.\nAbility to work independently in a fast-paced environment.\nAdditional Details:\nWork Mode: Onsite (Hyderabad)\nEmployment Type: Full-time\nNotice Period: Immediate joiners highly preferred, candidates serving notice period.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'SCALA', 'Spark', 'Hive', 'Hadoop', 'Kafka']",2025-06-11 05:22:49
Data Engineer,Forbes Global 2000 IT Services Firm,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Title: Big Data Engineer Java & Spark\nLocation: Hyderabad\nWork Mode: Onsite (5 days a week)\nExperience: 5 to 10 Years\nJob Summary:\nWe are hiring an experienced Big Data Engineer with strong expertise in Java, Apache Spark, and Big Data technologies. You will be responsible for designing and implementing scalable data pipelines that support real-time and batch processing for data-driven applications.\nKey Responsibilities:\nDevelop and maintain scalable batch and streaming data pipelines using Java and Apache Spark\nWork with Hadoop, Hive, Kafka, and HDFS to manage and process large datasets\nCollaborate with data analysts, scientists, and other engineering teams to understand data requirements\nOptimize Spark jobs and ensure performance and reliability in production\nMaintain data quality, governance, and security best practices\nRequired Skills:\n510 years of hands-on experience in data engineering or related roles\nStrong programming skills in both Java\nExpertise in Apache Spark for data processing and transformation\nGood understanding of Big Data frameworks: Hadoop, Hive, Kafka, HDFS\nExperience with distributed systems and large-scale data processing\nFamiliarity with cloud platforms such as AWS, GCP, or Azure\nGood to Have:\nExperience with workflow orchestration tools like Airflow or NiFi\nKnowledge of containerization (Docker, Kubernetes)\nExposure to CI/CD pipelines and version control (e.g., Git)\nEducation:\nBachelors or Masters degree in Computer Science, Engineering, or related field\nWhy Join Us:\nBe part of a high-impact data engineering team\nWork on modern data platforms with the latest open-source tools\nStrong tech culture with career growth opportunities",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Spark', 'Hive', 'Hadoop']",2025-06-11 05:22:51
Data Engineer Advisor,"NTT DATA, Inc.",8 - 13 years,Not Disclosed,['Bengaluru'],"Req ID: 327859\n\nWe are currently seeking a Data Engineer Advisor to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\nJob DutiesResponsibilitiesLead the development of backend systems using Django. Design and implement scalable and secure APIs. Integrate Azure Cloud services for application deployment and management. Utilize Azure Databricks for big data processing and analytics. Implement data processing pipelines using PySpark. Collaborate with front-end developers, product managers, and other stakeholders to deliver comprehensive solutions. Conduct code reviews and ensure adherence to best practices. Mentor and guide junior developers. Optimize database performance and manage data storage solutions. Ensure high performance and security standards for applications. Participate in architecture design and technical decision-making. Minimum Skills RequiredQualificationsBachelor's degree in Computer Science, Information Technology, or a related field. 8+ years of experience in backend development. 8+ years of experience with Django. Proven experience with Azure Cloud services. Experience with Azure Databricks and PySpark. Strong understanding of RESTful APIs and web services. Excellent communication and problem-solving skills. Familiarity with Agile methodologies. Experience with database management (SQL and NoSQL).\n\nSkills:\nDjango, Python, Azure Cloud, Azure Databricks, Delta Lake and Delta tables, PySpark, SQL/NoSQL databases, RESTful APIs, Git, and Agile methodologies",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['web services', 'rest', 'python', 'backend development', 'django', 'azure cloud services', 'css', 'pyspark', 'jquery', 'sql', 'git', 'asp.net', 'html', 'web api', 'mvc', 'wcf', 'agile methodology', 'azure databricks', 'c#', 'entity framework', 'azure cloud', 'javascript', 'sql server', 'nosql', 'angular', 'linq', '.net']",2025-06-11 05:22:52
Data Engineer,Diverse Lynx,5 - 10 years,Not Disclosed,['Kolkata'],Diverse Lynx is looking for Data Engineer to join our dynamic team and embark on a rewarding career journey\nLiaising with coworkers and clients to elucidate the requirements for each task.\nConceptualizing and generating infrastructure that allows big data to be accessed and analyzed.\nReformulating existing frameworks to optimize their functioning.\nTesting such structures to ensure that they are fit for use.\nPreparing raw data for manipulation by data scientists.\nDetecting and correcting errors in your work.\nEnsuring that your work remains backed up and readily accessible to relevant coworkers.\nRemaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['hive', 'cloudera', 'python', 'data analysis', 'scala', 'oozie', 'airflow', 'data warehousing', 'pyspark', 'apache pig', 'machine learning', 'data engineering', 'sql', 'mapreduce', 'spark', 'hadoop', 'sqoop', 'big data', 'aws', 'etl', 'hbase']",2025-06-11 05:22:54
Informatica Data Engineer,Malomatia,7 - 12 years,20-35 Lacs P.A. (Including Variable: 3%),['Pune'],"Job Title: Data Engineer Informatica IDMC\nLocation: Remote/Contract\nExperience Level: Mid to Senior (7+ years)\nJob Summary:\nWe are seeking a highly skilled Data Engineer with a minimum of 5 years of hands-on experience in Informatica Intelligent Data Management Cloud (IDMC). The successful candidate will design, implement, and maintain scalable data integration solutions using Informatica Cloud services. Experience with CI/CD pipelines is required to ensure efficient development and deployment cycles. Familiarity with Informatica Catalog, Data Governance, and Data Quality or Azure Data Factory is considered a strong advantage.\nKey Responsibilities:\nDesign, build, and optimize end-to-end data pipelines using Informatica IDMC, including Cloud Data Integration and Cloud Application Integration.\nImplement ETL/ELT processes to support data lakehouse, and EDW use cases.\nDevelop and maintain CI/CD pipelines to support automated deployment and version control.\nWork closely with data architects, analysts, and business stakeholders to translate data requirements into scalable solutions.\nMonitor job performance, troubleshoot issues, and ensure compliance with SLAs and data quality standards.\nDocument technical designs, workflows, and integration processes following best practices.\nCollaborate with DevOps and cloud engineering teams to ensure seamless integration within the cloud ecosystem.\nRequired Qualifications:\nBachelor’s degree in Computer Science, Information Systems, Engineering, or a related field.\nMinimum 5 years of hands-on experience with Informatica IDMC.\nExperience in building and deploying CI/CD pipelines using tools such as Git, or Azure DevOps.\nProficient in SQL, data modeling, and transformation logic.\nExperience with cloud platforms (Azure or OCI).\nStrong problem-solving skills in data operations and pipeline performance.\nPreferred / Nice-to-Have Skills:\nExperience with Informatica Data Catalog for metadata and lineage tracking.\nFamiliarity with Informatica Data Governance tools such as Axon and Business Glossary.\nHands-on experience with Informatica Data Quality (IDQ) for data profiling, cleansing.\nExperience developing data pipelines using Azure Data Factory.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'ADF', 'Informatica IDMC', 'Devops', 'Python', 'Azure Data Factory', 'SCALA', 'ETL', 'Azure Storage']",2025-06-11 05:22:56
Data Engineer-Data Platforms,IBM,4 - 7 years,Not Disclosed,['Bengaluru'],"A Data Engineer specializing in enterprise data platforms, experienced in building, managing, and optimizing data pipelines for large-scale environments. Having expertise in big data technologies, distributed computing, data ingestion, and transformation frameworks.\nProficient in Apache Spark, PySpark, Kafka, and Iceberg tables, and understand how to design and implement scalable, high-performance data processing solutions.What you’ll doAs a Data Engineer – Data Platform Services, responsibilities include:\n\nData Ingestion & Processing\nDesigning and developing data pipelines to migrate workloads from IIAS to Cloudera Data Lake.\nImplementing streaming and batch data ingestion frameworks using Kafka, Apache Spark (PySpark).\nWorking with IBM CDC and Universal Data Mover to manage data replication and movement.\nBig Data & Data Lakehouse Management\nImplementing Apache Iceberg tables for efficient data storage and retrieval.\nManaging distributed data processing with Cloudera Data Platform (CDP).\nEnsuring data lineage, cataloging, and governance for compliance with Bank/regulatory policies.\nOptimization & Performance Tuning\nOptimizing Spark and PySpark jobs for performance and scalability.\nImplementing data partitioning, indexing, and caching to enhance query performance.\nMonitoring and troubleshooting pipeline failures and performance bottlenecks.\nSecurity & Compliance\nEnsuring secure data access, encryption, and masking using Thales CipherTrust.\nImplementing role-based access controls (RBAC) and data governance policies.\nSupporting metadata management and data quality initiatives.\nCollaboration & Automation\nWorking closely with Data Scientists, Analysts, and DevOps teams to integrate data solutions.\nAutomating data workflows using Airflow and implementing CI/CD pipelines with GitLab and Sonatype Nexus.\nSupporting Denodo-based data virtualization for seamless data access\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n4-7 years of experience in big data engineering, data integration, and distributed computing.\nStrong skills in Apache Spark, PySpark, Kafka, SQL, and Cloudera Data Platform (CDP).\nProficiency in Python or Scala for data processing.\nExperience with data pipeline orchestration tools (Apache Airflow, Stonebranch UDM).\nUnderstanding of data security, encryption, and compliance frameworks\n\n\nPreferred technical and professional experience\nExperience in banking or financial services data platforms.\nExposure to Denodo for data virtualization and DGraph for graph-based insights.\nFamiliarity with cloud data platforms (AWS, Azure, GCP).\nCertifications in Cloudera Data Engineering, IBM Data Engineering, or AWS Data Analytics",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'scala', 'pyspark', 'sql', 'spark', 'cloudera', 'continuous integration', 'data analytics', 'data processing', 'airflow', 'big data technologies', 'ci/cd', 'microsoft azure', 'data engineering', 'distributed computing', 'gcp', 'kafka', 'data ingestion', 'gitlab', 'big data', 'aws', 'data integration']",2025-06-11 05:22:58
Data Engineer-Data Platforms-Azure,IBM,6 - 7 years,Not Disclosed,['Bengaluru'],"As Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in the development of data solutions using Spark Framework with Python or Scala on Hadoop and Azure Cloud Data Platform\n\nResponsibilities:\nExperienced in building data pipelines to Ingest, process, and transform data from files, streams and databases. Process the data with Spark, Python, PySpark and Hive, Hbase or other NoSQL databases on Azure Cloud Data Platform or HDFS\nExperienced in develop efficient software code for multiple use cases leveraging Spark Framework / using Python or Scala and Big Data technologies for various use cases built on the platform\nExperience in developing streaming pipelines\nExperience to work with Hadoop / Azure eco system components to implement scalable solutions to meet the ever-increasing data volumes, using big data/cloud technologies Apache Spark, Kafka, any Cloud computing etc\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nTotal 6 - 7+ years of experience in Data Management (DW, DL, Data Platform, Lakehouse) and Data Engineering skills\nMinimum 4+ years of experience in Big Data technologies with extensive data engineering experience in Spark / Python or Scala;\nMinimum 3 years of experience on Cloud Data Platforms on Azure;\nExperience in DataBricks / Azure HDInsight / Azure Data Factory, Synapse, SQL Server DB\nGood to excellent SQL skills\n\n\nPreferred technical and professional experience\nCertification in Azure and Data Bricks or Cloudera Spark Certified developers",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data management', 'big data technologies', 'sql', 'python', 'data engineering', 'hive', 'scala', 'pyspark', 'dbms', 'azure data factory', 'spark', 'ssrs', 'hadoop', 'big data', 'cloud computing', 'hbase', 'ssas', 'microsoft azure', 'azure hdinsight', 'azure cloud', 'sql server', 'nosql', 'data bricks', 'kafka', 'ssis']",2025-06-11 05:22:59
Data Engineer,Suzva Software Technologies,7 - 8 years,Not Disclosed,"['Mumbai', 'Hyderabad', 'Chennai']","Data Engineer (Contract | 6 Months)\n\nWe are seeking an experienced Data Engineer to join our team for a 6-month contract assignment. The ideal candidate will work on data warehouse development, ETL pipelines, and analytics enablement using Snowflake, Azure Data Factory (ADF), dbt, and other tools.\n\nThis role requires strong hands-on experience with data integration platforms, documentation, and pipeline optimizationespecially in cloud environments such as Azure and AWS.\n\n#KeyResponsibilities\nBuild and maintain ETL pipelines using Fivetran, dbt, and Azure Data Factory\n\nMonitor and support production ETL jobs\n\nDevelop and maintain data lineage documentation for all systems\n\nDesign data mapping and documentation to aid QA/UAT testing\n\nEvaluate and recommend modern data integration tools\n\nOptimize shared data workflows and batch schedules\n\nCollaborate with Data Quality Analysts to ensure accuracy and integrity of data flows\n\nParticipate in performance tuning and improvement recommendations\n\nSupport BI/MDM initiatives including Data Vault and Data Lakes\n\n#RequiredSkills\n7+ years of experience in data engineering roles\n\nStrong command of SQL, with 5+ years of hands-on development\n\nDeep experience with Snowflake, Azure Data Factory, dbt\n\nStrong background with ETL tools (Informatica, Talend, ADF, dbt, etc.)\n\nBachelor's in CS, Engineering, Math, or related field\n\nExperience in healthcare domain (working with PHI/PII data)\n\nFamiliarity with scripting/programming (Python, Perl, Java, Linux-based environments)\n\nExcellent communication and documentation skills\n\nExperience with BI tools like Power BI, Cognos, etc.\n\nOrganized, self-starter with strong time-management and critical thinking abilities\n\n#NiceToHave\nExperience with Data Lakes and Data Vaults\n\nQA & UAT alignment with clear development documentation\nMulti-cloud experience (especially Azure, AWS)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Azure Data Factory', 'ADF', 'Power BI', 'Cognos', 'Snowflake', 'Informatica', 'ETL', 'UAT testing']",2025-06-11 05:23:01
Data Engineer,Blend360 India,5 - 10 years,Not Disclosed,['Hyderabad'],"We are looking for an experienced Senior Data Engineer with a strong foundation in Python, SQL, and Spark , and hands-on expertise in AWS, Databricks . In this role, you will build and maintain scalable data pipelines and architecture to support analytics, data science, and business intelligence initiatives. You ll work closely with cross-functional teams to drive data reliability, quality, and performance.\nResponsibilities:\nDesign, develop, and optimize scalable data pipelines using Databricks in AWS such as Glue, S3, Lambda, EMR, Databricks notebooks, workflows and jobs.\nBuilding data lake in WS Databricks.\nBuild and maintain robust ETL/ELT workflows using Python and SQL to handle structured and semi-structured data.\nDevelop distributed data processing solutions using Apache Spark or PySpark .\nPartner with data scientists and analysts to provide high-quality, accessible, and well-structured data.\nEnsure data quality, governance, security, and compliance across pipelines and data stores.\nMonitor, troubleshoot, and improve the performance of data systems and pipelines.\nParticipate in code reviews and help establish engineering best practices.\nMentor junior data engineers and support their technical development.\n\n\nRequirements\nBachelors or masters degree in computer science, Engineering, or a related field.\n5+ years of hands-on experience in data engineering , with at lea",Industry Type: Industrial Automation,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Version control', 'GIT', 'Workflow', 'Data quality', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-11 05:23:02
Data Engineer-Data Warehouse,IBM,2 - 5 years,Not Disclosed,['Bengaluru'],"As an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviour’s.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nStrong experience in SQL.\nStrong experience in DBT.\nStrong experience in Data warehousing concepts.\nStrong experience in AWS or any other Cloud knowledge.\nRedshift is good to have\n\n\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['amazon redshift', 'data warehousing', 'sql', 'data warehousing concepts', 'aws', 'hive', 'python', 'sql development', 'data management', 'oracle', 'ssas', 'datastage', 'warehouse', 'sql server', 'plsql', 'unix shell scripting', 'data modeling', 'ssrs', 'hadoop', 'big data', 'etl', 'ssis', 'informatica', 'unix']",2025-06-11 05:23:04
Data Engineer (Python and SQL),Infraveo Technologies,2 - 5 years,Not Disclosed,[],"AI patterns recognition: you'll be developing multiple AI features, from user categorization to autofilled descriptions, market and conversations sentiment analysis and AI insights to help crypto marketers.\nSDK, User Graph improvement and reliability: we're constantly improving our proprietary user graph by matching wallets to social profiles. Your mission will be supporting new crypto wallets and networks, automating a system to match more users to their identities, and applying data checks to ensure reliability.\nIntegrations: As a customer data platform, we're expanding the number of data sources our customers can import from Web2 and Web3. you'll manage multiple API endpoints and integrate new third-party tools like Mixpanel, Amplitude, Segment, Dune Analytics, and DeFi Llama. This involves not only integration work but also data modeling and architectural design.\nSocial Data analysis: you'll work with social data APIs like Twitter to analyze Key Opinion Leaders performance and trends.\nRequirements\nProven track record as a Data Engineer delivering complex data solutions.\nAdvanced SQL skills and expertise with complex queries.\nMastery in Python development and strong experience with PySpark.\nExtensive proficiency managing cloud services, including AWS Redshift, RDS Postgres, S3, Lambda, Kinesis, SQS, ECS, EC2.\nStrong competency implementing and supporting various data models, such as highly normalized, star schema and Data Vault.\nPractical experience with orchestration tools like Airflow, Dagster or Prefect.\nDemonstrated proficiency consuming and automating interactions with APIs.\nHands-on experience creating data pipelines using dbt for various platforms (ideally AWS Redshift and Postgres).\nExperience in SaaS analytics, marketing, or crypto companies is a plus.\nBenefits\nWork Location: Remote\n5 days working",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Data modeling', 'Cloud Services', 'Architectural design', 'SDK', 'Management', 'AWS', 'Analytics', 'SQL', 'Python']",2025-06-11 05:23:06
Data Engineer,Sorice Solutions,4 - 8 years,6-12 Lacs P.A.,['Chennai'],"Location: Chennai\n\n\nRole & responsibilities\n\n\nBackend data model and data architecture, data migration, using python/java to build data ingestion pipelines and data validation processes, develop task schedulers for different data sources",Industry Type: Miscellaneous,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Migration', 'Data Modeling', 'Python', 'SQL']",2025-06-11 05:23:07
Python Data Engineer,GAVS Technologies,4 - 6 years,5.5-15.5 Lacs P.A.,['Pune'],"Skill Expectations\nMust-Have Skills:\nStrong hands-on experience in Python development\nExperience working with Fast API\nData migration and data engineering experience (ETL, pipelines, transformations)\nExperience in web scraping and data extraction techniques\nExperience working with GCP",,,,"['Data Extraction', 'Web Scraping', 'Python', 'GCP', 'Fast Api']",2025-06-11 05:23:09
Data Engineer-Data Platforms-AWS,IBM,2 - 6 years,Not Disclosed,['Bengaluru'],"As Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in the development of data solutions using Spark Framework with Python or Scala on Hadoop and AWS Cloud Data Platform\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nAWS Data Vault 2.0 development mechanism for agile data ingestion, storage and scaling\nDatabricks for complex queries on transformation, aggregation, business logic implementation aggregation, business logic implementation\nAWS Redshift and Redshift spectrum, for complex queries on transformation, aggregation, business logic implementation\nDWH Concept on star schema, Materialize view concept.\nStrong SQL and data manipulation/transformation skills\n\n\nPreferred technical and professional experience\nRobust and Scalable Cloud Infrastructure\nEnd-to-End Data Engineering Pipeline\nVersatile Programming Capabilities",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'spark', 'python', 'data manipulation', 'aws', 'schema', 'hive', 'scala', 'amazon redshift', 'pyspark', 'java', 'aws cloud', 'gcp', 'devops', 'linux', 'data ingestion', 'jenkins', 'hadoop', 'big data', 'etl', 'cloud computing', 'microsoft azure', 'data engineering', 'kafka', 'cloud infrastructure', 'agile', 'sqoop', 'unix']",2025-06-11 05:23:10
Data Engineer,Tekskills India pvt ltd,7 - 9 years,8-15 Lacs P.A.,['Hyderabad'],"Role & Responsibilities Role Overview: We are seeking a talented and forward-thinking Data Engineer for one of the large financial services GCC based in Hyderabad with responsibilities that include designing and constructing data pipelines, integrating data from multiple sources, developing scalable data solutions, optimizing data workflows, collaborating with cross-functional teams, implementing data governance practices, and ensuring data security and compliance.\n\nTechnical Requirements: • Proficiency in ETL, Batch, and Streaming Process • Experience with BigQuery, Cloud Storage, and CloudSQL • Strong programming skills in Python, SQL, and Apache Beam for data processing • Understanding of data modeling and schema design for analytics • Knowledge of data governance, security, and compliance in GCP • Familiarity with machine learning workflows and integration with GCP ML tools • Ability to optimize performance within data pipelines\n\nFunctional Requirements: • Ability to collaborate with Data Operations, Software Engineers, Data Scientists, and Business SMEs to develop Data Product Features • Experience in leading and mentoring peers within an existing development team • Strong communication skills to craft and communicate robust solutions • Proficient in working with Engineering Leads, Enterprise and Data Architects, and Business Architects to build appropriate data foundations • Willingness to work on contemporary data architecture in Public and Private Cloud environments This role offers a compelling opportunity for a seasoned Data Engineering to drive transformative cloud initiatives within the financial sector, leveraging unparalleled experience and expertise to deliver innovative cloud solutions that align with business imperatives and regulatory requirements. Qualification o Engineering Grad / Postgraduate CRITERIA o Proficient in ETL, Python, and Apache Beam for data processing efficiency. o Demonstrated expertise in BigQuery, Cloud Storage, and CloudSQL utilization. o Strong collaboration skills with cross-functional teams for data product development. o Comprehensive knowledge of data governance, security, and compliance in GCP. o Experienced in optimizing performance within data pipelines for efficiency.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ETL', 'GCP', 'Apache beam', 'Bigquery', 'Cloud sql', 'Cloudstorage', 'Python']",2025-06-11 05:23:12
Data Engineer,IntraEdge Technology,8 - 13 years,Not Disclosed,['Chennai'],"The Engineer will need to work on backend data model and data architecture, data migration, using python/java to build data ingestion pipelines and data validation processes, develop task schedulers for different data sources",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data migration', 'Backend', 'Data validation', 'Data modeling', 'Application development', 'Python', 'Data architecture']",2025-06-11 05:23:13
Data Engineer-Data Platforms-Google,IBM,5 - 7 years,Not Disclosed,['Bengaluru'],"Skilled Multiple GCP services - GCS, BigQuery, Cloud SQL, Dataflow, Pub/Sub, Cloud Run, Workflow, Composer, Error reporting, Log explorer etc.\nMust have Python and SQL work experience & Proactive, collaborative and ability to respond to critical situation\nAbility to analyse data for functional business requirements & front face customer\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n5 to 7 years of relevant experience working as technical analyst with Big Query on GCP platform.\nSkilled in multiple GCP services - GCS, Cloud SQL, Dataflow, Pub/Sub, Cloud Run, Workflow, Composer, Error reporting, Log explorer\nYou love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies\nAmbitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work\n\n\nPreferred technical and professional experience\nCreate up to 3 bullets maxitive individual with an ability to manage change and proven time management\nProven interpersonal skills while contributing to team effort by accomplishing related results as needed\nUp-to-date technical knowledge by attending educational workshops, reviewing publications (encouraging then to focus on required skills)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'gcp', 'bigquery', 'cloud sql', 'python', 'hive', 'gen', 'java', 'postgresql', 'spark', 'linux', 'mysql', 'hadoop', 'big data', 'pubsub', 'airflow', 'application engine', 'machine learning', 'sql server', 'dataproc', 'cloud storage', 'bigtable', 'agile', 'sqoop', 'aws', 'data flow']",2025-06-11 05:23:15
NLP Data Engineer - Risk Insights & Monitoring,MNC IT,10 - 12 years,25-30 Lacs P.A.,"['Pune', 'Mumbai (All Areas)']","Design and implement state-of-the-art NLP models, including but not limited to text classification, semantic search, sentiment analysis, named entity recognition, and summary generation.\nconduct data preprocessing, and feature engineering to improve model accuracy and performance.\nStay updated with the latest developments in NLP and ML, and integrate cutting-edge techniques into our solutions.\ncollaborate with Cross-Functional Teams: Work closely with data scientists, software engineers, and product managers to align NLP projects with business objectives.\ndeploy models into production environments and monitor their performance to ensure robustness and reliability.\nmaintain comprehensive documentation of processes, models, and experiments, and report findings to stakeholders.\nimplement and deliver high quality software solutions / components for the Credit Risk monitoring platform.\nleverage his/her expertise to mentor developers; review code and ensure adherence to standards.\napply a broad range of software engineering practices, from analyzing user needs and developing new features to automated testing and deployment\nensure the quality, security, reliability, and compliance of our solutions by applying our digital principles and implementing both functional and non-functional requirements\nbuild observability into our solutions, monitor production health, help to resolve incidents, and remediate the root cause of risks and issues\nunderstand, represent, and advocate for client needs\nshare knowledge and expertise with colleagues , help with hiring, and contribute regularly to our engineering culture and internal communities.\nExpertise -\nBachelor of Engineering or equivalent.\nIdeally 8-10Yrs years of experience in NLP based applications focused on Banking / Finance sector.\nPreference for experience in financial data extraction and classification.\nInterested in learning new technologies and practices, reuse strategic platforms and standards, evaluate options, and make decisions with long-term sustainability in mind.\nProficiency in programming languages such as Python & Java. Experience with frameworks like TensorFlow, PyTorch, or Keras.\nIn-depth knowledge of NLP techniques and tools, including spaCy, NLTK, and Hugging Face.\nExperience with data handling and processing tools like Pandas, NumPy, and SQL.\nPrior experience in agentic AI, LLMs ,prompt engineering and generative AI is a plus.\nBackend development and microservices using Java Spring Boot, J2EE, REST for implementing projects with high SLA of data availability and data quality.\nExperience of building cloud ready and migrating applications using Azure and understanding of the Azure Native Cloud services, software design and enterprise integration patterns.\nKnowledge of SQL and PL/SQL (Oracle) and UNIX, writing queries, packages, working with joins, partitions, looking at execution plans, and tuning queries.\nA real passion for and experience of Agile working practices, with a strong desire to work with baked in quality subject areas such as TDD, BDD, test automation and DevOps principles\nExperience in Azure development including Databricks , Azure Services , ADLS etc.\nExperience using DevOps toolsets like GitLab, Jenkins",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['data engineer', 'Natural Language Processing', 'Python', 'Java']",2025-06-11 05:23:17
Data Engineer,Revature,1 - 2 years,Not Disclosed,['Chennai'],"The ideal candidate should have a strong background in SQL, BigQuery, and Google Cloud Platform (GCP), with hands-on experience in developing reports and dashboards using Looker Studio, Looker Standard, and LookML. Excellent communication skills and the ability to work collaboratively with cross-functional teams are essential for success in this role.\nKey Responsibilities:\nDesign, develop, and maintain dashboards and reports using Looker Studio and Looker Standard.\nDevelop and maintain LookML models, explores, and views to support business reporting requirements.\nOptimize and write advanced SQL queries for data extraction, transformation, and analysis.\nWork with BigQuery as the primary data warehouse for managing and analyzing large datasets.\nCollaborate with business stakeholders to understand data requirements and translate them into scalable reporting solutions.\nImplement data governance, access controls, and performance optimizations within the Looker environment.\nPerform root-cause analysis and troubleshooting for reporting and data issues.\nMaintain documentation for Looker projects, data models, and data dictionaries.\nStay updated with the latest Looker and GCP features and best practices.",,,,"['SQL', 'Python', 'Bigquery', 'Gcp Cloud']",2025-06-11 05:23:18
Data Engineer-Data Platforms-Azure,IBM,3 - 6 years,Not Disclosed,['Bengaluru'],"As Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in the development of data solutions using Spark Framework with Python or Scala on Hadoop and Azure Cloud Data Platform\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nStrong and proven background in Information Technology & working knowledge of .NET Core, C#, REST API, LINQ, Entity Framework, XUnit.\nTroubleshooting issues related to code performance.\nWorking knowledge of Angular 15 or later, Typescript, Jest Framework, HTML 5 and CSS 3 & MS SQL Databases, troubleshooting issues related to DB performance\nGood understanding of CQRS, mediator, repository pattern.\nGood understanding of CI/CD pipelines and SonarQube & messaging and reverse proxy\n\n\nPreferred technical and professional experience\nGood understanding of AuthN and AuthZ techniques like (windows, basic, JWT).\nGood understanding of GIT and it’s process like Pull request.\nMerge, pull, commit Methodology skills like AGILE, TDD, UML",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'css', 'information technology', 'html', 'python', 'cqrs', 'scala', 'ci/cd', 'sql', 'git', 'spark', 'uml', 'typescript', 'hadoop', 'big data', 'rest', 'entity framework', 'sonarqube', 'jest', 'xunit', 'data engineering', 'sql server', 'azure cloud', 'angular', 'linq', 'tdd', 'troubleshooting', 'agile']",2025-06-11 05:23:20
Principal Data Engineer (RDU IT Data Engineering),AstraZeneca India Pvt. Ltd,10 - 15 years,Not Disclosed,['Bengaluru'],"As a Principal Data Engineer at Alexion, you will be at the forefront of developing cutting-edge data integration solutions that cater to the dynamic needs of our global data platforms. Your expertise will be pivotal in designing, implementing, and managing robust data pipelines and integration paradigms. Collaborate closely with diverse IT teams to support data-driven decision-making and strategic initiatives. Your mission will be to build scalable, reliable, and resilient data solutions, enhance data quality and observability, and ensure compliance with industry standards and regulations. Become an advocate for data governance and best practices, empowering Alexion to leverage its data assets for business innovation and success.\nAccountabilities:\nDevelop and maintain high-quality data integration solutions to support business needs and strategic initiatives.\nCollaborate with IT teams to identify data needs, structure problems, and deliver integrated information solutions.\nEnsure the quality and security of Alexion s data through the implementation of best practices in data governance and compliance.\nStay abreast of industry trends and emerging technologies to drive continuous improvement in data engineering practices.\nEssential Skills/Experience:\nmasters Degree in Computer Science, Information Systems, Engineering, or a related field.\nA minimum of 10 years of experience in data engineering, data management, and analytics.\nProven track record of delivering large-scale, scalable, secure, and robust data solutions in the pharmaceutical or life sciences industry.\nStrong experience with SQL, Python, ETL/ELT frameworks, and building data orchestration pipelines.\nExpertise in cloud architectures, particularly AWS.\nProficiency in Snowflake and its features (resource monitors, RBAC controls, etc), dbT, Fivetran, Apache Airflow.\nStrong analytical, problem-solving, and organizational skills.\nAbility to effectively communicate complex data insights and solutions to diverse audiences, including senior leaders.\nAdvanced understanding of data warehousing methodologies and data modeling techniques (Kimball, 3NF, Star Schema, ).\nUnderstanding of data governance, compliance standards (GDPR, HIPAA), and FAIR and TRUSTED data principles.\nDesirable Skills/Experience:\nExtensive experience (5+ years) within the biotech/pharma industry.\nFamiliarity with Kubernetes, Docker/containerization, and Terraform.\nKnowledge of data quality and observability tools and methodologies to enhance data reliability.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data management', 'Data modeling', 'Analytical', 'Pharma', 'Apache', 'biomedical', 'Analytics', 'SQL', 'Python']",2025-06-11 05:23:22
"Data engineer with Gen AI- Balewadi, pune- hybrid",Indian MNC,5 - 10 years,15-30 Lacs P.A.,['Pune( Balewadi )'],"Role & responsibilities\nWe are seeking a skilled Data Engineer with advanced expertise in Python, PySpark, Databricks, and Machine Learning, along with a working knowledge of Generative and Agentic AI. This role is critical in ensuring data integrity and driving innovation across enterprise systems. You will design and implement ML-driven solutions to enhance Data Governance & Data Privacy initiatives through automation, self-service capabilities, and scalable, AI-enabled innovation.\nKey Responsibilities:\nImplement ML and Generative/Agentic AI solutions to optimize Data Governance processes.\nDesign, develop, and maintain scalable data pipelines using Python, PySpark, and Databricks.\nDevelop automation frameworks to support data quality, lineage, classification, and access control.\nDevelop and deploy machine learning models to uncover data patterns, detect anomalies, and enhance data governance and privacy compliance\nCollaborate with data stewards, analysts, and governance teams to build self-service data capabilities.\nWork with Databricks, Azure Data Lake, AWS, and other cloud-based data platforms for data engineering.\nBuild, configure, and integrate APIs for seamless system interoperability.\nEnsure data integrity, consistency, and compliance across systems and workflows.\nIntegrate AI models to support data discovery, metadata enrichment, and intelligent recommendations.\nOptimize data architecture to support analytics, reporting, and governance use cases.\nMonitor and improve the performance of ML/AI components in production environments.\nStay updated with emerging AI and data engineering technologies to drive continuous innovation.\nTechnical Skills:\nStrong programming skills in Python, PySpark, SQL for data processing and automation.\nExperience with Databricks and Snowflake (preferred) for building and maintaining data pipelines.\nExperience with Machine Learning model development and Generative/Agentic AI frameworks (e.g. LLMs, Transformers, LangChain) especially in the Data Management space\nExperience working with REST APIs & JSON for service integration\nExperience working with cloud-based platforms such as Azure, AWS, or GCP\nPower BI dashboard development experience is a plus.\nSoft Skills:\nStrong problem-solving skills and attention to detail.\nExcellent communication and collaboration abilities, with experience working across technical and business teams",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Generative Ai', 'Azure Databricks', 'Ml']",2025-06-11 05:23:23
DATA ENGINEER-ADVANCED ANALYTICS,IBM,5 - 10 years,Not Disclosed,['Bengaluru'],"Develop, test and support future-ready data solutions for customers across industry verticals\nDevelop, test, and support end-to-end batch and near real-time data flows/pipelines\nDemonstrate understanding in data architectures, modern data platforms, big data, analytics, cloud platforms, data governance and information management and associated technologies\nCommunicates risks and ensures understanding of these risks.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nMinimum of 5+ years of related experience required\nExperience in modeling and business system designs\nGood hands-on experience on DataStage, Cloud based ETL Services\nHave great expertise in writing TSQL code\nWell versed with data warehouse schemas and OLAP techniques\n\n\nPreferred technical and professional experience\nAbility to manage and make decisions about competing priorities and resources.\nAbility to delegate where appropriate\nMust be a strong team player/leader\nAbility to lead Data transformation project with multiple junior data engineers\nStrong oral written and interpersonal skills for interacting and throughout all levels of the organization.\nAbility to clearly communicate complex business problems and technical solutions.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data architecture', 'big data', 'information management', 'cloud platforms', 'data governance', 'schema', 't-sql', 'ansible', 'docker', 'sql', 'java', 'git', 'devops', 'linux', 'jenkins', 'j2ee', 'shell scripting', 'mysql', 'etl', 'python', 'data analytics', 'datastage', 'microsoft azure', 'warehouse', 'olap', 'aws']",2025-06-11 05:23:25
Data Engineer,MNC,4 - 8 years,Not Disclosed,['Hyderabad'],"Job Title: Software Engineer -Data Engineer\nPosition: Software Engineer\nExperience: 4-6 years (Less YOE will be Rejected)\nCategory: Software Development/ Engineering\nShift Timings: 1:00 pm to 10:00 pm\nMain location: Hyderabad\nWork Type: Work from office\nNotice Period: 0-30 Days\nSkill: Python, Pyspark, Data Bricks\nEmployment Type: Full Time\n\n• Bachelor's in Computer Science, Computer Engineering or related field\nRequired qualifications to be successful in this role\nMust have Skills:\n• 3+ yrs. Development experience with Spark (PySpark), Python and SQL.\n• Extensive knowledge building data pipelines\n• Hands on experience with Databricks Devlopment\n• Strong experience with\n• Strong experience developing on Linux OS.\n• Experience with scheduling and orchestration (e.g. Databricks Workflows,airflow, prefect, control-m).\n\nGood to have skills:\n• Solid understanding of distributed systems, data structures, design principles.\n• Agile Development Methodologies (e.g. SAFe, Kanban, Scrum).\n• Comfortable communicating with teams via showcases/demos.\n• Play key role in establishing and implementing migration patterns for the Data Lake Modernization project.\n• Actively migrate use cases from our on premises Data Lake to Databricks on GCP.\n• Collaborate with Product Management and business partners to understand use case requirements and reporting.\n• Adhere to internal development best practices/lifecycle (e.g. Testing, Code Reviews, CI/CD, Documentation) .\n• Document and showcase feature designs/workflows.\n• Participate in team meetings and discussions around product development.\n• Stay up to date on industry latest industry trends and design patterns.\n• 3+ years experience with GIT.\n• 3+ years experience with CI/CD (e.g. Azure Pipelines).\n• Experience with streaming technologies, such as Kafka, Spark.\n• Experience building applications on Docker and Kubernetes.\n• Cloud experience (e.g. Azure, Google).\n\nInterested Candidates can drop your Resume on Mail id :- "" tarun.k@talent21.in """,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Bricks', 'Python', 'Pyspark', 'SQL', 'ETL', 'Airflow', 'Azure Pipelines', 'Kafka', 'Design', 'Docker', 'Azure Cloud', 'Control-M', 'Cicd Pipeline', 'Modernization', 'testing', 'Documentation', 'Workflow', 'Code Review', 'Gcp Cloud', 'Product Development', 'Agile Methodology', 'GIT', 'Linux', 'GCP', 'Data Lake', 'Kubernetes']",2025-06-11 05:23:27
Data Engineer-Data Platforms-Azure,IBM,5 - 10 years,Not Disclosed,['Bengaluru'],"As an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nWe are seeking a skilled Azure Data Engineer with 5+ years of experience\nIncluding 3+ years of hands-on experience with ADF/Databricks\nThe ideal candidate Data bricks,Data Lake, Phyton programming skills.\nThe candidate will also have experience for deploying to data bricks.\nFamiliarity with Azure Data Factory\n\n\nPreferred technical and professional experience\nGood communication skills.\n3+ years of experience with ADF/DB/DataLake.\nAbility to communicate results to technical and non-technical audiences",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data factory', 'data bricks', 'oracle adf', 'data lake', 'db', 'hive', 'azure databricks', 'python', 'data management', 'azure data lake', 'scala', 'microsoft azure', 'pyspark', 'data warehousing', 'power bi', 'data engineering', 'sql', 'sql azure', 'spark', 'hadoop', 'sqoop', 'big data', 'etl', 'ssis']",2025-06-11 05:23:28
Data Engineer-Data Platforms-Azure,IBM,5 - 7 years,Not Disclosed,['Bengaluru'],"As Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in the development of data solutions using Spark Framework with Python or Scala on Hadoop and Azure Cloud Data Platform\nResponsibilities\nExperienced in building data pipelines to Ingest, process, and transform data from files, streams and databases. Process the data with Spark, Python, PySpark and Hive, Hbase or other NoSQL databases on Azure Cloud Data Platform or HDFS\nExperienced in develop efficient software code for multiple use cases leveraging Spark Framework / using Python or Scala and Big Data technologies for various use cases built on the platform\nExperience in developing streaming pipelines\nExperience to work with Hadoop / Azure eco system components to implement scalable solutions to meet the ever-increasing data volumes, using big data/cloud technologies Apache Spark, Kafka, any Cloud computing etc\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nTotal 5 - 7+ years of experience in Data Management (DW, DL, Data Platform, Lakehouse) and Data Engineering skills\nMinimum 4+ years of experience in Big Data technologies with extensive data engineering experience in Spark / Python or Scala. Minimum 3 years of experience on Cloud Data Platforms on Azure\nExperience in DataBricks / Azure HDInsight / Azure Data Factory, Synapse, SQL Server DB\nExposure to streaming solutions and message brokers like Kafka technologies\nExperience Unix / Linux Commands and basic work experience in Shell Scripting\n\n\nPreferred technical and professional experience\nCertification in Azure and Data Bricks or Cloudera Spark Certified developers",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data management', 'big data technologies', 'microsoft azure', 'data engineering', 'hadoop', 'hive', 'python', 'scala', 'pyspark', 'azure data factory', 'azure hdinsight', 'azure cloud', 'sql server', 'nosql', 'sql', 'data bricks', 'spark', 'kafka', 'linux', 'shell scripting', 'big data', 'hbase', 'unix']",2025-06-11 05:23:30
Data Engineer,MNC,4 - 9 years,Not Disclosed,['Hyderabad'],"Job Title: Software Engineer -Data Engineer\nPosition: Software Engineer\nExperience: 4-9 years\nCategory: Software Development/ Engineering\nShift Timings: 1:00 pm to 10:00 pm\nMain location: Hyderabad\nWork Type: Work from office\nNotice Period: 0-30 Days\nSkill: Python, Pyspark, Data Bricks\nEmployment Type: Full Time\n\n• Bachelor's in Computer Science, Computer Engineering or related field\nRequired qualifications to be successful in this role\nMust have Skills:\n• 3+ yrs. Development experience with Spark (PySpark), Python and SQL.\n• Extensive knowledge building data pipelines\n• Hands on experience with Databricks Devlopment\n• Strong experience with\n• Strong experience developing on Linux OS.\n• Experience with scheduling and orchestration (e.g. Databricks Workflows,airflow, prefect, control-m).\n\nGood to have skills:\n• Solid understanding of distributed systems, data structures, design principles.\n• Agile Development Methodologies (e.g. SAFe, Kanban, Scrum).\n• Comfortable communicating with teams via showcases/demos.\n• Play key role in establishing and implementing migration patterns for the Data Lake Modernization project.\n• Actively migrate use cases from our on premises Data Lake to Databricks on GCP.\n• Collaborate with Product Management and business partners to understand use case requirements and reporting.\n• Adhere to internal development best practices/lifecycle (e.g. Testing, Code Reviews, CI/CD, Documentation) .\n• Document and showcase feature designs/workflows.\n• Participate in team meetings and discussions around product development.\n• Stay up to date on industry latest industry trends and design patterns.\n• 3+ years experience with GIT.\n• 3+ years experience with CI/CD (e.g. Azure Pipelines).\n• Experience with streaming technologies, such as Kafka, Spark.\n• Experience building applications on Docker and Kubernetes.\n• Cloud experience (e.g. Azure, Google).\n\nInterested Candidates can drop your Resume on Mail id :- "" kalyan.v@talent21.in """,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Bricks', 'Python', 'Pyspark', 'SQL', 'ETL', 'Airflow', 'Azure Pipelines', 'Kafka', 'Design', 'Docker', 'Azure Cloud', 'Control-M', 'Cicd Pipeline', 'Modernization', 'testing', 'Documentation', 'Workflow', 'Code Review', 'Gcp Cloud', 'Product Development', 'Agile Methodology', 'GIT', 'Linux', 'GCP', 'Data Lake', 'Kubernetes']",2025-06-11 05:23:32
Data Engineer-Data Platforms-AWS,IBM,5 - 7 years,Not Disclosed,['Bengaluru'],"As Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in the development of data solutions using Spark Framework with Python or Scala on Hadoop and AWS Cloud Data Platform\nExperienced in building data pipelines to Ingest, process, and transform data from files, streams and databases. Process the data with Spark, Python, PySpark, Scala, and Hive, Hbase or other NoSQL databases on Cloud Data Platforms (AWS) or HDFS\nExperienced in develop efficient software code for multiple use cases leveraging Spark Framework / using Python or Scala and Big Data technologies for various use cases built on the platform\nExperience in developing streaming pipelines\nExperience to work with Hadoop / AWS eco system components to implement scalable solutions to meet the ever-increasing data volumes, using big data/cloud technologies Apache Spark, Kafka, any Cloud computing etc\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nTotal 5 - 7+ years of experience in Data Management (DW, DL, Data Platform, Lakehouse) and Data Engineering skills\nMinimum 4+ years of experience in Big Data technologies with extensive data engineering experience in Spark / Python or Scala.\nMinimum 3 years of experience on Cloud Data Platforms on AWS; Exposure to streaming solutions and message brokers like Kafka technologies.\nExperience in AWS EMR / AWS Glue / DataBricks, AWS RedShift, DynamoDB\nGood to excellent SQL skills\n\n\nPreferred technical and professional experience\nCertification in AWS and Data Bricks or Cloudera Spark Certified developers\n AWS S3 ,  Redshift , and  EMR  for data storage and distributed processing.\n AWS Lambda ,  AWS Step Functions , and  AWS Glue  to build  serverless, event-driven data workflows  and orchestrate ETL processes",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data management', 'scala', 'big data technologies', 'sql', 'data engineering', 'hive', 'cloudera', 'glue', 'amazon redshift', 'pyspark', 'emr', 'spark', 'aws cloud', 'aws emr', 'hadoop', 'big data', 'etl', 'hbase', 'python', 'dynamo db', 'serverless', 'aws lambda', 'nosql', 'aws glue', 'data bricks', 'kafka', 'aws']",2025-06-11 05:23:34
Data Engineer,MNC Group,5 - 8 years,Not Disclosed,['Pune( Pune Nagar Road )'],"Knowledge and hands-on experience writing effective SQL queries and statements Understanding of AWS services At least 5 years of experience in a similar capacity At least 3 years of proficiency in using Python to develop and modify scripts At least 3 years of proficiency in managing data ingestion and DAG maintenance in airflow Preferred Requirements Knowledge in Hadoop Ecosystem like Spark or PySpark Knowledge in AWS services like S3, Data Lake, Redshift, EMR, EC2, Lambda, Glue, Aurora, RDS, Airflow.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Temporary/Contractual","['Pyspark', 'Airflow', 'Glue', 'AWS', 'Python', 'SQL']",2025-06-11 05:23:35
Data Engineer-Data Integration,IBM,2 - 5 years,Not Disclosed,['Pune'],"As an Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nVery good experience on Continuous Flow Graph tool used for point based development\nDesign, develop, and maintain ETL processes using Ab Initio tools.\nWrite, test, and deploy Ab Initio graphs, scripts, and other necessary components.\nTroubleshoot and resolve data processing issues and improve performance.\nData Integration:Extract, transform, and load data from various sources into data warehouses, operational data stores, or other target systems.\nWork with different data formats, including structured, semi-structured, and unstructured data\n\n\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ab initio', 'elastic search', 'splunk', 'etl', 'data integration', 'hlookup', 'conditional formatting', 'macros', 'charts', 'data management', 'data analysis', 'rtd', 'pivot table', 'data warehousing', 'vlookup', 'sql', 'control valves', 'advanced excel', 'pivot', 'instrumentation', 'big data', 'unix']",2025-06-11 05:23:37
Data Engineer-Data Integration,IBM,2 - 5 years,Not Disclosed,['Hyderabad'],"As an Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nVery good experience on Continuous Flow Graph tool used for point based development\nDesign, develop, and maintain ETL processes using Ab Initio tools.\nWrite, test, and deploy Ab Initio graphs, scripts, and other necessary components.\nTroubleshoot and resolve data processing issues and improve performance\n\nData Integration:\n\nExtract, transform, and load data from various sources into data warehouses, operational data stores, or other target systems.\n\nWork with different data formats, including structured, semi-structured, and unstructured data\n\n\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ab initio', 'elastic search', 'splunk', 'etl', 'data integration', 'hlookup', 'conditional formatting', 'macros', 'charts', 'data management', 'data analysis', 'rtd', 'pivot table', 'data warehousing', 'vlookup', 'sql', 'control valves', 'advanced excel', 'pivot', 'instrumentation', 'big data', 'unix']",2025-06-11 05:23:39
Data Engineer-Data Integration,IBM,2 - 5 years,Not Disclosed,['Pune'],"As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExpertise in Data warehousing/ information Management/ Data Integration/Business Intelligence using ETL tool Informatica PowerCenter\nKnowledge of Cloud, Power BI, Data migration on cloud skills.\nExperience in Unix shell scripting and python\nExperience with relational SQL, Big Data etc\n\n\nPreferred technical and professional experience\nKnowledge of MS-Azure Cloud\nExperience in Informatica PowerCenter\nExperience in Unix shell scripting and python",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['information management', 'data warehousing', 'business intelligence', 'etl', 'data integration', 'python', 'data management', 'informatica powercenter', 'power bi', 'relational sql', 'data migration', 'azure cloud', 'sql', 'unix shell scripting', 'java', 'etl tool', 'big data', 'informatica', 'unix']",2025-06-11 05:23:41
Data Engineer-Data Integration,IBM,2 - 5 years,Not Disclosed,['Pune'],"As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\n\n\n Your primary responsibilities include: \nDevelop & maintain data pipelines for batch & stream processing using informatica power centre or cloud ETL/ELT tools.\nLiaise with business team and technical leads, gather requirements, identify data sources, identify data quality issues, design target data structures, develop pipelines and data processing routines, perform unit testing and support UAT.\nWork with data scientist and business analytics team to assist in data ingestion and data-related technical issues.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExpertise in Data warehousing/ information Management/ Data Integration/Business Intelligence using ETL tool Informatica PowerCenter\nKnowledge of Cloud, Power BI, Data migration on cloud skills.\nExperience in Unix shell scripting and python\nExperience with relational SQL, Big Data etc\n\n\nPreferred technical and professional experience\nKnowledge of MS-Azure Cloud\nExperience in Informatica PowerCenter\nExperience in Unix shell scripting and python",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['information management', 'data warehousing', 'business intelligence', 'etl', 'data integration', 'python', 'informatica powercenter', 'power bi', 'relational sql', 'data migration', 'azure cloud', 'sql', 'elastic search', 'unix shell scripting', 'splunk', 'agile', 'big data', 'informatica']",2025-06-11 05:23:42
"Director, Data Engineering (AI/ML, GenAI, Spark, Java)",Visa,10 - 15 years,Not Disclosed,['Bengaluru'],"Payments Industry is a very exciting and fast-developing area with lot of new and innovative solutions coming to market. With strong demand for new solutions in this space, it promises to be an exciting area of innovation. VISA is a strong leader in the payment industry and is rapidly transitioning into a technology company with significant investments in this area.\nIf you want to be in the exciting payment space, learn fast and make big impacts, Ecosystem & Operational Risk technology which is part of Visa s Value-Added Services business unit is an ideal place for you!\n\nIn Ecosystem & Operational Risk (EOR) technology group, the Payment Fraud Disruption team is responsible for building critical risk and fraud detection and prevention applications and services at Visa. This includes idea generation, architecture, design, development, and testing of products, applications, and services that provide Visa clients with solutions to detect, prevent, and mitigate fraud for Visa and Visa client payment systems.\nWe are in search of inquisitive, creative, and skillful technologists to join our ranks. We are currently looking for a Director of Software Engineering who will take the lead and manage several strategic initiatives within our organization.\nThe right candidate will possess strong software engineering background, with demonstrated leadership experience in driving technical architecture, design and delivery of products and services that have created business value and delivered impact within the payments or payments risk domain or similar industries.\nThis position is ideal for an experienced engineering leader who is passionate about collaborating with business and technology partners and engineers to solve challenging business problems. You will be a key driver in the effort to define the shared strategic vision for the Payment Fraud Disruption platform and defining tools and services that safeguard Visa s payment systems.\nThis is a hybrid position. Expectation of days in office will be confirmed by your Hiring Manager.\n\n\nBasic Qualifications\n10+ years of relevant work experience and a Bachelors degree, OR 13+ years of relevant work experience\n\nPreferred Qualifications\n12 or more years of work experience with a Bachelor s Degree or 8-10 years of experience with an Advanced Degree (e.g. Masters, MBA, JD, MD) or 6+ years of work experience with a PhD\nExperience leading delivery and deployment of ML models, model refresh, and experimentation.\nExperience leading product development & delivery of AI/ML solutions, applied to real-world problems\nExperience leading design and development of mission-critical, secure, reliable systems\nExperience leading delivery across multiple technologies: Python, Java/J2EE, Apache Kafka, Apache Flink, Hive, MySQL, Hadoop, Spark, Scala, design patterns, test automation frameworks\nExperience leading delivery of streaming analytics platforms\nExcellent understanding of algorithms and data structures\nExcellent problem solving and analytics skills. Capable of forming and advocating an independent viewpoint\nStrong experience with agile methodologies\nExcel in partnering with Product leaders and technical product managers on requirements workshops, helping define joint product/technology roadmaps & driving prioritization\nExperience driving continuous improvements to processes/tools for better developer efficiency and productivity\nDemonstrated ability to drive measurable improvements across engineering, delivery and performance metrics\nDemonstrated success in leading high performing, multi-disciplinary and geographically distributed engineering teams. Demonstrated ability to hire, develop and retain high-caliber talent\nMust demonstrate longer-term strategic thinking and staying abreast with latest technologies to assess what s possible technically\nStrong collaboration and effective communication, with focus on win-win outcomes",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Payment systems', 'Operational risk', 'MySQL', 'SCALA', 'Agile', 'Manager Technology', 'Data structures', 'Apache', 'Analytics', 'Python']",2025-06-11 05:23:44
Data Engineer,Get Your Job,3 - 8 years,9.5-18 Lacs P.A.,"['Hyderabad', 'Gurugram']","3-8 Years exp.\nJob Location – Gurgaon and Hyderabad\nWork Mode – Hybrid (3-4 Days work from office)\nNotice Period – Immediate to 30 Days Official NP, OR 45 days serving NP candidates, ONLY.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Hive', 'hadoop', 'Python', 'Spark', 'SQL']",2025-06-11 05:23:46
Data Engineer-Data Integration,IBM,2 - 5 years,Not Disclosed,['Pune'],"As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You’ll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you’ll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you’ll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExpertise in designing and implementing scalable data warehouse solutions on Snowflake, including schema design, performance tuning, and query optimization.\nStrong experience in building data ingestion and transformation pipelines using Talend to process structured and unstructured data from various sources.\nProficiency in integrating data from cloud platforms into Snowflake using Talend and native Snowflake capabilities.\nHands-on experience with dimensional and relational data modelling techniques to support analytics and reporting requirements\n\n\nPreferred technical and professional experience\nUnderstanding of optimizing Snowflake workloads, including clustering keys, caching strategies, and query profiling.\nAbility to implement robust data validation, cleansing, and governance frameworks within ETL processes.\nProficiency in SQL and/or Shell scripting for custom transformations and automation tasks",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'sql', 'pipeline', 'data ingestion', 'shell scripting', 'python', 'data management', 'performance tuning', 'talend', 'data warehousing', 'cloud platforms', 'warehouse', 'docker', 'ansible', 'database design', 'elastic search', 'java', 'devops', 'splunk', 'etl', 'big data', 'aws', 'data integration']",2025-06-11 05:23:47
Data Engineer,Meritus Management Service,5 - 10 years,10-20 Lacs P.A.,"['Nagpur', 'Pune']","We are looking for a skilled Data Engineer to design, build, and manage scalable data pipelines and ensure high-quality, secure, and reliable data infrastructure across our cloud and on-prem platforms.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Synapse Analytics', 'SQL', 'Azure Data Factory', 'Python', 'API Integration', 'Postgresql', 'Data Bricks', 'Scripting', 'SCALA', 'Data Lake', 'MongoDB', 'Data Warehousing', 'Data Modeling', 'ETL', 'Azure Devops']",2025-06-11 05:23:49
"Tcs is hiring For Azure Data Engineer (ADF, Python, Pyspark,DataBricks",Tata Consultancy Services,5 - 10 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Role & responsibilities\nStrong understanding of Azure environment (PaaS, IaaS) and experience in working with Hybrid model\nAt least 1 project experience in Azure Data Stack that involves components like Azure Data Lake, Azure Synapse Analytics, Azure Data Factory, Azure Data Bricks, Azure Analysis Service, Azure SQL DWH\nStrong hands-on SQL/T-SQL/Spark SQL and database concepts",,,,"['Azure Databricks', 'Azure Data Factory', 'Pyspark', 'Python Data', 'Microsoft Azure', 'Devops', 'Python', 'SQL']",2025-06-11 05:23:50
Data Engineer-Data Integration,IBM,2 - 5 years,Not Disclosed,['Chennai'],"As a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\n\n\n\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nDesign, develop, and maintain Ab Initio graphs for extracting, transforming, and loading (ETL) data from diverse sources to various target systems.\nImplement data quality and validation processes within Ab Initio.\nData Modeling and Analysis:.\nCollaborate with data architects and business analysts to understand data requirements and translate them into effective ETL processes.\nAnalyze and model data to ensure optimal ETL design and performance.\nAb Initio Components:. . Utilize Ab Initio components such as Transform Functions, Rollup, Join, Normalize, and others to build scalable and efficient data integration solutions.\nImplement best practices for reusable Ab Initio components\n\n\nPreferred technical and professional experience\nOptimize Ab Initio graphs for performance, ensuring efficient data processing and minimal resource utilization.\nConduct performance tuning and troubleshooting as needed.\nCollaboration:. .\nWork closely with cross-functional teams, including data analysts, database administrators, and quality assurance, to ensure seamless integration of ETL processes.\nParticipate in design reviews and provide technical expertise to enhance overall solution quality.\nDocumentation:.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ab initio', 'performance tuning', 'data modeling', 'troubleshooting', 'etl', 'hive', 'python', 'data analysis', 'oracle', 'data warehousing', 'data engineering', 'sql', 'plsql', 'data quality', 'tableau', 'unix shell scripting', 'spark', 'hadoop', 'big data', 'aws', 'data integration', 'informatica', 'unix']",2025-06-11 05:23:52
Data Engineer-Data Integration,IBM,2 - 5 years,Not Disclosed,['Pune'],"As a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our client’s business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nDesign, develop, and maintain Ab Initio graphs for extracting, transforming, and loading (ETL) data from diverse sources to various target systems.\nImplement data quality and validation processes within Ab Initio.\nData Modeling and Analysis:.\nCollaborate with data architects and business analysts to understand data requirements and translate them into effective ETL processes.\nAnalyze and model data to ensure optimal ETL design and performance.\nAb Initio Components:. . Utilize Ab Initio components such as Transform Functions, Rollup, Join, Normalize, and others to build scalable and efficient data integration solutions.\nImplement best practices for reusable Ab Initio components\n\n\nPreferred technical and professional experience\nOptimize Ab Initio graphs for performance, ensuring efficient data processing and minimal resource utilization.\nConduct performance tuning and troubleshooting as needed.\nCollaboration:. .\nWork closely with cross-functional teams, including data analysts, database administrators, and quality assurance, to ensure seamless integration of ETL processes.\nParticipate in design reviews and provide technical expertise to enhance overall solution quality.\nDocumentation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ab initio', 'performance tuning', 'data modeling', 'troubleshooting', 'etl', 'hive', 'python', 'data analysis', 'oracle', 'data warehousing', 'data engineering', 'sql', 'plsql', 'data quality', 'tableau', 'unix shell scripting', 'spark', 'hadoop', 'big data', 'aws', 'data integration', 'informatica', 'unix']",2025-06-11 05:23:54
Data Engineer,Truefirms,6 - 9 years,7-14 Lacs P.A.,['Hyderabad'],"Role Overview:\nWe are seeking a talented and forward-thinking Data Engineer for one of the large financial services GCC based in Hyderabad with responsibilities that include designing and constructing data pipelines, integrating data from multiple sources, developing scalable data solutions, optimizing data workflows, collaborating with cross-functional teams, implementing data governance practices, and ensuring data security and compliance.\n\nTechnical Requirements:\n\n1. Proficiency in ETL, Batch, and Streaming Process\n2. Experience with BigQuery, Cloud Storage, and CloudSQL\n3. Strong programming skills in Python, SQL, and Apache Beam for data processing\n4. Understanding of data modeling and schema design for analytics\n5. Knowledge of data governance, security, and compliance in GCP\n6. Familiarity with machine learning workflows and integration with GCP ML tools\n7. Ability to optimize performance within data pipelines\n\nFunctional Requirements:\n\n1. Ability to collaborate with Data Operations, Software Engineers, Data Scientists, and Business SMEs to develop Data Product Features\n2. Experience in leading and mentoring peers within an existing development team\n3. Strong communication skills to craft and communicate robust solutions\n4. Proficient in working with Engineering Leads, Enterprise and Data Architects, and Business Architects to build appropriate data foundations\n5. Willingness to work on contemporary data architecture in Public and Private Cloud environments T\nhis role offers a compelling opportunity for a seasoned Data Engineering to drive transformative cloud initiatives within the financial sector, leveraging unparalleled experience and expertise to deliver innovative cloud solutions that align with business imperatives and regulatory requirements.\nQualification Engineering Grad / Postgraduate\nCRITERIA\n1. Proficient in ETL, Python, and Apache Beam for data processing efficiency.\n2. Demonstrated expertise in BigQuery, Cloud Storage, and CloudSQL utilization.\n3. Strong collaboration skills with cross-functional teams for data product development.\n4. Comprehensive knowledge of data governance, security, and compliance in GCP.\n5. Experienced in optimizing performance within data pipelines for efficiency.\n6. Relevant Experience: 6-9 years\n\nConnect at 9993809253",Industry Type: IT Services & Consulting,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['Cloud Storage', 'GCC', 'ETL Tool', 'Gcp Cloud', 'integration with GCP ML tools', 'Bigquery', 'Apache Beam', 'Data Modeling', 'Python']",2025-06-11 05:23:55
Azure Data Engineer,Infosys,5 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","We are looking for Azure Data Engineer's resources having minimum 5 to 9 years of Experience.\n\nTo Apply, use the below link:\nhttps://career.infosys.com/jobdesc?jobReferenceCode=INFSYS-EXTERNAL-210775&rc=0\n\nRole & responsibilities\nBlend of technical expertise with 5 to 9 year of experience, analytical problem-solving, and collaboration with cross-functional teams. Design and implement Azure data engineering solutions (Ingestion & Curation)\nCreate and maintain Azure data solutions including Azure SQL Database, Azure Data Lake, and Azure Blob Storage.\nDesign, implement, and maintain data pipelines for data ingestion, processing, and transformation in Azure.\nUtilizing Azure Data Factory or comparable technologies, create and maintain ETL (Extract, Transform, Load) operations\nUse Azure Data Factory and Databricks to assemble large, complex data sets\nImplementing data validation and cleansing procedures will ensure the quality, integrity, and dependability of the data.\nEnsure data quality / security and compliance.\nOptimize Azure SQL databases for efficient query performance.\nCollaborate with data engineers, and other stakeholders to understand requirements and translate them into scalable and reliable data platform architectures.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Databricks', 'Azure Synapse', 'Pyspark', 'Azure Data Warehouse', 'Azure Data Lake', 'Azure Blob Storage', 'SQL Azure']",2025-06-11 05:23:57
Data Engineer,reycruit,7 - 12 years,35-40 Lacs P.A.,['Hyderabad'],"Looking for 8+ years\nPython+Azure/Aws cloud is mandatory\n1st round- virtual\n2nd round- F2F\nMust have 7+ years of relevant experience- should have hands-on experience with ETL/ELT processes, cloud-based data solutions, and big data technologies.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Pipeline', 'Big Data', 'Elt', 'ETL']",2025-06-11 05:23:59
AWS Databricks Data Engineer,Tata Consultancy Services,6 - 11 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']","Role & responsibilities\n\nData Engineer, Expertise in AWS, Databricks and Pyspark",,,,"['Pyspark', 'databricks', 'AWS', 'data engineer', 'Aws Databricks']",2025-06-11 05:24:00
Data Engineer-Data Platforms-Google,IBM,6 - 11 years,Not Disclosed,['Bengaluru'],"6+ years of industry work experience\nExperience extracting data from a variety of sources, and a desire to expand those skills\nWorked on Google Looker tool\nWorked on Big Query and GCP technologies\nStrong SQL and Spark knowledge\nExcellent Data Analysis skills. Must be comfortable with querying and analyzing large amount of data on Hadoop HDFS using Hive and Spark\nKnowledge of Financial Accounting is a bonus\nWork independently with cross functional team and drive towards the resolution\nExperience with Object oriented programming using python and its design patterns\nExperience handling Unix systems, for optimal usage to host enterprise web applications GCP certifications preferred.\nPayments Industry Background good to have\nCandidate who has been part to google Cloud Migration is an ideal Fit\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n3-5 years of experience\nIntuitive individual with an ability to manage change and proven time management\nProven interpersonal skills while contributing to team effort by accomplishing related results as needed\nUp-to-date technical knowledge by attending educational workshops, reviewing publications\n\n\nPreferred technical and professional experience\n6+ years of industry work experience\nExperience extracting data from a variety of sources, and a desire to expand those skills\nWorked on Google Looker tool",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data analysis', 'sql', 'spark', 'hadoop', 'hive', 'snowflake', 'data warehousing', 'power bi', 'google', 'business intelligence', 'sql server', 'qlikview', 'tableau', 'data modeling', 'gcp', 'financial accounting', 'bigquery', 'object oriented programming', 'data visualization', 'etl', 'ssis', 'unix']",2025-06-11 05:24:02
Data Engineer-Master Data Management,IBM,6 - 11 years,Not Disclosed,['Bengaluru'],"Design, develop, and maintain applications using Java/J2EE and IBM Infosphere MDM Advanced Edition v11.x.\nParticipate in development and support projects including fail & fix and maintenance activities.\nCollaborate with clients and internal teams to understand requirements and deliver solutions.\nProvide timely and effective support for multiple clients, ensuring quick resolution of issues.\nEngage directly with clients in a client-facing role, ensuring clear communication and understanding of business needs.\nParticipate in on-call support and be flexible to extend support after hours when required.\nFollow software engineering best practices, including continuous integration and continuous delivery (CI/CD\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n6+ years of hands-on experience in Java/J2EE technologies.\n4+ years of hands-on development experience in IBM Infosphere MDM Advanced Edition v11.x.\nStrong understanding and practical experience with CI/CD tools and practices.\nProven experience in development and support projects, including maintenance and issue resolution.\nStrong client interaction and communication skills, with experience in direct client-facing roles\n\n\nPreferred technical and professional experience\nPrior experience in the Banking domain or financial services industry.\nExposure to Agile methodologies and collaborative development environments.\nFamiliarity with other enterprise data management tools or technologies",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'java', 'cd tools', 'client interaction', 'ci / cd tools', 'kubernetes', 'data management', 'ci/cd', 'ansible', 'docker', 'sql', 'plsql', 'git', 'devops', 'linux', 'j2ee', 'jenkins', 'shell scripting', 'ibm infosphere', 'python', 'maven', 'mdm', 'terraform', 'agile', 'aws', 'j2ee technologies', 'unix']",2025-06-11 05:24:04
Data Engineer-Data Platforms-Azure,IBM,3 - 6 years,Not Disclosed,['Bengaluru'],"Establish and implement  best practices  for  DBT workflows , ensuring efficiency, reliability, and maintainability.\nCollaborate with  data analysts, engineers, and business teams  to align data transformations with business needs.\nMonitor and troubleshoot  data pipelines  to ensure accuracy and performance.\nWork with  Azure-based cloud technologies  to support data storage, transformation, and processing\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nStrong MS SQL, Azure Databricks experience\nImplement and manage data models in DBT, data transformation and alignment with business requirements.\nIngest raw, unstructured data into structured datasets to cloud object store.\nUtilize DBT to convert raw, unstructured data into structured datasets, enabling efficient analysis and reporting.\nWrite and optimize SQL queries within DBT to enhance data transformation processes and improve overall performance\n\n\nPreferred technical and professional experience\nEstablish best DBT processes to improve performance, scalability, and reliability.\nDesign, develop, and maintain scalable data models and transformations using DBT in conjunction with Databricks\nProven interpersonal skills while contributing to team effort by accomplishing related results as required",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure databricks', 'sql queries', 'sql server', 'sql', 'ssis', 'microsoft power bi', 'python', 'azure data lake', 'ssas', 'microsoft azure', 'power bi', 'data warehousing', 'pyspark', 'azure data factory', 't-sql', 'azure logic apps', 'azure functions', 'sql azure', 'spark', 'ssrs', 'azure cosmosdb', 'etl', 'msbi']",2025-06-11 05:24:06
Data Engineer,Lance Labs,7 - 12 years,Not Disclosed,"['Noida', 'Chennai']","Deployment, configuration & maintenance of Databricks clusters & workspaces\nSecurity & Access Control\nAutomate administrative task using tools like Python, PowerShell &Terraform\nIntegrations with Azure Data Lake, Key Vault & implement CI/CD pipelines\n\nRequired Candidate profile\nAzure, AWS, or GCP; Azure experience is preferred\nStrong skills in Python, PySpark, PowerShell & SQL\nExperience with Terraform\nETL processes, data pipeline &big data technologies\nSecurity & Compliance",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Azure Databricks', 'SQL', 'Terraform', 'Python', 'Powershell', 'Ci/Cd', 'Data Pipeline', 'GCP', 'Azure Cloud', 'Azure Data Lake', 'ETL', 'AWS', 'Data Governance', 'Azure Devops']",2025-06-11 05:24:07
Data Engineering Specialist,Accenture,4 - 8 years,Not Disclosed,"['Pune', 'Gurugram', 'Bengaluru']","Job Summary:\nWe are:\nSales Excellence. Sales Excellence at Accenture empowers our people to compete, win and grow. We provide everything they need to grow their client portfolios, optimize their deals and enable their sales talent, all driven by sales intelligence.\nThe team will be aligned to the Client Success, which is a new function to support Accenture’s approach to putting client value and client experience at the heart of everything we do to foster client love. Our ambition is that every client loves working with Accenture and believes we’re the ideal partner to help them create and realize their vision for the future – beyond their expectations.\nYou are:\nA builder at heart – curious about new tools and their usefulness, eager to create prototypes, and adaptable to changing paths. You enjoy sharing your experiments with a small team and are responsive to the needs of your clients.\nThe work:\nThe Center of Excellence (COE) enables Sales Excellence to deliver best-in-class service offerings to Accenture leaders, practitioners, and sales teams.\nAs a member of the COE Analytics Tools & Reporting team, you will help in building and enhancing data foundation for reporting tools and Analytics tool to provide insights on underlying trends and key drivers of the business.\n\nRoles & Responsibilities:\nCollaborate with the Client Success, Analytics COE, CIO Engineering/DevOps team, and stakeholders to build and enhance Client success data lake.\nWrite complex SQL scripts to transform data for the creation of dashboards or reports and validate the accuracy and completeness of the data.\nBuild automated solutions to support any business operation or data transfer.\nDocument and build efficient data model for reporting and analytics use case.\nAssure the Data Lake data accuracy, consistency, and timeliness while ensuring user acceptance and satisfaction.\nWork with the Client Success, Sales Excellence COE members, CIO Engineering/DevOps team and Analytics Leads to standardize Data in data lake.\nProfessional & Technical Skills:\nBachelor’s degree or equivalent experience in Data Engineering, analytics, or similar field.\nAt least 4 years of professional experience in developing and managing ETL pipelines.\nA minimum of 2 years of GCP experience.\nAbility to write complex SQL and prepare data for dashboarding.\nExperience in managing and documenting data models.\nUnderstanding of Data governance and policies.\nProficiency in Python and SQL scripting language.\nAbility to translate business requirements into technical specification for engineering team.\nCuriosity, creativity, a collaborative attitude, and attention to detail.\nAbility to explain technical information to technical as well as non-technical users.\nAbility to work remotely with minimal supervision in a global environment.\nProficiency with Microsoft office tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['GCP', 'Python', 'SQL', 'Data Engineering', 'ETL']",2025-06-11 05:24:09
Software Engineer II - Data Engineer,JPMorgan Chase Bank,0 - 4 years,Not Disclosed,['Hyderabad'],"You thrive on diversity and creativity, and we welcome individuals who share our vision of making a lasting impact. Your unique combination of design thinking and experience will help us achieve new heights.\n\n\n\n\n\n\nAs a Data Engineer II at JPMorgan Chase within the Consumer Community Banking Team, you are part of an agile team that works to enhance, design, and deliver the data collection, storage, access, and analytics solutions in a secure, stable, and scalable way. As an emerging member of a data engineering team, you execute data solutions through the design, development, and technical troubleshooting of multiple components within a technical product, application, or system, while gaining the skills and experience needed to grow within your role.\n\n\nJob responsibilities\n\n\n\nOrganizes, updates, and maintains gathered data that will aid in making the data actionable\n\nDemonstrates basic knowledge of the data system components to determine controls needed to ensure secure data access\n\nBe responsible for making custom configuration changes in one to two tools to generate a product at the business or customer request\n\nUpdates logical or physical data models based on new use cases with minimal supervision\n\nAdds to team culture of diversity, equity, inclusion, and respect\n\nGathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of software applications and systems\n\nProactively identifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture\n\n\n\n\n\nRequired qualifications, capabilities, and skills\n\n\n\nFormal training or certification on Data Engineering concepts and 3+ years applied experience in AWS and Kubernetes\n\nProficiency in one or more large-scale data processing distributions such as JavaSpark/PySpark along with knowledge on Data Pipeline (DPL), Data Modeling, Data warehouse, Data Migration and so-on.\n\nExperience across the data lifecycle along with expertise with consuming data in any of batch (file), near real-time (IBM MQ, Apache Kafka), streaming (AWS kinesis, MSK)\n\nAdvanced at SQL (e. g. , joins and aggregations)\n\nWorking understanding of NoSQL databases\n\nExperience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages.\n\nSolid understanding of agile methodologies such as CI/CD, Application Resiliency, and Security\n\nSignificant experience with statistical data analysis and ability to determine appropriate tools and data patterns to perform analysis\n\nExperience customizing changes in a tool to generate product\n\n\n\nPreferred qualifications, capabilities, and skills\n\n\n\nFamiliarity with modern front-end technologies\n\nExperience designing and building REST API services using Java\n\nExposure to cloud technologies - knowledge on Hybrid cloud architectures is highly desirable.\n\nAWS Developer/Solutions Architect Certification is highly desired",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['System architecture', 'Data analysis', 'Data modeling', 'Coding', 'Debugging', 'Agile', 'Apache', 'Troubleshooting', 'Analytics', 'SQL']",2025-06-11 05:24:10
Software Engineer II - Data Engineer,JPMorgan Chase Bank,0 - 4 years,Not Disclosed,['Hyderabad'],"You thrive on diversity and creativity, and we welcome individuals who share our vision of making a lasting impact. Your unique combination of design thinking and experience will help us achieve new heights.\nAs a Data Engineer II at JPMorgan Chase within the Consumer Community Banking Team, you are part of an agile team that works to enhance, design, and deliver the data collection, storage, access, and analytics solutions in a secure, stable, and scalable way. As an emerging member of a data engineering team, you execute data solutions through the design, development, and technical troubleshooting of multiple components within a technical product, application, or system, while gaining the skills and experience needed to grow within your role.\nJob responsibilities\nExecutes software solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problems\nSupports review of controls to ensure sufficient protection of enterprise data\nAdvises and makes custom configuration changes in one to two tools to generate a product at the business or customer request\nUpdates logical or physical data models based on new use cases\nFrequently uses SQL and understands NoSQL databases and their niche in the marketplace\nAdds to team culture of diversity, equity, inclusion, and respect\nContributes to software and data engineering communities of practice and events that explore new and emerging technologies\nGathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of software applications and systems\nRequired qualifications, capabilities, and skills\nFormal training or certification on Data Engineering concepts and 3+ years applied experience in AWS and Kubernetes\nProficiency in one or more large-scale data processing distributions such as JavaSpark/PySpark along with knowledge on Data Pipeline (DPL), Data Modeling, Data warehouse, Data Migration and so-on.\nExperience across the data lifecycle along with expertise with consuming data in any of batch (file), near real-time (IBM MQ, Apache Kafka), streaming (AWS kinesis, MSK)\nGood at SQL (e. g. , joins and aggregations)\nWorking understanding of NoSQL databases\nExperience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages.\nSolid understanding of agile methodologies such as CI/CD, Application Resiliency, and Security\nSignificant experience with statistical data analysis and ability to determine appropriate tools and data patterns to perform analysis\nExperience customizing changes in a tool to generate product\nPreferred qualifications, capabilities, and skills\nFamiliarity with modern front-end technologies\nExperience designing and building REST API services using Java\nExposure to cloud technologies - knowledge on Hybrid cloud architectures is highly desirable.\nAWS Developer/Solutions Architect Certification is highly desired",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'Data migration', 'Front end', 'Data modeling', 'Debugging', 'Agile', 'Apache', 'Troubleshooting', 'Analytics', 'SQL']",2025-06-11 05:24:12
Cloud Data Engineer,PwC India,3 - 8 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Job Description:\n\nWe are seeking skilled and dynamic Cloud Data Engineers specializing in AWS, Azure, Databricks. The ideal candidate will have a strong background in data engineering, with a focus on data ingestion, transformation, and warehousing. They should also possess excellent knowledge of PySpark or Spark, and a proven ability to optimize performance in Spark job executions.\n\nKey Responsibilities:",,,,"['AWS OR Azure', 'Azure Data Engineer OR AWS Data Engineer', 'Azure', 'AWS']",2025-06-11 05:24:13
Data Engineer III - PostgreSQL & MongoDB Admin,JPMorgan Chase Bank,0 - 6 years,Not Disclosed,['Hyderabad'],"Be part of a dynamic team where your distinctive skills will contribute to a winning culture and team.\nAs a Data Engineer III at JPMorgan Chase within the Corporate Technology , you serve as a seasoned member of an agile team to design and deliver trusted data collection, storage, access, and analytics solutions in a secure, stable, and scalable way. You are responsible for developing, testing, and maintaining critical data pipelines and architectures across multiple technical areas within various business functions in support of the firm s business objectives.\nJob responsibilities\nSupport the review of controls to ensure sufficient protection of enterprise data.\nAdvise and make custom configuration changes in one to two tools to generate a product at the business or customer request.\nUpdate logical or physical data models based on new use cases.\nFrequently use SQL and understand NoSQL databases and their niche in the marketplace.\nImplement backup, recovery, and disaster recovery (DR) plans.\nMonitor database capacity, space, logs, and performance metrics.\nManage database security, access control, and user privilege management.\nConduct database health checks, troubleshoot, and resolve issues in real-time.\nScript using Shell and Python, and utilize database tools.\nConfigure and maintain MongoDB replica sets, sharding, and failover mechanisms.\nRequired qualifications, capabilities, and skills\nFormal training or certification in software engineering concepts and 3+ years of applied experience.\nExperience across the data lifecycle\nAdvanced at SQL (e. g. , joins and aggregations)\nWorking understanding of NoSQL databases\nExpertise in PostgreSQL MongoDB DBA.\nStrong expertise in SQL, PL/pgSQL, and NoSQL (MongoDB queries, aggregation, and indexing).\nHands-on experience with PostgreSQL replication, partitioning, and tuning.\nExperience managing MongoDB Atlas, Sharded Clusters, and Performance tuning.\nFamiliarity with database monitoring tools such as Prometheus, Grafana, or CloudWatch.\nStrong knowledge of database security best practices and encryption techniques.\nExperience in automating DB tasks using Bash, Python, or Ansible.\nPreferred qualifications, capabilities, and skills\nMongoDB Certified DBA\nPostgreSQL Professional Certification\nAWS Certified Database - Specialty / Data Engineer (preferred but not mandatory)\nExperience working with cloud-based databases (AWS RDS, Azure Cosmos DB, GCP Cloud SQL) is a plus.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'NoSQL', 'Postgresql', 'Disaster recovery', 'Database administration', 'Agile', 'MongoDB', 'Analytics', 'SQL', 'Python']",2025-06-11 05:24:15
AWS Data Engineer (Associate),Infogain,2 - 3 years,Not Disclosed,['Bengaluru'],"C#, AWS, SQL skill set required with 2-3 years experience and immediate joiner\nC#, AWS, SQL skill set required with 2-3 years experience and immediate joiner\nEXPERIENCE\n2-3 Years\nSKILLS\nPrimary Skill: Data Engineering\nSub Skill(s): Data Engineering\nAdditional Skill(s): C#, Python, AWS - CloudFormation, Apache Hive, SQL",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['hive', 'SUB', 'C', 'Apache', 'AWS', 'SQL', 'Python']",2025-06-11 05:24:17
Lead Data Engineer - VP,JPMorgan Chase Bank,11 - 15 years,Not Disclosed,['Hyderabad'],"Join us as we embark on a journey of collaboration and innovation, where your unique skills and talents will be valued and celebrated. Together we will create a brighter future and make a meaningful difference.\nAs a Lead Data Engineer at JPMorgan Chase within the CCB (Connected Commerce), you are an integral part of an agile team that works to enhance, build, and deliver data collection, storage, access, and analytics solutions in a secure, stable, and scalable way. As a core technical contributor, you are responsible for maintaining critical data pipelines and architectures across multiple technical areas within various business functions in support of the firm s business objectives.\n  Job responsibilities\nArchitect and oversee the design of complex data solutions that meet diverse business needs and customer requirements.\nGuide the evolution of logical and physical data models to support emerging business use cases and technological advancements.\nBuild and manage end-to-end cloud-native data pipelines in AWS, leveraging your hands-on expertise with AWS components.\nBuild analytical systems from the ground up, providing architectural direction, translating business issues into specific requirements, and identifying appropriate data to support solutions.\nWork across the Service Delivery Lifecycle on engineering major/minor enhancements and ongoing maintenance of existing applications.\nConduct feasibility studies, capacity planning, and process redesign/re-engineering of complex integration solutions.\nHelp others build code to extract raw data, coach the team on techniques to validate its quality, and apply your deep data knowledge to ensure the correct data is ingested across the pipeline.\nGuide the development of data tools used to transform, manage, and access data, and advise the team on writing and validating code to test the storage and availability of data platforms for resilience.\nOversee the implementation of performance monitoring protocols across data pipelines, coaching the team on building visualizations and aggregations to monitor pipeline health.\nCoach others on implementing solutions and self-healing processes that minimize points of failure across multiple product features.\nRequired qualifications, capabilities, and skills\nFormal training or certification on software engineering concepts and 5+ years applied experience\nExtensive experience in managing the full lifecycle of data, from collection and storage to analysis and reporting.\nProficiency in one or more large-scale data processing distributions such as JavaSpark along with knowledge on Data Pipeline (DPL), Data Modeling, Data warehouse, Data Migration and so-on.\nHands-on practical experience in system design, application development, testing, and operational stability\nProficient in coding in one or more modern programming languages\nShould have good hands-on experience on AWS services and its components along with good understanding on Kubernetes.\nExperience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages.\nStrong understanding of domain driven design, micro-services patterns, and architecture\nOverall knowledge of the Software Development Life Cycle along with experience with IBM MQ, Apache Kafka\nSolid understanding of agile methodologies such as CI/CD, Application Resiliency, and Security\nDemonstrated knowledge of software applications and technical processes within a technical discipline (eg, cloud, LLMs etc)\nPreferred qualifications, capabilities, and skills\nFamiliarity with modern front-end technologies\nExperience designing and building REST API services using Java\nExposure to cloud technologies - knowledge on Hybrid cloud architectures is highly desirable.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data migration', 'Front end', 'Data modeling', 'Coding', 'Debugging', 'Agile', 'System design', 'Application development', 'Apache', 'Analytics']",2025-06-11 05:24:18
Data Engineer (AI Infrastructure),FortitudeCareer,4 - 8 years,14.4-24 Lacs P.A.,['Pune'],Remote | 6+ Years Experience\nCC: recruitment@fortitudecareer.com\nWere seeking a Data Engineer with deep expertise in building scalable AI/ML data infrastructure.\nOrchestrate data flows using Apache Airflow\nDeep understanding of (GCP/AWS)\n\n\nFlexi working\nWork from home,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Airflow', 'Snowflake', 'GCP/AWS', 'Apache Spark', 'Python', 'feature stores', 'Bigquery', 'AI/ML applications', 'cloud data architectures', 'ETL pipelines', 'SQL']",2025-06-11 05:24:20
Data Ingest Engineer,"NTT DATA, Inc.",6 - 10 years,Not Disclosed,['Pune'],"Req ID: 323909\n\nWe are currently seeking a Data Ingest Engineer to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nJob DutiesThe Applications Development Technology Lead Analyst is a senior level position responsible for establishing and implementing new or revised application systems and programs in coordination with the Technology team.\nThis is a position within the Ingestion team of the DRIFT data ecosystem. The focus is on ingesting data in a timely , complete, and comprehensive fashion while using the latest technology available to Citi. The ability to leverage new and creative methods for repeatable data ingestion from a variety of data sources while always questioning ""is this the best way to solve this problem"" and ""am I providing the highest quality data to my downstream partners"" are the questions we are trying to solve.\n\nResponsibilities:\n""¢ Partner with multiple management teams to ensure appropriate integration of functions to meet goals as well as identify and define necessary system enhancements to deploy new products and process improvements\n""¢ Resolve variety of high impact problems/projects through in-depth evaluation of complex business processes, system processes, and industry standards\n""¢ Provide expertise in area and advanced knowledge of applications programming and ensure application design adheres to the overall architecture blueprint\n""¢ Utilize advanced knowledge of system flow and develop standards for coding, testing, debugging, and implementation\n""¢ Develop comprehensive knowledge of how areas of business, such as architecture and infrastructure, integrate to accomplish business goals\n""¢ Provide in-depth analysis with interpretive thinking to define issues and develop innovative solutions\n""¢ Serve as advisor or coach to mid-level developers and analysts, allocating work as necessary\n""¢ Appropriately assess risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding Citigroup, its clients and assets, by driving compliance with applicable laws, rules and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct and business practices, and escalating, managing and reporting control issues with transparency.\n\nMinimum Skills Required""¢ 6-10 years of relevant experience in Apps Development or systems analysis role\n""¢ Extensive experience system analysis and in programming of software applications\n""¢ Application Development using JAVA, Scala, Spark\n""¢ Familiarity with event driven applications and streaming data\n""¢ Experience with Confluent Kafka, HDFS, HIVE, structured and unstructured database systems (SQL and NoSQL)\n""¢ Experience with various schema and data types -> JSON, AVRO, Parquet, etc.\n""¢ Experience with various ELT methodologies and formats -> JDBC, ODBC, API, Web hook, SFTP, etc.\n""¢ Experience working within the Agile and version control tool sets (JIRA, Bitbucket, Git, etc.)\n""¢ Ability to adjust priorities quickly as circumstances dictate\n""¢ Demonstrated leadership and project management skills\n""¢ Consistently demonstrates clear and concise written and verbal communication",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['application software', 'system analysis', 'java', 'project management', 'applications programming', 'hive', 'scala', 'jdbc', 'bitbucket', 'sql', 'parquet', 'git', 'spark', 'data ingestion', 'json', 'debugging', 'api', 'jira', 'avro', 'odbc', 'elt', 'data engineering', 'nosql', 'app development', 'kafka', 'sftp', 'agile']",2025-06-11 05:24:22
"Senior Staff Engineer, Big Data AWS Engineer",Nagarro,10 - 13 years,Not Disclosed,['Gurugram'],"We're Nagarro.\n\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal experience 9+ years.\nHands-on experience in Big Data Engineering.\nStrong expertise in Apache Spark and PySpark/Python.\nDeep technical knowledge of AWS Glue (Crawler, Data Catalog).\nHands on working experience in Python.\nStrong working experience with AWS services, including S3, Lambda, SNS, Secret Manager, and Athena.\nProven experience with Infrastructure as Code using CloudFormation and Terraform.\nSolid experience in Snowflake.\nProficiency in setting up and maintaining CI/CD pipelines with GitHub Actions.\nFamiliarity with tools like Jira and GitHub.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\n\nRESPONSIBILITIES:\nUnderstanding the clients business use cases and technical requirements and be able to convert them into technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design documents explaining overall architecture, framework, and high-level design of the application for the developers\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken\nCarrying out POCs to make sure that suggested design/technologies meet the requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Spark', 'Aws Glue', 'Python', 'Pyspark', 'Aws Lambda', 'Snowflake']",2025-06-11 05:24:24
Senior Data Engineer,Impetus Technologies,5 - 10 years,Not Disclosed,['United Arab Emirates'],"The Opportunity:We are seeking a highly motivated and technically strong Module Lead Software Engineer with significant expertise in Python, PySpark, and Palantir Foundry. In this role, you will be responsible for the end-to-end technical ownership, design, and delivery of a specific module or component within our enterprise data platform. You will combine hands-on development with technical leadership, ensuring the highest standards of code quality, performance, and reliability.\n\nKey Responsibilities:\n\nModule Technical Leadership & Ownership: Take full technical ownership of a specific module or component within the data platform on Palantir Foundry. This includes defining its technical roadmap, architecture, design patterns, and ensuring its integration into the broader data ecosystem.\nHands-on Development and Complex Problem Solving: Act as a lead individual contributor, developing sophisticated data pipelines, transformations, and applications using Python and PySpark within Palantir Foundry's various tools (e.g., Code Workbook, Pipeline Builder). Tackle the most challenging technical problems and implement core functionalities for the module.\nQuality Assurance and Best Practices Advocacy: Drive and enforce high standards for code quality, test coverage, documentation, and operational excellence within your module. Conduct rigorous code reviews, provide constructive feedback, and mentor engineers within your immediate scope to elevate their technical skills.\nCross-Functional Collaboration and Module Integration: Collaborate extensively with other module leads, architects, data scientists, and business stakeholders to ensure seamless integration of your module's deliverables. Proactively identify and manage technical dependencies and ensure the module aligns with overall project goals and architectural vision.\n• Performance Optimization and Troubleshooting: Continuously monitor and optimize the performance of your module's data pipelines and applications. Efficiently troubleshoot and resolve complex technical issues, data quality concerns, and system failures specific to your module.\n\nRequired Qualifications:\n\nExperience: 6-8 years of progressive experience in software development with a strong focus on data engineering.\nPython Proficiency: Expert-level proficiency in Python, including advanced programming concepts, data structures, and performance optimization techniques.\nPySpark Expertise: Strong experience with PySpark for large-scale distributed data processing, transformations, and analytics.\nPalantir Foundry: Proven, hands-on experience designing, developing, and deploying solutions within Palantir Foundry is essential.\nDeep familiarity with Foundry's data integration capabilities, Code Workbook, Pipeline Builder, Data Health checks, and Ontology modeling.\nExperience with Foundry's approach to data governance and versioning.\nSQL Skills: Excellent SQL skills for complex data querying, manipulation, and optimization.\nData Warehousing/Lakes: Solid understanding of data warehousing concepts, data lake architectures, and ETL/ELT principles.\nCloud Platforms: Experience with at least one major cloud platform (AWS, Azure, GCP), particularly with data-related services.\nVersion Control: Strong experience with Git and collaborative development workflows.\n\nPreferred Qualifications (Nice-to-Have):\n\nExperience mentoring or leading small technical teams/pods.\nFamiliarity with containerization technologies (Docker, Kubernetes).\nExperience with streaming data technologies (e.g., Kafka, Kinesis).\nUnderstanding of CI/CD pipelines for data solutions.\nKnowledge of data governance, data quality, and metadata management best practices.\nExperience in [specific industry, e.g., Financial Services, Manufacturing, Healthcare].",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Palantir Foundry', 'Python', 'Spark', 'Data Warehousing', 'SQL']",2025-06-11 05:24:26
Azure Data Engineer (Standard),Infogain,5 - 6 years,Not Disclosed,['Bengaluru'],"Primary Skills: ADF, Databricks, Log Analytics\nSecondary Skills: Data Warehouse, Logic Apps, Log Analytics, Datadog, Atlan, Attacama\nEXPERIENCE\n4.5-6 Years\nSKILLS\nPrimary Skill: Data Engineering\nSub Skill(s): Data Engineering\nAdditional Skill(s): synapse, databricks, Azure Datalake, Azure Data Factory",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SUB', 'Data warehousing', 'Analytics']",2025-06-11 05:24:27
Senior Data Engineer,S&P Global Market Intelligence,6 - 11 years,Not Disclosed,['Gurugram'],"\n\nAbout the Role: \n\nGrade Level (for internal use):\n10\n \n\nPosition summary \n\n Our proprietary software-as-a-service helps automotive dealerships and sales teams better understand and predict exactly which customers are ready to buy, the reasons why, and the key offers and incentives most likely to close the sale. Its micro-marketing engine then delivers the right message at the right time to those customers, ensuring higher conversion rates and a stronger ROI. \n\n \n\nWhat You'll Do \n\n You will be part of our Data Platform & Product Insights data engineering team. As part of this agile team, you will work in our cloud native environment to \n\n Build & support data ingestion and processing pipelines in cloud. This will entail extraction, load and transformation of big data from a wide variety of sources, both batch & streaming, using latest data frameworks and technologies \n\n Partner with product team to assemble large, complex data sets that meet functional and non-functional business requirements, ensure build out of Data Dictionaries/Data Catalogue and detailed documentation and knowledge around these data assets, metrics and KPIs. \n\n Warehouse this data, build data marts, data aggregations, metrics, KPIs, business logic that leads to actionable insights into our product efficacy, marketing platform, customer behaviour, retention etc. \n\n Build real-time monitoring dashboards and alerting systems. \n\n Coach and mentor other team members. \n\n\n \n\nWho you are \n\n 6+ years of experience in Big Data and Data Engineering. \n\n Strong knowledge of advanced SQL, data warehousing concepts and DataMart designing. \n\n Have strong programming skills in SQL, Python/ PySpark etc. \n\n Experience in design and development of data pipeline, ETL/ELT process on-premises/cloud. \n\n Experience in one of the Cloud providers GCP, Azure, AWS. \n\n Experience with relational SQL and NoSQL databases, including Postgres and MongoDB. \n\n Experience workflow management toolsAirflow, AWS data pipeline, Google Cloud Composer etc. \n\n Experience with Distributed Versioning Control environments such as GIT, Azure DevOps \n\n Building Docker images and fetch/promote and deploy to Production. Integrate Docker container orchestration framework using Kubernetes by creating pods, config Maps, deployments using terraform. \n\n Should be able to convert business queries into technical documentation. \n\n Strong problem solving and communication skills. \n\n Bachelors or an advanced degree in Computer Science or related engineering discipline. \n\n\n \n\nGood to have some exposure to \n\n Exposure to any Business Intelligence (BI) tools like Tableau, Dundas, Power BI etc. \n\n Agile software development methodologies. \n\n Working in multi-functional, multi-location teams \n\n  \n \n\nGrade10  \n \n\nLocationGurugram \n \n\nHybrid Modeltwice a week work from office  \n \n\nShift Time12 pm to 9 pm IST  \n What You'll Love About Us Do ask us about these! \n\n \n\nTotal Rewards. Monetary, beneficial and developmental rewards! \n\n \n\nWork Life Balance. You can't do a good job if your job is all you do! \n\n \n\nPrepare for the Future.Academy we are all learners; we are all teachers! \n\n \n\nEmployee Assistance Program. Confidential and Professional Counselling and Consulting. \n\n \n\nDiversity & Inclusion. HeForShe! \n\n \n\nInternal Mobility.\n\nGrow with us! \n\n  \n  \n  \n  \n\nAbout automotiveMastermind\n\nWho we are:\n\nFounded in 2012, automotiveMastermind is a leading provider of predictive analytics and marketing automation solutions for the automotive industry and believes that technology can transform data, revealing key customer insights to accurately predict automotive sales. Through its proprietary automated sales and marketing platform, Mastermind, the company empowers dealers to close more deals by predicting future buyers and consistently marketing to them. automotiveMastermind is headquartered in New York City. For more information, visit automotivemastermind.com.\n\nAt automotiveMastermind, we thrive on high energy at high speed. Were an organization in hyper-growth mode and have a fast-paced culture to match. Our highly engaged teams feel passionately about both our product and our people. This passion is what continues to motivate and challenge our teams to be best-in-class. Our cultural values of Drive and Help have been at the core of what we do, and how we have built our culture through the years. This cultural framework inspires a passion for success while collaborating to win.\n\nWhat we do:\n\nThrough our proprietary automated sales and marketing platform, Mastermind, we empower dealers to close more deals by predicting future buyers and consistently marketing to them. In short, we help automotive dealerships generate success in their loyalty, service, and conquest portfolios through a combination of turnkey predictive analytics, proactive marketing, and dedicated consultative services.\n\nWhats In It For\n\nYou\n\nOur Purpose:\n\nProgress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technologythe right combination can unlock possibility and change the world.Our world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence, pinpointing risks and opening possibilities. We Accelerate Progress.\n\nOur People:\n\nOur Values:\n\nIntegrity, Discovery, Partnership\n\nAt S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of\n\nintegrity in all we do, bring a spirit of\n\ndiscovery to our work, and collaborate in close\n\npartnership with each other and our customers to achieve shared goals.\n\nBenefits:\n\nWe take care of you, so you cantake care of business. We care about our people. Thats why we provide everything youand your careerneed to thrive at S&P Global.\n\nHealth & WellnessHealth care coverage designed for the mind and body.\n\n\n\nContinuous LearningAccess a wealth of resources to grow your career and learn valuable new skills.\n\nInvest in Your FutureSecure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\n\nFamily Friendly PerksIts not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\n\nBeyond the BasicsFrom retail discounts to referral incentive awardssmall perks can make a big difference.\n\nFor more information on benefits by country visithttps://spgbenefits.com/benefit-summaries",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['advance sql', 'python', 'pyspark', 'sql', 'data warehousing concepts', 'kubernetes', 'microsoft azure', 'data warehousing', 'power bi', 'relational sql', 'elt', 'data engineering', 'business intelligence', 'azure devops', 'docker', 'nosql', 'tableau', 'git', 'postgresql', 'gcp', 'agile', 'big data', 'aws', 'mongodb']",2025-06-11 05:24:29
Senior Data Engineer,R Systems International,10 - 14 years,20-30 Lacs P.A.,"['Noida', 'Delhi / NCR']","Solid understanding of data pipeline architecture, cloud infrastructure, and best practices in data engineering.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and collaborate effectively in a team environment.\nSkilled in independently analyzing large datasets, identifying discrepancies and inconsistencies, and recommending corrective actions.\nDemonstrated expertise in working with SQL Server, Oracle, Azure SQL Databases, and APIs.\nExperience with at least one programming language (Python, Java, C#, etc.).",,,,"['Azure Data Factory', 'Data Engineering', 'Azure Databricks', 'SQL', 'Python', 'Pyspark', 'Java', 'Azure Cloud', 'Runbooks', 'Azure Logic Apps', 'ETL', 'Powershell Scripting']",2025-06-11 05:24:31
Lead AWS Glue Data Engineer,DXC Technology,4 - 9 years,Not Disclosed,['Chennai'],"We are seeking a skilled Lead AWS Data Engineer with strong programming and SQL skills to join our team. The ideal candidate will have hands-on experience with AWS Data Analytics services and a basic understanding of general AWS services. Additionally, prior experience with Oracle and Postgres databases and secondary skills in Python and Azure DevOps will be an advantage.\n  Key Responsibilities:\nDesign, develop, and optimize data pipelines using AWS Data Analytics services such as RDS, DMS, Glue, Lambda, Redshift, and Athena .\nImplement data migration and transformation processes using AWS DMS and Glue .\nWork with SQL (Oracle & Postgres) to query, manipulate, and analyse large datasets.\nDevelop and maintain ETL/ELT workflows for data ingestion and transformation.\nUtilize AWS services like S3, IAM, CloudWatch, and VPC to ensure secure and efficient data operations.\nWrite clean and efficient Python scripts for automation and data processing.\nCollaborate with DevOps teams using Azure DevOps for CI/CD pipelines and infrastructure management.\nMonitor and troubleshoot data workflows to ensure high availability and performance.\nPreferred Qualifications:\nAWS certifications in Data Analytics, Solutions Architect, or DevOps.\nExperience with data warehousing concepts and data lake implementations.\nHands-on experience with Infrastructure as Code (IaC) tools like Terraform or CloudFormation.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'Data migration', 'Infrastructure management', 'Social media', 'devops', 'Data processing', 'Data analytics', 'Dms', 'AWS', 'Python']",2025-06-11 05:24:32
Senior Big Data Engineer,Meritus Management Service,6 - 8 years,20-30 Lacs P.A.,"['Nagpur', 'Pune']","Build and maintain scalable Big Data pipelines using Hadoop, PySpark, and SQL for batch and real-time processing.\nCollaborate with cross-functional teams to transform, optimize, and secure large datasets while ensuring data quality and performance.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hadoop', 'Python', 'Big Data']",2025-06-11 05:24:33
"Big Data Engineer (Spark, Scala) , SQL",Black and white Business Solution,7 - 10 years,Not Disclosed,"['Bhubaneswar', 'Pune', 'Bengaluru']","About Client\n\nHiring for One of the Most Prestigious Multinational Corporations\n\nJob Title: Big Data Engineer (Spark, Scala) , SQL\n\nExperience: 6 to 10 years\n\nKey Responsibilities:\nDesign, develop, and optimize scalable big data pipelines using Apache Spark and Scala.\nBuild batch and real-time data processing workflows to ingest, transform, and aggregate large datasets.\nWrite high-performance SQL queries to support data analysis and reporting.\nCollaborate with data architects, data scientists, and business stakeholders to understand requirements and deliver high-quality data solutions.\nEnsure data quality, integrity, and governance across systems.\nParticipate in code reviews and maintain best practices in data engineering.\nTroubleshoot and optimize performance of Spark jobs and SQL queries.\nMonitor and maintain production data pipelines and perform root cause analysis of data issues.\n\nTechnical Skills:\n\n6 to10 years of overall experience in software/data engineering.\n4+ years of hands-on experience with Apache Spark using Scala.\nStrong proficiency in Scala and functional programming concepts.\nExtensive experience with SQL (preferably in distributed databases like Hive, Presto, Snowflake, or BigQuery).\nExperience working in Hadoop ecosystem (HDFS, Hive, HBase, Oozie, etc.).\nKnowledge of data modeling, data architecture, and ETL frameworks.\nFamiliarity with version control (Git), CI/CD pipelines, and DevOps practices.\nExperience with cloud platforms (AWS, Azure, or GCP) is a plus.\nStrong analytical and problem-solving skills.\nExcellent communication and collaboration abilities.\n\nNotice period : Till 60 days\n\nLocation: BLR//BBSR/PUNE\n\nMode of Work :WFO(Work From Office)\n\n\nThanks & Regards,\nSWETHA\nBlack and White Business Solutions Pvt.Ltd.\nBangalore,Karnataka,INDIA.\nContact Number:8067432433\nrathy@blackwhite.in |www.blackwhite.in",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['spark', 'scala', 'Big data', 'SQL']",2025-06-11 05:24:35
Senior Data Engineer,Dun & Bradstreet,5 - 9 years,Not Disclosed,['Hyderabad'],"About Us:\nOur global community of colleagues bring a diverse range of experiences and perspectives to our work. You'll find us working from a corporate office or plugging in from a home desk, listening to our customers and collaborating on solutions. Our products and solutions are vital to businesses of every size, scope and industry. And at the heart of our work youll find our core values: to be data inspired, relentlessly curious and inherently generous. Our values are the constant touchstone of our community; they guide our behavior and anchor our decisions.\n\nDesignation: Software Engineer II\nLocation: Hyderabad\n\nKEY RESPONSIBILITIES\n\nDesign, build, and deploy new data pipelines within our Big Data Eco-Systems using Streamsets/Talend/Informatica BDM etc. Document new/existing pipelines, Datasets.\nDesign ETL/ELT data pipelines using StreamSets, Informatica or any other ETL processing engine. Familiarity with Data Pipelines, Data Lakes and modern Data Warehousing practices (virtual data warehouse, push down analytics etc.)\nExpert level programming skills on Python\nExpert level programming skills on Spark\nCloud Based Infrastructure: GCP\nExperience with one of the ETL Informatica, StreamSets in creation of complex parallel loads, Cluster Batch Execution and dependency creation using Jobs/Topologies/Workflows etc.,\nExperience in SQL and conversion of SQL stored procedures into Informatica/StreamSets, Strong exposure working with web service origins/targets/processors/executors, XML/JSON Sources and Restful APIs.\nStrong exposure working with relation databases DB2, Oracle & SQL Server including complex SQL constructs and DDL generation.\nExposure to Apache Airflow for scheduling jobs\nStrong knowledge of Big data Architecture (HDFS), Cluster installation, configuration, monitoring, cluster security, cluster resources management, maintenance, and performance tuning\nCreate POCs to enable new workloads and technical capabilities on the Platform.\nWork with the platform and infrastructure engineers to implement these capabilities in production.\nManage workloads and enable workload optimization including managing resource allocation and scheduling across multiple tenants to fulfill SLAs.\nParticipate in planning activities, Data Science and perform activities to increase platform skills\n\nKEY Requirements\n\nMinimum 6 years of experience in ETL/ELT Technologies, preferably StreamSets/Informatica/Talend etc.,\nMinimum of 6 years hands-on experience with Big Data technologies e.g. Hadoop, Spark, Hive.\nMinimum 3+ years of experience on Spark\nMinimum 3 years of experience in Cloud environments, preferably GCP\nMinimum of 2 years working in a Big Data service delivery (or equivalent) roles focusing on the following disciplines:\nAny experience with NoSQL and Graph databases\nInformatica or StreamSets Data integration (ETL/ELT)\nExposure to role and attribute based access controls\nHands on experience with managing solutions deployed in the Cloud, preferably on GCP\nExperience working in a Global company, working in a DevOps model is a plus\n\nDun & Bradstreet is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, creed, sex, age, national origin, citizenship status, disability status, sexual orientation, gender identity or expression, pregnancy, genetic information, protected military and veteran status, ancestry, marital status, medical condition (cancer and genetic characteristics) or any other characteristic protected by law.\n\nWe are committed to Equal Employment Opportunity and providing reasonable accommodations to qualified candidates and employees. If you are interested in applying for employment with Dun & Bradstreet and need special assistance or an accommodation to use our website or to apply for a position, please send an e-mail with your requesttoacquisitiont@dnb.com Determinationon requests for reasonable accommodation are made on a case-by-case basis.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Bigquery', 'Spark', 'Google Cloud Platforms', 'Python', 'sql', 'Airflow', 'Docker', 'Streamsets', 'Informatica', 'ETL', 'Talend', 'AWS']",2025-06-11 05:24:36
"Senior Data Engineer (Snowflake, DBT)",Allegis Global Solutions (AGS),5 - 10 years,Not Disclosed,[],"Senior Data Engineer (Snowflake, DBT, Azure)\nJob Location: Hyderabad / Bangalore / Chennai / Kolkata / Noida/ Gurgaon / Pune / Indore / Mumbai\n\nJob Details\nTechnical Expertise:\nStrong proficiency in Snowflake architecture, including data sharing, partitioning, clustering, and materialized views.\nAdvanced experience with DBT for data transformations and workflow management.\nExpertise in Azure services, including Azure Data Factory, Azure Data Lake, Azure Synapse, and Azure Functions.\nData Engineering:\nProficiency in SQL, Python, or other relevant programming languages.\nStrong understanding of data modeling concepts, including star schema and normalization.\nHands-on experience with ETL/ELT pipelines and data integration tools.\n\nPreferred Qualifications:\nCertifications in Snowflake, DBT, or Azure Data Engineering.\nFamiliar with data visualization tools like Power BI or Tableau.\nKnowledge of CI/CD pipelines and DevOps practices for data workflows.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Data Build Tool', 'Azure']",2025-06-11 05:24:38
Sr. Data Engineer,Leading Client,6 - 11 years,Not Disclosed,['Bengaluru'],"Were looking for an experienced Senior Data Engineer to lead the design and development\n\nof scalable data solutions at our company. The ideal candidate will have extensive hands-on\n\nexperience in data warehousing, ETL/ELT architecture, and cloud platforms like AWS,\n\nAzure, or GCP. You will work closely with both technical and business teams, mentoring\n\nengineers while driving data quality, security, and performance optimization.\n\nResponsibilities:\n\nLead the design of data warehouses, lakes, and ETL workflows.\nCollaborate with teams to gather requirements and build scalable solutions.\nEnsure data governance, security, and optimal performance of systems.\nMentor junior engineers and drive end-to-end project delivery.:\n6+ years of experience in data engineering, including at least 2 full-cycle datawarehouse projects.\nStrong skills in SQL, ETL tools (e.g., Pentaho, dbt), and cloud platforms.\nExpertise in big data tools (e.g., Apache Spark, Kafka).\nExcellent communication skills and leadership abilities.PreferredExperience with workflow orchestration tools (e.g., Airflow), real-time data,and DataOps practices.",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'hive', 'scala', 'data warehousing', 'sql', 'plsql', 'apache', 'unix shell scripting', 'etl tool', 'spark', 'gcp', 'devops', 'linux', 'pentaho', 'hadoop', 'big data', 'etl', 'python', 'airflow', 'microsoft azure', 'cloud platforms', 'elt', 'sql server', 'data quality', 'kafka', 'data governance', 'aws', 'unix']",2025-06-11 05:24:39
Sr Data Engineer,Leading Client,3 - 6 years,Not Disclosed,['Bengaluru'],"Skills:\nMicrosoft Azure, Hadoop, Spark, Databricks, Airflow, Kafka, Py spark RequirmentsExperience working with distributed technology tools for developing Batch and Streaming pipelines using. SQL, Spark, Python Airflow Scala Kafka Experience in Cloud Computing, e.g., AWS, GCP, Azure, etc. Able to quickly pick up new programming languages, technologies, and frameworks. Strong skills building positive relationships across Product and Engineering. Able to influence and communicate effectively, both verbally and written, with team members and business stakeholders Experience with creating/ configuring Jenkins pipeline for smooth CI/CD process for Managed Spark jobs, build Docker images, etc. Working knowledge of Data warehousing, Data modelling, Governance and Data Architecture Experience working with Data platforms, including EMR, Airflow, Data bricks (Data Engineering & Delta Lake components) Experience working in Agile and Scrum development process. Experience in EMR/ EC2, Data bricks etc. Experience working with Data warehousing tools, including SQL database, Presto, and Snowflake Experience architecting data product in Streaming, Server less and Microservices Architecture and platform.",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL', 'scala', 'pyspark', 'data warehousing', 'emr', 'data architecture', 'docker', 'data modeling', 'spark', 'gcp', 'jenkins', 'hadoop', 'cloud computing', 'snowflake', 'python', 'airflow', 'microsoft azure', 'data engineering', 'data bricks', 'amazon ec2', 'kafka', 'scrum', 'agile', 'aws', 'presto', 'sql database']",2025-06-11 05:24:41
Data Engineer-Data Platforms-Google,IBM,2 - 5 years,Not Disclosed,['Hyderabad'],"As an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\n\n\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['elastic search', 'gcp', 'splunk', 'hadoop', 'big data', 'hive', 'python', 'data management', 'presentation skills', 'microsoft azure', 'machine learning', 'javascript', 'sql', 'docker', 'java', 'git', 'spark', 'linux', 'jenkins', 'html', 'mysql', 'aws']",2025-06-11 05:24:42
Data Engineer-Data Platforms-Google,IBM,2 - 5 years,Not Disclosed,['Hyderabad'],"As an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\n\n\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['elastic search', 'gcp', 'splunk', 'hadoop', 'big data', 'hive', 'python', 'data management', 'presentation skills', 'microsoft azure', 'machine learning', 'javascript', 'sql', 'docker', 'java', 'git', 'spark', 'linux', 'jenkins', 'html', 'mysql', 'aws']",2025-06-11 05:24:44
Data Engineer-Data Platforms-Google,IBM,2 - 5 years,Not Disclosed,['Gurugram'],"As an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\n\n\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['elastic search', 'gcp', 'splunk', 'hadoop', 'big data', 'hive', 'python', 'data management', 'presentation skills', 'microsoft azure', 'machine learning', 'javascript', 'sql', 'docker', 'java', 'git', 'spark', 'linux', 'jenkins', 'html', 'mysql', 'aws']",2025-06-11 05:24:46
Data Engineer-Data Warehouse,IBM,2 - 5 years,Not Disclosed,['Pune'],"As an Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nDesign and implement efficient database schemas and data models using Teradata.\nOptimize SQL queries and stored procedures for performance.\nPerform database administration tasks including installation, configuration, and maintenance of Teradata systems\n\n\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'elastic search', 'data modeling', 'sql queries', 'splunk', 'data management', 'manual testing', 'data warehousing', 'plsql', 'java', 'ssrs', 'mysql', 'hadoop', 'big data', 'etl', 'jira', 'python', 'sql development', 'oracle', 'data analysis', 'warehouse', 'sql server', 'tableau', 'ssis', 'aws', 'informatica', 'unix']",2025-06-11 05:24:48
Data Engineer-Data Platforms-Google,IBM,2 - 5 years,Not Disclosed,['Gurugram'],"As an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets\n\nIn this role, your responsibilities may include\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\n\n\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['elastic search', 'gcp', 'splunk', 'hadoop', 'big data', 'hive', 'python', 'data management', 'presentation skills', 'microsoft azure', 'machine learning', 'javascript', 'sql', 'docker', 'java', 'git', 'spark', 'linux', 'jenkins', 'html', 'mysql', 'aws']",2025-06-11 05:24:49
Data Engineer-Enterprise Content Management,IBM,2 - 5 years,Not Disclosed,['Hyderabad'],"Experience is creating templates in Open Text Extreme CE 23.4 Version\nResponsible to design and develop different documents and business forms using OpenText Exstream.\nUnderstanding of different input, output file formats, and Print file formats(PDF etc). Perform unit testing of templates/documents.\nApply styles and images to document design.\nUse output comparison tools to compare different outputs\nShould have experience working with Exstream Design Manager & Exstream Designer Tool.\nShould have prior knowledge on working with Exstream Web Service.\nDesigning Templates, Objects, Rules, Variables and creation of Documents based on Templates.\nUnderstand current SmartCOMM Templates and create templates based on that\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nIntuitive individual with an ability to manage change and proven time management\nProven interpersonal skills while contributing to team effort by accomplishing related results as needed\nUp-to-date technical knowledge by attending educational workshops, reviewing publications\n\n\nPreferred technical and professional experience\nExperience is creating templates in Open Text Extreme CE 23.4 Version\nResponsible to design and develop different documents and business forms using OpenText Exstream.\n* Understanding of different input, output file formats, and Print file formats (PDF etc). Perform unit testing of templates/documents",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['switching', 'data management', 'unit testing', 'networking', 'ospf', 'bgp', 'juniper', 'nortel', 'vrrp', 'asa', 'temenos t24', 'test data management', 'routing', 'firewall', 't24', 'hsrp', 'mpls', 'big data', 'cisco', 'ccna', 'fortigate']",2025-06-11 05:24:51
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Mumbai'],"As a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution.\nYour primary responsibilities include:\nLead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements.\nStrive for continuous improvements by testing the build solution and working under an agile framework.\nDiscover and implement the latest technologies trends to maximize and build creative solutions\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience with Apache Spark (PySpark)In-depth knowledge of Spark’s architecture, core APIs, and PySpark for distributed data processing.\nBig Data TechnologiesFamiliarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering\n\nSkills:\nStrong understanding of ETL pipelines, data modeling, and data warehousing concepts.\nStrong proficiency in PythonExpertise in Python programming with a focus on data processing and manipulation. Data Processing FrameworksKnowledge of data processing libraries such as Pandas, NumPy.\nSQL ProficiencyExperience writing optimized SQL queries for large-scale data analysis and transformation.\nCloud PlatformsExperience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems\n\n\nPreferred technical and professional experience\nDefine, drive, and implement an architecture strategy and standards for end-to-end monitoring.\nPartner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering,\nGood to have detection and prevention tools for Company products and Platform and customer-facing",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data processing', 'pyspark', 'data modeling', 'spark', 'data analysis', 'microsoft azure', 'data warehousing', 'cloud platforms', 'numpy', 'data engineering', 'sql', 'pandas', 'spring boot', 'etl pipelines', 'java', 'gcp', 'kafka', 'data warehousing concepts', 'hadoop', 'big data', 'aws', 'etl']",2025-06-11 05:24:53
Data Engineer-Enterprise Content Management,IBM,2 - 5 years,Not Disclosed,['Hyderabad'],"Experience is creating templates in Open Text Extreme CE 23.4 Version\nResponsible to design and develop different documents and business forms using OpenText Exstream.\n* Understanding of different input, output file formats, and Print file formats(PDF etc). Perform unit testing of templates/documents.\nApply styles and images to document design.\nUse output comparison tools to compare different outputs\nShould have experience working with Exstream Design Manager & Exstream Designer Tool.\nShould have prior knowledge on working with Exstream Web Service.\nDesigning Templates, Objects, Rules, Variables and creation of Documents based on Templates.\nUnderstand current SmartCOMM Templates and create templates based on that\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nIntuitive individual with an ability to manage change and proven time management\nProven interpersonal skills while contributing to team effort by accomplishing related results as needed\nUp-to-date technical knowledge by attending educational workshops, reviewing publications\n\n\nPreferred technical and professional experience\nExperience is creating templates in Open Text Extreme CE 23.4 Version\nResponsible to design and develop different documents and business forms using OpenText Exstream.\nUnderstanding of different input, output file formats, and Print file formats(PDF etc). Perform unit testing of templates/documents",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data management', 'unit testing', 'spring', 'java', 'general surgery', 'com', 'pile foundation', 'c++', 'c', 'data analysis', 'laparoscopic surgery', 'openshift', 'sql server', 'docker', 'test data management', 'urology', 'sapui5', 'aws', 'big data']",2025-06-11 05:24:55
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Navi Mumbai'],"As a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution.\nYour primary responsibilities include:\nLead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements.\nStrive for continuous improvements by testing the build solution and working under an agile framework.\nDiscover and implement the latest technologies trends to maximize and build creative solutions\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience with Apache Spark (PySpark)In-depth knowledge of Spark’s architecture, core APIs, and PySpark for distributed data processing.\nBig Data TechnologiesFamiliarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering\n\nSkills:\nStrong understanding of ETL pipelines, data modeling, and data warehousing concepts.\nStrong proficiency in PythonExpertise in Python programming with a focus on data processing and manipulation. Data Processing FrameworksKnowledge of data processing libraries such as Pandas, NumPy.\nSQL ProficiencyExperience writing optimized SQL queries for large-scale data analysis and transformation.\nCloud PlatformsExperience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems\n\n\nPreferred technical and professional experience\nDefine, drive, and implement an architecture strategy and standards for end-to-end monitoring.\nPartner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering,\nGood to have detection and prevention tools for Company products and Platform and customer-facing",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data processing', 'pyspark', 'data modeling', 'spark', 'data analysis', 'microsoft azure', 'data warehousing', 'cloud platforms', 'numpy', 'data engineering', 'sql', 'pandas', 'spring boot', 'etl pipelines', 'java', 'gcp', 'kafka', 'data warehousing concepts', 'hadoop', 'big data', 'aws', 'etl']",2025-06-11 05:24:56
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Kochi'],"As a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution\nYour primary responsibilities include:\nLead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements.\nStrive for continuous improvements by testing the build solution and working under an agile framework.\nDiscover and implement the latest technologies trends to maximize and build creative solutions\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience with Apache Spark (PySpark)In-depth knowledge of Spark’s architecture, core APIs, and PySpark for distributed data processing.\nBig Data TechnologiesFamiliarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering\n\nSkills:\nStrong understanding of ETL pipelines, data modeling, and data warehousing concepts.\n*\nStrong proficiency in PythonExpertise in Python programming with a focus on data processing and manipulation. Data Processing FrameworksKnowledge of data processing libraries such as Pandas, NumPy.\nSQL ProficiencyExperience writing optimized SQL queries for large-scale data analysis and transformation.\nCloud PlatformsExperience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems\n\n\nPreferred technical and professional experience\nDefine, drive, and implement an architecture strategy and standards for end-to-end monitoring.\nPartner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering,\nGood to have detection and prevention tools for Company products and Platform and customer-facing",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data processing', 'pyspark', 'data modeling', 'spark', 'data analysis', 'microsoft azure', 'data warehousing', 'cloud platforms', 'numpy', 'data engineering', 'sql', 'pandas', 'spring boot', 'etl pipelines', 'java', 'gcp', 'kafka', 'data warehousing concepts', 'hadoop', 'big data', 'aws', 'etl']",2025-06-11 05:24:58
Data Engineer-Data Platforms-Azure,IBM,6 - 7 years,Not Disclosed,['Kochi'],"As an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nTotal Exp-6-7 Yrs (Relevant-4-5 Yrs)\nMandatory\n\nSkills:\nAzure Databricks, Python/PySpark, SQL, Github, - Azure Devops - Azure Blob\nAbility to use programming languages like Java, Python, Scala, etc., to build pipelines to extract and transform data from a repository to a data consumer\nAbility to use Extract, Transform, and Load (ETL) tools and/or data integration, or federation tools to prepare and transform data as needed.\nAbility to use leading edge tools such as Linux, SQL, Python, Spark, Hadoop and Java\n\n\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['pyspark', 'azure devops', 'sql', 'azure databricks', 'python', 'data management', 'scala', 'azure data factory', 'java', 'sql azure', 'spark', 'linux', 'ssrs', 'hadoop', 'big data', 'etl', 'github', 'azure data lake', 'ssas', 'azure blob', 'microsoft azure', 'power bi', 'sql server', 'ssis', 'data integration']",2025-06-11 05:25:00
Data Engineer-Data Platforms-Google,IBM,5 - 7 years,Not Disclosed,['Hyderabad'],"Skilled Multiple GCP services - GCS, BigQuery, Cloud SQL, Dataflow, Pub/Sub, Cloud Run, Workflow, Composer, Error reporting, Log explorer etc.\nMust have Python and SQL work experience & Proactive, collaborative and ability to respond to critical situation\nAbility to analyse data for functional business requirements & front face customer\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n5 to 7 years of relevant experience working as technical analyst with Big Query on GCP platform.\nSkilled in multiple GCP services - GCS, Cloud SQL, Dataflow, Pub/Sub, Cloud Run, Workflow, Composer, Error reporting, Log explorer\nYou love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies\nAmbitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work\n\n\nPreferred technical and professional experience\nCreate up to 3 bullets maxitive individual with an ability to manage change and proven time management\nProven interpersonal skills while contributing to team effort by accomplishing related results as needed\nUp-to-date technical knowledge by attending educational workshops, reviewing publications (encouraging then to focus on required skills)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'gcp', 'bigquery', 'cloud sql', 'python', 'hive', 'gen', 'java', 'postgresql', 'spark', 'linux', 'mysql', 'hadoop', 'big data', 'pubsub', 'airflow', 'application engine', 'machine learning', 'sql server', 'dataproc', 'cloud storage', 'bigtable', 'agile', 'sqoop', 'aws', 'data flow']",2025-06-11 05:25:02
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Mumbai'],"Experience with Scala object-oriented/object function Strong SQL background.\nExperience in Spark SQL, Hive, Data Engineer.\nSQL Experience with data pipelines & Data Lake Strong background in distributed comp.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nSQL Experience with data pipelines & Data Lake Strong background in distributed comp\nExperience with Scala object-oriented/object function Strong SQL background\n\n\nPreferred technical and professional experience\nCore Scala Development Experience",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'scala', 'sql', 'spark', 'data lake', 'amazon redshift', 'pyspark', 'data warehousing', 'emr', 'java', 'data modeling', 'mysql', 'hadoop', 'big data', 'etl', 'python', 'microsoft azure', 'machine learning', 'data engineering', 'sql server', 'nosql', 'amazon ec2', 'kafka', 'sqoop', 'aws']",2025-06-11 05:25:03
Data Engineer-Data Platforms,IBM,6 - 11 years,Not Disclosed,['Mysuru'],"As an Application Developer, you will lead IBM into the future by translating system requirements into the design and development of customized systems in an agile environment.\nThe success of IBM is in your hands as you transform vital business needs into code and drive innovation. Your work will power IBM and its clients globally, collaborating and integrating code into enterprise systems. You will have access to the latest education, tools and technology, and a limitless career path with the world’s technology leader. Come to IBM and make a global impact\nResponsibilities:\nResponsible to manage end to end feature development and resolve challenges faced in implementing the same\nLearn new technologies and implement the same in feature development within the time frame provided\nManage debugging, finding root cause analysis and fixing the issues reported on Content Management back end software system\nfixing the issues reported on Content Management back end software system\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nOverall, more than 6 years of experience with more than 4+ years of Strong Hands on experience in Python and Spark\nStrong technical abilities to understand, design, write and debug to develop applications on Python and Pyspark.\nGood to Have;- Hands on Experience on cloud technology AWS/GCP/Azure\nstrong problem-solving skill\n\n\nPreferred technical and professional experience\nGood to Have;- Hands on Experience on cloud technology AWS/GCP/Azure",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['pyspark', 'spark', 'gcp', 'python', 'aws', 'hive', 'scala', 'apache pig', 'sql', 'docker', 'java', 'linux', 'mysql', 'hadoop', 'big data', 'etl', 'hbase', 'natural language processing', 'oozie', 'microsoft azure', 'machine learning', 'mapreduce', 'kafka', 'sqoop', 'unix']",2025-06-11 05:25:05
DATA ENGINEER-ADVANCED ANALYTICS,IBM,5 - 10 years,Not Disclosed,['Gurugram'],"Develop, test and support future-ready data solutions for customers across industry verticals\nDevelop, test, and support end-to-end batch and near real-time data flows/pipelines\nDemonstrate understanding in data architectures, modern data platforms, big data, analytics, cloud platforms, data governance and information management and associated technologies\nCommunicates risks and ensures understanding of these risks.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nMinimum of 5+ years of related experience required\nExperience in modeling and business system designs\nGood hands-on experience on DataStage, Cloud based ETL Services\nHave great expertise in writing TSQL code\nWell versed with data warehouse schemas and OLAP techniques\n\n\nPreferred technical and professional experience\nAbility to manage and make decisions about competing priorities and resources.\nAbility to delegate where appropriate\nMust be a strong team player/leader\nAbility to lead Data transformation project with multiple junior data engineers\nStrong oral written and interpersonal skills for interacting and throughout all levels of the organization.\nAbility to clearly communicate complex business problems and technical solutions.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data architecture', 'big data', 'information management', 'cloud platforms', 'data governance', 'schema', 't-sql', 'ansible', 'docker', 'sql', 'java', 'git', 'devops', 'linux', 'jenkins', 'j2ee', 'shell scripting', 'mysql', 'etl', 'python', 'data analytics', 'datastage', 'microsoft azure', 'warehouse', 'olap', 'aws']",2025-06-11 05:25:07
Data Engineer-Data Platforms-AWS,IBM,4 - 9 years,Not Disclosed,['Kochi'],"As Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in the development of data solutions using Spark Framework with Python or Scala on Hadoop and AWS Cloud Data Platform\n\nResponsibilities:\nExperienced in building data pipelines to Ingest, process, and transform data from files, streams and databases. Process the data with Spark, Python, PySpark, Scala, and Hive, Hbase or other NoSQL databases on Cloud Data Platforms (AWS) or HDFS\nExperienced in develop efficient software code for multiple use cases leveraging Spark Framework / using Python or Scala and Big Data technologies for various use cases built on the platform\nExperience in developing streaming pipelines\nExperience to work with Hadoop / AWS eco system components to implement scalable solutions to meet the ever-increasing data volumes, using big data/cloud technologies Apache Spark, Kafka, any Cloud computing etc\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nMinimum 4+ years of experience in Big Data technologies with extensive data engineering experience in Spark / Python or Scala.\nMinimum 3 years of experience on Cloud Data Platforms on AWS;\nExperience in AWS EMR / AWS Glue / DataBricks, AWS RedShift, DynamoDB\nGood to excellent SQL skills\nExposure to streaming solutions and message brokers like Kafka technologies.\n\n\nPreferred technical and professional experience\nCertification in AWS and Data Bricks or Cloudera Spark Certified developers.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['scala', 'sql', 'python', 'data engineering', 'aws', 'hive', 'cloudera', 'glue', 'amazon redshift', 'big data technologies', 'pyspark', 'emr', 'cloud technologies', 'apache', 'spark', 'aws cloud', 'aws emr', 'hadoop', 'big data', 'cloud computing', 'hbase', 'dynamo db', 'nosql', 'aws glue', 'data bricks', 'kafka']",2025-06-11 05:25:08
Data Engineer-Data Platforms,IBM,4 - 7 years,Not Disclosed,['Gurugram'],"A Data Engineer specializing in enterprise data platforms, experienced in building, managing, and optimizing data pipelines for large-scale environments. Having expertise in big data technologies, distributed computing, data ingestion, and transformation frameworks.\nProficient in Apache Spark, PySpark, Kafka, and Iceberg tables, and understand how to design and implement scalable, high-performance data processing solutions.What you’ll doAs a Data Engineer – Data Platform Services, responsibilities include:\n\nData Ingestion & Processing\nDesigning and developing data pipelines to migrate workloads from IIAS to Cloudera Data Lake.\nImplementing streaming and batch data ingestion frameworks using Kafka, Apache Spark (PySpark).\nWorking with IBM CDC and Universal Data Mover to manage data replication and movement.\nBig Data & Data Lakehouse Management\nImplementing Apache Iceberg tables for efficient data storage and retrieval.\nManaging distributed data processing with Cloudera Data Platform (CDP).\nEnsuring data lineage, cataloging, and governance for compliance with Bank/regulatory policies.\nOptimization & Performance Tuning\nOptimizing Spark and PySpark jobs for performance and scalability.\nImplementing data partitioning, indexing, and caching to enhance query performance.\nMonitoring and troubleshooting pipeline failures and performance bottlenecks.\nSecurity & Compliance\nEnsuring secure data access, encryption, and masking using Thales CipherTrust.\nImplementing role-based access controls (RBAC) and data governance policies.\nSupporting metadata management and data quality initiatives.\nCollaboration & Automation\nWorking closely with Data Scientists, Analysts, and DevOps teams to integrate data solutions.\nAutomating data workflows using Airflow and implementing CI/CD pipelines with GitLab and Sonatype Nexus.\nSupporting Denodo-based data virtualization for seamless data access\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n4-7 years of experience in big data engineering, data integration, and distributed computing.\nStrong skills in Apache Spark, PySpark, Kafka, SQL, and Cloudera Data Platform (CDP).\nProficiency in Python or Scala for data processing.\nExperience with data pipeline orchestration tools (Apache Airflow, Stonebranch UDM).\nUnderstanding of data security, encryption, and compliance frameworks\n\n\nPreferred technical and professional experience\nExperience in banking or financial services data platforms.\nExposure to Denodo for data virtualization and DGraph for graph-based insights.\nFamiliarity with cloud data platforms (AWS, Azure, GCP).\nCertifications in Cloudera Data Engineering, IBM Data Engineering, or AWS Data Analytics",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'scala', 'pyspark', 'sql', 'spark', 'cloudera', 'continuous integration', 'data analytics', 'data processing', 'airflow', 'big data technologies', 'ci/cd', 'microsoft azure', 'data engineering', 'distributed computing', 'gcp', 'kafka', 'data ingestion', 'gitlab', 'big data', 'aws', 'data integration']",2025-06-11 05:25:10
Data Engineer-Data Platforms,IBM,5 - 10 years,Not Disclosed,['Mumbai'],"As a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\n\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.\nGood working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\n\n\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['scala', 'hadoop spark', 'spark', 'big data', 'python', 'hive', 'cloudera', 'pyspark', 'sql', 'java', 'git', 'postgresql', 'devops', 'jenkins', 'data ingestion', 'mysql', 'hadoop', 'etl', 'hbase', 'data analysis', 'dynamo db', 'oozie', 'microsoft azure', 'impala', 'data engineering', 'lambda expressions', 'kafka', 'sqoop', 'aws']",2025-06-11 05:25:12
Data Engineer-Data Platforms,IBM,5 - 10 years,Not Disclosed,['Pune'],"As a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\n\nYour primary responsibilities include\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.\nGood working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\n\n\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['scala', 'hadoop spark', 'spark', 'big data', 'python', 'hive', 'cloudera', 'pyspark', 'sql', 'java', 'git', 'postgresql', 'devops', 'jenkins', 'data ingestion', 'mysql', 'hadoop', 'etl', 'hbase', 'data analysis', 'dynamo db', 'oozie', 'microsoft azure', 'impala', 'data engineering', 'lambda expressions', 'kafka', 'sqoop', 'aws']",2025-06-11 05:25:14
Data Engineer-Data Platforms,IBM,5 - 10 years,Not Disclosed,['Navi Mumbai'],"As a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\n\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.\nGood working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\n\n\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['scala', 'hadoop spark', 'spark', 'big data', 'python', 'hive', 'cloudera', 'pyspark', 'sql', 'java', 'git', 'postgresql', 'devops', 'jenkins', 'data ingestion', 'mysql', 'hadoop', 'etl', 'hbase', 'data analysis', 'dynamo db', 'oozie', 'microsoft azure', 'impala', 'data engineering', 'lambda expressions', 'kafka', 'sqoop', 'aws']",2025-06-11 05:25:15
Associate Analyst - Data Engineer,Pepsico,2 - 7 years,Not Disclosed,['Hyderabad'],"Overview\n\nPepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT. The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCo s global business scale to enable business insights, advanced analytics, and new product development. PepsiCo s Data Management and Operations team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations, and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.\nMaintain a predictable, transparent, global operating rhythm that ensures always-on access to high-quality data for stakeholders across the company.\nResponsible for day-to-day data collection, transportation, maintenance/curation, and access to the PepsiCo corporate data asset\nWork cross-functionally across the enterprise to centralize data and standardize it for use by business, data science or other stakeholders.\nIncrease awareness about available data and democratize access to it across the company.\nAs a data engineer, you will be the key technical expert building PepsiCo's data productsto drive a strong vision. You'll be empowered to create data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company.\nAs a member of the data engineering team, you will help developingvery large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premises data sources as well as cloud and remote systems.\n\nResponsibilities\nAct as a subject matter expert across different digital projects.\nOversee work with internal clients and external partners to structure and store data into unified taxonomies and link them together with standard identifiers.\nManage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.\nBuild and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.\nResponsible for implementing best practices around systems integration, security, performance, and data management.\nEmpower the business by creating value through the increased adoption of data, data science and business intelligence landscape.\nCollaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.\nEvolve the architectural capabilities and maturity of the data platform by engaging with enterprise architects and strategic internal and external partners.\nDevelop and optimize procedures to productionalize data science models.\nDefine and manage SLA s for data products and processes running in production.\nSupport large-scale experimentation done by data scientists.\nPrototype new approaches and build solutions at scale.\nResearch in state-of-the-art methodologies.\nCreate documentation for learnings and knowledge transfer.\nCreate and audit reusable packages or libraries.\n\nQualifications\n4+ years of overall technology experience that includes at least 3+ years of hands-on software development, data engineering, and systems architecture.\n3+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.\n3+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).\n2+ years in cloud data engineering experience in Azure.\nFluent with Azure cloud services. Azure Certification is a plus.\nExperience in Azure Log Analytics\nExperience with integration of multi cloud services with on-premises technologies.\nExperience with data modelling, data warehousing, and building high-volume ETL/ELT pipelines.\nExperience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.\nExperience building/operatinghighly available, distributed systems of data extraction, ingestion, and processing of large data sets.\nExperience with at least one MPP database technology such as Redshift, Synapse or Snowflake.\nExperience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.\nExperience with Statistical/ML techniques is a plus.\nExperience with building solutions in the retail or in the supply chain space is a plus.\nExperience with version control systems like Github and deployment & CI tools.\nWorking knowledge of agile development, including DevOps and DataOps concepts.\nB Tech/ BA/ BS in Computer Science, Math, Physics, or other technical fields.\nSkills, Abilities, Knowledge:\nExcellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.\nStrong change manager. Comfortable with change, especially that which arises through company growth.\nAbility to understand and translate business requirements into data and technical requirements.\nHigh degree of organization and ability to manage multiple, competing projects and priorities simultaneously.\nPositive and flexible attitude to enable adjusting to different needs in an ever-changing environment.\nStrong organizational and interpersonal skills; comfortable managing trade-offs.\nFoster a team culture of accountability, communication, and self-management.\nProactively drives impact and engagement while bringing others along.\nConsistently attain/exceed individual and team goals.\n4+ years of overall technology experience that includes at least 3+ years of hands-on software development, data engineering, and systems architecture.\n3+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.\n3+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).\n2+ years in cloud data engineering experience in Azure.\nFluent with Azure cloud services. Azure Certification is a plus.\nExperience in Azure Log Analytics\nExperience with integration of multi cloud services with on-premises technologies.\nExperience with data modelling, data warehousing, and building high-volume ETL/ELT pipelines.\nExperience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.\nExperience building/operatinghighly available, distributed systems of data extraction, ingestion, and processing of large data sets.\nExperience with at least one MPP database technology such as Redshift, Synapse or Snowflake.\nExperience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.\nExperience with Statistical/ML techniques is a plus.\nExperience with building solutions in the retail or in the supply chain space is a plus.\nExperience with version control systems like Github and deployment & CI tools.\nWorking knowledge of agile development, including DevOps and DataOps concepts.\nB Tech/BA/BS in Computer Science, Math, Physics, or other technical fields.",Industry Type: Beverage,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'Data Lake Infrastructure', 'Azure Data Factory', 'PySpark', 'Scala', 'Azure Log Analytics', 'Azure Databricks', 'Data Warehousing', 'Data Analytics', 'data collection', 'Python', 'SQL']",2025-06-11 05:25:17
Data Engineer-Business Intelligence,IBM,2 - 5 years,Not Disclosed,['Pune'],"Provide expertise in analysis, requirements gathering, design, coordination, customization, testing and support of reports, in client’s environment\nDevelop and maintain a strong working relationship with business and technical members of the team\nRelentless focus on quality and continuous improvement\nPerform root cause analysis of reports issues\nDevelopment / evolutionary maintenance of the environment, performance, capability and availability.\nAssisting in defining technical requirements and developing solutions\nEffective content and source-code management, troubleshooting and debugging\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nTableau Desktop Specialist, SQL -Strong understanding of SQL for Querying database Good to have - Python ; Snowflake, Statistics, ETL experience.\nExtensive knowledge on using creating impactful visualization using Tableau.\nMust have thorough understanding of SQL & advance SQL (Joining & Relationships).\nMust have experience in working with different databases and how to blend & create relationships in Tableau.\nMust have extensive knowledge to creating Custom SQL to pull desired data from databases. Troubleshooting capabilities to debug Data controls\n\n\nPreferred technical and professional experience\nTroubleshooting capabilities to debug Data controls Capable of converting business requirements into workable model.\nGood communication skills, willingness to learn new technologies, Team Player, Self-Motivated, Positive Attitude.\nMust have thorough understanding of SQL & advance SQL (Joining & Relationships",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['advance sql', 'dbms', 'sql', 'debugging', 'database queries', 'snowflake', 'python', 'data analysis', 'data analytics', 'power bi', 'business analysis', 'data warehousing', 'business analytics', 'business intelligence', 'sql server', 'tableau', 'troubleshooting', 'data visualization', 'etl', 'statistics']",2025-06-11 05:25:19
Data Engineer-Data Platforms-Google,IBM,2 - 5 years,Not Disclosed,['Pune'],"As an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\n\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\n\n\nIn this role, your responsibilities may include:\n\n\n\n\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nStrong MS SQL, Azure Databricks experience\nImplement and manage data models in DBT, data transformation and alignment with business requirements.\nIngest raw, unstructured data into structured datasets to cloud object store.\nUtilize DBT to convert raw, unstructured data into structured datasets, enabling efficient analysis and reporting.\nWrite and optimize SQL queries within DBT to enhance data transformation processes and improve overall performance\n\n\nPreferred technical and professional experience\nEstablish best DBT processes to improve performance, scalability, and reliability.\nDesign, develop, and maintain scalable data models and transformations using DBT in conjunction with Databricks\nProven interpersonal skills while contributing to team effort by accomplishing related results as required",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure databricks', 'data management', 'sql server', 'sql', 'data transformation', 'python', 'data analysis', 'data analytics', 'data warehousing', 'power bi', 'azure data factory', 'machine learning', 'business intelligence', 'tableau', 'data extraction', 'data modeling', 'data visualization', 'etl']",2025-06-11 05:25:21
Software Engineer III - Data Engineer,JPMorgan Chase Bank,3 - 8 years,Not Disclosed,['Hyderabad'],"As a Data Engineer III at JPMorgan Chase within the Consumer & Community Banking Team, you serve as a seasoned member of an agile team to design and deliver trusted data collection, storage, access, and analytics solutions in a secure, stable, and scalable way. You are responsible for developing, testing, and maintaining critical data pipelines and architectures across multiple technical areas within various business functions in support of the firm s business objectives.\nJob responsibilities\nExecutes software solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problems\nSupports review of controls to ensure sufficient protection of enterprise data\nAdvises and makes custom configuration changes in one to two tools to generate a product at the business or customer request\nUpdates logical or physical data models based on new use cases\nFrequently uses SQL and understands NoSQL databases and their niche in the marketplace\nAdds to team culture of diversity, equity, inclusion, and respect\nContributes to software and data engineering communities of practice and events that explore new and emerging technologies\nGathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of software applications and systems\nRequired qualifications, capabilities, and skills\nFormal training or certification on Data Engineering concepts and 3+ years applied experience in AWS and Kubernetes\nProficiency in one or more large-scale data processing distributions such as JavaSpark/PySpark along with knowledge on Data Pipeline (DPL), Data Modeling, Data warehouse, Data Migration and so-on.\nExperience across the data lifecycle along with expertise with consuming data in any of batch (file), near real-time (IBM MQ, Apache Kafka), streaming (AWS kinesis, MSK)\nAdvanced at SQL (eg, joins and aggregations)\nWorking understanding of NoSQL databases\nExperience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages.\nSolid understanding of agile methodologies such as CI/CD, Application Resiliency, and Security\nSignificant experience with statistical data analysis and ability to determine appropriate tools and data patterns to perform analysis\nExperience customizing changes in a tool to generate product\nPreferred qualifications, capabilities, and skills\nFamiliarity with modern front-end technologies\nExperience designing and building REST API services using Java\nExposure to cloud technologies - knowledge on Hybrid cloud architectures is highly desirable.\nAWS Developer/Solutions Architect Certification is highly desired",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'Data migration', 'Front end', 'Data modeling', 'Debugging', 'Agile', 'Apache', 'Troubleshooting', 'Analytics', 'SQL']",2025-06-11 05:25:22
DataBricks - Data Engineering,Wipro,2 - 7 years,Not Disclosed,['Hyderabad'],"About The Role  \n\nRole Purpose\n\nThe purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\n\n ? \n\nDo\n\n1. Instrumental in understanding the requirements and design of the product/ software\nDevelop software solutions by studying information needs, studying systems flow, data usage and work processes\nInvestigating problem areas followed by the software development life cycle\nFacilitate root cause analysis of the system issues and problem statement\nIdentify ideas to improve system performance and impact availability\nAnalyze client requirements and convert requirements to feasible design\nCollaborate with functional teams or systems analysts who carry out the detailed investigation into software requirements\nConferring with project managers to obtain information on software capabilities\n\n\n ? \n\n2. Perform coding and ensure optimal software/ module development\nDetermine operational feasibility by evaluating analysis, problem definition, requirements, software development and proposed software\nDevelop and automate processes for software validation by setting up and designing test cases/scenarios/usage cases, and executing these cases\nModifying software to fix errors, adapt it to new hardware, improve its performance, or upgrade interfaces.\nAnalyzing information to recommend and plan the installation of new systems or modifications of an existing system\nEnsuring that code is error free or has no bugs and test failure\nPreparing reports on programming project specifications, activities and status\nEnsure all the codes are raised as per the norm defined for project / program / account with clear description and replication patterns\nCompile timely, comprehensive and accurate documentation and reports as requested\nCoordinating with the team on daily project status and progress and documenting it\nProviding feedback on usability and serviceability, trace the result to quality risk and report it to concerned stakeholders\n\n\n ? \n\n3. Status Reporting and Customer Focus on an ongoing basis with respect to project and its execution\nCapturing all the requirements and clarifications from the client for better quality work\nTaking feedback on the regular basis to ensure smooth and on time delivery\nParticipating in continuing education and training to remain current on best practices, learn new programming languages, and better assist other team members.\nConsulting with engineering staff to evaluate software-hardware interfaces and develop specifications and performance requirements\nDocument and demonstrate solutions by developing documentation, flowcharts, layouts, diagrams, charts, code comments and clear code\nDocumenting very necessary details and reports in a formal way for proper understanding of software from client proposal to implementation\nEnsure good quality of interaction with customer w.r.t. e-mail content, fault report tracking, voice calls, business etiquette etc\nTimely Response to customer requests and no instances of complaints either internally or externally\n\n\n ? \n\nDeliver\n\n\nNo.\n\nPerformance Parameter\n\nMeasure 1. Continuous Integration, Deployment & Monitoring of Software 100% error free on boarding & implementation, throughput %, Adherence to the schedule/ release plan 2. Quality & CSAT On-Time Delivery, Manage software, Troubleshoot queries, Customer experience, completion of assigned certifications for skill upgradation 3. MIS & Reporting 100% on time MIS & report generation\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['software development life cycle', 'continuous integration', 'software development', 'mis', 'root cause analysis', 'data bricks', 'analyzing information', 'software validation', 'software management', 'data engineering', 'digital transformation', 'customer experience']",2025-06-11 05:25:24
"Data Engineer II, Payroll Tech",Amazon,3 - 8 years,Not Disclosed,['Hyderabad'],"Payroll Technology at Amazon is all about enabling our business to perform at scale as efficiently as possible with no defects. As Amazons workforce grows, both in size and geography, Amazons payroll operations become increasingly complex, and our customers are asked to do more with less. Process can only get them so far, and thats where we come in with technology solutions to integrate and automate systems, detect defects before payment, and provide insights. As a data engineer in payroll, you will have to onboard payroll vendors across various geographies by building versatile and scalable design solutions. Having strong written and verbal communication, and the ability to communicate with end users in non-technical terms, is vital to your long-term success.\n\nThe ideal candidate will have experience working with large datasets, distributed computing technologies and service-oriented architecture. The candidate should relish working with large volumes of data, and enjoys the challenge of highly complex technical contexts. He/she should be an expert with data modeling, ETL design and business intelligence tools and has hand-on knowledge on columnar databases. He/she is a self-starter, comfortable with ambiguity, able to think big and enjoys working in a fast-paced team.\n\nResponsibilities:\n\nDesign, build and own all the components of a high-volume data warehouse end to end.\nBuild efficient data models using industry best practices and metadata for ad-hoc and pre-built reporting\nProvide wing-to-wing data engineering support for project lifecycle execution (design, execution and risk assessment)\nInterface with business customers, gathering requirements and delivering complete data & reporting solutions owning the design, development, and maintenance of ongoing metrics, reports, dashboards, etc. to drive key business decisions\nContinually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers\nInterface with other technology teams to extract, transform, and load (ETL) data from a wide variety of data sources\nOwn the functional and nonfunctional scaling of software systems in your ownership area.\nImplement big data solutions for distributed computing.\nWilling to learn and develop strong skill set in AWS technologies\n\n\nAs a DE on our team, you will be responsible for leading the data modelling, database design, and launch of some of the core data pipelines. You will have significant influence on our overall strategy by helping define the data model, drive the database design, and spearhead the best practices to delivery high quality products.\n\nA day in the life\nYou are expected to do data modelling, database design, build data pipelines as per Amazon standards, design reviews, and supporting data privacy and security initiatives. You will attend regular stand-up meetings and provide your updates. You will keep an eye out for opportunities to improve the product or user experience and suggest those enhancements. You will participate in requirement grooming meetings to ensure the use cases we deliver are complete and functional. You will take your turn at on-call and own production operational maintenance. You will respond to customer issues and monitor databases for healthy state and performance.\n\nAbout the team\nOur mission is to build applications which can solve challenges Global Payroll Operations teams face on daily basis, automate the tasks they perform manually, provide them seamless experience by integrating with other dependent systems, and eventually reduce Pay Defects and improve pay accuracy 3+ years of data engineering experience\n4+ years of SQL experience\nExperience with data modeling, warehousing and building ETL pipelines Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions\nExperience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)",,,,"['Payroll', 'metadata', 'SOA', 'data engineer ii', 'Database design', 'Risk assessment', 'data privacy', 'Business intelligence', 'Operations', 'SQL']",2025-06-11 05:25:25
Data Engineer-Business Intelligence,IBM,5 - 10 years,Not Disclosed,['Hyderabad'],"Provide expertise in analysis, requirements gathering, design, coordination, customization, testing and support of reports, in client’s environment\nDevelop and maintain a strong working relationship with business and technical members of the team\nRelentless focus on quality and continuous improvement\nPerform root cause analysis of reports issues\nDevelopment / evolutionary maintenance of the environment, performance, capability and availability.\nAssisting in defining technical requirements and developing solutions\nEffective content and source-code management, troubleshooting and debugging\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n5+ years of experience with BI tools, with expertise and/or certification in at least one major BI platform – Tableau preferred.\nAdvanced knowledge of SQL, including the ability to write complex stored procedures, views, and functions.\nProven capability in data storytelling and visualization, delivering actionable insights through compelling presentations.\nExcellent communication skills, with the ability to convey complex analytical findings to non-technical stakeholders in a clear, concise, and meaningful way.\n5.Identifying and analyzing industry trends, geographic variations, competitor strategies, and emerging customer behavior\n\n\nPreferred technical and professional experience\nTroubleshooting capabilities to debug Data controls Capable of converting business requirements into workable model.\nGood communication skills, willingness to learn new technologies, Team Player, Self-Motivated, Positive Attitude.\nMust have thorough understanding of SQL & advance SQL (Joining & Relationships)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['advance sql', 'sql', 'bi tools', 'debugging', 'troubleshooting', 'python', 'data analysis', 'data analytics', 'bi', 'data warehousing', 'power bi', 'business analysis', 'machine learning', 'business intelligence', 'sql server', 'qlikview', 'tableau', 'r', 'data visualization', 'etl', 'ssis']",2025-06-11 05:25:27
DataBricks - Data Engineering,Wipro,5 - 8 years,Not Disclosed,['Pune'],"Role Purpose\nThe purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\n\n\n\nDo\n1. Instrumental in understanding the requirements and design of the product/ software\nDevelop software solutions by studying information needs, studying systems flow, data usage and work processes\nInvestigating problem areas followed by the software development life cycle\nFacilitate root cause analysis of the system issues and problem statement\nIdentify ideas to improve system performance and impact availability\nAnalyze client requirements and convert requirements to feasible design\nCollaborate with functional teams or systems analysts who carry out the detailed investigation into software requirements\nConferring with project managers to obtain information on software capabilities\n\n\n\n2. Perform coding and ensure optimal software/ module development\nDetermine operational feasibility by evaluating analysis, problem definition, requirements, software development and proposed software\nDevelop and automate processes for software validation by setting up and designing test cases/scenarios/usage cases, and executing these cases\nModifying software to fix errors, adapt it to new hardware, improve its performance, or upgrade interfaces.\nAnalyzing information to recommend and plan the installation of new systems or modifications of an existing system\nEnsuring that code is error free or has no bugs and test failure\nPreparing reports on programming project specifications, activities and status\nEnsure all the codes are raised as per the norm defined for project / program / account with clear description and replication patterns\nCompile timely, comprehensive and accurate documentation and reports as requested\nCoordinating with the team on daily project status and progress and documenting it\nProviding feedback on usability and serviceability, trace the result to quality risk and report it to concerned stakeholders\n\n\n\n3. Status Reporting and Customer Focus on an ongoing basis with respect to project and its execution\nCapturing all the requirements and clarifications from the client for better quality work\nTaking feedback on the regular basis to ensure smooth and on time delivery\nParticipating in continuing education and training to remain current on best practices, learn new programming languages, and better assist other team members.\nConsulting with engineering staff to evaluate software-hardware interfaces and develop specifications and performance requirements\nDocument and demonstrate solutions by developing documentation, flowcharts, layouts, diagrams, charts, code comments and clear code\nDocumenting very necessary details and reports in a formal way for proper understanding of software from client proposal to implementation\nEnsure good quality of interaction with customer w.r.t. e-mail content, fault report tracking, voice calls, business etiquette etc\nTimely Response to customer requests and no instances of complaints either internally or externally\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure\n1.Continuous Integration, Deployment & Monitoring of Software100% error free on boarding & implementation, throughput %, Adherence to the schedule/ release plan\n2.Quality & CSATOn-Time Delivery, Manage software, Troubleshoot queries,Customer experience, completion of assigned certifications for skill upgradation\n3.MIS & Reporting100% on time MIS & report generation\nMandatory Skills: DataBricks - Data Engineering.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['DataBricks', 'module development', 'software development life cycle', 'Data Engineering', 'software development', 'quality assurance']",2025-06-11 05:25:29
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Pune'],"As a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution.\nYour primary responsibilities include:\nLead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements.\nStrive for continuous improvements by testing the build solution and working under an agile framework.\nDiscover and implement the latest technologies trends to maximize and build creative solutions\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience with Apache Spark (PySpark)In-depth knowledge of Spark’s architecture, core APIs, and PySpark for distributed data processing.\nBig Data TechnologiesFamiliarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering\n\nSkills:\nStrong understanding of ETL pipelines, data modeling, and data warehousing concepts.\nStrong proficiency in PythonExpertise in Python programming with a focus on data processing and manipulation. Data Processing FrameworksKnowledge of data processing libraries such as Pandas, NumPy.\nSQL ProficiencyExperience writing optimized SQL queries for large-scale data analysis and transformation.\nCloud PlatformsExperience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems\n\n\nPreferred technical and professional experience\nDefine, drive, and implement an architecture strategy and standards for end-to-end monitoring.\nPartner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering,\nGood to have detection and prevention tools for Company products and Platform and customer-facing",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data processing', 'pyspark', 'data modeling', 'spark', 'data analysis', 'microsoft azure', 'data warehousing', 'cloud platforms', 'numpy', 'data engineering', 'sql', 'pandas', 'spring boot', 'etl pipelines', 'java', 'gcp', 'kafka', 'data warehousing concepts', 'hadoop', 'big data', 'aws', 'etl']",2025-06-11 05:25:31
Data Engineer-Business Intelligence,IBM,2 - 5 years,Not Disclosed,['Pune'],"As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You’ll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you’ll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you’ll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviour’s.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\nExpertise in designing and implementing scalable data warehouse solutions on Snowflake, including schema design, performance tuning, and query optimization.\nStrong experience in building data ingestion and transformation pipelines using Talend to process structured and unstructured data from various sources.\nProficiency in integrating data from cloud platforms into Snowflake using Talend and native Snowflake capabilities.\nHands-on experience with dimensional and relational data modelling techniques to support analytics and reporting requirements.\nUnderstanding of optimizing Snowflake workloads, including clustering keys, caching strategies, and query profiling.\nAbility to implement robust data validation, cleansing, and governance frameworks within ETL processes.\nProficiency in SQL and/or Shell scripting for custom transformations and automation tasks\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nTableau Desktop Specialist, SQL -Strong understanding of SQL for Querying database\nGood to have - Python ; Snowflake, Statistics, ETL experience.\nExtensive knowledge on using creating impactful visualization using Tableau.\nMust have thorough understanding of SQL & advance SQL (Joining & Relationships)\n\n\nPreferred technical and professional experience\nMust have experience in working with different databases and how to blend & create relationships in Tableau.\nMust have extensive knowledge to creating Custom SQL to pull desired data from databases.\nTroubleshooting capabilities to debug Data controls. Capable of converting business requirements into workable model. Good communication skills, willingness to learn new technologies, Team Player, Self-Motivated, Positive Attitude.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['advance sql', 'snowflake', 'sql', 'shell scripting', 'database queries', 'python', 'performance tuning', 'talend', 'dbms', 'cloud platforms', 'warehouse', 'machine learning', 'business intelligence', 'pipeline', 'elastic search', 'tableau', 'data ingestion', 'splunk', 'etl', 'big data', 'statistics']",2025-06-11 05:25:32
Data Engineer-Business Intelligence,IBM,3 - 8 years,Not Disclosed,['Pune'],"Provide expertise in analysis, requirements gathering, design, coordination, customization, testing and support of reports, in client’s environment\nDevelop and maintain a strong working relationship with business and technical members of the team\nRelentless focus on quality and continuous improvement\nPerform root cause analysis of reports issues\nDevelopment / evolutionary maintenance of the environment, performance, capability and availability.\nAssisting in defining technical requirements and developing solutions\nEffective content and source-code management, troubleshooting and debugging.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nCognos Developer & Admin Required. EducationThe resource should be full time MCA/M. Tech/B. Tech/B.E. and should preferably have relevant certifications\nExperienceThe resource should have a minimum of 3 years of experience of working in the in BI DW projects in areas pertaining to reporting and visualization using cognos.\nThe resources shall have worked in at least two projects where they were involved in developing reporting/ visualization\nHe shall have good understanding of UNIX.\nShould be well conversant in English and should have excellent writing, MIS, communication, time management and multi-tasking skill\n\n\nPreferred technical and professional experience\nExperience with various cloud and integration platforms (e.g. AWS, Google, Azure)\nAgile mindset – ability to process changes of priorities and requests, ownership, critical thinking\nExperience with an ETL/Data Integration tool (eg. IBM InfoSphere DataStage, Azure Data Factory, Informatica PowerCenter)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['informatica powercenter', 'microsoft azure', 'mis', 'bi dw', 'etl', 'datastage', 'bi', 'azure data factory', 'cognos', 'root cause analysis', 'business intelligence', 'sql', 'dw', 'ibm datastage', 'troubleshooting', 'debugging', 'agile', 'data visualization', 'aws', 'data integration', 'informatica', 'unix']",2025-06-11 05:25:34
Data Engineer-Data Platforms-Google,IBM,6 - 11 years,Not Disclosed,['Gurugram'],"6+ years of industry work experience\nExperience extracting data from a variety of sources, and a desire to expand those skills\nWorked on Google Looker tool\nWorked on Big Query and GCP technologies\nStrong SQL and Spark knowledge\nExcellent Data Analysis skills. Must be comfortable with querying and analyzing large amount of data on Hadoop HDFS using Hive and Spark\nKnowledge of Financial Accounting is a bonus\nWork independently with cross functional team and drive towards the resolution\nExperience with Object oriented programming using python and its design patterns\nExperience handling Unix systems, for optimal usage to host enterprise web applications GCP certifications preferred.\nPayments Industry Background good to have\nCandidate who has been part to google Cloud Migration is an ideal Fit\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n3-5 years of experience\nIntuitive individual with an ability to manage change and proven time management\nProven interpersonal skills while contributing to team effort by accomplishing related results as needed\nUp-to-date technical knowledge by attending educational workshops, reviewing publications\n\n\nPreferred technical and professional experience\n6+ years of industry work experience\nExperience extracting data from a variety of sources, and a desire to expand those skills\nWorked on Google Looker tool",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data analysis', 'sql', 'spark', 'hadoop', 'hive', 'snowflake', 'data warehousing', 'power bi', 'google', 'business intelligence', 'sql server', 'qlikview', 'tableau', 'data modeling', 'gcp', 'financial accounting', 'bigquery', 'object oriented programming', 'data visualization', 'etl', 'ssis', 'unix']",2025-06-11 05:25:36
Data Engineer - Azure,Blend360 India,5 - 9 years,Not Disclosed,['Hyderabad'],"Job Description\nAs a Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\nQualifications\n3+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar fiel",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-11 05:25:37
Data Engineer-Enterprise Content Management,IBM,2 - 5 years,Not Disclosed,['Hyderabad'],"Design and Development of ECM Solutions\nFileNet API Integration\nConfigure and manage Events, Subscriptions, and Triggers in the Content Engine to support business process automation.\nDefine and enforce security policies, including Access Control Lists (ACLs), role-based access control (RBAC), and document-level security configurations\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nFileNet Developer with IBM FileNet P8 platform.\nThe ideal candidate will be responsible for designing, developing, implementing, and supporting enterprise content management solutions using FileNet, including customization of IBM Content Navigator (ICN), working with FileNet APIs, managing Records Manager configurations, and executing large-scale content migrations.\nRequired Skills and ExperienceExperience in IBM FileNet P8 platform (5.2/5.5 or higher). Strong hands-on experience with FileNet Java APIs (CE/PE APIs)\nUnderstanding, configuration and management of Event, Triggers in Content Engine to automate business logic. Implement and maintain FileNet security, including ACLs, role-based access control (RBAC), and document-level security\nGood communication skills and ability to work independently or as part of a team\n\n\nPreferred technical and professional experience\nIBM FileNet certification (e.g., IBM Certified Specialist - FileNet Content Manager).\nExperience with workflow design using FileNet BPM/Case Manager.\nExperience integrating FileNet with other enterprise systems via REST/SOAP APIs",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ibm filenet', 'java', 'rest', 'filenet', 'soap', 'data management', 'web services', 'jsp', 'hibernate', 'ecm', 'ibm content manager', 'sql', 'spring', 'j2ee', 'html', 'api', 'ce', 'oracle', 'datacap', 'javascript', 'sql server', 'servlets', 'enterprise content management', 'websphere application server', 'dojo']",2025-06-11 05:25:39
Data Engineer-Master Data Management,IBM,6 - 11 years,Not Disclosed,['Pune'],"Design, develop, and maintain applications using Java/J2EE and IBM Infosphere MDM Advanced Edition v11.x.\nParticipate in development and support projects including fail & fix and maintenance activities.\nCollaborate with clients and internal teams to understand requirements and deliver solutions.\nProvide timely and effective support for multiple clients, ensuring quick resolution of issues.\nEngage directly with clients in a client-facing role, ensuring clear communication and understanding of business needs.\nParticipate in on-call support and be flexible to extend support after hours when required.\nFollow software engineering best practices, including continuous integration and continuous delivery (CI/CD\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n6+ years of hands-on experience in Java/J2EE technologies.\n4+ years of hands-on development experience in IBM Infosphere MDM Advanced Edition v11.x.\nStrong understanding and practical experience with CI/CD tools and practices.\nProven experience in development and support projects, including maintenance and issue resolution.\nStrong client interaction and communication skills, with experience in direct client-facing roles\n\n\nPreferred technical and professional experience\nPrior experience in the Banking domain or financial services industry.\nExposure to Agile methodologies and collaborative development environments.\nFamiliarity with other enterprise data management tools or technologies",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'java', 'cd tools', 'client interaction', 'ci / cd tools', 'kubernetes', 'data management', 'ci/cd', 'ansible', 'docker', 'sql', 'plsql', 'git', 'devops', 'linux', 'j2ee', 'jenkins', 'shell scripting', 'ibm infosphere', 'python', 'maven', 'mdm', 'terraform', 'agile', 'aws', 'j2ee technologies', 'unix']",2025-06-11 05:25:41
Data Engineer-Data Platforms,IBM,6 - 11 years,Not Disclosed,['Pune'],"As an Application Developer, you will lead IBM into the future by translating system requirements into the design and development of customized systems in an agile environment.\n\nThe success of IBM is in your hands as you transform vital business needs into code and drive innovation. Your work will power IBM and its clients globally, collaborating and integrating code into enterprise systems. You will have access to the latest education, tools and technology, and a limitless career path with the world’s technology leader. Come to IBM and make a global impact\n\nResponsibilities:\nResponsible to manage end to end feature development and resolve challenges faced in implementing the same\nLearn new technologies and implement the same in feature development within the time frame provided\nManage debugging, finding root cause analysis and fixing the issues reported on Content Management back end software system\nfixing the issues reported on Content Management back-end software system\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nOverall, more than 6 years of experience with more than 4+ years of Strong Hands on experience in Python and Spark\nStrong technical abilities to understand, design, write and debug to develop applications on Python and Pyspark.\nGood to Have;- Hands on Experience on cloud technology AWS/GCP/Azure\nstrong problem-solving skill\n\n\nPreferred technical and professional experience\n\nGood to Have;- Hands on Experience on cloud technology AWS/GCP/Azure",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['pyspark', 'spark', 'gcp', 'python', 'aws', 'hive', 'scala', 'apache pig', 'sql', 'docker', 'java', 'linux', 'mysql', 'hadoop', 'big data', 'etl', 'hbase', 'natural language processing', 'oozie', 'microsoft azure', 'machine learning', 'mapreduce', 'kafka', 'sqoop', 'unix']",2025-06-11 05:25:42
Data Engineering Specialist - Level 2,AstraZeneca India Pvt. Ltd,5 - 8 years,Not Disclosed,['Chennai'],"In this role, you'll be responsible for analyzing and improving ETL Data Integration standards, optimizing performance through automation, and supporting the development of automation solutions. you'll join a team of ETL authorities providing support across numerous projects, working closely with global business partners to ensure successful delivery. Your expertise will be crucial in coordinating database design and development, fix technical issues, and taking ownership of evolving CI/CD processes.\nEssential Skills/Experience:\nAdministering the ETL Platform including:\nConfiguration and Licensing\nUser Management\nProject Creation (including Bitbucket repositories)\nJob Server and Job management\nHandling of Software Updates/Patches\nMajor version migration\n8+ years coordinating Enterprise ETL platforms and working with ETL Data Integration in an enterprise scale environment.\nTechnical support to applications on fix Environment, software and application level issues.\nKnowledge and experience of working on Unix shell scripting or python script.\nExperience fix or debugging on Linux server is required.\nKnowledge and experience on AWS Cloud ecosystem.\nETL License Management.\nDesign and Code Reviews.\nAiding in the establishment of team development standard methodologies.\nData warehousing and database design concepts.\nPrior experience of Migration and Patching activity.\nDesirable Skills / Experience:\nMaintain and develop CI/CD pipeline using groovy scripts.\nMaintain and develop Ansible scripts for ETL service and infrastructure management.\nDesign and Develop tools to standardize and automate monitoring, application support activities.\nExcellent written and oral communication skills.\nAbility to work optimally independently or as part of a team to achieve objectives.\nA self-starter with high levels of drive, energy, and resilience.\nWilling to be involved in all areas of deployment process including testing and release planning.\nEager to learn and develop new tech skills as required.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'Application support', 'Linux', 'Database design', 'Debugging', 'Technical support', 'Unix shell scripting', 'Monitoring', 'Python', 'Recruitment']",2025-06-11 05:25:44
"Data Engineer- ETL (AWS Glue)/Redshift ,Java ,SQL -P3",Nielsen Sports,5 - 8 years,Not Disclosed,['Gurugram'],"Job Description:\nWe are seeking an experienced Developer with expertise in ETL, AWS Glue and Data Engineer experience, combined with strong skills in Java and SQL. The ideal candidate will have 5-8 years of experience designing, developing, and implementing ETL processes and data integration solutions. Responsibilities include developing ETL pipelines using AWS Glue, managing data workflows with Informatica, and writing complex SQL queries. Strong problem-solving abilities and experience with data warehousing are essential.\nKey Skills:\nProficiency in AWS Glue and Informatica ETL tools\n\nStrong Java programming skills\n\nAdvanced SQL querying and optimization\n\nExperience with data integration and data warehousing\n\nExcellent problem-solving and analytical skills",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Analytical skills', 'SQL queries', 'Data management', 'Social media', 'Programming', 'Informatica', 'Data warehousing', 'AWS', 'SQL']",2025-06-11 05:25:45
"Data Engineer- ETL (AWS Glue)/Redshift ,Java ,SQL -P3",Nielsen Sports,5 - 8 years,Not Disclosed,['Mumbai'],"Job Description:\nWe are seeking an experienced Developer with expertise in ETL, AWS Glue and Data Engineer experience, combined with strong skills in Java and SQL. The ideal candidate will have 5-8 years of experience designing, developing, and implementing ETL processes and data integration solutions. Responsibilities include developing ETL pipelines using AWS Glue, managing data workflows with Informatica, and writing complex SQL queries. Strong problem-solving abilities and experience with data warehousing are essential.\nKey Skills:\nProficiency in AWS Glue and Informatica ETL tools\n\nStrong Java programming skills\n\nAdvanced SQL querying and optimization\n\nExperience with data integration and data warehousing\n\nExcellent problem-solving and analytical skills",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Analytical skills', 'SQL queries', 'Data management', 'Social media', 'Programming', 'Informatica', 'Data warehousing', 'AWS', 'SQL']",2025-06-11 05:25:47
Data Engineer-Data Platforms-Azure,IBM,7 - 12 years,Not Disclosed,['Mumbai'],"Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.\n7+ Yrs total experience in Data Engineering projects & 4+ years of relevant experience on Azure technology services and Python\nAzure Azure data factory, ADLS- Azure data lake store, Azure data bricks,\nMandatory Programming languages Py-Spark, PL/SQL, Spark SQL\nDatabase SQL DB\nExperience with AzureADLS, Databricks, Stream Analytics, SQL DW, COSMOS DB, Analysis Services, Azure Functions, Serverless Architecture, ARM Templates\nExperience with relational SQL and NoSQL databases, including Postgres and Cassandra.\nExperience with object-oriented/object function scripting languagesPython, SQL, Scala, Spark-SQL etc.\nData Warehousing experience with strong domain\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nIntuitive individual with an ability to manage change and proven time management\nProven interpersonal skills while contributing to team effort by accomplishing related results as needed\nUp-to-date technical knowledge by attending educational workshops, reviewing publications\n\n\nPreferred technical and professional experience\nExperience with AzureADLS, Databricks, Stream Analytics, SQL DW, COSMOS DB, Analysis Services, Azure Functions, Serverless Architecture, ARM Templates\nExperience with relational SQL and NoSQL databases, including Postgres and Cassandra.\nExperience with object-oriented/object function scripting languagesPython, SQL, Scala, Spark-SQL etc.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data warehousing', 'sql', 'plsql', 'db', 'azure databricks', 'azure data lake', 'ssas', 'scala', 'microsoft azure', 'azure data factory', 'cosmos', 'data engineering', 'nosql', 'data bricks', 'azure functions', 'azure infrastructure', 'postgresql', 'spark', 'cassandra', 'mysql', 'engineering projects']",2025-06-11 05:25:49
Azure Data Engineer,PwC India,7 years,Not Disclosed,['Gurugram'],"Position: Azure Data Engineer\nExperience: 4-7 Years\nLocation: Gurugram\nType: Full Time\nNotice period : Immediate to 30 days\n\nPreferred Certifications: Azure Data Engineer Associate, Databricks",,,,"['Azure Data Factory', 'Synapse Analytics', 'Data Bricks']",2025-06-11 05:25:50
Senior Azure Data Engineer (Only Immediate Join),Adecco,7 - 12 years,15-30 Lacs P.A.,['Bengaluru'],"Position : Senior Azure Data Engineer (Only Immediate Joiner)\nLocation : Bangalore\nMode of Work : Work from Office\nExperience : 7 years relevant experience\nJob Type : Full Time (On Roll)\n\nJob Description\nRoles and Responsibilities:\nThe Data Engineer will work on data engineering projects for various business units, focusing on delivery of complex data management solutions by leveraging industry best practices. They work with the project team to build the most efficient data pipelines and data management solutions that make data easily available for consuming applications and analytical solutions. A Data engineer is expected to possess strong technical skills.\nKey Characteristics\nTechnology champion who constantly pursues skill enhancement and has inherent curiosity to understand work from multiple dimensions.\nInterest and passion in Big Data technologies and appreciates the value that can be brought in with an effective data management solution.\nHas worked on real data challenges and handled high volume, velocity, and variety of data.\nExcellent analytical & problem-solving skills, willingness to take ownership and resolve technical challenges.\nContributes to community building initiatives like CoE, CoP.\nMandatory skills:\nAzure - Master\nELT - Skill\nData Modeling - Skill\nData Integration & Ingestion - Skill\nData Manipulation and Processing - Skill\nGITHUB, Action, Azure DevOps - Skill\nData factory, Databricks, SQL DB, Synapse, Stream Analytics, Glue, Airflow, Kinesis, Redshift, SonarQube, PyTest - Skill\nOptional skills:\nExperience in project management, running a scrum team.\nExperience working with BPC, Planning.\nExposure to working with external technical ecosystem.\nMKDocs documentation\n\nInterested candidates kindly share your CV and below details to usha.sundar@adecco.com\n1) Present CTC (Fixed + VP) -\n2) Expected CTC -\n3) No. of years experience -\n4) Notice Period -\n5) Offer-in hand -\n6) Reason of Change -\n7) Present Location -",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Pyspark', 'SCALA', 'Azure Data Lake', 'Databricks', 'Stream Analytics', 'Azure Data Engineering', 'Azure Databricks', 'Data factory', 'Streaming data', 'Data Bricks', 'SQL']",2025-06-11 05:25:52
Sr Data Engineer - Remote,Teamplus Staffing Solution,5 - 10 years,12-18 Lacs P.A.,"['Pune', 'Bengaluru', 'Delhi / NCR']","SQL, SNOWFLAKE, TABLEAU\nSQL, SNOWFLAKE,DBT, Datawarehousing\nSQL, SNOWFLAKE, Python, DBT, Datawarehousing\nSQL, SNOWFLAKE, Datawarehousing, any ETL tool(preffered is Matillion)\nSQL, SNOWFLAKE, TABLEAU",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Matillion', 'SNOWFLAKE', 'Data Warehousing', 'TABLEAU', 'SQL', 'dbt', 'ETL', 'Python']",2025-06-11 05:25:54
Data Governance Engineer,TELUS Digital,5 - 8 years,8-16 Lacs P.A.,"['Gandhinagar', 'Ahmedabad']","About TELUS Digital\nTELUS Digital (NYSE and TSX: TIXT) designs, builds, and delivers next-generation digital\nsolutions to enhance the customer experience (CX) for global and disruptive brands. The\ncompanys services support the full lifecycle of its clients digital transformation journeys and\nenable them to more quickly embrace next-generation digital technologies to deliver better\nbusiness outcomes. TELUS Digitals integrated solutions and capabilities span digital strategy,\ninnovation, consulting and design, digital transformation and IT lifecycle solutions, data\nannotation, and intelligent automation, and omnichannel CX solutions that include content",,,,"['Mdm Informatica', 'collibra', 'Data Lineage', 'Informatica', 'Data Governance', 'Metadata Management', 'Data Catalog', 'Data Maintenance', 'Governance', 'Data Governance Analyst', 'sql', 'MDM', 'Data Warehousing', 'Data Analytics', 'Python']",2025-06-11 05:25:55
"Team Manager (L4), Ring Data Engineering Services",Amazon,1 - 5 years,Not Disclosed,['Hyderabad'],"Amazon is an E-commerce and Cloud Computing company with headquarters in Seattle, Washington. Since 1995, Amazon has focused on being the world s most customer centric company.\n\n\nAs Team Manager, you will be responsible for :\nManaging a team of ML Data Associates.\nExecutes plans for the team to handle multiple ML queues within a defined process area.\nIdentifies risks and ensures proper escalation; maintains confidentiality and compliance.\nResponsible for meeting SLAs and task completion targets for specific queues within capacity limits.\nCollaborates with internal and external teams to achieve business goals.\nAnalyzes data to highlight trends and gaps; reports key metrics.\nPresents data during business meetings and reviews.\nDesigns and implements process improvement projects affecting team performance; participates in new workflow rollout projects.\nProvides guidance on work types and prioritization; manages straightforward objectives as well as ad hoc requests.\nCreates and implements performance improvement plans for associates; offers regular coaching and feedback on quality, performance, behavior, and career development.\nManages team performance based on metrics and deliverables.\nHolds team members accountable for performance, adherence to rules, and guidelines.\nSupport hiring and training of new Associates\n\nA day in the life\nAs Team Manager, you will be responsible to :\nManage monitor performance on job or queue adherence, volume, and quality\nSupport hiring and training of new Associates\nEnsure productivity is maximized through supervision, training, analysis, and feedback of performance data on a periodic basis\nTrack quality and utilization metrics\nProvide regular, formal informal feedback to direct reports\nIdentify and help implement process-related improvement using methodologies\nCommunicate effectively\n\nAbout the team\nArtificial General Intelligence Data Services (AGI DS) mission is to provide high-quality labelled data at high-speed and low-cost for machine learning (ML) technologies. Bachelor Degree (Any Stream) or advanced college education or experience in a Leadership or related position with management.\nProficiency in verbal and written communication skills\nExperience in understanding performance metrics and developing them to measure progress against key performance indicators\nOverall 4+ yrs of work experience out of which, min 1+ yrs of people management experience\nMust have driven process improvements in the current role Experience with process improvement/quality control tools and methods\nDemonstrated ability to lead diverse talent within a team, work cross-functionally, and build consensus on difficult issues\nExcellent communication, strong organizational skills and very detail-oriented\nStrong interest in hiring and developing people in their respective roles\nLeadership experience in coaching and performance management\nExperience in managing process and operational escalations\nExperience with aspects of speech and language technology",,,,"['Engineering services', 'Career development', 'Cloud computing', 'Performance management', 'Process improvement', 'Machine learning', 'Training analysis', 'Workflow', 'Operations', 'Performance improvement']",2025-06-11 05:25:57
Amazing Hiring For GCP Data Engineer,HTC Global Services,4 - 9 years,5-15 Lacs P.A.,['Chennai( Chennai Central RS )'],"Hello Everyone,\n\nGreetings from HTC!\nPosition Description:\nWe're seeking a Data Engineer to lead our India-based supplier delivery team in migrating eight Teradata databases to the Google Cloud Platform (GCP). Oversee the entire migration process, ensuring successful data ingestion, quality assurance, and data protection using standard engineering patterns. Responsibilities: Lead the lift and refactor efforts for full data migration. Drive business adoption of the new GCP platform. Oversee the decommissioning of Teradata and related technologies. Collaborate with technical leads, program managers, and other stakeholders to execute the migration plan within an Agile framework. Qualifications: Hands-on experience with GCP services, data pipelines, BigQuery, SQL, and Python. Proven ability to manage technical and process-related requests to maintain project timelines. Strong collaboration and communication skills",,,,"['Bigquery', 'Gcp Cloud', 'Cloud Sql', 'Data Flow', 'Dataproc', 'Python']",2025-06-11 05:25:58
"Senior Data Engineer (Exp into Azure Databricks,Pyspark, SQL)",Adecco India,7 - 12 years,22.5-30 Lacs P.A.,"['Pune', 'Bengaluru']","Job Role & responsibilities:-\n\nUnderstanding operational needs by collaborating with specialized teams\nSupporting key business operations. This involves architecture designing, building and deploying data systems, pipelines etc\nDesigning and implementing agile, scalable, and cost efficiency solution on cloud data services.\nLead a team of developers, implement Sprint planning and executions to ensure timely deliveries\n\nTechnical Skill, Qualification & experience required:-\n\n7-10 years of experience in Azure Cloud Data Engineering, Azure Databricks, datafactory , Pyspark, SQL,Python\nHands on experience in Data Engineer, Azure Databricks, Data factory, Pyspark, SQL\nProficient in Cloud Services Azure\nArchitect and implement ETL and data movement solutions.\nMigrate data from traditional database systems to Cloud environment\nStrong hands-on experience for working with Streaming dataset\nBuilding Complex Notebook in Databricks to achieve business Transformations.\nHands-on Expertise in Data Refinement using Pyspark and Spark SQL\nFamiliarity with building dataset using Scala.\nFamiliarity with tools such as Jira and GitHub\nExperience leading agile scrum, sprint planning and review sessions\nGood communication and interpersonal skills\nComfortable working in a multidisciplinary team within a fast-paced environment\n\n* Immediate Joiners will be preferred only",Industry Type: Insurance,Department: Other,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineer', 'Azure data engineer', 'Data Bricks', 'SQL', 'Data Engineering', 'Python']",2025-06-11 05:26:00
Data Engineering Consultant- ETL/pyspark/SQL,Optum,6 - 11 years,Not Disclosed,['Hyderabad'],"Primary Responsibilities:\nDesign, code, test, document, and maintain high-quality and scalable data pipelines/solutions in cloud \nWork in both dev and ops and should be open to work in ops with flexible timings in ops\nIngest and transform data using variety of technologies from variety of sources (APIs, streaming, Files, Databases)\nDevelop reusable patterns and encourage innovation that will increase team’s velocity\nDesign and develop applications in an agile environment, deploy using CI/CD\nParticipate with prototypes as well as design and code reviews, own or assist with incident and problem management\nSelf-starter who can learn things quickly, who is enthusiastic and actively engaged\nComply with the terms and conditions of the employment contract, company policies and procedures, and any and all directives (such as, but not limited to, transfer and/or re-assignment to different work locations, change in teams and/or work shifts, policies in regards to flexibility of work benefits and/or work environment, alternative work arrangements, and other decisions that may arise due to the changing business environment). The Company may adopt, vary or rescind these policies and directives in its absolute discretion and without any limitation (implied or otherwise) on its ability to do so\n\nRequired Qualifications:\nBachelor's degree in technical domain.\nRequired experience with the following:\nDatabricks, Python, Spark, pyspark, SQL, Azure Data factory\nDesign and Implementation of Datawarehouse/Datalake (Databricks/snowflake)\nData architecture, Data modelling\nOperations Processes, reporting from operations, Incident resolutions\nGithub actions/Jenkins or similar CICD tool, Cloud CICD, GitHub\nNoSQL and relational databases\nPreferred Qualifications:\nExperience or knowledge in Apache Kafka\nExperience or knowledge in Data ingestions from variety of API’s\nWorking in Agile/Scrum environment",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Pyspark', 'ETL', 'Data Bricks', 'SQL']",2025-06-11 05:26:01
Gcp Data Engineer,PwC India,5 - 8 years,Not Disclosed,"['Navi Mumbai', 'Gurugram', 'Mumbai (All Areas)']","Job Description (GCP Data Engineer) :-\n\nGCP Developer 5to 9 years of experience\nLocation : Mumbai and Gurugram\n\ndesigning, building and deploying cloud solution for enterprise applications, with expertise in Cloud Platform Engineering.",,,,"['GCP', 'Bigquery', 'Kubernetes']",2025-06-11 05:26:03
Technical Lead - Sr. Data Engineer,Edgematics Consulting,8 - 13 years,Not Disclosed,['Pune'],"About This Role :\n\nWe are looking for a talented and experienced Data Engineer with Tech Lead with hands-on expertise in any ETL Tool with full knowledge about CI/CD practices with leading a team technically more than 5 and client facing and create Data Engineering, Data Quality frameworks. As a tech lead must ensure to build ETL jobs, Data Quality Jobs, Big Data Jobs performed performance optimization by understanding the requirements, create re-usable assets and able to perform production deployment and preferably worked in DWH appliances Snowflake / redshift / Synapse\n\nResponsibilities\nWork with a team of engineers in designing, developing, and maintaining scalable and efficient data solutions using Any Data Integration (any ETL tool like Talend / Informatica) and any Big Data technologies.\nDesign, develop, and maintain end-to-end data pipelines using Any ETL Data Integration (any ETL tool like Talend / Informatica) to ingest, process, and transform large volumes of data from heterogeneous sources.\nHave good experience in designing cloud pipelines using Azure Data Factory or AWS Glues/Lambda.\nImplemented Data Integration end to end with any ETL technologies.\nImplement database solutions for storing, processing, and querying large volumes of structured and unstructured and semi-structured data\nImplement Job Migrations of ETL Jobs from Older versions to New versions.\nImplement and write advanced SQL scripts in SQL Database at medium to expert level. \nWork with technical team with client and provide guidance during technical challenges.\nIntegrate and optimize data flows between various databases, data warehouses, and Big Data platforms.\nCollaborate with cross-functional teams to gather data requirements and translate them into scalable and efficient data solutions.\nOptimize ETL, Data Load performance, scalability, and cost-effectiveness through optimization techniques.\nInteract with Client on a daily basis and provide technical progress and respond to technical questions.\nImplement best practices for data integration.\nImplement complex ETL data pipelines or similar frameworks to process and analyze massive datasets.\nEnsure data quality, reliability, and security across all stages of the data pipeline.\nTroubleshoot and debug data-related issues in production systems and provide timely resolution.\nStay current with emerging technologies and industry trends in data engineering technologies, CI/CD, and incorporate them into our data architecture and processes.\nOptimize data processing workflows and infrastructure for performance, scalability, and cost-effectiveness.\nProvide technical guidance and foster a culture of continuous learning and improvement.\nImplement and automate CI/CD pipelines for data engineering workflows, including testing, deployment, and monitoring.\nPerform migration to production deployment from lower environments, test & validate\n\nMust Have Skills\nMust be certified in any ETL tools, Database, Cloud.(Snowflake certified is more preferred)\nMust have implemented at least 3 end-to-end projects in Data Engineering.\nMust have worked on performance management optimization and tuning for data loads, data processes, data transformation in big data\nMust be flexible to write code using JAVA/Scala/Python etc. as required\nMust have implemented CI/CD pipelines using tools like Jenkins, GitLab CI, or AWS CodePipeline.\nMust have managed a team technically of min 5 members and guided the team technically.\nMust have the Technical Ownership capability of Data Engineering delivery.\nStrong communication capabilities with client facing.\nBachelor's or Master's degree in Computer Science, Engineering, or a related field.\n5 years of experience in software engineering or a related role, with a strong focus on Any ETL Tool, database, integration.\nProficiency in Any ETL tools like Talend , Informatica etc for Data Integration for building and orchestrating data pipelines.\nHands-on experience with relational databases such as MySQL, PostgreSQL, or Oracle, and NoSQL databases such as MongoDB, Cassandra, or Redis.\nSolid understanding of database design principles, data modeling, and SQL query optimization.\nExperience with data warehousing, Data Lake , Delta Lake concepts and technologies, data modeling, and relational databases.",Industry Type: IT Services & Consulting,"Department: Production, Manufacturing & Engineering","Employment Type: Full Time, Permanent","['Data Engineering', 'Snowflake', 'ETL', 'Azure Aws', 'Data Management', 'Big Data', 'Ci/Cd', 'Data Integration', 'Data Quality', 'Data Pipeline', 'Data Warehousing', 'Data Modeling', 'Data Governance']",2025-06-11 05:26:04
Lead AWS Glue Data Engineer,Allegis Group,8 - 13 years,Not Disclosed,[],"Lead AWS Glue Data Engineer\nJob Location : Hyderabad / Bangalore / Chennai / Noida/ Gurgaon / Pune / Indore / Mumbai/ Kolkata\n\nWe are seeking a skilled Lead AWS Data Engineer with 8+ years of strong programming and SQL skills to join our team. The ideal candidate will have hands-on experience with AWS Data Analytics services and a basic understanding of general AWS services. Additionally, prior experience with Oracle and Postgres databases and secondary skills in Python and Azure DevOps will be an advantage.\n\nKey Responsibilities:\nDesign, develop, and optimize data pipelines using AWS Data Analytics services such as RDS, DMS, Glue, Lambda, Redshift, and Athena.\nImplement data migration and transformation processes using AWS DMS and Glue.\nWork with SQL (Oracle & Postgres) to query, manipulate, and analyse large datasets.\nDevelop and maintain ETL/ELT workflows for data ingestion and transformation.\nUtilize AWS services like S3, IAM, CloudWatch, and VPC to ensure secure and efficient data operations.\nWrite clean and efficient Python scripts for automation and data processing.\nCollaborate with DevOps teams using Azure DevOps for CI/CD pipelines and infrastructure management.\nMonitor and troubleshoot data workflows to ensure high availability and performance.\n\nPreferred Qualifications:\nAWS certifications in Data Analytics, Solutions Architect, or DevOps.\nExperience with data warehousing concepts and data lake implementations.\nHands-on experience with Infrastructure as Code (IaC) tools like Terraform or CloudFormation.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['RDS', 'Glue', 'DMS', 'Lambda', 'Redshift', 'Athena']",2025-06-11 05:26:06
Senior Data Engineer,Expian Technologies,5 - 10 years,Not Disclosed,['Bengaluru( MG Road )'],"Role & responsibilities\nCollaborate with cross-functional teams to understand data requirements and design scalable and efficient data processing solutions.\nDevelop and maintain data pipelines using PySpark and SQL on the Databricks platform.\nOptimize and tune data processing jobs for performance and reliability.\nImplement automated testing and monitoring processes to ensure data quality and reliability.\nWork closely with data scientists, data analysts, and other stakeholders to understand their data needs and provide effective solutions.\nTroubleshoot and resolve data-related issues, including performance bottlenecks and data quality problems.\nStay up to date with industry trends and best practices in data engineering and Databricks.\n\nPreferred candidate profile\n5+ years of experience as a Data Engineer, with a focus on Databricks and cloud-based data platforms with a minimum of 2 years of experience in writing unit/end-to-end tests for data pipelines and ETL processes on Databricks.\nHands-on experience in PySpark programming for data manipulation, transformation, and analysis.\nStrong experience in SQL and writing complex queries for data retrieval and manipulation.\nExperience in developing and implementing test cases for data processing pipelines using a test-driven development approach.\nExperience in Docker for containerising and deploying data engineering applications is good to have.\nExperience in the scripting language Python is mandatory.\nStrong knowledge of Databricks platform and its components, including Databricks notebooks, clusters, and jobs.\nExperience in designing and implementing data models to support analytical and reporting needs will be an added advantage.\nStrong Knowledge of Azure Data Factory for Data orchestration, ETL workflows, and data integration is good to have.\nGood to have knowledge of cloud-based storage such as Amazon S3 and Azure Blob Storage.\nBachelor's or Master's degree in Computer Science, Engineering, or a related field.\nStrong analytical and problem-solving skills.\nStrong English communication skills, both written and spoken, are crucial.\nCapability to solve complex technical issues and comprehend risks prior to the circumstance.",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pytest', 'databricks', 'pyspark', 'Data Modeling', 'SQL']",2025-06-11 05:26:07
Data Engineer Oracle Analytics Cloud & OCI Big Data,Malomatia,7 - 12 years,25-35 Lacs P.A.,['Pune'],"Job Title: Data Engineer Oracle Analytics Cloud & OCI Big Data\nLocation: Remote / Contract\nExperience Level: Mid to Senior (6+ years)\nJob Summary:\nWe are seeking a skilled and experienced Data Engineer with a minimum of 5 years of hands-on experience in building and optimizing data solutions on Oracle Analytics Cloud (OAC), OCI Data Flow, and OCI Big Data platforms. The ideal candidate will play a critical role in designing, developing, and maintaining scalable data pipelines and analytics solutions that support data lakehouse, enterprise reporting and advanced data analytics use cases.\nKey Responsibilities:\nDesign and implement robust data ingestion and transformation pipelines using OCI Data Flow and Big Data services.\nDevelop data models, datasets, and reports within Oracle Analytics Cloud to support business intelligence and analytics needs.\nIntegrate data from multiple sources (structured and unstructured) into cloud-based data platforms using Oracle-native tools and services.\nEnsure data quality and consistency through standardized processes and validation routines.\nOptimize performance of ETL/ELT jobs and troubleshoot data-related issues in Oracle Cloud Infrastructure environments.\nMaintain data platform infrastructure using OCI-native services and automation tools.\nDocument technical processes, data models, and data pipeline architecture.\nRequired Qualifications:\nBachelor’s in Computer Science, Information Systems, Engineering, or a related field.\nMinimum 5 years of hands-on experience in data engineering, with proven expertise in Oracle Analytics Cloud (OAC), OCI Data Flow, and OCI Big Data.\nExperience with Oracle Autonomous Data Warehouse and Data Lakehouse architecture.\nStrong understanding of Oracle Cloud Infrastructure (OCI) and related data services.\nExperience with Spark, PySpark, Hive, and Big Data ecosystems.\nSolid experience in SQL, PL/SQL, and data wrangling using scripting languages (e.g., Python).\nExperience in ETL/ELT development, performance tuning, and data pipeline optimization.\nFamiliarity with data security, privacy, and governance best practices in cloud environments.\nExcellent problem-solving skills and ability to work in a fast-paced, team-oriented environment.\nKnowledge of DevOps practices and CI/CD tools in the context of data platform operations.\nExposure to data cataloging and metadata management tools.\nMust have:\n5+ years hands on data engineering experience using Oracle Analytics Cloud, OCI Data Flow, OCI Big Data, Oracle Autonomous Data Warehouse\nKnowledge of DevOps practices and CI/CD\nCertifications (Nice to Have):\nOracle Cloud Infrastructure (OCI) Data Engineer or Architect Certification\nOracle Analytics Cloud Certification",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Oac', 'OCI Data Flow', 'OCI Big Data']",2025-06-11 05:26:09
Data Engineer - SAS Migration,Crisil,2 - 4 years,Not Disclosed,['Mumbai'],"The SAS to Databricks Migration Developer will be responsible for migrating existing SAS code, data processes, and workflows to the Databricks platform. This role requires expertise in both SAS and Databricks, with a focus on converting SAS logic into scalable PySpark and Python code. The developer will design, implement, and optimize data pipelines, ensuring seamless integration and functionality within the Databricks environment. Collaboration with various teams is essential to understand data requirements and deliver solutions that meet business needs",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'scala', 'pyspark', 'data warehousing', 'data migration', 'sql', 'spark', 'gcp', 'mysql', 'hadoop', 'bigquery', 'big data', 'etl', 'python', 'sas', 'teradata', 'airflow', 'microsoft azure', 'data engineering', 'sql server', 'dataproc', 'data bricks', 'cloud data flow', 'kafka', 'migration', 'sqoop', 'data flow']",2025-06-11 05:26:11
Collibra Data Governance Engineer,Allegis Group,6 - 11 years,Not Disclosed,[],"Collibra Data Governance Engineer\nJob Location : Hyderabad / Bangalore / Chennai / Noida/ Gurgaon / Pune / Indore / Mumbai/ Kolkata\nRequired Skills\n5+ years of experience in data governance and/or metadata management.\nHands-on experience with Collibra Data Governance Center (Collibra DGC), including workflow configuration, cataloging, and operating model customization.\nStrong knowledge of metadata management, data lineage, and data quality principles.\nHands-on experience with Snowflake\nFamiliarity with data integration tools and AWS cloud platform\nExperience with SQL and working knowledge of relational databases.\nUnderstanding of data privacy regulations (e.g., GDPR, CCPA) and compliance frameworks.\nPreferred Skills\nCertifications such as Collibra Certified Solution Architect.\nExperience integrating Collibra with tools like Snowflake, Tableau or other BI/analytics platforms.\nExposure to DataOps, MDM (Master Data Management), and data governance frameworks like DAMA-DMBOK.\nStrong communication and stakeholder management skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Collibra', 'Metadata', 'Data Governance']",2025-06-11 05:26:12
Senior Data Engineer,Client of Hiresquad Resources,5 - 8 years,22.5-30 Lacs P.A.,"['Noida', 'Hyderabad', 'Bengaluru']","Role: Data Engineer\nExp: 5 to 8 Years\nLocation: Bangalore, Noida, and Hyderabad (Hybrid, weekly 2 Days office must)\nNP: Immediate to 15 Days (Try to find only immediate joiners)\n\n\nNote:\nCandidate must have experience in Python, Kafka Streams, Pyspark, and Azure Databricks.\nNot looking for candidates who have only Exp in Pyspark and not in Python.\n\n\nJob Title: SSE Kafka, Python, and Azure Databricks (Healthcare Data Project)\nExperience:  5 to 8 years\n\nRole Overview:\nWe are looking for a highly skilled with expertise in Kafka, Python, and Azure Databricks (preferred) to drive our healthcare data engineering projects. The ideal candidate will have deep experience in real-time data streaming, cloud-based data platforms, and large-scale data processing. This role requires strong technical leadership, problem-solving abilities, and the ability to collaborate with cross-functional teams.\n\nKey Responsibilities:\nLead the design, development, and implementation of real-time data pipelines using Kafka, Python, and Azure Databricks.\nArchitect scalable data streaming and processing solutions to support healthcare data workflows.\nDevelop, optimize, and maintain ETL/ELT pipelines for structured and unstructured healthcare data.\nEnsure data integrity, security, and compliance with healthcare regulations (HIPAA, HITRUST, etc.).\nCollaborate with data engineers, analysts, and business stakeholders to understand requirements and translate them into technical solutions.\nTroubleshoot and optimize Kafka streaming applications, Python scripts, and Databricks workflows.\nMentor junior engineers, conduct code reviews, and ensure best practices in data engineering.\nStay updated with the latest cloud technologies, big data frameworks, and industry trends.\n\n\nRequired Skills & Qualifications:\n4+ years of experience in data engineering, with strong proficiency in Kafka and Python.\nExpertise in Kafka Streams, Kafka Connect, and Schema Registry for real-time data processing.\nExperience with Azure Databricks (or willingness to learn and adopt it quickly).\nHands-on experience with cloud platforms (Azure preferred, AWS or GCP is a plus).\nProficiency in SQL, NoSQL databases, and data modeling for big data processing.\nKnowledge of containerization (Docker, Kubernetes) and CI/CD pipelines for data applications.\nExperience working with healthcare data (EHR, claims, HL7, FHIR, etc.) is a plus.\nStrong analytical skills, problem-solving mindset, and ability to lead complex data projects.\nExcellent communication and stakeholder management skills.\n\n\n\nEmail: Sam@hiresquad.in",Industry Type: Medical Services / Hospital,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Kafka', 'Azure Databricks', 'Python', 'Etl Pipelines', 'Data Engineering', 'Data Streaming', 'Healthcare Data', 'python scripts', 'schema registry', 'SQL Database', 'Nosql Databases', 'Kafka Streams', 'kafka connect']",2025-06-11 05:26:14
Senior AWS Data Engineer,Genspark,5 - 10 years,Not Disclosed,"['Chennai', 'Coimbatore', 'Bengaluru']","Job Summary:\nWe are looking for a highly skilled Senior AWS Data Engineer to design, develop, and lead enterprise-grade data solutions on the AWS cloud. This position requires a blend of deep AWS technical proficiency, hands-on PySpark experience, and the ability to engage with business stakeholders in solution design. The ideal candidate will build scalable, secure, and high-performance data platforms using AWS-native tools and best practices.\n\nRole & responsibilities:\nDesign and implement scalable AWS cloud-native data architectures, including data lakes, warehouses, and streaming pipelines\nDevelop ETL/ELT pipelines using AWS Glue (PySpark/Scala), Lambda, and Step Functions\nOptimize Redshift-based data warehouses including schema design, data distribution, and materialized views\nLeverage Athena, Glue Data Catalog, and S3 for efficient serverless query patterns\nImplement IAM-based data access control, lineage tracking, and encryption for secure data workflows\nAutomate infrastructure and data deployments using CDK, Terraform, or CloudFormation\nDrive data modelling standards (Star/Snowflake, 3NF, Data Vault) and ensure data quality and governance\nCollaborate with data scientists, DevOps, and business stakeholders to deliver end-to-end data solutions\nMentor junior engineers and lead code reviews and architecture discussions\nParticipate in client-facing activities including requirements gathering, technical proposal preparation, and solution demos\n\nMust-Have Qualifications:\nAWS Expertise: Proven hands-on experience with AWS Glue, Redshift, Athena, S3, Lake Formation, Kinesis, Lambda, Step Functions, EMR, and Cloud Watch\nPySpark & Big Data: Minimum 2 years of hands-on PySpark/Spark experience for large-scale data processing\nETL/ELT Engineering:Expertise in Python, dbt, or similar automation frameworks\nData Modelling: Proficiency in designing and implementing normalized and dimensional models\nPerformance Optimization:Ability to tune Spark jobs with custom partitioning, broadcast joins, and memory management\nCI/CD & Automation: Experience with GitHub Actions, Code Pipeline, or similar tools\nConsulting & Pre-sales: Prior exposure to client-facing roles including proposal drafting and cost estimation\nGood-to-Have Skills:\nKnowledge of Iceberg, Hudi, or Delta Lake file formats\nExperience with Athena Federated Queries and AWS OpenSearch\nFamiliarity with Data Zone, Data Brew, and data profiling tools\nUnderstanding of compliance frameworks like GDPR, HIPAA, SOC2\nBI integration skills using Power BI, Quick Sight, or Tableau\nKnowledge of event-driven architectures (e.g., Kinesis, MSK, Lambda)\nExposure to lake house or data mesh architectures\nExperience with Lucid chart, Miro, or other documentation/storyboarding tools\n\nWhy Join Us?\nWork on cutting-edge AWS data platforms\nCollaborate with a high-performing team of engineers and architects\nOpportunity to lead key client engagements and shape large-scale solutions\nFlexible work environment and strong learning culture",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Pyspark', 'Big Data', 'AWS Expertise', 'Data Modeling', 'ETL/ELT Engineering', 'Automation', 'CI/CD']",2025-06-11 05:26:16
Senior AWS Data Engineer,Sightspectrum,4 - 7 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Must-Have Qualifications:\nAWS Expertise: Strong hands-on experience with AWS data services including Glue, Redshift, Athena, S3, Lake Formation, Kinesis, Lambda, Step Functions, EMR, and CloudWatch.\nETL/ELT Engineering: Deep proficiency in designing robust ETL/ELT pipelines with AWS Glue (PySpark/Scala), Python, dbt, or other automation frameworks.\nData Modeling: Advanced knowledge of dimensional (Star/Snowflake) and normalised data modeling, optimised for Redshift and S3-based lakehouses.\nProgramming Skills: Proficient in Python, SQL, and PySpark, with automation and scripting skills for data workflows.\nArchitecture Leadership: Demonstrated experience leading large-scale AWS data engineering projects across teams and domains.\nPre-sales & Consulting: Proven experience working with clients, responding to technical RFPs, and designing cloud-native data solutions.\nAdvanced PySpark Expertise: Deep hands-on experience in writing optimized PySpark code for distributed data processing, including transformation pipelines using DataFrames, RDDs, and Spark SQL, with a strong grasp of lazy evaluation, catalyst optimizer, and Tungsten execution engine.\nPerformance Tuning & Partitioning: Proven ability to debug and optimize Spark jobs through custom partitioning strategies, broadcast joins, caching, and checkpointing, with proficiency in tuning executor memory, shuffle configurations, and leveraging Spark UI for performance diagnostics in large-scale data workloads (>TB scale).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'AWS Data Engineer', 'SQL', 'ETL', 'Python', 'Airflow', 'Shell Scripting', 'Elt', 'S', 'Glue', 'Amazon Redshift', 'Redshift Aws', 'AWS', 'Athena']",2025-06-11 05:26:17
Sr. Azure Data Engineer,Tech Mahindra,5 - 8 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","Roles and Responsibilities :\nDesign, develop, and maintain large-scale data pipelines using Azure Data Factory (ADF) to extract, transform, and load data from various sources into Azure Databricks.\nCollaborate with cross-functional teams to understand business requirements and design scalable solutions for big data processing using PySpark on Azure Databricks.\nDevelop complex SQL queries to optimize database performance and troubleshoot issues in Azure SQL databases.\nEnsure high availability of critical systems by implementing monitoring tools such as Prometheus and Grafana.\nJob Requirements :\nExperience in designing and developing large-scale data pipelines using ADF or similar technologies.\nStrong proficiency in Python programming language with experience working with libraries like Pandas, NumPy, etc.\nExperience working with Azure Databricks platform including creating clusters, managing workloads, and optimizing resource utilization.\nProficiency in writing complex SQL queries for querying relational databases.",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Pyspark', 'Azure Data Lake', 'Python', 'SQL']",2025-06-11 05:26:18
Data Engineer Intern,Infoweave,6 months duration,"20,000/month",['Bengaluru'],"DataWeave is a cutting-edge AI-powered digital commerce analytics platform that empowers retailers with competitive intelligence and equips consumer brands with digital shelf analytics on a global scale. By harnessing the power of DataWeave, retailers gain the ability to make smarter pricing and merchandising decisions, while consumer brands can optimize their digital shelf performance across key performance indicators such as share of search, content quality, price competitiveness, and stock availability.\n\nAt the heart of DataWeave's capabilities lies its state-of-the-art AI-powered proprietary technology, which aggregates and analyzes 500+ billion data points, covering over 400,000 brands, 4,000+ websites, and spanning more than 20 industry verticals.\nWe are a globally distributed team, composed of over 200 talented engineers, product managers, and eCommerce experts located across San Francisco, Seattle, Austin, and Toronto in North America, complemented by our technology-focused offices in Bangalore and Coimbatore.\n\nData Engineering and Delivery @DataWeave\nWe the Delivery / Data engineering team at DataWeave, deliver the Intelligence with actionable data to the customer. One part of the work is to write effective crawler bots to collect data over the web, which calls for reverse engineering and writing scalable python code. Other part of the job is to crunch data with our big data stack / pipeline. Underpinnings are Tooling, domain\nawareness, fast paced delivery, and pushing the envelope.\n\nHow we work?\nIt's hard to tell what we love more, problems or solutions! Every day, we choose to address some of the hardest data problems that there are. We are in the business of making sense of messy public data on the web. At serious scale! Read more on Become a DataWeaver\n\nWhat do we offer?\nSome of the most challenging data problems. Huge text and image datasets that you can play with!\nAbility to see the impact of your work and the value you're adding to our customers almost immediately.\nOpportunity to work on different problems and explore a wide variety of tools to figure out what really excites you.\nA culture of openness. Fun work environment. A flat hierarchy. Organization wide visibility. Flexible working hours.\nLearning opportunities with courses and tech conferences. Mentorship from seniors in the team.\nLast but not the least, competitive salary packages and fast paced growth opportunities.\n\nRelevant set of skills\nCandidate must have good written and oral communication skills, be a fast learner and have the ability to adapt quickly to a fast-paced development environment\nHas a strong grasp of CS fundamentals\nHands on with Python programming\nGood understanding of RDBMS / SQL\nExcellent problem-solving and analytical abilities\n\nOptional Skills\nKnowledge of building crawlers and data mining is a plus.\nWorking knowledge of open-source tools such as MySQL, Solr, Elasticsearch, Cassandra (data stores) would be a plus.\n\nRole and responsibilities\nInclined towards working in a start-up environment.\nDesign and build robust and scalable data engineering solutions for structured and unstructured data for delivering business insights, reporting and analytics.\nHelp continually improve ongoing analysis processes, optimizing or simplifying self-service support for customers\nImplement various bots Configure and write Regex to extract data from simple to complex websites.\nDevelop tools & techniques related to the various processes or data extraction automation.\nWork on data lake platform and different components in the data lake such as Hadoop, Amazon S3 etc.\nWork on SQL technologies on Hadoop such as Spark, Hive, Impala etc.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['RDBMS', 'Cs Fundamentals', 'Problem Solving', 'Python', 'SQL', 'Analytical Ability']",2025-06-11 05:26:20
Senior Data Engineer,Egon Zehnder,3 - 8 years,Not Disclosed,"['Noida', 'New Delhi', 'Gurugram']","Role & responsibilities\n\nAs a Data Engineer, you will be responsible for establishing and optimizing the flow of data throughout the organization while ensuring its security.\nExecuting end-to-end data pipeline, from designing the technical architecture, and developing the application to finally testing and implementing the proposed solution.\nThe Data Engineer is expected to help in driving the database architecture and design for Egon Zehnder large-scale, Intranet and Internet-based applications.\nThe Data Engineer should research new tools and technologies and come up with recommendations on how they can be used in Egon Zehnder applications.\nThe Data Engineer will also be expected to actively participate in design and implementation of projects bearing a high degree of technical complexity and/or scalability and performance significance.\nIdentifying data sources, both internal and external, and working out a plan for data management that is aligned with organizational data strategy.\nCollaborate cross-functionally with roles such as IT infrastructure, Digital/application development, and Legal to identify and highlight gaps and risks around Cybersecurity and Data Protection such as GDPR.\n\nExperience & Key Competencies\nEngineering Degree or equivalent.\n3+ years of experience of SQL writing and data modelling skills, with a solid understanding of data technologies including RDBMS, No-SQL databases.\nExperience with one or more of the following databases: SQL Server, MySQL, PostgreSQL, Oracle, Couch-base, Redis, Elastic Search or other NoSQL technologies.\nProven experience in building ETL/ELT pipelines, preferably from SQL to Azure services\nArchitecture experience of making ETL operations such as Medallion Architecture etc.\nFamiliarity with CI/CD practices for data pipelines and version control using Git\nExperience in integrating new/replacing vendor products in existing ecosystem.\nExperience in migrating data structured, unstructured data to Data lake\nExperience or understanding of performance engineering both at system level and database level.\nExperience or understanding of Data Governance, Data Quality, Data Issue Management.\nWork closely with the implementation teams of the various product lines to ensure that data architectural standards and best practices are being followed consistently across all Egon Zehnder applications\nEnsure compliance with all regulations, policies, and procedures.\nEscalate issues/risks pro-actively to appropriate stakeholders.\nRegularly communicate status and challenges to team members and management.\nSelf-driven with keenness to master, suggest and work with different technologies & toolsets.\nExcellent communication skills and interpersonal skills suitable for a diverse audience with ability to communicate in a positive friendly and effective manner with technical or non-technical users/customers\nExcellent and resourceful problem-solving skills, adaptable and willingness to learn.\nGood analysis skills - to be able to join the dots across multiple applications and interfaces between them.\n\n\nPreferred candidate profile\n\nExperience with one or more of the following databases: SQL Server, MySQL, PostgreSQL, Oracle, Couch-base, Redis, Elastic Search or other NoSQL technologies.\nExperience in Data factory and Data Lake technologies.\nRich Experience in data modelling techniques and creating various data models.\nExperience in Azure cloud services and architecture patterns.\nUnderstanding of RESTful APIs for data distribution.\nUnderstanding of deployment architecture and infrastructure in both on-prem and cloud hosting environments\nExcellent oral and written communication with an ability to articulate complex systems to multiple teams.\nSelf-motivation and the ability to work under minimal supervision\n\nBenefits\nBenefits which make us unique\nAt EZ, we know that great people are what makes a great firm. We value our people and offer employees a comprehensive benefits package. Learn more about what working at Egon Zehnder can mean for you!\nBenefits Highlights:\n•       5 Days working in a Fast-paced work environment\n•       Work directly with the senior management team\n•       Reward and Recognition\n•       Employee friendly policies\n•       Personal development and training\n•       Health Benefits, Accident Insurance\nPotential Growth for you!\nWe will nurture your talent in an inclusive culture that values diversity. You will be doing regular catchups with your Manager who will act as your career coach and guide you in your career goals and aspirations.\nLocation\nThe position is based at Egon Zehnders KCI office in Gurgaon, Plot no. 29, Institutional Area Sector 32.\nEZIRS Commitment to Diversity & Inclusion\nEgon Zehnder Information Research & Services (EZIRS) aims for a diverse workplace and strive to continuously lead with our firm values. We respect personal values of every individual irrespective of race, national or social origin, gender, religion, political or other opinion, disability, age and sexual orientation as warranted by basic rights enshrined in the UN Declaration of Human Rights. We believe diversity of our firm is central to the success and enables us to deliver better solutions for our clients. We are committed to creating an inclusive environment and supportive work environment, where everyone feels comfortable to be themselves and treated with dignity and respect and there is no unlawful discrimination related to employment, recruitment, training, promotion or remuneration.\nEgon Zehnder is an Equal Opportunity Employer\nEgon Zehnder provides equal employment opportunities to all applicants and employees without regard to race, color, creed, religion, sex, sexual orientation, gender identity, marital status, citizenship status, age, national origin, disability, or any other legally protected status and to affirmatively seek to advance the principles of equal employment opportunity.",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL Server', 'PostgreSQL', 'MySQL', 'Couch-base', 'Redis', 'Oracle', 'Elastic Search or other NoSQL technologies']",2025-06-11 05:26:22
Snowflake Cloud Data Engineer-Tech / POD Lead - Pan India,Pan India Opening-HYBRID,7 - 12 years,27.5-35 Lacs P.A.,"['Hyderabad', 'Bengaluru', 'india']","Job Title: Data & Analytics Cloud Platform Manager AWS & Snowflake\nJob Summary:\nWe are seeking an experienced Data & Analytics Cloud Platform Manager to oversee and optimize a Snowflake-based data platform on AWS. The ideal candidate will have deep expertise in managing operations, automation, support governance, SLI optimization, and change management to ensure reliability and performance. This role requires strong leadership skills to drive efficiency, automation, and operational excellence in a cloud-native data environment.\nKey Responsibilities:\nCloud Platform Management: Oversee and optimize the AWS-based data & analytics infrastructure, ensuring performance, security, and scalability.\nIncident Management: Lead incident resolution using structured problem management practices, ensuring minimal business impact.\nSupport Governance: Establish robust support frameworks to drive consistency in monitoring, issue resolution, and operations.\nSLI/SLO Management: Define, track, and optimize Service Level Indicators (SLIs) & Service Level Objectives (SLOs) for platform reliability.\nEnd-to-End Automation: Implement automation for incident response, monitoring, alerting, and self-healing systems to streamline support operations.\nChange Management: Manage platform updates, deployments, and data governance changes with minimal disruption.\nSecurity & Compliance: Ensure data security, access controls, and regulatory compliance across AWS and Snowflake environments.\nCollaboration & Leadership: Work closely with engineering, analytics, DevOps, and business teams to drive efficient platform operations.\nRequired Skills & Qualifications:\nExperience managing cloud-based data platforms, especially AWS & Snowflake.\nStrong expertise in Incident Management, Support Governance, and SLI/SLO optimization.\nHands-on experience in automating data operations and support processes.\nDeep understanding of AWS cloud services, Snowflake architecture, and data pipeline optimizations.\nSolid knowledge of ITIL frameworks, DevOps, and cloud-native operations.\nProficiency in SQL, Python, Spark, and data orchestration tools.\nExperience with monitoring tools like AWS CloudWatch, Snowflake Query Performance Dashboard, and Application Insights.\nExcellent problem-solving, analytical, and communication skills.\nPreferred Qualifications:\nExposure to CI/CD pipelines for data applications.\nExperience with Infrastructure-as-Code (IaC) tools such as Terraform or CloudFormation.\nKnowledge of machine learning integration for optimizing platform reliability.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Cortex', 'AWS', 'Spark', 'SQL', 'Python']",2025-06-11 05:26:23
Sr. Data Engineer,Greystar,7 - 12 years,Not Disclosed,['Mohali'],"Overview\nGreystar is looking for dedicated and hard-working individuals who want to help us continue to be the best at what we do. Today, we are the largest rental housing operator and developer in the US and one of the largest global investment management companies, delivering industry-leading services to investors, clients, and residents. We offer unrivaled professional development and career growth opportunities to our team members and look forward to welcoming you to Greystar, where our people are what make us the Global Leader in Rental Housing.",,,,"['Pyspark', 'Azure Databricks', 'SQL']",2025-06-11 05:26:25
Senior Data Engineer,Atidiv,5 - 8 years,10-17 Lacs P.A.,[],"Were Hiring! | Senior Data Engineer (Remote)\n\nLocation: Remote |\nShift: US - CST Time |\nDepartment: Data Engineering\n\nAre you a data powerhouse who thrives on solving complex data challenges? Do you love working with Python, AWS, and cutting-edge data tools? If yes, Atidiv wants YOU!",,,,"['SQL', 'Snowflake', 'Python', 'Airflow', 'Pyspark', 'Kafka', 'Lamda', 'Ci/Cd', 'EMR', 'Aws Glue', 'Lambda Aws', 'Cd Tools', 'Glue', 'Kinesis', 'Redshift Aws', 'DBT', 'aws']",2025-06-11 05:26:27
Senior Data Engineer,A leading Bank of India,8 - 13 years,Not Disclosed,['Mumbai (All Areas)'],"Role & responsibilities:\nDesign, optimize, and manage complex SQL queries across large Oracle databases.\nBuild and maintain metadata layers (table schema, DDL, column descriptions, relationships).\nDevelop and fine-tune solutions for converting natural language to SQL using various python libraries or other solutions.\nBuild and integrate chatbot interfaces using Python (Streamlit, FastAPI, Flask). Preferred is Flask.\nHave handled feature building for creating dynamic query agents.\nImplement user query handling, ambiguity detection, and feedback loops.\nDesign and test proof-of-concept (PoC) solutions to solve business queries using Python.\nBuild modular components (e.g., SQL generators, data sanitizers, security layers).\nCollaborate with business and business analysts to convert ideas into working prototypes.\nImplement query safety checks to prevent injection and unauthorized data access.\nMaintain logs and audit trails for executed queries and system usage.\n\nPreferred candidate profile:\n\nStrong experience in data engineering, analytics, or backend systems.\nExpert-level SQL skills (especially Oracle SQL).\nStrong Python programming skills, including libraries like pandas, sqlalchemy,strea, flask, etc.\nExperience with one or more NLP libraries: OpenAI GPT (function calling), LangChain, HuggingFace Transformers.\nKnowledge of database metadata modeling and handling DDL for large-scale systems.\nExperience with REST APIs and building microservices in Python.\nExposure to modern LLM tools like DSPy, sqlcoder, Text2SQL, or LangChain SQL Agent.\nFamiliarity with RDBMS performance tuning and optimization strategies.\nKnowledge of visualization tools like Plotly, Dash, or Streamlit for result rendering.\nUnderstanding of RBAC and secure data access practices.",Industry Type: Banking,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Banking domain', 'Natural Language Processing', 'Large Language Model', 'Chatbot Development', 'Python', 'Tensorflow', 'Oracle SQL', 'Artificial Intelligence', 'FASTapi', 'Machine Learning', 'Deep Learning', 'Numpy', 'Scikit-Learn', 'Pytorch', 'Django', 'Pandas', 'Aiml', 'Flask']",2025-06-11 05:26:28
Senior Data Engineer,Straive,6 - 10 years,Not Disclosed,['Mumbai (All Areas)'],Senior Data Engineer\nYou will have the following responsibilities:\nDesign\nAnalyse relevant internally and externally sourced data (raw data) to generate BI and Advanced\nAnalytics datasets based on your stakeholders requirements,,,,"['AWS', 'Postgresql', 'Snowflake', 'Data Warehousing', 'ETL', 'Aws Rds Oracle']",2025-06-11 05:26:30
Senior Data Engineer,Sailpoint Technologies,4 - 9 years,Not Disclosed,['Pune'],"Want to be on a team that full of results-driven individuals who are constantly seeking to innovate? Want to make an impact? At SailPoint, our Engineering team does just that. Our engineering is where high-quality professional engineering meets individual impact. Our team creates products are built on a mature, cloud-native event-driven microservices architecture hosted in AWS.\nSailPoint is seeking a Backend Software Engineer to help build a new cloud-based SaaS identity analytics product. We are looking for well-rounded backend or full stack engineers who are passionate about building and delivering reliable, scalable microservices and infrastructure for SaaS products.\nAs one of the first members on the team, you will be integral in building this product and will be part of an agile team that is in startup mode. This is a unique opportunity to build something from scratch but have the backing of an organization that has the muscle to take it to market quickly, with a very satisfied customer base.\nResponsibilities\nDeliver efficient, maintainable data pipelines\nDeliver robust, bug free code Java based micro services\nBuild and maintain Data Analytics and Machine Learning features\nProduce designs and rough estimates, and implement features based on product requirements.\nCollaborate with peers on designs, code reviews, and testing.\nProduce unit and end-to-end tests to improve code quality and maximize code coverage for new and existing features.\nResponsible for on-call production support\nRequirements\n4+ years of professional software development experience\nStrong Python, SQL, Java experience\nGreat communication skills\nBS in Computer Science, or a related field\nComprehensive experience with object-oriented analysis and design skills\nExperience with Workflow engines\nExperience with Continuous Delivery, Source control\nExperience with Observability platforms for performance metrics collection and monitoring.\nPreferred\nStrong Experience in AirFlow, Snowflake, DBT\nExperience with ML Pipelines (SageMaker)\nExperience with Continuous Delivery\nExperience working on a Big Data/Machine Learning product\nCompensation and benefits\nExperience a Small-company Atmosphere with Big-company Benefits.\nRecharge your batteries with a flexible vacation policy and paid holidays.\nGrow with us with both technical and career growth opportunities.\nEnjoy a healthy work-life balance with flexible hours, family-friendly company events and charitable work.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Backend', 'Production support', 'Machine learning', 'Agile', 'sailpoint', 'Workflow', 'Monitoring', 'SQL', 'Python']",2025-06-11 05:26:31
Senior Data Engineer,Denodo Technologies,8 - 12 years,Not Disclosed,['Chennai'],"Denodo is a leader in data management. The award-winning Denodo Platform is the leading data integration, management, and delivery platform using a logical approach to enable self- service BI, data science, hybrid/multi-cloud data integration, and enterprise data services. Realizing more than 400% ROI and millions of dollars in benefits, Denodo s large enterprise and mid-market customers across 30+ industries have received payback in less than 6 months. For more information, visit www.denodo.com .\nWe are a fast-growing, international organization with teams across four continents and we work with a cutting-edge technology, but thats not all we have to offer. At Denodo, we are like a family and it is of the utmost importance to us that we help support your professional growth every step of the way\nJob Description\nThe Opportunity\nDenodo is always looking for technical, passionate people to join our Customer Success team. We want a professional who will travel, consult, develop, train and troubleshoot to enhance our clients journey around Data Virtualization.\nYour mission: to help people realize their full potential through accelerated adoption and productive use of Denodo solutions.\nIn this role you will successfully employ a combination of high technical expertise and client management skills to conduct on-site and off-site consulting, product implementation and solutions development in either short or long-term engagements being a critical point of contact for getting things done among Denodo, partners and client teams.\nJob Responsibilities & Duties\nAs a Sr. Data Engineer you will successfully employ a combination of high technical expertise, research and investigative know-how, troubleshooting and problem-solving techniques, and communication skills between clients and internal Denodo teams to achieve your mission.\nObtain and maintain strong knowledge of the Denodo Platform, be able to deliver a superb technical discussion, including an overview of our key and advanced features and benefits, services offerings, differentiation, and competitive positioning.\nConstantly learn new things and maintain an overview of modern technologies.\nProvide technical consulting, training and support.\nDiagnose and resolve clients inquiries related to operating Denodo software products in their environment.\nParticipate in problem escalation and call prevention activities to help clients and other technical specialists increase their efficiency when using Denodo products.\nBe able to address a majority of technical questions concerning customization, integration, enterprise architecture and general feature/functionality of our product.\nProvide timely, prioritized and complete customer-based feedback to Product Management, Sales, Support and/or Development regarding client s business cases, requirements and issues.\nTrain and engage clients in the product architecture, configuration, and use of the Denodo Platform.\nPromote knowledge and best practices while managing deliverables and timelines.\nCapable of building and/or leading the development of custom deployments based on and even beyond client s requirements.\nManage client expectations, establish credibility at all levels within the client and build problem-solving partnerships with the client, partners and colleagues.\nBe willing to travel as necessary to address or service customer needs.\nLocation\nChennai, INDIA\nFunction\nCustomer Success\nDesired Skills & Experience\nRequired Skills\n8 - 12 years of experience in SQL/ ETL / Data warehousing / Data Integration / Data Virtualization technologies.\nBS or higher degree in Computer Science.\nSolid understanding of SQL and good grasp of relational and analytical database management theory and practice.\nGood knowledge of JDBC, XML and Web Services APIs.\nExcellent verbal and written communication skills to be able to interact with technical and business counterparts.\nActive listener.\nStrong analytical and problem solving abilities.\nLots of curiosity. You never stop learning new things.\nCreativity. We love to be surprised with innovative solutions.\nWillingness to travel on occasion.\nBe a team worker with a positive attitude.\nWe Value\nPrior experience in Denodo Platform.\nDenodo Certified Administrator (9.0 / 8.0) and Denodo Certified Developer (9.0 / 8.0).\nExperience working with GIT or other version control systems.\nExperience working with modern data architecture like lakehouse\nKnowledge and experience with systems and services hosted in the main cloud vendors (AWS, Azure, GCP).\nExperience in Windows & Linux (and UNIX) operating systems in server environments.\nBusiness software implementation and integration projects (e.g. ETL/Data Warehouse architectures, CEP, BPM).\nIntegration with packaged applications (e.g. relational databases, SAP, Siebel, Oracle Financials, Business Intelligence tools, )\nExperience / Knowledge in Containerization and Orchestration\nIndustry experience in supporting mission critical software components.\nExperience in attending customer engagements and writing technical documentation.\nForeign language skills are a plus.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Unix', 'SAP', 'Siebel', 'Linux', 'XML', 'JDBC', 'Windows', 'Business intelligence', 'Virtualization', 'SQL']",2025-06-11 05:26:33
Senior Data Engineer,Go Digital Technology Consulting,3 - 8 years,Not Disclosed,"['Mumbai', 'Pune']","Role: Senior Data Engineer\nLocation: Mumbai & Pune\nExperience: 3yrs to 8yrs\n\nTechnologies / Skills: Advanced SQL, Python and associated libraries like Pandas, NumPy etc., Pyspark , Shell scripting, Data-Modelling, Big data, Hadoop, Hive, ETL pipelines.\n\nResponsibilities: • Proven success in communicating with users, other technical teams, and\nsenior management to collect requirements, describe data modeling decisions and develop data\nengineering strategy.\n• Ability to work with business owners to define key business requirements and convert to user stories with required technical specifications.\n• Communicate results and business impacts of insight initiatives to key stakeholders to\ncollaboratively solve business problems.\n• Working closely with the overall Enterprise Data & Analytics Architect and Engineering practice leads to ensure adherence with the best practices and design principles.\n• Assures quality, security and compliance requirements are met for supported area.\n• Design and create fault-tolerance data pipelines running on cluster\n• Excellent communication skills with the ability to influence client business and IT teams\n• Should have design data engineering solutions end to end. Ability to come up with scalable and\nmodular solutions.\n\nRequired Qualification:\n• 3+ years of hands-on experience Designing and developing Data Pipelines for Data Ingestion or\nTransformation using Python (PySpark)/Spark SQL in AWS cloud\n• Experience in design and development of data pipelines and processing of data at scale.\n• Advanced experience in writing and optimizing efficient SQL queries with Python and Hive handling Large Data Sets in Big-Data Environments\n• Experience in debugging, tunning and optimizing PySpark data pipelines\n• Should have implemented concepts and have good knowledge of Pyspark data frames, joins,\ncaching, memory management, partitioning, parallelism etc.\n• Understanding of Spark UI, Event Timelines, DAG, Spark config parameters, in order to tune the\nlong running data pipelines.\n• Experience working in Agile implementations\n• Experience with building data pipelines in streaming and batch mode.\n• Experience with Git and CI/CD pipelines to deploy cloud applications\n• Good knowledge of designing Hive tables with partitioning for performance.\n\nDesired Qualification:\n• Experience in data modelling.\n• Hands on creating workflows on any Scheduling Tool like Autosys, CA Workload Automation.\n• Proficiency in using SDKsfor interacting with native AWS services.\n• Strong understanding of concepts of ETL, ELT and data modeling.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Python', 'SQL', 'Hadoop', 'Big Data', 'Kafka', 'Spark', 'AWS']",2025-06-11 05:26:35
Sr Azure Data Engineer,Zealogics.com,3 - 6 years,Not Disclosed,[],"Responsibilities : Able to participate in business discussions and assist gathering data requirements. Good analytical and problem-solving skills to help address data challenges.\nProficiency in writing complex SQL queries for data extraction, transformation, and analysis. Knowledge of SQL functions, joins, subqueries, and performance tuning. Able to navigate source systems with minimal guidance to understand how data is related and use like data profiling to gain a better understanding of the data. Hands on experience with PySQL/Pyspark etc.\nHands on Experience in creating and managing data pipelines using Azure Data Factory. Understanding of data integration, transformation, and workflow orchestration in Azure environments.\nKnowledge of data engineering workflows and best practices in Databricks. Able to understand existing templates and patterns for development. Hands on experience with Unity Catalog and Databricks workflow.\nProficiency in using Git for version control and collaboration in data projects. Ability to work effectively in a team environment, especially in agile or collaborative settings.\nClear and effective communication skills to articulate findings and recommendations for other team members. Ability to document processes, workflows, and data analysis results effectively.\nWillingness to learn new tools, technologies, and techniques as the field of data analytics evolves. Being adaptable to changing project requirements and priorities.\n\nSkills\n\nAzure Databricks, Data Lakehouse architectures, and Azure Data Factory.\nExpertise in optimizing data workflows and predictive modeling.\nDesigning and implementing data pipelines using Databricks, Spark,\nExpertise in batch and streaming data solutions, automating workflows with CI/CD tools like Jenkins and Azure DevOps, and ensuring data governance with Delta Lake\nSpark, PySpark, Delta Lake, Azure DevOps, Python.\nAdvance SQL expertise",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Data analysis', 'Analytical', 'Agile', 'data governance', 'Workflow', 'Predictive modeling', 'SQL', 'Data extraction', 'Python']",2025-06-11 05:26:36
Senior Data Engineer,Tredence,4 - 8 years,Not Disclosed,['Pune'],"About Tredence:\nTredence is a global data science solutions provider founded in 2013 by Shub Bhowmick, Sumit Mehra, and Shashank Dubey focused on solving the last-mile problem in AI. Headquartered in San Jose, California, the company embraces a vertical-first approach and an outcome-driven mindset to help clients win and accelerate value realization from their analytics investments. The aim is to bridge the gap between insight delivery and value realization by providing customers with a differentiated approach to data and analytics through tailor-made solutions. Tredence is 1,800-plus employees strong with offices in San Jose, Foster City, Chicago, London, Toranto, and Bangalore, with the largest companies in retail, CPG, hi-tech, telecom, healthcare, travel, and industrials as clients.",,,,"['azure databricks', 'python', 'azure data lake', 'rdbms', 'data management', 'performance tuning', 'analytical', 'data', 'pyspark', 'data warehousing', 'azure data factory', 'data engineering', 'tools', 'artificial intelligence', 'sql', 'plsql', 'unix shell scripting', 'spark', 'etl', 'communication skills', 'agile methodology']",2025-06-11 05:26:38
Senior Data Engineer,Kryon Knowledge Works,6 - 9 years,Not Disclosed,['Hyderabad'],"We are looking for a skilled Data Engineer with hands-on experience in Airflow, Python, AWS, and Big Data technologies like Spark to join our dynamic team.\n\nKey Responsibilities\n\nDesign and implement data pipelines and workflows using Apache Airflow\nDevelop robust and scalable data processing applications using Python\nLeverage AWS services (S3, EMR, Lambda, Glue, Redshift, etc.) for data engineering and ETL pipelines\nWork with Big Data technologies like Apache Spark to process large-scale datasets\nOptimize and monitor data pipelines for performance, reliability, and scalability\nCollaborate with Data Scientists, Analysts, and Business teams to understand data needs and deliver solutions\nEnsure data quality, consistency, and governance across all data pipelines\nDocument processes, pipelines, and best practices\nMandatory Skills\n\nApache Airflow - workflow orchestration and scheduling\nPython - strong programming skills for data engineering\nAWS - hands-on experience with core AWS data services\nBig Data technologies particularly Apache Spark\n\nLocation: Hyderabad (Hybrid)\nPlease share your resume with +91 9361912009",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Airflow', 'python', 'Spark', 'AWS']",2025-06-11 05:26:39
Senior Data Engineer,TechAffinity,6 - 10 years,Not Disclosed,['Chennai'],"We are looking for a highly skilled Senior Data Engineer with strong expertise in Data Warehousing & Analytics to join our team. The ideal candidate will have extensive experience in designing and managing data solutions, advanced SQL proficiency, and hands-on expertise in Python.\nKey Responsibilities:\nDesign, develop, and maintain scalable data warehouse solutions.\nWrite and optimise complex SQL queries for data extraction, transformation, and Reporting.\nDevelop and automate data pipelines using Python.\nWork with AWS cloud services for data storage, processing, and analytics.\nCollaborate with cross-functional teams to provide data-driven insights and solutions.\nEnsure data integrity, security, and performance optimisation.\n\nRequired Skills & Experience:\n6-10 years of experience in Data Warehousing & Analytics.\nStrong proficiency in writing complex SQL queries with deep understanding of query optimization, stored procedures, and indexing.\nHands-on experience with Python for data processing and automation.\nExperience working with AWS cloud services.\nAbility to work independently and collaborate with teams across different time zones.\nGood to Have:\nExperience in the SAS domain and understanding of financial data structures.\nHands-on experience with reporting tools like Power BI or Tableau.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Power Bi', 'Tableau', 'SQL', 'Python', 'Data Warehousing Concepts', 'SAS', 'Azure Cloud', 'ETL', 'Aws Cloud Services', 'AWS', 'financial data structures.']",2025-06-11 05:26:41
Senior Data Engineer,Aventior Digital,7 - 12 years,Not Disclosed,[],"Designation: Senior Data Engineer\nPreferred Experience: 7+ years\n\nResponsibilities:\nDesign, develop, and maintain data pipelines for ingesting, processing, and transforming data from various sources into actionable insights.\nIntegrate data from disparate sources (databases, APIs, and files) into a unified data platform using ETL processes and data integration techniques.\nDesign and implement data models, schemas, and data structures to support analytical queries, reporting, and business intelligence needs.\nOptimize database performance, query execution, and data processing workflows for efficiency, scalability, and reliability.\nEnsure data quality, integrity, and consistency through data validation, cleansing, deduplication, and error-handling mechanisms.\nArchitect and implement data solutions on Azure cloud platforms, leveraging Azure services for data storage, processing, and analytics.\nImplement data security measures, encryption techniques, and access controls to protect sensitive data and ensure compliance with regulations (e.g., GDPR, HIPAA).\nWork closely with cross-functional teams, data scientists, analysts, and stakeholders to understand data requirements, provide data solutions, and communicate insights effectively.\nDocument data processes, workflows, and best practices, and promote data governance standards, data lineage, and metadata management.\nStay updated with emerging technologies, industry trends, and best practices in data engineering, cloud computing, and data analytics to drive innovation and continuous improvement.\n\nRequired Skills:\nProficiency in writing complex SQL queries, stored procedures, and functions for data extraction, transformation, and analysis.\nExperience in database design, optimization, and management using SQL Server/Azure SQL Database.\nKnowledge of data modeling techniques, including entity-relationship diagrams, dimensional modeling, and data normalization, is needed to design efficient data structures.\nFamiliarity with ETL (Extract, Transform, Load) processes and tools such as Azure Data Factory, SSIS (SQL Server Integration Services), or other data integration platforms.\nHands-on experience with Azure cloud services, including Azure SQL Database, Azure Data Factory and Azure Storage.\nExperience handling large-scale data processing and analytics with strong analytical skills and attention to detail.\nStrong statistical knowledge.\nExcellent communication skills.\n\nGood To Have Skills:\nExperience working with C#, .Net Framework\nExperience with other Azure cloud services and sharepoint\nExperience with Python as programming language\nExperience manipulating unstructured data with regular expressions.\nBasic understanding of machine learning concepts and algorithms for data mining, predictive modeling, and statistical analysis.\nKnowledge of data warehousing concepts, methodologies, and tools for building and maintaining data warehouses or data marts.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL Queries', 'Azure Data Factory', 'Data Engineering', 'ETL', 'SSIS', 'SQL']",2025-06-11 05:26:42
Senior Data Engineer,Parkar Global Technologies,6 - 10 years,Not Disclosed,['Pune'],"About Position:\nWe are looking for a Senior Data Engineer to play a key role in building, optimizing, and maintaining our Azure-based data platform, which supports IoT data processing, analytics, and AI/ML applications. As part of our Data Platform Team, you will design and develop scalable data pipelines, implement data governance frameworks, and ensure high-performance data processing to drive digital transformation across our business.\n\nResponsibilities:\nData Pipeline Development: Design, build, and maintain high-performance, scalable ETL/ELT pipelines using Azure Data Factory, Databricks, and ADLS.\nData Platform Enhancement: Contribute to the development and optimization of our Azure-based data platform, ensuring efficiency, reliability, and security.\nIoT & High-Volume Data Processing: Work with large-scale IoT and operational datasets, optimizing data ingestion, transformation, and storage.\nData Governance & Quality: Implement data governance best practices, ensuring data integrity, consistency, and compliance.\nPerformance Optimization: Improve query performance and storage efficiency for analytics and reporting use cases.\nCollaboration: Work closely with data scientists, architects, and business teams to ensure data availability and usability.\nInnovation & Automation: Identify opportunities for automation and process improvements, leveraging modern tools and technologies.\n\nRequirement:\n6+ years of experience in data engineering with a focus on Azure cloud technologies.\nStrong expertise in Azure Data Factory, Databricks, ADLS, and Power BI.\nProficiency in SQL, Python, and Spark for data processing and transformation.\nExperience with IoT data ingestion and processing, handling high-volume, real-time data streams.\nStrong understanding of data modeling, lakehouse architectures, and medallion frameworks.\nExperience in building and optimizing scalable ETL/ELT processes.\nKnowledge of data governance, security, and compliance frameworks.\nExperience with monitoring, logging, and performance tuning of data workflows.\nStrong problem-solving and analytical skills with a platform-first mindset.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Pyspark', 'Azure Databricks', 'Azure Data Lake', 'Azure Devops', 'SQL']",2025-06-11 05:26:44
Sr Data Engineer,Leading Client,1 - 3 years,Not Disclosed,['Chennai'],"Strong experience in Python\nGood experience in Databricks\nExperience working in AWS/Azure Cloud Platform.\nExperience working with REST APIs and services, messaging and event technologies.\nExperience with ETL or building Data Pipeline tools\nExperience with streaming platforms such as Kafka.\nDemonstrated experience working with large and complex data sets.\nAbility to document data pipeline architecture and design\nExperience in Airflow is nice to have\nTo build complex Deltalake",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'hive', 'cloudera', 'scala', 'amazon redshift', 'pyspark', 'data warehousing', 'sql', 'spark', 'data pipeline architecture', 'hadoop', 'big data', 'etl', 'hbase', 'rest', 'python', 'oozie', 'airflow', 'microsoft azure', 'azure cloud', 'nosql', 'data bricks', 'mapreduce', 'kafka', 'sqoop', 'aws']",2025-06-11 05:26:46
Senior Specialist- Data Engineer,Leading Client,8 - 12 years,Not Disclosed,['Pune'],"Roles & Responsibilities:\nTotal 8-10 years of working experience Experience/Needs\n8-10 Years of experience with big data tools like Spark, Kafka, Hadoop etc.\nDesign and deliver consumer-centric high performant systems.\nYou would be dealing with huge volumes of data sets arriving through batch and streaming platforms.\nYou will be responsible to build and deliver data pipelines that process, transform, integrate and enrich data to meet various demands from business\nMentor team on infrastructural, networking, data migration, monitoring and troubleshooting aspects\nFocus on automation using Infrastructure as a Code (IaaC), Jenkins, devOps etc.\nDesign, build, test and deploy streaming pipelines for data processing in real time and at scale\nExperience with stream-processing systems like Storm, Spark-Streaming, Flink etc..\nExperience with object-oriented/object function scripting languagesScala, Java, etc.\nDevelop software systems using test driven development employing CI/CD practices\nPartner with other engineers and team members to develop software that meets business needs\nFollow Agile methodology for software development and technical documentation\nGood to have banking/finance domain knowledge\nStrong written and oral communication, presentation and interpersonal skills.\nExceptional analytical, conceptual, and problem-solving abilities\nAble to prioritize and execute tasks in a high-pressure environment\nExperience working in a team-oriented, collaborative environment\n8-10 years of hand on coding experience\nProficient in Java, with a good knowledge of its ecosystems\nExperience with writing Spark code using scala language\nExperience with BigData tools like Sqoop, Hive, Pig, Hue\nSolid understanding of object-oriented programming and HDFS concepts\nFamiliar with various design and architectural patterns\nExperience with big data toolsHadoop, Spark, Kafka, fink, Hive, Sqoop etc.\nExperience with relational SQL and NoSQL databases like MySQL, PostgreSQL, Mongo dB and Cassandra\nExperience with data pipeline tools like Airflow, etc.\nExperience with AWS cloud servicesEC2, S3, EMR, RDS, Redshift, BigQuery\nExperience with stream-processing systemsStorm, Spark-Streaming, Flink etc.\nExperience with object-oriented/object function scripting languagesPython, Java, Scala, etc.\nExpertise in design / developing platform components like caching, messaging, event processing, automation, transformation and tooling frameworks\nLocation:Pune/ Mumbai/ Bangalore/ Chennai",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Senior Specialist- Data Engineer', 'hive', 'continuous integration', 'scala', 'amazon redshift', 'apache storm', 'ci/cd', 'apache pig', 'emr', 'java', 'postgresql', 'aws cloud', 'spark', 'devops', 'jenkins', 'mysql', 'bigquery', 'hadoop', 'big data', 'mongodb', 'presentation skills', 'amazon rds', 'apache flink', 'nosql', 'cassandra', 'kafka', 'sqoop']",2025-06-11 05:26:48
Senior Data engineer,Apexon,7 - 12 years,Not Disclosed,['Hyderabad'],"We enable #HumanFirstDigital\n\nJob Summary:\nWe are seeking a highly skilled Lead/Senior Data Engineer to provide L2 and L3 production support for enterprise-scale data platforms. The ideal candidate should have strong experience in Informatica PowerCenter, Informatica PowerExchange, Yellowbrick (or any RDBMS), Unix Shell Scripting, and Tivoli Workload Scheduler. You will be responsible for ensuring seamless, reliable, and efficient data processing operations.\nKey Responsibilities:\nProvide L2/L3 production support for complex ETL/data integration workflows using Informatica PowerCenter and PowerExchange.\nMonitor and troubleshoot ETL failures, data quality issues, and performance problems in batch and real-time pipelines.\nSupport and maintain data pipelines involving Yellowbrick/ Netezza or other RDBMS platforms.\nDevelop and enhance Unix Shell Scripts to automate operational and support tasks.\nManage job scheduling, dependencies, and batch cycles using Tivoli Workload Scheduler (TWS).\nPerform incident triage, root cause analysis, and implement long-term fixes.\nCollaborate with development, infrastructure, and business teams to resolve issues and drive stability.\nMaintain and update operational documentation, SOPs, and knowledge base articles.\nParticipate in on-call rotation and adhere to SLA and escalation procedures.\nRequired Skills and Experience:\n7 to 12 years of experience in Data Engineering/ETL with a strong focus on L2/L3 support.\nExpertise in Informatica PowerCenter and PowerExchange (including support for mainframe, flat files, and RDBMS sources/targets).\nStrong SQL and database experience with Yellowbrick or any other RDBMS (e.g., Netezza, Oracle, PostgreSQL, SQL Server).\nProficient in Unix/Linux Shell Scripting.\nHands-on experience with Tivoli Workload Scheduler (TWS) or equivalent job scheduling tools.\nStrong problem-solving skills for production issue analysis and resolution.\nUnderstanding of data warehousing concepts, job orchestration, and data quality principles.\nExperience with ticketing and incident management tools like ServiceNow or JIRA.\nExcellent communication, documentation, and coordination skills.\nOur Commitment to Diversity & Inclusion:\nOur Perks and Benefits:\nOur benefits and rewards program has been thoughtfully designed to recognize your skills and contributions, elevate your learning/upskilling experience and provide care and support for you and your loved ones. As an Apexon Associate, you get continuous skill-based development, opportunities for career advancement, and access to comprehensive health and well-being benefits and assistance.\nWe also offer:\no Group Health Insurance covering family of 4\no Term Insurance and Accident Insurance\no Paid Holidays & Earned Leaves\no Paid Parental LeaveoLearning & Career Development\no Employee Wellness\nJob Location : Hyderabad, India",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Linux', 'Production support', 'RDBMS', 'Shell scripting', 'Wellness', 'Informatica', 'Oracle', 'Unix shell scripting', 'SQL']",2025-06-11 05:26:49
Data Engineer - Associate,FedEx,1 - 2 years,Not Disclosed,"['Gurugram', 'Bengaluru', 'Mumbai (All Areas)']","Role & responsibilities :\n\nDevelop and maintain data workflows using Ab Initio tools.\nAnalyze data, troubleshoot issues, and resolve defects within data pipelines.\nParticipate in Agile ceremonies including daily stand-ups, sprint planning, and reviews.\nApply CI/CD practices using tools like Jenkins and work within Unix/Linux environments.\nMaintain documentation including metadata definitions, onboarding materials, and SOPs.\nContribute to modernization and cloud-readiness efforts, particularly leveraging Azure and Databricks.\n\n\n\nPreferred candidate profile\n\n1-2 years of hands-on experience in data engineering, ETL development, or data analytics.\nExperience with SQL and working knowledge of Unix/Linux systems.\nFamiliarity with ETL tools such as Ab Initio.\nAbility to analyze, debug, and optimize data workflows.\nExposure to CI/CD pipelines and version control practices.\nStrong analytical thinking, attention to detail, and documentation skills.\nComfortable working in Agile development environments",Industry Type: Courier / Logistics (Logistics Tech),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Ab Initio', 'ETL', 'Unix', 'Linux', 'Informatica', 'Data Modeling', 'Data Warehousing', 'SQL']",2025-06-11 05:26:51
Data Engineer - Analyst,FedEx,3 - 5 years,Not Disclosed,"['Gurugram', 'Bengaluru', 'Mumbai (All Areas)']","Role & responsibilities :\n\nDesign,develop, and maintain ETL workflows using Ab Initio.\nManage and support critical data pipelines and data sets across complex,high-volume environments.\nPerform data analysis and troubleshoot issues across Teradata and Oracle data sources.\nCollaborate with DevOps for CI/CD pipeline integration using Jenkins, and manage deployments in Unix/Linux environments.\nParticipate in Agile ceremonies including stand-ups, sprint planning, and roadmap discussions.\nSupport cloud migration efforts, including potential adoption of Azure,Databricks, and PySparkbased solutions.\nContribute to project documentation, metadata management (LDM, PDM), onboarding guides, and SOPs\n\n\n\nPreferred candidate profile\n\n3 years of experience in data engineering, with proven expertise in ETL development and maintenance.\nProficiency with Ab Initio tools (GDE, EME, Control Center).\nStrong SQL skills, particularly with Oracle or Teradata.\nSolid experience with Unix/Linux systems and scripting.\nFamiliarity with CI/CD pipelines using Jenkins or similar tools.\nStrong communication skills and ability to collaborate with cross-functional teams.",Industry Type: Courier / Logistics (Logistics Tech),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Ab Initio', 'Data Modeling', 'ETL', 'Cicd Pipeline', 'Informatica', 'Data Warehousing', 'Databricks', 'Teradata', 'Oracle', 'SQL']",2025-06-11 05:26:53
Staff Data Engineer - Machine Learning,Netradyne,5 - 8 years,22.5-35 Lacs P.A.,['Bengaluru'],"Role and Responsibilities:\n\nYou will be embedded within a team of machine learning engineers and data scientists; responsible for building and productizing generative AI and deep learning solutions. You will:\nDesign, develop and deploy production ready scalable solutions that utilizes GenAI, Traditional ML models, Data science and ETL pipelines\nCollaborate with cross-functional teams to integrate AI-driven solutions into business operations.\nBuild and enhance frameworks for automation, data processing, and model deployment.\nUtilize Gen-AI tools and workflows to improve the efficiency and effectiveness of AI solutions.\nConduct research and stay updated with the latest advancements in generative AI and related technologies.\nDeliver key product features within cloud analytics.\n\nRequirements:\n\nB. Tech, M. Tech or PhD in Computer Science, Data Science, Electrical Engineering, Statistics, Maths, Operations Research or related domain.\nStrong programming skills in Python, SQL and solid fundamentals in computer science, particularly in algorithms, data structures, and OOP.\nExperience with building end-to-end solutions on AWS cloud infra.\nGood understanding of internals and schema design for various data stores (RDBMS, Vector databases and NoSQL).\nExperience with Gen-AI tools and workflows, and large language models (LLMs).\nExperience with cloud platforms and deploying models at scale.\nStrong analytical and problem-solving skills with a keen attention to detail.\nStrong knowledge of statistics, probability, and estimation theory.\n\nDesired Skills:\n\nFamiliarity with frameworks such as PyTorch, TensorFlow and Hugging Face.\nExperience with data visualization tools like Tableau, Graphana, Plotly-Dash.\nExposure to AWS services like Kinesis, SQS, EKS, ASG, lambda etc.\nExpertise in at least one popular Python web-framework (like FastAPI, Django or Flask).\nExposure to quick prototyping using Streamlit, Gradio, Dash etc.\nExposure to Big Data processing (Snowflake, Redshift, HDFS, EMR)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'AWS', 'Generative Artificial Intelligence', 'Python', 'Big Data Technologies']",2025-06-11 05:26:55
Azure Data Engineer,Management Consulting,5 - 10 years,6-15 Lacs P.A.,['Bengaluru'],"Urgent Hiring _ Azure Data Engineer with a leading Management Consulting Company @ Bangalore Location.\n\nStrong expertise in Databricks & Pyspark while dealing with batch processing or live (streaming) data sources.\n4+ relevant years of experience in Databricks & Pyspark/Scala\n7+ total years of experience\nGood in data modelling and designing.\n\nCtc- Hike Shall be considered on Current/Last Drawn Pay\n\nApply - rohita.robert@adecco.com\n\nHas worked on real data challenges and handled high volume, velocity, and variety of data.\nExcellent analytical & problem-solving skills, willingness to take ownership and resolve technical challenges.\nContributes to community building initiatives like CoE, CoP.\nMandatory skills:\nAzure - Master\nELT - Skill\nData Modeling - Skill\nData Integration & Ingestion - Skill\nData Manipulation and Processing - Skill\nGITHUB, Action, Azure DevOps - Skill\nData factory, Databricks, SQL DB, Synapse, Stream Analytics, Glue, Airflow, Kinesis, Redshift, SonarQube, PyTest - Skill",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Azure Databricks', 'Airflow', 'Streaming Data', 'Elt', 'Data Bricks', 'Redshift', 'SQL', 'PyTest', 'Azure Data Engineer', 'Datafactory', 'Synapse', 'Glue', 'Stream Analytics', 'Kinesis', 'SCALA', 'SonarQube', 'Data Modeling']",2025-06-11 05:26:57
Aws Data Engineer | Gurgaon | Deloitte,Deloitte,4 - 9 years,12-22 Lacs P.A.,"['Gurugram', 'Bengaluru']","To Apply - Submit Details via Google Form - https://forms.gle/8SUxUV2cikzjvKzD9\n\nAs a Senior Consultant in our Consulting team, youll build and nurture positive working relationships with teams and clients with the intention to exceed client expectations\nSeeking experienced AWS Data Engineers to design, implement, and maintain robust data pipelines and analytics solutions using AWS services. The ideal candidate will have a strong background in AWS data services, big data technologies, and programming languages. \n\nRole & responsibilities\n1. Design and implement scalable, high-performance data pipelines using AWS services \n2. Develop and optimize ETL processes using AWS Glue, EMR, and Lambda \n3. Build and maintain data lakes using S3 and Delta Lake \n4. Create and manage analytics solutions using Amazon Athena and Redshift \n5. Design and implement database solutions using Aurora, RDS, and DynamoDB \n6. Develop serverless workflows using AWS Step Functions \n7. Write efficient and maintainable code using Python/PySpark, and SQL/PostgrSQL \n8. Ensure data quality, security, and compliance with industry standards \n9. Collaborate with data scientists and analysts to support their data needs \n10. Optimize data architecture for performance and cost-efficiency \n11. Troubleshoot and resolve data pipeline and infrastructure issues \n\nPreferred candidate profile\n1. Bachelors degree in computer science, Information Technology, or related field \n2. Relevant years of experience as a Data Engineer, with at least 60% of experience focusing on AWS \n3. Strong proficiency in AWS data services: Glue, EMR, Lambda, Athena, Redshift, S3\n4. Experience with data lake technologies, particularly Delta Lake \n5. Expertise in database systems: Aurora, RDS, DynamoDB, PostgreSQL\n6. Proficiency in Python and PySpark programming \n7. Strong SQL skills and experience with PostgreSQL\n8. Experience with AWS Step Functions for workflow orchestration \n\nTechnical Skills: \n- AWS Services: Glue, EMR, Lambda, Athena, Redshift, S3, Aurora, RDS, DynamoDB, Step Functions \n- Big Data: Hadoop, Spark, Delta Lake\n- Programming: Python, PySpark\n- Databases: SQL, PostgreSQL, NoSQL\n- Data Warehousing and Analytics \n- ETL/ELT processes \n- Data Lake architectures \n- Version control: Git \n- Agile methodologies",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Aws Data Lake', 'Etl Process', 'Python', 'Postgresql', 'Hadoop', 'Aws Emr', 'Aws Dms', 'Aurora Db', 'Aws Lambda', 'Data Pipeline', 'Redshift Aws', 'Aws Aurora', 'Hadoop Spark', 'AWS', 'Etl Pipelines', 'Pyspark', 'Aura Framework', 'Aws Glue', 'Amazon Redshift', 'Dynamo Db', 'Delta Lake', 'Nosql Databases', 'Data Lake', 'ETL', 'Athena', 'Amazon Rds']",2025-06-11 05:26:58
Data Engineer 2,Uplers,3 - 8 years,Not Disclosed,['Bengaluru'],"About the Role:\nAs a Data Engineer, you will be part of the Data Engineering team with this role being inherently multi-functional, and the ideal candidate will work with Data Scientist, Analysts, Application teams across the company, as well as all other Data Engineering squads at Wayfair. We are looking for someone with a love for data, understanding requirements clearly and the ability to iterate quickly. Successful candidates will have strong engineering skills and communication and a belief that data-driven processes lead to phenomenal products.\n\nWhat you'll do:\nBuild and launch data pipelines, and data products focussed on SMART Org.\nHelping teams push the boundaries of insights, creating new product features using data, and powering machine learning models.\nBuild cross-functional relationships to understand data needs, build key metrics and standardize their usage across the organization.\nUtilize current and leading edge technologies in software engineering, big data, streaming, and cloud infrastructure\n\nWhat You'll Need:\nBachelor/Master degree in Computer Science or related technical subject area or equivalent combination of education and experience 3+ years relevant work experience in the Data Engineering field with web scale data sets.\nDemonstrated strength in data modeling, ETL development and data lake architecture.\nData Warehousing Experience with Big Data Technologies (Hadoop, Spark, Hive, Presto, Airflow etc.).\nCoding proficiency in at least one modern programming language (Python, Scala, etc)\nExperience building/operating highly available, distributed systems of data extraction, ingestion, and processing and query performance tuning skills of large data sets.\nIndustry experience as a Big Data Engineer and working along cross functional teams such as Software Engineering, Analytics, Data Science with a track record of manipulating, processing, and extracting value from large datasets.\nStrong business acumen. Experience leading large-scale data warehousing and analytics projects, including using GCP technologies Big Query, Dataproc, GCS, Cloud Composer, Dataflow or related big data technologies in other cloud platforms like AWS, Azure etc.\nBe a team player and introduce/follow the best practices on the data engineering space.\nAbility to effectively communicate (both written and verbally) technical information and the results of engineering design at all levels of the organization.\n\nGood to have :\nUnderstanding of NoSQL Database exposure and Pub-Sub architecture setup.\nFamiliarity with Bl tools like Looker, Tableau, AtScale, PowerBI, or any similar tools.\n\nPS: This role is with one of our clients who is a leading name in Retail Industry.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Data Engineering', 'Cloud Platform', 'Hive', 'GCP', 'Bigquery', 'Hadoop', 'SCALA', 'Big Data Technologies', 'Etl Development', 'Spark', 'Python']",2025-06-11 05:27:00
Tech. PM - Data Engineering-Data Analytics@ Gurgaon/Blore_Urgent,A global leader in delivering innovative...,5 - 10 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Job Title - Technical Project Manager\n\nLocation - Gurgaon/ Bangalore\n\nNature of Job - Permanent\n\nDepartment - data analytics\n\nWhat you will be doing\n\n\nDemonstrated client servicing and business analytics skills with at least 5 - 9 years of experience as data engineer, BI developer, data analyst, technical project manager, program manager etc.\nTechnical project management- drive BRD, project scope, resource allocation, team\ncoordination, stakeholder communication, UAT, Prod fix, change requests, project governance\nSound knowledge of banking industry (payments, retail operations, fraud etc.)\nStrong ETL experience or experienced Teradata developer\nManaging team of business analysts, BI developers, ETL developers to ensure that projects are completed on time\nResponsible for providing thought leadership and technical advice on business issues\nDesign methodological frameworks and solutions.\n\n\nWhat were looking for\n\n\nBachelors/masters degree in computer science/data science/AI/statistics, Certification in Gen AI. Masters degree Preferred.\nManage multiple projects, at a time, from inception to delivery\nSuperior problem-solving, analytical, and quantitative skills\nEntrepreneurial mindset, coupled with a “can do” attitude\nDemonstrated ability to collaborate with cross-functional, cross-border teams and coach / mentor colleagues.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Technical Project Manager', 'Data Engineering', 'multiple projects', 'Technical project management', 'Data Analytics', 'project scope', 'ETL Pipeline', 'team coordination', 'resource allocation', 'Prod fix', 'drive BRD', 'program manager', 'Big data']",2025-06-11 05:27:01
Azure Data Engineer,Our client is a multinational professio...,7 - 12 years,Not Disclosed,['Bengaluru'],"Urgently Hiring for Senior Azure Data Engineer\n\nJob Location- Bangalore\nMinimum exp - 7yrs- 11yrs\n\nKeywords Databricks, Pyspark, SCALA, SQL, Live / Streaming data, batch processing data\n\nShare CV Mohini.sharma@adecco.com\nOR Call 9740521948\n\nRoles and Responsibilities:\nThe Data Engineer will work on data engineering projects for various business units, focusing on delivery of complex data management solutions by leveraging industry best practices. They work with the project team to build the most efficient data pipelines and data management solutions that make data easily available for consuming applications and analytical solutions. A Data engineer is expected to possess strong technical skills.\nKey Characteristics\nTechnology champion who constantly pursues skill enhancement and has inherent curiosity to understand work from multiple dimensions.\nInterest and passion in Big Data technologies and appreciates the value that can be brought in with an effective data management solution.\nHas worked on real data challenges and handled high volume, velocity, and variety of data.\nExcellent analytical & problem-solving skills, willingness to take ownership and resolve technical challenges.\nContributes to community building initiatives like CoE, CoP.\nMandatory skills:\nAzure - Master\nELT - Skill\nData Modeling - Skill\nData Integration & Ingestion - Skill\nData Manipulation and Processing - Skill\nGITHUB, Action, Azure DevOps - Skill\nData factory, Databricks, SQL DB, Synapse, Stream Analytics, Glue, Airflow, Kinesis, Redshift, SonarQube, PyTest - Skill\nOptional skills:\nExperience in project management, running a scrum team.\nExperience working with BPC, Planning.\nExposure to working with external technical ecosystem.\nMKDocs documentation\n\nShare CV Mohini.sharma@adecco.com\nOR Call 9740521948",Industry Type: Financial Services (Asset Management),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Databricks', 'Pyspark', 'SCALA', 'SQL']",2025-06-11 05:27:03
Consultant Data Engineering - Connected Medicine,Eli Lilly And Company,7 - 10 years,Not Disclosed,['Bengaluru'],"The role will be responsible for setting up the data warehouses necessary to handle large volumes of data, create meaningful analyses, and deliver recommendations to leadership.\nCore Responsibilities\nCreate and maintain optimal data pipeline architecture ETL/ ELT into structured data\nAssemble large, complex data sets that meet business requirements and create and maintain multi-dimensional modelling like Star Schema and Snowflake Schema, normalization, de-normalization, joining of datasets.\nExpert level experience in creating a scalable data warehouse including Fact tables, Dimensional tables and ingest datasets into cloud based tools.\nIdentify, design, and implement internal process improvements including automating manual processes, optimizing data delivery and re-designing infrastructure for greater scalability.\nCollaborate with stakeholders to ensure seamless integration of data with internal data marts, enhancing advanced reporting\nSetup and maintain data ingestion, streaming, scheduling, and job monitoring automation using AWS services. Setup Lambda, code pipeline (CI/CD), Glue, S3, Redshift, Power BI needs to be maintained for uninterrupted automation.\nBuild analytics tools that utilize the data pipeline to provide actionable insight into customer acquisition, operational efficiency, and other key business performance metrics.\nWork with stakeholders to assist with data-related technical issues and support their data infrastructure needs.\nUtilize GitHub for version control, code collaboration, and repository management. Implement best practices for code reviews, branching strategies, and continuous integration.\nCreate data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader\nEnsure data privacy and compliance with relevant regulations (eg, GDPR) when handling customer data.\nMaintain data quality and consistency within the application, addressing data-related issues as they arise.\nRequired\n7-10 years of relevant experience\nAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as we'll as working familiarity with a variety of databases and Cloud Data warehouse like AWS Redshift\nExperience in creating scalable, efficient schema designs to support diverse business needs.\nExperience with database normalization, schema evolution, and maintaining data integrity\nProactively share best practices, contributing to team knowledge and improving schema design transitions.\nDevelop data models, create dimensions and facts, and establish views and procedures to enable automation programmability.\nCollaborate effectively with cross-functional teams to gather requirements, incorporate feedback, and align analytical work with business objectives\nPrior Data Modelling, OLAP cube modelling\nData compression into PARQUET to improve processing and finetuning SQL programming skills.\nExperience building and optimizing big data data pipelines, architectures and data sets.\nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\nExperience with manipulating, processing and extracting value from large disconnected unrelated datasets\nStrong analytic skills related to working with structured and unstructured datasets.\nWorking knowledge of message queuing, stream processing, and highly scalable big data stores.\nExperience supporting and working with cross-functional teams and Global IT.\nFamiliarity of working in an agile based working models.\nPreferred Qualifications/Expertise\nExperience with relational SQL and NoSQL databases, especially AWS Redshift.\nExperience with AWS cloud services Preferable : S3, EC2, Lambda, Glue, EMR, Code pipeline highly preferred. Experience with similar services on another platform would also be considered.\nEducation:\nbachelors or masters degree on Technology and Computer Science background",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'Process improvement', 'Analytical', 'Healthcare', 'Scheduling', 'Data quality', 'Operations', 'Analytics', 'Monitoring', 'SQL']",2025-06-11 05:27:05
Data Engineer - Senior,Iris Solutions,4 - 8 years,Not Disclosed,['Noida'],"Design, implement, and maintain data pipelines for processing large datasets, ensuring data availability, quality, and efficiency for machine learning model training and inference.\n\nCollaborate with data scientists to streamline the deployment of machine learning models, ensuring scalability, performance, and reliability in production environments.\n\nDevelop and optimize ETL (Extract, Transform, Load) processes, ensuring data flow from various sources into structured data storage systems.\n\nAutomate ML workflows using ML Ops tools and frameworks (e. g. , Kubeflow, MLflow, TensorFlow Extended (TFX)).\n\nEnsure effective model monitoring, versioning, and logging to track performance and metrics in a production setting.\n\nCollaborate with cross-functional teams to improve data architectures and facilitate the continuous integration and deployment of ML models.\n\nWork on data storage solutions, including databases, data lakes, and cloud-based storage systems (e. g. , AWS, GCP, Azure).\n\nEnsure data security, integrity, and compliance with data governance policies.\n\nPerform troubleshooting and root cause analysis on production-level machine learning systems.\n\nSkills: Glue, Pyspark, AWS Services, Strong in SQL; Nice to have : Redshift, Knowledge of SAS Dataset\n\nMandatory Competencies\nDevOps - CLOUD AWS\nDevOps - Docker\nETL - AWS Glue\nDevOps - Kubernetes\nDatabase - SQL\nBig Data - PySpark\nDatabase - Redshift\nCloud - Azure\nData Science - Azure ML\nData on Cloud - Azure Data Lake (ADL)\nBeh - Communication and collaboration\n\n\nAt Iris Software, we offer world-class benefits designed to support the financial, health and well-being needs of our associates to help achieve harmony between their professional and personal growth. From comprehensive health insurance and competitive salaries to flexible work arrangements and ongoing learning opportunities, were committed to providing a supportive and rewarding work environment.\n\nJoin us and experience the difference of working at a company that values its employees success and happiness.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Health insurance', 'data science', 'SAS', 'data security', 'GCP', 'Machine learning', 'Cloud', 'data governance', 'Troubleshooting', 'Monitoring']",2025-06-11 05:27:07
"Data Engineer (Python, Kafka Stream, Pyspark, and Azure Databricks.)",Hire Squad,5 - 8 years,20-30 Lacs P.A.,"['Noida', 'Hyderabad', 'Bengaluru']","Looking for Data Engineers, immediate joiners only, for Hyderabad, Bengaluru and Noida Location.\n\n*Must have experience in Python, Kafka Stream, Pyspark, and Azure Databricks.*\n\nRole and responsibilities:\nLead the design, development, and implementation of real-time data pipelines using Kafka, Python, and Azure Databricks.\nArchitect scalable data streaming and processing solutions to support healthcare data workflows.\nDevelop, optimize, and maintain ETL/ELT pipelines for structured and unstructured healthcare data.\nEnsure data integrity, security, and compliance with healthcare regulations (HIPAA, HITRUST, etc.).\nCollaborate with data engineers, analysts, and business stakeholders to understand requirements and translate them into technical solutions.\nTroubleshoot and optimize Kafka streaming applications, Python scripts, and Databricks workflows.\nMentor junior engineers, conduct code reviews, and ensure best practices in data engineering.\nStay updated with the latest cloud technologies, big data frameworks, and industry trends.\n\n\nPreferred candidate profile :\n5+ years of experience in data engineering, with strong proficiency in Kafka and Python.\nExpertise in Kafka Streams, Kafka Connect, and Schema Registry for real-time data processing.\nExperience with Azure Databricks (or willingness to learn and adopt it quickly).\nHands-on experience with cloud platforms (Azure preferred, AWS or GCP is a plus).\nProficiency in SQL, NoSQL databases, and data modeling for big data processing.\nKnowledge of containerization (Docker, Kubernetes) and CI/CD pipelines for data applications.\nExperience working with healthcare data (EHR, claims, HL7, FHIR, etc.) is a plus.\nStrong analytical skills, problem-solving mindset, and ability to lead complex data projects.\nExcellent communication and stakeholder management skills.\n\n\n\nInterested, call:\nRose (9873538143 / WA : 8595800635)\nrose2hiresquad@gmail.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Azure Databricks', 'Kafka Streams', 'Python', 'Etl Pipelines', 'development', 'Data Engineering', 'cloud-based data platforms', 'implementation', 'Data Streaming', 'Lead the design', 'processing solutions', 'Hitrust', 'large-scale data processing', 'ELT pipelines', 'HIPAA', 'real-time data streaming']",2025-06-11 05:27:08
"Google Cloud Platform Data Engineer -GCP,BigQuery,SQL, Cloud Function",Tredence,5 - 10 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","Role - GCP Data Engineer\nExperience:4+ years\nPreferred - Data Engineering Background\nLocation - Bangalore, Chennai, Pune, Gurgaon, Kolkata\nRequired Skills - GCP DE Experience, Big query, SQL, Cloud compressor/Python, Cloud functions, Dataproc+pyspark, Python injection, Dataflow+PUB/SUB\n\nHere is the job description for the same -\nJob Requirement:\nHave Implemented and Architected solutions on Google Cloud Platform using the components of GCP\nExperience with Apache Beam/Google Dataflow/Apache Spark in creating end to end data pipelines.\nExperience in some of the following: Python, Hadoop, Spark, SQL, Big Query, Big Table Cloud Storage, Datastore, Spanner, Cloud SQL, Machine Learning.\nExperience programming in Java, Python, etc.\nExpertise in at least two of these technologies: Relational Databases, Analytical Databases, NoSQL databases.\nCertified in Google Professional Data Engineer/ Solution Architect is a major Advantage",,,,"['Pubsub', 'GCP', 'Bigquery', 'Google Cloud Platforms', 'SQL', 'Data Flow', 'Dataproc']",2025-06-11 05:27:10
Senior Data Engineer,Reuters,6 - 11 years,Not Disclosed,"['Mumbai', 'Hyderabad']","Develop and maintain data solutions using resources such as dbt, Alteryx, and Python.\nDesign and optimize data pipelines, ensuring efficient data flow and processing.\nWork extensively with databases, SQL, and various data formats including JSON, XML, and CSV.\nTune and optimize queries to enhance performance and reliability.\nDevelop high-quality code in SQL, dbt, and Python, adhering to best practices.\nUnderstand and implement data automation and API integrations.\nLeverage AI capabilities to enhance data engineering practices.\nUnderstand integration points related to upstream and downstream requirements.\nProactively manage tasks and work towards completion against tight deadlines.\nAnalyze existing processes and offer suggestions for improvement.\nAbout You\nStrong interest and knowledge in data engineering principles and methods.\n6+ years of experience developing data solutions or pipelines.\n6+ years of hands-on experience with databases and SQL.\n2+ years of experience programming in an additional language.\n2+ years of experience in query tuning and optimization.\nExperience working with SQL, JSON, XML, and CSV content.\nUnderstanding of data automation and API integration.\nFamiliarity with AI capabilities and their application in data engineering.\nAbility to adhere to best practices for developing programmatic solutions.\nStrong problem-solving skills and ability to work independently.\nWhat s in it For You\nHybrid Work Model: we've adopted a flexible hybrid working environment (2-3 days a week in the office depending on the role) for our office-based roles while delivering a seamless experience that is digitally and physically connected.\nFlexibility Work-Life Balance: Flex My Way is a set of supportive workplace policies designed to help manage personal and professional responsibilities, whether caring for family, giving back to the community, or finding time to refresh and reset. This builds upon our flexible work arrangements, including work from anywhere for up to 8 weeks per year, empowering employees to achieve a better work-life balance.\nCareer Development and Growth: By fostering a culture of continuous learning and skill development, we prepare our talent to tackle tomorrow s challenges and deliver real-world solutions. Our Grow My Way programming and skills-first approach ensures you have the tools and knowledge to grow, lead, and thrive in an AI-enabled future.\nIndustry Competitive Benefits: We offer comprehensive benefit plans to include flexible vacation, two company-wide Mental Health Days off, access to the Headspace app, retirement savings, tuition reimbursement, employee incentive programs, and resources for mental, physical, and financial we'llbeing.\nCulture: Globally recognized, award-winning reputation for inclusion and belonging, flexibility, work-life balance, and more. We live by our values: Obsess over our Customers, Compete to Win, Challenge (Y)our Thinking, Act Fast / Learn Fast, and Stronger Together.\nSocial Impact: Make an impact in your community with our Social Impact Institute. We offer employees two paid volunteer days off annually and opportunities to get involved with pro-bono consulting projects and Environmental, Social, and Governance (ESG) initiatives.\nMaking a Real-World Impact: We are one of the few companies globally that helps its customers pursue justice, truth, and transparency. Together, with the professionals and institutions we serve, we help uphold the rule of law, turn the wheels of commerce, catch bad actors, report the facts, and provide trusted, unbiased information to people all over the world.",Industry Type: Internet,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Career development', 'Automation', 'XML', 'Consulting', 'Flex', 'JSON', 'Taxation', 'Downstream', 'SQL', 'Python']",2025-06-11 05:27:11
Data Engineering Advisor,ManipalCigna Health Insurance,11 - 13 years,Not Disclosed,['Hyderabad'],"Data Engineering Advisor - HIH - Evernorth\nAbout Evernorth:\nEvernorth Health Services, a division of The Cigna Group (NYSE: CI), creates pharmacy, care, and benefits solutions to improve health and increase vitality. We relentlessly innovate to make the prediction, prevention, and treatment of illness and disease more accessible to millions of people.\nRole Title: Software Engineering Advisor\nPosition Summary:\nData engineer on the Data integration team\nJob Description & Responsibilities:\nWork with business and technical leadership to understand requirements.\nDesign to the requirements and document the designs\nAbility to write product-grade performant code for data extraction, transformations and loading using Spark, Py-Spark\nDo data modeling as needed for the requirements.\nWrite performant queries using Teradata SQL, Hive SQL and Spark SQL against Teradata and Hive\nImplementing dev-ops pipelines to deploy code artifacts on to the designated platform/servers like AWS.\nTroubleshooting the issues, providing effective solutions and jobs monitoring in the production environment\nParticipate in sprint planning sessions, refinement/story-grooming sessions, daily scrums, demos and retrospectives.\nExperience Required:\nOverall 11-13 years of experience\nExperience Desired:\nStrong development experience in Spark, Py-Spark, Shell scripting, Teradata.\nStrong experience in writing complex and effective SQLs (using Teradata SQL, Hive SQL and Spark SQL) and Stored Procedures\nHealthcare domain knowledge is a plus\nEducation and Training Required:\nPrimary Skills:\nExcellent work experience on Databricks as Data Lake implementations\nExperience in Agile and working knowledge on DevOps tools (Git, Jenkins, Artifactory)\nAWS (S3, EC2, SNS, SQS, Lambda, ECS, Glue, IAM, and CloudWatch)\nDatabricks (Delta lake, Notebooks, Pipelines, cluster management, Azure/AWS integration\nAdditional Skills:\nExperience in Jira and Confluence\nExercises considerable creativity, foresight, and judgment in conceiving, planning, and delivering initiatives.\nLocation & Hours of Work\nHyderabad /General Shift (11:30 AM - 8:30 PM IST / 1:00 AM - 10:00 AM EST / 2:00 AM - 11:00 AM EDT)\nEqual Opportunity Statement\nEvernorth is an Equal Opportunity Employer actively encouraging and supporting organization-wide involvement of staff in diversity, equity, and inclusion efforts to educate, inform and advance both internal practices and external work with diverse client populations\n\nAbout Evernorth Health Services",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GIT', 'Data modeling', 'Shell scripting', 'Agile', 'Healthcare', 'Stored procedures', 'Troubleshooting', 'JIRA', 'Monitoring', 'Data extraction']",2025-06-11 05:27:13
Data Engineer with Machine Learning Specialization,Ortseam Technologies,5 - 10 years,Not Disclosed,[],"Job Requirement for Offshore Data Engineer (with ML expertise)\nWork Mode: Remote\nBase Location: Bengaluru\nExperience: 5+ Years\n\nTechnical Skills & Expertise:\n\nPySpark & Apache Spark:\nExtensive experience with PySpark and Spark for big data processing and transformation.\nStrong understanding of Spark architecture, optimization techniques, and performance tuning.\nAbility to work with Spark jobs in distributed computing environments like Databricks.\nData Mining & Transformation:\nHands-on experience in designing and implementing data mining workflows.\nExpertise in data transformation processes, including ETL (Extract, Transform, Load) pipelines.\nExperience in large-scale data ingestion, aggregation, and cleaning.\nProgramming Languages:\nPython & Scala: Proficient in Python for data engineering tasks, including using libraries like Pandas and NumPy. Scala proficiency is preferred for Spark job development.\nBig Data Concepts: In-depth knowledge of big data frameworks and paradigms, such as distributed file systems, parallel computing, and data partitioning.\nBig Data Technologies:\nCassandra & Hadoop: Experience with NoSQL databases like Cassandra and distributed storage systems like Hadoop.\nData Warehousing Tools: Proficiency with Hive for data warehousing solutions and querying.\nETL Tools: Experience with Beam architecture and other ETL tools for large-scale data workflows.\nCloud Technologies (GCP):\nExpertise in Google Cloud Platform (GCP), including core services like Cloud Storage, BigQuery, and DataFlow.\nExperience with DataFlow jobs for batch and stream processing.\nFamiliarity with managing workflows using Airflow for task scheduling and orchestration in GCP.\nMachine Learning & AI:\nGenAI Experience: Familiarity with Generative AI and its applications in ML pipelines.\nML Model Development: Knowledge of basic ML model building using tools like Pandas, NumPy, and visualization with Matplotlib.\nML Ops Pipeline: Experience in managing end-to-end ML Ops pipelines for deploying models in production, particularly LLM (Large Language Models) deployments.\nRAG Architecture: Understanding and experience in building pipelines using Retrieval-Augmented Generation (RAG) architecture to enhance model performance and output.\n\nTech stack : Spark, Pyspark, Python, Scala, GCP data flow, Data composer (Air flow), ETL, Databricks, Hadoop, Hive, GenAI, ML Modeling basic knowledge, ML Ops experience , LLM deployment, RAG",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Machine Learning', 'Gcp Cloud', 'Python', 'Airflow', 'Hadoop', 'Data Bricks', 'Hive', 'SCALA', 'Data Flow', 'Spark', 'ETL']",2025-06-11 05:27:15
Senior Data Engineer - Azure,Blend360 India,6 - 11 years,Not Disclosed,['Hyderabad'],"As a Sr Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\n7+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar field\nMust have experience e",Industry Type: Industrial Automation,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-11 05:27:17
Data Engineering Advisor,ManipalCigna Health Insurance,8 - 10 years,Not Disclosed,['Hyderabad'],"Data Engineering Advisor - HIH - Evernorth\nAbout Evernorth:\nEvernorth Health Services, a division of The Cigna Group (NYSE: CI), creates pharmacy, care, and benefits solutions to improve health and increase vitality. We relentlessly innovate to make the prediction, prevention, and treatment of illness and disease more accessible to millions of people.\nPosition Summary:\nData engineer on the Data integration team\nJob Description & Responsibilities:\nWork with business and technical leadership to understand requirements.\nDesign to the requirements and document the designs\nAbility to write product-grade performant code for data extraction, transformations and loading using Spark, Py-Spark\nDo data modeling as needed for the requirements.\nWrite performant queries using Teradata SQL, Hive SQL and Spark SQL against Teradata and Hive\nImplementing dev-ops pipelines to deploy code artifacts on to the designated platform/servers like AWS.\nTroubleshooting the issues, providing effective solutions and jobs monitoring in the production environment\nParticipate in sprint planning sessions, refinement/story-grooming sessions, daily scrums, demos and retrospectives.\nExperience Required:\nOverall 8-10 years of experience\nExperience Desired:\nStrong development experience in Spark, Py-Spark, Shell scripting, Teradata.\nStrong experience in writing complex and effective SQLs (using Teradata SQL, Hive SQL and Spark SQL) and Stored Procedures\nHealth care domain knowledge is a plus\nPrimary Skills:\nExcellent work experience on Databricks as Data Lake implementations\nExperience in Agile and working knowledge on DevOps tools (Git, Jenkins, Artifactory)\nAWS (S3, EC2, SNS, SQS, Lambda, ECS, Glue, IAM, and CloudWatch)\nDatabricks (Delta lake, Notebooks, Pipelines, cluster management, Azure/AWS integration\nAdditional Skills:\nExperience in Jira and Confluence\nExercises considerable creativity, foresight, and judgment in conceiving, planning, and delivering initiatives.\nLocation & Hours of Work\nHyderabad /General Shift (11:30 AM - 8:30 PM IST / 1:00 AM - 10:00 AM EST / 2:00 AM - 11:00 AM EDT)\nEqual Opportunity Statement\nEvernorth is an Equal Opportunity Employer actively encouraging and supporting organization-wide involvement of staff in diversity, equity, and inclusion efforts to educate, inform and advance both internal practices and external work with diverse client populations\nAbout Evernorth Health Services",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GIT', 'Data modeling', 'Shell scripting', 'Agile', 'Engineering Manager', 'Stored procedures', 'Troubleshooting', 'JIRA', 'Monitoring', 'Data extraction']",2025-06-11 05:27:18
Manager/Sr. Manager Data Engineering,Cognitio Analytics,8 - 12 years,Not Disclosed,['Gurugram'],"Clinical Coder(Contract)\nGurugram (Hybrid)\n7 To 9 years\n+ Job Description\nApply\nKnowledge, and Expectations:\nAdvanced knowledge of medical coding and billing systems, groupers, crosswalks, and classification systems including proficiency in regulatory requirements.\nPossess thorough knowledge of anatomical and medical terminology, demonstrating a natural curiosity and analytical mindset.\nAbility to create and maintain crosswalks matching up/ recommend the equivalent codes based on coding guidelines.\nResearch and bring in international and regional medical coding schemas/ classifications, crosswalks, risk adjustment tools, reference lists/ value sets and drug/ medical device directories to the database, enhancing company s medical coding assets and highlighting the standards licensing requirements (wherever applied).\nAnalyse and interpret claims line level descriptions, and other documentation, and convert them into codable format to the best of clinical and coding knowledge.\nReview and verify codes for diagnoses, procedures and treatment, and observations for coding inaccuracies and deficiencies as part of codes quality checks.\nServe as resource and subject matter expert to other coding staff.",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Claims', 'Coding', 'Medical coding', 'Analytical', 'Billing', 'Database', 'Subject Matter Expert', 'Research', 'Licensing']",2025-06-11 05:27:20
Azure Data Engineer,Tech Mahindra,5 - 8 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","Company Name: Tech Mahindra\nExperience: 5-8 Years\nLocation: Bangalore/Hyderabad (Hybrid Model)\nInterview Mode: Virtual\nInterview Rounds: 2 Rounds\nNotice Period: Immediate to 30 days\nGeneric Responsibilities :\nDesign, develop, and maintain large-scale data pipelines using Azure Data Factory (ADF) to extract, transform, and load data from various sources into Azure Databricks.\nCollaborate with cross-functional teams to gather requirements and design solutions for complex business problems.\nDevelop SQL queries and stored procedures to optimize database performance and troubleshoot issues in Azure Databricks.\nEnsure high availability, scalability, and security of the deployed solutions by monitoring logs, metrics, and alerts.\nGeneric Requirements :\n5-8 years of experience in designing and developing large-scale data engineering projects on Microsoft Azure platform.\nStrong expertise in Azure Data Factory (ADF), Azure Databricks, SQL Server Management Studio (T-SQL).\nExperience working with big data technologies such as Hadoop Distributed File System (HDFS), Spark Core/Scala programming languages.",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Azure Data Factory', 'Azure Data Lake', 'Azure Data Services', 'Data Bricks', 'SQL']",2025-06-11 05:27:21
Junior Data Engineer,Talent Corner Hr Services,1 - 5 years,7-10 Lacs P.A.,['Bengaluru( JP Nagar )'],"We are looking for a Junior Data Engineer with 1–3 years of experience, primarily focused on database management and data processing using MySQL. The candidate will support the data engineering team in maintaining reliable data pipelines",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MySQL', 'Query Optimization', 'Data Processing']",2025-06-11 05:27:23
Automation & Data Engineer,Vyometra Global Llp,2 - 3 years,3.6-3.96 Lacs P.A.,['Bengaluru'],"Work with ops teams to digitize manual processes.\nCapture data via PLCs/IoT, build backend (Python, Flask/Django), SQL DB & frontend dashboard.\nTrack OEE, downtime, rejections. Deploy, maintain & contribute to future product roadmap.",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Automation', 'Automation', 'Machine Learning', 'Javascript', 'Python', 'Ai Techniques', 'Artificial Intelligence', 'Software Testing', 'Flask Web Framework', 'Numpy', 'Scikit-Learn', 'English', 'Pandas', 'Data Analysis', 'SQL Database', 'Ai Builder', 'Flask']",2025-06-11 05:27:24
Junior Data Engineer,Xequalto Analytics,3 - 4 years,Not Disclosed,['Bengaluru'],"Location: Remote\nEmployment Type: Full-Time\nExperience: 3+ Years\nAbout Us\nWe're a fast-growing company driven by data\nWe're looking for a skilled and enthusiastic Junior Data Engineer to join our team and help us shape the future of our data infrastructure\nThis is a fully remote role work from wherever you're most productive\nIf you're passionate about data and eager to make a real impact, we want to hear from you\nAbout the Role\nAs a Junior Data Engineer, you'll be a key player in our data engineering efforts\nYou'll be working hands-on, collaborating with a talented team, and contributing directly to the development and maintenance of our data pipelines and infrastructure\nThis role offers a unique opportunity to learn, grow, and make a tangible difference in how we leverage data\nWhat You'll Do:\nDesign and build robust data pipelines using tools like Databricks, Spark, and PySpark\nDevelop and maintain our data warehouse and data models, ensuring they meet the needs of our analytics and operations teams\nDive into data transformation and processing with SQL and Python\nPartner with engineers, analysts, and stakeholders across the company to understand their data needs and deliver effective solutions\nMaintain clean and organized code using Git\nContribute to our ongoing efforts to improve data quality and ensure data integrity\nWhat You'll Need:\n3+ years of experience in data engineering\nA solid understanding of cloud platforms like AWS or Azure\nStrong skills in Python, SQL, Spark, and PySpark\nPractical experience with cloud-based ETL tools\nA genuine passion for problem-solving and a desire to learn and grow\nExcellent communication skills and a collaborative spirit\nBonus Points:\nExperience with DevOps tools (Docker, Terraform, Airflow, GitHub Actions the more the merrier)\nFamiliarity with CI/CD pipelines and infrastructure as code\nA knack for optimizing workflows and boosting performance\nWhat We Offer:\n100% remote work work from anywhere!\nA supportive and collaborative team environment\nOpportunities for professional development and growth\nA competitive salary and benefits package\nShow more Show less",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data bricks', 'python', 'spark', 'pyspark', 'microsoft azure', 'data engineering', 'sql', 'communication skills']",2025-06-11 05:27:26
Data Analytics and Applied AI Engineer,Nvidia,4 - 8 years,Not Disclosed,['Bengaluru'],"NVIDIA has continuously reinvented itself. Our invention of the GPU sparked the growth of the PC gaming market, redefined modern computer graphics, and revolutionized parallel computing. Today, research in artificial intelligence is booming worldwide, which calls for highly scalable and massively parallel computation horsepower that NVIDIA GPUs excel.\nNVIDIA is a learning machine that constantly evolves by adapting to new opportunities that are hard to solve, that only we can address, and that matter to the world. This is our life s work , to amplify human creativity and intelligence. As an NVIDIAN, you ll be immersed in a diverse, supportive environment where everyone is inspired to do their best work. Come join our diverse team and see how you can make a lasting impact on the world! Design-for-Test Engineering at NVIDIA works on groundbreaking innovations involving crafting creative solutions for DFT architecture, verification, and post-silicon validation on some of the industrys most complex semiconductor chips.\nWhat youll be doing:\nAs an integral member in our team, you will work on exploring Applied AI solutions for DFX and VLSI problem statements.\nArchitect end-to-end generative AI solutions with a focus on LLMs, RAGs Agentic AI workflows.\nWork on deploying predictive ML models for efficient Silicon Lifecycle Management of NVIDIAs chips.\nCollaborate closely with various VLSI DFX teams to understand their language-related engineering challenges and design tailored solutions.\nPartner closely with cross-functional AI teams to provide feedback and contribute to the evolution of generative AI technologies.\nWork closely with DFX teams to integrate Agentic AI workflows into their applications and systems and stay abreast of the latest developments in language models and generative AI technologies.\nDefine how data will be collected, stored, consumed and managed for next-generation AI use cases.\nYou will also help mentor junior engineers on test designs and trade-offs including cost and quality.\nWhat we need to see:\nBSEE or MSEE from reputed institutions with 2+ years of experience in DFT, VLSI Applied Machine Learning\nExperience in Applied ML solutions for chip design problems\nSignificant experience in deploying generative AI solutions for engineering use cases\nGood understanding of fundamental DFT VLSI concepts - ATPG, scan, RTL clocks design, STA, place-n-route and power\nExperience in application of AI for EDA-related problem-solving is a plus\nExcellent knowledge in using statistical tools for data analysis insights\nStrong programming and scripting skills in Perl, Python, C++ or TCL desired\nStrong organization and time management skills to work in a fast-pace multi-task environment\nSelf-motivated, independent, ability to work independently with minimal day-to-day direction\nOutstanding written and oral communication skills with the curiosity to work on rare challenges\nNVIDIA offers highly competitive salaries and a comprehensive benefits package. We have some of the most brilliant and talented people in the world working for us and, due to unprecedented growth, our world-class engineering teams are growing fast. If youre a creative and autonomous engineer with real passion for technology, we want to hear from you!\n#LI-Hybrid",Industry Type: Electronic Components / Semiconductors,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['C++', 'Data analysis', 'Semiconductor', 'DFT', 'VLSI', 'Chip design', 'Machine learning', 'Perl', 'Gaming', 'Python']",2025-06-11 05:27:28
Hiring For Azure Data Engineer with MNC client-FTE-Hyd/Bangalore/Pune,The It Mind Services,4 - 6 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","JOB DESCRIPTION:\n\n• Strong experience in Azure Datafactory,Databricks, Eventhub, Python,PySpark ,Azure Synapse and SQL\n• Azure Devops experience to deploy the ADF pipelines.\n• Knowledge/Experience with Azure cloud stack.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Azure Synapse', 'Eventhub', 'Azure Datafactory', 'Databricks', 'Python', 'PySpark', 'Azure Devops']",2025-06-11 05:27:29
Assistant Manager - Data Engineer,Ajanta Pharma,5 - 10 years,6-12 Lacs P.A.,['Mumbai (All Areas)'],"Position Overview:\nThe Assistant Manager - Data Engineer will play a pivotal role in the design, development, and maintenance of data pipelines that ensure the efficiency, scalability, and reliability of our data infrastructure. This role will involve optimizing and automating ETL/ELT processes, as well as developing and refining databases, data warehouses, and data lakes. As an Assistant Manager, you will also mentor junior engineers and collaborate closely with cross-functional teams to support business goals and drive data excellence.\n\nKey Responsibilities:\nData Pipeline Development: Design, build, and maintain efficient, scalable, and reliable data pipelines to support data analytics, reporting, and business intelligence initiatives.\nDatabase and Data Warehouse Management: Develop, optimize, and manage databases, data warehouses, and data lakes to enhance data accessibility and business decision-making.\nETL/ELT Optimization: Automate and optimize data extraction, transformation, and loading (ETL/ELT) processes, ensuring efficient data flow and improved system performance.\nData Modeling & Architecture: Develop and maintain data models to enable structured data storage, analysis, and reporting in alignment with business needs.\nWorkflow Management Systems: Implement, optimize, and maintain workflow management tools (e.g., Apache Airflow, Talend) to streamline data engineering tasks and improve operational efficiency.\nTeam Leadership & Mentorship: Guide, mentor, and support junior data engineers to enhance their skills and contribute effectively to projects.\nCollaboration with Cross-Functional Teams: Work closely with data scientists, analysts, business stakeholders, and IT teams to understand requirements and deliver solutions that align with business objectives.\nPerformance Optimization: Continuously monitor and optimize data pipelines and storage solutions to ensure maximum performance and cost efficiency.\nDocumentation & Process Improvement: Create and maintain documentation for data models, workflows, and systems. Contribute to the continuous improvement of data engineering practices.\n\nQualifications:\nEducational Background: B.E., B.Tech., MCA\nProfessional Experience: At least 5 to 7 years of experience in a data engineering or similar role, with hands-on experience in building and optimizing data pipelines, ETL processes, and database management.\nTechnical Skills:\nProficiency in Python and SQL for data processing, transformation, and querying.\nExperience with modern data warehousing solutions (e.g., Amazon Redshift, Snowflake, Google BigQuery, Azure Data Lake).\nStrong background in data modeling (dimensional, relational, star/snowflake schema).\nHands-on experience with ETL tools (e.g., Apache Airflow, Talend, Informatica) and workflow management systems.\nFamiliarity with cloud platforms (AWS, Azure, Google Cloud) and distributed data processing frameworks (e.g., Apache Spark).\nData Visualization & Exploration: Familiarity with data visualization tools (e.g., Tableau, Power BI) for analysis and reporting.\nLeadership Skills: Demonstrated ability to manage and mentor a team of junior data engineers while fostering a collaborative and innovative work environment.\nProblem-Solving & Analytical Skills: Strong analytical and troubleshooting skills with the ability to optimize complex data systems for performance and scalability.\nExperience in Pharma/Healthcare (preferred but not required): Knowledge of the pharmaceutical industry and experience with data in regulated environments.\nDesired Skills:\nFamiliarity with industry-specific data standards and regulations.\nExperience working with machine learning models or data science pipelines is a plus.\nStrong communication skills with the ability to present technical data to non-technical stakeholders.\n\nWhy Join Us:\nImpactful Work: Contribute to the pharmaceutical industry by improving data-driven decisions that impact public health.\nCareer Growth: Opportunities to develop professionally in a fast-growing industry and company.\nCollaborative Environment: Work with a dynamic and talented team of engineers, data scientists, and business stakeholders.\nCompetitive Benefits: Competitive salary, health benefits and more.",Industry Type: Pharmaceutical & Life Sciences,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Pipeline Development', 'ELT optimisation', 'Data Visualization', 'Data Warehouse Management', 'Data Modeling', 'Workflow Management Systems', 'Data Warehousing', 'ETL', 'machine learning', 'Google Cloud Platforms', 'Python', 'azure']",2025-06-11 05:27:31
Aws Data Engineer,Innova Solutions,4 - 7 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities\nLooking For AWS data Eng-Immediate joiners for Hyderabad,Chennai,Noida,Pune,Bangalore locations.\nMandatory Skill-Python,Pyspark,SQL,Aws Glue\n\nStrong technical skills in services like S3,Athena, Lambda, RGlue and Glue(Pyspark), SQL, Data Warehousing, Informatica, OracleDesign, develop, and implement custom solutions within the Collibra platform to support data governance initiatives.\n\nPreferred candidate profile\nSnowflake, Agile methodology and Tableau. Proficiency in Python/Scala, Spark architecture, complex SQL, and RDBMS. Hands-on experience with ETL tools (e.g., Informatica) and SCD1, SCD2. 2-6 years of DWH, AWS Services and ETL design knowledge.\n\nDevelop ETL processes for data ingestion, transformation, and loading into data lakes and warehouses.\nCollaborate with data scientists and analysts to ensure data availability for analytics and reporting.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Glue', 'ETL', 'Aws Cloud']",2025-06-11 05:27:32
Hiring- Azure Cloud Automation and Data Analytics Engineer PAN India,"NTT DATA, Inc.",5 - 8 years,Not Disclosed,"['Hyderabad', 'Gurugram', 'Bengaluru']","Overview:\n\nWe are seeking a dynamic and experienced Azure Cloud Automation and Data Analytics Engineer with a proactive attitude and strong independence. This role requires expertise in scripting and automating processes using IAC / scription, along with proficiency in languages like PowerShell, Python, and Bash. The ideal candidate will also have a background in Data Analytics and SQL. We are looking for a candidate who excels in automation and development, rather than merely operational tasks.\n\nList of Key Responsibilities:\n- Design, implement, and manage cloud infrastructure on Azure using Terraform and scripting languages.\n- Automate deployment, configuration, and management of cloud resources.\n- Develop and maintain data analytics solutions, including data pipelines and ETL processes.\n- Write, optimize, and manage SQL queries and databases and be aware of the results\n- Ensure security and compliance of cloud environments.\n- Collaborate on designing and implementing networking solutions within Azure.\n- Conduct performance tuning, troubleshooting, and root cause analysis.\n- Implement and manage monitoring, logging, and alerting systems.\n- Being able to interact with other teams in the company to GTD from other teams we depend by tracking of the tickets\nBut at the same time independent to understand what is required.\n- Participate in on-call rotations and provide support for cloud operations.\n\nQualifications:\n- Proven experience in cloud infrastructure management, specifically with Microsoft Azure with scripting approach more than just click-ops\n- Expertise in scripting and automation using Terraform, PowerShell, Python, and Bash.\n- Background in Data Analytics, including proficiency in SQL.\n- Knowledge of security best practices in a cloud environment.\n- Familiarity with Azure networking concepts and services.\n- Experience with DevOps practices and tools, including CI/CD pipelines and version control.\n\nMust Have Skills: Azure Storage / Azure Services / Azure Permissions, Azure Databricks / Spark , Azure SQL Server / Databases /SQL , Docker and Containers / Azure Container Registry ,Azure Devops Pipeline, Terraform, Cloud Platform.\n\nIf interested in the position, please apply to procced further-",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Analytics', 'Terraform', 'Azure Cloud Automation', 'Ci/Cd', 'Azure Devops', 'Azure Cloud Security', 'Docker', 'Aks Kubernetes', 'Python']",2025-06-11 05:27:34
TS - GCP data engineer,Srivango,6 - 11 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","Exp: 6-9Yrs\nSkill: GCP, Data engineer",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Engineering', 'GCP', 'Google Cloud Platforms']",2025-06-11 05:27:35
Data Engineer-Having Stratup-Mid-Size company Exp.@ Bangalore_Urgent,"As a leader in this space, we deliver wo...",8 - 13 years,Not Disclosed,['Bengaluru'],"Data Engineer\n\nLocation: Bangalore - Onsite\nExperience: 8 - 15 years\nType: Full-time\n\nRole Overview\n\nWe are seeking an experienced Data Engineer to build and maintain scalable, high-performance data pipelines and infrastructure for our next-generation data platform. The platform ingests and processes real-time and historical data from diverse industrial sources such as airport systems, sensors, cameras, and APIs. You will work closely with AI/ML engineers, data scientists, and DevOps to enable reliable analytics, forecasting, and anomaly detection use cases.\nKey Responsibilities\nDesign and implement real-time (Kafka, Spark/Flink) and batch (Airflow, Spark) pipelines for high-throughput data ingestion, processing, and transformation.\nDevelop data models and manage data lakes and warehouses (Delta Lake, Iceberg, etc) to support both analytical and ML workloads.\nIntegrate data from diverse sources: IoT sensors, databases (SQL/NoSQL), REST APIs, and flat files.\nEnsure pipeline scalability, observability, and data quality through monitoring, alerting, validation, and lineage tracking.\nCollaborate with AI/ML teams to provision clean and ML-ready datasets for training and inference.\nDeploy, optimize, and manage pipelines and data infrastructure across on-premise and hybrid environments.\nParticipate in architectural decisions to ensure resilient, cost-effective, and secure data flows.\nContribute to infrastructure-as-code and automation for data deployment using Terraform, Ansible, or similar tools.\n\n\nQualifications & Required Skills\n\nBachelors or Master’s in Computer Science, Engineering, or related field.\n6+ years in data engineering roles, with at least 2 years handling real-time or streaming pipelines.\nStrong programming skills in Python/Java and SQL.\nExperience with Apache Kafka, Apache Spark, or Apache Flink for real-time and batch processing.\nHands-on with Airflow, dbt, or other orchestration tools.\nFamiliarity with data modeling (OLAP/OLTP), schema evolution, and format handling (Parquet, Avro, ORC).\nExperience with hybrid/on-prem and cloud platforms (AWS/GCP/Azure) deployments.\nProficient in working with data lakes/warehouses like Snowflake, BigQuery, Redshift, or Delta Lake.\nKnowledge of DevOps practices, Docker/Kubernetes, Terraform or Ansible.\nExposure to data observability, data cataloging, and quality tools (e.g., Great Expectations, OpenMetadata).\nGood-to-Have\nExperience with time-series databases (e.g., InfluxDB, TimescaleDB) and sensor data.\nPrior experience in domains such as aviation, manufacturing, or logistics is a plus.\n\nRole & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['aviation', 'Data Modeling', 'Python', 'OLAP', 'Cloud', 'ORC', 'logistics', 'Avro', 'Terraform', 'Snowflake', 'manufacturing', 'AWS', 'Parquet', 'Java', 'Azure', 'BigQuery', 'Data', 'Redshift', 'SQL', 'TimescaleDB', 'GCP', 'InfluxDB', 'dbt', 'Ansible', 'OLTP', 'Kubernetes']",2025-06-11 05:27:37
"Tech Lead- Data Engineer (SQL, Data Lake, Azure Data Factory)",Global Technology Company @ Pune,6 - 10 years,25-30 Lacs P.A.,"['Pune', 'Bengaluru', 'Mumbai (All Areas)']","Minimum of 6 yrs of Data Engineering Exp\nMust be an expert in SQL, Data Lake, Azure Data Factory, Azure Synapse, ETL, Databricks\nMust be an expert in data modeling, writing complex queries in SQL\nAbility to convert SQL code to PySpark\n\nRequired Candidate profile\nExp with SQL, Python, data modelling, data warehousing & dimensional modelling concept\nFamiliarity with data governance, data security & Production deployments using Azure DevOps CICD pipelines.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Synapse', 'Pyspark', 'Azure Data factory', 'Data Lake', 'Azure Services', 'Databricks', 'ETL', 'SQL']",2025-06-11 05:27:38
Azure Data Engineer/Lead/Architect (5 - 20 Years) (Pan India Location),Allegis Group,5 - 10 years,Not Disclosed,[],"Azure Data Engineer/Lead/Architect (5 - 20 Years) (Pan India Location)\nJob Location : Hyderabad / Bangalore / Chennai / Kolkata / Noida/ Gurgaon / Pune / Indore / Mumbai\n\n\n5 -20 years of relevant hands on development experience. And 4+ years as Azure Data Engineering role\nProficient in Azure technologies like ADB, ADF, SQL(capability of writing complex SQL queries), ADB, PySpark, Python, Synapse, Delta Tables, Unity Catalog\nHands on in Python, PySpark or Spark SQL\nHands on in Azure Analytics and DevOps\nTaking part in Proof of Concepts (POCs) and pilot solutions preparation\nAbility to conduct data profiling, cataloguing, and mapping for technical design and construction of technical data flows\nExperience in business processing mapping of data and analytics solutions",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Analytics', 'Azure Data Engineering', 'Azure Databricks', 'Devops', 'Python', 'Azure Data Factory', 'Pyspark', 'Azure', 'Adb']",2025-06-11 05:27:40
Cloud Data Engineer,Acesoft,7 - 9 years,19-20 Lacs P.A.,['Bengaluru'],"Hi all,\nWe are hiring for the role Cloud Data Engineer\nExperience: 7 - 9 years\nLocation: Bangalore\nNotice Period: Immediate - 15 Days\nSkills:\nOverall, 7 to 9 years of experience in cloud data and analytics platforms such as AWS, Azure, or GCP\n• Including 3+ years experience with Azure cloud Analytical tools is a must\n• Including 5+ years of experience working with data & analytics concepts such as SQL, ETL, ELT, reporting and report building, data visualization, data lineage, data importing & exporting, and data warehousing\n• Including 3+ years of experience working with general IT concepts such as integrations, encryption, authentication & authorization, batch processing, real-time processing, CI/CD, automation\n• Advanced knowledge of cloud technologies and services, specifically around Azure Data Analytics tools\no Azure Functions (Compute)\no Azure Blob Storage (Storage)\no Azure Cosmos DB (Databases)\no Azure Synapse Analytics (Databases)\no Azure Data Factory (Analytics)\no Azure Synapse Serverless SQL Pools (Analytics)\no Azure Event Hubs (Analytics- Realtime data)\n• Strong coding skills in languages such as\no SQL\no Python\no PySpark\n• Experience in data streaming technologies such as Kafka or Azure Event Hubs\n• Experience in handling unstructured streaming data is highly desired\n• Knowledge of Business Intelligence Dimensional Modelling, Star Schemas, Slowly Changing Dimensions\n• Broad understanding of data engineering methodologies and tools, including Data warehousing, DevOps/DataOps, Data ingestion, ELT/ETL and Data visualization tools\n• Knowledge of database management systems, data modelling, and data warehousing best practices\n• Experience in software development on a team using Agile methodology\n• Knowledge of data governance and security practices\n\nIf you are interested drop your resume at mojesh.p@acesoftlabs.com\nCall: 9701971793",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Cloud Analytics', 'Azure Cloud', 'Data Warehousing', 'AWS', 'Python', 'Azure Data Analytics tools', 'authentication & authorization', 'PySpark', 'batch processing', 'Elt', 'sql', 'automation', 'encryption', 'Azure Cosmos DB', 'real-time processing', 'CI/CD', 'etl', 'integrations']",2025-06-11 05:27:41
"Senior Engineer, MS - Data Center Operations","NTT DATA, Inc.",2 - 5 years,Not Disclosed,['Mumbai'],"Additional Career Level Description:\nKnowledge and application:\nApplies learned techniques, as well as company policies and procedures to resolve a variety of issues.\nProblem solving:\nWorks on problems of moderate scope, often varied and nonroutine where analysis requires a review of a variety of factors.\nFocuses on providing standard professional advice and creating initial analysis for review.\nInteraction:\nBuilds productive internal/external working relationships to resolve mutual problems by collaborating on procedures or transactions.\nImpact:\nWork mainly impacts short term team performance and occasionally medium-term goals.\nSupports the achievement of goals through own personal effort, assessing own progress.\nAccountability:\nExercises some of own judgement and is responsible for meeting own targets, normally receiving little instruction on day-to-day work, general instructions on new assignments.\nManages own impact on cost and profitability.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Center Operations', 'system administration', 'VMware', 'technical support', 'active directory', 'windows administration', 'networking']",2025-06-11 05:27:43
Data Engineer- AWS and Databricks Expert,Acuity It Solutions,6 - 10 years,11-21 Lacs P.A.,['Bengaluru'],"Design and implement scalable data ingestion and transformation pipelines using Databricks and AWS.\nDevelop and optimize ETL/ELT workflows in PySpark and Spark SQL, ensuring performance and reliability, and use CI/CD tools.\n\nRequired Candidate profile\nExperience Required 6 to 9 years. Minimum 4+ years of experience with Databricks and AWS.Design and develop scalable ETL/ELT pipelines, PySpark SQL and Python. Immediate joiner to 30 days NP required.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ETL', 'AWS', 'Data Bricks', 'Pyspark', 'Python', 'SQL']",2025-06-11 05:27:44
"Mid Data Science & AIML, GenAI Lead/Engineer","NTT DATA, Inc.",3 - 6 years,Not Disclosed,['Bengaluru'],"Req ID: 312501\n\nWe are currently seeking a Mid Data Science & AIML, GenAI Lead/Engineer to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\nJob DutiesJob TitleData Science & AIML, GenAI Lead/Engineer\n\nKey Responsibilities:\n""¢ Develop and implement traditional machine learning algorithms.\n""¢ Deploy at least one model in a production environment.\n""¢ Write and maintain Python code for data science and machine learning projects.\n\nMinimum Skills RequiredPreferred Qualifications:\n""¢ Knowledge of Deep Learning (DL) techniques.\n""¢ Experience working with Generative AI (GenAI) and Large Language Models (LLM).\n""¢ Exposure to Langchain.\n\n""‹""‹""‹""‹""‹""‹""‹",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'artificial intelligence', 'deep learning', 'data science', 'algorithms', 'natural language processing', 'scikit-learn', 'dl', 'aiml', 'numpy', 'sql', 'tensorflow', 'r', 'predictive modeling', 'statistical modeling', 'machine learning algorithms', 'statistics']",2025-06-11 05:27:46
Azure or AWS Data Engineer,Cignex Datamatics,5 - 10 years,Not Disclosed,['Bengaluru'],"We are looking for a highly skilled Lead Data Engineer with expertise in Azure or AWS and Databricks to join our team. The ideal candidate will lead the design, development, and implementation of data engineering solutions, ensuring scalability, security, and efficiency in our data infrastructure. This role requires strong technical skills, and experience in managing large-scale data processing pipelines.\nKey Responsibilities:\nLead the design and development of scalable and reliable data pipelines using Azure Data Services or AWS Data Services and Databricks.\nArchitect, implement, and optimize ETL/ELT processes to process large volumes of structured and unstructured data.\nDevelop and maintain data models, data lakes, and data warehouses to support analytics and business intelligence needs.\nCollaborate with data scientists, analysts, and business stakeholders to ensure data availability and integrity.\nImplement and enforce data governance, security, and compliance best practices.\nOptimize and monitor performance of data processing frameworks (Spark, Databricks, etc.).\nAutomate and orchestrate data workflows using tools such as Apache Airflow, Azure Data Factory, AWS Step Functions, or Glue.\nGuide and mentor junior data engineers in best practices and modern data engineering techniques.\nMandatory Qualifications:\n5+ years of experience in data engineering\nStrong expertise in Azure Data Services (Azure Data Lake, Azure Synapse, Azure Data Factory) or AWS Data Services (S3, Redshift, Glue, Lambda, Step Functions, EMR).\nProficiency in Databricks and experience with Apache Spark for large-scale data processing.\nStrong programming skills in Python\nExperience working with SQL and NoSQL databases such as PostgreSQL, MySQL, DynamoDB, or CosmosDB.\nSolid understanding of data governance, security, and compliance (GDPR, HIPAA, etc.) is a plus.\nExperience with real-time streaming technologies such as Kafka, Kinesis, or Event Hubs is a plus.\nExcellent problem-solving skills and the ability to work in a fast-paced, agile environment.\nGood to have Qualifications:\nExperience with machine learning pipelines and MLOps.\nFamiliarity with data visualization and BI tools like Power BI, Tableau, or Looker.\nStrong communication and leadership skills to drive best practices across the team.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Postgresql', 'MySQL', 'HIPAA', 'Machine learning', 'Agile', 'Data processing', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-11 05:27:48
"Snowflake Data Engineer-Cortex AI, Aws / azure","Location - Kolkata, Hyderabad, Bangalore...",7 - 12 years,20-35 Lacs P.A.,"['Kolkata', 'Hyderabad', 'Bengaluru']","Skill set\nSnowflake, AWS, Cortex AI, Horizon Catalog\nor\nSnowflake, AWS, (Cortex AI or Horizon Catalog)\nor\nSnowflake, Azure, Cortex AI, Horizon Catalog\nOr\nSnowflake, Azure, (Cortex AI or Horizon Catalog)\nPreferred Qualifications:\nBachelors degree in Computer Science, Data Engineering, or a related field.\nExperience in data engineering, with at least 3 years of experience working with Snowflake.\nProven experience in Snowflake, Cortex AI/ Horizon Catalog focusing on data extraction, chatbot development, and Conversational AI.\nStrong proficiency in SQL, Python, and data modeling.\nExperience with data integration tools (e.g., Matillion, Talend, Informatica).\nKnowledge of cloud platforms such as AWS or Azure, or GCP.\nExcellent problem-solving skills, with a focus on data quality and performance optimization.\nStrong communication skills and the ability to work effectively in a cross-functional team.\nProficiency in using DBT's testing and documentation features to ensure the accuracy and reliability of data transformations.\nUnderstanding of data lineage and metadata management concepts, and ability to track and document data transformations using DBT's lineage capabilities.\nUnderstanding of software engineering best practices and ability to apply these principles to DBT development, including version control, code reviews, and automated testing.\nShould have experience building data ingestion pipeline.\nShould have experience with Snowflake utilities such as SnowSQL, SnowPipe, bulk copy, Snowpark, tables, Tasks, Streams, Time travel, Cloning, Optimizer, Metadata Manager, data sharing, stored procedures and UDFs, Snowsight.\nShould have good experience in implementing CDC or SCD type 2\nProficiency in working with Airflow or other workflow management tools for scheduling and managing ETL jobs.\nGood to have experience in repository tools like Github/Gitlab, Azure repo",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Kafka', 'horizon Catalog', 'cortex AI', 'Python', 'aws', 'azure']",2025-06-11 05:27:49
Data Engineering,Stack Digital,3 - 7 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Job Description\nDesign, develop, and optimize large-scale data processing pipelines using PySpark .\nWork with various Apache tools and frameworks (like Hadoop, Hive, HDFS, etc.) to ingest, transform, and manage large datasets.\nEnsure high performance and reliability of ETL jobs in production.\nCollaborate with Data Scientists, Analysts, and other stakeholders to understand data needs and deliver robust data solutions.\nImplement data quality checks and data lineage tracking for transparency and auditability.\nWork on data ingestion, transformation, and integration from multiple structured and unstructured sources.\nLeverage Apache NiFi for automated and repeatable data flow management (if applicable).\nWrite clean, efficient, and maintainable code in Python and Java .\nContribute to architectural decisions, performance tuning, and scalability planning.\nRequired Skills:\n5 7 years of experience.\nStrong hands-on experience with PySpark for distributed data processing.\nDeep understanding of Apache ecosystem (Hadoop, Hive, Spark, HDFS, etc.).\nSolid grasp of data warehousing , ETL principles , and data modeling .\nExperience working with large-scale datasets and performance optimization.\nFamiliarity with SQL and NoSQL databases.\nProficiency in Python and basic to intermediate knowledge of Java .\nExperience in using version control tools like Git and CI/CD pipelines.\nNice-to-Have Skills:\nWorking experience with Apache NiFi for data flow orchestration.\nExperience in building real-time streaming data pipelines .\nKnowledge of cloud platforms like AWS , Azure , or GCP .\nFamiliarity with containerization tools like Docker or orchestration tools like Kubernetes .\nSoft Skills:\nStrong analytical and problem-solving skills.\nExcellent communication and collaboration abilities.\nSelf-driven with the ability to work independently and as part of a team.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Version control', 'GIT', 'orchestration', 'Architecture', 'Data modeling', 'Analytical', 'Data processing', 'Data quality', 'Apache', 'SQL']",2025-06-11 05:27:52
Associate Data Engineer | Pricing | Full Time Contract 18 Months,Argus India Price Reporting Services,1 - 3 years,5-11 Lacs P.A.,['Mumbai (All Areas)'],"Associate Data Engineer - (Fixed Term Contract)\nMumbai\nJob Purpose:\nDue to the continued growth of the business and the importance of the data we use on a daily basis, we are currently looking for a for a junior data engineer to join our global Data team in Mumbai.\nYou will work closely with internal clients of the data team to support and maintain R (incl. R Shiny), Excel and database-based processes for gathering data, calculating prices and producing the reports and data feeds. The work also involves writing robust automated processes in R.\nThe team supports Argus key business processes every day, as such you will be required to work on a shift-based rota with other members of the team supporting the business until 8pm. Typically support hours run from 11pm to 8pm with each member of the team participating 2/3 times a week.\n\nKey Responsibilities:\nSupport and development of data processing systems\nClient support with queries relating to\nintegration of Argus data and metadata into client systems\ndata validation\nprovision of data\nData and systems support to Argus staff\nProject development\nMaintenance and development of existing systems\nmetadata modification\ndata cleansing\ndata checking\nRequired Skills and Experience:\nSignificant recent experience of developing tools using R (incl., R Shiny applications) in a commercial (work) environment.\nGood knowledge and experience of SQL, preferably in Oracle or MySQL, including stored procedures, functions, and triggers.\nExperience with version control systems (e.g., Git) and Unit Testing in R is required.\nAbility to work both as part of a team and autonomously\nExcellent communication skills.\n\nDesired Skills and Experience:\nBS degree in Computer Science, Mathematics, Business, Engineering or related field.\nExperience in visualisation techniques is desirable.\nAny experience working with energy markets and commodities data is highly desirable\nExperience developing production-grade scalable applications in R.\n\nPersonal Attributes:\nAbility to interact with non-technical people in plain language\nInnovative thinker with good problem-solving abilities and attention to detail\nNumerically proficient\nSelf-motivated with ability to work independently, prioritising tasks to meet deadlines\nCustomer service focused.\n\nBenefits:\nCompetitive salary\nFlexible Working Policy\nGroup healthcare scheme\n18 days annual leave\n8 days casual leave\nExtensive internal and external training",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['R', 'R Shiny', 'Unit Testing', 'Version Control Systems Svn', 'SQL']",2025-06-11 05:27:53
DBA DATA Engineer || 12 Lakhs CTC,Robotics Technologies,5 - 9 years,12 Lacs P.A.,['Hyderabad( Banjara hills )'],"Dear Candidate,\nWe are seeking a skilled and experienced DBA Data Engineer to join our growing data team. The ideal candidate will play a key role in designing, implementing, and maintaining our databases and data pipeline architecture. You will collaborate with software engineers, data analysts, and DevOps teams to ensure efficient data flow, data integrity, and optimal database performance across all systems. This role requires a strong foundation in database administration, SQL performance tuning, data modeling, and experience with both on-prem and cloud-based environments.\n\nRequirements:\nBachelors degree in Computer Science, Information Systems, or a related field.\n5+ years of experience in database administration and data engineering.\nProven expertise in RDBMS (Oracle, MySQL, and PostgreSQL) and NoSQL systems (MongoDB, Cassandra).\nExperience managing databases in cloud environments (AWS, Azure, or GCP).\nProficiency in ETL processes and tools (e.g., Apache NiFi, Talend, Informatica, AWS Glue).\nStrong experience with scripting languages such as Python, Bash, or PowerShell.\n\nDBA Data Engineer Roles & Responsibilities:\nDesign and maintain scalable and high-performance database architectures.\nMonitor and optimize database performance using tools like CloudWatch, Oracle Enterprise Manager, pgAdmin, Mongo Compass, or Dynatrace.\nDevelop and manage ETL/ELT pipelines to support business intelligence and analytics.\nEnsure data integrity and security through best practices in backup, recovery, and encryption.\nAutomate regular database maintenance tasks using scripting and scheduled jobs.\nImplement high availability, failover, and disaster recovery strategies.\nConduct performance tuning of queries, stored procedures, indexes, and table structures.\nCollaborate with DevOps to automate database deployments using CI/CD and IaC tools (e.g., Terraform, AWS CloudFormation).\nDesign and implement data models, including star/snowflake schemas for data warehousing.\nDocument data flows, data dictionaries, and database configurations.\nManage user access and security policies using IAM roles or database-native permissions.\nAnalyze existing data systems and propose modernization or migration plans (on-prem to cloud, SQL to NoSQL, etc.).\nUse AWS RDS, Amazon Redshift, Azure SQL Database, or Google BigQuery as needed.\nStay up-to-date with emerging database technologies and make recommendations.\n\nMust-Have Skills:\nDeep knowledge of SQL and database performance tuning.\nHands-on experience with database migrations and replication strategies.\nFamiliarity with data governance, data quality, and compliance frameworks (GDPR, HIPAA, etc.).\nStrong problem-solving and troubleshooting skills.\nExperience with data streaming platforms such as Apache Kafka, AWS Kinesis, or Apache Flink is a plus.\nExperience with data lake and data warehouse architectures.\nExcellent communication and documentation skills.\n\nSoft Skills:\nProblem-Solving: Ability to analyze complex problems and develop effective solutions.\nCommunication Skills: Strong verbal and written communication skills to effectively collaborate with cross-functional teams.\nAnalytical Thinking: Ability to think critically and analytically to solve technical challenges.\nTime Management: Capable of managing multiple tasks and deadlines in a fast-paced environment.\nAdaptability: Ability to quickly learn and adapt to new technologies and methodologies.\n\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database Administration', 'Database Migration', 'Dba Skills', 'High Availability', 'Db Administration', 'Hadr', 'Database Security', 'Disaster Recovery', 'Dr Testing', 'DR', 'Luw', 'Db Upgrade', 'Brtools', 'UDDI', 'Os Migration', 'Dbase', 'DBMS', 'IBM DB2', 'Db Migration', 'Disaster Recovery Planning', 'Db Dba', 'Backup And Recovery']",2025-06-11 05:27:54
Data Engineer Graph – Research Data and Analytics,Amgen Inc,2 - 4 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will be part Researchs Semantic Graph Team is seeking a qualified individual to design, build, and maintain solutions for scientific data that drive business decisions for Research. The successful candidate will construct scalable and high-performance data engineering solutions for extensive scientific datasets and collaborate with Research partners to address their data requirements. The ideal candidate should have experience in the pharmaceutical or biotech industry, leveraging their expertise in semantics, taxonomies, and linked data principles to ensure data harmonization and interoperability. Additionally, this individual should demonstrate robust technical skills, proficiency with data engineering technologies, and a thorough understanding of data architecture and ETL processes.\nRoles & Responsibilities:\nDesign, develop, and implement data pipelines, ETL/ELT processes, and data integration solutions\nTake ownership of data pipeline projects from inception to deployment, manage scope, timelines, and risks\nDevelop and maintain semantic data models for biopharma scientific data, data dictionaries, and other documentation to ensure data accuracy and consistency\nOptimize large datasets for query performance\nCollaborate with global multi-functional teams including research scientists to understand data requirements and design solutions that meet business needs\nImplement data security and privacy measures to protect sensitive data\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions\nCollaborate with Data Architects, Business SMEs, Software Engineers and Data Scientists to design and develop end-to-end data pipelines to meet fast paced business needs across geographic regions\nIdentify and resolve [complex] data-related challenges\nAdhere to standard processes for coding, testing, and designing reusable code/component\nExplore new tools and technologies that will help to improve ETL platform performance\nParticipate in sprint planning meetings and provide estimations on technical implementation\nMaintain comprehensive documentation of processes, systems, and solutions\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. T\nBasic Qualifications and Experience:\nDoctorate Degree OR Masters degree with 2- 4years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nBachelors degree with 4- 6years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nDiploma with 7- 9 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field\n\n\nPreferred Qualifications and Experience:\n4+ years of experience in designing and supporting biopharma scientific research data analytics (software platforms)\n\n\nFunctional Skills:\nMust-Have Skills:\nProficiency in SQL and Python for data engineering, test automation frameworks (pytest), and scripting tasks\nHands on experience with data technologies and platforms, such as Databricks, workflow orchestration, performance tuning on big data processing.\nExcellent problem-solving skills and the ability to work with large, complex datasets\n\n\nGood-to-Have Skills:\nA passion for tackling complex challenges in drug discovery with technology and data\nExperience with system administration skills, such as managing Linux and Windows servers, configuring network infrastructure, and automating tasks with shell scripting. Examples include setting up and maintaining virtual machines, troubleshooting server issues, and ensuring data security through regular updates and backups.\nSolid understanding of data modeling, data warehousing, and data integration concepts\nSolid experience using RDBMS (e.g. Oracle, MySQL, SQL server, PostgreSQL)\nKnowledge of cloud data platforms (AWS preferred)\nExperience with data visualization tools (e.g. Dash, Plotly, Spotfire)\nExperience with diagramming and collaboration tools such as Miro, Lucidchart or similar tools for process mapping and brainstorming\nExperience writing and maintaining user documentation in Confluence\nUnderstanding of data governance frameworks, tools, and standard processes\n\n\nProfessional Certifications:\nDatabricks Certified Data Engineer Professional preferred\n\n\nSoft Skills:\nExcellent critical-thinking and problem-solving skills\nGood communication and collaboration skills\nDemonstrated awareness of how to function in a team setting\nDemonstrated presentation skills",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analytics', 'PostgreSQL', 'MySQL', 'ETL', 'ELT', 'Oracle', 'SQL server', 'AWS']",2025-06-11 05:27:56
Snowflake Data Engineer / Database Lead,TechStar Group,9 - 14 years,15-20 Lacs P.A.,['Hyderabad'],"Job Description:\nSQL & Database Management: Deep knowledge of relational databases (PostgreSQL), cloud-hosted data platforms (AWS, Azure, GCP), and data warehouses like Snowflake.\nETL/ELT Tools: Experience with SnapLogic, StreamSets, or DBT for building and maintaining data pipelines. / ETL Tools Extensive Experience on data Pipelines\nData Modeling & Optimization: Strong understanding of data modeling, OLAP systems, query optimization, and performance tuning.\nCloud & Security: Familiarity with cloud platforms and SQL security techniques (e.g., data encryption, TDE).\nData Warehousing: Experience managing large datasets, data marts, and optimizing databases for performance.\nAgile & CI/CD: Knowledge of Agile methodologies and CI/CD automation tools.\n\n\nRole & responsibilities\nBuild the data pipeline for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and cloud database technologies.\nWork with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data needs.\nWork with data and analytics experts to strive for greater functionality in our data systems.\nAssemble large, complex data sets that meet functional / non-functional business requirements.\n– Ability to quickly analyze existing SQL code and make improvements to enhance performance, take advantage of new SQL features, close security gaps, and increase robustness and maintainability of the code.\n– Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery for greater scalability, etc.\n– Unit Test databases and perform bug fixes.\n– Develop best practices for database design and development activities.\n– Take on technical leadership responsibilities of database projects across various scrum teams.\nManage exploratory data analysis to support dashboard development (desirable)\n\n\nRequired Skills:\n– Strong experience in SQL with expertise in relational database(PostgreSQL preferrable cloud hosted in AWS/Azure/GCP) or any cloud-based Data Warehouse (like Snowflake, Azure Synapse).\n– Competence in data preparation and/or ETL/ELT tools like SnapLogic, StreamSets, DBT, etc. (preferably strong working experience in one or more) to build and maintain complex data pipelines and flows to handle large volume of data.\n– Understanding of data modelling techniques and working knowledge with OLAP systems\n– Deep knowledge of databases, data marts, data warehouse enterprise systems and handling of large datasets.\n– In-depth knowledge of ingestion techniques, data cleaning, de-dupe, etc.\n– Ability to fine tune report generating queries.\n– Solid understanding of normalization and denormalization of data, database exception handling, profiling queries, performance counters, debugging, database & query optimization techniques.\n– Understanding of index design and performance-tuning techniques\n– Familiarity with SQL security techniques such as data encryption at the column level, Transparent Data Encryption(TDE), signed stored procedures, and assignment of user permissions\n– Experience in understanding the source data from various platforms and mapping them into Entity Relationship Models (ER) for data integration and reporting(desirable).\n– Adhere to standards for all database e.g., Data Models, Data Architecture and Naming Conventions\n– Exposure to Source control like GIT, Azure DevOps\n– Understanding of Agile methodologies (Scrum, Itanban)\n– experience with NoSQL database to migrate data into other type of databases with real time replication (desirable).\n– Experience with CI/CD automation tools (desirable)\n– Programming language experience in Golang, Python, any programming language, Visualization tools (Power\nBI/Tableau) (desirable).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ETL', 'Snowflake Developer', 'SQL', 'Snowflake Sql', 'Snowflake Data Engineer', 'Snowsql', 'Snowflake', 'Schema', 'Data Engineer', 'Data Sharing', 'Snowpipe', 'Streams']",2025-06-11 05:27:58
Data Engineering Manager,Amgen Inc,3 - 5 years,Not Disclosed,['Hyderabad'],"We are seeking a seasoned Engineering Manager (Data Engineering) to lead the end-to-end management of enterprise data assets and operational data workflows. This role is critical in ensuring the availability, quality, consistency, and timeliness of data across platforms and functions, supporting analytics, reporting, compliance, and digital transformation initiatives. You will be responsible for the day-to-day data operations, manage a team of data professionals, and drive process excellence in data intake, transformation, validation, and delivery. You will work closely with cross-functional teams including data engineering, analytics, IT, governance, and business stakeholders to align operational data capabilities with enterprise needs.\nRoles & Responsibilities:\nLead and manage the enterprise data operations team, responsible for data ingestion, processing, validation, quality control, and publishing to various downstream systems.\nDefine and implement standard operating procedures for data lifecycle management, ensuring accuracy, completeness, and integrity of critical data assets.\nOversee and continuously improve daily operational workflows, including scheduling, monitoring, and troubleshooting data jobs across cloud and on-premise environments.\nEstablish and track key data operations metrics (SLAs, throughput, latency, data quality, incident resolution) and drive continuous improvements.\nPartner with data engineering and platform teams to optimize pipelines, support new data integrations, and ensure scalability and resilience of operational data flows.\nCollaborate with data governance, compliance, and security teams to maintain regulatory compliance, data privacy, and access controls.\nServe as the primary escalation point for data incidents and outages, ensuring rapid response and root cause analysis.\nBuild strong relationships with business and analytics teams to understand data consumption patterns, prioritize operational needs, and align with business objectives.\nDrive adoption of best practices for documentation, metadata, lineage, and change management across data operations processes.\nMentor and develop a high-performing team of data operations analysts and leads.\nFunctional Skills:\nMust-Have Skills:\nExperience managing a team of data engineers in biotech/pharma domain companies.\nExperience in designing and maintaining data pipelines and analytics solutions that extract, transform, and load data from multiple source systems.\nDemonstrated hands-on experience with cloud platforms (AWS) and the ability to architect cost-effective and scalable data solutions.\nExperience managing data workflows in cloud environments such as AWS, Azure, or GCP.\nStrong problem-solving skills with the ability to analyze complex data flow issues and implement sustainable solutions.\nWorking knowledge of SQL, Python, or scripting languages for process monitoring and automation.\nExperience collaborating with data engineering, analytics, IT operations, and business teams in a matrixed organization.\nFamiliarity with data governance, metadata management, access control, and regulatory requirements (e.g., GDPR, HIPAA, SOX).\nExcellent leadership, communication, and stakeholder engagement skills.\nWell versed with full stack development & DataOps automation, logging frameworks, and pipeline orchestration tools.\nStrong analytical and problem-solving skills to address complex data challenges.\nEffective communication and interpersonal skills to collaborate with cross-functional teams.\nGood-to-Have Skills:\nData Engineering Management experience in Biotech/Life Sciences/Pharma\nExperience using graph databases such as Stardog or Marklogic or Neo4J or Allegrograph, etc.\nEducation and Professional Certifications\nDoctorate Degree with 3-5 + years of experience in Computer Science, IT or related field\nOR\nMasters degree with 6 - 8 + years of experience in Computer Science, IT or related field\nOR\nBachelors degree with 10 - 12 + years of experience in Computer Science, IT or related field\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Neo4J', 'HIPAA', 'Stardog', 'Databricks', 'Marklogic', 'AWS', 'SOX', 'GDPR']",2025-06-11 05:27:59
CMMS Data Engineer,Idexcel,3 - 6 years,Not Disclosed,['Vadodara'],"Role & responsibilities\n\nProfessional Summary:\nExperience: Extensive experience in major oil & gas industry equipment including pumps, compressors, turbines, piping, storage tanks, and vessels. Proficient in understanding master data and Bill of Materials (BoM) structures.\nKnowledge: In-depth knowledge of equipment classification, codes, and characteristics. Skilled in reviewing materials classification, codes, and characteristics.",,,,"['SAP PM', 'Material Management', 'SAP MM', 'Data Management', 'CMMS', 'SAP', 'MDM', 'Master Data Management']",2025-06-11 05:28:01
Data Engineer (Azure),Reqroots,7 - 12 years,Not Disclosed,['Coimbatore'],"Data Engineer (Azure) Jobs | 7+ years | Coimbatore, Tamil Nadu(Full-time)\nJob Description\nPurpose:\nIT applications (Implementation and support): The employee should be able to develop ETL pipelines, data visualizations and analysis.\nMinimum Qualifications and Knowledge:\nBachelor s degree in computer science or information technologies.\nMinimum Experience:\n8+ years in Data Engineering (design, model and building ETL pipelines, data/lake house design and development)\n4+ years in Python and Spark (strong knowledge in PySpark, Pandas, Numpy)\n4+ year in Azure Data Factory and Databricks (strong Performance Optimization skills)\n8+ years in SQL\nJob-Specific Skills:\nSelf-motivated and committed to quality. Interpersonal and analytical skills.\nProblem solving skills.\nExcellent organization skills.\nRequired for 6-months, with the possibility of renewal based on project requirements\nFinance industry working experience is preferred (Insurance background is 01st preference).\nCandidates currently working in any region is fine ( Abu Dhabi/India)\nApplication Development:\nData modelling, development and deployment of end-to-end data analytics solutions in cloud environment - Azure.\nData processing including big data processing, integration, data modelling in line with the concepts of lake house medallion architecture.\nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\nCollaborate with vendor to create pipelines for Open Insurance project for four Line of Business Motor, Home, Medical & Travel\nCreate robust data models, data catalogue & data mesh architecture for selfservice analytics, facilitating user-friendly data exploration and reporting.\nData Governance - Ensure data quality, security, discoverability\nApplication Support:\nMaintain and execute performance tuning and data quality fixes.\nRequired Knowledge, Skills, and Abilities\nhttps://www.tebs.com\nWhom we are looking for\np Purpose: br IT applications (Implementation and support): The employee should be able to develop ETL pipelines, data visualizations and analysis. br /p p Minimum Qualifications and Knowledge: br Bachelor s degree in computer science or information technologies. br Minimum Experience: br 8+ years in Data Engineering (design, model and building ETL pipelines, data/lake house design and development) br 4+ years in Python and Spark (strong knowledge in PySpark, Pandas, Numpy) br 4+ year in Azure Data Factory and Databricks (strong Performance Optimization skills) br 8+ years in SQL br Job-Specific Skills: br Self-motivated and committed to quality. Interpersonal and analytical skills. br Problem solving skills. br Excellent organization skills. br Required for 6-months, with the possibility of renewal based on project requirements br Finance industry working experience is preferred (Insurance background is 01st preference). br Candidates currently working in any region is fine ( Abu Dhabi/India) br /P p Application Development: br Data modelling, development and deployment of end-to-end data analytics solutions in cloud environment - Azure. br Data processing including big data processing, integration, data modelling in line with the concepts of lake house medallion architecture. br Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. br Collaborate with vendor to create pipelines for Open Insurance project for four Line of Business Motor, Home, Medical & Travel br Create robust data models, data catalogue & data mesh architecture for selfservice analytics, facilitating user-friendly data exploration and reporting. br Data Governance - Ensure data quality, security, discoverability br /p p Application Support: br Maintain and execute performance tuning and data quality fixes. br /p",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Performance tuning', 'Root cause analysis', 'Application support', 'data governance', 'Application development', 'Data quality', 'Engineering Design', 'SQL', 'Python']",2025-06-11 05:28:02
Data Engineer - GCP,Egen Solutions,4 - 6 years,Not Disclosed,['Hyderabad'],"Job Overview:\n\nWe are looking for a skilled and motivated Data Engineer with strong experience in Python programming and Google Cloud Platform (GCP) to join our data engineering team. The ideal candidate will be responsible for designing, developing, and maintaining robust and scalable ETL (Extract, Transform, Load) data pipelines. The role involves working with various GCP services, implementing data ingestion and transformation logic, and ensuring data quality and consistency across systems.\nKey Responsibilities:\nDesign, develop, test, and maintain scalable ETL data pipelines using Python.\nWork extensively on Google Cloud Platform (GCP) services such as:\nDataflow for real-time and batch data processing\nCloud Functions for lightweight serverless compute\nBigQuery for data warehousing and analytics\nCloud Composer for orchestration of data workflows (based on Apache Airflow)\nGoogle Cloud Storage (GCS) for managing data at scale\nIAM for access control and security\nCloud Run for containerized applications\nPerform data ingestion from various sources and apply transformation and cleansing logic to ensure high-quality data delivery.\nImplement and enforce data quality checks, validation rules, and monitoring.\nCollaborate with data scientists, analysts, and other engineering teams to understand data needs and deliver efficient data solutions.\nManage version control using GitHub and participate in CI/CD pipeline deployments for data projects.\nWrite complex SQL queries for data extraction and validation from relational databases such as SQL Server, Oracle, or PostgreSQL.\nDocument pipeline designs, data flow diagrams, and operational support procedures.\nRequired Skills:\n4-6 years of hands-on experience in Python for backend or data engineering projects.\nStrong understanding and working experience with GCP cloud services (especially Dataflow, BigQuery, Cloud Functions, Cloud Composer, etc.).\nSolid understanding of data pipeline architecture, data integration, and transformation techniques.\nExperience in working with version control systems like GitHub and knowledge of CI/CD practices.\nStrong experience in SQL with at least one enterprise database (SQL Server, Oracle, PostgreSQL, etc.).\n\nGood to Have (Optional Skills):\nExperience working with Snowflake cloud data platform.\nHands-on knowledge of Databricks for big data processing and analytics.\nFamiliarity with Azure Data Factory (ADF) and other Azure data engineering tools.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Backend', 'Version control', 'Postgresql', 'Data quality', 'Oracle', 'Apache', 'Analytics', 'Monitoring', 'Python', 'Data extraction']",2025-06-11 05:28:04
AWS Data Engineer,Leading Client,1 - 3 years,Not Disclosed,['Chennai'],"Mandatory Skills:\nAWS, Python, SQL, spark, Airflow, SnowflakeResponsibilities\nCreate and manage cloud resources in AWS\nData ingestion from different data sources which exposes data using different technologies, such asRDBMS, REST HTTP API, flat files, Streams, and Time series data based on various proprietary systems. Implement data ingestion and processing with the help of Big Data technologies\nData processing/transformation using various technologies such as Spark and Cloud Services. You will need to understand your part of business logic and implement it using the language supported by the base data platform\nDevelop automated data quality check to make sure right data enters the platform and verifying the results of the calculations\nDevelop an infrastructure to collect, transform, combine and publish/distribute customer data.\nDefine process improvement opportunities to optimize data collection, insights and displays.\nEnsure data and results are accessible, scalable, efficient, accurate, complete and flexible\nIdentify and interpret trends and patterns from complex data sets\nConstruct a framework utilizing data visualization tools and techniques to present consolidated analytical and actionable results to relevant stakeholders.\nKey participant in regular Scrum ceremonies with the agile teams\nProficient at developing queries, writing reports and presenting findings\nMentor junior members and bring best industry practices",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AWS', 'hive', 'scala', 'amazon redshift', 'big data technologies', 'pyspark', 'apache pig', 'microservices', 'sql', 'java', 'spark', 'data ingestion', 'api', 'hadoop', 'big data', 'etl', 'hbase', 'rest', 'snowflake', 'cloud services', 'python', 'data processing', 'airflow', 'kafka', 'http', 'scrum', 'agile', 'sqoop']",2025-06-11 05:28:05
Lead Data Engineer (IDMC-CDI),Mchoovers Consulting,7 - 12 years,Not Disclosed,[],"Job Title: Lead Data Engineer (IDMC-CDI)\nPrimary Skills: SQL, IDMC (CDI), ETL Concepts\nBase Location: Gurugram (Delhi/NCR)\nMode of Work: Work from Home\nExperience: 8+ Years\n\nAbout the Job\nAre you someone with a deep understanding of ETL and a strong background in developing IDMC ETL-based solutions, capable of developing, documenting, unit testing, and maintaining ETL applications while delivering code that meets customer expectations? If yes, this opportunity could be your next career step.\n\nKey Responsibilities\nLead the design, development, and implementation of complex data integration solutions using Informatica Intelligent Data Management Cloud (IDMC).\nDevelop, document, unit test, and maintain high-quality ETL applications that meet customer expectations and adhere to best practices.\nProvide technical leadership and guidance to junior team members through code reviews, mentoring, and knowledge sharing.\nCollaborate closely with project managers, data analysts, and business stakeholders to understand requirements, define technical solutions, and ensure successful project delivery.\nParticipate in all phases of the software development lifecycle, including requirements gathering, design, development, testing, deployment, and maintenance.\nEnsure data quality and integrity throughout the data pipeline by implementing appropriate data validation and cleansing mechanisms.\nTroubleshoot and resolve complex data integration issues, ensuring timely resolution and minimal disruption to business operations.\nStay abreast of the latest advancements in data integration technologies and best practices.\nContribute to the continuous improvement of our data integration processes and methodologies.\n\nRequired Skills\n8+ years of experience in IT with a focus on Data Management.\n1+ year of experience as a Lead handling both technical and non-technical activities.\n4+ years of experience with Informatica products.\n2+ years of hands-on experience with Informatica IDMC.\nMust have implemented at least one full lifecycle IDMC projects.\nHands-on experience with Informatica CDI.\nMust have led large projects involving large teams.\nExperience integrating external business applications with Informatica IDMC using batch processes, API calls, and message queues.\nIn-depth knowledge of at least one relational databases (e.g., Oracle, SQL Server, MySQL).\nExperience with data warehousing concepts and methodologies (e.g., Kimball, Inmon).\nExcellent analytical and problem-solving skills, with the ability to identify and resolve complex data integration challenges.\nStrong communication and interpersonal skills, capable of effectively collaborating with cross-functional teams.\nExperience with Agile methodologies in a fast-paced, iterative environment.\n\nDesired Skills\nExperience with cloud platforms (e.g., AWS, Azure, GCP) and cloud-based data integration tools.\nKnowledge of data quality tools and techniques.\nExperience with scripting languages (e.g., Python, Shell).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['idmc', 'ETL Tool', 'SQL', 'Cdi']",2025-06-11 05:28:07
Analytics Data science and IOT Engineer,Hexaware Technologies,4 - 6 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","Job Description:\n\nMinimum 2 years of hands-on experience in Generative AI (GenAI), including working with various LLMs, prompt engineering, fine-tuning, Retrieval-Augmented Generation (RAG), and deploying GenAI solutions in production environments.\nAt least 3 years of experience in Python and SQL, with strong fundamentals in data manipulation and backend logic.",,,,"['Generative Ai', 'Python', 'Aws Sagemaker', 'Retrieval Augmented Generation', 'SQL']",2025-06-11 05:28:09
Data Engineer (AWS Databricks),Fortune India 500 IT Services Firm,5 - 7 years,15-22.5 Lacs P.A.,['Chennai'],"Role & responsibilities :\n\nJob Description:\n\nPrimarily looking for a Data Engineer (AWS) with expertise in processing data pipelines using Data bricks, PySpark SQL on Cloud distributions like AWS\nMust have AWS Data bricks ,Good-to-have PySpark, Snowflake, Talend\n\nRequirements-\n\n• Candidate must be experienced working in projects involving\n• Other ideal qualifications include experiences in\n• Primarily looking for a data engineer with expertise in processing data pipelines using Databricks Spark SQL on Hadoop distributions like AWS EMR Data\nbricks Cloudera etc.\n• Should be very proficient in doing large scale data operations using Databricks and overall very comfortable using Python\n• Familiarity with AWS compute storage and IAM concepts\n• Experience in working with S3 Data Lake as the storage tier\n\n• Any ETL background Talend AWS Glue etc. is a plus but not required\n• Cloud Warehouse experience Snowflake etc. is a huge plus\n• Carefully evaluates alternative risks and solutions before taking action.\n• Optimizes the use of all available resources\n• Develops solutions to meet business needs that reflect a clear understanding of the objectives practices and procedures of the corporation department and business unit\n\n• Skills\n• Hands on experience on Databricks Spark SQL AWS Cloud platform especially S3 EMR Databricks Cloudera etc.\n• Experience on Shell scripting\n• Exceptionally strong analytical and problem-solving skills\n• Relevant experience with ETL methods and with retrieving data from dimensional data models and data warehouses\n• Strong experience with relational databases and data access methods especially SQL\n• Excellent collaboration and cross functional leadership skills\n• Excellent communication skills both written and verbal\n• Ability to manage multiple initiatives and priorities in a fast-paced collaborative environment\n• Ability to leverage data assets to respond to complex questions that require timely answers\n• has working knowledge on migrating relational and dimensional databases on AWS Cloud platform\nSkills\nMandatory Skills: Apache Spark, Databricks, Java, Python, Scala, Spark SQL.\n\n\nNote : Need only Immediate joiners/ Serving notice period.\n\nInterested candidates can apply.\n\nRegards,\nHR Manager",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineer', 'Aws Databricks', 'SQL', 'Data Engineering', 'Azure Databricks', 'ETL', 'Talend', 'Python']",2025-06-11 05:28:11
Azure Data Engineer,Tek Ninjas,4 - 9 years,Not Disclosed,[],"he JD for the Azure DE + Palantir Resource is as below:\nMandatory - Strong proficiency in python, SQL, pyspark, Azure databricks, Azure Data Factory, strong communication skills as part of the Azure Data Engineer Role\nAdded to the Data engineering experience, the must have for Palantir are:\nRequired (hands-on resource who can demonstrate this during the interview)\no             Data Integration: building pipelines, python transform & source types reference, data connection, codes & repos\no             Model Integration: models, business logic, templated analysis & reports\no             Ontology Manager: object types, functions, object views, actions, permissions\no             Application Building:  Workshop, writeback, advanced actions\no             Security: data foundation, data protection & governance, restricted access & restricted views\no             AIP",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Palantir', 'Azure Databricks', 'SQL', 'Python', 'Pyspark', 'Data Bricks']",2025-06-11 05:28:12
Data Engineer with GCP,Egen (Formerly SpringML),4 - 6 years,Not Disclosed,['Hyderabad( Nanakramguda )'],"Job Overview:\n\nWe are looking for a skilled and motivated Data Engineer with strong experience in Python programming and Google Cloud Platform (GCP) to join our data engineering team. The ideal candidate will be responsible for designing, developing, and maintaining robust and scalable ETL (Extract, Transform, Load) data pipelines. The role involves working with various GCP services, implementing data ingestion and transformation logic, and ensuring data quality and consistency across systems.",,,,"['GCP', 'Python', 'Azure Data Factory', 'Cloud Functions', 'IAM', 'Bigquery', 'Snowflake', 'Google Cloud Storage', 'SQL Server', 'Oracle', 'Data Integration']",2025-06-11 05:28:14
Lead Data Engineer,Wiser Solutions,10 - 12 years,Not Disclosed,['Mysuru'],"Job Description\nWhen looking to buy a product, whether it is in a brick and mortar store or online, it can be hard enough to find one that not only has the characteristics you are looking for but is also at a price that you are willing to pay. It can also be especially frustrating when you finally find one, but it is out of stock. Likewise, brands and retailers can have a difficult time getting the visibility they need to ensure you have the most seamless experience as possible in selecting their product. We at Wiser believe that shoppers should have this seamless experience, and we want to do that by providing the brands and retailers the visibility they need to make that belief a reality.\nOur goal is to solve a messy problem elegantly and cost effectively. Our job is to collect, categorize, and analyze lots of structured and semi-structured data from lots of different places every day (whether it s 20 million+ products from 500+ websites or data collected from over 300,000 brick and mortar stores across the country). We help our customers be more competitive by discovering interesting patterns in this data they can use to their advantage, while being uniquely positioned to be able to do this across both online and instore.\nWe are looking for a lead-level software engineer to lead the charge on a team of like-minded individuals responsible for developing the data architecture that powers our data collection process and analytics platform. If you have a passion for optimization, scaling, and integration challenges, this may be the role for you.\nWhat You Will Do\nThink like our customers - you will work with product and engineering leaders to define data solutions that support customers business practices.\nDesign/develop/extend our data pipeline services and architecture to implement your solutions - you will be collaborating on some of the most important and complex parts of our system that form the foundation for the business value our organization provides\nFoster team growth - provide mentorship to both junior team members and evangelizing expertise to those on others.\nImprove the quality of our solutions - help to build enduring trust within our organization and amongst our customers by ensuring high quality standards of the data we manage\nOwn your work - you will take responsibility to shepherd your projects from idea through delivery into production\nBring new ideas to the table - some of our best innovations originate within the team\nTechnologies We Use\nLanguages: SQL, Python\nInfrastructure: AWS, Docker, Kubernetes, Apache Airflow, Apache Spark, Apache Kafka, Terraform\nDatabases: Snowflake, Trino/Starburst, Redshift, MongoDB, Postgres, MySQL\nOthers: Tableau (as a business intelligence solution)\n\n\nQualifications\nBachelors/Master s degree in Computer Science or relevant technical degree\n10+ years of professional software engineering experience\nStrong proficiency with data languages such as Python a",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Linux', 'MySQL', 'Agile', 'Data structures', 'OLAP', 'Apache', 'Business intelligence', 'SQL', 'Python']",2025-06-11 05:28:15
Manager Data Engineering,Renew,6 - 11 years,Not Disclosed,['Haryana'],"About Company\nJob Description\nKey responsibilities:\n1. Understand, implement, and automate ETL pipelines with better industry standards \n2. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, design infrastructure for greater scalability, etc \n3. Developing, integrating, testing, and maintaining existing and new applications \n4. Design, and create data pipelines (data lake / data warehouses) for real world energy analytical solutions \n5. Expert-level proficiency in Python (preferred) for automating everyday tasks \n6. Strong understanding and experience in distributed computing frameworks, particularly Spark, Spark-SQL, Kafka, Spark Streaming, Hive, Azure Databricks etc \n7. Limited experience in using other leading cloud platforms preferably Azure. \n8. Hands on experience on Azure data factory, logic app, Analysis service, Azure blob storage etc.\n9. Ability to work in a team in an agile setting, familiarity with JIRA and clear understanding of how Git works\n10. Must have 5-7 years of experience",Industry Type: Power,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'scala', 'data warehousing', 'azure data factory', 'distributed computing', 'sql', 'cloud', 'spark streaming', 'spark', 'gcp', 'hadoop', 'etl', 'big data', 'data lake', 'hbase', 'jira', 'azure databricks', 'python', 'oozie', 'microsoft azure', 'warehouse', 'data engineering', 'sql server', 'framework', 'mapreduce', 'kafka', 'hdfs', 'sqoop', 'aws']",2025-06-11 05:28:17
Urgent Hiring For Data Engineer with Product based MNC Pune,Peoplefy,7 - 12 years,Not Disclosed,['Pune'],Greetings from Peoplefy Infosolutions !!!\n\nWe are hiring for one of our reputed MNC client based in Pune.\nWe are looking for candidates with 7 + years of experience in below skills -\n\nPrimary skills :\nUnderstanding of AI ML in DE\nPython\nData Engineers\nDatabase -Big query or Snowflake\n\n\nInterested candidates for above position kindly share your CVs on chitralekha.so@peoplefy.com with below details -\n\nExperience :\nCTC :\nExpected CTC :\nNotice Period :\nLocation :,Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Aiml', 'ETL', 'Python', 'Bigquery', 'AWS']",2025-06-11 05:28:19
Azure Data Engineer,Leading Client,3 - 6 years,Not Disclosed,['Chennai'],Azure Data Factory\nAzure Databricks\nAzure SQL database\nSynapse Analytics\nLogic App\nAzure Functions\nAzure Analysis Service\nActive Directory\nAzure Devops\nPython\nPyspark,Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'azure databricks', 'python', 'ssas', 'microsoft azure', 'pyspark', 'power bi', 'data warehousing', 'azure data factory', 'azure analysis services', 'azure devops', 'sql server', 'sql', 'azure functions', 'sql azure', 'active directory', 'azure analysis', 'ssrs', 'dax', 'ssis', 'etl', 'msbi', 'sql database']",2025-06-11 05:28:20
Snowflake Data Engineer,Randomtrees,6 - 11 years,Not Disclosed,['Chennai'],"Data Architect/Engineer and implement data solutions across Retail industry(SCM, Marketing, Sales, and Customer Service, using technologies such as DBT, Snowflake, and Azure/AWS/GCP.\nDesign and optimize data pipelines that integrate various data sources (1st party, 3rd party, operational) to support business intelligence and advanced analytics.\nDevelop data models and data flows that enable personalized customer experiences and support omnichannel marketing and customer engagement.\nLead efforts to ensure data governance, data quality, and data security, adhering to compliance with regulations such as and .",,,,"['Snowflake', 'Data Build Tool', 'Data Warehousing', 'Data Modeling', 'SQL', 'Azure', 'GCP', 'DBT', 'ETL', 'AWS', 'Python']",2025-06-11 05:28:22
Gcp Data Engineer,One Of the MNC company,6 - 8 years,Not Disclosed,"['Hyderabad', 'Pune', 'Mumbai (All Areas)']","Role & responsibilities\n\nDevelop, implement, and optimize ETL/ELT pipelines for processing large datasets efficiently.\n• Work extensively with BigQuery for data processing, querying, and optimization.\n• Utilize Cloud Storage, Cloud Logging, Dataproc, and Pub/Sub for data ingestion, storage, and event-driven processing.\n• Perform performance tuning and testing of the ELT platform to ensure high efficiency and scalability.\n• Debug technical issues, perform root cause analysis, and provide solutions for production incidents.\n• Ensure data quality, accuracy, and integrity across data pipelines.\n• Collaborate with cross-functional teams to define technical requirements and deliver solutions.\n• Work independently on assigned tasks while maintaining high levels of productivity and efficiency.\nSkills Required:\n• Proficiency in SQL and PL/SQL for querying and manipulating data.\n• Experience in Python for data processing and automation.\n• Hands-on experience with Google Cloud Platform (GCP), particularly:\no BigQuery (must-have)\no Cloud Storage\no Cloud Logging\no Dataproc\no Pub/Sub\n• Experience with GitHub and CI/CD pipelines for automation and deployment.\n• Performance tuning and performance testing of ELT processes.\n• Strong analytical and debugging skills to resolve data and pipeline issues efficiently.\n• Self-motivated and able to work independently as an individual contributor.\n• Good understanding of data modeling, database design, and data warehousing concepts.\n\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GCP', 'Bigquery', 'ETL', 'PL/SQL', 'SQL']",2025-06-11 05:28:24
Azure Data Engineer - Remote,Software Company,4 - 8 years,8-13 Lacs P.A.,[],"Azure Cloud Technologies, Azure Data Factory, Azure Databricks (Advance Knowledge), PySpark, CI/CD Pipeline (Jenkins, GitLab CVCD or Azure DevOps), Data Ingestion, SOL\ndesigning, developing, & optimizing scalable data solutions.\n\nRequired Candidate profile\nAzure Databricks, Azure Data Factory expertise, PySpark proficiency, Big Data CI/CD, Troubleshoot, Jenkins, Gitlab CI/ CD, Data Pipeline Development & Deployment",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['Azure Develops', 'Azure Data Engineer', 'Azure Data Factory', 'Jenkins', 'Azure Cloud Technologies', 'PySpark', 'Azure Databricks', 'GitLab CI/CD', 'Data Injection, SOL', 'GitLab CVCD']",2025-06-11 05:28:25
Azure Data Engineers For Pune IT Companies urgent,International IT Companies,3 - 8 years,9-16 Lacs P.A.,['Pune'],"We are looking for a skilled Azure Data Engineer to design, develop, optimize data pipelines for following\n1, SQL+ETL+AZURE+Python+Pyspark+Databricks\n2, SQL+ADF+ Azure\n3, SQL+Python+Pyspark\n- Strong proficiency in SQL for data manipulation querying\n\nRequired Candidate profile\n- Python and PySpark for data engineering tasks.\n- Exp with Databricks for big data processing analytics.\n- Knowledge of data modeling, warehousing, governance.\n- CI/CD pipelines for data deployment.\n\nPerks and benefits\nPerks and Benefits",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure', 'PySpark', 'SQL', 'ETL', 'Python', 'Data Transformation', 'Business Intelligence', 'ADF', 'Data Engineering', 'Data Pipelines', 'Big Data', 'AI', 'Machine Learning', 'Analytics', 'Cloud Computing', 'Data Security', 'Databricks', 'Data Governance']",2025-06-11 05:28:27
DBA DATA Engineer || 12 Lakhs CTC,Robotics Technologies,6 - 9 years,12 Lacs P.A.,['Hyderabad( Banjara hills )'],"Dear Candidate,\nWe are seeking a skilled and experienced DBA Data Engineer to join our growing data team. The ideal candidate will play a key role in designing, implementing, and maintaining our databases and data pipeline architecture. You will collaborate with software engineers, data analysts, and DevOps teams to ensure efficient data flow, data integrity, and optimal database performance across all systems. This role requires a strong foundation in database administration, SQL performance tuning, data modeling, and experience with both on-prem and cloud-based environments.\n\nRequirements:\nBachelors degree in Computer Science, Information Systems, or a related field.\n6+ years of experience in database administration and data engineering.\nProven expertise in RDBMS (Oracle, MySQL, and PostgreSQL) and NoSQL systems (MongoDB, Cassandra).\nExperience managing databases in cloud environments (AWS, Azure, or GCP).\nProficiency in ETL processes and tools (e.g., Apache NiFi, Talend, Informatica, AWS Glue).\nStrong experience with scripting languages such as Python, Bash, or PowerShell.\n\nDBA Data Engineer Roles & Responsibilities:\nDesign and maintain scalable and high-performance database architectures.\nMonitor and optimize database performance using tools like CloudWatch, Oracle Enterprise Manager, pgAdmin, Mongo Compass, or Dynatrace.\nDevelop and manage ETL/ELT pipelines to support business intelligence and analytics.\nEnsure data integrity and security through best practices in backup, recovery, and encryption.\nAutomate regular database maintenance tasks using scripting and scheduled jobs.\nImplement high availability, failover, and disaster recovery strategies.\nConduct performance tuning of queries, stored procedures, indexes, and table structures.\nCollaborate with DevOps to automate database deployments using CI/CD and IaC tools (e.g., Terraform, AWS CloudFormation).\nDesign and implement data models, including star/snowflake schemas for data warehousing.\nDocument data flows, data dictionaries, and database configurations.\nManage user access and security policies using IAM roles or database-native permissions.\nAnalyze existing data systems and propose modernization or migration plans (on-prem to cloud, SQL to NoSQL, etc.).\nUse AWS RDS, Amazon Redshift, Azure SQL Database, or Google BigQuery as needed.\nStay up-to-date with emerging database technologies and make recommendations.\n\nMust-Have Skills:\nDeep knowledge of SQL and database performance tuning.\nHands-on experience with database migrations and replication strategies.\nFamiliarity with data governance, data quality, and compliance frameworks (GDPR, HIPAA, etc.).\nStrong problem-solving and troubleshooting skills.\nExperience with data streaming platforms such as Apache Kafka, AWS Kinesis, or Apache Flink is a plus.\nExperience with data lake and data warehouse architectures.\nExcellent communication and documentation skills.\n\nSoft Skills:\nProblem-Solving: Ability to analyze complex problems and develop effective solutions.\nCommunication Skills: Strong verbal and written communication skills to effectively collaborate with cross-functional teams.\nAnalytical Thinking: Ability to think critically and analytically to solve technical challenges.\nTime Management: Capable of managing multiple tasks and deadlines in a fast-paced environment.\nAdaptability: Ability to quickly learn and adapt to new technologies and methodologies.\n\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database Administration', 'Database Migration', 'Dba Skills', 'High Availability', 'Db Administration', 'Hadr', 'Database Security', 'Disaster Recovery', 'Dr Testing', 'DR', 'Luw', 'Db Upgrade', 'Brtools', 'UDDI', 'Os Migration', 'Dbase', 'DBMS', 'IBM DB2', 'Db Migration', 'Disaster Recovery Planning', 'Db Dba', 'Backup And Recovery']",2025-06-11 05:28:28
Cloud Data Engineer,Vesz Consultancy Services,5 - 10 years,9-15.6 Lacs P.A.,['Chennai'],"SQL, Python, Spark\nAWS Glue, Lambda, Step Functions, Azure Data Factory / Data bricks\nData validation, transformation, and quality assurance.\nBuilding and maintaining automated data pipelines, for data integrity\nWorking with large datasets\nGood Comm.",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Cloud', 'Spark', 'AWS', 'Python', 'Jenkins', 'Github', 'Postgresql']",2025-06-11 05:28:30
Snowflake Data Engineer,Prudent Globaltech Solutions,7 - 12 years,16-27.5 Lacs P.A.,['Hyderabad( Madhapur )'],"Job Description Data Engineer\nWe are seeking a highly skilled Data Engineer with extensive experience in Snowflake, Data Build Tool (dbt), Snaplogic, SQL Server, PostgreSQL, Azure Data Factory, and other ETL tools. The ideal candidate will have a strong ability to optimize SQL queries and a good working knowledge of Python. A positive attitude and excellent teamwork skills are essential.\n\nRole & responsibilities\nData Pipeline Development: Design, develop, and maintain scalable data pipelines using Snowflake, DBT, Snaplogic, and ETL tools.",,,,"['Snowflake', 'Azure Data Factory', 'ADF', 'Data Engineer', 'ETL', 'SQL']",2025-06-11 05:28:31
Openings For Data Engineer,Creative Solutions,5 - 10 years,4-9 Lacs P.A.,['Hyderabad'],"Skills: ADF,ETL,Rest API\nMicrosoft Power Platform / Snowflake Certification is a Plus\nPower Bi / Talend and Integration\nPower Apps\nSAP S4 Hana Abap,Odata Rest and Soap\nJob LOcation : Hyderabad",Industry Type: Software Product,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['ADF', 'power APPS', 'Azure Data Factory', 'Power Bi', 'Sap S4 Hana', 'ETL DEveloper', 'Talend']",2025-06-11 05:28:33
Aws Data Engineer,Binary Infoways,5 - 8 years,18-30 Lacs P.A.,['Hyderabad'],"AWS Data Engineer with Glue, Terraform, Business Intelligence (Tableau) development\n* Design, develop & maintain AWS data pipelines using Glue, Lambda & Redshift\n* Collaborate with BI team on ETL processes & dashboard creation with Tableau",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Tableau', 'Glue', 'Terraform', 'AWS', 'Business Intelligence', 'Ci/Cd', 'Aws Glue', 'GIT', 'Aws Lambda', 'Amazon Redshift', 'Redshift Aws', 'Spark', 'Splunk', 'Kubernetes']",2025-06-11 05:28:34
Data Engineer 3,Vichara Technologies,7 - 12 years,30-40 Lacs P.A.,"['Pune', 'Bengaluru', 'Delhi / NCR']","Support enhancements to the MDM platform\nDevelop pipelines using snowflake python SQL and airflow\nTrack System Performance\nTroubleshoot issues\nResolve production issues\n\nRequired Candidate profile\n5+ years of hands on expert level Snowflake, Python, orchestration tools like Airflow\nGood understanding of investment domain\nExperience with dbt, Cloud experience (AWS, Azure)\nDevOps",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Airflow', 'SQL Server', 'Snowflake', 'dbt', 'Python', 'Pyspark', 'Markit Edm', 'Data Engineering', 'Github', 'SQL Development', 'Fast Api', 'Semarchy', 'Aws Glue', 'GIT', 'Orchestration Framework', 'MDM', 'Etl Development', 'Data Modeling', 'ETL', 'React.Js', 'flask', 'Database Development', 'SAP MDM']",2025-06-11 05:28:36
Python Developer/ Python Data Engineer (Python+ ETL+ Pandas),Atyeti,5 - 8 years,Not Disclosed,['Hyderabad'],"Role & responsibilities\nB.Tech or M.Tech in Computer Science, or equivalent experience.\n5+ years of experience working professionally as a Python Software Developer.\nOrganized, self-directed, and resourceful.\nExcellent written and verbal communication skills.\nExpert in python & pandas.\nExperience in building data pipelines, ETL and ELT processes.",,,,"['Pandas', 'ETL', 'Python', 'SQL']",2025-06-11 05:28:38
Analytics Data Engineer (Infrastructure as Code),Unison Consulting,2 - 4 years,Not Disclosed,['Mumbai'],"IAC\ndeploy scalable, secure, and high-performing Snowflake environments in line with data governance and security in palce using Terraform and other automation scripit\nAutomate infrastructure provisioning, testing, and deployment for seamless operations.\nStrong SQL & DBT Expertise\nExperience building and maintaining scalable data models in DBT .\nProficient in modular SQL , Jinja templating , testing strategies, and DBT best practices.\nData Warehouse Proficiency\nHands-on experience with Snowflake including:\nDimensional and data vault modeling (star/snowflake schemas)\nPerformance optimization and query tuning\nRole-based access and security management\nData Pipeline & Integration Tools\nExperience with Kafka (or similar event streaming tools) for ingesting real-time data.\nFamiliarity with SnapLogic for ETL/ELT workflow design, orchestration, and monitoring.\nVersion Control & Automation\nProficient in Git and GitHub for code versioning and collaboration.\nExperience with GitHub Actions or other CI/CD tools to automate DBT model testing, deployment, and documentation updates.\nData Quality & Governance\nStrong understanding of data validation, testing (e.g., dbt tests), and lineage tracking.\nEmphasis on maintaining data trust across pipelines and models.\nStakeholder Management\nPartner with business and technical stakeholders to define data needs and deliver insights.\nAbility to explain complex data concepts in clear, non-technical terms.\nDocumentation & Communication\nMaintain clear documentation for models, metrics, and data transformations (using DBT docs or similar).\nStrong verbal and written communication skills; able to work cross-functionally across teams.\nProblem-Solving & Ownership\nProactive in identifying and resolving data gaps or issues.\nSelf-starter with a continuous improvement mindset and a focus on delivering business value through data.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Version control', 'GIT', 'Security management', 'Workflow', 'Data quality', 'Analytics', 'Monitoring', 'Testing', 'SQL']",2025-06-11 05:28:40
Azure Data Engineer,Cognitio Analytics,3 - 8 years,Not Disclosed,['Gurugram'],"Sr. Data Scientist\nGurugram (Hybrid)\n3 To 6 years\n+ Job Description\nApply\nIdeal qualifications, skills and experiences we are looking for are:\nWe are actively seeking a talented and results-driven Data Scientist to join our team and take ownership of deliverables through the power of data analytics and insights.\nYour contributions will be instrumental in making data-informed decisions, identifying growth opportunities, and propelling our organization to new levels of success.\nDoctorate/Master s/bachelor s degree in data science, Statistics, Computer Science, Mathematics, Economics, commerce or a related field.\nMinimum of 3 years of experience working as a Data Scientist or in a similar analytical role, with experience leading data science projects and teams.\nExperience in Healthcare domain with exposure to clinical operations, financial, risk rating, fraud, digital, sales and marketing, and wellness, e-commerce or the ed tech industry is a plus.\nExpertise in programming languages such as SQL, Python/PySpark and proficiency with data manipulation, analysis, and visualization libraries (e.g., pandas, NumPy, Matplotlib, seaborn).\nVery strong python and exceptional with pandas, NumPy, advanced python (pytest, class, inheritance, docstrings).\nDeep understanding of machine learning algorithms, model evaluation, and feature engineering. Experience with frameworks like scikit-learn, TensorFlow, or Py torch.",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'data science', 'Analytical', 'Financial risk', 'Machine learning', 'Programming', 'Healthcare', 'Wellness', 'SQL', 'Python']",2025-06-11 05:28:41
Data Security and Privacy Engineer,Finastra,3 - 8 years,Not Disclosed,['Bengaluru'],"What will you contribute?\nAs a Data Security and Privacy Engineer, you will be part of the Information Security team, reporting to the Chief Data Security Architect. Your role involves governing and protecting Finastra s product and customer data. You will collaborate with Security, Legal, Product Development, Data Engineering, and other teams to classify data and ensure compliance with confidentiality, integrity, availability, and data privacy requirements. You will also enhance the data governance framework, policies, processes, and standards, and handle tasks such as data classification, asset registration, enterprise data security, product risk assessments, and risk remediation. Additionally, you will develop process automation solutions for repeatable tasks.\nResponsibilities & Deliverables\nComplete data governance tasks and maintain due diligence records, ensuring compliance with policies, standards, and regulations.\nConduct data classification reviews, identify data minimization opportunities, prescribe de-identification methods, and track risk remediation.\nReview asset metadata records for anomalies and guide asset owners on corrections.\nCoordinate with stakeholders to achieve data governance framework goals, including data ownership, stewardship, dictionaries, flow diagrams, handling methods, usage rights, metadata management, and regulatory compliance.\nConduct training and awareness on data governance and protection standards and procedures.\nDevelop metrics and reports for continued compliance with the data governance framework and company standards.\nAssist with requirements, implementation, integration, and training for new data security tools, standards, and processes.\nIndependently carry out assignments, coordinate work with others, and keep team members informed of project statuses and risks.\nRequired Experience\nBachelor s or advanced degree in Information Technology or related field and 3+ years of professional experience in Data Governance, Data Security, or a similar role.\nStrong organizational skills and motivation for self-improvement and achieving results.\nExcellent verbal and written communication skills, with the ability to effectively communicate with technical personnel, business stakeholders, and management.\nUnderstanding of key data governance concepts and data management practices.\nFamiliarity with common data privacy concepts and compliance methodologies.\nAbility to make independent judgments and accurately identify risks.\nFamiliarity with API calls, SQL queries and practical scripting/coding experience (e.g., Python, PowerShell).\nNice to Haves\nCertification as a Certified Data Management Professional (CDMP), Certified Information Privacy Technologist (CIPT), or other Security certification (CISSP, CCSP, CISA).\nExperience with JIRA, ServiceNow, Power BI, and data management software.\nExposure to cloud-native application hosting, containers, and APIs.\nWe are proud to offer a range of incentives to our employees worldwide. These benefits are available to everyone, regardless of grade, and reflect the values we uphold:\n\nFlexibility: Enjoy unlimited vacation, based on your location and business priorities. Hybrid working arrangements, and inclusive policies such as paid time off for voting, bereavement, and sick leave.\nWell-being: Access confidential one-on-one therapy through our Employee Assistance Program, unlimited personalized coaching via our coaching app, and access to our Gather Groups for emotional and mental support.\nMedical, life & disability insurance, retirement plan, lifestyle and other benefits*\nESG: Benefit from paid time off for volunteering and donation matching.\nDEI: Participate in multiple DE&I groups for . We learn from one another, embrace and celebrate our differences, and create an environment where everyone feels safe to be themselves.\nBe unique, be exceptional, and help us make a difference at Finastra!",Industry Type: Financial Services,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Process automation', 'Career development', 'Due diligence', 'Usage', 'Data management', 'Cisa', 'Coding', 'Information security', 'Information technology', 'Financial services']",2025-06-11 05:28:43
MDM Associate Data Engineer,Horizon Therapeutics,1 - 8 years,Not Disclosed,['Hyderabad'],"Career Category Information Systems Job Description\nABOUT AMGEN\nAmgen harnesses the best of biology and technology to fight the world s toughest diseases, and make people s lives easier, fuller and longer. We discover, develop, manufacture and deliver innovative medicines to help millions of patients. Amgen helped establish the biotechnology industry more than 40 years ago and remains on the cutting-edge of innovation, using technology and human genetic data to push beyond what s known today.\nABOUT THE ROLE\nRole Description:\nWe are seeking an MDM Associate Data Engineer with 2 -5 years of experience to support and enhance our enterprise MDM (Master Data Management) platforms using Informatica/Reltio. This role is critical in delivering high-quality master data solutions across the organization, utilizing modern tools like Databricks and AWS to drive insights and ensure data reliability. The ideal candidate will have strong SQL, data profiling, and experience working with cross-functional teams in a pharma environment. To succeed in this role, the candidate must have strong data engineering experience along with MDM knowledge, hence the candidates having only MDM experience are not eligible for this role. Candidate must have data engineering experience on technologies like (SQL, Python, PySpark , Databricks, AWS etc ), along with knowledge of MDM (Master Data Management)\nRoles Responsibilities:\nAnalyze and manage customer master data using Reltio or Informatica MDM solutions.\nPerform advanced SQL queries and data analysis to validate and ensure master data integrity.\nLeverage Python, PySpark , and Databricks for scalable data processing and automation.\nCollaborate with business and data engineering teams for continuous improvement in MDM solutions.\nImplement data stewardship processes and workflows, including approval and DCR mechanisms.\nUtilize AWS cloud services for data storage and compute processes related to MDM.\nContribute to metadata and data modeling activities.\nTrack and manage data issues using tools such as JIRA and document processes in Confluence.\nApply Life Sciences/Pharma industry context to ensure data standards and compliance.\nBasic Qualifications and Experience:\nMaster s degree with 1 - 3 years of experience in Business, Engineering, IT or related field OR\nBachelor s degree with 2 - 5 years of experience in Business, Engineering, IT or related field OR\nDiploma with 6 - 8 years of experience in Business, Engineering, IT or related field\nFunctional Skills:\nMust-Have Skills:\nAdvanced SQL expertise and data wrangling.\nStrong experience in Python and PySpark for data transformation workflows.\nStrong experience with Databricks and AWS architecture.\nMust have knowledge of MDM, data governance, stewardship, and profiling practices.\nIn addition to above, candidates having experience with Informatica or Reltio MDM platforms will be preferred.\nGood-to-Have Skills:\nExperience with IDQ, data modeling and approval workflow/DCR.\nBackground in Life Sciences/Pharma industries.\nFamiliarity with project tools like JIRA and Confluence.\nStrong grip on data engineering concepts.\nProfessional Certifications :\nAny ETL certification ( e. g. Informatica)\nAny Data Analysis certification (SQL, Python, Databricks)\nAny cloud certification (AWS or AZURE)\nSoft Skills:\nStrong analytical abilities to assess and improve master data processes and solutions.\nExcellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders.\nEffective problem-solving skills to address data-related issues and implement scalable solutions.\nAbility to work effectively with global, virtual teams\nEQUAL OPPORTUNITY STATEMENT\nAmgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.\nWe will ensure that individuals with disabilities are provided with reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.\n.",Industry Type: Biotechnology,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'metadata', 'Automation', 'Data modeling', 'Master data management', 'Life sciences', 'Informatica', 'AWS', 'SQL', 'Python']",2025-06-11 05:28:45
Azure Data Engineer,Meritus Management Service,4 - 7 years,10-20 Lacs P.A.,['Pune'],"Experience in designing, developing, implementing, and optimizing data solutions on Microsoft Azure. Proven expertise in leveraging Azure services for ETL processes, data warehousing and analytics, ensuring optimal performance and scalability.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Pipeline', 'Synapse Analytics', 'Azure Databricks', 'Spark', 'Python', 'Pyspark', 'Ci/Cd', 'azure']",2025-06-11 05:28:46
Software Engineer - Snowflake Data Engineer,Beyond Key,4 - 9 years,Not Disclosed,['Indore'],"About Beyond Key:\n\nWe are a Microsoft Gold Partner and a Great Place to Work-certified company. ""Happy Team Members, Happy Clients"" is a principle we hold dear. We are an international IT consulting and software services firm committed to providing. Cutting-edge services and products that satisfy our clients global needs. Our company was established in 2005, and since then weve expanded our team by including more than 350+ Talented skilled software professionals. Our clients come from the United States, Canada, Europe, Australia, the Middle East, and India, and we create and design IT solutions for them. If you need any more details, you can get them at https: / / www.beyondkey.com / about.\n\nJob Description:\n\nWe are seeking an experienced Snowflake Data Engineer with strong expertise in DBT, Azure Data Factory (ADF), and Azure DevOps CI/CD to design, develop, and optimize data solutions. The ideal candidate will have hands-on experience in writing Snowflake SQL & JavaScript procedures, building ADF pipelines, and implementing DBT with Jinja templating and macros.\n\nResponsibilities:\n\nDesign, develop, and optimize Snowflake data models, stored procedures (SQL & JavaScript), and workflows.\nImplement DBT (data build tool) transformations with expertise in Jinja templating and macro creation.\nBuild and manage Azure Data Factory (ADF) pipelines for ETL/ELT processes.\nSet up and maintain CI/CD pipelines in Azure DevOps for automated deployments.\nIntegrate Azure Logic Apps for workflow automation where applicable.\nTroubleshoot performance bottlenecks and optimize data processes.\nCollaborate with cross-functional teams to ensure seamless data delivery.\nMaintain documentation and adhere to best practices in data engineering.\n\nQualifications:\n\n4+ years of hands-on experience in Snowflake (modeling, scripting, optimization).\n3+ years of experience in DBT, with strong knowledge of Jinja & macros.\n2+ years in Azure Data Factory (ADF) - building and managing pipelines.\n2+ years in Azure DevOps CI/CD (YAML pipelines, deployments).\nProficiency in Snowflake SQL & JavaScript stored procedures.\nExperience with Azure LogicApps is a plus.\nStrong problem-solving skills and ability to debug complex data issues.\nExcellent communication skills for stakeholder collaboration.\n\nPreferred Qualifications:\n\nCertification in Snowflake, Azure Data Engineer, or DBT.\nExposure to data orchestration tools (e.g., Airflow, Databricks).\nKnowledge of data governance and security best practices.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'orchestration', 'Javascript', 'data governance', 'Workflow', 'Stored procedures', 'microsoft', 'Macros', 'Software services', 'SQL']",2025-06-11 05:28:48
Snowflake Data Engineer,Epam Systems,5 - 10 years,Not Disclosed,['Chennai'],"Key Skills:\nSnowflake (Snow SQL, Snow PLSQL and Snowpark)\nStrong Python\nAirflow/DBT\nAny DevOps tools\nAWS/Azure Cloud Skills\n\nRequirements:\nLooking for engineer for information warehouse\nWarehouse is based on AWS/Azure, DBT, Snowflake.\nStrong programming experience with Python.\nExperience with workflow management tools like Argo/Oozie/Airflow.\nExperience in Snowflake modelling - roles, schema, databases\nExperience in data Modeling (Data Vault).\nExperience in design and development of data transformation pipelines using the DBT framework.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Python', 'Azure Cloud', 'AWS', 'SQL']",2025-06-11 05:28:50
Data Engineer - Azure,Blend360 India,5 - 9 years,Not Disclosed,['Hyderabad'],"As a Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\n3+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar field\nMust have experience e",Industry Type: Industrial Automation,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-11 05:28:51
Lead Data Engineer ( Exp: 6+ Years ),Atyeti,6 - 11 years,Not Disclosed,"['Hyderabad', 'Pune']",Job Description :\n\nStrong experience on Python programming.\nExperience on Databricks.\nExperience on Database like SQL\nPerform database performance tuning and optimization. Databricks Platform\nWork with Databricks platform for big data processing and analytics.,,,,"['Pyspark', 'ETL', 'Data Bricks', 'Python', 'SQL']",2025-06-11 05:28:53
AWS Data Engineer,Kryon Knowledge Works,5 - 8 years,Not Disclosed,['Hyderabad'],"Location: Hyderabad (Hybrid)\n\nKey Responsibilities:\n\n1. Data Engineering (AWS Glue & AWS Services):\nDesign, develop, and optimize ETL pipelines using AWS Glue (PySpark).\nManage and transform structured and unstructured data from multiple sources into AWS S3, Redshift, or Snowflake.\nWork with AWS Lambda, S3, Athena, Redshift for data orchestration.\nImplement data lake and data warehouse solutions in AWS.\n\n2. Infrastructure as Code (Terraform & AWS Services):\nDesign and deploy AWS infrastructure using Terraform.\nAutomate resource provisioning and manage Infrastructure as Code.\nMonitor and optimize cloud costs, security, and compliance.\nMaintain and improve CI/CD pipelines for deploying data applications.\n\n3. Business Intelligence (Tableau Development & Administration):\nDevelop interactive dashboards and reports using Tableau.\nConnect Tableau with AWS data sources such as Redshift, Athena, and Snowflake.\nOptimize SQL queries and extracts for performance efficiency.\nManage Tableau Server administration, including security, access controls, and performance tuning.\n\nRequired Skills & Experience:\n5+ years of experience in AWS Data Engineering with Glue, Redshift, and S3.\nStrong expertise in ETL development using AWS Glue (PySpark, Scala, or Python).\nExperience with Terraform for AWS infrastructure automation.\nProficiency in SQL, Python, or Scala for data processing.\nHands-on experience in Tableau development & administration.\nStrong understanding of cloud security, IAM roles, and permissions.\nExperience with CI/CD pipelines (Git, Jenkins, AWS Code Pipeline, etc.).\nKnowledge of data modeling, warehousing, and performance optimization.\n\n\nPlease share your resume to: +91 9361912009",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Business Intelligence', 'Terraform', 'Glue AWS', 'Tableau Development', 'AWS Data Engineer', 'Ci Cd Pipeline', 'Redshift Aws', 'Etl Development', 'Python', 'SQL']",2025-06-11 05:28:55
Azure Data Engineer,UK Client,5 - 8 years,Not Disclosed,[],"* 4–8 yrs exp as Azure Data Engineer\n\n* Strong in Azure Databricks, Data Lake & Data Factory\n\n* Experience with Azure Synapse Analytics\n\n* Hands-on with PySpark & Python\n\n* Build & optimize scalable data pipelines\n\n* 6Months Contract & Extendable job",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Temporary/Contractual","['Azure Data Factory', 'Azure Synapse', 'Pyspark', 'Azure Databricks', 'Python', 'Azure Data Lake', 'SQL']",2025-06-11 05:28:56
Trainee Engineer - Projects (Data Analyst),Indian Financial Technology And Alliedservices,0 - 1 years,Not Disclosed,['Hyderabad'],"Summary:\nThe candidate will be supporting various projects handled unders the Payment Systems Department\nWork with development and support team members across different teams and IT vendors including clients to understand the expected outcome.\nWork with functional and technical teams to define project scope and render all support for prerquisites towards development team.\nAssist technical team to identify all the limitations and challenges, resolve issues, enable the system for smooth workflow.\nStrong problem solving and analytical skills.\nGood written and verbal communication skills.\nPassionate to learn new technologies / tools as required.\n\nResponsibilities:\nBridge the gap between technical and business teams.\nConvey the functional requirements to developers, helping them determine the technical requirements for the work and preparation of Low level design document.\nResponsible for tracking, managing, and reporting on the progress of the requirements.\nDocumenting process flows and execution for he same.\nTracking the progress of the project requirements.\nWork with program/project managers, product owners, and other team members in providing necessary support in order to complete the specific SOW.\nObtain a strong functional understanding of business processes and systems.\nAssist in analysing data and prepare action.\nInvolve in root cause and impact analysis.\n\nEducation:\nBE./ B.Tech/M.C.A/M.Sc (Computer Science)\n\nTechnical Knowledge:\n1. Understanding of Database structures, Middleware clusters and Application environments.\n2. Understanding of SLQ statements and basics of Java\n3. Understanding of Open source technologies, API management.\n\nExperience: Freshers\n1. Good verbal and written communication skills\n2. Willing to work in all shifts (24x7x365)",Industry Type: FinTech / Payments,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Database', 'Middleware', 'SQL', 'Open source']",2025-06-11 05:28:58
Software Development Engineer - Algorithms & Data Structures,Uplers,1 - 5 years,Not Disclosed,['Bengaluru'],"Software Development Engineer\n\nExperience: 1 - 5 Years Exp.\nSalary : Competitive\nPreferred Notice Period: Within 30 Days\nShift: 10:00AM to 7:00PM IST\nOpportunity Type: Onsite (Bengaluru)\nPlacement Type: Permanent\n\n(*Note: This is a requirement for one of Uplers' Clients)\n\nMust have skills required :\nDSA skills, Problem Solving, Coding\n\nInfibeam (One of Uplers' Clients) is Looking for:\nSoftware Development Engineer who is passionate about their work, eager to learn and grow, and who is committed to delivering exceptional results. If you are a team player, with a positive attitude and a desire to make a difference, then we want to hear from you.\n\nRole Overview Description\nSoftware Development Engineer\nInfibeam is seeking a talented Software Development Engineer who is passionate about technology, problem-solving, and creating reliable software solutions. The ideal candidate will demonstrate strong proficiency in data structures, algorithms, and coding, and will have the opportunity to work on exciting and challenging projects in a collaborative environment. With a focus on innovative software development, you will play a vital role in our dynamic team.\n\nResponsibilities\nDesign, develop and maintain high-quality software applications.\nCollaborate with product managers, designers, and other engineers to create functional, user-friendly software solutions.\nPerform unit testing and integration testing to ensure software quality.\nAnalyze requirements and troubleshoot complex issues to ensure system stability and enhance user experience.\nUtilize strong analytical skills and coding expertise to solve problems efficiently and effectively.\nConduct peer reviews of code to maintain high coding standards and foster a culture of continuous improvement.\nCollaborate with the engineering team to ensure best practices in software design and development.\nCreate and maintain documentation of software functionalities and technical specifications for future reference.\n\nQualifications\nBE/BTech in Computer Science/Information Technology\n1 to 5 years of professional software development experience.\nStrong understanding of data structures, algorithms, and problem-solving methodologies.\nProficiency in coding with a strong attention to detail and ability to write clean, maintainable code.\nFamiliarity with languages such as Java, Ruby on Rails is a plus.\nExcellent communication skills and the ability to work effectively in a team environment.\nStrong analytical and critical thinking skills.\n\nHow to apply for this opportunity:\nEasy 3-Step Process:\n1. Click On Apply! And Register or log in on our portal\n2. Upload updated Resume & Complete the Screening Form\n3. Increase your chances to get shortlisted & meet the client for the Interview!\n\nAbout Our Client:\nInfibeam Avenues Limited (IAL) is a leading digital payments and e-commerce technology platforms company in India and provides a comprehensive suite of web services spanning digital payment solutions, data centre infrastructure and software platforms. We provide solutions to merchants, enterprises, corporations and governments in both domestic as well as international markets to enable online commerce. Our digital technology facilitates businesses and governments to execute e-commerce transactions in a safe and secure manner.\n\nAbout Uplers:\nOur goal is to make hiring and getting hired reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant product and engineering job opportunities and progress in their career.\n\n(Note: There are many more opportunities apart from this on the portal.)\n\nSo, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you!",Industry Type: FinTech / Payments,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Coding', 'Data Structures', 'Problem Solving', 'DSA', 'Algorithms']",2025-06-11 05:29:00
QA Engineer ( Data and Functional Testing),worxogo,2 - 5 years,Not Disclosed,[],"Responsibilities:\nPerform manual testing of web and mobile applications to ensure quality and performance.\nConduct various types of testing such as Functional, Usability, Integration, and GUI testing.\nDesign, develop, and execute comprehensive test cases and test scenarios based on product requirements.\nValidate data accuracy and consistency through extensive MySQL queries and checks.\nWork with large datasets and perform data validation using MS Excel.Collaborate with developers, product managers, and other QA team members to ensure timely delivery of high-quality features.\nUnderstand and follow the complete software testing lifecycle, including test planning, test execution, defect tracking, and reporting.\nUtilize tools like JIRA and Confluence for test management and documentation.\nContribute to automation testing efforts using Python, including scripting and executing automated test cases.\nParticipate in team discussions to improve product quality and testing processes.\nMaintain clear and effective communication across technical and non-technical teams.\nRequirements:\nMinimum 2 years of experience in Data Testing and Manual Testing.\nStrong knowledge of Manual Testing methodologies, techniques, and best practices.\nHands-on experience with MySQL and writing complex queries.\nProficiency in MS Excel for data validation and analysis.\nGood understanding of testing documentation like test plans, test cases, and test scenarios.\nExperience working with JIRA and Confluence is a plus.\nAbility to work independently and in a team-oriented, collaborative environment.\nWorking knowledge of Python and experience in developing automation test scripts.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Testing', 'Functional Testing', 'Automation', 'GUI Testing', 'Usability Testing', 'MySQL', 'Integration Testing', 'Manual Testing', 'Test Cases', 'Python']",2025-06-11 05:29:01
Data Software Quality Engineer 36Lakhs CTC|| Kandi Srinivasa Reddy,Integra Technologies,8 - 11 years,35-37.5 Lacs P.A.,"['Kolkata', 'Ahmedabad', 'Bengaluru']","Dear Candidate,\nWe are looking for a Software Quality Engineer to ensure software products meet quality standards through test planning, automation, and defect analysis.\n\nKey Responsibilities:\nDevelop and execute manual and automated test cases.\nDesign test strategies for functional, regression, and performance testing.\nCollaborate with development and product teams to identify requirements.\nLog defects and track resolutions using tools like JIRA.\nContribute to continuous improvement of QA processes.\nRequired Skills & Qualifications:\nExperience with testing frameworks (Selenium, JUnit, TestNG, Cypress).\nProficient in scripting or programming (Python, Java, or JavaScript).\nFamiliarity with CI/CD tools (Jenkins, GitHub Actions).\nStrong understanding of SDLC, Agile, and QA methodologies.\nExperience with API testing (Postman, REST-assured) is a plus.\nSoft Skills:\nStrong troubleshooting and problem-solving skills.\nAbility to work independently and in a team.\nExcellent communication and documentation skills.\n\nNote: If interested, please share your updated resume and preferred time for a discussion. If shortlisted, our HR team will contact you.\n\nKandi Srinivasa Reddy\nDelivery Manager\nIntegra Technologies",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automotive Spice', 'Software Quality Assurance', 'Software Quality', 'Aspice', 'V-Model', 'SQA', 'Software QA', 'ISO', 'Spice', 'Process Definition', 'Software Development Life Cycle', 'Internal Quality Auditor', 'Metrics Analysis', 'Quality Process', 'Process Compliance', 'CMMI']",2025-06-11 05:29:03
Data Scientist - L3,Wipro,3 - 6 years,Not Disclosed,['Bengaluru'],"Wipro Limited (NYSEWIT, BSE507685, NSEWIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients’ most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.\n About The Role  \n\nRole Purpose\n\nThe purpose of the role is to define, architect and lead delivery of machine learning and AI solutions.\n\n ? \n\nDo\n\n1. Demand generation through support in Solution development\n\na. Support Go-To-Market strategy\n\ni. Contribute to development solutions, proof of concepts aligned to key offerings to enable solution led sales\n\nb. Collaborate with different colleges and institutes for research initiatives and provide data science courses\n\n2. Revenue generation through Building & operationalizing Machine Learning, Deep Learning solutions\n\na. Develop Machine Learning / Deep learning models for decision augmentation or for automation solutions\n\nb. Collaborate with ML Engineers, Data engineers and IT to evaluate ML deployment options\n\n3. Team Management\n\na. Talent Management\n\ni. Support on boarding and training to enhance capability & effectiveness\n\n ? \n\nDeliver\n\n\nNo.\n\nPerformance Parameter\n\nMeasure 1. Demand generation # PoC supported 2. Revenue generation through delivery Timeliness, customer success stories, customer use cases 3. Capability Building & Team Management # Skills acquired\n\n\n ? \n\n ? \nMandatory\n\nSkills:\nData Analysis.\n\nExperience3-5 Years.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analysis', 'machine learning', 'deep learning', 'data science', 'ml', 'python', 'natural language processing', 'scikit-learn', 'neural networks', 'ml deployment', 'data engineering', 'artificial intelligence', 'sql', 'tensorflow', 'r', 'predictive modeling', 'statistical modeling', 'statistics']",2025-06-11 05:29:04
Data Analyst,Capgemini,4 - 6 years,Not Disclosed,['Bengaluru'],"Overview\nWe are seeking a highly motivated Data Analyst with strong technical and analytical skills to join our ADAS (Advanced Driver Assistance Systems) team. This role involves working with large-scale data from vehicle systems to drive insights, support data science initiatives, and contribute to the development of safer and smarter automotive technologies.\n\nResponsibilities:\nPerform data cleansing, aggregation, and analysis on large, complex datasets related to ADAS components and systems.\nBuild, maintain, and update dashboards and data visualizations to communicate insights effectively (Power BI preferred).\nDevelop and optimize data pipelines and ETL processes.\nCreate and maintain technical documentation, including data catalogs and process documentation.\nCollaborate with cross-functional teams including data scientists, software engineers, and system engineers.\nContribute actively to the internal data science community by sharing knowledge, tools, and best practices.\nWork independently on assigned projects, managing priorities and delivering results in a dynamic, unstructured environment.Required Qualifications:\nBachelors degree or higher in Computer Science, Data Science, or a related field.\nMinimum 3 years of experience in the IT industry, with at least 2 years in data analytics or data engineering roles.\nProficient in Python or Pyspark with solid software development fundamentals.\nStrong experience with SQL and relational databases.\nHands-on experience with data science, data engineering, or machine learning techniques.\nKnowledge of data modeling, data warehousing concepts, and ETL processes.\nFamiliarity with data visualization tools (Power BI preferred).\nBasic understanding of cloud platforms such as Azure or AWS.\nFundamental knowledge of ADAS functionalities is a plus.\nStrong problem-solving skills, self-driven attitude, and the ability to manage projects independently.Preferred Skills:\nExperience in automotive data or working with sensor data (e.g., radar, lidar, cameras).\nFamiliarity with agile development methodologies.\nUnderstanding of big data tools and platforms such as Databricks or Spark. Works in the area of Software Engineering, which encompasses the development, maintenance and optimization of software solutions/applications.1. Applies scientific methods to analyse and solve software engineering problems.2. He/she is responsible for the development and application of software engineering practice and knowledge, in research, design, development and maintenance.3. His/her work requires the exercise of original thought and judgement and the ability to supervise the technical and administrative work of other software engineers.4. The software engineer builds skills and expertise of his/her software engineering discipline to reach standard software engineer skills expectations for the applicable role, as defined in Professional Communities.5. The software engineer collaborates and acts as team player with other software engineers and stakeholders.- Grade SpecificIs fully competent in it's own area and has a deep understanding of related programming concepts software design and software development principles. Works autonomously with minimal supervision. Able to act as a key contributor in a complex environment, lead the activities of a team for software design and software development. Acts proactively to understand internal/external client needs and offers advice even when not asked. Able to assess and adapt to project issues, formulate innovative solutions, work under pressure and drive team to succeed against its technical and commercial goals. Aware of profitability needs and may manage costs for specific project/work area. Explains difficult concepts to a variety of audiences to ensure meaning is understood. Motivates other team members and creates informal networks with key contacts outside own area.Skills (competencies)Verbal Communication",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'software development', 'pyspark', 'relational databases', 'sql', 'data analytics', 'software design', 'data warehousing', 'microsoft azure', 'power bi', 'machine learning', 'data engineering', 'data bricks', 'data science', 'data modeling', 'spark', 'adas', 'agile', 'etl', 'aws', 'big data']",2025-06-11 05:29:06
Data / Analytics Engineer,Service based Top B2C/B2B MNC in Analyti...,6 - 10 years,Not Disclosed,"['Pune', 'Gurugram', 'Bengaluru']","Job Description:\nWe are looking for a skilled Data / Analytics Engineer with hands-on experience in vector databases and search optimization techniques. You will help build scalable, high-performance infrastructure to support AI-powered applications like semantic search, recommendation systems, and RAG pipelines.\nKey Responsibilities:\nOptimize vector search algorithms for performance and scalability.\nBuild pipelines to process high-dimensional embeddings (e.g., BERT, CLIP, OpenAI).\nImplement ANN indexing techniques like HNSW, IVF, PQ.\nIntegrate vector search with data platforms and APIs.\nCollaborate with cross-functional teams (data scientists, engineers, product).\nMonitor and resolve latency, throughput, and scaling issues.\nMust-Have Skills:\nPython\nAWS\nVector Databases (e.g., Elasticsearch, FAISS, Pinecone)\nVector Search / Similarity Search\nANN Search Algorithms HNSW, IVF, PQ\nSnowflake / Databricks\nEmbedding Models – BERT, CLIP, OpenAI\nKafka / Flink for real-time data pipelines\nREST APIs, GraphQL, or gRPC for integration\nGood to Have:\nKnowledge of semantic caching and hybrid retrieval\nExperience with distributed systems and high-performance computing\nFamiliarity with RAG (Retrieval-Augmented Generation) workflows\nApply Now if You:\nEnjoy solving performance bottlenecks in AI infrastructure\nLove working with cutting-edge ML models and search technologies\nThrive in collaborative, fast-paced environments",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Vector DB', 'AWS', 'Python', 'PQ', 'ANN Search', 'GraphQL', 'Kafka', 'HNSW', 'Flink', 'Vector Search', 'gRPC', 'CLIP', 'Snowflake', 'IVF', 'BERT', 'OpenAI', 'Databricks', 'REST APIs']",2025-06-11 05:29:08
Data Analyst,"NTT DATA, Inc.",3 - 8 years,Not Disclosed,['Pune'],"Req ID: 324676\n\nWe are currently seeking a Data Analyst to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nKey Responsibilities:\n\nExtract, transform, and load (ETL) data from various sources, ensuring data quality, integrity, and accuracy.\n\nPerform data cleansing, validation, and preprocessing to prepare structured and unstructured data for analysis.\n\nDevelop and execute queries, scripts, and data manipulation tasks using SQL, Python, or other relevant tools.\n\nAnalyze large datasets to identify trends, patterns, and correlations, drawing meaningful conclusions that inform business decisions.\n\nCreate clear and concise data visualizations, dashboards, and reports to communicate findings effectively to stakeholders.\n\nCollaborate with clients and cross-functional teams to gather and understand data requirements, translating them into actionable insights.\n\nWork closely with other departments to support their data needs.\n\nCollaborate with Data Scientists and other analysts to support predictive modeling, machine learning, and statistical analysis.\n\nContinuously monitor data quality and proactively identify anomalies or discrepancies, recommending corrective actions.\n\nStay up-to-date with industry trends, emerging technologies, and best practices to enhance analytical techniques.\n\nAssist in the identification and implementation of process improvements to streamline data workflows and analysis.\n\nBasic Qualifications:\n\n3 + years of proficiency in data analysis tools such as [Tools - e.g., Excel, SQL, R, Python].\n\n3+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\n\nUndergraduate or Graduate degree preferred\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nStrong proficiency in data analysis tools such as Python, SQL, Talend (any ETL).\n\nExperience with data visualization tools like PowerBI.\n\nExperience with cloud data platforms .\n\nFamiliarity with ETL (Extract, Transform, Load) processes and tools.\n\nKnowledge of machine learning techniques and tools.\n\nExperience in a specific industry (e.g., financial services, healthcare, manufacturing) can be a plus.\n\nUnderstanding of data governance and data privacy regulations.\n\nAbility to query and manipulate databases and data warehouses.\n\nExcellent analytical and problem-solving skills.\n\nStrong communication skills with the ability to explain complex data insights to non-technical stakeholders.\n\nDetail-oriented with a commitment to accuracy.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analysis', 'data analytics', 'data engineering', 'analysis tools', 'software engineering', 'python', 'data manipulation', 'talend', 'power bi', 'data warehousing', 'machine learning', 'dashboards', 'sql', 'data cleansing', 'data quality', 'r', 'predictive modeling', 'data visualization', 'etl']",2025-06-11 05:29:10
Engineer-Data Science,Hitachi Energy,5 - 10 years,Not Disclosed,['Vadodara'],". Analysis may be applied to various areas of the business (e.g., Market Economics, Supply Chain, Marketing/Advertising, Scientific Research, etc.). Researching and applying knowledge of existing and emerging data science principles, theories, and techniques to inform business decisions. At higher career levels, may conduct scientific research projects with the goal of breaking new ground in data analytics An Experienced Professional (P2) applies practical knowledge of job area typically obtained through advanced education and work experience. May require the following proficiency: Works independently with general supervision. Problems faced are difficult but typically not complex. May influence others within the job area through explanation of facts, policies and practices.\nHow you ll make an impact\nThe success candidate will be the part of an International Design and Engineering Team heavily specialized in Power Transformers design covering US factory.\nResponsible for building visualizations in PBI based on various sources and datasets of power transformers factories\nResponsible for DAX queries / DAX functions / Power Query Editor\nResponsible for development of transformer dashboard in coordination with global Hitachi Energy factory based on requirement.\nExpertise in using advance level calculations on the data set.\nAble to develop tabular and multidimensional models that are compatible with warehouse standards.\nAble to properly understand the business requirements and develop data models accordingly by taking care of the resources.\nFamiliar with Row Level Security (RLS)\nBasic knowledge and skills for secondary tools such as Microsoft Azure, SQL data warehouse, SSAS, Visual Studio, Power Apps etc.\nLiving Hitachi Energy s core values of safety and integrity, which means taking responsibility for your own actions while caring for your colleagues and the business.\nResponsible to ensure compliance with applicable external and internal regulations, procedures, and guidelines.\nYour Background\nBachelor s degree of Electrical or Mechanical or Data Science Engineering.\n5 - 10 years experience working in Data Analytics from start to end process. Candidates with higher experience also to be considered.\nExperience of manufacturing industry is an additional advantage.\nExtended MS Office knowledge & skills, especially excel but also eg PowerPoint, etc.\nExperienced in building MS Teams Space & SharePoint pages.\nSpecialist on building visualizations in PBI based on various sources and datasets.\nStrong capabilities of DAX queries / DAX functions / Power Query Editor. DA-100 certification preferred.\nExperience with SAP / S4 HANA -Data handling preferred, data sources in Power BI.\nProficiency in both spoken & written English language is required.\n.",Industry Type: Power,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Supply chain', 'Data analysis', 'SAP', 'Data analytics', 'Visual Studio', 'Editor', 'MS Office', 'Data mining', 'Business intelligence', 'SQL']",2025-06-11 05:29:11
Data Validation and ETL Testing Engineer,Skills and Placement Services,7 - 10 years,11-20 Lacs P.A.,"['Hyderabad', 'Bengaluru']","Looking for a Business Analyst with strong SQL, ETL/data warehouse testing, insurance domain knowledge (GuideWire), and experience in defect tracking & issue management tools.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Business Analyst', 'SQL', 'Data Validation', 'Defect Tracking', 'Insurance Domain', 'UAT', 'Issue Management', 'ETL Testing', 'GuideWire', 'SDLC', 'QA Testing', 'Mappings', 'Data Pipeline', 'Agile', 'Data Warehousing']",2025-06-11 05:29:13
Senior Data Science Engineer - Computer Vision,Uplers,6 - 7 years,25-35 Lacs P.A.,['Bengaluru'],"Senior Data Science Engineer - Computer Vision\n\nExperience: 6 - 7 Years Exp\nSalary : INR 25-35 Lacs per annum\nPreferred Notice Period: Within 30 Days\nShift: 09:00AM to 6:00PM IST\nOpportunity Type: Onsite (Bengaluru)\nPlacement Type: Contractual\nContract Duration: Full-Time, Indefinite Period\n\n(*Note: This is a requirement for one of Uplers' Clients)\n\nMust have skills required :\nComputer Vision, Deep Learning, Large scale Visual Datasets, MLFlow or DVC, Prometheus Or Grafana Or Sentry, Pytorch, Cloud Server (Google / AWS), Python\nGood to have skills :\nCV/AI Research, Healthcare, or surveillance/IoT-based CV applications., Publications in CVPR/NeurIPS/ICCU, Stakeholder Communication, TensorRT, Work in domains such as retail analytics\n\nRadius AI (One of Uplers' Clients) is Looking for:\nSenior Data Science Engineer - Computer Vision who is passionate about their work, eager to learn and grow, and who is committed to delivering exceptional results. If you are a team player, with a positive attitude and a desire to make a difference, then we want to hear from you.\n\nRole Overview Description\nSenior Data Science Engineer Computer Vision\nJob Summary\nWe are seeking a highly skilled and forward-thinking Senior Data Science Engineer to lead the development of advanced computer vision models and systems. You will play a pivotal role in designing, training, and deploying deep learning models, with a strong focus on real-time inference and edge deployment.\nThe ideal candidate will bring hands-on experience with state-of-the-art architectures and have a deep understanding of the complete ML lifecycle from data acquisition to deployment at scale.\n\nKey Responsibilities\nLead the development and implementation of computer vision models for tasks such as object detection, tracking, image retrieval, and scene understanding.\nDesign and execute end-to-end pipelines for data preparation, model training, evaluation, and deployment.\n¢ Perform fine-tuning and transfer learning on large-scale vision-language models to meet application-specific needs.\n¢ Optimize deep learning models for edge inference (NVIDIA Jetson, TensorRT, OpenVINO) and real-time performance.\n¢ Develop scalable and maintainable ML pipelines using tools such as MLflow, DVC, and Kubeflow.\n¢ Automate experimentation and deployment processes using CI/CD workflows.\n¢ Collaborate cross-functionally with MLOps, backend, and product teams to align technical efforts with business needs.\n¢ Monitor, debug, and enhance model performance in production environments.\n¢ Stay up-to-date with the latest trends in CV/AI research and rapidly prototype new ideas for real-world use.\n\nRequired Qualifications\n¢ 67+ years of hands-on experience in data science and machine learning, with at least 4 years focused on computer vision.\n¢ Strong experience with deep learning frameworks: PyTorch (preferred), TensorFlow, Hugging Face Transformers.\n¢ In-depth understanding and practical experience with Class-incremental learning and lifelong learning systems\n¢ Proficient in Python, including data processing libraries like NumPy, Pandas, and OpenCV.\n¢ Strong command of version control and reproducibility tools (e.g., MLflow, DVC, Weights & Biases).\n¢ Experience with training and optimizing models for GPU inference and edge deployment (Jetson, Coral, etc.).\n¢ Familiarity with ONNX, TensorRT, and model quantization/conversion techniques.\n¢ Demonstrated ability to analyze and work with large-scale visual datasets in real-time or near-real-time systems.\n\nPreferred Qualifications\n¢ Experience working in fast-paced startup environments with ownership of production AI systems.\n¢ Exposure to cloud platforms such as AWS (SageMaker, Lambda), GCP, or Azure for ML workflows.\n¢ Experience with video analytics, real-time inference, and event-based vision systems. ¢ Familiarity with monitoring tools for ML systems (e.g., Prometheus, Grafana, Sentry).\n\nEasy 3-Step Process:\n1. Click On Apply! And Register or log in on our portal\n2. Upload updated Resume & Complete the Screening Form\n3. Increase your chances to get shortlisted & meet the client for the Interview!\n\nAbout Our Client:\nRadiusAI is a pioneering computer vision analytics company revolutionizing retail operations with advanced, human-centric AI solutions. We offer the world's most advanced VisionAI checkout and we provide real-time data to improve operational efficiency across the entire retail industry, focusing on enterprise-level customers and secure edge integration\n\nAbout Uplers:\nOur goal is to make hiring and getting hired reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant product and engineering job opportunities and progress in their career.\n\n(Note: There are many more opportunities apart from this on the portal.)\n\nSo, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you!",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer Vision', 'Deep Learning', 'Pytorch', 'Large scale Visual Datasets', 'MLFlow or DVC', 'Prometheus Or Grafana Or Sentry']",2025-06-11 05:29:14
MLOps Engineer / Data Scientist,CEVA Logistics,3 - 7 years,Not Disclosed,['Mumbai'],"CEVA Logistics provides global supply chain solutions to connect people, products, and providers all around the world\nPresent in 170+ countries and with more than 110,000 employees spread over 1,500 sites, we are proud to be a Top 5 global 3PL\nWe believe that our employees are the key to our success\nWe want to engage and empower our diverse, global team to co-create value with our customers through our solutions in contract logistics and air, ocean, ground, and finished vehicle transport\nThat is why CEVA Logistics offers a dynamic and exceptional work environment that fosters personal growth, innovation, and continuous improvement\nDARE TO GROW! Join CEVA Logistics, and you will be part of a team that values imagination and continued learning and is committed to excellence in everything we do\nJoin us in our mission to shape the future of global logistics\nAs we continue growing at a fast pace, will you Dare to Grow with us\nJoin the forefront of AI-driven logistics innovation as a MLOps Engineer/Data Scientist at CEVA Logistics, a global leader in supply chain solutions\nYOUR ROLE\nAs a MLOps Engineer/Data scientist, you will play a pivotal role in CEVA LogisticsGlobal IT Data & Digital organization, reporting directly to the Global IT Data & Digital BI & Advanced Analytics AI & Data Science Manager\nThis hybrid role combines the expertise of data science with operational practices focused on the industrialization and deployment of AI and machine learning solutions at scale\nYou will collaborate with cross-functional teams in IT, Corporate Support Functions, Business Development, and Operations to drive the development and industrialization of machine learning models\nYour work will ensure that AI-driven insights and use cases are seamlessly integrated, scalable, and continuously optimized to provide ongoing value to CEVA Logistics and its customers\nThis position requires expertise not only in data science and machine learning but also in the processes that turn innovative solutions into robust, enterprise-grade systems\nThis position is open in Spain (Madrid / Barcelona) or Poland (Warsaw) or India (Mumbai)\nWHAT ARE YOU GOING TO DO\nExpertise in data science and machine learning techniques with experience in designing, building, and deploying models to solve business challenges\nProficiency in industrializing use cases, focusing on turning proof-of-concept models into full-scale, production-ready AI solutions that are both scalable and sustainable\nHands-on experience with MLOps tools and technologies (such as Kubernetes, Docker, Jenkins, MLflow, TensorFlow, etc-) for automating the deployment, monitoring, and maintenance of machine learning models\nStrong experience with cloud platforms (AWS, Google Cloud, Azure) and modern deployment architectures for AI/ML workloads\nAdvanced proficiency in Python, SQL, and experience with big data technologies to work with large datasets\nStrong understanding of model governance, model monitoring, and model retraining to ensure AI solutions deliver consistent and ongoing business value\nAbility to work collaboratively across cross-functional teams to ensure smooth transitions from development to deployment and operationalization\nExcellent communication skills to explain complex technical concepts to non-technical stakeholders and influence business strategies through AI-driven insights\nWHAT ARE WE LOOKING FOR\nA minimum of 5 years of professional experience with proven results in AI, Data Science & MLOps\nMasters degree in computer science, Data Science, Engineering, Mathematics, or a related field\nAdvanced certifications in AI, Data Science, Machine Learning, or MLOps would be plus\nProven ability to collaborate effectively with cross-functional teams, including business stakeholders, IT, and operations, to translate business needs into technical solutions\nStrong written and verbal communication skills, with the ability to explain complex technical concepts to non-technical audiences\nTechnical Expertise:\nData Science:\nSolid understanding of machine learning algorithms, statistical methods, and predictive modeling\nProficiency in Python and libraries such as TensorFlow, Keras, scikit-learn, XGBoost, and PyTorch\nMLOps:\nHands-on experience implementing MLOps practices for model deployment, automation, and continuous integration/continuous delivery (CI/CD)\nFamiliarity with tools like MLflow, Kubeflow, Jenkins, Docker, Kubernetes, and Terraform for automating machine learning pipelines\nData Integration & Management:\nProficient in using Snowflake for data integration, storage, and processing at scale\nExperience with Dataiku for creating and managing end-to-end data science workflows\nBig Data Technologies:\nFamiliarity with big data platforms and tools to process and analyze large datasets\nData Visualization & Reporting:\nExpertise in using Qlik Sense, Streamlite, or other BI, webapp & data visualization tools for developing interactive and insightful dashboards and reports to communicate complex results to non-technical stakeholders\nModel Development & Industrialization:\nExperience in transitioning machine learning models from prototype to production at scale, ensuring robustness, scalability, and reliability in real-world applications\nFamiliarity with best practices in model versioning, testing, monitoring, and retraining to maintain model accuracy and performance over time\nCloud Platforms:\nStrong experience with cloud-based platforms like AWS, Google Cloud and Azure for deploying, managing, and scaling AI/ML models\nCollaboration & Communication:\nProven ability to collaborate effectively with cross-functional teams, including business stakeholders, IT, and operations, to translate business needs into technical solutions\nStrong written and verbal communication skills, with the ability to explain complex technical concepts to non-technical audiences\nProblem Solving & Innovation:\nStrong analytical and problem-solving skills, with a creative mindset for leveraging AI and machine learning to solve business problems and drive innovation\nAbility to stay up to date with the latest developments in AI, machine learning, and MLOps, and apply cutting-edge techniques to business challenges\nSoft Skills:\nSelf-motivated, adaptable, and able to thrive in a fast-paced and dynamic work environment\nStrong attention to detail with a focus on quality, accuracy, and reliability in all work\nEffective time management and organizational skills, with the ability to handle multiple projects simultaneously and meet deadlines\nAdditional Information:\nThis position offers the opportunity to work with both existing tools at CEVA and new, modern tools available in the Cloud, leveraging our newly developed CEVA Data Platform\nThe role requires a proactive individual with a strong commitment to driving data-driven strategies and solutions within the organization\nWHAT DO WE HAVE TO OFFER\nWith a genuine culture of recognition, we want our employees to grow, develop and be part of our journey\nYou have access to the CEVA academy for training\nYou receive healthcare benefits, reimbursement of the transportation card (50%) and meal vouchers for each working day\nWe are a team in every sense, and we support each other and work collaboratively to achieve our goals together\nIt is our goal that you will be compensated for your hard work and commitment, so if youd like to work for one of the top Logistics providers in the world then lets work together to help you find your new role",Industry Type: Courier / Logistics,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['model monitoring', 'snowflake', 'python', 'predictive modeling', 'cloud platforms', 'machine learning algorithms', 'sql', 'communication skills']",2025-06-11 05:29:16
Big Data Developer-STG(P),Infosys,5 - 10 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities\n\nA day in the life of an Infoscion\nAs part of the Infosys delivery team, your primary role would be to ensure effective Design, Development, Validation and Support activities, to assure that our clients are satisfied with the high levels of service in the technology domain.\nYou will gather the requirements and specifications to understand the client requirements in a detailed manner and translate the same into system requirements.\nYou will play a key role in the overall estimation of work requirements to provide the right information on project estimations to Technology Leads and Project Managers.\nYou would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\nIf you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\n\nPrimary skills:Technology->Functional Programming->Scala\n\nAdditional information(Optional)\nKnowledge of design principles and fundamentals of architecture\nUnderstanding of performance engineering\nKnowledge of quality processes and estimation techniques\nBasic understanding of project domain\nAbility to translate functional / nonfunctional requirements to systems requirements\nAbility to design and code complex programs\nAbility to write test cases and scenarios based on the specifications\nGood understanding of SDLC and agile methodologies\nAwareness of latest technologies and trends\nLogical thinking and problem solving skills along with an ability to collaborate\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SCALA', 'Big Data', 'Spark', 'Hive', 'Kafka']",2025-06-11 05:29:18
Cloud Data Support Streaming Engineer,Infogrowth,10 - 13 years,16-18 Lacs P.A.,['Bengaluru'],"Looking for a Cloud Data Support Streaming Engineer with 8+ years of experience in Azure Data Lake, Databricks, PySpark, and Python. Role includes monitoring, troubleshooting, and support for streaming data pipelines.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Pyspark', 'Azure Data Lake', 'Azure SQL', 'Data Bricks', 'Python', 'Azure Data Factory', 'Application Support', 'Data Streaming', 'API', 'SSIS', 'Monitoring', 'Batch Processing']",2025-06-11 05:29:19
Site Reliability Engineer - Data Center storage | Aziro,Aziro,5 - 9 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","Site Reliability Engineer - Data Center Storage\n--------------------------------------------------------------------\nAs a Site Reliability Engineer - Data Center Storage, you will be responsible for:\nMust haves:\nHands-on working knowledge of the command line in Linux systems\nUnderstanding of networking, data center infrastructure, and server provisioning and\nbooting",,,,"['Linux', 'Site Reliability Engineering', 'Data Center Infrastructure', 'Devops', 'Jenkins', 'GIT', 'shell', 'Booting', 'Ansible', 'Sre', 'Puppet', 'Python']",2025-06-11 05:29:21
Senior Web Crawler & Data Extraction Engineer,Netgraph Networking,5 - 10 years,Not Disclosed,[],"Summary\n\nTo enhance user profiling and risk assessment, we are building web crawlers to collect relevant user data from third-party sources, forums, and the dark web. We are seeking a Senior Web Crawler & Data Extraction Engineer to design and implement these data collection solutions.\n\nJob Responsibilities\n\nDesign, develop, and maintain web crawlers and scrapers to extract data from open web sources, forums, marketplaces, and the dark web.\nImplement data extraction pipelines that aggregate, clean, and structure data for fraud detection and risk profiling.\nUse Tor, VPNs, and other anonymization techniques to safely crawl the dark web while avoiding detection.\nDevelop real-time monitoring solutions for tracking fraudulent activities, data breaches, and cybercrime discussions.\nOptimize crawling speed and ensure compliance with website terms of service, ethical standards, and legal frameworks.\nIntegrate extracted data with fraud detection models, risk scoring algorithms, and cybersecurity intelligence tools.\nWork with data scientists and security analysts to develop threat intelligence dashboards from collected data.\nImplement anti-bot detection evasion techniques and handle CAPTCHAs using AI-driven solvers where necessary.\nStay updated on OSINT (Open-Source Intelligence) techniques, web scraping best practices, and cybersecurity trends.\n\nRequirements\n5+ years of experience in web crawling, data scraping, or cybersecurity data extraction.\nStrong proficiency in Python, Scrapy, Selenium, BeautifulSoup, Puppeteer, or similar frameworks.\nExperience working with Tor, proxies, and VPNs for anonymous web scraping.\nDeep understanding of HTTP protocols, web security, and bot detection mechanisms.\nExperience parsing structured and unstructured data from JSON, XML, and web pages.\nStrong knowledge of database management (SQL, NoSQL) for storing large-scale crawled data.\nFamiliarity with AI/ML-based fraud detection techniques and data classification methods.\nExperience working with cybersecurity intelligence sources, dark web monitoring, and OSINT tools.\nAbility to implement scalable, distributed web crawling architectures.\nKnowledge of data privacy regulations (GDPR, CCPA) and ethical data collection practices.\n\nNice to Have\nExperience in fintech, fraud detection, or threat intelligence.\nKnowledge of natural language processing (NLP) for analyzing cybercrime discussions.\nFamiliarity with machine learning-driven anomaly detection for fraud prevention.\nHands-on experience with cloud-based big data solutions (AWS, GCP, Azure, Elasticsearch, Kafka).",Industry Type: FinTech / Payments,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Scrapy', 'Web Crawling', 'Data Extraction', 'Web Scraping', 'Python', 'Automation', 'BeautifulSoup', 'Data Scraping', 'Selenium', 'Python Development']",2025-06-11 05:29:23
Senior Data Science Engineer – Computer Vision,Uplers,6 - 11 years,Not Disclosed,['Bengaluru'],"Looking for a skilled Senior Data Science Engineer with 6-12 years of experience to lead the development of advanced computer vision models and systems.\nThe ideal candidate will have hands-on experience with state-of-the-art architectures and a deep understanding of the complete ML lifecycle. This position is based in Bengaluru.\nRoles and Responsibility\nLead the development and implementation of computer vision models for tasks such as object detection, tracking, image retrieval, and scene understanding.\nDesign and execute end-to-end pipelines for data preparation, model training, evaluation, and deployment.\nPerform fine-tuning and transfer learning on large-scale vision-language models to meet application-specific needs.\nOptimize deep learning models for edge inference (NVIDIA Jetson, TensorRT, OpenVINO) and real-time performance.\nDevelop scalable and maintainable ML pipelines using tools such as MLflow, DVC, and Kubeflow.\nAutomate experimentation and deployment processes using CI/CD workflows.\nCollaborate cross-functionally with MLOps, backend, and product teams to align technical efforts with business needs.\nMonitor, debug, and enhance model performance in production environments.\nStay up-to-date with the latest trends in CV/AI research and rapidly prototype new ideas for real-world use.\nJob Requirements\n6-7+ years of hands-on experience in data science and machine learning, with at least 4 years focused on computer vision.\nStrong experience with deep learning frameworks: PyTorch (preferred), TensorFlow, Hugging Face Transformers.\nIn-depth understanding and practical experience with Class-incremental learning and lifelong learning systems.\nProficient in Python, including data processing libraries like NumPy, Pandas, and OpenCV.\nStrong command of version control and reproducibility tools (e.g., MLflow, DVC, Weights & Biases).\nExperience with training and optimizing models for GPU inference and edge deployment (Jetson, Coral, etc.).\nFamiliarity with ONNX, TensorRT, and model quantization/conversion techniques.\nDemonstrated ability to analyze and work with large-scale visual datasets in real-time or near-real-time systems.\nExperience working in fast-paced startup environments with ownership of production AI systems.\nExposure to cloud platforms such as AWS (SageMaker, Lambda), GCP, or Azure for ML workflows.\nExperience with video analytics, real-time inference, and event-based vision systems.\nFamiliarity with monitoring tools for ML systems (e.g., Prometheus, Grafana, Sentry).\nPrior work in domains such as retail analytics, healthcare, or surveillance/IoT-based CV applications.\nContributions to open-source computer vision libraries or publications in top AI/ML conferences (e.g., CVPR, NeurIPS, ICCV).\nComfortable mentoring junior engineers and collaborating with cross-functional stakeholders.",Industry Type: Banking,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'SageMaker', 'Azure', 'MLOps', 'PyTorch', 'GCP', 'Pandas', 'CI/CD', 'Computer Vision', 'AWS', 'Lambda', 'NumPy']",2025-06-11 05:29:24
AWS Data/API Gateway Pipeline Engineer,Kyndryl,6 - 10 years,Not Disclosed,['Bengaluru'],"Who We Are\nAt Kyndryl, we design, build, manage and modernize the mission-critical technology systems that the world depends on every day. So why work at Kyndryl? We are always moving forward – always pushing ourselves to go further in our efforts to build a more equitable, inclusive world for our employees, our customers and our communities.\n\nThe Role\nAre you ready to dive headfirst into the captivating world of data engineering at Kyndryl? As a Data Engineer, you'll be the visionary behind our data platforms, crafting them into powerful tools for decision-makers. Your role? Ensuring a treasure trove of pristine, harmonized data is at everyone's fingertips.",,,,"['continuous integration', 'kubernetes', 'glue', 'ci/cd', 'data pipeline', 'kinesis', 'redis', 'docker', 'sql', 'serverless architecture', 'java', 'git', 'postgresql', 'backend', 'aws api gateway', 'cd', 'rest', 'python', 'dynamo db', 'streams', 'rest api design', 'aws lambda', 'aws glue', 'lambda expressions', 'kafka', 'athena', 'design principles', 'aws']",2025-06-11 05:29:26
Data Scientist - L3,Wipro,3 - 5 years,Not Disclosed,['Ramdurg'],"Role Purpose\nThe purpose of the role is to define, architect and lead delivery of machine learning and AI solutions.\nDo\n1. Demand generation through support in Solution development\na. Support Go-To-Market strategy\ni. Contribute to development solutions, proof of concepts aligned to key offerings to enable solution led sales\nb. Collaborate with different colleges and institutes for research initiatives and provide data science courses\n2. Revenue generation through Building & operationalizing Machine Learning, Deep Learning solutions\na. Develop Machine Learning / Deep learning models for decision augmentation or for automation solutions\nb. Collaborate with ML Engineers, Data engineers and IT to evaluate ML deployment options\n3. Team Management\na. Talent Management\ni. Support on boarding and training to enhance capability & effectiveness\nDeliver\n\nNo.Performance ParameterMeasure\n1.Demand generation# PoC supported\n2.Revenue generation through deliveryTimeliness, customer success stories, customer use cases\n3.Capability Building & Team Management# Skills acquired\n\n\nMandatory Skills: Data Analysis.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'python', 'team management', 'natural language processing', 'scikit-learn', 'ml deployment', 'machine learning', 'data engineering', 'artificial intelligence', 'sql', 'deep learning', 'tensorflow', 'data science', 'predictive modeling', 'statistical modeling', 'ml']",2025-06-11 05:29:27
Junior Fullstack Engineer,Data Core Systems,1 - 2 years,Not Disclosed,['Kolkata'],Required\n• Degree in Computer Science or equivalent practical experience\n• 1+ year of experience developing with Python and React/TypeScript\n• Experience working in a team development environment (not just personal projects)\n• Basic understanding of web development\n\nPreferred\n• Experience working in a startup environment\n• Understanding of the full product development lifecycle\n• Ability to read technical documents in English\n• Contributions to open source projects,Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['React', 'Web Development', 'AWS', 'Python', 'Rest Web Api', 'HTML']",2025-06-11 05:29:29
Data Governance - Engineer,Apidel Technologies,1 - 2 years,Not Disclosed,['Mumbai( Vikhroli )'],Strong SQL expertise\nHands-on experience with Advent Geneva and dataset setup\nExcellent communication skills to work across teams and functions.,Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['SQL Queries', 'Geneva', 'advent geneva', 'Complex Queries', 'SQL']",2025-06-11 05:29:31
Data Warehouse Engineer,Pando,6 - 11 years,20-35 Lacs P.A.,['Chennai'],"Technical Lead AI & Data Warehouse (DWH)\nPando is a global leader in supply chain technology, building the world's quickest time-to-value Fulfillment Cloud platform. Pandos Fulfillment Cloud provides manufacturers, retailers, and 3PLs with a single pane of glass to streamline end-to-end purchase order fulfillment and customer order fulfillment to improve service levels, reduce carbon footprint, and bring down costs. As a partner of choice for Fortune 500 enterprises globally, with a presence across APAC, the Middle East, and the US, Pando is recognized as a Technology Pioneer by the World Economic Forum (WEF), and as one of the fastest growing technology companies by Deloitte.\n\nRole\nAs the Senior Lead for AI and Data Warehouse at Pando, you will be responsible for building and scaling the data and AI services team. You will drive the design and implementation of highly scalable, modular, and reusable data pipelines, leveraging big data technologies and low-code implementations. This is a senior leadership position where you will work closely with cross-functional teams to deliver solutions that power advanced analytics, dashboards, and AI-based insights.\n\nKey Responsibilities\nLead the development of scalable, high-performance data pipelines using PySpark or Big Data ETL pipeline technologies.\nDrive data modeling efforts for analytics, dashboards, and knowledge graphs.\nOversee the implementation of parquet-based data lakes.\nWork on OLAP databases, ensuring optimal data structure for reporting and querying.\nArchitect and optimize large-scale enterprise big data implementations with a focus on modular and reusable low-code libraries.\nCollaborate with stakeholders to design and deliver AI and DWH solutions that align with\nbusiness needs.\nMentor and lead a team of engineers, building out the data and AI services organization.\n\nRequired\n8-10 years of experience in big data and AI technologies, with expertise in PySpark or similar Big Data ETL pipeline technologies.\nStrong proficiency in SQL and OLAP database technologies.\nFirsthand experience with data modeling for analytics, dashboards, and knowledge graphs.\nProven experience with parquet-based data lake implementations.\nExpertise in building highly scalable, high-volume data pipelines.\nExperience with modular, reusable, low-code-based implementations.\nInvolvement in large-scale enterprise big data implementations.\nInitiative-taker with strong motivation and the ability to lead a growing team.\n\nPreferred\nExperience leading a team or building out a new department.\nExperience with cloud-based data platforms and AI services.\nFamiliarity with supply chain technology or fulfilment platforms is a plus.",Industry Type: Film / Music / Entertainment,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Agentic Ai', 'Pyspark', 'Generative Ai', 'Data Pipeline', 'Artificial Intelligence', 'AI', 'Data Warehousing', 'Data Modeling', 'ETL', 'AWS']",2025-06-11 05:29:33
AI Engineer / Data Analyst,Meru Software,2 - 5 years,Not Disclosed,['Hyderabad'],Meru Software Pvt Ltd is looking for AI Engineer / Data Analyst to join our dynamic team and embark on a rewarding career journey.\n\nDesigning and developing AI algorithms and models to solve specific business problems. Creating and maintaining databases for storing and processing large amounts of data. Developing and deploying machine learning and deep learning models. Implementing and integrating AI solutions with existing systems and software. Analyzing and interpreting complex data sets to extract insights and drive decision - making. Collaborating with cross - functional teams to develop and deploy AI applications. Ensuring the security and privacy of data used in AI applications. Communicating and presenting technical information to non - technical stakeholders. Excellent communication skills & attention to detail.,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine learning', 'Data Analyst', 'SQL', 'Python']",2025-06-11 05:29:34
"Delivery Head - Infrastructure Engineering, Data Center",Bajaj Allianz General Insurance,8 - 13 years,Not Disclosed,['Pune'],"The role requires strong leadership, strategic thinking, and the ability to drive innovation and efficiency within the technology department. It demands extensive experience in leading complex data center infrastructures, focusing on servers, SAN storage, high availability, disaster recovery, and hybrid environments, including data center operations and physical servers (blade and rack). Responsibilities include designing and testing backup strategies, maintaining documentation, ensuring compliance with regulations, and conducting product and vendor evaluations. Collaboration with various IT teams, the security team, and business stakeholders is essential\nThe role includes setting up processes, prioritizing tasks, and defining KPIs for on role and offrole team members must have experience in managing large technical teams, coaching and motivating them. Required to lead IT infrastructure audits, oversee IT projects, and implement ITIL service management solutions. Managing vendor relationships and handling budgets (CAPEX and OPEX) are crucial. The focus is on saving costs and using technology and personnel efficiently.",,,,"['Service management', 'VMware', 'SAN', 'Automation', 'Data analysis', 'Data center operations', 'Disaster recovery', 'Manager Technology', 'Virtualization', 'Monitoring']",2025-06-11 05:29:36
Engineer - Investment Data Platform - 3+ Years - Pune,Crescendo Global,3 - 6 years,Not Disclosed,['Pune'],"Engineer - Investment Data Platform - 3+ Years - Pune\n\nWe are hiring a skilled Engineer to join the Investment Data Platform in Pune in Financial services. If you're passionate about data software and engineering and delivering high-quality software solutions using Azure and .Net technologies, this opportunity is for you.\n\nLocation: Pune\n\nYour Future Employer: Our client is a leading financial services firm with a global presence. They are committed to creating an inclusive and diverse workplace where all employees feel valued and have the opportunity to reach their full potential.\n\nResponsibilities:\nDeveloping and maintain software solutions aligned with business outcomes.\nCollaborating within agile teams to review user stories and implement features.\nMaintaining existing data platform artefacts and contribute to continuous improvement.\nBuilding scalable, robust software adhering to data engineering best practices.\nSupporting development of data ingestion, modeling, transformation, and deployment pipelines.\n\nRequirements:\n3+ years of experience in software engineering and 2+ years in data engineering.\nProficiency in C#, .Net Framework, SQL; exposure to Python, Java, PowerShell, or JavaScript.\nExperience with Azure Data Factory, CI/CD pipelines, and DevOps principles.\nStrong interpersonal and communication skills.\nBachelor's degree in computer science, engineering, finance or related field\n\nWhat is in it for you:\nJoin a high-performing team at a global investment leader\nExposure to cutting-edge Azure data platform technologies\nCompetitive compensation with hybrid work flexibility\n\nReach us: If you think this role is aligned with your career, kindly write to me along with your updated CV at aayushi.goyal@crescendogroup.in for a confidential discussion on the role.\n\nDisclaimer: Crescendo Global specializes in Senior to C-level niche recruitment. We are passionate about empowering job seekers and employers with an engaging and memorable job search and leadership hiring experience. Crescendo Global does not discriminate based on race, religion, color, origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\n\nNote: We receive a lot of applications daily, so it may not be possible to respond to each one individually. Please assume that your profile has not been shortlisted if you don't hear from us in a week. Thank you for your understanding.\n\nScammers can misuse Crescendo Globals name for fake job offers. We never ask for money, purchases, or system upgrades. Verify all opportunities at www.crescendo-global.com and report fraud immediately. Stay alert!\n\nProfile Keywords: Azure Data Engineering, C# Developer, .Net Engineer, SQL Data Engineer, DevOps Data, Databricks Jobs, Data Ingestion Engineer, Financial Services Tech Jobs, Asset Management IT, Financial Services",Industry Type: Financial Services (Asset Management),Department: Other,"Employment Type: Full Time, Permanent","['C#', 'dot net', 'SQL', 'Financial markets', 'asset management process', 'Azure Databricks', 'Azure DevOps']",2025-06-11 05:29:38
Big Data Lead,Persistent,6 - 10 years,Not Disclosed,['Bengaluru'],"About Persistent\nWe are a trusted Digital Engineering and Enterprise Modernization partner, combining deep technical expertise and industry experience to help our clients anticipate what’s next. Our offerings and proven solutions create a unique competitive advantage for our clients by giving them the power to see beyond and rise above. We work with many industry-leading organizations across the world including 12 of the 30 most innovative US companies, 80% of the largest banks in the US and India, and numerous innovators across the healthcare ecosystem.\nOur growth trajectory continues, as we reported $1,231M annual revenue (16% Y-o-Y). Along with our growth, we’ve onboarded over 4900 new employees in the past year, bringing our total employee count to over 23,500+ people located in 19 countries across the globe.",,,,"['big data technologies', 'hive', 'cloudera', 'scala', 'data warehousing', 'apache pig', 'sql', 'streaming', 'java', 'apache', 'data modeling', 'spark', 'design', 'flume', 'hadoop', 'big data', 'etl', 'big data hadoop', 'hbase', 'python', 'oracle', 'oozie', 'talend', 'data processing', 'machine learning', 'sql server', 'nosql', 'mapreduce', 'kafka', 'hdfs', 'sqoop', 'aws']",2025-06-11 05:29:40
Data Center Operations Engineer,Locuz,3 - 8 years,4-7.5 Lacs P.A.,"['Thane', 'Vikhroli - Mumbai']","Job Description: Data Center Operations (3-8 years experience)\nLocation: Thane, Maharashtra\nRole Overview:\nWe are seeking an experienced Data Center Operations professional to manage and oversee the day-to-day operations of our data center facilities. The ideal candidate should have 3-8 years of hands-on experience in data center management, ensuring high availability, performance, and security of critical infrastructure.\nKey Responsibilities:\nOversee the operations, maintenance, and monitoring of data centre infrastructure.\nManage server hardware, networking equipment, and storage systems.\nEnsure uptime and availability of IT services by conducting proactive maintenance and troubleshooting.\nLead and coordinate teams for incident management and disaster recovery processes.\nImplement and maintain data centre best practices and compliance standards.\nWork with vendors for hardware procurement and software upgrades.\nManage capacity planning and resource optimization for the data centre.\nSkills & Qualifications:\n3-8 years of experience in data center operations or related fields.\nIn-depth knowledge of data center infrastructure, networking, and virtualization.\nExperience with server management, backup solutions, and monitoring tools.\nStrong understanding of security and disaster recovery processes.\nExcellent communication, leadership, and problem-solving skills.\nAbility to work in a fast-paced and high-pressure environment.\n\nInterested candidates share your resume to supriya.battula@locuz.com\nImmediate joiners are preferred.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Center Operations', 'Networking', 'Server', 'Physical Security', 'RAID']",2025-06-11 05:29:41
Senior Associate Data Scientist,"NTT DATA, Inc.",2 - 5 years,Not Disclosed,['Bengaluru'],"Your day at NTT DATA\nThe Data Scientist is a seasoned subject matter expert, tasked with participating in the adoption of data science and analytics within the organization.\n\nThe primary responsibility of this role is to participate in the creation and delivery of data-driven solutions that add business value using statistical models, machine learning algorithms, data mining, and visualization techniques.\n\nWhat youll be doing\n\nKey Responsibilities:\nDesigns, develops, and programs methods, processes, and systems to consolidate and analyze unstructured, diverse big data sources to generate actionable insights and solutions for client services and product enhancement.\nDesigns and enhances data collection procedures to include information that is relevant for building analytic systems.\nAccountable for ensuring that data used for analysis is processed, cleaned and, integrally verified and build algorithms necessary to find meaningful answers.\nDesigns and codes software programs, algorithms, and automated processes to cleanse, integrate and evaluate large datasets from multiple disparate sources.\nAccountable for providing meaningful insights from large data and metadata sources; interprets and communicates insights and findings from analysis and experiments to product, service, and business managers.\nAccountable for performing analysis using programming languages or statistical packages such as Python, pandas etc.\nDesigns scalable and highly available applications leveraging the latest tools and technologies.\nAccountable for creatively visualizing and effectively communicating results of data analysis, insights, and ideas in a variety of formats to key decision-makers within the business.\nCreates SQL queries for the analysis of data and visualize the output of the models.\nCreates documentation around processes and procedures and manages code reviews.\nAccountable for ensuring that industry standards best practices are applied to development activities.\nKnowledge and Attributes:\nSeasoned in data modelling, statistical methods and machine learning techniques.\nAbility to thrive in a dynamic, fast-paced environment.\nQuantitative and qualitative analysis skills.\nDesire to acquire more knowledge to keep up to speed with the ever-evolving field of data science.\nCuriosity to sift through data to find answers and more insights.\nGood understanding of the information technology industry within a matrixed organization and the typical business problems such organizations face.\nAbility to translate technical findings clearly and fluently to non-technical team business stakeholders to enable informed decision-making.\nAbility to create a storyline around the data to make it easy to interpret and understand.\nSelf-driven and able to work independently yet acts as a team player.\nAble to apply data science principles through a business lens.\nDesire to create strategies and solutions that challenge and expand the thinking of peers and business stakeholders.\nAcademic Qualifications and Certifications:\nBachelors degree or equivalent in Data Science, Business Analytics, Mathematics, Economics, Engineering, Computer Science or a related field.\nRelevant programming (Python) certification preferred.\nAgile certification preferred.\nRequired Experience:\nSeasoned experience in a data science position in a corporate environment and/or related industry.\nSeasoned experience in statistical modelling and data modelling, machine learning, data mining, unstructured data analytics, natural language processing.\nSeasoned experience in programming languages (Python, etc.).\nSeasoned experience working in databases (MySQL, Microsoft SQL Server, Azure Synapse, MongoDB)\nSeasoned experience working with and creating data architectures.\nSeasoned experience with extracting, cleaning, and transforming data and working with data owners to understand the data.\nSeasoned experience visualizing and/or presenting data for stakeholder use and reuse across the business.\nSeasoned experience on working with API (creating and using APIs)\nAutomation experience using Python scripting, UIPath, Selenium, PowerAutomate.\nSeasoned experience working on Linux operating system (Ubuntu)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Azure Synapse', 'Microsoft SQL Server', 'MySQL', 'Python scripting', 'PowerAutomate', 'Linux operating system', 'MongoDB', 'Selenium', 'Python', 'UIPath']",2025-06-11 05:29:43
Data Loss Prevention Engineer,Theorynine Technologies,2 - 5 years,3-8 Lacs P.A.,"['Thane', 'Goregaon']","We are hiring a DLP Specialist with 3 to 6 years of experience in managing\nendpoint security technologies. The ideal candidate will play a critical role in\ndeploying, maintaining, and optimizing Data Loss Prevention.\nForecepoint DLP is Compulsory\n\nRequired Candidate profile\nImplement and manage endpoint security tools, including DLP, XDR, and\nencryption solutions. Investigate and analyze DLP alerts and incidents, ensuring swift and\neffective response.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Loss Prevention', 'Forcepoint', 'xdr', 'Dlp']",2025-06-11 05:29:44
Data Center Ops Engineer,Clover Infotech,2 - 5 years,Not Disclosed,"['Mumbai', 'Navi Mumbai']","Job Role : Data Center Operations Engineer\nExperience : 2+yrs\nLocation : Navi Mumbai\nJob Type - Permanent Work from Office\nRoles & Responsibilities :\nHardware Monitoring & Inventory Management\nMin exp in DC environment activities 2-3 years\nDC monitoring and Operations support\nKnowledge of mounting of Servers and Network devices in RACK\nKnowledge on MUX, Primary & Secondary Power, UPS, ATS, STS\nCrimping of Network cable\nGood in communication\nManaging the DC L1 Support for day-to-day operations\nMaintaining DC Environment & Data center Hygiene.\nEscorting visitors when required & follow escalation matrix",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Inventory Management', 'Data Center Operations', 'Mounting', 'Crimping', 'Vendor Coordination', 'Device Management', 'Server Monitoring']",2025-06-11 05:29:46
Data Analytics Engineer,Automotive Industry,5 - 6 years,6-11 Lacs P.A.,['Chennai'],"Position: Data Analytics Engineer\nExp: 5 -6 years\nNP: Immediate - 30 days\nQualification: B.tech\nLocation: Chennai Hybrid\nPrimary: Google Cloud Platform ,Python skills and Big Data pipeline\nSecondary: Big Query SQ, coding, testing, implementing, debugging workflows and apps\nKindly share your updated resume to aishwarya_s@onwardgroup.com\nKindly fill the below details\nTotal Exp:\nRelevant Exp:\nNotice Period: CTC:\nECTC:\nIf servicing NP, Last working Day, offered location & CTC:\nAvailable for Video modes interview on Weekdays (Y/N) :\nPAN Number:\nName as Per PAN Card:\nDate of Birth:\nAlternative Contact No:\nReason for Job Change:",Industry Type: Automobile,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['GCP', 'Bigquery', 'Google Cloud Platform', 'Query SQ', 'Python']",2025-06-11 05:29:48
Software Engineering Manager - Research Data and Analytics,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nWe are seeking a Manager - Software Engineering to lead the design and development of software data and analytics applications to drive business decisions for Research. The ideal candidate possesses a deep understanding of software engineering principles, coupled with strong leadership and problem-solving skills. This position requires close collaboration with business analysts, scientists, and other engineers to create high-quality, scalable software solutions. You will continuously strive for innovation in the technologies and practices used for software engineering and develop a team of expert software engineers. You will collaborate with multi-functional teams, including, platform, functional IT, and business collaborators, to ensure that the solutions that are built align with business goals and are scalable, secure, and efficient.\nRoles & Responsibilities:\nTalent Growth & People Leadership: Lead, mentor, and manage a high-performing team of engineers, fostering an environment that encourages learning, collaboration, and innovation. Focus on nurturing future leaders and providing growth opportunities through coaching, training, and mentorship.\nPartner closely with product team owners, the business team including scientists, and other collaborators to lead the software engineering solutioning, ensuring deliverables are completed on time and within scope to deliver real value and meet business objectives\nDesign, develop, and implement applications and modules, including custom reports, interfaces, and enhancements\nAnalyze and understand the functional and technical requirements of applications, solutions and systems and translate them into software architecture and design specifications\nDevelop and implement unit tests, integration tests, and other testing strategies to ensure the quality of the software\nWork closely with product team, business team including scientists, and other collaborators\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients.\nBasic Qualifications:\nMasters degree with 4 - 6 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nBachelors degree with 6 - 8 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nDiploma with 10 - 12 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field\nExcellent leadership and project management skills, with the ability to manage multiple priorities simultaneously\nProficient in General Purpose High Level Language (e.g. NodeJS/Koa, Python or Java)\nProficient in Javascript UI Framework (e.g. React or ExtJs)\nProficient in SQL (e.g. Oracle, PostGres or Databricks)\nPreferred Qualifications:\n3+ years of experience in implementing and supporting custom software development for drug discovery\nStrong understanding of cloud platforms (e.g AWS) and containerization technologies (e.g., Docker, Kubernetes)\nExperience in automated testing tools and frameworks (e.g. Jest, Playwright, Cypress or Selenium)\nExperience with DevOps practices and CI/CD pipelines\nExperience with big data technologies (e.g., Spark, Databricks)\nExperienced with API integration, serverless, microservices architecture\nExperience with monitoring and logging tools (e.g., Prometheus, Grafana, Splunk)\nExperience of infrastructure as code (IaC) tools (Terraform, CloudFormation)\nExperience with version control systems like Git\nStrong understanding of software development methodologies, mainly Agile and Scrum\nStrong problem solving, analytical skills; Ability to learn quickly & work independently; Excellent communication and interpersonal skills\nProfessional Certifications\nAWS Certified Cloud Practitioner preferred\nSoft Skills:\nExcellent critical-thinking and problem-solving skills\nStrong communication and collaboration skills\nDemonstrated awareness of how to function in a team setting\nDemonstrated presentation skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Software Engineering', 'DevOps', 'microservices architecture', 'Spark', 'Databricks', 'API integration', 'AWS']",2025-06-11 05:29:49
Senior Data Scientist,"NTT DATA, Inc.",2 - 6 years,Not Disclosed,['Bengaluru'],"Your day at NTT DATA\nThe Senior Data Scientist is an advanced subject matter expert, tasked with taking accountability in the adoption of data science and analytics within the organization.\n\nThe primary responsibility of this role is to participate in the creation and delivery of data-driven solutions that add business value using statistical models, machine learning algorithms, data mining, and visualization techniques.\n\nKey responsibilities:\nDesigns, develops, and programs methods, processes, and systems to consolidate and analyze unstructured, diverse big data sources to generate actionable insights and solutions for client services and product enhancement.\nDesigns and enhances data collection procedures to include information that is relevant for building analytic systems.\nResponsible for ensuring that data used for analysis is processed, cleaned and, integrally verified and build algorithms necessary to find meaningful answers.\nDesigns and codes software programs, algorithms, and automated processes to cleanse, integrate and evaluate large datasets from multiple disparate sources\nProvides meaningful insights from large data and metadata sources; interprets and communicates insights and findings from analysis and experiments to product, service, and business managers.\nDirects scalable and highly available applications leveraging the latest tools and technologies.\nAccountable for creatively visualizing and effectively communicating results of data analysis, insights, and ideas in a variety of formats to key decision-makers within the business.\nCreates SQL queries for the analysis of data and visualizes the output of the models.\nResponsible for ensuring that industry standards best practices are applied to development activities.\n\nTo thrive in this role, you need to have:\nAdvanced understanding of data modelling, statistical methods and machine learning techniques.\nStrong ability to thrive in a dynamic, fast-paced environment.\nStrong quantitative and qualitative analysis skills.\nDesire to acquire more knowledge to keep up to speed with the ever-evolving field of data science.\nCuriosity to sift through data to find answers and more insights.\nAdvanced understanding of the information technology industry within a matrixed organization and the typical business problems such organizations face.\nStrong ability to translate technical findings clearly and fluently to non-technical team business stakeholders to enable informed decision-making.\nStrong ability to create a storyline around the data to make it easy to interpret and understand.\nSelf-driven and able to work independently yet acts as a team player.\n\nAcademic qualifications and certifications:\nBachelors degree or equivalent in Data Science, Business Analytics, Mathematics, Economics, Engineering, Computer Science or a related field.\nRelevant programming certification preferred.\nAgile certification preferred.\n\nRequired experience:\nAdvanced demonstrated experience in a data science position in a corporate environment and/or related industry.\nAdvanced demonstrated experience in statistical modelling and data modelling, machine learning, data mining, unstructured data analytics, natural language processing.\nAdvanced demonstrated experience in programming languages (R, Python, etc.).\nAdvanced demonstrated experience working with and creating data architectures.\nAdvanced demonstrated experience with extracting, cleaning, and transforming data and working with data owners to understand the data.\nAdvanced demonstrated experience visualizing and/or presenting data for stakeholder use and reuse across the business.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'data analytics', 'natural language processing', 'data modeling', 'data mining', 'statistical modeling', 'data architecture', 'machine learning']",2025-06-11 05:29:51
Data Architect Sr. Advisor,"NTT DATA, Inc.",12 - 15 years,Not Disclosed,['Bengaluru'],"Req ID: 323777\n\nWe are currently seeking a Data Architect Sr. Advisor to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\n""Job DutiesThe Data & AI Architect is a seasoned level expert who is responsible for participating in the delivery of multi-technology consulting services to clients by providing strategies and solutions on all aspects of infrastructure and related technology components.\n\nThis role collaborates with other stakeholders on the development of the architectural approach for one or more layer of a solution. This role has the primary objective is to work on strategic projects that ensure the optimal functioning of the client""™s technology infrastructure.\n""¢ Key Responsibilities:\n""¢ Ability and experience to have conversations with the CEO, Business owners and CTO/CDO\n""¢ Break down intricate business challenges, devise effective solutions, and focus on client needs.\n""¢ Craft high level innovative solution approach for complex business problems\n""¢ Utilize best practices and creativity to address challenges\n""¢ Leverage market research, formulate perspectives, and communicate insights to clients\n""¢ Establish strong client relationships\n""¢ Interact at appropriate levels to ensure client satisfaction\n""¢ Knowledge and Attributes:\n""¢ Ability to focus on detail with an understanding of how it impacts the business strategically.\n""¢ Excellent client service orientation.\n""¢ Ability to work in high-pressure situations.\n""¢ Ability to establish and manage processes and practices through collaboration and the understanding of business.\n""¢ Ability to create new and repeat business for the organization.\n""¢ Ability to contribute information on relevant vertical markets\n""¢ Ability to contribute to the improvement of internal effectiveness by contributing to the improvement of current methodologies, processes and tools.\n\nMinimum Skills RequiredAcademic Qualifications and Certifications:\n""¢ BE/BTech or equivalent in Information Technology and/or Business Management or a related field.\n""¢ Scaled Agile certification desirable.\n""¢ Relevant consulting and technical certifications preferred, for example TOGAF.\n\nRequired Experience12-15 years\n""¢ Seasoned demonstrable experience in a similar role within a large scale (preferably multi- national) technology services environment.\n""¢ Very good understanding of Data, AI, Gen AI and Agentic AI\n""¢ Must have Data Architecture and Solutioning experience. Capable of E2E Data Architecture and GenAI Solution design.\n""¢ Must be able to work on Data & AI RFP responses as Solution Architect\n""¢ 10+ years of experience in Solution Architecting of Data & Analytics, AI/ML & Gen AI Technical Architect\n""¢ Develop Cloud-native technical approach and proposal plans identifying the best practice solutions meeting the requirements for a successful proposal. Create, edit, and review documents, diagrams, and other artifacts in response to RPPs RFQs and Contribute to and participate in presentations to customers regarding proposed solutions.\n""¢ Proficient with Snowflake, Databricks, Azure, AWS, GCP cloud, Data Engineering & AI tools\n""¢ Experience with large scale consulting and program execution engagements in AI and data\n""¢ Seasoned multi-technology infrastructure design experience.\n""¢ Seasoned demonstrable level of expertise coupled with consulting and client engagement experience, demonstrating good experience in client needs assessment and change management.\n""¢ Additional\nAdditional\nAdditional Career Level Description:\nKnowledge and application:\n""¢ Seasoned, experienced professional; has complete knowledge and understanding of area of specialization.\n""¢ Uses evaluation, judgment, and interpretation to select right course of action.\nProblem solving:\n""¢ Works on problems of diverse scope where analysis of information requires evaluation of identifiable factors.\n""¢ Resolves and assesses a wide range of issues in creative ways and suggests variations in approach.\nInteraction:\n""¢ Enhances relationships and networks with senior internal/external partners who are not familiar with the subject matter often requiring persuasion.\n""¢ Works""",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['snowflake', 'microsoft azure', 'data engineering', 'data bricks', 'aws', 'python', 'client engagement', 'ai solutions', 'aiml', 'data architecture', 'machine learning', 'solution architecting', 'artificial intelligence', 'change management', 'gen', 'service orientation', 'gcp cloud', 'ml']",2025-06-11 05:29:53
Data Migration Engineer,A Global Engineering and Technology Solu...,4 - 6 years,Not Disclosed,['Pune'],"Roles and Responsibilities\nDesign, develop, test, and deploy data migration solutions using various tools such as PTC, WBM, CAD, PDM Link, Java Utilities, Loaders, SQL, and Oracle.\nCollaborate with cross-functional teams to identify business requirements and design data migration strategies that meet those needs.\nDevelop complex database queries to extract relevant data from legacy systems for migration into new platforms.\nConduct thorough testing of migrated data to ensure accuracy and integrity.\nProvide technical guidance on best practices for data management and governance.\nDesired Candidate Profile\n4-6 years of experience in Data Migration Engineering with expertise in one or more of the following areas: PTC/WBM/CAD/PDM Link/Java Utilities/Loaders/SQL/Oracle.\nBachelor's degree in Any Specialization (B.Tech/B.E.).\nStrong understanding of software development life cycle (SDLC) principles and methodologies.\nProficiency in writing efficient code using programming languages like Java or Python.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['WBM', 'Pdm Link', 'Java utilities', 'Oracle', 'Windchill', 'loaders', 'SQL']",2025-06-11 05:29:54
MDM Data Platform Engineer,TechAffinity,5 - 8 years,Not Disclosed,['Chennai'],"Key Responsibilities:\nTechnical Skills:\nStrong proficiency in SQL for data manipulation and querying.\nKnowledge of Python scripting for data processing and automation.\nExperience in Reltio Integration Hub (RIH) and handling API-based integrations.\nFamiliarity with Data Modelling Matching, Survivorship concepts and methodologies.\nExperience with D&B, ZoomInfo, and Salesforce connectors for data enrichment.\nUnderstanding of MDM workflow configurations and role-based data governance\nExperience with AWS Databricks, Data Lake and Warehouse\nImplement and configure MDM solutions using Reltio while ensuring alignment with business requirements and best practices.\nDevelop and maintain data models, workflows, and business rules within the MDM platform.\nWork on Reltio Workflow (DCR Workflow & Custom Workflow) to manage data approvals and role-based assignments.\nSupport data integration efforts using Reltio Integration Hub (RIH) to facilitate data movement across multiple systems.\nDevelop ETL pipelines using SQL, Python, and integration tools to extract, transform, and load (ETL) data.\nWork with D&B, ZoomInfo, and Salesforce connectors for data enrichment and integration.\nPerform data analysis and profiling to identify data quality issues and recommend solutions for data cleansing and enrichment.\nCollaborate with stakeholders to define and document data governance policies, procedures, and standards.\nOptimize MDM workflows to enhance data stewardship and governance.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Master Data Management', 'Data Lake', 'Aws Databricks', 'SQL', 'Python', 'Reltio Integration Hub', 'MDM', 'Data Warehousing', 'Data Governance', 'Data Modelling Matching']",2025-06-11 05:29:56
Data Integration - MSSQL + SSIS + SSMA,Hexaware Technologies,4 - 6 years,Not Disclosed,"['Chennai', 'Bengaluru']","Deep understanding of MSSQL, SSIS, and SSMA technologies.\nCollaborate with clients to understand project requirements and translate them into technical solutions.\nDevelop, test, and deploy MSSQL databases, SSIS packages, SSMS, and Python scripts to meet project objectives.\nDesign and implement project management artifacts, including project plans, timelines, status reports, and documentation.\nLead and participate in project meetings, providing updates on project status, risks, and issues.",,,,"['SSMA', 'MSSQL', 'SSIS', 'Python']",2025-06-11 05:29:58
"Sr Data Scientist, Supply Chain AI Delivery",Kimberly-Clark Corporation,4 - 9 years,Not Disclosed,['Bengaluru'],"You we're made to do this work: designing new technologies, diving into data, optimizing digital experiences, and constantly developing better, faster ways to get results. You want to be part of a performance culture dedicated to building technology for a purpose that matters. You want to work in an environment that promotes sustainability, inclusion, we'llbeing, and career development. In this role, you'll help us deliver better care for billions of people around the world. It starts with YOU.\n  In this role, you will:",,,,"['Procurement', 'Supply chain', 'Data analysis', 'Configuration management', 'Flex', 'Data processing', 'Monitoring', 'SQL', 'Logistics', 'Python']",2025-06-11 05:29:59
Data Analyst,PwC India,4 - 8 years,Not Disclosed,"['Bengaluru', 'Mumbai (All Areas)']","If Interested, please apply in the given link : https://forms.office.com/r/h5Qzqnb1Kr\n\nJob Title: Data Analyst (4 - 8 Years Experience)\nLocation: Bengaluru/ Mumbai\nType: Full-Time\nAbout the Role:\nWe are on the lookout for a sharp, self-driven Data Analyst with a strong command of SQL, Python, and relational databases. If solving complex data problems, building efficient data pipelines, and collaborating across teams excites you - youll thrive in this role.",,,,"['Python', 'SQL', 'Data analyst']",2025-06-11 05:30:01
Data Analyst - L3,Wipro,3 - 5 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of this role is to interpret data and turn into information (reports, dashboards, interactive visualizations etc) which can offer ways to improve a business, thus affecting business decisions.\nDo\n1. Managing the technical scope of the project in line with the requirements at all stages\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\nb. Develop record management process and policies\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\nd. Providing sales data, proposals, data insights and account reviews to the client base\ne. Identify areas to increase efficiency and automation of processes\nf. Set up and maintain automated data processes\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\nh. Produce and track key performance indicators\n2. Analyze the data sets and provide adequate information\na. Liaise with internal and external clients to fully understand data content\nb. Design and carry out surveys and analyze survey data as per the customer requirement\nc. Analyze and interpret complex data sets relating to customers business and prepare reports for internal and external audiences using business analytics reporting tools\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\nf. Develop predictive models and share insights with the clients as per their requirement\n\nDeliver\n\nNo Performance Parameter Measure\n1. Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\nMandatory Skills: Tableau.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'Tableau', 'data warehouses', 'data integration', 'Data modelling']",2025-06-11 05:30:02
Data Analyst - L3,Wipro,4 - 8 years,Not Disclosed,['Bengaluru'],"? \n\nDo\n\n1. Managing the technical scope of the project in line with the requirements at all stages\n\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\n\nb. Develop record management process and policies\n\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\n\nd. Providing sales data, proposals, data insights and account reviews to the client base\n\ne. Identify areas to increase efficiency and automation of processes\n\nf. Set up and maintain automated data processes\n\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\n\nh. Produce and track key performance indicators\n\n2. Analyze the data sets and provide adequate information\n\na. Liaise with internal and external clients to fully understand data content\n\nb. Design and carry out surveys and analyze survey data as per the customer requirement\n\nc. Analyze and interpret complex data sets relating to customer’s business and prepare reports for internal and external audiences using business analytics reporting tools\n\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\n\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\n\nf. Develop predictive models and share insights with the clients as per their requirement",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analysis', 'data mining', 'data warehousing', 'business analytics', 'data integration', 'python', 'data analytics', 'data validation', 'business analysis', 'dbms', 'dashboards', 'cleansing', 'business intelligence', 'sales', 'sql', 'analytics reporting', 'tableau', 'reporting tools']",2025-06-11 05:30:04
Data Analyst - L3,Wipro,4 - 8 years,Not Disclosed,['Bengaluru'],"About The Role  \n\nRole Purpose\n\nThe purpose of this role is to interpret data and turn into information (reports, dashboards, interactive visualizations etc) which can offer ways to improve a business, thus affecting business decisions.\n\n ? \n\nDo\n\n1. Managing the technical scope of the project in line with the requirements at all stages\n\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\n\nb. Develop record management process and policies\n\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\n\nd. Providing sales data, proposals, data insights and account reviews to the client base\n\ne. Identify areas to increase efficiency and automation of processes\n\nf. Set up and maintain automated data processes\n\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\n\nh. Produce and track key performance indicators\n\n\n\n2. Analyze the data sets and provide adequate information\n\na. Liaise with internal and external clients to fully understand data content\n\nb. Design and carry out surveys and analyze survey data as per the customer requirement\n\nc. Analyze and interpret complex data sets relating to customer’s business and prepare reports for internal and external audiences using business analytics reporting tools\n\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\n\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\n\nf. Develop predictive models and share insights with the clients as per their requirement\n\n ? \n\nDeliver\nNoPerformance ParameterMeasure1.Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\n\n ? \n\n ? \nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analysis', 'data mining', 'data warehousing', 'dashboards', 'data integration', 'python', 'data validation', 'pivot table', 'business analytics', 'dbms', 'vlookup', 'cleansing', 'sales', 'sql', 'analytics reporting', 'tableau', 'vba', 'advanced excel', 'reporting tools', 'digital transformation']",2025-06-11 05:30:06
Data Scientist - L3,Wipro,3 - 5 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of the role is to define, architect and lead delivery of machine learning and AI solutions.\n\n\n\nDo\n1. Demand generation through support in Solution development\na. Support Go-To-Market strategy\ni. Contribute to development solutions, proof of concepts aligned to key offerings to enable solution led sales\nb. Collaborate with different colleges and institutes for research initiatives and provide data science courses\n2. Revenue generation through Building & operationalizing Machine Learning, Deep Learning solutions\na. Develop Machine Learning / Deep learning models for decision augmentation or for automation solutions\nb. Collaborate with ML Engineers, Data engineers and IT to evaluate ML deployment options\n3. Team Management\na. Talent Management\ni. Support on boarding and training to enhance capability & effectiveness\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure\n1.Demand generation# PoC supported\n2.Revenue generation through deliveryTimeliness, customer success stories, customer use cases\n3.Capability Building & Team Management# Skills acquired\n\n\n\n\n\n\nMandatory Skills: Data Analysis.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'ML deployment', 'Deep learning models', 'Solution development', 'Talent Management', 'Machine Learning']",2025-06-11 05:30:08
Chief Analytics Office (CAO) - Data Scientist,IBM,2 - 4 years,Not Disclosed,['Bengaluru'],"Role Overview: \n\nAs a Data Scientist within IBM's Chief Analytics Office, you will support AI-driven projects across the enterprise. You will apply your technical skills in AI, machine learning, and data analytics to assist in implementing data-driven solutions that align with business goals. This role involves working with team members to translate data insights into actionable recommendations.\n\n  \n\n Key Responsibilities: \n Technical Execution and Leadership: \nDevelop and deploy AI models and data analytics solutions.\nSupport the implementation and optimization of AI-driven strategies per business stakeholder requirements.\nHelp refine data-driven methodologies for transformation projects.\n Data Science and AI: \nDesign and implement machine learning solutions and statistical models, from problem formulation through deployment, to analyze complex datasets and generate actionable insights.\nLearn and utilize cloud platforms to ensure the scalability of AI solutions.\nLeverage reusable assets and apply IBM standards for data science and development.\n Project Support: \nLead and contribute to various stages of AI and data science projects, from data exploration to model development.\nMonitor project timelines and help resolve technical challenges.\nDesign and implement measurement frameworks to benchmark AI solutions, quantifying business impact through KPIs.\n Collaboration: \nEnsure alignment to stakeholders’ strategic direction and tactical needs.\nWork with data engineers, software developers, and other team members to integrate AI solutions into existing systems.\nContribute technical expertise to cross-functional teams.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n Education: Bachelor’s or Master’s in Computer Science, Data Science, Statistics, or a related field is required; an advanced degree strongly preferred\n\n Experience: \n2-4 yearsof experience in data science, AI, or analytics with a focus on implementing data-driven solutions\nExperience with data cleaning, data analysis, A/B testing, and data visualization\nExperience with AI technologies through coursework or projects\n\n\n Technical\n\nSkills:\n \nProficiency in SQL and Python for performing data analysis and developing machine learning models\nKnowledge of common machine learning algorithms and frameworkslinear regression, decision trees, random forests, gradient boosting (e.g., XGBoost, LightGBM), neural networks, and deep learning frameworks such as TensorFlow and PyTorch\nExperience with cloud-based platforms and data processing frameworks\nUnderstanding of large language models (LLMs)\nFamiliarity with IBM’s watsonx product suite\nFamiliarity with object-oriented programming\n\n\n Analytical\n\nSkills:\n \nStrong problem-solving abilities and eagerness to learn\nAbility to work with datasets and derive insights\n\n\n Other : \nGood communication skills, with the ability to explain technical concepts clearly\nEnthusiasm for learning and applying new technologies\nStrong project management skills, with the ability to balance multiple initiatives, prioritize tasks effectively, and meet deadlines in a fast-paced environment\nSuccessful completion of Coding Assessment\n\n\nPreferred technical and professional experience\n\nAdvanced degree strongly preferred",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'project management', 'artificial intelligence', 'sql', 'data science', 'data analytics', 'data analysis', 'neural networks', 'random forest', 'machine learning', 'data cleansing', 'tensorflow', 'ab testing', 'pytorch', 'statistical modeling', 'data visualization', 'statistics']",2025-06-11 05:30:10
Data Scientist Lead - L1,Wipro,4 - 9 years,Not Disclosed,['Bengaluru'],"About The Role  \n\nRole Purpose\n\nThe purpose of the role is to define, architect and lead delivery of machine learning and AI solutions\n\n ? \n\nDo\n\n1. Demand generation through support in Solution development\n\na. Support Go-To-Market strategy\n\ni. Collaborate with sales, pre-sales &consulting team to assist in creating solutions and propositions for proactive demand generation\n\nii. Contribute to development solutions, proof of concepts aligned to key offerings to enable solution led sales\n\nb. Collaborate with different colleges and institutes for recruitment, joint research initiatives and provide data science courses\n\n2. Revenue generation through Building & operationalizing Machine Learning, Deep Learning solutions\n\na. Develop Machine Learning / Deep learning models for decision augmentation or for automation solutions\n\nb. Collaborate with ML Engineers, Data engineers and IT to evaluate ML deployment options\n\nc. Integrate model performance management tools into the current business infrastructure\n\n3. Team Management\n\na. Resourcing\n\ni. Support recruitment process to on-board right resources for the team\n\nb. Talent Management\n\ni. Support on boarding and training for the team members to enhance capability & effectiveness\n\nii. Manage team attrition\n\nc. Performance Management\n\ni. Conduct timely performance reviews and provide constructive feedback to own direct reports\n\nii. Be a role model to team for five habits\n\niii. Ensure that the Performance Nxt is followed for the entire team\n\nd. Employee Satisfaction and Engagement\n\ni. Lead and drive engagement initiatives for the team\n\n ? \n\nDeliver\n\n\nNo.\n\nPerformance Parameter\n\nMeasure 1. Demand generation Order booking 2. Revenue generation through delivery Timeliness, customer success stories, customer use cases 3. Capability Building & Team Management % trained on new skills, Team attrition %\n\n\n ? \n\n ? \nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['team management', 'machine learning', 'deep learning', 'data science', 'performance management', 'python', 'natural language processing', 'neural networks', 'ml deployment', 'data engineering', 'artificial intelligence', 'sql', 'tensorflow', 'predictive modeling', 'statistical modeling', 'ml']",2025-06-11 05:30:12
Data Scientist-Artificial Intelligence,IBM,3 - 7 years,Not Disclosed,['Bengaluru'],"As an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\n In this role, your responsibilities may include: \nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours’.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nProof of Concept (POC) DevelopmentDevelop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions.\nHelp in showcasing the ability of Gen AI code assistant to refactor/rewrite and document code from one language to another\nDocument solution architectures, design decisions, implementation details, and lessons learned.\nStay up to date with the latest trends and advancements in AI, foundation models, and large language models.\nEvaluate emerging technologies, tools, and frameworks to assess their potential impact on solution design and implementation\n\n\nPreferred technical and professional experience\nExperience and working knowledge in COBOL & JAVA would be preferred\nHaving experience in Code generation, code matching & code translation leveraging LLM capabilities would be a Big plus\nDemonstrate a growth mindset to understand clients' business processes and challenges",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['elastic search', 'java', 'proof of concept', 'cobol', 'splunk', 'targetlink', 'simulink', 'data management', 'stateflow', 'sil', 'big data', 'can bus', 'matlab', 'python', 'c', 'predictive', 'machine learning', 'presales', 'autosar', 'code generation', 'rtw', 'rfi', 'embedded c', 'model based development', 'rfp']",2025-06-11 05:30:13
Data Scientist-Advanced Analytics,IBM,3 - 7 years,Not Disclosed,['Bengaluru'],"As an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviour’s.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nProof of Concept (POC) DevelopmentDevelop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions.\nHelp in showcasing the ability of Gen AI code assistant to refactor/rewrite and document code from one language to another\nDocument solution architectures, design decisions, implementation details, and lessons learned.\nStay up to date with the latest trends and advancements in AI, foundation models, and large language models.\nEvaluate emerging technologies, tools, and frameworks to assess their potential impact on solution design and implementation\n\n\nPreferred technical and professional experience\nExperience and working knowledge in COBOL & JAVA would be preferred\nHaving experience in Code generation, code matching & code translation leveraging LLM capabilities would be a Big plus\nDemonstrate a growth mindset to understand clients' business processes and challenges",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['elastic search', 'java', 'code generation', 'cobol', 'splunk', 'matlab', 'time series analysis', 'financial analysis', 'simulink', 'python', 'c', 'predictive analytics', 'analytics data', 'stateflow', 'machine learning', 'financial projections', 'sil', 'embedded c', 'clustering', 'big data']",2025-06-11 05:30:15
Data Scientist,"NTT DATA, Inc.",2 - 5 years,Not Disclosed,['Bengaluru'],"Your day at NTT DATA\nThe Senior Data Scientist is an advanced subject matter expert, tasked with taking accountability in the adoption of data science and analytics within the organization.\n\nThe primary responsibility of this role is to participate in the creation and delivery of data-driven solutions that add business value using statistical models, machine learning algorithms, data mining, and visualization techniques.\n\nWhat youll be doing\n\nKey Responsibilities:\nDesigns, develops, and programs methods, processes, and systems to consolidate and analyze unstructured, diverse big data sources to generate actionable insights and solutions for client services and product enhancement.\nDesigns and enhances data collection procedures to include information that is relevant for building analytic systems.\nResponsible for ensuring that data used for analysis is processed, cleaned and, integrally verified and build algorithms necessary to find meaningful answers.\nDesigns and codes software programs, algorithms, and automated processes to cleanse, integrate and evaluate large datasets from multiple disparate sources\nProvides meaningful insights from large data and metadata sources; interprets and communicates insights and findings from analysis and experiments to product, service, and business managers.\nDirects scalable and highly available applications leveraging the latest tools and technologies.\nAccountable for creatively visualizing and effectively communicating results of data analysis, insights, and ideas in a variety of formats to key decision-makers within the business.\nCreates SQL queries for the analysis of data and visualizes the output of the models.\nResponsible for ensuring that industry standards best practices are applied to development activities.\nKnowledge and Attributes:\nAdvanced understanding of data modelling, statistical methods and machine learning techniques.\nStrong ability to thrive in a dynamic, fast-paced environment.\nStrong quantitative and qualitative analysis skills.\nDesire to acquire more knowledge to keep up to speed with the ever-evolving field of data science.\nCuriosity to sift through data to find answers and more insights.\nAdvanced understanding of the information technology industry within a matrixed organization and the typical business problems such organizations face.\nStrong ability to translate technical findings clearly and fluently to non-technical team business stakeholders to enable informed decision-making.\nStrong ability to create a storyline around the data to make it easy to interpret and understand.\nSelf-driven and able to work independently yet acts as a team player.\nAcademic Qualifications and Certifications:\nBachelors degree or equivalent in Data Science, Business Analytics, Mathematics, Economics, Engineering, Computer Science or a related field.\nRelevant programming certification preferred.\nAgile certification preferred.\nRequired Experience:\nAdvanced demonstrated experience in a data science position in a corporate environment and/or related industry.\nAdvanced demonstrated experience in statistical modelling and data modelling, machine learning, data mining, unstructured data analytics, natural language processing.\nAdvanced demonstrated experience in programming languages (R, Python, etc.).\nAdvanced demonstrated experience working with and creating data architectures.\nAdvanced demonstrated experience with extracting, cleaning, and transforming data and working with data owners to understand the data.\nAdvanced demonstrated experience visualizing and/or presenting data for stakeholder use and reuse across the business.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data science', 'R', 'data modelling', 'data mining', 'statistical modelling', 'machine learning', 'Python', 'SQL']",2025-06-11 05:30:17
Big Data Developer,Persistent,4 - 8 years,Not Disclosed,['Bengaluru'],"About Persistent\nWe are a trusted Digital Engineering and Enterprise Modernization partner, combining deep technical expertise and industry experience to help our clients anticipate what’s next. Our offerings and proven solutions create a unique competitive advantage for our clients by giving them the power to see beyond and rise above. We work with many industry-leading organizations across the world including 12 of the 30 most innovative US companies, 80% of the largest banks in the US and India, and numerous innovators across the healthcare ecosystem.\nOur growth trajectory continues, as we reported $1,231M annual revenue (16% Y-o-Y). Along with our growth, we’ve onboarded over 4900 new employees in the past year, bringing our total employee count to over 23,500+ people located in 19 countries across the globe.",,,,"['big data administration', 'hive', 'cloudera', 'data', 'business requirements', 'scala', 'big data technologies', 'data architecture', 'coding', 'java', 'spark', 'hadoop', 'big data', 'hbase', 'programming', 'python', 'software development', 'software testing', 'performance tuning', 'engineering', 'angular', 'hdfs', 'agile', 'sdlc', 'informatica']",2025-06-11 05:30:19
BFSI Data and Analytics Project Lead - CITI Bank,"NTT DATA, Inc.",12 - 17 years,Not Disclosed,['Bengaluru'],"Req ID: 326459\n\nWe are currently seeking a BFSI Data and Analytics Project Lead - CITI Bank to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\n""Job DutiesData & Analytics Project Lead with over 12+ years of experience in BFSI Domain\nThe Data and Analytics Delivery Manager will oversee the successful delivery of the Client's data and analytics projects, ensuring our clients derive maximum value from their data assets. This leadership role involves setting strategy, managing delivery teams, collaborating across functions, and upholding data governance and quality standards. The ideal candidate brings strong technical and business acumen to build and execute data-driven strategies aligned with the Client's mission of transforming their business with data-driven insights.\nThe core responsibilities for the job include the following:\nProject and Program Oversight:\n""¢ Oversee end-to-end delivery of complex data and analytics projects, ensuring timely, high-quality, and cost-effective outcomes.\n""¢ Establish project governance, risk management, and quality assurance standards for effective project delivery.\n""¢ Monitor project portfolios and allocate resources to optimize productivity across multiple client engagements.\n\nStakeholder Collaboration and Engagement:\n""¢ Serve as the primary liaison between data delivery teams, sales, product, and client-facing teams to ensure client needs are met.\n""¢ Present data strategies, project status, and insights effectively to both technical and non-technical stakeholders, fostering alignment.\n""¢ Drive collaboration with product management and engineering teams to align on data needs and operational goals.\n\nInnovation and Technology Adoption:\n""¢ Stay abreast of the latest trends in GenAI, Agentic AI in data engineering, data science, machine learning, and AI to enhance the Client's data capabilities.\n""¢ Must have experience in Cloud Modernization, DWH, Datalake project execution\n""¢ Drive the adoption of advanced analytics tools and technologies to improve data delivery efficiency and solution impact.\n""¢ Assess and recommend new tools, platforms, and partners to continuously improve data solutions.\nTeam Development and Leadership:\n""¢ Recruit, mentor, and retain a high-performing data and analytics team, fostering a culture of collaboration and continuous improvement.\n""¢ Set performance goals, conduct regular evaluations, and provide ongoing feedback to support team growth.\n\nMinimum Skills Required:\n""¢ Educational BackgroundBachelor's or Master's degree in Data Science, Computer Science, Business Administration, or a related field.\n""¢ Experience15+ years of experience in data and analytics, including at least 5 years in a leadership role with a proven track record in delivery management.\n""¢ Technical ProficiencyDeep understanding of data warehousing, data visualization, data governance, data trust and big data tools (SQL, Python, R, Tableau, Power BI, and cloud platforms like AWS, Azure, or Google Cloud).\n""¢ Must have experience in Cloud Modernization, DWH, Datalake project execution\n""¢ BFSI KnowledgeMandatory to have worked in BFSI projects delivered Data & Analytics projects to BFSI clients\n""¢ Project Management ExpertiseStrong background in Agile, Scrum, or other project management methodologies.\n""¢ Leadership and CommunicationExcellent interpersonal and communication skills, with a demonstrated ability to lead, influence, and engage stakeholders at all levels.\n""¢ Analytical and Problem-Solving\n\nSkills:\nStrong analytical mindset with a track record of delivering actionable insights from complex data\nThe Data and Analytics Delivery Manager will oversee the successful delivery of the Client's data and analytics projects, ensuring our retail clients derive maximum value from their data assets. This leadership role involves setting strategy, managing delivery teams, collaborating across functions, and upholding data governance and quality standards. The ideal candidate brings strong technical and business acumen to build and execute data-driven strategies aligned with the Client's mission of transforming retail with data-driven insights.""",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data governance', 'scrum', 'agile', 'data visualization', 'big data', 'advanced analytics', 'python', 'data analytics', 'data warehousing', 'power bi', 'microsoft azure', 'project management process', 'machine learning', 'sql', 'tableau', 'r', 'bfsi', 'data science', 'gcp', 'project execution', 'aws']",2025-06-11 05:30:20
Senior/Lead Data Scientist,Tiger Analytics,6 - 11 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities\n\nCurious about the role? What your typical day would look like?As a Senior Data Scientist, your work is a combination of hands-on contribution to Loreum Ipsum, Loreum Ipsum, etc. More specifically, this will involve:\nLead and contribute to developing sophisticated machine learning models, predictive analytics, and statistical analyses to solve complex business problems.",,,,"['Data Science', 'Time Series Analysis', 'Machine Learning', 'Python', 'Time Series Forecasting', 'Regression', 'Clustering', 'neural nets', 'Optimization', 'SQL']",2025-06-11 05:30:22
SAS Data Scientist,Lericon Informatics,2 - 6 years,Not Disclosed,"['Kolkata', 'Pune', 'Chennai']","Job Summary:\nWe are seeking a skilled Data Scientist with experience in both Python and SAS to join our high-performing analytics team. The ideal candidate will be adept at developing predictive models, analyzing large datasets, and delivering actionable insights using modern data science tools and platforms.\n\nKey Responsibilities:\n\nDesign and develop machine learning and statistical models using Python and SAS.\nConduct data exploration, preprocessing, and analysis on structured and unstructured datasets.\nUse SAS (Base, Advanced, Enterprise Guide, Visual Analytics, or SAS Viya) for reporting, data preparation, and statistical modeling as required.\nWork with large-scale datasets using Python libraries such as Pandas, NumPy, Scikit-learn, and TensorFlow.\nTranslate business requirements into technical solutions in collaboration with cross-functional teams.\nBuild data pipelines and automate workflows for data analysis and model deployment.\nPresent data-driven insights through visualizations using tools such as Seaborn, Plotly, Matplotlib, or SAS VA.\nDocument models, code, and methodologies for reproducibility and auditability.\n\nQualifications:\n\n2-10 years of experience in data science, machine learning, or advanced analytics.\nProficient in Python (Pandas, NumPy, Scikit-learn, TensorFlow, PyTorch, etc.) and SAS (Base, Advanced, DI, VA, or Viya).\nStrong knowledge of SQL and experience working with relational databases.\nSolid foundation in statistical analysis, hypothesis testing, regression, classification, clustering, etc.\nExperience in building, evaluating, and deploying predictive models.\nExcellent communication skills and ability to convey complex findings to non-technical stakeholders.\n\nPreferred Qualifications :\n\nExperience with big data tools (Spark, Hive, Hadoop).\nExposure to MLOps tools (MLflow, Airflow, Docker, etc.).\nFamiliarity with cloud platforms (AWS, Azure, GCP).\nUnderstanding of SAS integration with cloud or open-source tools.\nExperience with NLP, image recognition, or deep learning frameworks.\nLocation: Pan India-Delhi / NCR,Bangalore/Bengaluru,Hyderabad/Secunderabad,Chennai,Pune,Kolkata,Ahmedabad,Mumbai",Industry Type: Banking,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SAS', 'Predictive Modeling', 'Data Engineering', 'Scikit-learn', 'Machine Learning', 'NumPy', 'SQL', 'Data Science', 'Cloud Platforms', 'SAS Viya', 'Pandas', 'Data Analysis', 'Statistical Modeling', 'Data Visualization', 'Python']",2025-06-11 05:30:23
Big Data Architect,Persistent,8 - 13 years,Not Disclosed,['Bengaluru'],"About Persistent\nWe are a trusted Digital Engineering and Enterprise Modernization partner, combining deep technical expertise and industry experience to help our clients anticipate what’s next. Our offerings and proven solutions create a unique competitive advantage for our clients by giving them the power to see beyond and rise above. We work with many industry-leading organizations across the world including 12 of the 30 most innovative US companies, 80% of the largest banks in the US and India, and numerous innovators across the healthcare ecosystem.\nOur growth trajectory continues, as we reported $1,231M annual revenue (16% Y-o-Y). Along with our growth, we’ve onboarded over 4900 new employees in the past year, bringing our total employee count to over 23,500+ people located in 19 countries across the globe.",,,,"['data warehousing', 'cloudera', 'hive', 'vertica', 'soa', 'amazon redshift', 'apache storm', 'apache pig', 'distributed computing', 'sql', 'spring', 'cloud', 'jms', 'java', 'data modeling', 'spark', 'looker', 'pentaho', 'hadoop', 'big data', 'etl', 'activemq', 'rdbms', 'power bi', 'impala', 'hdf', 'analysis', 'rabbitmq', 'nosql', 'apache nifi', 'saas', 'kafka', 'ioc', 'aws']",2025-06-11 05:30:25
Data & AI Technical Solution Architects,"NTT DATA, Inc.",12 - 15 years,Not Disclosed,['Bengaluru'],"Req ID: 323775\n\nWe are currently seeking a Data & AI Technical Solution Architects to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\n""Job DutiesThe Data & AI Architect is a seasoned level expert who is responsible for participating in the delivery of multi-technology consulting services to clients by providing strategies and solutions on all aspects of infrastructure and related technology components.\n\nThis role collaborates with other stakeholders on the development of the architectural approach for one or more layer of a solution. This role has the primary objective is to work on strategic projects that ensure the optimal functioning of the client""™s technology infrastructure.\n""¢ Key Responsibilities:\n""¢ Ability and experience to have conversations with the CEO, Business owners and CTO/CDO\n""¢ Break down intricate business challenges, devise effective solutions, and focus on client needs.\n""¢ Craft high level innovative solution approach for complex business problems\n""¢ Utilize best practices and creativity to address challenges\n""¢ Leverage market research, formulate perspectives, and communicate insights to clients\n""¢ Establish strong client relationships\n""¢ Interact at appropriate levels to ensure client satisfaction\n""¢ Knowledge and Attributes:\n""¢ Ability to focus on detail with an understanding of how it impacts the business strategically.\n""¢ Excellent client service orientation.\n""¢ Ability to work in high-pressure situations.\n""¢ Ability to establish and manage processes and practices through collaboration and the understanding of business.\n""¢ Ability to create new and repeat business for the organization.\n""¢ Ability to contribute information on relevant vertical markets\n""¢ Ability to contribute to the improvement of internal effectiveness by contributing to the improvement of current methodologies, processes and tools.\n\nMinimum Skills RequiredAcademic Qualifications and Certifications:\n""¢ BE/BTech or equivalent in Information Technology and/or Business Management or a related field.\n""¢ Scaled Agile certification desirable.\n""¢ Relevant consulting and technical certifications preferred, for example TOGAF.\n\nRequired Experience12-15 years\n""¢ Seasoned demonstrable experience in a similar role within a large scale (preferably multi- national) technology services environment.\n""¢ Very good understanding of Data, AI, Gen AI and Agentic AI\n""¢ Must have Data Architecture and Solutioning experience. Capable of E2E Data Architecture and GenAI Solution design.\n""¢ Must be able to work on Data & AI RFP responses as Solution Architect\n""¢ 10+ years of experience in Solution Architecting of Data & Analytics, AI/ML & Gen AI Technical Architect\n""¢ Develop Cloud-native technical approach and proposal plans identifying the best practice solutions meeting the requirements for a successful proposal. Create, edit, and review documents, diagrams, and other artifacts in response to RPPs RFQs and Contribute to and participate in presentations to customers regarding proposed solutions.\n""¢ Proficient with Snowflake, Databricks, Azure, AWS, GCP cloud, Data Engineering & AI tools\n""¢ Experience with large scale consulting and program execution engagements in AI and data\n""¢ Seasoned multi-technology infrastructure design experience.\n""¢ Seasoned demonstrable level of expertise coupled with consulting and client engagement experience, demonstrating good experience in client needs assessment and change management.\n""¢ Additional\nAdditional\nAdditional Career Level Description:\nKnowledge and application:\n""¢ Seasoned, experienced professional; has complete knowledge and understanding of area of specialization.\n""¢ Uses evaluation, judgment, and interpretation to select right course of action.\nProblem solving:\n""¢ Works on problems of diverse scope where analysis of information requires evaluation of identifiable factors.\n""¢ Resolves and assesses a wide range of issues in creative ways and suggests variations in approach.\nInteraction:\n""¢ Enhances relationships and networks with senior internal/external partners who are not familiar with the subject matter often requiring persuasion.\n""¢ Works""",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'microsoft azure', 'data engineering', 'data bricks', 'aws', 'client engagement', 'ai solutions', 'togaf', 'aiml', 'data architecture', 'solution architecting', 'artificial intelligence', 'change management', 'gen', 'service orientation', 'solution design', 'gcp', 'gcp cloud', 'ml']",2025-06-11 05:30:27
BFSI Data and Analytics Delivery Manager,"NTT DATA, Inc.",18 - 23 years,Not Disclosed,['Bengaluru'],"Req ID: 326457\n\nWe are currently seeking a BFSI Data and Analytics Delivery Manager to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\n""Job DutiesData & Analytics Delivery Manager with over 18+ years of experience in BFSI Domain\nThe Data and Analytics Delivery Manager will oversee the successful delivery of the Client's data and analytics projects, ensuring our clients derive maximum value from their data assets. This leadership role involves setting strategy, managing delivery teams, collaborating across functions, and upholding data governance and quality standards. The ideal candidate brings strong technical and business acumen to build and execute data-driven strategies aligned with the Client's mission of transforming their business with data-driven insights.\nThe core responsibilities for the job include the following:\nProject and Program Oversight:\n""¢ Oversee end-to-end delivery of complex data and analytics projects, ensuring timely, high-quality, and cost-effective outcomes.\n""¢ Establish project governance, risk management, and quality assurance standards for effective project delivery.\n""¢ Monitor project portfolios and allocate resources to optimize productivity across multiple client engagements.\n\nStakeholder Collaboration and Engagement:\n""¢ Serve as the primary liaison between data delivery teams, sales, product, and client-facing teams to ensure client needs are met.\n""¢ Present data strategies, project status, and insights effectively to both technical and non-technical stakeholders, fostering alignment.\n""¢ Drive collaboration with product management and engineering teams to align on data needs and operational goals.\n\nInnovation and Technology Adoption:\n""¢ Stay abreast of the latest trends in GenAI, Agentic AI in data engineering, data science, machine learning, and AI to enhance the Client's data capabilities.\n""¢ Must have experience in Cloud Modernization, DWH, Datalake project execution\n""¢ Drive the adoption of advanced analytics tools and technologies to improve data delivery efficiency and solution impact.\n""¢ Assess and recommend new tools, platforms, and partners to continuously improve data solutions.\nTeam Development and Leadership:\n""¢ Recruit, mentor, and retain a high-performing data and analytics team, fostering a culture of collaboration and continuous improvement.\n""¢ Set performance goals, conduct regular evaluations, and provide ongoing feedback to support team growth.\n\nMinimum Skills Required:\n""¢ Educational BackgroundBachelor's or Master's degree in Data Science, Computer Science, Business Administration, or a related field.\n""¢ Experience15+ years of experience in data and analytics, including at least 5 years in a leadership role with a proven track record in delivery management.\n""¢ Technical ProficiencyDeep understanding of data warehousing, data visualization, data governance, data trust and big data tools (SQL, Python, R, Tableau, Power BI, and cloud platforms like AWS, Azure, or Google Cloud).\n""¢ Must have experience in Cloud Modernization, DWH, Datalake project execution\n""¢ BFSI KnowledgeMandatory to have worked in BFSI projects delivered Data & Analytics projects to BFSI clients\n""¢ Project Management ExpertiseStrong background in Agile, Scrum, or other project management methodologies.\n""¢ Leadership and CommunicationExcellent interpersonal and communication skills, with a demonstrated ability to lead, influence, and engage stakeholders at all levels.\n""¢ Analytical and Problem-Solving\n\nSkills:\nStrong analytical mindset with a track record of delivering actionable insights from complex data\nThe Data and Analytics Delivery Manager will oversee the successful delivery of the Client's data and analytics projects, ensuring our retail clients derive maximum value from their data assets. This leadership role involves setting strategy, managing delivery teams, collaborating across functions, and upholding data governance and quality standards. The ideal candidate brings strong technical and business acumen to build and execute data-driven strategies aligned with the Client's mission of transforming retail with data-driven insig""",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data governance', 'scrum', 'agile', 'data visualization', 'big data', 'advanced analytics', 'python', 'data analytics', 'data warehousing', 'power bi', 'microsoft azure', 'project management process', 'machine learning', 'sql', 'tableau', 'r', 'bfsi', 'data science', 'gcp', 'project execution', 'aws']",2025-06-11 05:30:29
Data Architect,.,7 - 12 years,20-35 Lacs P.A.,"['Hyderabad', 'Bengaluru']","Job Description\nWe are seeking a highly skilled Azure Data Engineer with strong expertise in Data Architecture, PySpark/Python, Azure Databricks, and data streaming solutions. The ideal candidate will have hands-on experience in designing and implementing large-scale data pipelines, along with solid knowledge of data governance and data modeling.\nKey Responsibilities\nDesign, develop, and optimize PySpark/Python-based data streaming jobs on Azure Databricks.\nBuild scalable and efficient data pipelines for batch and real-time processing.\nImplement data governance policies, ensuring data quality, security, and compliance.\nDevelop and maintain data models (dimensional, relational, NoSQL) to support analytics and reporting.\nCollaborate with cross-functional teams (data scientists, analysts, and business stakeholders) to deliver data solutions.\nTroubleshoot performance bottlenecks and optimize Spark jobs for efficiency.\nEnsure best practices in CI/CD, automation, and monitoring of data workflows.\nMentor junior engineers and lead technical discussions (for senior/managerial roles).\nMandatory Skills & Experience\n5+ years of relevant experience as a Data Engineer/Analyst/Architect (8+ years for Manager/Lead positions).\nExpert-level proficiency in PySpark/Python and Azure Databricks (must have worked on real production projects).\nStrong experience in building and optimizing streaming data pipelines (Kafka, Event Hubs, Delta Lake, etc.).\n4+ years of hands-on experience in data governance & data modeling (ER, star schema, data vault, etc.).\nIn-depth knowledge of Azure Data Factory, Synapse, ADLS, and SQL/NoSQL databases.\nExperience with Delta Lake, Databricks Workflows, and performance tuning.\nFamiliarity with data security, metadata management, and lineage tracking.\nExcellent communication skills (must be able to articulate technical concepts clearly).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Azure Databricks', 'Data Modeling', 'Data Governance', 'Python', 'ETL']",2025-06-11 05:30:30
Senior Big Data Developer - Youappi,Affle,7 - 10 years,Not Disclosed,"['Gurugram', 'Bengaluru']","We are looking for an experienced Senior Big Data Developer to join our team and help build and optimize high-performance, scalable, and resilient data processing systems. You will work in a fast-paced startup environment, handling highly loaded systems and developing data pipelines that process billions of records in real time.\nAs a key member of the Big Data team, you will be responsible for architecting and optimizing distributed systems, leveraging modern cloud-native technologies, and ensuring high availability and fault tolerance in our data infrastructure.\nPrimary Responsibilities:\nDesign, develop, and maintain real-time and batch processing pipelines using Apache Spark, Kafka, and Kubernetes.\nArchitect high-throughput distributed systems that handle large-scale data ingestion and processing.\nWork extensively with AWS services, including Kinesis, DynamoDB, ECS, S3, and Lambda.\nManage and optimize containerized workloads using Kubernetes (EKS) and ECS.\nImplement Kafka-based event-driven architectures to support scalable, low-latency applications.\nEnsure high availability, fault tolerance, and resilience of data pipelines.\nWork with MySQL, Elasticsearch, Aerospike, Redis, and DynamoDB to store and retrieve massive datasets efficiently.\nAutomate infrastructure provisioning and deployment using Terraform, Helm, or CloudFormation.\nOptimize system performance, monitor production issues, and ensure efficient resource utilization.\nCollaborate with data scientists, backend engineers, and DevOps teams to support advanced analytics and machine learning initiatives.\nContinuously improve and modernize the data architecture to support growing business needs.\nRequired Skills:\n7-10+ years of experience in big data engineering or distributed systems development.\nExpert-level proficiency in Scala, Java, or Python.\nDeep understanding of Kafka, Spark, and Kubernetes in large-scale environments.\nStrong hands-on experience with AWS (Kinesis, DynamoDB, ECS, S3, etc.).\nProven experience working with highly loaded, low-latency distributed systems.\nExperience with Kafka, Kinesis, Flink, or other streaming technologies for event-driven architectures.\nExpertise in SQL and database optimizations for MySQL, Elasticsearch, and NoSQL stores.\nStrong experience in automating infrastructure using Terraform, Helm, or CloudFormation.\nExperience managing production-grade Kubernetes clusters (EKS).\nDeep knowledge of performance tuning, caching strategies, and data consistency models.\nExperience working in a startup environment, adapting to rapid changes and building scalable solutions from scratch.\nNice to Have\nExperience with machine learning pipelines and AI-driven analytics.\nKnowledge of workflow orchestration tools such as Apache Airflow.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['kubernetes', 'python', 'aws iam', 'performance tuning', 'dynamo db', 'scala', 'data engineering', 'redis', 'helm', 'sql', 'nosql', 'database optimization', 'elastic search', 'java', 'system', 'lambda expressions', 'ecs', 'spark', 'automating', 'kafka', 'caching techniques', 'mysql', 'aws', 'big data']",2025-06-11 05:30:32
Salesforce Data Cloud Architect,"NTT DATA, Inc.",4 - 8 years,Not Disclosed,"['Chennai', 'Gurugram', 'Bengaluru']","We are currently seeking a Salesforce Data Cloud Architect to join our team in ""‹""‹""‹""‹""‹""‹""‹Hyderabad, Telangana, India.\n\n\nSalesforce Data Cloud Expertise: Extensive knowledge of Salesforce Data Cloud features, capabilities, and best practices.\n\n\nData Modeling: Strong experience in designing and implementing data models.\n\n\nData Integration: Experience with data integration tools and techniques.\n\n\nData Quality: Understanding of data quality concepts and practices.\n\n\nData Governance: Knowledge of data governance principles and practices.\n\n\nSQL: Proficiency in SQL for data querying and manipulation.\n\n\nProblem-Solving: Strong analytical and problem-solving skills.\n\n\nCommunication: Excellent communication and collaboration skills.\n\n\nLocation - Bengaluru,Chennai,Gurugram,Hyderabad,Noida,Pune",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'salesforce', 'data modeling', 'data governance', 'data integration', 'c#', 'python', 'business analysis', 'sharepoint', 'user stories', 'news writing', 'javascript', 'editing', 'react.js', 'java', 'product management', 'content writing', 'scrum', 'html', 'agile', 'etl', 'jira']",2025-06-11 05:30:34
Data Scientist Lead - L1,Wipro,5 - 8 years,Not Disclosed,['Gurugram'],"About The Role  \n\nRole Purpose\n\nThe purpose of the role is to define, architect and lead delivery of machine learning and AI solutions\n\n ? \n\nDo\n\n1. Demand generation through support in Solution development\n\na. Support Go-To-Market strategy\n\ni. Collaborate with sales, pre-sales &consulting team to assist in creating solutions and propositions for proactive demand generation\n\nii. Contribute to development solutions, proof of concepts aligned to key offerings to enable solution led sales\n\nb. Collaborate with different colleges and institutes for recruitment, joint research initiatives and provide data science courses\n\n2. Revenue generation through Building & operationalizing Machine Learning, Deep Learning solutions\n\na. Develop Machine Learning / Deep learning models for decision augmentation or for automation solutions\n\nb. Collaborate with ML Engineers, Data engineers and IT to evaluate ML deployment options\n\nc. Integrate model performance management tools into the current business infrastructure\n\n3. Team Management\n\na. Resourcing\n\ni. Support recruitment process to on-board right resources for the team\n\nb. Talent Management\n\ni. Support on boarding and training for the team members to enhance capability & effectiveness\n\nii. Manage team attrition\n\nc. Performance Management\n\ni. Conduct timely performance reviews and provide constructive feedback to own direct reports\n\nii. Be a role model to team for five habits\n\niii. Ensure that the Performance Nxt is followed for the entire team\n\nd. Employee Satisfaction and Engagement\n\ni. Lead and drive engagement initiatives for the team\n\n ? \n\nDeliver\n\n\nNo.\n\nPerformance Parameter\n\nMeasure 1. Demand generation Order booking 2. Revenue generation through delivery Timeliness, customer success stories, customer use cases 3. Capability Building & Team Management % trained on new skills, Team attrition %\n\n\n ? \n\n ? \nMandatory\n\nSkills:\nPython for Data Science.\n\nExperience5-8 Years.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'deep learning', 'data science', 'ml', 'data analytics', 'natural language processing', 'neural networks', 'predictive analytics', 'ml deployment', 'data engineering', 'artificial intelligence', 'sql', 'r', 'predictive modeling', 'statistical modeling', 'statistics']",2025-06-11 05:30:35
Data Bricks,PwC India,7 - 12 years,Not Disclosed,['Bengaluru'],"Job Summary:\n\nWe are seeking a talented Data Engineer with strong expertise in Databricks, specifically in Unity Catalog, PySpark, and SQL, to join our data team. Youll play a key role in building secure, scalable data pipelines and implementing robust data governance strategies using Unity Catalog.\n\nKey Responsibilities:",,,,"['DataBricks', 'Data Bricks', 'Pyspark', 'Delta Lake', 'Databricks Engineer', 'Unity Catalog', 'SQL']",2025-06-11 05:30:37
Data Analyst - Gurugram,Infosys,5 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru', 'PAN INDIA']","Responsibilities:\nUnderstand architecture requirements and ensure effective design, development, validation, and support activities.\nAnalyze user requirements, envisioning system features and functionality.\nIdentify bottlenecks and bugs, and recommend system solutions by comparing advantages and disadvantages of custom development.\nContribute to team meetings, troubleshooting development and production problems across multiple environments and operating platforms.\nEnsure effective design, development, validation, and support activities for Big Data solutions.\nTechnical and Professional Requirements:\nSkills:\nProficiency in Scala, Spark, Hive, and Kafka.\nIn-depth knowledge of design issues and best practices.\nSolid understanding of object-oriented programming.\nFamiliarity with various design, architectural patterns, and software development processes.\nExperience with both external and embedded databases.\nCreating database schemas that represent and support business processes.\nImplementing automated testing platforms and unit tests.\nPreferred Skills:\nTechnology -> Big Data -> Scala, Spark, Hive, Kafka\nAdditional Responsibilities:\nCompetencies:\nGood verbal and written communication skills.\nAbility to communicate with remote teams effectively.\nHigh flexibility to travel.\nEducational Requirements:Master of Computer Applications, Master of Technology, Master of Engineering, MSc, Bachelor of Technology, Bachelor of Computer Applications, Bachelor of Computer Science, Bachelor of Engineering",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Spark', 'Hive', 'Hadoop', 'Big Data', 'Kafka']",2025-06-11 05:30:38
Data Scientist-Artificial Intelligence,IBM,5 - 7 years,Not Disclosed,['Mumbai'],"Work with broader team to build, analyze and improve the AI solutions.\nYou will also work with our software developers in consuming different enterprise applications\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nResource should have 5-7 years of experience. Sound knowledge of Python and should know how to use the ML related services.\nProficient in Python with focus on Data Analytics Packages.\nStrategy Analyse large, complex data sets and provide actionable insights to inform business decisions. Strategy Design and implementing data models that help in identifying patterns and trends. Collaboration Work with data engineers to optimize and maintain data pipelines.\nPerform quantitative analyses that translate data into actionable insights and provide analytical, data-driven decision-making. Identify and recommend process improvements to enhance the efficiency of the data platform. Develop and maintain data models, algorithms, and statistical models\n\n\nPreferred technical and professional experience\nExperience with conversation analytics. Experience with cloud technologies\nExperience with data exploration tools such as Tableu",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['algorithms', 'python', 'data analytics', 'tableau', 'ml', 'hive', 'data analysis', 'natural language processing', 'pyspark', 'data warehousing', 'machine learning', 'artificial intelligence', 'sql', 'pandas', 'deep learning', 'java', 'data science', 'spark', 'kafka', 'hadoop', 'big data', 'aws', 'etl']",2025-06-11 05:30:40
Data Architect,Ford,14 - 17 years,Not Disclosed,['Chennai'],"We are looking for Data Solution Architect to join FC India IT Architecture team. In this role, you will define analytics solutions and guide engineering teams to implement big data solutions on the cloud. Work involves migrating data from legacy on-prem warehouses to Google cloud data platform. This role will provide architecture assistance to data engineering teams in India, with key responsibility of supporting applications globally. This role will also drive business adoption of the new platform and sunset of legacy platforms.\nGoogle Professional Solution Architect certification.\n8+ years of relevant work experience in analytics application and data architecture, with deep understanding of cloud hosting concepts and implementations.\n5+ years experience in Data and Solution Architecture in analytics space. Solid knowledge of cloud data architecture, data modelling principles, and expertise in Data Modeling tools.\nExperience in migrating legacy analytics applications to Cloud platform and business adoption of these platforms to build insights and dashboards through deep knowledge of traditional and cloud Data Lake, Warehouse and Mart concepts.\nGood understanding of domain driven design and data mesh principles.\nExperience with designing, building, and deploying ML models to solve business challenges using Python/BQML/Vertex AI on GCP.\nKnowledge of enterprise frameworks and technologies. Strong in architecture design patterns, experience with secure interoperability standards and methods, architecture tolls and process.\nDeep understanding of traditional and cloud data warehouse environment, with hands on programming experience building data pipelines on cloud in a highly distributed and fault-tolerant manner. Experience using Dataflow, pub/sub, Kafka, Cloud run, cloud functions, Bigquery, Dataform, Dataplex , etc.\nStrong understanding on DevOps principles and practices, including continuous integration and deployment (CI/CD), automated testing & deployment pipelines.\nGood understanding of cloud security best practices and be familiar with different security tools and techniques like Identity and Access Management (IAM), Encryption, Network Security, etc. Strong understanding of microservices architecture.\nNice to Have\nBachelor s degree in Computer science/engineering, Data science or related field.\nStrong leadership, communication, interpersonal, organizing, and problem-solving skills\nGood presentation skills with ability to communicate architectural proposals to diverse audiences (user groups, stakeholders, and senior management).\nExperience in Banking and Financial Regulatory Reporting space.\nAbility to work on multiple projects in a fast paced & dynamic environment.\nExposure to multiple, diverse technologies, platforms, and processing environments.\nUtilize Google Cloud Platform & Data Services to modernize legacy applications.\nUnderstand technical business requirements and define architecture solutions that align to Ford Motor & Credit Companies Patterns and Standards.\nCollaborate and work with global architecture teams to define analytics cloud platform strategy and build Cloud analytics solutions within enterprise data factory.\nProvide Architecture leadership in design & delivery of new Unified data platform on GCP.\nUnderstand complex data structures in analytics space as well as interfacing application systems. Develop and maintain conceptual, logical & physical data models. Design and guide Product teams on Subject Areas and Data Marts to deliver integrated data solutions.\nProvide architectural guidance for optimal solutions considering regional Regulatory needs.\nProvide architecture assessments on technical solutions and make recommendations that meet business needs and align with architectural governance and standard.\nGuide teams through the enterprise architecture processes and advise teams on cloud-based design, development, and data mesh architecture.\nProvide advisory and technical consulting across all initiatives including PoCs, product evaluations and recommendations, security, architecture assessments, integration considerations, etc.\nLeverage cloud AI/ML Platforms to deliver business and technical requirements.",Industry Type: Auto Components,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Solution architecture', 'Data modeling', 'Access management', 'Enterprise architecture', 'Consulting', 'Network security', 'Data structures', 'Analytics', 'Python']",2025-06-11 05:30:42
Data Analytics Lead (Consulting),Capgemini,10 - 15 years,Not Disclosed,['Pune'],"Capgemini Invent \n\nCapgemini Invent is the digital innovation, consulting and transformation brand of the Capgemini Group, a global business line that combines market leading expertise in strategy, technology, data science and creative design, to help CxOs envision and build whats next for their businesses.\n\n Your Role \nUse Design thinking and a consultative approach to conceive cutting edge technology solutions for business problems, mining core Insights as a service model\nEngage with project activities across the Information lifecycle.\nUnderstanding client requirements, develop data analytics strategy and solution that meets client requirements\nApply knowledge and explain the benefits to organizations adopting strategies relating to NextGen/ New age Data Capabilities\nBe proficient in evaluating new technologies and identifying practical business cases to develop enhanced business value and increase operating efficiency\nArchitect large scale AI/ML products/systems impacting large scale clients across industry\nOwn end to end solutioning and delivery of data analytics/transformation programs\nMentor and inspire a team of data scientists and engineers solving AI/ML problems through R&D while pushing the state-of-the-art solution\nLiaise with colleagues and business leaders across Domestic & Global Regions to deliver impactful analytics projects and drive innovation at scale\nAssist sales team in reviewing RFPs, Tender documents, and customer requirements\nDeveloping high-quality and impactful demonstrations, proof of concept pitches, solution documents, presentations, and other pre-sales assets\nHave in-depth business knowledge across a breath of functional areas across sectors such as CPRD/FS/MALS/Utilities/TMT\n\n\n Your Profile \nB.E. / B.Tech. + MBA (Systems / Data / Data Science/ Analytics / Finance) with a good academic background\nMinimum 10 years + on Job experience in data analytics with at least 7 years ofCPRD, FS, MALS, Utilities, TMT or other relevant domain experience required\nSpecialization in data science, data engineering or advance analytics filed is strongly recommended\nExcellent understanding and hand-on experience of data-science and machine learning techniques & algorithms for supervised & unsupervised problems, NLP and computer vision\nGood, applied statistics skills, such as distributions, statistical inference & testing, etc.\nExcellent understanding and hand-on experience on building Deep-learning models for text & image analytics (such as ANNs, CNNs, LSTM, Transfer Learning, Encoder and decoder, etc).\nProficient in coding in common data science language & tools such as R, Python, Go, SAS, Matlab etc.\nAt least 7 years experience deploying digital and data science solutions on large scale project is required\nAt least 7 years experience leading / managing a data Science team is required\n\nExposure or knowledge in cloud (AWS/GCP/Azure) and big data technologies such as Hadoop, Hive\n\n What you will love about working here \nWe recognize the significance of flexible work arrangements to provide support. Be it remote work, or flexible work hours, you will get an environment to maintain healthy work life balance.\nAt the heart of our mission is your career growth. Our array of career growth programs and diverse professions are crafted to support you in exploring a world of opportunities.\nEquip yourself with valuable certifications in the latest technologies such as Generative AI.\n\n\n About Capgemini \n\nCapgemini is a global business and technology transformation partner, helping organizations to accelerate their dual transition to a digital and sustainable world, while creating tangible impact for enterprises and society. It is a responsible and diverse group of 340,000 team members in more than 50 countries. With its strong over 55-year heritage, Capgemini is trusted by its clients to unlock the value of technology to address the entire breadth of their business needs. It delivers end-to-end services and solutions leveraging strengths from strategy and design to engineering, all fueled by its market leading capabilities in AI, cloud and data, combined with its deep industry expertise and partner ecosystem. The Group reported 2023 global revenues of 22.5 billion.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['fs', 'data science', 'tmt', 'python', 'data analytics', 'hive', 'pq', 'golang', 'neural networks', 'artificial intelligence', 'deep learning', 'gcp', 'hadoop', 'ml', 'matlab', 'cnn', 'oq', 'sas', 'natural language processing', 'iq', 'microsoft azure', 'machine learning', 'data engineering', 'r', 'aws', 'statistics']",2025-06-11 05:30:44
Data Architect,Ford,8 - 13 years,Not Disclosed,['Chennai'],"We are looking for Data Solution Architect to join FC India IT Architecture team. In this role, you will define analytics solutions and guide engineering teams to implement big data solutions on the cloud. Work involves migrating data from legacy on-prem warehouses to Google cloud data platform. This role will provide architecture assistance to data engineering teams in India, with key responsibility of supporting applications globally. This role will also drive business adoption of the new platform and sunset of legacy platforms.\nGoogle Professional Solution Architect certification.\n8+ years of relevant work experience in analytics application and data architecture, with deep understanding of cloud hosting concepts and implementations.\n5+ years experience in Data and Solution Architecture in analytics space. Solid knowledge of cloud data architecture, data modelling principles, and expertise in Data Modeling tools.\nExperience in migrating legacy analytics applications to Cloud platform and business adoption of these platforms to build insights and dashboards through deep knowledge of traditional and cloud Data Lake, Warehouse and Mart concepts.\nGood understanding of domain driven design and data mesh principles.\nExperience with designing, building, and deploying ML models to solve business challenges using Python/BQML/Vertex AI on GCP.\nKnowledge of enterprise frameworks and technologies. Strong in architecture design patterns, experience with secure interoperability standards and methods, architecture tolls and process.\nDeep understanding of traditional and cloud data warehouse environment, with hands on programming experience building data pipelines on cloud in a highly distributed and fault-tolerant manner. Experience using Dataflow, pub/sub, Kafka, Cloud run, cloud functions, Bigquery, Dataform, Dataplex , etc.\nStrong understanding on DevOps principles and practices, including continuous integration and deployment (CI/CD), automated testing & deployment pipelines.\nGood understanding of cloud security best practices and be familiar with different security tools and techniques like Identity and Access Management (IAM), Encryption, Network Security, etc. Strong understanding of microservices architecture.\nNice to Have\nBachelor s degree in Computer science/engineering, Data science or related field.\nStrong leadership, communication, interpersonal, organizing, and problem-solving skills\nGood presentation skills with ability to communicate architectural proposals to diverse audiences (user groups, stakeholders, and senior management).\nExperience in Banking and Financial Regulatory Reporting space.\nAbility to work on multiple projects in a fast paced & dynamic environment.\nExposure to multiple, diverse technologies, platforms, and processing environments.\nUtilize Google Cloud Platform & Data Services to modernize legacy applications.\nUnderstand technical business requirements and define architecture solutions that align to Ford Motor & Credit Companies Patterns and Standards.\nCollaborate and work with global architecture teams to define analytics cloud platform strategy and build Cloud analytics solutions within enterprise data factory.\nProvide Architecture leadership in design & delivery of new Unified data platform on GCP.\nUnderstand complex data structures in analytics space as well as interfacing application systems. Develop and maintain conceptual, logical & physical data models. Design and guide Product teams on Subject Areas and Data Marts to deliver integrated data solutions.\nProvide architectural guidance for optimal solutions considering regional Regulatory needs.\nProvide architecture assessments on technical solutions and make recommendations that meet business needs and align with architectural governance and standard.\nGuide teams through the enterprise architecture processes and advise teams on cloud-based design, development, and data mesh architecture.\nProvide advisory and technical consulting across all initiatives including PoCs, product evaluations and recommendations, security, architecture assessments, integration considerations, etc.\nLeverage cloud AI/ML Platforms to deliver business and technical requirements.",Industry Type: Automobile,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Solution architecture', 'Data modeling', 'Access management', 'Enterprise architecture', 'Consulting', 'Network security', 'Data structures', 'Analytics', 'Python']",2025-06-11 05:30:45
Big Data Developer,Persistent,4 - 8 years,Not Disclosed,['Pune'],"About Persistent\nWe are a trusted Digital Engineering and Enterprise Modernization partner, combining deep technical expertise and industry experience to help our clients anticipate what’s next. Our offerings and proven solutions create a unique competitive advantage for our clients by giving them the power to see beyond and rise above. We work with many industry-leading organizations across the world including 12 of the 30 most innovative US companies, 80% of the largest banks in the US and India, and numerous innovators across the healthcare ecosystem.\nOur growth trajectory continues, as we reported $1,231M annual revenue (16% Y-o-Y). Along with our growth, we’ve onboarded over 4900 new employees in the past year, bringing our total employee count to over 23,500+ people located in 19 countries across the globe.",,,,"['big data administration', 'hive', 'cloudera', 'data', 'business requirements', 'scala', 'big data technologies', 'data architecture', 'coding', 'java', 'spark', 'hadoop', 'big data', 'hbase', 'programming', 'python', 'software development', 'software testing', 'performance tuning', 'engineering', 'angular', 'hdfs', 'agile', 'sdlc', 'informatica']",2025-06-11 05:30:47
Big Data Lead,Persistent,6 - 10 years,Not Disclosed,['Hyderabad'],"About Persistent\nWe are a trusted Digital Engineering and Enterprise Modernization partner, combining deep technical expertise and industry experience to help our clients anticipate what’s next. Our offerings and proven solutions create a unique competitive advantage for our clients by giving them the power to see beyond and rise above. We work with many industry-leading organizations across the world including 12 of the 30 most innovative US companies, 80% of the largest banks in the US and India, and numerous innovators across the healthcare ecosystem.\nOur growth trajectory continues, as we reported $1,231M annual revenue (16% Y-o-Y). Along with our growth, we’ve onboarded over 4900 new employees in the past year, bringing our total employee count to over 23,500+ people located in 19 countries across the globe.",,,,"['big data technologies', 'hive', 'cloudera', 'scala', 'data warehousing', 'apache pig', 'sql', 'streaming', 'java', 'apache', 'data modeling', 'spark', 'design', 'flume', 'hadoop', 'big data', 'etl', 'big data hadoop', 'hbase', 'python', 'oracle', 'oozie', 'talend', 'data processing', 'machine learning', 'sql server', 'nosql', 'mapreduce', 'kafka', 'hdfs', 'sqoop', 'aws']",2025-06-11 05:30:49
Big Data Lead,Persistent,6 - 10 years,Not Disclosed,['Pune'],"About Persistent\nWe are a trusted Digital Engineering and Enterprise Modernization partner, combining deep technical expertise and industry experience to help our clients anticipate what’s next. Our offerings and proven solutions create a unique competitive advantage for our clients by giving them the power to see beyond and rise above. We work with many industry-leading organizations across the world including 12 of the 30 most innovative US companies, 80% of the largest banks in the US and India, and numerous innovators across the healthcare ecosystem.\nOur growth trajectory continues, as we reported $1,231M annual revenue (16% Y-o-Y). Along with our growth, we’ve onboarded over 4900 new employees in the past year, bringing our total employee count to over 23,500+ people located in 19 countries across the globe.",,,,"['big data technologies', 'hive', 'cloudera', 'scala', 'data warehousing', 'apache pig', 'sql', 'streaming', 'java', 'apache', 'data modeling', 'spark', 'design', 'flume', 'hadoop', 'big data', 'etl', 'big data hadoop', 'hbase', 'python', 'oracle', 'oozie', 'talend', 'data processing', 'machine learning', 'sql server', 'nosql', 'mapreduce', 'kafka', 'hdfs', 'sqoop', 'aws']",2025-06-11 05:30:51
Big Data Developer,Persistent,4 - 8 years,Not Disclosed,['Hyderabad'],"About Persistent\nWe are a trusted Digital Engineering and Enterprise Modernization partner, combining deep technical expertise and industry experience to help our clients anticipate what’s next. Our offerings and proven solutions create a unique competitive advantage for our clients by giving them the power to see beyond and rise above. We work with many industry-leading organizations across the world including 12 of the 30 most innovative US companies, 80% of the largest banks in the US and India, and numerous innovators across the healthcare ecosystem.\nOur growth trajectory continues, as we reported $1,231M annual revenue (16% Y-o-Y). Along with our growth, we’ve onboarded over 4900 new employees in the past year, bringing our total employee count to over 23,500+ people located in 19 countries across the globe.",,,,"['big data administration', 'cloudera', 'hive', 'data', 'business requirements', 'scala', 'big data technologies', 'data architecture', 'coding', 'java', 'spark', 'hadoop', 'big data', 'hbase', 'programming', 'python', 'software development', 'software testing', 'performance tuning', 'engineering', 'angular', 'hdfs', 'agile', 'sdlc', 'informatica']",2025-06-11 05:30:52
Big Data Architect,Persistent,8 - 13 years,Not Disclosed,['Pune'],"About Persistent\nWe are a trusted Digital Engineering and Enterprise Modernization partner, combining deep technical expertise and industry experience to help our clients anticipate what’s next. Our offerings and proven solutions create a unique competitive advantage for our clients by giving them the power to see beyond and rise above. We work with many industry-leading organizations across the world including 12 of the 30 most innovative US companies, 80% of the largest banks in the US and India, and numerous innovators across the healthcare ecosystem.\nOur growth trajectory continues, as we reported $1,231M annual revenue (16% Y-o-Y). Along with our growth, we’ve onboarded over 4900 new employees in the past year, bringing our total employee count to over 23,500+ people located in 19 countries across the globe.",,,,"['big data', 'cloudera', 'hive', 'vertica', 'soa', 'amazon redshift', 'data warehousing', 'apache storm', 'apache pig', 'distributed computing', 'sql', 'spring', 'cloud', 'jms', 'java', 'data modeling', 'spark', 'looker', 'pentaho', 'hadoop', 'etl', 'activemq', 'rdbms', 'power bi', 'impala', 'hdf', 'analysis', 'rabbitmq', 'nosql', 'apache nifi', 'saas', 'kafka', 'ioc', 'aws']",2025-06-11 05:30:54
Big Data Architect,Persistent,8 - 13 years,Not Disclosed,['Hyderabad'],"About Persistent\nWe are a trusted Digital Engineering and Enterprise Modernization partner, combining deep technical expertise and industry experience to help our clients anticipate what’s next. Our offerings and proven solutions create a unique competitive advantage for our clients by giving them the power to see beyond and rise above. We work with many industry-leading organizations across the world including 12 of the 30 most innovative US companies, 80% of the largest banks in the US and India, and numerous innovators across the healthcare ecosystem.\nOur growth trajectory continues, as we reported $1,231M annual revenue (16% Y-o-Y). Along with our growth, we’ve onboarded over 4900 new employees in the past year, bringing our total employee count to over 23,500+ people located in 19 countries across the globe.",,,,"['data warehousing', 'cloudera', 'hive', 'vertica', 'soa', 'amazon redshift', 'apache storm', 'apache pig', 'distributed computing', 'sql', 'spring', 'cloud', 'jms', 'java', 'data modeling', 'spark', 'looker', 'pentaho', 'hadoop', 'big data', 'etl', 'activemq', 'rdbms', 'power bi', 'impala', 'hdf', 'analysis', 'rabbitmq', 'nosql', 'apache nifi', 'saas', 'kafka', 'ioc', 'aws']",2025-06-11 05:30:56
"Big Data Lead (Subject Matter Expert- SME) | Lower Parel, Mumbai",Pracemo Global Solutions,8 - 13 years,35-50 Lacs P.A.,['Mumbai( Lower Parel )'],"Hiring Big Data Lead with 8+ years experience for US Shift time:\n\nMust Have:\n- Big Data: Spark, Hadoop, Kafka, Hive, Flink\n- Backend: Python, Scala\n- NoSQL: MongoDB, Cassandra\n- Cloud: AWS/AZURE/GCP, Snowflake, Databricks\n- Docker, Kubernetes, CI/CD\n\nRequired Candidate profile\n- Excellent in Mentoring/ Training in Big Data- HDFS, YARN, Airflow, Hive, Mapreduce, Hbase, Kafka & ETL/ELT, real-time streaming, data modeling\n- Immediate Joiner is plus\n- Excellent in Communication",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Hadoop', 'Big Data', 'Hive', 'SCALA', 'Spark', 'Cloudera', 'Cassandra', 'Kafka', 'Flume', 'Mapreduce', 'Hdfs', 'Impala', 'Spark Streaming', 'YARN', 'HBase', 'Apache Zookeeper', 'Apache Storm', 'NoSQL', 'Apache Pig', 'Sqoop', 'Kudu', 'MongoDB', 'Oozie']",2025-06-11 05:30:58
Senior Data Developer-MSBI,Intelliswift software,9 - 14 years,Not Disclosed,['Gurugram'],"Job Title: Senior Data Developer MSBI\nKey Responsibilities:\nDevelop new OLAP cubes using SSAS.\nDesign and implement robust ETL workflows using SSIS.\nBuild and maintain SSRS reports, including development of new static reports.\nLead new data integration projects and complex ETL flow implementations.\nPerform in-depth maintenance and optimization of existing cubes and data pipelines.\nCollaborate closely with Samsung Electronics and SDSE clients, particularly with the Samsung Nordics BI team.\nGather business requirements, provide solutions, and ensure timely delivery.\nUphold best coding practices, clean code principles, and performance optimization.\nRequired Skills:\nDeep expertise in MSBI Stack – SSIS, SSAS, SSRS.\nStrong command of MSSQL and relational database concepts.\nHands-on experience in data warehousing, OLAP cube development, and BI reporting.\nProven experience working in large enterprise environments with cross-functional teams.\nExcellent communication, problem-solving, and stakeholder management skills.",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['MS SQL', 'Tabular cube', 'SSAS', 'MSBI', 'SSIS', 'MSBI Developer']",2025-06-11 05:30:59
Lead Data Scientist - Media Mix Modelling (MMM),Blend360 India,7 - 10 years,Not Disclosed,['Hyderabad'],"Job Description\nWe are seeking a highly skilled Data Scientist/Analyst with expertise in Media Mix Modeling (MMM) to join our dynamic analytics team. The ideal candidate will play a critical role in providing actionable insights and optimizing marketing spend across various channels by leveraging statistical models and data-driven techniques.\nKey Responsibilities:\nDevelop and implement Media Mix Models to optimize marketing spend across different channels (e.g., TV, digital, radio, print, etc.).\nAnalyze historical data to understand the impact of marketing efforts and determine the effectiveness of different media channels.\nCollaborate with marketing and business teams to translate business objectives into quantitative analyses and actionable insights.\nBuild predictive models to forecast the impact of future marketing activities and recommend budget allocation.\nPresent and communicate complex findings in a clear, concise, and actionable manner to both technical and non-technical stakeholders.\nPerform deep-dive analyses of marketing campaigns and customer data to identify trends, opportunities, and areas for improvement.\nEnsure data integrity, accuracy, and consistency in all analyses and models.\nStay up-to-date with the latest trends and advancements in media mix modeling, marketing analytics, and data science.\nCollaborate with cross-functional teams including Data Engineering, Marketing, and Business Intelligence to ensure seamless data flow and integration.\nCreate and maintain documentation for all models, methodologies, and analysis processes.\n\n\nQualifications\nBachelor s or Master s degree in Data Science, Statistics, Economics, Mathematics, or a related field.\nProven experience (6+ years) working in Media Mix Modeling (MMM) and/or marketing analytics.",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Analytical', 'Social media', 'Consulting', 'data integrity', 'Regression analysis', 'Business intelligence', 'CRM', 'SQL', 'Python']",2025-06-11 05:31:01
Data Consultant-Data Governance,IBM,3 - 6 years,Not Disclosed,['Hyderabad'],"As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You’ll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you’ll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you’ll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\n\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviour’s.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience in the integration efforts between Alation and Manta, ensuring seamless data flow and compatibility.\nCollaborate with cross-functional teams to gather requirements and design solutions that leverage both Alation and Manta platforms effectively.\nDevelop and maintain data governance processes and standards within Alation, leveraging Manta's data lineage capabilities.\nAnalyze data lineage and metadata to provide insights into data quality, compliance, and usage patterns..\n\n\nPreferred technical and professional experience\nLead the evaluation and implementation of new features and updates for both Alation and Manta platforms\nEnsuring alignment with organizational goals and objectives.\nDrive continuous improvement initiatives to enhance the efficiency and effectiveness of data management processes, leveraging Alati",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['elastic search', 'data quality', 'data governance', 'splunk', 'agile', 'ab initio', 'metadata', 'data management', 'data analysis', 'data warehousing', 'business intelligence', 'sql', 'data cleansing', 'tableau', 'data modeling', 'big data', 'etl', 'informatica', 'data integration', 'data profiling']",2025-06-11 05:31:03
Collibra Data Governance Specialist,DXC Technology,5 - 15 years,Not Disclosed,['Bengaluru'],"Job Description:\nCollibra Data Governance Specialist\nJob Location: Hyderabad / Bangalore / Chennai / Kolkata / Noida/ Gurgaon / Pune / Indore / Mumbai\nRequired Skills\n5 - 15 years of experience in data governance and/or metadata management.\nHands-on experience with Collibra Data Governance Center (Collibra DGC), including workflow configuration, cataloging, and operating model customization .\nStrong knowledge of metadata management, data lineage, and data quality principles .\nHands-on experience with Snowflake\nFamiliarity with data integration tools and AWS cloud platform\nExperience with SQL and working knowledge of relational databases.\nUnderstanding of data privacy regulations (e. g. , GDPR, CCPA) and compliance frameworks.\nPreferred Skills\nCertifications such as Collibra Certified Solution Architect .\nExperience integrating Collibra with tools like Snowflake, Tableau or other BI/analytics platforms.\nExposure to DataOps, MDM (Master Data Management) , and data governance frameworks like DAMA-DMBOK.\nStrong communication and stakeholder management skills.\nRecruitment fraud is a scheme in which fictitious job opportunities are offered to job seekers typically through online services, such as false websites, or through unsolicited emails claiming to be from the company. These emails may request recipients to provide personal information or to make payments as part of their illegitimate recruiting process. DXC does not make offers of employment via social media networks and DXC never asks for any money or payments from applicants at any point in the recruitment process, nor ask a job seeker to purchase IT or other equipment on our behalf. More information on employment scams is available here .",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['metadata', 'Social media', 'data governance', 'Workflow', 'Data quality', 'data privacy', 'Stakeholder management', 'Solution Architect', 'Analytics', 'SQL']",2025-06-11 05:31:04
Cloudera Data Platform- Hive/Impala/Kudo,IBM,3 - 8 years,Not Disclosed,['Mumbai'],"Role Overview :\nAs a Big Data Engineer, you'll design and build robust data pipelines on Cloudera using Spark (Scala/PySpark) for ingestion, transformation, and processing of high-volume data from banking systems.\n\n Key Responsibilities :\nBuild scalable batch and real-time ETL pipelines using Spark and Hive\nIntegrate structured and unstructured data sources\nPerform performance tuning and code optimization\nSupport orchestration and job scheduling (NiFi, Airflow)\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience 3-15 Years\nProficiency in PySpark/Scala with Hive/Impala\nExperience with data partitioning, bucketing, and optimization\nFamiliarity with Kafka, Iceberg, NiFi is a must\nKnowledge of banking or financial datasets is a plus",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'scala', 'impala', 'apache nifi', 'kafka', 'cloudera', 'pyspark', 'sql', 'etl pipelines', 'apache', 'java', 'unix shell scripting', 'spark', 'gcp', 'shell scripting', 'hadoop', 'big data', 'etl', 'hbase', 'python', 'performance tuning', 'oozie', 'airflow', 'microsoft azure', 'nosql', 'cassandra', 'sqoop', 'aws', 'unix']",2025-06-11 05:31:06
Data Scientist,Ford,3 - 7 years,Not Disclosed,['Chennai'],"As a Data Scientist, you will be part of a high performing team working on exciting opportunities in AI/ML within Ford Credit. We are looking for a seasoned Data Scientist with proven expertise in implementing Machine Learning/Optimization solutions with familiarity in Generative AI and a good grasp of Statistics.\nDevelop Machine Learning (Supervised/Unsupervised learning), Neural Networks (ANN, CNN, RNN, LSTM, Decision tree, Encoder, Decoder), Natural Language Processing, Generative AI (LLMs, Lang Chain, RAG, Vector Database) .\nHands-on Expertise in Python programming (OOPs concepts), SQL (relational/non-relational databases), experience in handling various data science libraries (Pandas, NumPy, SciPy, Sklearn, TensorFlow, Keras, Pytorch, etc. ) would be a necessary requirement.\nExposure to Cloud technologies (e. g. , Google Cloud/AWS/Azure), including executing Machine Learning algorithms on Cloud is necessary.\nExposure to Generative AI technologies.\nProfessional Qualification:\nPotential candidates should possess 3 to 7 years of working experience as a Data Scientist.\nBE/MSc/ MTech /ME/PhD (Computer Science/Maths, Statistics).\nPossess a strong analytical mindset and be very comfortable with data.\nExperience with handling both relational and non-relational data.\nHands-on with analytics methods (descriptive / predictive / prescriptive) , Statistical Analysis, Probability and Data Visualization tools (Python-Matplotlib, Seaborn).\nBackground of Computer Science with excellent Data Science working experience.\nTechnical Experience:\nDevelop Machine Learning (Supervised/Unsupervised learning), Neural Networks (ANN, CNN, RNN, LSTM, Decision tree, Encoder, Decoder), Natural Language Processing, Generative AI (LLMs, Lang Chain, RAG, Vector Database) .\nHands-on Expertise in Python programming (OOPs concepts), SQL (relational/non-relational databases), experience in handling various data science libraries (Pandas, NumPy, SciPy, Sklearn, TensorFlow, Keras, Pytorch, etc. ) would be a necessary requirement.\nExposure to Cloud technologies (e. g. , Google Cloud/AWS/Azure), including executing Machine Learning algorithms on Cloud is necessary.\nExposure to Generative AI technologies.\nAbility to scope the problem statement, data preparation, training and making the AI/ML model production ready.\nWork with business partners to understand the problem statement, translate the same into analytical problem.\nAbility to manipulate structured and unstructured data.\nDevelop, test and improve existing machine learning models.\nAnalyse large and complex data sets to derive valuable insights.\nResearch and implement best practices to enhance existing machine learning infrastructure. Develop prototypes for future exploration.\nDesign and evaluate approaches for handling large volume of real data streams.\nAbility to determine appropriate analytical methods to be used.\nCollaborate with data engineers, solutions architects, application engineers, and product teams across time zones to develop data and model pipelines",Industry Type: Auto Components,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'data science', 'Neural networks', 'Analytical', 'Machine learning', 'Natural language processing', 'data visualization', 'Analytics', 'SQL', 'Python']",2025-06-11 05:31:08
Data Analyst - L4,Wipro,5 - 8 years,Not Disclosed,['Hyderabad'],"Role Purpose\nThe purpose of this role is to interpret data and turn into information (reports, dashboards, interactive visualizations etc) which can offer ways to improve a business, thus affecting business decisions.\nDo\n1. Managing the technical scope of the project in line with the requirements at all stages\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\nb. Develop record management process and policies\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\nd. Providing sales data, proposals, data insights and account reviews to the client base\ne. Identify areas to increase efficiency and automation of processes\nf. Set up and maintain automated data processes\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\nh. Produce and track key performance indicators\n2. Analyze the data sets and provide adequate information\na. Liaise with internal and external clients to fully understand data content\nb. Design and carry out surveys and analyze survey data as per the customer requirement\nc. Analyze and interpret complex data sets relating to customers business and prepare reports for internal and external audiences using business analytics reporting tools\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\nf. Develop predictive models and share insights with the clients as per their requirement\n\nDeliver\n\nNo Performance Parameter Measure\n1. Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\nMandatory Skills: Tableau.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'Tableau', 'data warehouses', 'data integration', 'Data modelling']",2025-06-11 05:31:09
Data Analyst - L4,Wipro,5 - 8 years,Not Disclosed,['Hyderabad'],"Role Purpose\nThe purpose of this role is to interpret data and turn into information (reports, dashboards, interactive visualizations etc) which can offer ways to improve a business, thus affecting business decisions.\nDo\n1. Managing the technical scope of the project in line with the requirements at all stages\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\nb. Develop record management process and policies\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\nd. Providing sales data, proposals, data insights and account reviews to the client base\ne. Identify areas to increase efficiency and automation of processes\nf. Set up and maintain automated data processes\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\nh. Produce and track key performance indicators\n2. Analyze the data sets and provide adequate information\na. Liaise with internal and external clients to fully understand data content\nb. Design and carry out surveys and analyze survey data as per the customer requirement\nc. Analyze and interpret complex data sets relating to customers business and prepare reports for internal and external audiences using business analytics reporting tools\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\nf. Develop predictive models and share insights with the clients as per their requirement\n\n\n\nDeliver\n\nNoPerformance ParameterMeasure\n1.Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\nMandatory Skills: Database Architecting.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'data validation', 'data mining', 'data warehousing', 'business analytics', 'dashboards', 'data integration']",2025-06-11 05:31:11
Data Analyst - L4,Wipro,5 - 8 years,Not Disclosed,['Hyderabad'],"Role Purpose\nThe purpose of this role is to interpret data and turn into information (reports, dashboards, interactive visualizations etc) which can offer ways to improve a business, thus affecting business decisions.\n\n\n\nDo\n1. Managing the technical scope of the project in line with the requirements at all stages\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\nb. Develop record management process and policies\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\nd. Providing sales data, proposals, data insights and account reviews to the client base\ne. Identify areas to increase efficiency and automation of processes\nf. Set up and maintain automated data processes\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\nh. Produce and track key performance indicators\n2. Analyze the data sets and provide adequate information\na. Liaise with internal and external clients to fully understand data content\nb. Design and carry out surveys and analyze survey data as per the customer requirement\nc. Analyze and interpret complex data sets relating to customers business and prepare reports for internal and external audiences using business analytics reporting tools\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\nf. Develop predictive models and share insights with the clients as per their requirement\n\n\n\nDeliver\n\nNoPerformance ParameterMeasure1.Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\n\n\n\n\n\nMandatory Skills: Database Architecting.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Database Architecting', 'data analysis', 'data mining', 'business analytics', 'reporting tools']",2025-06-11 05:31:13
Data Scientist,Ford,3 - 7 years,Not Disclosed,['Chennai'],"As a Data Scientist, you will be part of a high performing team working on exciting opportunities in AI/ML within Ford Credit. We are looking for a seasoned Data Scientist with proven expertise in implementing Machine Learning/Optimization solutions with familiarity in Generative AI and a good grasp of Statistics.\nDevelop Machine Learning (Supervised/Unsupervised learning), Neural Networks (ANN, CNN, RNN, LSTM, Decision tree, Encoder, Decoder), Natural Language Processing, Generative AI (LLMs, Lang Chain, RAG, Vector Database) .\nHands-on Expertise in Python programming (OOPs concepts), SQL (relational/non-relational databases), experience in handling various data science libraries (Pandas, NumPy, SciPy, Sklearn, TensorFlow, Keras, Pytorch, etc.) would be a necessary requirement.\nExposure to Cloud technologies (e.g., Google Cloud/AWS/Azure), including executing Machine Learning algorithms on Cloud is necessary.\nExposure to Generative AI technologies.\nProfessional Qualification:\nPotential candidates should possess 3 to 7 years of working experience as a Data Scientist.\nBE/MSc/ MTech /ME/PhD (Computer Science/Maths, Statistics).\nPossess a strong analytical mindset and be very comfortable with data.\nExperience with handling both relational and non-relational data.\nHands-on with analytics methods (descriptive / predictive / prescriptive) , Statistical Analysis, Probability and Data Visualization tools (Python-Matplotlib, Seaborn).\nBackground of Computer Science with excellent Data Science working experience.\nTechnical Experience:\nDevelop Machine Learning (Supervised/Unsupervised learning), Neural Networks (ANN, CNN, RNN, LSTM, Decision tree, Encoder, Decoder), Natural Language Processing, Generative AI (LLMs, Lang Chain, RAG, Vector Database) .\nHands-on Expertise in Python programming (OOPs concepts), SQL (relational/non-relational databases), experience in handling various data science libraries (Pandas, NumPy, SciPy, Sklearn, TensorFlow, Keras, Pytorch, etc.) would be a necessary requirement.\nExposure to Cloud technologies (e.g., Google Cloud/AWS/Azure), including executing Machine Learning algorithms on Cloud is necessary.\nExposure to Generative AI technologies.\nAbility to scope the problem statement, data preparation, training and making the AI/ML model production ready.\nWork with business partners to understand the problem statement, translate the same into analytical problem.\nAbility to manipulate structured and unstructured data.\nDevelop, test and improve existing machine learning models.\nAnalyse large and complex data sets to derive valuable insights.\nResearch and implement best practices to enhance existing machine learning infrastructure. Develop prototypes for future exploration.\nDesign and evaluate approaches for handling large volume of real data streams.\nAbility to determine appropriate analytical methods to be used.\nCollaborate with data engineers, solutions architects, application engineers, and product teams across time zones to develop data and model pipelines",Industry Type: Automobile,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'data science', 'Neural networks', 'Analytical', 'Machine learning', 'Natural language processing', 'data visualization', 'Analytics', 'SQL', 'Python']",2025-06-11 05:31:15
Data Analyst - L4,Wipro,5 - 8 years,Not Disclosed,['Chennai'],"Role Purpose\nThe purpose of this role is to interpret data and turn into information (reports, dashboards, interactive visualizations etc) which can offer ways to improve a business, thus affecting business decisions.\n\n\n\nDo\n1. Managing the technical scope of the project in line with the requirements at all stages\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\nb. Develop record management process and policies\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\nd. Providing sales data, proposals, data insights and account reviews to the client base\ne. Identify areas to increase efficiency and automation of processes\nf. Set up and maintain automated data processes\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\nh. Produce and track key performance indicators\n2. Analyze the data sets and provide adequate information\na. Liaise with internal and external clients to fully understand data content\nb. Design and carry out surveys and analyze survey data as per the customer requirement\nc. Analyze and interpret complex data sets relating to customers business and prepare reports for internal and external audiences using business analytics reporting tools\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\nf. Develop predictive models and share insights with the clients as per their requirement\n\n\n\nDeliver\n\nNoPerformance ParameterMeasure1.Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\n\n\n\n\n\nMandatory Skills: Google BigQuery.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Google BigQuery', 'analytics reporting', 'data analysis', 'data mining', 'business analytics', 'bigquery', 'data integration']",2025-06-11 05:31:17
Data and Analytics Architect - L1,Wipro,8 - 10 years,Not Disclosed,['Hyderabad'],"Role Purpose\nThe purpose of the role is to define and develop Enterprise Data Structure along with Data Warehouse, Master Data, Integration and transaction processing with maintaining and strengthening the modelling standards and business information.\n\n\n\nDo\n1. Define and Develop Data Architecture that aids organization and clients in new/ existing deals\na. Partnering with business leadership (adopting the rationalization of the data value chain) to provide strategic, information-based recommendations to maximize the value of data and information assets, and protect the organization from disruptions while also embracing innovation\nb. Assess the benefits and risks of data by using tools such as business capability models to create an data-centric view to quickly visualize what data matters most to the organization, based on the defined business strategy\nc. Create data strategy and road maps for the Reference Data Architecture as required by the clients\nd. Engage all the stakeholders to implement data governance models and ensure that the implementation is done based on every change request\ne. Ensure that the data storage and database technologies are supported by the data management and infrastructure of the enterprise\nf. Develop, communicate, support and monitor compliance with Data Modelling standards\ng. Oversee and monitor all frameworks to manage data across organization\nh. Provide insights for database storage and platform for ease of use and least manual work\ni. Collaborate with vendors to ensure integrity, objectives and system configuration\nj. Collaborate with functional & technical teams and clients to understand the implications of data architecture and maximize the value of information across the organization\nk. Presenting data repository, objects, source systems along with data scenarios for the front end and back end usage\nl. Define high-level data migration plans to transition the data from source to target system/ application addressing the gaps between the current and future state, typically in sync with the IT budgeting or other capital planning processes\nm. Knowledge of all the Data service provider platforms and ensure end to end view.\nn. Oversight all the data standards/ reference/ papers for proper governance\no. Promote, guard and guide the organization towards common semantics and the proper use of metadata\n\n\n\np. Collecting, aggregating, matching, consolidating, quality-assuring, persisting and distributing such data throughout an organization to ensure a common understanding, consistency, accuracy and control\nq. Provide solution of RFPs received from clients and ensure overall implementation assurance\ni. Develop a direction to manage the portfolio of all the databases including systems, shared infrastructure services in order to better match business outcome objectives\nii. Analyse technology environment, enterprise specifics, client requirements to set a collaboration solution for the big/small data\niii. Provide technical leadership to the implementation of custom solutions through thoughtful use of modern technology\niv. Define and understand current issues and problems and identify improvements\nv. Evaluate and recommend solutions to integrate with overall technology ecosystem keeping consistency throughout\nvi. Understand the root cause problem in integrating business and product units\nvii. Validate the solution/ prototype from technology, cost structure and customer differentiation point of view\nviii. Collaborating with sales and delivery leadership teams to identify future needs and requirements\nix. Tracks industry and application trends and relates these to planning current and future IT needs\n\n\n\n2. Building enterprise technology environment for data architecture management\na. Develop, maintain and implement standard patterns for data layers, data stores, data hub & lake and data management processes\nb. Evaluate all the implemented systems to determine their viability in terms of cost effectiveness\nc. Collect all the structural and non-structural data from different places integrate all the data in one database form\nd. Work through every stage of data processing: analysing, creating, physical data model designs, solutions and reports\ne. Build the enterprise conceptual and logical data models for analytics, operational and data mart structures in accordance with industry best practices\nf. Implement the best security practices across all the data bases based on the accessibility and technology\ng. Strong understanding of activities within primary discipline such as Master Data Management (MDM), Metadata Management and Data Governance (DG)\nh. Demonstrate strong experience in Conceptual, Logical and physical database architectures, design patterns, best practices and programming techniques around relational data modelling and data integration\n\n\n\n3. Enable Delivery Teams by providing optimal delivery solutions/ frameworks\na. Build and maintain relationships with delivery and practice leadership teams and other key stakeholders to become a trusted advisor\nb. Define database physical structure, functional capabilities, security, back-up and recovery specifications\nc. Develops and establishes relevant technical, business process and overall support metrics (KPI/SLA) to drive results\nd. Monitor system capabilities and performance by performing tests and configurations\ne. Integrate new solutions and troubleshoot previously occurred errors\nf. Manages multiple projects and accurately reports the status of all major assignments while adhering to all project management standards\ng. Identify technical, process, structural risks and prepare a risk mitigation plan for all the projects\nh. Ensure quality assurance of all the architecture or design decisions and provides technical mitigation support to the delivery teams\ni. Recommend tools for reuse, automation for improved productivity and reduced cycle times\nj. Help the support and integration team for better efficiency and client experience for ease of use by using AI methods.\nk. Develops trust and builds effective working relationships through respectful, collaborative engagement across individual product teams\nl. Ensures architecture principles and standards are consistently applied to all the projects\nm. Ensure optimal Client Engagement\ni. Support pre-sales team while presenting the entire solution design and its principles to the client\nii. Negotiate, manage and coordinate with the client teams to ensure all requirements are met\niii. Demonstrate thought leadership with strong technical capability in front of the client to win the confidence and act as a trusted advisor\nMandatory Skills: Prophecy.AI.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Prophecy.AI', 'metadata management', 'design patterns', 'data governance', 'data warehousing', 'AI methods', 'master data management']",2025-06-11 05:31:19
Internal Audit-Data Strategy - Analyst,Goldman Sachs,1 - 3 years,Not Disclosed,['Hyderabad'],"What We Do\nInternal Audit s mission is to independently assess the firm s internal control structure, including the\nfirm s governance processes and controls, risk management, capital and anti-financial crime\nframework. In addition, it is also to raise awareness of control risk and monitor the implementation of\nmanagement s control measures.\nIn doing so, internal Audit:\nCommunicates and reports on the effectiveness of the firm s governance, risk management and controls that mitigate current and evolving risk\nRaise awareness of control risk\nAssesses the firm s control culture and conduct risks; and\nMonitors management s implementation of control measures\nGoldman Sachs Internal Audit comprises individuals from diverse backgrounds including chartered\naccountants, developers, risk management professionals, cybersecurity professionals, and data\nscientists. We are organized into global teams comprising business and technology auditors to cover\nall the firm s businesses and functions, including securities, investment banking, consumer and\ninvestment management, risk management, finance, cyber-security and technology risk, and\nengineering.\nWho We Look For\nGoldman Sachs Internal Auditors demonstrate a strong risk, control and analytical mindset, exercise\nprofessional skepticism and challenge status quo on risks and control measures effectively with\nmanagement. We look for individuals who enjoy learning about audit, businesses, and processes,\nhave innovative and creative mindset in adapting analytical techniques to enhance audit function,\ndevelop teamwork and build relationships and are able to evolve and thrive in a fast-paced global\nenvironment.\nEmbedded - Data Analytics\nIn Internal Audit, we ensure that Goldman Sachs maintains effective controls by assessing the\nreliability of financial reports, monitoring the firm s compliance with laws and regulations, and\nadvising management on developing smart control solutions. Embed Data Analytics team leverages\nits programming and analytical capabilities to build innovative data driven solutions. The team works\nclosely with auditors to understand their pain points and develop data-centric solutions to address\nthe same.\nYour Impact\nAs part of the third line of defense, you will be involved in independently assessing the firm s overall\ncontrol environment and its effectiveness as it relates to current and emerging risks and\ncommunicating the results to local/ global management. In doing so, you will be supporting the\nprovision of independent, objective and timely assurance around the firm s internal control structure,\nthereby supporting the Audit Committee, Board of Directors and Risk Committee in fulfilling their\noversight responsibilities.\nWe are looking for a strong data scientist, passionate about using data to challenge the norm, to join\nour Embed Data Analytics team. The candidate will work closely with the audit teams to build\ninnovative and reusable analytical tools that will help make audit testing more efficient and provide\nmeaningful insights into firm s control environment.\nResponsibilities\nExecute on DA strategy developed by IA management within the context of audit responsibilities, such as risk assessment, audit planning, creation of reusable tools and providing innovative solutions to complex problems\nPartner with audit teams to help identify risks associated with businesses and facilitate strategic data sourcing and develop innovative solutions to increase efficiency and effectiveness of audit testing\nBuild production ready analytical tools to automate repeatable and reusable processes within IA\nBuild and manage relationships and communications with Audit team members\nBasic Qualifications\n1-3 years of experience with a minimum of Bachelor s in Computer Science, Math, or Statistics\nExperience with RDBMS/ SQL\nProficiency in programming languages, such as Python, Java, or C++\nKnowledge of basic statistics, including descriptive statistics, data distribution models,\nTime Series Analysis, correlation, and regression, and its application to data\nStrong team player with excellent communication skills (written and oral). Ability to communicate what is relevant and important in a clear and concise manner and ability to handle multiple tasks\nStrong contributing member of Data Science team and help build analytical capabilities for Internal Audit Division\nDriven and motivated and constantly taking initiative to improve performance\nPreferred Qualifications\nExperience with advanced data analytics tools and techniques\nStrong experience in RDBMS/ SQL and Data Warehousing.\nExposure to ETL Processes and Data Engineering.\nExperience in implementing Data Quality measures and entitlement models\nFamiliarity in programming languages such as Python.\nStrong team player with excellent communication skills (written and oral). Ability to communicate what is relevant and important in a clear and concise manner and ability to handle multiple tasks\nSelf-driven and motivated to take up initiatives to improve our processes.\nWe re committed to finding reasonable accommodations for candidates with special needs or disabilities during our recruiting process. Learn more: https: / / www.goldmansachs.com / careers / footer / disability-statement.html",Industry Type: Banking,Department: Risk Management & Compliance,"Employment Type: Full Time, Permanent","['C++', 'Manager Internal Audit', 'Assurance', 'Analytical', 'Data analytics', 'HTML', 'Data quality', 'Investment banking', 'Risk management', 'SQL']",2025-06-11 05:31:20
Data Visualization Expert - Quick sight,"NTT DATA, Inc.",2 - 5 years,Not Disclosed,['Chennai'],"We are currently seeking a Data Visualization Expert - Quick sight to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\n\n\n\n What awaits you/ Job Profile  \n\n   \nDesign and develop data visualizations using Amazon QuickSight to present complex data in clear and understandable Dashboards.\nCreate interactive dashboards and reports that allow end-users to explore data and draw meaningful conclusions.\nWork on Data preparation and ensure the good quality data is used in Visualization.\nCollaborate with data analysts and business stakeholders to understand data requirements, gather insights, and transform raw data into actionable visualizations.\nEnsure that the data visualizations are user-friendly, intuitive, and aesthetically pleasing. Optimize the user experience by incorporating best practices.\nIdentify and address performance bottlenecks in data queries and visualization.\nEnsure compliance with data security policies and governance guidelines when handling sensitive data within QuickSight.\nProvide training and support to end-users and stakeholders on how to interact with Dashboards.\nSelf-Managing and explore the latest technical development and incorporate in the project.\nExperience in analytics, reporting and business intelligence tools.\nUsing the Agile Methodology, attending daily standups and use of the Agile tools.\nLead Technical discussions with customers to find the best possible solutions.\n\n What should you bring along  \n\n   \n\n Must Have  \nOverall experience of 2-5 years in Data visualization development.\nMinimum of 2 years in QuickSight and 1-2 years in other BI Tools like Tableau, PowerBI, Qlik\nGood In writing complex SQL Scripting, Dataset Modeling.\nHands on in AWS -Athena, RDS, S3, IAM, permissions, Logging and monitoring Services.\nExperience working with various data sources and databases like Oracle, mySQL, S3, Athena.\nAbility to work with large datasets and design efficient data models for visualization.\nPrior experience in working in Agile, Scrum/Kanban working model.\n\n Nice to Have  \nKnowledge on Data ingestion and Data pipeline in AWS.\nKnowledge Amazon Q or AWS LLM Service to enable AI integration\n\n Must have skill  \n\nQuick sight, Tableau, SQL , AWS\n\n Good to have skills  \n\nQlikview ,Data Engineer, AWS LLM\n\n\n\n\n\n",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['sql', 'data visualization', 'quicksight', 'tableau', 'aws', 'hive', 'bi', 'data warehousing', 'dashboards', 'bi tools', 'iam', 'spark', 'kanban', 'ssrs', 'mysql', 'hadoop', 'etl', 'python', 'oracle', 'data analysis', 'power bi', 'amazon rds', 'sql server', 'qlikview', 'sql scripting', 'scrum', 'athena', 'agile', 'ssis']",2025-06-11 05:31:22
Data Science - Director job opening at GlobalData(Hyderabad),Globaldata,15 - 20 years,Not Disclosed,['Hyderabad( Kondapur )'],"R\nHello,\n\nUrgent job openings for Data Science - Director @ GlobalData(Hyderebad)\n\nJob Description given below please go through to understand the requirement.\n\nif requirement is matching to your profile & interested to apply please share your updated resume @ mail id (m.salim@globaldata.com).",,,,"['Data Science', 'Pytorch', 'Generative Ai Tools', 'Large Language Model', 'Python', 'Tensorflow', 'Natural Language Processing', 'Deep Learning']",2025-06-11 05:31:24
Data Visualization Expert - Quick sight,"NTT DATA, Inc.",4 - 5 years,Not Disclosed,['Chennai'],"We are currently seeking a Data Visualization Expert - Quick sight to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\n\n\n\n What awaits you/ Job Profile  \n\n\n\n Location Bangalore and Chennai, Hybrid mode,Immediate to 10 Days Notice period \nDevelop reports using Amazon Quicksight\nData Visualization DevelopmentDesign and develop data visualizations using Amazon Quicksight to present complex data in a clear and understandable format. Create interactive dashboards and reports that allow end-users to explore data and draw meaningful conclusions.\nData AnalysisCollaborate with data analysts and business stakeholders to understand data requirements, gather insights, and transform raw data into actionable visualizations.\nDashboard User Interface (UI) and User Experience (UX)Ensure that the data visualizations are user-friendly, intuitive, and aesthetically pleasing. Optimize the user experience by incorporating best practices in UI/UX design.\nData IntegrationWork closely with data engineers and data architects to ensure seamless integration of data sources into Quicksight, enabling real-time and up-to-date visualizations.\nPerformance OptimizationIdentify and address performance bottlenecks in data queries and visualization rendering to ensure quick and responsive dashboards.\nData Security and GovernanceEnsure compliance with data security policies and governance guidelines when handling sensitive data within Quicksight.\nTraining and DocumentationProvide training and support to end-users and stakeholders on how to interact with and interpret visualizations effectively. Create detailed documentation of the visualization development process.\nStay Updated with Industry TrendsKeep up to date with the latest data visualization trends, technologies, and best practices to continuously enhance the quality and impact of visualizations.\nUsing the Agile Methodology, attending daily standups and use of the Agile tools\nCollaborating with cross-functional teams and stakeholders to ensure data security, privacy, and compliance with regulations.\nusing Scrum/Kanban\nProficiency in Software Development best practices - Secure coding standards, Unit testing frameworks, Code coverage, Quality gates.\nAbility to lead and deliver change in a very productive way\nLead Technical discussions with customers to find the best possible solutions.\nW orking closely with the Project Manager, Solution Architect and managing client communication (as and when required)\n\n What should you bring along  \n\n\n\nMust Have\nPerson should have relevant work experience in analytics, reporting and business intelligence tools.\n4-5 years of hands-on experience in data visualization.\nRelatively 2-year Experience developing visualization using Amazon Quicksight.\nExperience working with various data sources and databases.\nAbility to work with large datasets and design efficient data models for visualization.\n\n\n\nNice to Have\nAI Project implementation and AI methods.\n\n Must have technical skill  \n\nQuick sight , SQL , AWS\n\n Good to have Technical skills  \n\nTableau, Data Engineer\n\n\n\n",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'data visualization', 'quicksight', 'software development', 'aws', 'hive', 'amazon redshift', 'unit testing', 'data warehousing', 'dashboards', 'business intelligence', 'spark', 'kanban', 'hadoop', 'etl', 'python', 'data analysis', 'ux', 'power bi', 'sql server', 'tableau', 'scrum', 'athena', 'agile', 'ssis']",2025-06-11 05:31:25
"Associate Director, Data and Analytics",Hsbc,10 - 15 years,Not Disclosed,['Pune'],"Engineer the data transformations and analysis for the Cash Equities Trading platform.\nTechnology SME on the real-time stream processing paradigm.\nBring your experience in Low latency, High through-put, auto scaling platform design and implementation.\nImplementing an end-to-end platform service, assessing the operations and non-functional needs clearly.\nDrive and document technical and functional decisions with appropriate diligence.\nProvide operational support and manage incidents.\nRequirements\nTo be successful in this role, you should meet the following requirements:\n10+ years of experience in data engineering technology and tools.\nPreferred having experience with Java / Scala based implementations for enterprise-wide platforms.\nExperience with Apache Beam, Google Dataflow, Apache Kafka for real-time steam processing technology stack.\nComplex state-full processing of events with partitioning for higher throughputs.\nHave dealt with fine-tuning the through-puts and improving the performance aspects on data pipelines.\nExperience with analytical data store optimizations, querying and managing them.\nExperience with alternate data engineering tools (Apache Flink, Apache Spark etc)\nAutomated CI/CD or operations concerns on the engineering platforms.\nInterpreting problems from functional context and transforming them into technology solutions.",Industry Type: Consumer Electronics & Appliances,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['operational support', 'Usage', 'spark', 'Analytical', 'SCALA', 'Banking', 'Technology solutions', 'Associate Director', 'Analytics', 'Financial services']",2025-06-11 05:31:27
Data Science Manager job opening at GlobalData(Hyd),Globaldata,10 - 15 years,Not Disclosed,['Hyderabad( Kondapur )'],"Hello,\n\nUrgent job openings for Data Science Manager role @ GlobalData(Hyd).\n\nJob Description given below please go through to understand the requirement.\nif requirement is matching to your profile & interested to apply please share your updated resume @ mail id (m.salim@globaldata.com).",,,,"['Artificial Intelligence', 'Natural Language Processing', 'Machine Learning', 'Deep Learning', 'Data Science', 'Tensorflow', 'Predictive Modeling', 'Azure', 'Power Bi', 'Tableau', 'NLP', 'Hadoop Spark', 'AWS', 'Python']",2025-06-11 05:31:28
Data & AI Technical Solution ArchitectsData & AI Technical Solution,"NTT DATA, Inc.",12 - 15 years,Not Disclosed,['Pune'],"Title : Data & AI Technical Solution ArchitectsData & AI Technical Solution Architects\n\nReq ID: 323749\n\nWe are currently seeking a Data & AI Technical Solution ArchitectsData & AI Technical Solution Architects to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nJob DutiesThe Data & AI Architect is a seasoned level expert who is responsible for participating in the delivery of multi-technology consulting services to clients by providing strategies and solutions on all aspects of infrastructure and related technology components.\n\nThis role collaborates with other stakeholders on the development of the architectural approach for one or more layer of a solution. This role has the primary objective is to work on strategic projects that ensure the optimal functioning of the client""™s technology infrastructure.\n""¢ Key Responsibilities:\n""¢ Ability and experience to have conversations with the CEO, Business owners and CTO/CDO\n""¢ Break down intricate business challenges, devise effective solutions, and focus on client needs.\n""¢ Craft high level innovative solution approach for complex business problems\n""¢ Utilize best practices and creativity to address challenges\n""¢ Leverage market research, formulate perspectives, and communicate insights to clients\n""¢ Establish strong client relationships\n""¢ Interact at appropriate levels to ensure client satisfaction\n""¢ Knowledge and Attributes:\n""¢ Ability to focus on detail with an understanding of how it impacts the business strategically.\n""¢ Excellent client service orientation.\n""¢ Ability to work in high-pressure situations.\n""¢ Ability to establish and manage processes and practices through collaboration and the understanding of business.\n""¢ Ability to create new and repeat business for the organization.\n""¢ Ability to contribute information on relevant vertical markets\n""¢ Ability to contribute to the improvement of internal effectiveness by contributing to the improvement of current methodologies, processes and tools.\n\nMinimum Skills RequiredAcademic Qualifications and Certifications:\n""¢ BE/BTech or equivalent in Information Technology and/or Business Management or a related field.\n""¢ Scaled Agile certification desirable.\n""¢ Relevant consulting and technical certifications preferred, for example TOGAF.\n\nRequired Experience12-15 years\n""¢ Seasoned demonstrable experience in a similar role within a large scale (preferably multi- national) technology services environment.\n""¢ Very good understanding of Data, AI, Gen AI and Agentic AI\n""¢ Must have Data Architecture and Solutioning experience. Capable of E2E Data Architecture and GenAI Solution design.\n""¢ Must be able to work on Data & AI RFP responses as Solution Architect\n""¢ 10+ years of experience in Solution Architecting of Data & Analytics, AI/ML & Gen AI Technical Architect\n""¢ Develop Cloud-native technical approach and proposal plans identifying the best practice solutions meeting the requirements for a successful proposal. Create, edit, and review documents, diagrams, and other artifacts in response to RPPs RFQs and Contribute to and participate in presentations to customers regarding proposed solutions.\n""¢ Proficient with Snowflake, Databricks, Azure, AWS, GCP cloud, Data Engineering & AI tools\n""¢ Experience with large scale consulting and program execution engagements in AI and data\n""¢ Seasoned multi-technology infrastructure design experience.\n""¢ Seasoned demonstrable level of expertise coupled with consulting and client engagement experience, demonstrating good experience in client needs assessment and change management.\n""¢ Additional\nAdditional\nAdditional Career Level Description:\nKnowledge and application:\n""¢ Seasoned, experienced professional; has complete knowledge and understanding of area of specialization.\n""¢ Uses evaluation, judgment, and interpretation to select right course of action.\nProblem solving:\n""¢ Works on problems of diverse scope where analysis of information requires evaluation of identifiable factors.\n""¢ Resolves and assesses a wide range of issues in creative ways and suggests variations in approach.\nInteraction:\n""¢ Enhances relationships and networks with senior internal/external partners who are not familiar with the subject matter often requiring persuasion.\n""¢ Works",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'microsoft azure', 'data engineering', 'data bricks', 'aws', 'needs assessment', 'client engagement', 'ai solutions', 'togaf', 'aiml', 'data architecture', 'solution architecting', 'artificial intelligence', 'change management', 'gen', 'service orientation', 'gcp', 'gcp cloud', 'ml']",2025-06-11 05:31:30
Data & AI Technical Solution Architects,"NTT DATA, Inc.",12 - 15 years,Not Disclosed,['Pune'],"Req ID: 323754\n\nWe are currently seeking a Data & AI Technical Solution Architects to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nJob DutiesThe Data & AI Architect is a seasoned level expert who is responsible for participating in the delivery of multi-technology consulting services to clients by providing strategies and solutions on all aspects of infrastructure and related technology components.\n\nThis role collaborates with other stakeholders on the development of the architectural approach for one or more layer of a solution. This role has the primary objective is to work on strategic projects that ensure the optimal functioning of the client""™s technology infrastructure.\n""¢ Key Responsibilities:\n""¢ Ability and experience to have conversations with the CEO, Business owners and CTO/CDO\n""¢ Break down intricate business challenges, devise effective solutions, and focus on client needs.\n""¢ Craft high level innovative solution approach for complex business problems\n""¢ Utilize best practices and creativity to address challenges\n""¢ Leverage market research, formulate perspectives, and communicate insights to clients\n""¢ Establish strong client relationships\n""¢ Interact at appropriate levels to ensure client satisfaction\n""¢ Knowledge and Attributes:\n""¢ Ability to focus on detail with an understanding of how it impacts the business strategically.\n""¢ Excellent client service orientation.\n""¢ Ability to work in high-pressure situations.\n""¢ Ability to establish and manage processes and practices through collaboration and the understanding of business.\n""¢ Ability to create new and repeat business for the organization.\n""¢ Ability to contribute information on relevant vertical markets\n""¢ Ability to contribute to the improvement of internal effectiveness by contributing to the improvement of current methodologies, processes and tools.\n\nMinimum Skills RequiredAcademic Qualifications and Certifications:\n""¢ BE/BTech or equivalent in Information Technology and/or Business Management or a related field.\n""¢ Scaled Agile certification desirable.\n""¢ Relevant consulting and technical certifications preferred, for example TOGAF.\n\nRequired Experience12-15 years\n""¢ Seasoned demonstrable experience in a similar role within a large scale (preferably multi- national) technology services environment.\n""¢ Very good understanding of Data, AI, Gen AI and Agentic AI\n""¢ Must have Data Architecture and Solutioning experience. Capable of E2E Data Architecture and GenAI Solution design.\n""¢ Must be able to work on Data & AI RFP responses as Solution Architect\n""¢ 10+ years of experience in Solution Architecting of Data & Analytics, AI/ML & Gen AI Technical Architect\n""¢ Develop Cloud-native technical approach and proposal plans identifying the best practice solutions meeting the requirements for a successful proposal. Create, edit, and review documents, diagrams, and other artifacts in response to RPPs RFQs and Contribute to and participate in presentations to customers regarding proposed solutions.\n""¢ Proficient with Snowflake, Databricks, Azure, AWS, GCP cloud, Data Engineering & AI tools\n""¢ Experience with large scale consulting and program execution engagements in AI and data\n""¢ Seasoned multi-technology infrastructure design experience.\n""¢ Seasoned demonstrable level of expertise coupled with consulting and client engagement experience, demonstrating good experience in client needs assessment and change management.\n""¢ Additional\nAdditional\nAdditional Career Level Description:\nKnowledge and application:\n""¢ Seasoned, experienced professional; has complete knowledge and understanding of area of specialization.\n""¢ Uses evaluation, judgment, and interpretation to select right course of action.\nProblem solving:\n""¢ Works on problems of diverse scope where analysis of information requires evaluation of identifiable factors.\n""¢ Resolves and assesses a wide range of issues in creative ways and suggests variations in approach.\nInteraction:\n""¢ Enhances relationships and networks with senior internal/external partners who are not familiar with the subject matter often requiring persuasion.\n""¢ Works",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'microsoft azure', 'data engineering', 'data bricks', 'aws', 'client engagement', 'ai solutions', 'togaf', 'aiml', 'data architecture', 'solution architecting', 'artificial intelligence', 'change management', 'gen', 'service orientation', 'solution design', 'gcp', 'gcp cloud', 'ml']",2025-06-11 05:31:32
Specialist Data Science,Merck Sharp & Dohme (MSD),5 - 7 years,Not Disclosed,"['Hyderabad', 'Pune']","Specialist - Data Science\nAt our company we are leveraging analytics and technology, as we invent for life on behalf of patients around the world. We are seeking those who have a passion for using data, analytics, and insights to drive decision making, that will allow us to tackle some of the world s greatest health threats.\nWithin our commercial Insights, Analytics, and Data organization we are transforming to better power decision-making across our end-to-end commercialization process, from business development to late lifecycle management. As we endeavor, we are seeking a dynamic talent to serve in the role of Analyst - Data Science.\nThis role involves working with our partners in different Therapeutic areas (e.g. Oncology, Vaccines, Pharma & Rare Disease, etc.) and Domain areas (HCP Analytics, Patient Analytics, Segmentation & targeting, Market Access, etc.) across the organization to help create scalable and production-grade analytics solutions, ranging from data visualization and reporting to advanced statistical and AI/ML models.\nYou will work in one of the three therapeutic areas of Brand Strategy and Performance Analytics - Oncology/Vaccines/Pharma & Rare Disease, where you will play a pivotal role in leveraging your statistical and machine learning expertise to address critical business challenges and derive insights to drive key decisions. Working alongside experienced data scientists and business analysts, you will have the opportunity to collaborate in translating business queries into analytical problems, employing your critical thinking, problem-solving, statistical, machine learning, and data visualization skills to deliver impactful solutions.\nWe are seeking candidates with prior experience in the healthcare analytics or consulting sectors, prior hands-on experience in Data Science (building end-to-end ML models). It is preferred that you have a good understanding of Physician and Patient-level data (PLD) from leading vendors such as IQVIA, Komodo, and Optum. Familiarity with HCP Analytics, PLD analytics, concepts like persistence, compliance, line of therapy, etc., or Segmentation & Targeting is highly desirable. You will be part of a dynamic team that collaborates with our partners across therapeutic areas. Furthermore, effective communication skills are crucial, as this role requires interfacing with executive and business stakeholders.\nWho you are:\nYou understand the foundations of statistics and machine learning and can work in high performance computing/cloud environments, with experience/knowledge in aspects across statistical analysis, machine learning, model development, data engineering, data visualization, and data interpretation\nYou are self-motivated, and have demonstrated abilities to think independently as a data scientist\nYou structure your data science approach according to the necessary task, while appropriately applying the correct level of model complexity to the problem at hand\nYou have an agile mindset of continuous learning and will focus on integrating enterprise value into team culture\nYou are kind, collaborative, and capable of seeking and giving candid feedback that effectively contributes to a more seamless day-to-day execution of tasks\nKey responsibilities:\nLead a team of Analysts - Data Science to solve complex business problems.\nLead the team in understanding the business requirements and translating them into analytical problem statements.\nDefine technical requirements (datasets, business rules, technical architecture), provide technical direction to the team and manage end-to-end projects\nCollaborate with cross-functional teams to design and implement solutions that meet business requirements\nPresent the findings to senior business stakeholders in a clear and concise manner\nManage and mentor junior data scientists through technical and professional guidance, trainings etc.\nDevelop deep expertise in the therapeutic area of interest, contribute to thought leadership in the domain through publications and conference presentations.\nMinimum Qualifications:\nBachelor s degree with 5-7 years industry experience\nProficiency in Python/R & SQL\nExperience in healthcare analytics or consulting sectors\nExperience working with real world evidence (RWE) and patient level data (PLD) from leading vendors such as IQVIA, Komodo, Optum etc.\nExperience in HCP Analytics, Segmentation and Targeting and Patient Level Data analytics (e.g., creating Patient Cohorts, knowledge of Lines of Therapy, Persistency, Compliance, etc.)\nExperience in leading small sized teams\nStrong Python/R, SQL, Excel skills\nStrong foundations of statistics and machine learning\nPreferred Qualifications:\nAdvanced degree in STEM (MS, MBA, PhD)\nExperience in Oncology/Vaccine/Pharma & Rare Diseases therapeutic area commercial analytics\nKnowledge of statistics, data science and machine learning & commercial\nExperience of supporting End to End Project Management\nCurrent Employees apply HERE\nCurrent Contingent Workers apply HERE\nSearch Firm Representatives Please Read Carefully\nEmployee Status:\nRegular\nRelocation:\nVISA Sponsorship:\nTravel Requirements:\nFlexible Work Arrangements:\nHybrid\nShift:\nValid Driving License:\nHazardous Material(s):\n\nRequired Skills:\nBusiness Intelligence (BI), Database Design, Data Engineering, Data Modeling, Data Science, Data Visualization, Machine Learning, Software Development, Stakeholder Relationship Management, Waterfall Model\n\nPreferred Skills:\nJob Posting End Date:\n07/31/2025\n*A job posting is effective until 11:59:59PM on the day BEFORE the listed job posting end date. Please ensure you apply to a job posting no later than the day BEFORE the job posting end date.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Relationship management', 'data science', 'Data modeling', 'Project management', 'Pharma', 'Analytical', 'Consulting', 'Agile', 'Business intelligence', 'SQL']",2025-06-11 05:31:33
Data & AI Technical Solution Architects,"NTT DATA, Inc.",12 - 15 years,Not Disclosed,['Hyderabad'],"Req ID: 323774\n\nWe are currently seeking a Data & AI Technical Solution Architects to join our team in Hyderabad, Telangana (IN-TG), India (IN).\n\n""Job DutiesThe Data & AI Architect is a seasoned level expert who is responsible for participating in the delivery of multi-technology consulting services to clients by providing strategies and solutions on all aspects of infrastructure and related technology components.\n\nThis role collaborates with other stakeholders on the development of the architectural approach for one or more layer of a solution. This role has the primary objective is to work on strategic projects that ensure the optimal functioning of the client""™s technology infrastructure.\n""¢ Key Responsibilities:\n""¢ Ability and experience to have conversations with the CEO, Business owners and CTO/CDO\n""¢ Break down intricate business challenges, devise effective solutions, and focus on client needs.\n""¢ Craft high level innovative solution approach for complex business problems\n""¢ Utilize best practices and creativity to address challenges\n""¢ Leverage market research, formulate perspectives, and communicate insights to clients\n""¢ Establish strong client relationships\n""¢ Interact at appropriate levels to ensure client satisfaction\n""¢ Knowledge and Attributes:\n""¢ Ability to focus on detail with an understanding of how it impacts the business strategically.\n""¢ Excellent client service orientation.\n""¢ Ability to work in high-pressure situations.\n""¢ Ability to establish and manage processes and practices through collaboration and the understanding of business.\n""¢ Ability to create new and repeat business for the organization.\n""¢ Ability to contribute information on relevant vertical markets\n""¢ Ability to contribute to the improvement of internal effectiveness by contributing to the improvement of current methodologies, processes and tools.\n\nMinimum Skills RequiredAcademic Qualifications and Certifications:\n""¢ BE/BTech or equivalent in Information Technology and/or Business Management or a related field.\n""¢ Scaled Agile certification desirable.\n""¢ Relevant consulting and technical certifications preferred, for example TOGAF.\n\nRequired Experience12-15 years\n""¢ Seasoned demonstrable experience in a similar role within a large scale (preferably multi- national) technology services environment.\n""¢ Very good understanding of Data, AI, Gen AI and Agentic AI\n""¢ Must have Data Architecture and Solutioning experience. Capable of E2E Data Architecture and GenAI Solution design.\n""¢ Must be able to work on Data & AI RFP responses as Solution Architect\n""¢ 10+ years of experience in Solution Architecting of Data & Analytics, AI/ML & Gen AI Technical Architect\n""¢ Develop Cloud-native technical approach and proposal plans identifying the best practice solutions meeting the requirements for a successful proposal. Create, edit, and review documents, diagrams, and other artifacts in response to RPPs RFQs and Contribute to and participate in presentations to customers regarding proposed solutions.\n""¢ Proficient with Snowflake, Databricks, Azure, AWS, GCP cloud, Data Engineering & AI tools\n""¢ Experience with large scale consulting and program execution engagements in AI and data\n""¢ Seasoned multi-technology infrastructure design experience.\n""¢ Seasoned demonstrable level of expertise coupled with consulting and client engagement experience, demonstrating good experience in client needs assessment and change management.\n""¢ Additional\nAdditional\nAdditional Career Level Description:\nKnowledge and application:\n""¢ Seasoned, experienced professional; has complete knowledge and understanding of area of specialization.\n""¢ Uses evaluation, judgment, and interpretation to select right course of action.\nProblem solving:\n""¢ Works on problems of diverse scope where analysis of information requires evaluation of identifiable factors.\n""¢ Resolves and assesses a wide range of issues in creative ways and suggests variations in approach.\nInteraction:\n""¢ Enhances relationships and networks with senior internal/external partners who are not familiar with the subject matter often requiring persuasion.\n""¢ Works""",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'microsoft azure', 'data engineering', 'data bricks', 'aws', 'client engagement', 'ai solutions', 'togaf', 'aiml', 'data architecture', 'solution architecting', 'artificial intelligence', 'change management', 'gen', 'service orientation', 'solution design', 'gcp', 'gcp cloud', 'ml']",2025-06-11 05:31:35
Data Engineering_PLM Consulting,Capgemini,5 - 8 years,Not Disclosed,['Pune'],"Capgemini Invent \n\nCapgemini Invent is the digital innovation, consulting and transformation brand of the Capgemini Group, a global business line that combines market leading expertise in strategy, technology, data science and creative design, to help CxOs envision and build whats next for their businesses.\n\n What you will love about working here \nWe recognize the significance of flexible work arrangements to provide support. Be it remote work, or flexible work hours, you will get an environment to maintain healthy work life balance.\nAt the heart of our mission is your career growth. Our array of career growth programs and diverse professions are crafted to support you in exploring a world of opportunities.\nEquip yourself with valuable certifications in the latest technologies such as Generative AI.\n\n\n Your role \nBusiness consulting\nExperience in Discrete Manufacturing /R&D/Product Development\nEngineering Transformation on either of the below areas :\n- Process/project transformation\n- manufacturing transformation\n- servitization/service transformations\nRobust Product development and R&D experience\nDesign for X (Value, Six Sigma, Cost)\nLean Manufacturing (waste reduction, DFSS, DMAIC, Process Optimization)\nProcess mapping, Value-stream mapping\nWell versed in Agile/Scrum and SDLC processes\n\n\n Your Profile \nSupporting our clients in Digital continuity strategy, R&D transformation, New product introduction (NPI) process management, IT-Architecture & -specification, project management, change management and rollout.\n5-8 years of overall experience with 1+ years into consulting\nExperience in requirements gathering, design, optimization and / or implementation of business processes in product development or R&D within Aerospace & Defense Domain.\nBackground in analyzing As-Is Engineering Processes, methods and tools for modernization and efficiency improvement.\nExperience/Familiarity in PLM Tools Such as 3DX/Siemens Teamcenter/PTC Windchill etc.\nExperience in implementing large scale engineering transformations programs in the field of Process, Manufacturing, Service, R&D\nFamiliar with business process mapping techniques and use of tools such as Visio, SAP Signavio, Celonis.\nPossesses combination of technological understanding, strategic analysis, and implementation aspects as well as the talent to think conceptually and analytically.\nExperience eliciting high-level business requirements, documentation of user stories, performing fit gap analysis of requirement vs OOTB functionalities, creation of roadmaps, milestones, delivery timelines etc\nExperience in creating detailed functional specifications and other documentation, such as requirement traceability matrices, work-flow diagrams and use-cases.\nDeveloped POCs, and/or contributed in solution development for Industry.\n\n\n About Capgemini \n\nCapgemini is a global business and technology transformation partner, helping organizations to accelerate their dual transition to a digital and sustainable world, while creating tangible impact for enterprises and society. It is a responsible and diverse group of 340,000 team members in more than 50 countries. With its strong over 55-year heritage, Capgemini is trusted by its clients to unlock the value of technology to address the entire breadth of their business needs. It delivers end-to-end services and solutions leveraging strengths from strategy and design to engineering, all fueled by its market leading capabilities in AI, cloud and data, combined with its deep industry expertise and partner ecosystem. The Group reported 2023 global revenues of 22.5 billion.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['visio', 'scrum', 'agile', 'sdlc', 'sdlc process', 'discrete manufacturing', 'sap', 'business process mapping', 'process optimization', 'solution development', 'signavio', 'data engineering', 'plm', 'business consulting', 'siemens teamcenter', 'product development', 'new product introduction']",2025-06-11 05:31:37
Data Platform Architect,Accenture,15 - 20 years,Not Disclosed,['Hyderabad'],"Project Role :Data Platform Architect\n\n\n\nProject Role Description :Architects the data platform blueprint and implements the design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\nMust have skills :Microsoft Azure Data Services\n\n\nGood to have skills :Microsoft SQL Server, Python (Programming Language), Microsoft Azure DatabricksMinimum\n\n5 year(s) of experience is required\n\n\nEducational Qualification :15 years full time education\nSummary:As a Data Platform Architect, you will be responsible for architecting the data platform blueprint and implementing the design, which includes various data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure seamless integration between systems and data models, while also addressing any challenges that arise during the implementation process. You will engage in discussions with stakeholders to gather requirements and provide insights that drive the overall architecture of the data platform, ensuring it meets the needs of the organization effectively.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Facilitate knowledge sharing sessions to enhance team capabilities.- Develop and maintain documentation related to architecture and design decisions.\nProfessional & Technical\n\n\nSkills:\n- Must To Have\n\n\nSkills:\nProficiency in Microsoft Azure Data Services.- Good To Have\n\n\nSkills:\nExperience with Microsoft Azure Databricks, Microsoft SQL Server, Python (Programming Language).- Strong understanding of data architecture principles and best practices.- Experience with cloud-based data solutions and services.- Familiarity with data governance and compliance standards.\nAdditional Information:- The candidate should have minimum 5 years of experience in Microsoft Azure Data Services.- This position is based in Hyderabad.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure databricks', 'data services', 'python', 'microsoft azure', 'data architecture principles', 'hive', 'architecting', 'data warehousing', 'power bi', 'data architecture', 'sql server', 'sql', 'data modeling', 'spark', 'data governance', 'hadoop', 'etl', 'big data', 'informatica', 'data integration']",2025-06-11 05:31:39
Data Platform Architect,Accenture,15 - 20 years,Not Disclosed,['Hyderabad'],"Project Role :Data Platform Architect\n\n\n\nProject Role Description :Architects the data platform blueprint and implements the design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\nMust have skills :Microsoft Azure Data Services\n\n\nGood to have skills :Microsoft SQL Server, Python (Programming Language), Microsoft Azure DatabricksMinimum\n\n7.5 year(s) of experience is required\n\n\nEducational Qualification :15 years full time education\nSummary:As a Data Platform Architect, you will be responsible for architecting the data platform blueprint and implementing the design, which includes various data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure seamless integration between systems and data models, while also addressing any challenges that arise during the implementation process. You will engage in discussions with stakeholders to gather requirements and provide insights that drive the overall architecture of the data platform, ensuring it meets the needs of the organization effectively.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Facilitate knowledge sharing sessions to enhance team capabilities.- Develop and maintain documentation related to data architecture and design.\nProfessional & Technical\n\n\nSkills:\n- Must To Have\n\n\nSkills:\nProficiency in Microsoft Azure Data Services.- Good To Have\n\n\nSkills:\nExperience with Microsoft Azure Databricks, Python (Programming Language), Microsoft SQL Server.- Strong understanding of data modeling techniques and best practices.- Experience with cloud-based data storage solutions and data processing frameworks.- Familiarity with data governance and compliance standards.\nAdditional Information:- The candidate should have minimum 7.5 years of experience in Microsoft Azure Data Services.- This position is based at our Hyderabad office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure databricks', 'data services', 'python', 'microsoft azure', 'data modeling', 'hive', 'architecting', 'data processing', 'data warehousing', 'power bi', 'data architecture', 'sql server', 'sql', 'tableau', 'spark', 'data governance', 'hadoop', 'etl', 'big data', 'informatica', 'data integration']",2025-06-11 05:31:41
Data Analyst (Consultant),Capgemini,3 - 5 years,Not Disclosed,['Pune'],"Capgemini Invent\n\nCapgemini Invent is the digital innovation, consulting and transformation brand of the Capgemini Group, a global business line that combines market leading expertise in strategy, technology, data science and creative design, to help CxOs envision and build whats next for their businesses.\n\nYour Role\nEngage with project activities across the Information lifecycle, often related to paradigms like -Building & managing Business data lakes and ingesting data streams to prepare data,Developing machine learning and predictive models to analyze data, Visualizing data ,Empowering Information consumers with agile Data Models that enable Self-Service BI,Specialize in Business Models and architectures across various Industry verticals\nParticipate in business requirements / functional specification definition, scope management, data analysis and design, in collaboration with both business stakeholders and IT teams, document detailed business requirements, develop solution design and specifications\nYour Profile\n\nB.E. / B.Tech. + MBA (Systems / Data / Data Science/ Analytics / Finance) with a good academic background\nStrong communication, facilitation, relationship-building, presentation, and negotiation skills\nConsultant must have a flair for storytelling and be able to present interesting insights from the data.\nConsultant should have good Soft skills like good communication, proactive, self-learning skills etc\nConsultants are expected to be flexible to the dynamically changing needs of the industry.\nMust have good exposure to Database management systems,\nGood to have knowledge about big data ecosystem like Hadoop.\nHands on with SQL and good knowledge of noSQL based databases.\nGood to have working knowledge of R/Python language.\nExposure to / Knowledge about one of the cloud ecosystems Google / AWS/ Azure What you will love about working here\nWe recognize the significance of flexible work arrangements to provide support. Be it remote work, or flexible work hours, you will get an environment to maintain healthy work life balance.\nAt the heart of our mission is your career growth. Our array of career growth programs and diverse professions are crafted to support you in exploring a world of opportunities.\nEquip yourself with valuable certifications in the latest technologies such as Generative AI.\nAbout Capgemini\n\nCapgemini is a global business and technology transformation partner, helping organizations to accelerate their dual transition to a digital and sustainable world, while creating tangible impact for enterprises and society. It is a responsible and diverse group of 340,000 team members in more than 50 countries. With its strong over 55-year heritage, Capgemini is trusted by its clients to unlock the value of technology to address the entire breadth of their business needs. It delivers end-to-end services and solutions leveraging strengths from strategy and design to engineering, all fueled by its market leading capabilities in AI, cloud and data, combined with its deep industry expertise and partner ecosystem. The Group reported 2023 global revenues of 22.5 billion.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['facilitation', 'sql', 'database management system', 'python', 'presentation skills', 'algorithms', 'c++', 'dbms', 'sales', 'java', 'data science', 'oops', 'data structures', 'html', 'mysql', 'hadoop', 'data analysis', 'c', 'microsoft azure', 'machine learning', 'javascript', 'nosql', 'marketing', 'r', 'selling', 'aws']",2025-06-11 05:31:42
Data Scientist-Artificial Intelligence,IBM,3 - 7 years,Not Disclosed,['Pune'],"As an Associate Data Scientist at IBM, you will work to solve business problems using leading edge and open-source tools such as Python, R, and TensorFlow, combined with IBM tools and our AI application suites. You will prepare, analyze, and understand data to deliver insight, predict emerging trends, and provide recommendations to stakeholders.\n\nIn your role, you may be responsible for:\nImplementing and validating predictive and prescriptive models and creating and maintaining statistical models with a focus on big data & incorporating machine learning. techniques in your projects\nWriting programs to cleanse and integrate data in an efficient and reusable manner\nWorking in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors\nCommunicating with internal and external clients to understand and define business needs and appropriate modelling techniques to provide analytical solutions.\nEvaluating modelling results and communicating the results to technical and non-technical audiences\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nProof of Concept (POC) DevelopmentDevelop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions.\nCollaborate with development teams to implement and iterate on POCs, ensuring alignment with customer requirements and expectations.\nHelp in showcasing the ability of Gen AI code assistant to refactor/rewrite and document code from one language to another, particularly COBOL to JAVA through rapid prototypes/ PoC\nDocument solution architectures, design decisions, implementation details, and lessons learned.\nCreate technical documentation, white papers, and best practice guides\n\n\nPreferred technical and professional experience\nStrong programming skills, with proficiency in Python and experience with AI frameworks such as TensorFlow, PyTorch, Keras or Hugging Face.\nUnderstanding in the usage of libraries such as SciKit Learn, Pandas, Matplotlib, etc. Familiarity with cloud platforms\nExperience and working knowledge in COBOL & JAVA would be preferred",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'scikit-learn', 'tensorflow', 'pytorch', 'keras', 'natural language processing', 'neural networks', 'predictive', 'huggingface', 'machine learning', 'prototype', 'artificial intelligence', 'sql', 'pandas', 'deep learning', 'r', 'java', 'cobol', 'data science', 'matplotlib', 'big data', 'statistics']",2025-06-11 05:31:44
Business Data Analyst,CGI,5 - 8 years,5-15 Lacs P.A.,['Hyderabad'],"Job Summary\nWe are seeking an experienced and results-driven Business Data Analyst with 5+ years of hands-on experience in data analytics, visualization, and business insight generation. This role is ideal for someone who thrives at the intersection of business and datatranslating complex data sets into compelling insights, dashboards, and strategies that support decision-making across the organization.\nYou will collaborate closely with stakeholders across departments to identify business needs, design and build analytical solutions, and tell compelling data stories using advanced visualization tools.\nKey Responsibilities\nData Analytics & Insights\nAnalyze large and complex data sets to identify trends, anomalies, and opportunities that help drive business strategy and operational efficiency.\nDashboard Development & Data Visualization\nDesign, develop, and maintain interactive dashboards and visual reports using tools like Power BI, Tableau, or Looker to enable data-driven decisions.\nBusiness Stakeholder Engagement\nCollaborate with cross-functional teams to understand business goals, define metrics, and convert ambiguous requirements into concrete analytical deliverables.\nKPI Definition & Performance Monitoring\nDefine, track, and report key performance indicators (KPIs), ensuring alignment with business objectives and consistent measurement across teams.\nData Modeling & Reporting Automation\nWork with data engineering and BI teams to create scalable, reusable data models and automate recurring reports and analysis processes.\nStorytelling with Data\nCommunicate findings through clear narratives supported by data visualizations and actionable recommendations to both technical and non-technical audiences.\nData Quality & Governance\nEnsure accuracy, consistency, and integrity of data through validation, testing, and documentation practices.\nRequired Qualifications\nBachelor’s or Master’s degree in Business, Economics, Statistics, Computer Science, Information Systems, or a related field.\n5+ years of professional experience in a data analyst or business analyst role with a focus on data visualization and analytics.\nProficiency in data visualization tools: Power BI, Tableau, Looker (at least one).\nStrong experience in SQL and working with relational databases to extract, manipulate, and analyze data.\nDeep understanding of business processes, KPIs, and analytical methods.\nExcellent problem-solving skills with attention to detail and accuracy.\nStrong communication and stakeholder management skills with the ability to explain technical concepts in a clear and business-friendly manner.\nExperience working in Agile or fast-paced environments.\nPreferred Qualifications\nExperience working with cloud data platforms (e.g., Snowflake, BigQuery, Redshift).\nExposure to Python or R for data manipulation and statistical analysis.\nKnowledge of data warehousing, dimensional modeling, or ELT/ETL processes.\nDomain experience in Healthcare is a plus.",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power BI', 'Data Visualization', 'Data Analytics', 'Insights', 'KPI', 'business insight generation', 'Tableau', 'Data Modeling', 'Looker', 'SQL', 'Performance Monitoring']",2025-06-11 05:31:46
Field Engineer,Genpact,0 - 5 years,Not Disclosed,['Bengaluru'],"Genpact (NYSE: G) is a global professional services and solutions firm delivering outcomes that shape the future. Our 125,000+ people across 30+ countries are driven by our innate curiosity, entrepreneurial agility, and desire to create lasting value for clients. Powered by our purpose the relentless pursuit of a world that works better for people – we serve and transform leading enterprises, including the Fortune Global 500, with our deep business and industry knowledge, digital operations services, and expertise in data, technology, and AI.\nInviting applications for the role of Field Data Engineer!\nIn this role - The team developing industry leading X-ray generation subsystems (ie. tubes and generators) is looking for highly motivated engineers to join them in the efforts to monitor and maintain the large installed base of the imaging scanners that use these X-ray generation subsystem in the field.",,,,"['Field Engineering', 'Field Support']",2025-06-11 05:31:48
Sr. Analytics Engineer,Adobe,12 - 13 years,Not Disclosed,['Bengaluru'],"Adobe DMe B2B analytics team is looking for a Senior Analytics Engineer to build the foundational data assets and analytical reports for the B2B customer journey analytics. The data assets and reports will provide quantitative and qualitative analysis for acquisition, engagement and retention. In addition, the candidate will contribute on multi-functional projects to help business achieve its potential in terms of revenue, customer success, and operational excellence.\nWhat you'll Do\nAnalyze complex business needs, profile diverse datasets, and optimize scalable data products on various data platforms, like Databricks, prioritizing data quality, performance, and efficiency.\nBuild and maintain foundational datasets and reporting solutions that power dashboards, self-service tools, and ad hoc analytics across the organization.\nPartner closely with Business Intelligence, Product Management, and Business Stakeholders to understand data needs.\nGuide and assist junior analytics engineers and analysts through code reviews, advocating standard methodologies, and encouraging a culture of code simplicity, clarity, and technical excellence.\nWhat you need to succeed\nStrong analytical approach with critical thinking and problem-solving skills.\nAbility to quickly understand Adobe products, business processes, existing data structures, and legacy code.\nMinimum of 8 years of experience in data analytics, data engineering, or a related field.\nSolid expertise in data warehousing, data modeling, and crafting scalable data solutions.\nProficiency in SQL and/or Python for building, optimizing, and fixing data pipelines.\nExperience using AI tools (eg, GitHub Copilot, Claude) to speed up and improve development workflows.\nExcellent communication skills needed to effectively work with business collaborators, engineers, and analysts",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Product management', 'github', 'Data modeling', 'Analytical', 'Data structures', 'Data quality', 'Business intelligence', 'Adobe', 'SQL', 'Python']",2025-06-11 05:31:49
Data Architect,CGI,8 - 12 years,15-30 Lacs P.A.,['Hyderabad'],"Job Title: Data Architect / Data Modeler\nExperience Level: 8+ Years\nLocation: Hyderabad\nJob Summary\nWe are seeking a highly experienced Data Architect to join our growing Data & Analytics team. This role demands a strategic thinker and technical expert who can design and build robust, scalable, and efficient data solutions. You will play a critical role in architecting end-to-end data pipelines, designing optimized data models, and delivering business-centric data infrastructure using cutting-edge technologies such as Python, PySpark, SQL, Snowflake, and/or Databricks.\nThe ideal candidate will have a deep understanding of data engineering best practices and a proven track record of enabling data-driven decision-making through innovative and scalable data solutions.\nKey Responsibilities\nArchitect & Design Scalable Data Pipelines\nLead the design and implementation of high-performance, scalable, and maintainable data pipelines that support batch and real-time processing.\nData Modeling & Data Architecture\nDesign and implement optimized data models and database schemas to support analytics, reporting, and machine learning use cases.\nCloud Data Platforms\nDevelop and manage modern cloud-based data architectures using platforms like Snowflake or Databricks, ensuring performance, security, and cost-efficiency.\nData Integration & ETL Development\nBuild robust ETL/ELT workflows to ingest, transform, and provision data from a variety of internal and external sources.\nCollaboration with Stakeholders\nWork closely with data analysts, data scientists, product managers, and business leaders to translate business requirements into technical specifications and data solutions.\nData Quality & Governance\nImplement and advocate for best practices in data quality, security, compliance, lineage, and governance.\nPerformance Optimization\nOptimize data storage and query performance using advanced SQL, partitioning, indexing, caching strategies, and compute resource tuning.\nMentorship & Best Practices\nProvide mentorship to junior engineers, establish coding standards, and contribute to the growth and maturity of the data engineering practice.\nRequired Qualifications\nBachelors or Master’s degree in Computer Science, Engineering, Data Science, or a related field.\n8+ years of experience in data engineering or related roles.\nStrong expertise in Python and PySpark for data processing and transformation.\nProficient in advanced SQL with a deep understanding of query optimization and performance tuning.\nHands-on experience with Snowflake and/or Databricks in a production environment.\nExperience in designing and implementing data warehouses and data lakes.\nSolid understanding of distributed computing frameworks, big data ecosystems, and modern data architecture patterns.\nExperience with CI/CD, version control systems (e.g., Git), and workflow orchestration tools (e.g., Airflow, dbt, etc.).\nStrong communication skills with the ability to clearly articulate technical concepts to non-technical stakeholders.Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'pyspark', 'sql', 'snowflake', 'Data Architecture']",2025-06-11 05:31:51
Specialist Data Scientist,NICE,8 - 11 years,Not Disclosed,['Pune'],"So, what’s the role all about?\nNICE provides state-of-the-art enterprise level AI and analytics for all forms of business communications between speech and digital.   We are a world class research team developing new algorithms and approaches to help companies with solving critical issues such as identifying their best performing agents, preventing fraud, categorizing customer issues, and determining overall customer satisfaction.  If you have interacted with a major contact center in the last decade, it is very likely we have processed your call. \nThe research group partners with all areas of NICE’s business to scale out the delivery of new technology and AI models to customers around the world that are tailored to their company, industry, and language needs.",,,,"['python', 'confluence', 'natural language processing', 'presentation skills', 'big data technologies', 'pyspark', 'microsoft azure', 'bert', 'machine learning', 'sql', 'tensorflow', 'data science', 'gcp', 'pytorch', 'machine learning algorithms', 'aws', 'big data', 'communication skills', 'statistics', 'jira']",2025-06-11 05:31:53
Data Scientist-Advanced Analytics,IBM,3 - 7 years,Not Disclosed,['Pune'],"We are seeking a highly skilled Advanced Analytics Specialist to join our dynamic team. The successful candidate will be responsible for leveraging advanced analytics techniques to derive actionable insights, inform business decisions, and drive strategic initiatives. This role requires a deep understanding of data analysis, statistical modelling, machine learning, and data visualization.\nIn this role, you will be responsible for architecting and delivering AI solutions using cutting-edge technologies, with a strong focus on foundation models and large language models. You will work closely with customers, product managers, and development teams to understand business requirements and design custom AI solutions that address complex challenges. Experience with tools like Github Copilot, Amazon Code Whisperer etc. is desirable.\nSuccess is our passion, and your accomplishments will reflect this, driving your career forward, propelling your team to success, and helping our clients to thrive.\nDay-to-Day Duties:\nProof of Concept (POC) DevelopmentDevelop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions. Collaborate with development teams to implement and iterate on POCs, ensuring alignment with customer requirements and expectations.\nHelp in showcasing the ability of Gen AI code assistant to refactor/rewrite and document code from one language to another, particularly COBOL to JAVA through rapid prototypes/ PoC\nDocumentation and Knowledge SharingDocument solution architectures, design decisions, implementation details, and lessons learned. Create technical documentation, white papers, and best practice guides. Contribute to internal knowledge sharing initiatives and mentor new team members.\nIndustry Trends and InnovationStay up to date with the latest trends and advancements in AI, foundation models, and large language models. Evaluate emerging technologies, tools, and frameworks to assess their potential impact on solution design and implementation\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nDevelop and implement advanced analytical models and algorithms to solve complex business problems, analyze large datasets to uncover trends, patterns, and insights that drive business performance.\nCollaborate with cross-functional teams to identify key business challenges and opportunities, Create and maintain data pipelines and workflows to ensure the accuracy and integrity of data, Design and deliver insightful reports and dashboards to communicate findings to stakeholders.\nStay up to date with the latest advancements in analytics, machine learning, and data science. Provide technical expertise and mentorship to junior team members.\nQualificationsBachelor’s or master’s degree in data science, Statistics, Mathematics, Computer Science, or a related field. Proven experience in advanced analytics, data science, or a similar role. Proficiency in programming languages such as Python, R, or SQL. Experience with data visualization tools like Tableau, Power BI, or similar.\nStrong understanding of statistical modelling and machine learning algorithms. Excellent analytical, problem-solving, and critical thinking skills. Ability to communicate complex analytical concepts to non-technical stakeholders. Experience with big data technologies (e.g., Hadoop, Spark) is a plus\n\n\nPreferred technical and professional experience\nFamiliarity with cloud-based analytics platforms (e.g., AWS, Azure).\nKnowledge of natural language processing (NLP) and deep learning techniques.\nExperience with project management and agile methodologies",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analysis', 'machine learning', 'statistical modeling', 'data visualization', 'machine learning algorithms', 'advanced analytics', 'python', 'github', 'natural language processing', 'power bi', 'microsoft azure', 'sql', 'tableau', 'r', 'java', 'data science', 'spark', 'hadoop', 'aws']",2025-06-11 05:31:55
Data and Analytics Architect - L1,Wipro,10 - 15 years,Not Disclosed,['Chennai'],"Wipro Limited (NYSE:WIT, BSE:507685, NSE:WIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.\n\nAbout The Role :\n\n\n\nRole Purpose\n\nThe purpose of the role is to define and develop Enterprise Data Structure along with Data Warehouse, Master Data, Integration and transaction processing with maintaining and strengthening the modelling standards and business information.\n\n\n\n\n\nDo\n\n\n\n1. Define and Develop Data Architecture that aids organization and clients in new/ existing deals\n\na. Partnering with business leadership (adopting the rationalization of the data value chain) to provide strategic, information-based recommendations to maximize the value of data and information assets, and protect the organization from disruptions while also embracing innovation\n\nb. Assess the benefits and risks of data by using tools such as business capability models to create an data-centric view to quickly visualize what data matters most to the organization, based on the defined business strategy\n\nc. Create data strategy and road maps for the Reference Data Architecture as required by the clients\n\nd. Engage all the stakeholders to implement data governance models and ensure that the implementation is done based on every change request\n\ne. Ensure that the data storage and database technologies are supported by the data management and infrastructure of the enterprise\n\nf. Develop, communicate, support and monitor compliance with Data Modelling standards\n\ng. Oversee and monitor all frameworks to manage data across organization\n\nh. Provide insights for database storage and platform for ease of use and least manual work\n\ni. Collaborate with vendors to ensure integrity, objectives and system configuration\n\nj. Collaborate with functional & technical teams and clients to understand the implications of data architecture and maximize the value of information across the organization\n\nk. Presenting data repository, objects, source systems along with data scenarios for the front end and back end usage\n\nl. Define high-level data migration plans to transition the data from source to target system/ application addressing the gaps between the current and future state, typically in sync with the IT budgeting or other capital planning processes\n\nm. Knowledge of all the Data service provider platforms and ensure end to end view.\n\nn. Oversight all the data standards/ reference/ papers for proper governance\n\no. Promote, guard and guide the organization towards common semantics and the proper use of metadata\n\n\n\np. Collecting, aggregating, matching, consolidating, quality-assuring, persisting and distributing such data throughout an organization to ensure a common understanding, consistency, accuracy and control\n\nq. Provide solution of RFPs received from clients and ensure overall implementation assurance\n\ni. Develop a direction to manage the portfolio of all the databases including systems, shared infrastructure services in order to better match business outcome objectives\n\nii. Analyse technology environment, enterprise specifics, client requirements to set a collaboration solution for the big/small data\n\niii. Provide technical leadership to the implementation of custom solutions through thoughtful use of modern technology\n\niv. Define and understand current issues and problems and identify improvements\n\nv. Evaluate and recommend solutions to integrate with overall technology ecosystem keeping consistency throughout\n\nvi. Understand the root cause problem in integrating business and product units\n\nvii. Validate the solution/ prototype from technology, cost structure and customer differentiation point of view\n\nviii. Collaborating with sales and delivery leadership teams to identify future needs and requirements\n\nix. Tracks industry and application trends and relates these to planning current and future IT needs\n\n\n\n\n\n2. Building enterprise technology environment for data architecture management\n\na. Develop, maintain and implement standard patterns for data layers, data stores, data hub & lake and data management processes\n\nb. Evaluate all the implemented systems to determine their viability in terms of cost effectiveness\n\nc. Collect all the structural and non-structural data from different places integrate all the data in one database form\n\nd. Work through every stage of data processing:analysing, creating, physical data model designs, solutions and reports\n\ne. Build the enterprise conceptual and logical data models for analytics, operational and data mart structures in accordance with industry best practices\n\nf. Implement the best security practices across all the data bases based on the accessibility and technology\n\ng. Strong understanding of activities within primary discipline such as Master Data Management (MDM), Metadata Management and Data Governance (DG)\n\nh. Demonstrate strong experience in Conceptual, Logical and physical database architectures, design patterns, best practices and programming techniques around relational data modelling and data integration\n\n\n\n\n\n3. Enable Delivery Teams by providing optimal delivery solutions/ frameworks\n\na. Build and maintain relationships with delivery and practice leadership teams and other key stakeholders to become a trusted advisor\n\nb. Define database physical structure, functional capabilities, security, back-up and recovery specifications\n\nc. Develops and establishes relevant technical, business process and overall support metrics (KPI/SLA) to drive results\n\nd. Monitor system capabilities and performance by performing tests and configurations\n\ne. Integrate new solutions and troubleshoot previously occurred errors\n\nf. Manages multiple projects and accurately reports the status of all major assignments while adhering to all project management standards\n\ng. Identify technical, process, structural risks and prepare a risk mitigation plan for all the projects\n\nh. Ensure quality assurance of all the architecture or design decisions and provides technical mitigation support to the delivery teams\n\ni. Recommend tools for reuse, automation for improved productivity and reduced cycle times\n\nj. Help the support and integration team for better efficiency and client experience for ease of use by using AI methods.\n\nk. Develops trust and builds effective working relationships through respectful, collaborative engagement across individual product teams\n\nl. Ensures architecture principles and standards are consistently applied to all the projects\n\nm. Ensure optimal Client Engagement\n\ni. Support pre-sales team while presenting the entire solution design and its principles to the client\n\nii. Negotiate, manage and coordinate with the client teams to ensure all requirements are met\n\niii. Demonstrate thought leadership with strong technical capability in front of the client to win the confidence and act as a trusted advisor\nReinvent your world.We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['metadata management', 'data architecture', 'master data management', 'design patterns', 'data governance', 'project management', 'data analytics', 'data management', 'data processing', 'data warehousing', 'warehouse', 'sql', 'data quality', 'data modeling', 'mdm', 'data integration']",2025-06-11 05:31:56
Big Data Architect,Meritus Management Service,8 - 10 years,20-32.5 Lacs P.A.,"['Nagpur', 'Pune']","Design and implement scalable Big Data architecture and pipelines using tools like Hadoop, Spark, Kafka, and Hive.\nCollaborate with cross-functional teams to build real-time/batch systems, ensure data quality and governance.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Hadoop', 'SQL', 'architect level exposure', 'Pyspark']",2025-06-11 05:31:58
Senior Data Scientist,Comviva Technology,2 - 5 years,Not Disclosed,['Gurugram'],"Job Description for Sr Data Scientist (relevant experience: 2 to 5 years)\n\nWhat we look for:\nExpertise in Big Data/ML: Should be able to build cutting edge credit/churn/usage models using advanced algorithms in a Big data/Machine Learning environment. Should have hands on experience and track record of delivering projects in individual capacity.\n- Process oriented: Should help in building a process that maximizes operating efficiency while maintaining risk across multiple lending cycles. There needs to be an obsession with collecting and analyzing data to drive business iterations and improvements.\n- Willingness to go above and beyond: For start-ups the responsibilities and needs of the business change quickly. Were looking for someone who is not afraid to take on calculated risks and can deal with ambiguity.\n\nJob Responsibilities:\n- Develop innovative credit risk / churn / usage models using mobile wallet transaction data, Call data, Telecom usage data, customer bureau data etc\n- Partner with the Data Engineering team to define the required data pipelines to build and enhance the feature bank (foundational capability) to build/deploy the various ML algorithms both for batch and real time use cases\n- Collaborate with credit policy/portfolio mgmt. team to drive P&L outcomes.\n- Building reports for model monitoring and drive enhancements\nRequired Qualifications & Skills:\nSolid expertise in end-to-end risk model lifecycle management (develop, deploy, monitor)\nPrevious hands on in credit, fraud, churn model development and deployment\nPrevious experience in PD/EAD/LGD model development/validation\nExperience in CSI/PSI model monitoring process\nHands on experience in data extraction using SQL/Pyspark SQL; data cleaning, feature creation and building models using Py Spark/Python on Spark; Scale will be a plus.\nPrevious exposure to below algorithms (preferably multiple):\nLogistic Regression\nRandom forest\nXGBOOST\nMarkov Chain\nPSI/CSI for model monitoring\nStrategy performance tracking and swap in / swap out analysis\n- Strong entrepreneurial drive\n- Good to have:\no Good business understanding of the fintech/consumer finance space\no Experience in working with credit card / personal lending space, esp fintech\no hands on experience in working with Telecom data",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Telecom', 'Logistic regression', 'Consumer finance', 'Machine learning', 'model development', 'Deployment', 'Business understanding', 'SQL', 'Python', 'Data extraction']",2025-06-11 05:31:59
Senior Data Analyst,Stack Digital,3 - 7 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Job Description\nRoles and Responsibilities:\nThe Senior Data Analyst is an active thought partner who shapes the Business demand and work closely with the Project / Product teams and stakeholders. The Senior Data Analyst gathers, analyses and models data and key performance indicators to develop quantitative and qualitative business insights. Develops processes and design reports to boost the business intelligence and is good at effectively processing large amounts of data into meaningful information. Key interface towards the Project / Product Managers, Design Architects, Data Engineers, Testers, End users etc. as a natural team to deliver the Business demands.\nKey Characteristics\nCollating Business requirements, Analyzing the value drivers and functional requirements, usability and supportability considerations.\nPerform root cause analysis on Data problems and translate Data requirements into functionality and assess the risks, feasibility, opportunities and various solution options.\nCreate/Update clear documentation to communicate requirements and related information.\nSupports in Creating acceptance criteria and validate that solution by testing and ensure it meet business needs.\nDescribe technology in terms easily understood by business customers and set realistic customer expectations for the project outcome.\nExcellent analytical & problem-solving skills, willingness to take ownership and resolve technical challenges.\nGenerate innovative approaches to existing problems or new opportunities",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Root cause analysis', 'Analytical', 'Senior Data Analyst', 'Manager Technology', 'Business intelligence', 'Testing']",2025-06-11 05:32:01
Lead Data Analyst-Business Intelligence,Tresvista Financial Services,6 - 10 years,Not Disclosed,['Bengaluru'],"Roles and Responsibilities\nArchitect and incorporate an effective Data framework enabling end to end Data Solution.\nUnderstand business needs, use cases and drivers for insights and translate them into detailed technical specifications.\nCreate epics, features and user stories with clear acceptance criteria for execution and delivery by the data engineering team.\nCreate scalable and robust data solution designs that incorporate governance, security and compliance aspects.\nDevelop and maintain logical and physical data models and work closely with data engineers, data analysts and data testers for successful implementation of them.\nAnalyze, assess and design data integration strategies across various sources and platforms.\nCreate project plans and timelines while monitoring and mitigating risks and controlling progress of the project.\nConduct daily scrum with the team with a clear focus on meeting sprint goals and timely resolution of impediments.\nAct as a liaison between technical teams and business stakeholders and ensure.\nGuide and mentor the team for best practices on Data solutions and delivery frameworks.\nActively work, facilitate and support the stakeholders/ clients to complete User Acceptance Testing ensure there is strong adoption of the data products after the launch.\nDefining and measuring KPIs/KRA for feature(s) and ensuring the Data roadmap is verified through measurable outcomes\n\nPrerequisites\n5 to 8 years of professional, hands on experience building end to end Data Solution on Cloud based Data Platforms including 2+ years working in a Data Architect role.\nProven hands on experience in building pipelines for Data Lakes, Data Lake Houses, Data Warehouses and Data Visualization solutions\nSound understanding of modern Data technologies like Databricks, Snowflake, Data Mesh and Data Fabric.\nExperience in managing Data Life Cycle in a fast-paced, Agile / Scrum environment.\nExcellent spoken and written communication, receptive listening skills, and ability to convey complex ideas in a clear, concise fashion to technical and non-technical audiences\nAbility to collaborate and work effectively with cross functional teams, project stakeholders and end users for quality deliverables withing stipulated timelines\nAbility to manage, coach and mentor a team of Data Engineers, Data Testers and Data Analysts. Strong process driver with expertise in Agile/Scrum framework on tools like Azure DevOps, Jira or Confluence\nExposure to Machine Learning, Gen AI and modern AI based solutions.\n\nExperience\nTechnical Lead Data Analytics with 6+ years of overall experience out of which 2+ years is on Data architecture.\n\nEducation\nEngineering degree from a Tier 1 institute preferred.\n\nCompensation\nThe compensation structure will be as per industry standards",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'Data Bricks', 'Data Lake', 'Data Warehousing', 'Python', 'Business Intelligence', 'Databricks Engineer', 'Machine Learning', 'Redshift Aws', 'Snowflake', 'Data Visualization', 'ETL', 'Data Mesh']",2025-06-11 05:32:02
Sr. Data Scientist-Stratup-Mid-Size companies Exp.@ Bangalore_Urgent,"A leader in this space, we deliver world...",8 - 13 years,Not Disclosed,['Bengaluru'],"Senior Data Scientist\n\nLocation: Onsite Bangalore\nExperience: 8+ years\n\nRole Overview\n\nWe are seeking a Senior Data Scientist with a strong foundation in machine learning, deep learning, and statistical modeling, with the ability to translate complex operational problems into scalable AI/ML solutions. In addition to core data science responsibilities, the role involves building production-ready backends in Python and contributing to end-to-end model lifecycle management. Exposure to computer vision is a plus, especially for industrial use cases like identification, intrusion detection, and anomaly detection.\n\nKey Responsibilities\n\nDevelop, validate, and deploy machine learning and deep learning models for forecasting, classification, anomaly detection, and operational optimization\nBuild backend APIs using Python (FastAPI, Flask) to serve ML/DL models in production environments\nApply advanced computer vision models (e.g., YOLO, Faster R-CNN) to object detection, intrusion detection, and visual monitoring tasks\nTranslate business problems into analytical frameworks and data science solutions\nWork with data engineering and DevOps teams to operationalize and monitor models at scale\nCollaborate with product, domain experts, and engineering teams to iterate on solution design\nContribute to technical documentation, model explainability, and reproducibility practices\n\n\nRequired Skills\n\nStrong proficiency in Python for data science and backend development\nExperience with ML/DL libraries such as scikit-learn, TensorFlow, or PyTorch\nSolid knowledge of time-series modeling, forecasting techniques, and anomaly detection\nExperience building and deploying APIs for model serving (FastAPI, Flask)\nFamiliarity with real-time data pipelines using Kafka, Spark, or similar tools\nStrong understanding of model validation, feature engineering, and performance tuning\nAbility to work with SQL and NoSQL databases, and large-scale datasets\nGood communication skills and stakeholder engagement experience\n\n\nGood to Have\n\nExperience with ML model deployment tools (MLflow, Docker, Airflow)\nUnderstanding of MLOps and continuous model delivery practices\nBackground in aviation, logistics, manufacturing, or other industrial domains\nFamiliarity with edge deployment and optimization of vision models\n\n\nQualifications\n\nMasters or PhD in Data Science, Computer Science, Applied Mathematics, or related field\n7+ years of experience in machine learning and data science, including end-to-end deployment of models in production",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scikit-learn', 'time-series modeling', 'ML/DL libraries', 'data science', 'Python', 'Airflow', 'Kafka', 'MLflow', 'logistics', 'anomaly detection', 'aviation', 'SQL', 'PyTorch', 'NoSQL', 'MLOps', 'forecasting techniques', 'Docker', 'manufacturing', 'FastAPI', 'Spark', 'TensorFlow', 'Flask']",2025-06-11 05:32:04
Senior Data Scientist,Fastenal,4 - 9 years,Not Disclosed,['Bengaluru'],"Job Description:\nWe are seeking a highly skilled and experienced Senior Data Scientist to join our team. The ideal candidate will have a strong background in data science, machine learning, and statistical analysis. As a Senior Data Scientist, you will be responsible for leading data-driven projects, developing predictive models, and providing actionable insights to drive business decisions.\n\nKey Responsibilities:",,,,"['Data Science', 'SQL', 'Pyspark', 'R', 'Python']",2025-06-11 05:32:06
Senior DBT Engineer,DXC Technology,4 - 12 years,Not Disclosed,['Bengaluru'],"Job Description:\nSenior DBT Engineer\nJob Location : Hyderabad / Bangalore / Chennai / Kolkata / Noida/ Gurgaon / Pune / Indore / Mumbai\nResponsibilities:\n- Experience Level - 4 -12 years.\n- Design, develop, and maintain DBT models, transformations, and SQL code to build efficient data pipelines for analytics and reporting.\n- Design, develop, and maintain ETL/ELT pipelines using DBT and pulling data from Snowflake.\n- Define and implement data modelling best practices, including data warehousing, ETL processes, and data transformations using DBT.\n- Build complex SQL queries within DBT to build incremental models, enhancing data processing efficiency.\n- Establish data governance practices and ensure data accuracy, quality, and consistency within the data transformation process.\n- Collaborate with data engineers, data analysts, and other stakeholders to understand and meet data requirements for various business units.\n- Identify and address performance bottlenecks in data transformation processes and optimize DBT models for faster query performance.\n- Maintain thorough documentation of DBT models, transformations, and data dictionaries to ensure transparency and accessibility to team members.\n- Implement data security measures to protect sensitive information and comply with data privacy regulations.\n- Stay updated on industry best practices and new features in DBT, and continuously improve the data transformation processes.\n- Provide training and support to other team members in using DBT effectively.\n- Implement data quality checks and validation processes to ensure data accuracy and consistency.\n- Hands-on experience in implementing data governance, data quality rules and validation mechanisms within Collibra is added plus.\n- Knowledge of workflow orchestration tools like Tidal.\n- Experience with Python or other scripting languages is a plus.\n- Familiarity with Azure cloud platforms.\n- Exposure to DevOps practices and CI/CD pipelines for data engineering.\nRecruitment fraud is a scheme in which fictitious job opportunities are offered to job seekers typically through online services, such as false websites, or through unsolicited emails claiming to be from the company. These emails may request recipients to provide personal information or to make payments as part of their illegitimate recruiting process. DXC does not make offers of employment via social media networks and DXC never asks for any money or payments from applicants at any point in the recruitment process, nor ask a job seeker to purchase IT or other equipment on our behalf. More information on employment scams is available here .",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL queries', 'data security', 'Social media', 'data governance', 'Data processing', 'Workflow', 'Data quality', 'data privacy', 'Analytics', 'Python']",2025-06-11 05:32:07
Senior Qa Engineer,TEKsystems,8 - 10 years,15-27.5 Lacs P.A.,['Bengaluru'],"Job Description:\nAbout the role and how you will spend your time:\no Work in an Agile Scrum environment to support quality engineering activities in the commerce and payments space\no Work with architects and developers to create test plans for new functionality.\no Develop and implement best practices for test driven development, continuous build and deploy, and automated testing,\no Establish and influence quality engineering best practices across the development community.\no Partner across Product Management, Architects, Engineering, Hosting and Operations.\no Work with globally distributed teams to engage early and provide continuous delivery of high quality software products that delight Sony customers.\n\nRoles & Responsibilities:\nBachelors or Masters Degree in Engineering, Computer Science or equivalent experience.\no 8+ years experience as an engineer in test.\no Extensive hands-on technical expertise, including experience with Java.\no Experience with all aspects of software test, including test planning, test automation, load test, failure mode test, etc.\no Experience testing RESTful web services and web applications\no Experience with build automation and continuous integration tools (e.g., Jenkins).\no Experience with test automation frameworks (e.g., Selenium, Cucumber, etc.), code coverage tools (Clover, Karma).\no Experience with SQL and database schema designs.\no Experience reviewing logs in Splunk or similar.\no Ability to estimate effort and size of features\no Experience with agile development methodologies and test driven development processes\no Experience using source control (esp. Git) and bug tracking systems in a team environment\nNice to have:\no Experience delivering high performance, active-active, linearly scalable services (Enterprise Java/J2EE, Web Services, Big Data/NoSQL).\no Ecommerce platform domain expertise.\no Experience testing AWS Managed Services: Kinesis, S3, Lambda, Glue, DynamoDB, SNS, SQS\nAbout you:\no You possess a drive and passion for quality with the ability to inspire, excite and motivate other team members.\no You have outstanding verbal and written communication skills, and are able to work with others at all levels.\no You're effective at working with geographically remote and culturally diverse teams.\no Your curiosity drives you to go beyond your immediate assignments and actively learn everything you can about the systems you support. You're not afraid to ask questions.\no You have associated technology product knowledge, broad industry knowledge and excellent communication skills",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'EDA', 'Rest Assured', 'Rest Api Testing', 'Testing Microservices', 'Event Driven Architecture']",2025-06-11 05:32:09
Senior QA DBT Engineer,DXC Technology,4 - 11 years,Not Disclosed,['Bengaluru'],"Job Description:\nQA DBT Engineer\nJob Location: Hyderabad / Bangalore / Chennai / Kolkata / Noida/ Gurgaon / Pune / Indore / Mumbai\nMinimum of 8 years experience as a software tester with proven experience in defining and leading QA cycles\nStrong experience with DBT (Data Build Tool) and writing/validating SQL models.\nHands-on experience with Collibra for metadata management and data governance validation.\nSolid understanding of data warehousing concepts and ETL/ELT processes.\nProficiency in SQL for data validation and transformation testing.\nFamiliarity with version control tools like Git.\nUnderstanding of data governance, metadata, and data quality principles.\nStrong analytical and problem-solving skills with attention to detail.\nRecruitment fraud is a scheme in which fictitious job opportunities are offered to job seekers typically through online services, such as false websites, or through unsolicited emails claiming to be from the company. These emails may request recipients to provide personal information or to make payments as part of their illegitimate recruiting process. DXC does not make offers of employment via social media networks and DXC never asks for any money or payments from applicants at any point in the recruitment process, nor ask a job seeker to purchase IT or other equipment on our behalf. More information on employment scams is available here .",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['QA', 'Data validation', 'metadata', 'Version control', 'GIT', 'Analytical', 'Social media', 'data governance', 'Data quality', 'SQL']",2025-06-11 05:32:11
Senior Analyst Data Cloud,NOVARTIS,3 - 8 years,Not Disclosed,['Hyderabad'],"Summary\nThe Salesforce Data Cloud Analyst will play a crucial role in leveraging Salesforce Data Cloud to transform how our organization uses customer data. This position sits within the Data Cloud Business Enablement Team and focuses on building, managing, and optimizing our data unification strategy to power business intelligence, marketing automation, and customer experience initiatives\nAbout the Role\nLocation - Hyderabad",,,,"['Computer science', 'Data analysis', 'Data modeling', 'Pharma', 'Analytical', 'Agile', 'Data quality', 'Business intelligence', 'SQL', 'CRM']",2025-06-11 05:32:12
Big Data Architect,Leading Client,12 - 15 years,Not Disclosed,['Mumbai'],"12+ Years experience in Big data Space across Architecture, Design, Development, testing & Deployment, full understanding in SDLC.\n\n1. Experience of Hadoop and related technology stack experience\n\n2. Experience of the Hadoop Eco-system(HDP+CDP) / Big Data (especially HIVE)\nHand on experience with programming languages such as Java/Scala/python\nHand-on experience/knowledge on Spark3. Being responsible and focusing on uptime and reliable running of all or ingestion/ETL jobs4. Good SQL and used to work in a Unix/Linux environment is a must.5. Create and maintain optimal data pipeline architecture.6. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.7. Good to have cloud experience8. Good to have experience for Hadoop integration with data visualization tools like PowerBI.Location Mumbai, Pune, Chennai, Hyderabad, Coimbatore, Kolkata",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'hive', 'cloudera', 'scala', 'amazon redshift', 'big data technologies', 'pyspark', 'data warehousing', 'apache pig', 'linux internals', 'sql', 'java', 'spark', 'linux', 'flume', 'hadoop', 'etl', 'hbase', 'python', 'oozie', 'power bi', 'impala', 'nosql', 'mapreduce', 'cassandra', 'kafka', 'sqoop', 'aws', 'sdlc', 'unix']",2025-06-11 05:32:14
Business Intelligence Engineer,Amazon,5 - 10 years,Not Disclosed,['Bengaluru'],"Amazon s Spectrum Analytics team is looking for a Business Intelligence Engineer to help build the next generation of analytics solutions for Selling Partner Developer Services. This is an opportunity to get in on the ground floor as we transform from a reactive, request-directed team to a proactive, roadmap-driven organization that accelerates the business.\nWe need someone who is passionate about data and the insights that large amounts of data can provide. In addition to broad experience with data technologies from ingestion to visualization and consumption (e.g. data pipelines, ETL, reporting and dashboarding), the ideal candidate will have strong analysis skills and an insatiable curiosity to answer the question ""why?"". You will also be able to articulate the story the data is telling with compelling verbal and written communication.\n\n\nDeliver minimally to moderately complex data analysis; collaborating as needed with Data Science as complexity increases.\nDevelopment of dashboards and reports.\nDevelopment of minimally to moderately complex data processing jobs using appropriate technologies (e.g. SQL, Python, Spark, AWS Lambda, etc.), collaborating with Data Engineers as needed.\nCollaborate with stakeholders to understand business domains, requirements, and expectations. Additionally, working with owners of data source systems to understand capabilities and limitations.\nManage the deliverables of projects, anticipate risks and resolve issues.\nAdopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.\n\nAbout the team\nSpectrum offers a world-class suite of data products and experiences to empower the creation of innovative solutions on behalf of Partners. Our foundational systems and tools solve for cross-cutting Builder needs in externalizing data, and are easily extensible using federated policy and reusable technology. Experience with data visualization using Tableau, Quicksight, or similar tools\nExperience with data modeling, warehousing and building ETL pipelines\nExperience using SQL to pull data from a database or data warehouse and scripting experience (Python) to process data for modeling\n5+ years of relevant professional experience in business intelligence, analytics, statistics, data engineering, data science or related field.\nDemonstrated data analysis and visualization skills.\nHighly proficient with SQL.\nKnowledge of AWS products such as Redshift, Quicksight, and Lambda.\nExcellent verbal/written communication & data presentation skills; ability to succinctly summarize key findings and effectively communicate with both business and technical teams. Experience in data mining, ETL, etc. and using databases in a business environment with large-scale, complex datasets",,,,"['Data analysis', 'Data modeling', 'Test design', 'Data processing', 'data visualization', 'Business intelligence', 'Data mining', 'Analytics', 'SQL', 'Python']",2025-06-11 05:32:16
Automation Engineer,Capgemini,3 - 6 years,Not Disclosed,['Bengaluru'],"\n\nDesign, develop, and implement MLOps pipelines for the continuous deployment and integration of machine learning models.\n\nCollaborate with data scientists and engineers to understand model requirements and optimize deployment processes.\n\nTake offline models data scientists build and turn them into a real machine learning production system.\n\nAutomate the training, testing and deployment processes for machine learning models.\n\nContinuously monitor and maintain models in production, ensuring optimal performance, accuracy and reliability.\n\nImplement best practices for version control, model reproducibility and governance.\n\nOptimize machine learning pipelines for scalability, efficiency and cost-effectiveness.\n\nTroubleshoot and resolve issues related to model deployment and performance.\n\nEnsure compliance with security and data privacy standards in all MLOps activities.\n\nKeep up-to-date with the latest MLOps tools, technologies and trends.\n\nProvide support and guidance to other team members on MLOps practices.\n\nCommunicate with a team of data scientists, data engineers and architect, document the processe\n\nExperience in designing and implementing pipelines MLOps on AWS, Azure, or GCP.\n\nHands on building CI/CD pipelines orchestration using TeamCity, Jenkins, Airflow or similar tools.\n\nExperience with MLOps Frameworks like Kubeflow, MLFlow, DataRobot, Airflow etc., experience with Docker and Kubernetes, OpenShift.\n\nProgramming languages like Python, Go, Ruby or Bash, good understanding of Linux, knowledge of frameworks such as scikit-learn, Keras, PyTorch, Tensorflow, etc.\n\nAbility to understand tools used by data scientist and experience with software development and test automation.\n\nFluent in English, good communication skills and ability to work in a team.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'kubernetes', 'python', 'ci/cd', 'aws', 'software testing', 'scikit-learn', 'automation testing', 'openshift', 'airflow', 'golang', 'microsoft azure', 'docker', 'ruby', 'tensorflow', 'gcp', 'automation engineering', 'linux', 'jenkins', 'pytorch', 'keras', 'bash', 'teamcity']",2025-06-11 05:32:17
"Senior Engineer, Database",Nagarro,3 - 5 years,Not Disclosed,['Chennai'],"We're Nagarro.\n\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal experience 3+ years.\nHands-on working experience as a DBA managing large, mission-critical MySQL databases.\nStrong understanding of Relational Database Management Systems (RDBMS).\nProficiency in SQL (CRUD operations, Views, Materialized Views).\nDesign, develop, and optimize PL/SQL code including Packages, Procedures, Functions, Triggers, and advanced Exception Handling.\nExperience with PL/SQL code optimization techniques.\nExperience working with DB2 database engine.\nIntermediate knowledge of Unix and Shell Scripting.\nHands-on experience with IBM Data Studio.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\n\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them into technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers\nIdentifying different solutions and being able to narrow down the best option that meets the client’s requirements\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design documents explaining overall architecture, framework, and high-level design of the application for the developers\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken\nCarrying out POCs to make sure that suggested design/technologies meet the requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['RDBMS', 'Database', 'PLSQL', 'Db2 Engine']",2025-06-11 05:32:19
GEN AI Engineer,HCLTech,3 - 8 years,Not Disclosed,"['Noida', 'Chennai', 'Bengaluru']",Skill Needed\nJob:\nDefine the Agentic Function for Industrial Quality Inspection\nWorkflow design\nBring up the LLM and AI baseline framework -open source (Llava),,,,"['GEN AI', 'RAG', 'LLM', 'Open source', 'ML', 'Tensorflow']",2025-06-11 05:32:21
"Senior Data Scientist, Operations || Mumbai || 29 LPA",Argus India Price Reporting Services,5 - 10 years,20-25 Lacs P.A.,"['Mumbai Suburban', 'Navi Mumbai', 'Mumbai (All Areas)']","Senior Data Scientist, Operations\nMumbai, India\nAbout Argus:\n\nArgus is the leading independent provider of market intelligence to the global energy and commodity markets. We offer essential price assessments, news, analytics, consulting services, data science tools and industry conferences to illuminate complex and opaque commodity markets.\nHeadquartered in London with 1,500 staff, Argus is an independent media organisation with 30 offices in the worlds principal commodity trading hubs.\nCompanies, trading firms and governments in 160 countries around the world trust Argus data to make decisions, analyse situations, manage risk, facilitate trading and for long-term planning. Argus prices are used as trusted benchmarks around the world for pricing transportation, commodities and energy.\nFounded in 1970, Argus remains a privately held UK-registered company owned by employee shareholders and global growth equity firm General Atlantic.\n\nWhat were looking for:\nJoin our Generative AI team as a Senior Data Scientist, reporting directly to the Lead Data Scientist in India. You will play a crucial role in building, optimizing, and maintaining AI-ready data infrastructure for advanced Generative AI applications. Your focus will be on hands-on implementation of cutting-edge data extraction, curation, and metadata enhancement techniques for both text and numerical data. You will be a key contributor to the development of innovative solutions, ensuring rapid iteration and deployment, and supporting the Lead in achieving the team's strategic goals.\n\nWhat will you be doing:\nAI-Ready Data Development: Design, develop, and maintain high-quality AI-ready datasets, ensuring data integrity, usability, and scalability to support advanced Generative AI models.\nAdvanced Data Processing: Drive hands-on efforts in complex data extraction, cleansing, and curation for diverse text and numerical datasets. Implement sophisticated metadata enrichment strategies to enhance data utility and accessibility for AI systems.\nAlgorithm Implementation & Optimization: Implement and optimize state-of-the-art algorithms and pipelines for efficient data processing, feature engineering, and data transformation tailored for LLM and GenAI applications.\nGenAI Application Development: Apply and integrate frameworks like LangChain and Hugging Face Transformers to build modular, scalable, and robust Generative AI data pipelines and applications.\nPrompt Engineering Application: Apply advanced prompt engineering techniques to optimize LLM performance for specific data extraction, summarization, and generation tasks, working closely with the Lead's guidance.\nLLM Evaluation Support: Contribute to the systematic evaluation of Large Language Models (LLMs) outputs, analysing quality, relevance, and accuracy, and supporting the implementation of LLM-as-a-judge frameworks.\nRetrieval-Augmented Generation (RAG) Contribution: Actively contribute to the implementation and optimization of RAG systems, including working with embedding models, vector databases, and, where applicable, knowledge graphs, to enhance data retrieval for GenAI.\nTechnical Mentorship: Act as a technical mentor and subject matter expert for junior data scientists, providing guidance on best practices in coding and PR reviews, data handling, and GenAI methodologies.\nCross-Functional Collaboration: Collaborate effectively with global data science teams, engineering, and product stakeholders to integrate data solutions and ensure alignment with broader company objectives.\nOperational Excellence: Troubleshoot and resolve data-related issues promptly to minimize potential disruptions, ensuring high operational efficiency and responsiveness.\nDocumentation & Code Quality: Produce clean, well-documented, production-grade code, adhering to best practices for version control and software engineering.\n\nSkills and Experience:\nAcademic Background: Advanced degree in AI, statistics, mathematics, computer science, or a related field.\nProgramming and Frameworks: 2+ years of hands-on experience with Python, TensorFlow or PyTorch, and NLP libraries such as spaCy and Hugging Face.\nGenAI Tools: 1+ years Practical experience with LangChain, Hugging Face Transformers, and embedding models for building GenAI applications.\nPrompt Engineering: Deep expertise in prompt engineering, including prompt tuning, chaining, and optimization techniques.\nLLM Evaluation: Experience evaluating LLM outputs, including using LLM-as-a-judge methodologies to assess quality and alignment.\nRAG and Knowledge Graphs: Practical understanding and experience using vector databases. In addition, familiarity with graph-based RAG architectures and the use of knowledge graphs to enhance retrieval and reasoning would be a strong plus.\nCloud: 2+ years of experience with Gemini/OpenAI models and cloud platforms such as AWS, Google Cloud, or Azure. Proficient with Docker for containerization.\nData Engineering: Strong understanding of data extraction, curation, metadata enrichment, and AI-ready dataset creation.\nCollaboration and Communication: Excellent communication skills and a collaborative mindset, with experience working across global teams.\n\nWhats in it for you:\nCompetitive salary\nHybrid Working Policy (3 days in Mumbai office/ 2 days WFH once fully inducted)\nGroup healthcare scheme\n18 days annual leave\n8 days of casual leave\nExtensive internal and external training\n\nHours:\nThis is a full-time position operating under a hybrid model, with three days in the office and up to two days working remotely.\nThe team supports Argus key business processes every day, as such you will be required to work on a shift-based rota with other members of the team supporting the business until 8pm. Typically support hours run from 11am to 8pm with each member of the team participating up to 2/3 times a week.\n\nFor more details about the company and to apply please make sure you send your CV and cover letter via our website: www.argusmedia.com/en/careers/open-positions\nBy submitting your job application, you automatically acknowledge and consent to the collection, use and/or disclosure of your personal data to the Company. Argus is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, sexual orientation, gender identity, disability or veteran status.",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pytorch', 'Artificial Intelligence', 'LangChain', 'hugging face', 'Spacy', 'Tensorflow']",2025-06-11 05:32:23
Senior Data Scientist,Tractable,5 - 10 years,Not Disclosed,['Noida'],"Who we are\nTractable is an Artificial Intelligence company bringing the speed and insight of Applied AI to visual assessment. Trained on millions of data points, our AI-powered solutions connect everyone involved in insurance, repairs, and sales of homes and cars - helping people work faster and smarter, while reducing friction and waste.\nFounded in 2014, Tractable is now the AI tool of choice for world-leading insurance and automotive companies. Our solutions unlock the potential of Applied AI to transform the whole recovery ecosystem, from assessing damage and accelerating claims and repairs to recycling parts. They help make response to recovery up to ten times faster - even after full-scale disasters like floods and hurricanes.\nTractable has a world-class culture, backed up by our team, making us a global employer of choice!\nWere a diverse team, uniting individuals of over 40 different nationalities and from varied backgrounds, with machine learning researchers and motor engineers collaborating together on a daily basis. We empower each team member to have tangible impact and grow their own scope by intentionally building a culture centred around collaboration, transparency, autonomy and continuous learning.\nWere seeking a Senior Data Scientist to lead the delivery of real-world AI solutions across industries like insurance and automotive. Youll drive end-to-end ML development from scalable pipelines to production deployment while mentoring teammates and collaborating with enterprise partners to deliver business impact.\nYour impact\nArchitect Solutions: Design and optimise scalable ML systems, focusing on computer vision and NLP/LLM applications\nBuild & Deploy Models: Develop and productionise deep learning models using Python and modern ML frameworks\nLead Cross-Functionally: Align research, engineering, and product goals through hands-on leadership\nMentor & Guide: Support junior team members and promote best practices in ML development\nEngage Customers: Collaborate directly with enterprise clients to ensure seamless integration of AI solutions\nDrive Innovation: Evaluate and implement new AI tools and methodologies to enhance performance and scalability\nWhat youll need to be successful\nApplied AI Expertise: 5+ years building ML systems, especially in computer vision or NLP/LLM\nStrong Python & ML Skills: Proficiency in Python and ML libraries (e.g. PyTorch, TensorFlow)\nProduction Experience: Hands-on experience deploying ML models in production environments\nCloud & Data Engineering: Knowledge of cloud platforms (ideally AWS) and scalable data pipelines\nCollaboration & Communication: Ability to work across teams and communicate complex ideas clearly\nProblem-Solving Mindset: Analytical thinking and a focus on continuous improvement\nPreferred experience\nIndustry Impact: Experience applying AI in insurance, automotive, or similar sectors\nLLM/NLP Expertise: Background in large language models or conversational AI\nScalable Product Delivery: Proven success scaling ML solutions in production\nLeadership: Track record of mentorship and technical leadership in data science teams\nDiversity commitment\nAt Tractable, we are committed to building a diverse team and inclusive workplace where people s varied backgrounds and experiences are valued and recognised.\nWe encourage applications from candidates of all backgrounds and offer equal opportunities without discrimination.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer vision', 'Claims', 'Analytical', 'Artificial Intelligence', 'Machine learning', 'Technical leadership', 'Mentor', 'Continuous improvement', 'Automotive', 'Python']",2025-06-11 05:32:24
SENIOR SOFTWARE ENGINEER,Hsbc,2 - 11 years,Not Disclosed,['Pune'],"Some careers shine brighter than others.\nIf you re looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further.\nHSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions.\nWe are currently seeking an experienced professional to join our team in the role of Senior Software Engineer.\nIn this role, you will:\nPreparation of a low-level design after understanding functional specifications\nImplementation of given specifications as per HSBC standards\nReview peer developer s code\nCreate small or technical tickets\nRelease changes to production using CICD Jenkins pipeline\nWrite Junit and automated test cases in the project\nInteraction with business analyst and product owner\nGroom junior developers on Development activities GCP implementation\nCo-ordinate with other application technical teams\n\n\n\n\n\n\n\n\n\n\nRequirements\n\n\n\nTo be successful in this role, you should meet the following requirements:\nCandidate is expected to have prior experience of ETL application with large data volume (million rows)\nCandidate should have development experience of more than 3+ years, banking transaction data experience is preferred\nCandidate should have hands on experience on Apache Hadoop or equivalent big data technology\nCandidate should have good knowledge of spark or equivalent technology\nCandidate should have acute sense of data frame objects design and memory allocation techniques\nCandidate should have design experience\nCandidate should have RDBMS, SQL, Unix Scripting, ETL experience\nCandidate should have very good understanding of data base or file system write operations\nCandidate should have prior experience of an application which exposes multiple interfaces\nCandidate should understand micros services architecture API driven system\nCandidate should be able to design system considering UI and other external interfaces\nCandidate should have Cloud implementation exp. Either GCP (preferred) or equivalent cloud provider\nUnderstanding of cluster deployment for spark applications\nUnderstanding of Big data concept, Devops principles Container technology\nJava 1. 8+, Apache Spark 2. 3 or 3. X, Apache Hadoop (Spark/hdfs/Yarn)\nSpring Boot (4+), reactive spring boot\nXML, XSLT, Junit 5 or equivalent\nBit Bucket/ GitHub, Jenkins or similar for CI/CD",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['github', 'RDBMS', 'GCP', 'XML', 'XSLT', 'Test cases', 'big data', 'YARN', 'Financial services', 'SQL']",2025-06-11 05:32:26
Lead Enterprise Data Architect,Bread Financial,8 - 13 years,Not Disclosed,['Bengaluru'],"Every career journey is personal. Thats why we empower you with the tools and support to create your own success story.\nBe challenged. Be heard. Be valued. Be you ... be here.\nJob Summary\nLead Enterprise Data Architect provides strategic leadership in the development and implementation of complex architectural solutions across the enterprise. Guide projects to ensure alignment with architectural standards.\nthis position is for a broad architecture role with a focus on Data Architecture. Architects in this role are expected to have solid Data Architecture experiences at both the strategical and technical end of the spectrum. Expertise in enterprise data concepts such as MDM, data modeling (both transactional and analytical), various analytical platorm architectures (data lake, data lakehouse, data fabric, data mesh, etc.), data governance (data catalog, quality, lineage), data stream processing, data science, AI/ML, Generative AI. Deep familiarity with data engineering and analysis tools such as Airflow, Microstrategy, cloud data-related services, Snowflake, Collibra, Databricks, and other similar tools would be important.\n\nEssential Job Functions\nCollaborate with both business and IT function to foster innovation and ensure strategic alignment. - (30%)\nDrive the adoption of innovative technologies to improve business processes. - (20%)\nOversee architectural governance and ensure compliance with IT policies. - (20%)\nMentor junior architects and provide thought leadership across the organization. - (20%)\nAct as a subject-matter expert for enterprise architecture. - (10%)\n\nMinimum Qualifications\nBachelor s Degree or equivalent education and / or experience in computer Science, Engineering, Mathematics or related field.\n8+ years in Information Technology\n\nPreferred Qualifications\nRelevant certifications, such as TOGAF or AWS Solutions Architect\n\nSkills\nApplication Development\nBusiness Alignment\nBusiness Process Modeling\nBusiness Case Development\nCode Inspection\nCloud Architectures\nEnterprise Architecture Framework\nIT Architecture\nIT Roadmap\nSolution Architecture\n\nReports To : Senior Manager and above\n\nDirect Reports : 0\n\nWork Environment\nNormal office environment, hybrid.\n\nOther Duties\nThis job description is illustrative of the types of duties typically performed by this job. It is not intended to be an exhaustive listing of each and every essential function of the job. Because job content may change from time to time, the Company reserves the right to add and/or delete essential functions from this job at any time.\nAbout Bread Financial\nAt Bread Financial, you ll have the opportunity to grow your career, give back to your community, and be part of our award-winning culture. We ve been consistently recognized as a best place to work nationally and in many markets and we re proud to promote an environment where you feel appreciated, accepted, valued, and fulfilled both personally and professionally. Bread Financial supports the overall wellness of our associates with a diverse suite of benefits and offers boundless opportunities for career development and non-traditional career progression.\nBread Financial (NYSE: BFH) is a tech-forward financial services company that provides simple, personalized payment, lending, and saving solutions to millions of U.S consumers. Our payment solutions, including Bread Financial general purpose credit cards and savings products, empower our customers and their passions for a better life. Additionally, we deliver growth for some of the most recognized brands in travel & entertainment, health & beauty, jewelry and specialty apparel through our private label and co-brand credit cards and pay-over-time products providing choice and value to our shared customers.\nTo learn more about Bread Financial, our global associates and our sustainability commitments, visit breadfinancial.com or follow us on Instagram and LinkedIn .\nAll job offers are contingent upon successful completion of credit and background checks.\nBread Financial is an Equal Opportunity Employer.\nJob Family:\nInformation Technology\nJob Type:\nRegular",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Microstrategy', 'Solution architecture', 'Computer science', 'Career development', 'Architecture', 'Analytical', 'Wellness', 'Application development', 'Information technology', 'Financial services']",2025-06-11 05:32:28
Sr Associate HR Data Analysis (Visier Admin),Amgen Inc,5 - 8 years,Not Disclosed,['Hyderabad'],"Role Description:\nAmgen is seeking a Sr Associate HR Data Analysis (Visier Admin). The Sr Associate HR Data Analysis (Visier Admin) will report to the Associate Director HR Technology.\nThe successful incumbent will have previous Visier reporting tool Admin experience.\nRoles & Responsibilities:\nHands on experience supporting Visier\nPrevious experience with Vee\nAdministrative tasks associated with Visier such as role assignments and creating roles\nVisier Security configuration, data integration, and data exports\nAbility to analyze, troubleshoot and resolve Visier data issues\nMust have previous experience handling large datasets and sensitive HR data\nBasic Qualifications and Experience:\n5 years minimum experience in human resources with hands on experience with Visier\nMasters degree, OR\nBachelors degree and 5 years of HR IS experience\nFunctional Skills:\nMust-Have Skills:\nStrong working knowledge of Visier\n5+ years experience in human resources and corporate service center supporting Workday\nSoft Skills:\nExcellent analytical and troubleshooting skills\nStrong quantitative, analytical (technical and business), problem solving skills, and attention to detail\nStrong verbal, written communication and presentation skills\nAbility to work effectively with global, virtual teams\nStrong technical acumen, logic, judgement and decision-making\nStrong initiative and desire to learn and grow\nAbility to manage multiple priorities successfully\nExemplary adherence to ethics, data privacy and compliance policies",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'Visier Admin', 'troubleshooting', 'data integration', 'HR Data Analysis']",2025-06-11 05:32:29
Cloud Engineer,"NTT DATA, Inc.",9 - 14 years,Not Disclosed,['Bengaluru'],"Req ID: 324901\n\nWe are currently seeking a Cloud Engineer to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\nAt NTT DATA, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company""™s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring, the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA and for the people who work here.\n\n\n\nPreferred Experience\nSolid understanding of cloud computing, networking, and storage principles with focus on Azure. Should have strong delivery knowledge and experience around cloud adoption and workload management in public cloud (IaaS and PaaS platforms)\nWilling to work on multiple cloud platforms. Understanding of the customer strategy, business, technical, security and compliance requirements.\nExpertise in Cloud Infrastructure Networking, Security, IAM, Data Security and leveraging the right solutions in these areas is required.\nSolid scripting and automation experience (DevOps & scripting). Implementation experience on Infrastructure as Code (IaC) using tools like ARM templates and/or Terraform.\nGood experience on emerging technologies, including Container bases orchestration, AKS, AI services etc.,\nInnovative self-starter, willing to learn, test, implement new technologies on existing and new public cloud providers.\nCollaborate with different teams ""“ Data Engineering, Data Analytics, InfoSec, Network/firewall, etc.\nClear understanding of automation.\nDefine requirements and evaluate existing architectures, Various technical solutions, products, Partners against our technical requirements ""“ technical, NFR such as operational resilience, scalability, reliability, performance.\nRegular interactions with senior management or executive levels on matters concerning several functional areas and/or customers are also part of the job\nAssist with the design and implementation of best governance practices for design, security, development, usability, cost optimization / control.\nKnowledge of best practices and market trends pertaining to Cloud and overall industry to provide thought leadership (seminars, whitepapers etc.,) and mentor team to build necessary competency\nCollaborating in the creation work/design documents with team assistance. Able to work independently in a project scenario and do POCs.\nShould be able to manage & mentor the junior team members from L2 / L1.\nAble to work on On-Call rotations.\nExperience handling P1/P2 calls and escalations\nAble to write quality KB articles, Problem Management articles, and SOPs/Runbooks.\nPassion for delivering timely and outstanding customer service\nGreat written and oral communication skills with internal and external customers\n\n\n\nBasic Qualifications\nAt least 9+ years of overall operational experience.\n6+ years of Azure experience\n3+ years of experience working in a diverse cloud support environment in a 24*7 production support model\n2+ years DevOps/scripting/APIs experience\n\n\nPreferred Certifications\nAzure Administrator Associate/Solution Architect Certifications.\nMicrosoft DevOps / Terraform certifications is an added advantage.\nAWS SysOps Administrator/Developer/Solutions Architect Associate Certifications\nFour Years degree on Information Technology degree or equivalent experience",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['data security', 'networking', 'iam', 'cloud infrastructure', 'cloud computing', 'container', 'public cloud', 'aks', 'microsoft azure', 'cloud platforms', 'cloud support', 'workload management', 'production support', 'firewall', 'devops', 'paas', 'terraform', 'api', 'iaas', 'arm templates', 'aws']",2025-06-11 05:32:31
Data Modeller,Leading Client,1 - 3 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Locations - Pune/Bangalore/Hyderabad/Indore\n\nContract duration- 6 months\n\nResponsibilities\n\nBe responsible for the development of the conceptual, logical, and physical data models, the implementation of RDBMS, operational data store (ODS), data marts, and data lakes on target platforms.\n\nImplement business and IT data requirements through new data strategies and designs across all data platforms (relational & dimensional - MUST and NoSQL-optional) and data tools (reporting, visualization, analytics, and machine learning).\n\nWork with business and application/solution teams to implement data strategies, build data flows, and develop conceptual/logical/physical data models\n\nDefine and govern data modeling and design standards, tools, best practices, and related development for enterprise data models.\n\nIdentify the architecture, infrastructure, and interfaces to data sources, tools supporting automated data loads, security concerns, analytic models, and data visualization.\n\nHands-on modeling, design, configuration, installation, performance tuning, and sandbox POC.\n\nWork proactively and independently to address project requirements and articulate issues/challenges to reduce project delivery risks.\n\nMust have Payments Background\n\nSkills\n\nHands-on relational, dimensional, and/or analytic experience (using RDBMS, dimensional, NoSQL data platform technologies, and ETL and data ingestion protocols).\n\nExperience with data warehouse, data lake, and enterprise big data platforms in multi-data-center contexts required.\n\nGood knowledge of metadata management, data modeling, and related tools (Erwin or ER Studio or others) required.\n\nExperience in team management, communication, and presentation.\n\nExperience with Erwin, Visio or any other relevant tool.",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data modelling', 'metadata', 'metadata management', 'payments', 'data warehousing', 'er studio', 'erwin', 'sql', 'data mart', 'data modeling', 'data visualization', 'etl', 'big data', 'data lake', 'python', 'rdbms', 'performance tuning', 'presentation skills', 'machine learning', 'er', 'nosql', 'visio', 'ssis', 'data flow', 'ods']",2025-06-11 05:32:33
Digital Engineering Staff Engineer,"NTT DATA, Inc.",2 - 5 years,Not Disclosed,['Bengaluru'],"Req ID: 310007\n\nWe are currently seeking a Digital Engineering Staff Engineer to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\nData Modeler\n\n\n\nPosition Overview: The Data Modeler will be responsible for designing and implementing data models that support the organization's data management and analytics needs.\n\nThis role involves collaborating with various stakeholders to understand data sources, relationships, and business requirements, and translating them into effective data structures.\n\n\n\nKey Responsibilities:\n\n\nCollaborate with Business Analysts: Understand different data sources and their relationships.\n\n\nPrepare Conformed Dimension Matrix: Identify different grains of facts, finalize dimensions, and harmonize data across sources.\n\n\nCreate Data Models: Develop Source to Target Mapping (STMs) documentation and custom mappings (both technical and non-technical).\n\n\nInclude Transformation Rules: Ensure STMs include pseudo SQL queries for transformation rules.\n\n\nCoordinate Reviews: Work with Data Architects, Product Owners, and Enablement teams to review and approve models, STMs, and custom mappings.\n\n\nEngage with Data Engineers: Clarify any questions related to STMs and custom mappings.\n\n\n\nRequired Technical\n\nSkills:\n\n\n\nProficiency in SQL: Strong understanding of SQL and database management systems.\n\n\nData Modeling Tools: Familiarity with tools such as ERwin, IBM InfoSphere Data Architect, or similar.\n\n\nData Warehousing Concepts: Solid knowledge of data warehousing principles, ETL processes, and OLAP.\n\n\nData Governance and Compliance: Understanding of data governance frameworks and compliance requirements.\n\n\n\nKey Competencies:\n\n\nAnalytical\n\nSkills:\nAbility to analyze complex data sets and derive meaningful insights.\n\n\nAttention to Detail: Ensure accuracy and consistency in data models.\n\n\nCommunication\n\nSkills:\nEffectively collaborate with stakeholders and articulate technical concepts to non-technical team members.\n\n\nProject Management\n\nSkills:\nAbility to prioritize tasks, manage timelines, and coordinate with cross-functional teams.\n\n\nContinuous Learning and Adaptability: Commitment to ongoing professional development and adaptability to changing business needs and technologies.\n\n\n\nAdditional :\n\n\nProblem-Solving Abilities: Innovative solutions to data integration, quality, and performance challenges.\n\n\nKnowledge of Data Modeling Methodologies: Entity-relationship modeling, dimensional modeling, normalization techniques.\n\n\nFamiliarity with Business Intelligence Tools: Enhance ability to design data structures that facilitate data analysis and visualization.\n\n\n\nPreferred Qualifications:\n\n\nExperience in SDLC: Understanding of all phases of the Software Development Life Cycle.\n\n\nCertifications: Relevant certifications in data modeling, data warehousing, or related fields.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['database management system', 'data warehousing', 'sql', 'olap', 'etl process', 'c#', 'ibm infosphere', 'dimensional modeling', 'erwin', 'business intelligence', 'javascript', 'sql server', 'software development life cycle', 'linq', 'java', 'data modeling', 'asp.net', 'data structures', 'etl', 'sdlc']",2025-06-11 05:32:35
Senior Data Scientist,Go Digital Technology Consulting,4 - 7 years,Not Disclosed,"['Pune', 'Mumbai (All Areas)']","Job Title: Sr. Data Scientist\nLocation: Mumbai / Pune\nJob Type: Full-time\nExperience: 4-7 years\n\nAbout the Role:\nWe are seeking a Sr. Data Scientist with experience in Computer Vision to join our team. You will be responsible for building and maintaining models including but not limited to Vision Transformer neural networks. This role involves collaborating with data engineers and backend developers to deliver quality AI solutions.\n\nResponsibilities:\nDesign and implement end-to-end machine learning workflows for image and computer vision applications, from data collection to model deployment.\nCollaborate with cross-functional teams, including data engineers, product managers, and domain experts, to define and prioritize machine learning initiatives.\nDocument technical designs and model specifications, ensuring clarity and accessibility for stakeholders and team members.\nEnsure adherence to best practices in model development, deployment, and monitoring, in alignment with the overall AI strategy.\nMonitor model performance and implement strategies for continuous improvement and retraining as needed.\nDevelop scalable and efficient deep learning models using PyTorch, optimizing for performance and resource utilization.\n\nQualifications:\nBachelors degree in Computer Science or a related field.\n4-7 years of hands-on experience in developing and deploying machine learning models, particularly in computer vision tasks.\nProficient in using PyTorch for developing deep learning models, with a strong understanding of CNNs, transfer learning, vision transformers, and data augmentation techniques.\nSolid understanding of computer vision concepts, including image classification, object detection, and image segmentation.\nStrong programming skills in Python, with experience in data manipulation libraries such as NumPy and Pandas.\nExperience with version control systems like Git.\nExcellent analytical and problem-solving skills, strong communication abilities, and a collaborative mindset.\n\nPreferred Qualifications:\nExperience with cloud platforms (e.g., AWS, GCP, Azure) and their ML services, particularly those related to model deployment and GPU training.\nUnderstanding of MLOps principles and practices, including model monitoring, versioning, and governance.\nKnowledge of GPU computing and tools for managing GPU resources (e.g., CUDA, cuDNN).",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Senior Data Scientist', 'Computer Vision', 'Pytorch', 'Pandas', 'Machine Learning', 'Numpy', 'Python']",2025-06-11 05:32:37
Senior .NET/GCP Engineer (REMOTE),"NTT DATA, Inc.",3 - 7 years,Not Disclosed,['Chennai'],"Req ID: 316318\n\nWe are currently seeking a Senior .NET/GCP Engineer (REMOTE) to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\n\n\n\n\n\nSenior .NET/GCP Engineer - Remote\n\n\n\n\n\nHow You""™ll Help Us:\n\nA Senior Application Developer is first and foremost a software developer who specializes in .NET C",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['dns', 'networking', 'technical support', 'active directory', 'dhcp', 'switching', 'medidata rave', 'routing', 'system administration', 'it recruitment', 'remote support', 'recruitment', 'application support', 'microsoft windows', 'desktop support', 'troubleshooting', '.net', 'hadoop', 'big data', 'ccna']",2025-06-11 05:32:38
Lead Data Scientist - RevX,Affle,5 - 9 years,Not Disclosed,"['Mumbai', 'Gurugram', 'Bengaluru']","Research and Problem-Solving: Identify and frame business problems, conduct exploratory data analysis, and propose innovative data science solutions tailored to business needs.\nLeadership & Communication: Serve as a technical referent for the research team, driving high-impact, high-visibility initiatives. Effectively communicate complex scientific concepts to senior stakeholders, ensuring insights are actionable for both technical and non-technical audiences. Mentor and develop scientists within the team, fostering growth and technical excellence.\nAlgorithm Development: Design, optimize, and implement advanced machine learning algorithms, including neural networks, ensemble models (XGBoost, random forests), and clustering techniques.\nEnd-to-End Project Ownership: Lead the development, deployment, and monitoring of machine learning models and data pipelines for large-scale applications.\nModel Optimization and Scalability: Focus on optimizing algorithms for performance and scalability, ensuring robust, well-calibrated models suitable for real-time environments.\nA/B Testing and Validation: Design and execute experiments, including A/B testing, to validate model effectiveness and business impat.\nBig Data Handling: Leverage tools like BigQuery, advanced SQL, and cloud platforms (e.g., GCP) to process and analyze large datasets.\nCollaboration and Mentorship: Work closely with engineering, product, and campaign management teams, while mentoring junior data scientists in best practices and advanced techniques.\nData Visualization: Create impactful visualizations using tools like matplotlib, seaborn, Looker, and Grafana to communicate insights effectively to stakeholders.\nRequired Experience/Skills\n5–8 years of hands-on experience in data science or machine learning roles.\n2+ years leading data science projects in AdTech\nStrong hands-on skills in Advanced Statistics, Machine Learning, and Deep Learning.\nDemonstrated ability to implement and optimize neural networks and other advanced ML models.\nProficiency in Python for developing machine learning models, with a strong grasp of TensorFlow or PyTorch.\nExpertise handling large datasets using advanced SQL and big data tools like BigQuery\nIn-depth knowledge of MLOps pipelines, from data preprocessing to deployment and monitoring.\nStrong background in A/B testing, statistical analysis, and experimental design.\nProven capability in clustering, segmentation, and unsupervised learning methods.\nStrong problem-solving and analytical skills with a focus on delivering business value.\nEducation:\nA Master’s in Data Science, Computer Science, Mathematics, Statistics, or a related field is preferred. A Bachelor's degree with exceptional experience will also be considered.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['advance sql', 'python', 'adtech', 'neural networks', 'random forest', 'machine learning', 'sql', 'pipeline', 'deep learning', 'tensorflow', 'ab testing', 'seaborn', 'data science', 'grafana', 'design', 'matplotlib', 'pytorch', 'bigquery', 'big data', 'xgboost', 'machine learning algorithms', 'statistics', 'ml']",2025-06-11 05:32:40
Senior Data Consultant,Go Digital Technology Consulting,6 - 11 years,Not Disclosed,"['Pune', 'Mumbai (All Areas)']","Role: Senior Data Consultant\nExperience: 6yrs-12yrs\nLocation: Mumbai & Pune\n\nAs a Senior Data Consultant, you will serve as a trusted advisor and subject matter expert, using data to drive actionable business insights. In this role, you will not only lead client engagements and deliver high-quality data solutions but also work closely with our sales teams to create accelerators, architectural artifacts, and pre-sales assets for RFPs.\n\nRole Summary\nLead strategic data consulting projects and work directly with clients to design and implement data-driven solutions.\nDevelop and maintain a library of accelerators, frameworks, and artifacts that support sales proposals and RFP responses.\nCollaborate with cross-functional teams to ensure successful client delivery and seamless handoffs between sales and project implementation.\n\nKey Skills / Technologies\nMust-Have:\nData Analytics & Visualization (Tableau, Power BI, etc.)\nAdvanced SQL and data querying skills\nStrong statistical and analytical expertise\nExperience in data integration, cleansing, and modeling\nExcellent communication and stakeholder management skills\nGood-to-Have:\nFamiliarity with programming languages (Python, R) for advanced analytics\nKnowledge of data warehousing and big data platforms\nExperience in a consulting or client-facing role\nFamiliarity with data governance and business intelligence frameworks\n\nResponsibilities\nClient Consulting & Delivery:\nLead data analysis projects and work directly with clients to define requirements and deliver actionable insights.\nDesign data models and visualizations that support strategic decision-making across multiple business areas.\nAdvise clients on best practices for data management and analytics to optimize their business processes.\nSales & Pre-Sales Support:\nCreate accelerators, consulting frameworks, and architectural artifacts that can be used in RFP responses and sales proposals.\nSupport the sales team by participating in client presentations, technical workshops, and presales engagements.\nProvide expert insights and technical recommendations that bridge the gap between client needs and technology solutions.\nCollaboration & Mentoring:\nWork closely with technical teams, sales, and delivery teams to ensure cohesive strategies and smooth client handoffs.\nMentor junior consultants and share best practices in data analysis and consulting methodologies.\n\nRequired Qualifications\nBachelors or Masters degree in Data Science, Business Analytics, Statistics, or a related field.\n5+ years of experience in data consulting or analytics roles, with significant client-facing responsibilities.\nDemonstrated ability to develop data-driven solutions that positively impact business performance.\nExcellent problem-solving and communication skills, with experience supporting both sales and client delivery.\n\nWhy Join Us Be at the forefront of transforming businesses through data by working on diverse, high-impact projects. Play a dual role in driving sales success and ensuring high-quality client delivery. Collaborate with a passionate team of experts and enjoy continuous professional growth in a dynamic environment with competitive benefits.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Consultant', 'Cloud Platform', 'RFP', 'Business Intelligence', 'Client Delivery', 'Sales Support', 'Data Modeling', 'Data Analytics', 'SQL']",2025-06-11 05:32:42
"Data Migration, CBS (Finacle), Senior Manager",Wikilabs India,6 - 9 years,Not Disclosed,['Navi Mumbai'],"Job Title: Senior Manager - Data Migration(Legacy to Finacle Application )\nLocation: Seawoods , Navi Mumbai\nShift: General.\nRelocation is not allowed; only candidates from Mumbai/Navi Mumbai are preferred.\n\nJob Summary:\nWe are seeking a highly experienced and hands-on Senior Manager, Finacle Data Migration, to lead and execute complex data migration projects for banking . The ideal candidate will have a proven track record of migrating legacy banking systems to Finacle 11x, with deep expertise across key modules including Assets, Liabilities, Customer Data Hub, and Trade Finance. This role requires a strong technical background in ETL processes, database management, and reporting tools, coupled with the ability to independently manage and deliver end-to-end data migration cycles.\n\nKey Responsibilities:\nLead and execute end-to-end data migration projects from legacy systems to Finacle 11x, specifically for modules such as Assets, Liabilities, Customer Data Hub, and Trade Finance (TF).\nDevelop, test, and maintain robust data migration scripts, data quality rules, and reconciliation scripts.\nConduct detailed data mapping activities from various legacy formats to Finacle 11x standards. Perform all necessary data extraction requirements from surrounding systems, ensuring data integrity and completeness.\nExecute multiple mock migrations, continuously updating and refining scripts based on findings and business requirements.\nConduct thorough data validation and correction of uploaded data to ensure accuracy and consistency post-migration.\nOversee and manage pre-migration preparation and post-migration activities, including cutover planning and post-go-live support.\nDevelop and maintain reports using SSRS/Power BI to support migration activities, reconciliation, and post-migration data analysis.\nProactively track project progress, identify potential risks, and report status to stakeholders effectively and transparently.\nCollaborate closely with business users, IT teams, and vendors to ensure successful project delivery.\n\nRequired Skills & Experience:\nMinimum of 6 years of experience in IT, with a significant focus on data migration within the banking or financial services sector.\nMust have hands-on experience working on migrating data to Finacle 11x versions.\nDemonstrable experience with Finacle modules including Assets, Liabilities, Customer Data Hub, and Trade Finance (TF).\nStrong background in ETL (Extract, Transform, Load) processes and tools.\nProven hands-on experience in Finacle data migration projects, having successfully executed at least 1-2 full-cycle Finacle data migration projects from legacy to Finacle 11x.\nProficiency in database management systems, specifically MS SQL and/or Oracle.\nExpert-level knowledge and hands-on experience with SQL, PL/SQL, and SQL Uploader.\nHands-on experience in report development using SSRS and/or Power BI.\nExcellent understanding of data validation techniques, data quality principles, and data reconciliation processes.\nAbility to independently manage tasks, create technical deliverables, and troubleshoot complex data issues.\nStrong analytical and problem-solving skills with meticulous attention to detail.\nExcellent communication and interpersonal skills to effectively collaborate with various teams and stakeholders.\n\nEducation:\nBachelor's or Master's degree in Computer Science, Information Technology, or a related field.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Migration', 'legacy Migration', 'Legacy to Finacle Application Migration', 'Core banking']",2025-06-11 05:32:43
Software Applications Development Engineer,"NTT DATA, Inc.",5 - 10 years,Not Disclosed,"['Mumbai', 'Hyderabad', 'Bengaluru']","Your day at NTT DATA\nThe Software Applications Development Engineer is a seasoned subject matter expert, responsible for developing new applications and improving upon existing applications based on the needs of the internal organization and or external clients.\nWhat you'll be doing\nYrs. Of Exp: 5 Yrs.\n\nData Engineer-\nWork closely with Lead Data Engineer to understand business requirements, analyse and translate these requirements into technical specifications and solution design.\nWork closely with Data modeller to ensure data models support the solution design\nDevelop , test and fix ETL code using Snowflake, Fivetran, SQL, Stored proc.\nAnalysis of the data and ETL for defects/service tickets (for solution in production ) raised and service tickets.\nDevelop documentation and artefacts to support projects.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Application Development', 'Data Engineering', 'Snowflake', 'ETL', 'Fivetran', 'SQL']",2025-06-11 05:32:45
Sr Data Architect,HMH,5 - 10 years,Not Disclosed,['Pune'],"The data architect is responsible for designing, creating, and managing an organizations data architecture. This role is critical in establishing a solid foundation for data management within an organization, ensuring that data is organized, accessible, secure, and aligned with business objectives. The data architect designs data models, warehouses, file systems and databases, and defines how data will be collected and organized.\nResponsibilities\nInterprets and delivers impactful strategic plans improving data integration, data quality, and data delivery in support of business initiatives and roadmaps",,,,"['Data Architecture', 'Java', 'Azure', 'data analysis', 'SAS', 'PySpark', 'Hadoop', 'Azure Databricks', 'data warehousing', 'Teradata', 'SQL', 'C/C++', 'Power BI/Tableau', 'R', 'NoSQL', 'data modeling', 'GCP', 'Snowflake', 'metadata systems', 'Databricks', 'Oracle', 'AWS', 'Python']",2025-06-11 05:32:47
Data Scientist I,Affle,1 - 5 years,Not Disclosed,"['Mumbai', 'Gurugram', 'Bengaluru']","We are seeking a talented and motivated Data Scientist with 1-3 years of experience to join our Data Science team. If you have a strong passion for data science, expertise in machine learning, and experience working with large-scale datasets, we want to hear from you.\nAs a Data Scientist at RevX, you will play a crucial role in developing and implementing machine learning models to drive business impact. You will work closely with teams across data science, engineering, product, and campaign management to build predictive models, optimize algorithms, and deliver actionable insights. Your work will directly influence business strategy, product development, and campaign optimization.\n\nMajor Responsibilities:\nDevelop and implement machine learning models, particularly neural networks, decision trees, random forests, and XGBoost, to solve complex business problems.\nWork on deep learning models and other advanced techniques to enhance predictive accuracy and model performance.\nAnalyze and interpret large, complex datasets using Python, SQL, and big data technologies to derive meaningful insights.\nCollaborate with cross-functional teams to design, build, and deploy end-to-end data science solutions, including data pipelines and model deployment frameworks.\nUtilize advanced statistical techniques and machine learning methodologies to optimize business strategies and outcomes.\nEvaluate and improve model performance, calibration, and deployment strategies for real-time applications.\nPerform clustering, segmentation, and other unsupervised learning techniques to discover patterns in large datasets.\nConduct A/B testing and other experimental designs to validate model performance and business strategies.\nCreate and maintain data visualizations and dashboards using tools such as matplotlib, seaborn, Grafana, and Looker to communicate findings.\nProvide technical expertise in handling big data, data warehousing, and cloud-based platforms like Google Cloud Platform (GCP).\n\nRequired Experience/Skills:\nBachelors or Masters degree in Data Science, Computer Science, Statistics,\nMathematics, or a related field.\n1-3 years of experience in data science or machine learning roles.\nStrong proficiency in Python for machine learning, data analysis, and deep learning applications.\nExperience in developing, deploying, and monitoring machine learning models, particularly neural networks, and other advanced algorithms.\nExpertise in handling big data technologies, with experience in tools such as BigQuery and cloud platforms (GCP preferred).\nAdvanced SQL skills for data querying and manipulation from large datasets.\nExperience in data visualization tools like matplotlib, seaborn, Grafana, and Looker.\nStrong understanding of A/B testing, statistical tests, experimental design, and methodologies.\nExperience in clustering, segmentation, and other unsupervised learning techniques.\nStrong problem-solving skills and the ability to work with complex datasets and machine learning pipelines.\nExcellent communication skills, with the ability to explain complex technical concepts to non-technical stakeholders.\n\nPreferred Skills:\nExperience with deep learning frameworks such as TensorFlow or PyTorch.\nFamiliarity with data warehousing concepts and big data tools.\nKnowledge of MLOps practices, including model deployment, monitoring, and management.\nExperience with business intelligence tools and creating data-driven dashboards.\nUnderstanding of reinforcement learning, natural language processing (NLP), or other advanced AI techniques.\n  Education:\nBachelor of Engineering or similar degree from any reputed University.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['advance sql', 'python', 'data analysis', 'big data technologies', 'data warehousing', 'random forest', 'machine learning', 'sql', 'deep learning', 'ab testing', 'tensorflow', 'seaborn', 'data science', 'grafana', 'gcp', 'design', 'matplotlib', 'pytorch', 'bigquery', 'big data', 'xgboost', 'communication skills']",2025-06-11 05:32:48
F2F Drive-14th June-Bangalore LTIM - Data Scientist,Ltimindtree,6 - 11 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']",DS\n\nKey Responsibilities\nCombine expertise in mathematics statistics computer science and domain knowledge to create AIML models to solve various business challenges\nCollaborate closely with the AI Technical Manager and GCC Petro technical professionals and data engineers to integrate models into the business framework\nIdentify and frame opportunities to apply advanced analytics modeling and related technologies to data to help businesses gain insight and improve decision making workflow and automation\nUnderstand and communicate the value of proposed opportunity with team members and other stakeholders\nIdentify needed data and appropriate technology to solve identified business challenges\nClean data and develop and test models\nEstablish the life cycle management process for models\nProvide technical mentoring in modeling and analytics technologies the specifics of the modeling process and general consulting skills\nDrive innovation in AIML to enhance capabilities in data driven decision making\nAligns with team on shared goals and outcomes recognizes others contributions and work collaboratively seek diverse perspectives\nTakes actions to develop self and others beyond existing skillset\nEncourages innovative ideas adapts to change and changing technologies\nUnderstand and communicate data insights and model behaviors to stakeholders with varying levels of technical expertise\nRequired Qualification\nMinimum 5 years of experience in designing and developing AIML models and or various optimization algorithms 5 to 9 years of experience\nSolid foundation in mathematics probability and statistics with demonstrated depth of knowledge and experience in advanced analytics and data science methodologies eg supervised and unsupervised learning statistics data science model development\nProficiency in Python and working knowledge of cloud AIML services Azure Machine Learning and Databricks preferred\nDomain knowledge relevant to the energy sector and working knowledge of Oil and Gas value chain eg upstream midstream or downstream and associated business workflows\nProven ability to frame data science opportunities leverage standard foundational tools and Azure services to perform exploratory data analysis for purposes of data cleaning and discovery visualize data and identify actions to reach needed results\nAbility to quickly assess current state and apply technical concepts across cross functional business workflows\nExperience with driving successful execution deliverables and accountabilities to meet quality and schedule goals\nAbility to translate complex data into actionable insights that drive business val\nDemonstrated ability to engage and establish collaborative relationships both inside and outside immediate workgroup at various organizational levels across functional and geographic boundaries to achieve desired outcomes\nDemonstrated ability to adjust behavior based on feedback and provide feedback to other\nTeam oriented mindset with effective communication skills and the ability to work collaboratively\nStrong problem solving skills and attention to detail\nExcellent communication and collaboration skills,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Machine Learning', 'GCP', 'Data Scientist', 'Python', 'Predictive Modeling', 'Azure', 'Generative AI', 'Natural Language Processing', 'Deep Learning', 'Data Science', 'Azure Machine Learning', 'Computer Vision', 'AWS']",2025-06-11 05:32:50
Director Data Science,Astar Data,10 - 17 years,Not Disclosed,['Bengaluru'],"Sigmoid enables business transformation using data and analytics, leveraging real-time insights to make accurate and fast business decisions, by building modern data architectures using cloud and open source. Some of the worlds largest data producers engage with Sigmoid to solve complex business problems. Sigmoid brings deep expertise in data engineering, predictive analytics, artificial intelligence, and DataOps. Sigmoid has been recognized as one of the fastest growing technology companies in North America, 2021, by Financial Times, Inc. 5000, and Deloitte Technology Fast 500.\nOffices: New York | Dallas | San Francisco | Lima | Bengaluru\nThe below role is for our Bengaluru office.\n\nWhy Join Sigmoid?\n• Sigmoid provides the opportunity to push the boundaries of what is possible by seamlessly\ncombining technical expertise and creativity to tackle intrinsically complex business\nproblems and convert them into straight-forward data solutions.\n• Despite being continuously challenged, you are not alone. You will be part of a fast-paced\ndiverse environment as a member of a high-performing team that works together to\nenergize and inspire each other by challenging the status quo\n• Vibrant inclusive culture of mutual respect and fun through both work and play\nRoles and Responsibilities:\n• Convert broad vision and concepts into a structured data science roadmap, and guide a\nteam to successfully execute on it.\n• Handling end-to-end client AI & analytics programs in a fluid environment. Your role will be a\ncombination of hands-on contribution, technical team management, and client interaction.\n• Proven ability to discover solutions hidden in large datasets and to drive business results\nwith their data-based insights\n• Contribute to internal product development initiatives related to data science.\n• Drive excellent project management required to deliver complex projects, including\neffort/time estimation.\n• Be proactive, with full ownership of the engagement. Build scalable client engagement level\nprocesses for faster turnaround & higher accuracy\n• Define Technology/ Strategy and Roadmap for client accounts, and guides implementation\nof that strategy within projects\n• Manage the team-members, to ensure that the project plan is being adhered to over the\ncourse of the project\n• Build a trusted advisor relationship with the IT management at clients and internal accounts\nleadership.\nMandated Skills:\n• A B-Tech/M-Tech/MBA from a top tier Institutepreferably in a quantitativesubject\n• 10+ years of hands-onexperience in applied Machine Learning, AI and analytics\n• Experience of scientific programming in scripting languages like Python, R, SQL, NoSQL,\nSpark with ML tools & Cloud Technology (AWS, Azure, GCP)\n• Experience in Python libraries such as numpy, pandas, scikit-learn, tensor-flow, scrapy, BERT\netc. Strong grasp of depth and breadth of machine learning, deep learning, data mining, and\nstatistical concepts and experience in developing models and solutions in these areas\n• Expertise with client engagement, understanding complex problem statements, and offering\nsolutions in the domains of Supply Chain, Manufacturing, CPG, Marketing etc.\nDesired Skills:\nDeep understanding of ML algorithms for common use cases in both structured and\nunstructured data ecosystems.\nComfortable with large scale data processing and distributed computing\nProviding required inputs to sales, and pre-sales activities\nA self-starter who can work well with minimalguidance\nExcellent written and verbal communication skills",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Machine Learning', 'Algorithm Development', 'Pattern Recognition', 'Opencv', 'Image Processing', 'Artificial Intelligence', 'Natural Language Processing', 'Neural Networks', 'Computer Vision', 'Deep Learning']",2025-06-11 05:32:51
Lead Data Scientist,Tothr,8 - 12 years,20-22.5 Lacs P.A.,"['Chennai', 'Bengaluru']","Experience working closely with other data scientists, data engineers' software engineers, data managers and business partners.\n7+ years in designing, planning, prototyping, productionizing, maintaining\nknowledge in Python, Go, Java,\nSQL knowledge",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Statistical Modeling', 'Python Development', 'Machine Learning', 'Deep Learning', 'Generative Ai', 'Advance Sql']",2025-06-11 05:32:53
Azure DevOps DBT Engineer,DXC Technology,2 - 4 years,Not Disclosed,['Bengaluru'],"Job Description:\nAzure DevOps DBT Engineer\nJob Location: Hyderabad / Bangalore / Chennai / Kolkata / Noida/ Gurgaon / Pune / Indore / Mumbai\nGood knowledge in DBT in integration with Azure DevOps.\nDesign, develop, and maintain DBT models, transformations, and SQL code to build efficient data pipelines for analytics and reporting.\nDefine, analyse, improve, and implement technical standards\nBe an excellent and creative problem solver\nDesign, test, implement, manage, and maintain business critical big data platforms\nDesign, develop and supervise implementation of test plans\nTo deploy releases and hotfixes to test and production environments.\nTroubleshoot issues for internal and external customers, providing problem identification and resolution\nMigrate manual configuration to automated framework wherever possible\nDesign solutions to meet internal and external customer needs\nAssist in designing and applying system standards\nWork with development, testing and QA to improve the product\nCreate and update tooling used by the support team\nKnowledge and Experience:-\n3+ years experience in DevOps implementation.\nThe resource should have hands on experience with DBT.\nOverall 10+ years of experience in IT industry.\nDemonstrable experience of working as a DevOps Engineer within an enterprise scale environment\nExperience working within the constraints of change management\nPropensity for knowledge sharing in a team environment\nRecruitment fraud is a scheme in which fictitious job opportunities are offered to job seekers typically through online services, such as false websites, or through unsolicited emails claiming to be from the company. These emails may request recipients to provide personal information or to make payments as part of their illegitimate recruiting process. DXC does not make offers of employment via social media networks and DXC never asks for any money or payments from applicants at any point in the recruitment process, nor ask a job seeker to purchase IT or other equipment on our behalf. More information on employment scams is available here .",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Change management', 'Usage', 'development testing', 'Social media', 'devops', 'big data', 'Analytics', 'SQL', 'Testing', 'Recruitment']",2025-06-11 05:32:54
"Data Architect (Exp in Azure Databricks, Pyspark, SQL)",Adecco India,10 - 15 years,30-40 Lacs P.A.,"['Pune', 'Bengaluru']","Job Role & responsibilities:-\n\nUnderstanding operational needs by collaborating with specialized teams\nSupporting key business operations. This involves architecture designing, building and deploying data systems, pipelines etc\nDesigning and implementing agile, scalable, and cost efficiency solution on cloud data services.\nLead a team of developers, implement Sprint planning and executions to ensure timely deliveries\n\nTechnical Skill, Qualification & experience required:-\n\n9-11 years of experience in Cloud Data Engineering.\nExperience in Azure Cloud Data Engineering, Azure Databricks, datafactory , Pyspark, SQL,Python\nHands on experience in Data Engineer, Azure Databricks, Data factory, Pyspark, SQL\nProficient in Cloud Services Azure\nArchitect and implement ETL and data movement solutions.\nBachelors/Master's Degree in Computer Science or related field\nDesign and implement data solutions using medallion architecture, ensuring effective organization and flow of data through bronze, silver, and gold layers.\nOptimize data storage and processing strategies to enhance performance and data accessibility across various stages of the medallion architecture.\nCollaborate with data engineers and analysts to define data access patterns and establish efficient data pipelines.\nDevelop and oversee data flow strategies to ensure seamless data movement and transformation across different environments and stages of the data lifecycle.\nMigrate data from traditional database systems to Cloud environment\nStrong hands-on experience for working with Streaming dataset\nBuilding Complex Notebook in Databricks to achieve business Transformations.\nHands-on Expertise in Data Refinement using Pyspark and Spark SQL\nFamiliarity with building dataset using Scala.\nFamiliarity with tools such as Jira and GitHub\nExperience leading agile scrum, sprint planning and review sessions\nGood communication and interpersonal skills\n\n* Immediate Joiners will be preferred only",Industry Type: Insurance,Department: Other,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Azure data Architect', 'Data Architect', 'Azure Databricks', 'Data Architecture', 'Data Bricks', 'SQL', 'Python']",2025-06-11 05:32:56
Software Development Engineer,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Project Role :Software Development Engineer\n\n\n\nProject Role Description :Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.\n\nMust have skills :SAP Plant Maintenance (PM)\n\n\nGood to have skills :NAMinimum\n\n3 year(s) of experience is required\n\n\nEducational Qualification :15 years full time education\nSummary:As a Software Development Engineer, you will engage in a dynamic work environment where you will analyze, design, code, and test various components of application code across multiple clients. Your day will involve collaborating with team members to perform maintenance and enhancements, ensuring that the applications meet the evolving needs of users while adhering to best practices in software development.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with cross-functional teams to gather requirements and translate them into technical specifications.- Conduct thorough testing and debugging of application components to ensure high-quality deliverables.\nProfessional & Technical\n\n\nSkills:\n- Must To Have\n\n\nSkills:\nProficiency in SAP Plant Maintenance (PM).- Strong understanding of software development life cycle methodologies.- Experience with application coding and testing frameworks.- Familiarity with database management and data integration techniques.- Ability to troubleshoot and resolve technical issues efficiently.\nAdditional Information:- The candidate should have minimum 3 years of experience in SAP Plant Maintenance (PM).- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sap', 'software development', 'software development life cycle', 'database management', 'sap plant maintenance', 'c++', 'python', 'c', 'sap implementation', 'sql', 'java', 'debugging', 'technical specifications', 'mysql', 'html', 'sap quality management', 'sap pm', 'sap hana', 'data integration', 'sap qm']",2025-06-11 05:32:58
Data Scientist - ML,Client Of EDGE,8 - 13 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Delhi / NCR']","Our client is a India's marquee global technology company. They are an international flag-bearer of technical and managerial excellence. With offices around the globe, the company has a comprehensive presence across multiple segments of the IT product and service industries.\nWe are Seeking to identify a Data Scientist Engineer role, responsible for Leading the design, development, and deployment of advanced machine learning models and algorithms. Software engineering experience with Strong knowledge of machine learning frameworks (e.g., TensorFlow, PyTorch) and generative AI libraries., Understanding of C++ programming principles.\nLead the design, development, and deployment of advanced machine learning models and algorithms.\nTrain and fine-tune generative models on large datasets, optimizing model performance and efficiency.\nExperience of developing and deploying solutions on Nvidia or Intel software stacks will be an added advantage.\nExcellent problem-solving skills and the ability to work on complex, unstructured challenges.\nProficient Understanding of distributed Computing Principles. Working knowledge of frameworks such as accelerate, deepspeed, etc.\nExperience with hiring, mentoring and leading teams of ML engineers, data engineers, etc.\nConduct in-depth data analysis, feature engineering, and data pre-processing to extract meaningful insights.\nDevelop and execute data strategies to collect, process, and store data effectively.\nWork closely with data engineers and architects to ensure data availability and quality.\nCollaborate with cross-functional teams to develop AI-powered solutions that address business challenges and opportunities.\nEnsure the successful integration of AI models into production systems.\nStay up-to-date with the latest trends and advancements in data science and AI.\nDrive research initiatives to explore and implement innovative techniques and technologies.\nLead research initiatives to develop novel AI techniques and technologies.\nCommunicate findings, insights, and project progress to non-technical stakeholders in a clear and understandable manner.\nPublications or contributions to the field of CV, NLP or generative AI are a plus.\nYour Profile:\nAn Engineer with 5+ years of experience in Leading the design, development, and deployment of advanced machine learning models and algorithms.\nUnderstanding of C++ programming.\nStrong knowledge of machine learning frameworks (e.g., TensorFlow, PyTorch) and generative AI libraries.",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['C++', 'Machine Learning', 'Data Scientist', 'AI', 'Development', 'Deployment']",2025-06-11 05:32:59
Data Scientist(0217),A Reputed Organization,5 - 10 years,Not Disclosed,"['Kolkata', 'Pune', 'Bengaluru']","Lead the design, development, and deployment of advanced machine learning models and algorithms.\nTrain and fine-tune generative models on large datasets, optimizing model performance and efficiency.\nStrong knowledge of machine learning frameworks (e.g., TensorFlow, PyTorch) and generative AI libraries., Understanding of C++ programming principles.\nExperience of developing and deploying solutions on Nvidia or Intel software stacks will be an added advantage.\nExcellent problem-solving skills and the ability to work on complex, unstructured challenges.\nProficient Understanding of distributed Computing Principles. Working knowledge of frameworks such as accelerate, deepspeed, etc.\nExperience with hiring, mentoring and leading teams of ML engineers, data engineers, etc.\nConduct in-depth data analysis, feature engineering, and data preprocessing to extract meaningful insights.\nDevelop and execute data strategies to collect, process, and store data effectively.\nWork closely with data engineers and architects to ensure data availability and quality.\nCollaborate with cross-functional teams to develop AI-powered solutions that address business challenges and opportunities.\nEnsure the successful integration of AI models into production systems.\nStay up-to-date with the latest trends and advancements in data science and AI.\nDrive research initiatives to explore and implement innovative techniques and technologies.\nLead research initiatives to develop novel AI techniques and technologies.\nCommunicate findings, insights, and project progress to non-technical stakeholders in a clear and understandable manner.\nPublications or contributions to the field of CV, NLP or generative AI are a plus.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'C++', 'Data Scientist', 'AI']",2025-06-11 05:33:01
Site Reliability Engineer,Apple,5 - 10 years,Not Disclosed,['Bengaluru'],"The Service Reliability Engineer (SRE) role in Apple Services Engineering requires a mix of strategic engineering and design along with hands-on, technical work. This SRE will configure, tune, and fix multi-tiered systems to achieve optimal application performance, stability and availability.We manage jobs as well as applications on bare-metal and cloud computing platforms to deliver data processing for many of Apple s global products. Our teams work with exabytes of data, petabytes of memory, and tens of thousands of jobs to enable predicable and performant data analytics enabling features in Apple Music, TV+, Appstore and other world class products.If you love designing, running systems that will impact millions of users, then this is the place for you!- Support Java-based applications & Spark/Flink jobs on Baremetal, AWS & Kubernetes- Ability to understand the application requirements (Performance, Security, Scalability, etc.) and assess the right services/topology on AWS, Baremetal & Kubernetes- Build automation to enable self-healing systems- Build tools to monitor high performance & alert the low-latency applications- Ability to troubleshoot application-specific, core network, system & performance issues.- Involvement in challenging and fast paced projects supporting Apples business by delivering innovative solutions.- Monitor production, staging, test and development environments for a myriad of applications in an agile and dynamic organisation.\nBS degree in computer science or equivalent field with 5+ years or MS degree with 3+ years experience, or equivalent.\nAt least 5 years in a Site Reliability Engineering (SRE), DevOps role\n5+ years of running services in a large-scale *nix environment\nUnderstanding of SRE principles and goals along with prior on-call experience\nExtensive experience in managing applications on AWS & Kubernetes\nDeep understanding and experience in one or more of the following - Hadoop, Spark, Flink, Kubernetes, AWS\nPreferred Qualifications\nFast learner with excellent analytical problem solving and interpersonal skills\nExperience supporting Java applications\nExperience with Big Data Technologies\nExperience working with geographically distributed teams and implementing high level projects and migrations\nStrong communication skills and ability to deliver results on time with high quality",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Cloud computing', 'Interpersonal skills', 'spark', 'Analytical', 'Agile', 'Data processing', 'core network', 'big data', 'AWS']",2025-06-11 05:33:02
"Manager, Data & Analytics - Financial Services",RSM US in India,5 - 10 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","RSM is looking for an experienced Hands-On Technical Manager with expertise in big data technologies and multi-cloud platforms to lead our technical team for the financial services industry. The ideal candidate will possess a strong background in big data architecture, cloud computing, and a deep understanding of the financial services industry. As a Technical Manager, you will be responsible for leading technical projects, hands-on development, delivery management, sales and ensuring the successful implementation of data solutions across multiple cloud platforms. This role requires a unique blend of technical proficiency, sales acumen, and presales experience to drive business growth and deliver innovative data solutions to our clients.\nResponsibilities:\nProvide technical expertise and guidance on the selection, and hands-on implementation, and optimization of big data platforms, tools, and technologies across multiple cloud environments (e.g., AWS, Azure, GCP, Snowflake, etc.)\nArchitect and build scalable and secure data pipelines, data lakes, and data warehouses to support the storage, processing, and analysis of large volumes of structured and unstructured data.\nLead and mentor a team of technical professionals in the design, development, and implementation of big data solutions and data analytics projects within the financial services domain.\nStay abreast of emerging trends, technologies, and industry developments in big data, cloud computing, and financial services, and assess their potential impact on the organization.\nDevelop and maintain best practices, standards, and guidelines for data management, data governance, and data security in alignment with regulatory requirements and industry standards.\nCollaborate with the sales and business development teams to identify customer needs, develop solution proposals, and present technical demonstrations and presentations to prospective clients.\nCollaborate with cross-functional teams including data scientists, engineers, business analysts, and stakeholders to define project requirements, objectives, and timelines.\nBasic Qualifications:\nBachelor's degree or higher in Computer Science, Information Technology, Business Administration, Engineering or related field.\nMinimum of ten years of overall technical experience in solution architecture, design, hands-on development with a focus on big data technologies, multi-cloud platforms, and with at-least 5 years of experience specifically in financial services.\nStrong understanding of the financial services industry - capital markets, retail and business banking, asset management, insurance, etc.\nIn-depth knowledge of big data technologies such as Hadoop, Spark, Kafka, and cloud platforms such as AWS, Azure, GCP, Snowflake, Databricks, etc.\nExperience with SQL, Python, Pyspark or other programming languages used for data transformation, analysis, and automation.\nExcellent communication, presentation, and interpersonal skills, with the ability to articulate technical concepts to both technical and non-technical audiences.\nHands-on experience extracting (ETL using CDC, Transaction Logs, Incremental) and processing large data sets for Streaming and Batch data loads.\nAbility to work from our Hyderabad, India office at least twice a week\nPreferred Qualifications:\nProfessional certifications in cloud computing (e.g., AWS Certified Solutions Architect, Microsoft Certified Azure Solutions Architect, Azure Data Engineer, SnowPro Core) and/or big data technologies.\nExperience with Power BI, Tableau or other Reporting and Data Visualization tools\nFamiliarity with DevOps practices, CI/CD pipelines, and infrastructure-as-code tools\nEducation/Experience:\nBachelor s degree in MIS, CS, Engineering or equivalent field.\nMaster s degree is CS or MBA is preferred.\nAdvanced Data and Cloud Certifications are a plus.",Industry Type: Banking,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Financial Services', 'Azure', 'CI/CD pipelines', 'GCP', 'DevOps practices', 'Snowflake', 'Databricks', 'AWS']",2025-06-11 05:33:04
"Director, Software Engineering/Architecture","NTT DATA, Inc.",10 - 15 years,Not Disclosed,['Bengaluru'],"Additional Career Level Description:\nKnowledge and application:\nUses extensive knowledge across functional areas to direct the application of existing policies and principles and guide the development of new policies and ideas across the function.\nLeads, integrates and directs work applying substantial practical expertise across function disciplines.\nProblem solving:\nSolutions are devised based on limited information and issues that are occasionally complex and fundamental principles and data may be in conflict.\nNew concepts and solutions consider multiple perspectives and future implications.\nInteraction:\nInteracts with senior management, executives, and/or major customers which frequently involves negotiating matters of significance to the organization.\nReconciles multiple stakeholder views to drive business results.\nImpact:\nWorks with senior management to establish strategic plans and translates business segment strategy into functional plans and guides execution.\nErroneous decisions will have a critical long term (typically up to five years) impact on the overall success of function or multi departments.\nAccountability:\nAccountable for results which impact function or multiple departments including budgets.\nDirect management of a team of professional managers and experienced individual contributors.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Software Engineering', 'python', 'c++', 'project management', 'java', 'software development', 'c', 'data structures', 'big data', 'aws', 'sql']",2025-06-11 05:33:05
Consultant - Data Governance (Immediate joiner),A leading Insurance Consulting Client,4 - 7 years,Not Disclosed,"['Pune', 'Bengaluru']","We are hiring for a leading Insurance Consulting organisation for Data Strategy & Governance role\n\nExp: 4 to 6 yyrs\n\nLocation : BangalorePune\n\nResponsibility:\nDevelop and Drive Data Capability Maturity Assessment, Data & Analytics Operating Model & Data Governance exercises for clients\nManaging Critical Data Elements (CDEs) and coordinating with Finance Data Stewards\nOverseeing data quality standards and governance implementation\nEstablish processes around effective data management ensuring Data Quality & Governance standards as well as roles for Data Stewards",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Governance', 'Data Transformation', 'Data Strategy', 'Metadata Management', 'Data Lineage', 'Data Engineering', 'Data Management', 'Data Quality Management', 'Data Integration', 'Data Quality', 'Data Pipeline', 'Data Stewardship', 'Master Data Management', 'Data Architecture']",2025-06-11 05:33:07
Data Analyst-Having Stratup-Mid-Size companies Exp.@ Bangalore_Urgent,"A leader in this space, we deliver world...",8 - 13 years,Not Disclosed,['Bengaluru'],"Data Analyst\n\nLocation: Bangalore\nExperience: 8 - 15 Yrs\nType: Full-time\n\nRole Overview\n\nWe are seeking a skilled Data Analyst to support our platform powering operational intelligence across airports and similar sectors. The ideal candidate will have experience working with time-series datasets and operational information to uncover trends, anomalies, and actionable insights. This role will work closely with data engineers, ML teams, and domain experts to turn raw data into meaningful intelligence for business and operations stakeholders.\n\nKey Responsibilities\n\nAnalyze time-series and sensor data from various sources\nDevelop and maintain dashboards, reports, and visualizations to communicate key metrics and trends.\nCorrelate data from multiple systems (vision, weather, flight schedules, etc) to provide holistic insights.\nCollaborate with AI/ML teams to support model validation and interpret AI-driven alerts (e.g., anomalies, intrusion detection).\nPrepare and clean datasets for analysis and modeling; ensure data quality and consistency.\nWork with stakeholders to understand reporting needs and deliver business-oriented outputs.\n\n\nQualifications & Required Skills\n\nBachelors or Masters degree in Data Science, Statistics, Computer Science, Engineering, or a related field.\n5+ years of experience in a data analyst role, ideally in a technical/industrial domain.\nStrong SQL skills and proficiency with BI/reporting tools (e.g., Power BI, Tableau, Grafana).\nHands-on experience analyzing structured and semi-structured data (JSON, CSV, time-series).\nProficiency in Python or R for data manipulation and exploratory analysis.\nUnderstanding of time-series databases or streaming data (e.g., InfluxDB, Kafka, Kinesis).\nSolid grasp of statistical analysis and anomaly detection methods.\nExperience working with data from industrial systems or large-scale physical infrastructure.\n\n\nGood-to-Have Skills\n\nDomain experience in airports, smart infrastructure, transportation, or logistics.\nFamiliarity with data platforms (Snowflake, BigQuery, Custom-built using open-source).\nExposure to tools like Airflow, Jupyter Notebooks and data quality frameworks.\nBasic understanding of AI/ML workflows and data preparation requirements.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Kafka', 'SQL', 'airports', 'InfluxDB', 'Airflow', 'structured Data', 'time-series', 'JSON', 'Tableau', 'Grafana', 'R', 'AI/ML', 'Kinesis', 'Snowflake', 'time-series databases', 'Data Preparation', 'Python', 'smart infrastructure', 'BigQuery', 'streaming data', 'Power BI', 'CSV', 'transportation', 'logistic', 'reporting tools']",2025-06-11 05:33:09
Data Architect,Coforge,11 - 16 years,Not Disclosed,"['Noida', 'Greater Noida', 'Delhi / NCR']","-Data Architect Department:\nData & Analytics The Data Architect having more than 14 years of experience and should play a pivotal role in designing, developing, and governing scalable data architectures to support enterprise-wide data integration, analytics, and reporting.\nThis role will focus on creating unified data models, optimizing data pipelines, and ensuring compliance with regulatory standards (GDPR) using cloud-based platforms.\nThe ideal candidate is a strategic thinker with deep expertise in data modeling, cloud data platforms, and governance.",,,,"['Data Migration', 'Data Warehousing', 'Data Modeling', 'Informatica', 'SSIS', 'ETL Tool']",2025-06-11 05:33:10
QA DBT Engineer,DXC Technology,5 - 10 years,Not Disclosed,['Bengaluru'],"Minimum of 5 years experience as a software tester with proven experience in defining and leading QA cycles\nStrong experience with DBT (Data Build Tool) and writing/validating SQL models.\nHands-on experience with Collibra for metadata management and data governance validation.\nSolid understanding of data warehousing concepts and ETL/ELT processes.\nProficiency in SQL for data validation and transformation testing.\nFamiliarity with version control tools like Git.\nUnderstanding of data governance, metadata, and data quality principles.\nStrong analytical and problem-solving skills with attention to detail.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['QA', 'Data validation', 'metadata', 'Version control', 'GIT', 'Analytical', 'Social media', 'data governance', 'Data quality', 'SQL']",2025-06-11 05:33:12
Data Modelling and Data Visualization Specialist (Power BI),Krish Services Group,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Description Data Modelling and Data Visualization Specialist (Power BI)\n\n\nCompany Description - Krish is committed to enabling customers to achieve their technological goals by delivering solutions that combine the right technology, people, and costs. Our approach emphasizes building long-term relationships while ensuring customer success through tailored solutions, leveraging the expertise and integrity of our consultants and robust delivery processes.",,,,"['Power Bi', 'SSRS', 'SSIS', 'SQL Azure', 'Azure Data Factory', 'Azure Databricks', 'SQL Server', 'Tableau', 'Power Query', 'SQL', 'Onpremise', 'Azure Data Lake', 'Data Visualization', 'Dax', 'Data Modeling', 'ETL']",2025-06-11 05:33:14
Lead Engineering - ML || Bharti Airtel || gurgaon,Airtel,5 - 10 years,Not Disclosed,['Gurugram'],"Engineer ML\nAbout the role\nThe mission of the team is to transform terabytes of data into robust models for improving Network Experience and Customer Experience and reducing Operational Expenditure.\nCandidate will be required to understand the different Datasets being supplied from different sources and design a production grade big data architecture for Reporting and Analytics Platform. He will be involved in developing in-house algorithms as per Business Requirements. He will also be involved in vendor reviews of solution designs and related code on the principle of adherence to high-quality development principles while delivering solutions on time and on budget.\nKey Responsibilities\nDevelop best quality software design and Big Data architecture for a Product that is being migrated to Big Data Platform from MYSQL based architecture.\nThis product is spread across dimensions of big data, analytics, machine learning, graph problems, etc.\nThe person is responsible to design and develop systems that use machine learning, or AI, to build models that can automate processes. They use data to create models, perform statistical analysis, and train systems to improve performance.\nIdentify, prioritize and execute tasks in the software development life cycle.\nDevelop tools and applications by producing clean, efficient code, automate tasks through appropriate tools and scripting.\nReview and debug code and perform validation and verification testing\nCollaborate with internal teams and external vendors to fix and improve current Architecture that helps to improve software development process and team productivity.\nDocument development phases and monitor systems. Guide daily DevOps and ensure timely hardware expansions or architecture revamp.\nProvide statistical analysis to develop, test, and optimize databases to their full potential.\nEnsure software is up-to-date with latest technologies.\nReview Vendor Codes and solution designs for on-time deployment.\nExperience & Skills\nMust have experience in Machine Learning and experience in machine learning frameworks like TensorFlow or PyTorch.\nKnowledge of selected programming languages (e.g. Python, JAVA, Scala Python, C++)\nIn-depth knowledge of relational databases (e.g. PostgreSQL, MySQL) and NoSQL databases (e.g. MongoDB or Hadoop)\nHands-on experience working with Big Data technologies like Hadoop, MongodB, Hive, Pig, Oozie, Map Reduce, Spark, Sqoop, Kafka, Flume, etc.\nExtensive experience in software design, development and scripting.\nExperience with real-time streaming tools like Apache Kafka and NiFi is a plus.\nHas hands-on experience working with large volumes of data including different patterns of data ingestion, processing (batch & real-time), movement, storage and access (for both internal and external to BU) and is able to make independent decisions within scope of project. ( Spark, Kafka, Kstream/Flink)\nVery strong analytical skills with the demonstrated ability to research & make decisions based on the day-to-day and complex customer problems\nUnderstanding of HTTP and RESTful web services is a plus\nExperience with version control tools (SVN or Git).\nEducational Qualifications: Bachelor's or Master's Degree in Computer Science or related field; or equivalent related professional experience\nWork Experience: 1-7 years of total experience preferably in Big Data",Industry Type: Telecom / ISP,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Data Science', 'Python', 'Pyspark', 'Artificial Intelligence', 'Big Data', 'Hypothesis Testing', 'Deep Learning', 'SQL', 'Pytorch', 'Machine Learning Algorithms', 'TensorFlow', 'ML']",2025-06-11 05:33:15
"Lead Data Scientist, Operations || Mumbai || Max 38 LPA",Argus India Price Reporting Services,5 - 10 years,20-35 Lacs P.A.,"['Mumbai Suburban', 'Navi Mumbai', 'Mumbai (All Areas)']","Lead Data Scientist, Operations\nMumbai, India\n\nAbout Argus:\n\nArgus is the leading independent provider of market intelligence to the global energy and commodity markets. We offer essential price assessments, news, analytics, consulting services, data science tools and industry conferences to illuminate complex and opaque commodity markets.\nHeadquartered in London with 1,500 staff, Argus is an independent media organisation with 30 offices in the worlds principal commodity trading hubs.\nCompanies, trading firms and governments in 160 countries around the world trust Argus data to make decisions, analyse situations, manage risk, facilitate trading and for long-term planning. Argus prices are used as trusted benchmarks around the world for pricing transportation, commodities and energy.\nFounded in 1970, Argus remains a privately held UK-registered company owned by employee shareholders and global growth equity firm General Atlantic.\nWhat were looking for:\nJoin our Generative AI team to lead a new group in India, focused on creating and maintaining AI-ready data. As the point of contact in Mumbai, you will guide the local team and ensure seamless collaboration with our global counterparts. Your contributions will directly impact the development of innovative solutions used by industry leaders worldwide, supporting text and numerical data extraction, curation, and metadata enhancements to accelerate development and ensure rapid response times. You will play a pivotal role in transforming how our data are seamlessly integrated with AI systems, paving the way for the next generation of customer interactions.\n\nWhat will you be doing:\n\nLead and Develop the Team: Oversee a team of data scientists in Mumbai. Mentoring and guiding junior team members, fostering their professional growth and development.\nStrategic Planning: Develop and implement strategic plans for data science projects, ensuring alignment with the company's goals and objectives.\nAI-Ready Data Development: Design, develop, and maintain high-quality AI-ready datasets, ensuring data integrity, usability, and scalability to support advanced Generative AI models.\nAdvanced Data Processing: Drive hands-on efforts in complex data extraction, cleansing, and curation for diverse text and numerical datasets. Implement sophisticated metadata enrichment strategies to enhance data utility and accessibility for AI systems.\nAlgorithm Implementation & Optimization: Implement and optimize state-of-the-art algorithms and pipelines for efficient data processing, feature engineering, and data transformation tailored for LLM and GenAI applications.\nGenAI Application Development: Apply and integrate frameworks like LangChain and Hugging Face Transformers to build modular, scalable, and robust Generative AI data pipelines and applications.\nPrompt Engineering Application: Apply advanced prompt engineering techniques to optimize LLM performance for specific data extraction, summarization, and generation tasks, working closely with the Lead's guidance.\nLLM Evaluation Support: Contribute to the systematic evaluation of Large Language Models (LLMs) outputs, analysing quality, relevance, and accuracy, and supporting the implementation of LLM-as-a-judge frameworks.\nRetrieval-Augmented Generation (RAG) Contribution: Actively contribute to the implementation and optimization of RAG systems, including working with embedding models, vector databases, and, where applicable, knowledge graphs, to enhance data retrieval for GenAI.\nTechnical Leadership: Act as a technical leader and subject matter expert for junior data scientists, providing guidance on best practices in coding and PR reviews, data handling, and GenAI methodologies.\nCross-Functional Collaboration: Collaborate effectively with global data science teams, engineering, and product stakeholders to integrate data solutions and ensure alignment with broader company objectives.\nOperational Excellence: Troubleshoot and resolve data-related issues promptly to minimize potential disruptions, ensuring high operational efficiency and responsiveness.\nDocumentation & Code Quality: Produce clean, well-documented, production-grade code, adhering to best practices for version control and software engineering.\n\nSkills and Experience:\n\nLeadership Experience: Proven track record in leading and mentoring data science teams, with a focus on strategic planning and operational excellence.\nAcademic Background: Advanced degree in AI, statistics, mathematics, computer science, or a related field.\nProgramming and Frameworks: 5+ years of hands-on experience with Python, TensorFlow or PyTorch, and NLP libraries such as spaCy and Hugging Face.\nGenAI Tools: 2+ years of Practical experience with LangChain, Hugging Face Transformers, and embedding models for building GenAI applications.\nPrompt Engineering: Deep expertise in prompt engineering, including prompt tuning, chaining, and optimization techniques.\nLLM Evaluation: Experience evaluating LLM outputs, including using LLM-as-a-judge methodologies to assess quality and alignment.\nRAG and Knowledge Graphs: Practical understanding and experience using vector databases. In addition, familiarity with graph-based RAG architectures and the use of knowledge graphs to enhance retrieval and reasoning would be a strong plus.\nCloud: 2+ years of experience with Gemini/OpenAI models and cloud platforms such as AWS, Google Cloud, or Azure. Proficient with Docker for containerization.\nData Engineering: Strong understanding of data extraction, curation, metadata enrichment, and AI-ready dataset creation.\nCollaboration and Communication: Excellent communication skills and a collaborative mindset, with experience working across global teams.\n\nWhats in it for you:\n\nCompetitive salary\nHybrid Working Policy (3 days in Mumbai office/ 2 days WFH once fully inducted)\nGroup healthcare scheme\n18 days annual leave\n8 days of casual leave\nExtensive internal and external training\n\nHours:\n\nThis is a full-time position operating under a hybrid model, with three days in the office and up to two days working remotely.\nThe team supports Argus key business processes every day, as such you will be required to work on a shift-based rota with other members of the team supporting the business until 8pm. Typically support hours run from 11am to 8pm with each member of the team participating up to 2/3 times a week.\n\nFor more details about the company and to apply please make sure you send your CV and cover letter via our website: www.argusmedia.com/en/careers/open-positions\nBy submitting your job application, you automatically acknowledge and consent to the collection, use and/or disclosure of your personal data to the Company. Argus is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, sexual orientation, gender identity, disability or veteran status.",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Huggingface', 'Langchain', 'Spacy', 'Python', 'TensorFlow', 'Pytorch']",2025-06-11 05:33:17
Principal Architect-Data & Cloud,Tothr,12 - 18 years,40-45 Lacs P.A.,"['Chennai', 'Bengaluru']","Experience in Hadoop, GCP/AWS/Azure Cloud\nETL technologies on Cloud like Spark, Pyspark/Scala, Dataflow\nETL tools like Informatica/DataStage/OWB/Talend\nExperience inS3, Cloud Storage, Athena, Glue, Sqoop, Flume, Hive, Kafka, Pub-Sub\n\nRequired Candidate profile\nMore than 10 years of experience in Technical, Solutioning, and Analytical roles.\n5+ years of experience in building and managing Data Lakes, Data Warehouse, Data Integration,",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Cloud', 'Technical', 'Analytical', 'MongoDB', 'data lake', 'Pyspark', 'Data Migration', 'Data Warehousing', 'Data Integration']",2025-06-11 05:33:19
Senior Data Operations Eng. For Product Based MNC-Pune,A client of Seventh contact,5 - 9 years,22.5-25 Lacs P.A.,[],"Monitor and maintain data pipelines and ETL processes to ensure reliability and performance.\nAutomate routine data operations tasks and optimize workflows for scalability and efficiency.\n\nRequired Candidate profile\nData operations, data engineering, or a related role.\nStrong SQL skills and experience with relational databases (e.g., PostgreSQL, MySQL).\nProficiency with data pipeline tools (e.g., Apache Airflow",Industry Type: Software Product,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Database Queries', 'SQL Queries', 'SQL']",2025-06-11 05:33:20
TS - Data Architect & Database Expert,IT Company,7 - 12 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","DATA ARCHITECT- Data Architecture, Big Data, Data Modeling, or Database Administration, any DBMS,Oracle/SQL Server/PostgreSQL/MySQL\n\nDatabase Expert-Database Mgmt, SQL, Data Modeling, Data Warehousing, ETL, any DBMS Oracle/SQL Server/PostgreSQL/MySQL",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Database Administration', 'Data Architect', 'Database Management', 'Data Modeling', 'Data Architecture', 'Postgres Database', 'Postgresql', 'Big Data', 'Data Warehousing', 'Oracle', 'SQL', 'Data Warehousing Concepts']",2025-06-11 05:33:22
Opening For Senior Machine Learning Engineer with Fareportal,Fareportal,2 - 5 years,Not Disclosed,['Gurugram'],"Title: Senior Machine Learning Engineer\nLocation: Gurgaon, IN\nType: (Hybrid, In-Office)\nJob Description\n\nWho We Are:\nFareportal is a travel technology company powering a next-generation travel concierge service. Utilizing its innovative technology and company owned and operated global contact centres, Fareportal has built strong industry partnerships providing customers access to over 500 airlines, a million lodgings, and hundreds of car rental companies around the globe. With a portfolio of consumer travel brands including CheapOair and OneTravel, Fareportal enables consumers to book-online, on mobile apps for iOS and Android, by phone, or live chat. Fareportal provides its airline partners with access to a broad customer base that books high-yielding international travel and add-on ancillaries.",,,,"['Containerization', 'Machine Learning', 'Python', 'azure', 'sql', 'api', 'nosql', 'deployment']",2025-06-11 05:33:23
Data Lead,Multiverse Solutions,8 - 10 years,10-20 Lacs P.A.,[],"Job Summary:\nWe are seeking an experienced and hands-on Data Lead with deep expertise in Microsoft Azure Data Analytics ecosystem. The ideal candidate will lead the design, development, and implementation of scalable data pipelines and analytics solutions using Azure Data Factory (ADF), Synapse Analytics, Microsoft Fabric, Apache Spark, and modern data modeling techniques. A strong grasp of CDC mechanisms, performance tuning, and cloud-native architecture is essential.\nKey Responsibilities:\nLead the architecture and implementation of scalable data integration and analytics solutions in Azure.\nDesign and build end-to-end data pipelines using ADF, Azure Synapse Analytics, Azure Data Lake, and Microsoft Fabric.\nImplement and manage large-scale data processing using Apache Spark within Synapse or Fabric.\nDevelop and maintain data models using Star and Snowflake schema for optimal reporting and analytics performance.\nImplement Change Data Capture (CDC) strategies to ensure near real-time or incremental data processing.\nCollaborate with stakeholders to translate business requirements into technical data solutions.\nManage and mentor a team of data engineers and analysts.\nMonitor, troubleshoot, and optimize performance of data workflows and queries.\nEnsure best practices in data governance, security, lineage, and documentation.\nStay updated with the latest developments in the Azure data ecosystem and recommend enhancements.\nRequired Skills and Qualifications:\n810 years of overall experience in data engineering and analytics, with at least 3+ years in a data lead role.\nStrong expertise in Azure Data Factory, Azure Synapse, and Microsoft Fabric.\nHands-on experience with Apache Spark for large-scale data processing.\nProficient in SQL, Python, or PySpark for data transformation and automation.\nSolid experience with CDC patterns (e.g., using ADF, or SQL-based approaches).\nIn-depth understanding of data warehousing concepts and data modeling (Star, Snowflake).\nKnowledge of Power BI integration with Synapse/Fabric is a plus.\nFamiliarity with DevOps for data pipelines, version control (Git), and CI/CD for data solutions.\nStrong problem-solving skills and ability to lead architecture discussions and POCs.\nExcellent communication and stakeholder management skills.\nPreferred Qualifications:\nMicrosoft Certifications in Azure Data Engineering or Analytics.\nExperience with Delta Lake, Databricks, or Snowflake (as source/target).\nKnowledge of data privacy and compliance standards like GDPR, HIPAA.\nWhat We Offer:\nOpportunity to lead strategic data initiatives on the latest Azure stack.\nA dynamic and collaborative work environment.\nAccess to continuous learning, certifications, and upskilling programs.\nCompetitive compensation and benefits package.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Synapse', 'Cdc', 'Architecture', 'Microsoft Fabric', 'Pyspark', 'Azure Data Lake', 'Data Lake', 'Data Modeling', 'Data Bricks', 'SQL', 'Python']",2025-06-11 05:33:25
"Senior Manager, Data Science",Oportun,7 - 12 years,Not Disclosed,[],"We are growing our world-class team of mission-driven, entrepreneurial Data Scientists who are passionate about broadening financial inclusion by untapping insights from non-traditional data. Be part of the team responsible for developing and enhancing Oportuns core intellectual property used in scoring risk for underbanked consumers that lack a traditional credit bureau score. In this role you will be on the cutting edge working with large and diverse (i.e. data from dozens of sources including transactional, mobile, utility, and other financial services) alternative data sets and utilize machine learning and statistical modeling to build scores and strategies for managing risk, collection/loss mitigation, take-up rates and fraud. You will also drive growth and optimize marketing spend across channels by leveraging alternative data to help predict which consumers would likely be interested in Oportuns affordable, credit building loan product.\n\nRESPONSIBILITIES:\nDevelop data products and machine learning models used in Risk, Fraud, Collections, and portfolio management, and provide frictionless customer experience for various products and services Oportun provides.\nBuild accurate and automated monitoring tools which can help us to keep a close eye on the performance of the models and rules.\nBuild model deployment platform which can shorten the time of implementing new models.\nBuild end-to-end reusable pipelines from data acquisition to model output delivery.\nLead initiatives to drive business value from start to finish including project planning,\ncommunication, and stakeholder management.\nLead discussions with Compliance, Bank Partners, and Model Risk Management teams to\nfacilitate the Model Governance Activities such as Model Validations and Monitoring.\nLead, coach and partner with the DS and non-DS team to deliver results\nQUALIFICATIONS:\nA relentless problem solver and out of the box thinker with a proven track record of driving business results in a timely manner\nMasters degree or PhD in Statistics, Mathematics, Computer Science, Engineering or Economics or other quantitative discipline\nHands on experience leveraging machine learning techniques such as Gradient Boosting, Logistic Regression and Neural Network to solve real world problems\n7+ years of hands-on experience with data extraction, cleaning, analysis and building reusable data pipelines; Proficient in SQL, Spark SQL and/or Hive\n7+ years of experience in leveraging modern machine learning toolset and programming languages such as Python\nExcellent written and oral communication skills\nStrong stakeholder management and project management skills\nComfortable in a high-growth, fast-paced, agile environment\nExperience working with AWS EMR, Sage-maker or other cloud-based platforms is a plus\nExperience with HDFS, Hive, Shell script and other big data tools is a plus",Industry Type: FinTech / Payments,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'ML model building', 'Credit Risk', 'sql', 'cloud']",2025-06-11 05:33:27
Senior Data Scientist,Oportun,3 - 6 years,Not Disclosed,[],"We are growing our world-class team of mission-driven, entrepreneurial Data Scientists who are passionate about broadening financial inclusion by untapping insights from non-traditional data. Be part of the team responsible for developing and enhancing Oportun's core intellectual property used in scoring risk for underbanked consumers that lack a traditional credit bureau score.\n\nIn this role you will be on the cutting edge working with large and diverse (i.e. data from dozens of sources including transactional, mobile, utility, and other financial services) alternative data sets and utilize machine learning and statistical modeling to build scores and strategies for managing risk, collection/loss mitigation, and fraud. You will also drive growth and optimize marketing spend across channels by leveraging alternative data to help predict which consumers would likely be interested in Oportuns affordable, credit building loan product.\n\nRESPONSIBILITIES\n\nDevelop data products and machine learning models used in Risk, Fraud, Collections, and portfolio management, and provide frictionless customer experience for various products and services Oportun provides.\nBuild accurate and automated monitoring tools which can help us to keep a close eye on the performance of the models and rules.\nBuild model deployment platform which can shorten the time of implementing new models.\nBuild end-to-end reusable pipelines from data acquisition to model output delivery.\nLead initiatives to drive business value from start to finish including project planning,\ncommunication, and stakeholder management.\nLead discussions with Compliance, Bank Partners, and Model Risk Management teams to\nfacilitate the Model Governance Activities such as Model Validations and Monitoring\n\n\nREQUIREMENTS\nA relentless problem solver and out of the box thinker with a proven track record of driving business results in a timely manner\nMasters degree or PhD in Statistics, Mathematics, Computer Science, Engineering or Economics or other quantitative discipline (Bachelor’s degree with significant relevant experience will be considered).\nHands on experience leveraging machine learning techniques such as Gradient Boosting, Logistic Regression and Neural Network to solve real world problems\n3+ years of hands-on experience with data extraction, cleaning, analysis and building reusable data pipelines; Proficient in SQL, Spark SQL and/or Hive\n3+ years of experience in leveraging modern machine learning toolset and programming languages such as Python\nExcellent written and oral communication skills\nStrong stakeholder management and project management skills\nComfortable in a high-growth, fast-paced, agile environment\nExperience working with AWS EMR, Sage-maker or other cloud-based platforms is a plus\nExperience with HDFS, Hive, Shell script and other big data tools is a plus",Industry Type: FinTech / Payments,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['cloud', 'ML model building', 'Statistics', 'Python', 'sql']",2025-06-11 05:33:28
SQL Server 2012/SQL Server Engineer,INTERNAL,5 - 10 years,Not Disclosed,['Bengaluru'],"Skill Name :SQL Server 2012/SQL Server Engineer (5 - 10 years)\nWork Location : Bangalore (preferred) / Any Deloitte USI location\n\nKey responsibilities:\nUnderstand customer use case, available data to prepare for automated & ongoing data\ningestion to meet customer requirements\n• Design technical solution with all data ingestion, transformation & scheduling to\nreview & sign-off with customer\n• Play role of the tech lead and responsible for end-to-end technical solutions\n• Identifying AEP enhancements, extending frameworks and incorporating new ideas\n• Closely collaborating with other AEP team members (sales teams, engineers,\nconsultants) and onshore teams for delivering projects\n• Enterprise level software development leveraging Big Data, Cloud technologies &\nPython\n• Building/operating highly scalable, fault tolerant, distributed systems for extraction,\ningestion to process large data sets\n• Experience with data analysis, modeling and mapping to coordinate closely with\nData Architect(s)\n• Build the necessary schemas, workflows to ingest customers data into AEP\nsuccessfully\n• Create necessary identity namespaces, privacy settings and merge poilicys required\nto build the solution\n• Build Audiences (segmentations) and create necessary pipeline for Destination\nactivation\n• Deploying the final solution to a production environment (or end-state\nenvironment)\n• Post-Deployment, provide ongoing maintenance & support of solution & knowledge\ntransfer to offshore support team",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Part Time, Temporary/Contractual","['AEP Architect', 'Data Architect', 'SQL Server', 'Data base', 'SQL']",2025-06-11 05:33:30
"Sustainable, Client and Regulatory Reporting Data Product Owner",Capital Markets,15 - 20 years,Not Disclosed,['Bengaluru'],"Hiring, Sustainable, Client and Regulatory Reporting Data Product Owner - ISS Data (Associate Director)\nAbout your team\n\nThe Technology function provides IT services that are integral to running an efficient run-the business operating model and providing change-driven solutions to meet outcomes that deliver on our business strategy. These include the development and support of business applications that underpin our revenue, operational, compliance, finance, legal, marketing and customer service functions. The broader organisation incorporates Infrastructure services that the firm relies on to operate on a day-to-day basis including data centre, networks, proximity services, security, voice, incident management and remediation.\nThe Technology group is responsible for providing Technology solutions to the Investment Solutions & Services business (which covers Investment Management, Asset Management Operations & Distribution business units globally)\n\nThe Technology team supports and enhances existing applications as well as designs, builds and procures new solutions to meet requirements and enable the evolving business strategy.\nAs part of this group, a dedicated Data Programme team has been mobilised as a key foundational programme to support the execution of the overarching Investment Solutions and Service strategy.\n\nAbout your role\nThe Investment Reporting Data Product Owner role is instrumental in the creation and execution of a future state data reporting product to enable Regulatory, Client, Vendor, Internal & MI reporting and analytics. The successful candidate will have an in- depth knowledge of all data domains that represent institutional clients , the investment life cycle , regulatory and client reporting data requirements.\nThe role will sit within the ISS Delivery Data Analysis chapter and fully aligned with our cross functional ISS Data Programme in Technology, and the candidate will leverage their extensive industry knowledge to build a future state platform in collaboration with Business Architecture, Data Architecture, and business stakeholders.\nThe role is to maintain strong relationships with the various business contacts to ensure a superior service to our internal business stakeholders and our clients.\n\nKey Responsibilities\n\nLeadership and Management:\nLead the ISS distribution, Client Propositions, Sustainable Investing and Regulatory reporting data outcomes defining the data roadmap and capabilities and supporting the execution and delivery of the data solutions as a Data Product lead within the ISS Data Programme.\nLine management responsibilities for junior data analysts within the chapter, coaching, influencing and motivating them for high performance.\nDefine the data product vision and strategy with end-to-end thought leadership.\nLead and define the data product backlog , documentation, enable peer-reviews, analysis effort estimation, maintain backlog, and support end to end planning.\nBe a catalyst of change for driving efficiencies, scale and innovation.\n\nData Quality and Integrity:\nDefine data quality use cases for all the required data sets and contribute to the technical frameworks of data quality.\nAlign the functional solution with the best practice data architecture & engineering.\n\nCoordination and Communication:\nSenior management level communication to influence senior tech and business stakeholders globally, get alignment on the roadmaps.\nCoordinate with internal and external teams to communicate with those impacted by data flows.\nAn advocate for the ISS Data Programme.\nCollaborate closely with Data Governance, Business Architecture, and Data owners etc.\nConduct workshops within the scrum teams and across business teams, effectively document the minutes and drive the actions.\n\nAbout you\nThe Investment Reporting Data Product Owner role is instrumental in the creation and execution of a future state data reporting product to enable Regulatory, Client, Vendor, Internal & MI reporting and analytics. The successful candidate will have an in- depth knowledge of all data domains that represent institutional clients , the investment life cycle , regulatory and client reporting data requirements.\nThe role will sit within the ISS Delivery Data Analysis chapter and fully aligned with cross functional ISS Data Programme in Technology, and the candidate will leverage their extensive industry knowledge to build a future state platform in collaboration with Business Architecture, Data Architecture, and business stakeholders.\nThe role is to maintain strong relationships with the various business contacts to ensure a superior service to our internal business stakeholders and our clients.\n\nKey Responsibilities\n\nLeadership and Management:\nLead the ISS distribution, Client Propositions, Sustainable Investing and Regulatory reporting data outcomes defining the data roadmap and capabilities and supporting the execution and delivery of the data solutions as a Data Product lead within the ISS Data Programme.\nLine management responsibilities for junior data analysts within the chapter, coaching, influencing and motivating them for high performance.\nDefine the data product vision and strategy with end-to-end thought leadership.\nLead and define the data product backlog , documentation, enable peer-reviews, analysis effort estimation, maintain backlog, and support end to end planning.\nBe a catalyst of change for driving efficiencies, scale and innovation.\n\nData Quality and Integrity:\nDefine data quality use cases for all the required data sets and contribute to the technical frameworks of data quality.\nAlign the functional solution with the best practice data architecture & engineering.\n\nCoordination and Communication:\nSenior management level communication to influence senior tech and business stakeholders globally, get alignment on the roadmaps.\nCoordinate with internal and external teams to communicate with those impacted by data flows.\nAn advocate for the ISS Data Programme.\nCollaborate closely with Data Governance, Business Architecture, and Data owners etc.\nConduct workshops within the scrum teams and across business teams, effectively document the minutes and drive the actions.\n\nYour Skills and Experience\n\nStrong leadership and senior management level communication, internal and external client management and influencing skills.\nAt least 15 years of proven experience as a senior business/technical/data analyst within technology and/or business change delivering data led business outcomes within the financial services/asset management industry.\n5-10 years as a data product owner adhering to agile methodology, delivering data solutions using industry leading data platforms such as Snowflake, State Street Alpha Data, Refinitiv Eikon, SimCorp Dimension, BlackRock Aladdin, FactSet etc.\nOutstanding knowledge of Client life cycle covering institutional & wholesale with a focus on CRM data, Transfer agency data.\nVery good understanding of the data generated by investment management processes and how that is leveraged in Go-to market capabilities such as client reporting, Sales, Marketing.\nExcellent knowledge of regulatory environment with a focus on European regulations and ESG specific ones such as MIFID II, EMIR, SFDR.\nWork effortlessly in different operating models such as insourcing, outsourcing and hybrid models.\nAutomation mindset that can drive efficiencies and quality in the reporting landscape.\nKnowledge of industry standard data calcs for fund factsheets, Institutional admin and investment reports would be an added advantage.\nIn Depth expertise in data and calculations across the investment industry covering the below.\nClient Specific data: This includes institutional and wholesale client, account and channels data, client preferences and data sets needed for client analytics. Knowledge of Salesforce desirable.\nTransfer Agency & Platform data: This includes granular client holdings at various levels, client transactions and relevant ref data. Knowledge of role of TPAs as TA and integrating external feeds/products with strategic inhouse data platforms.\nInvestment data: This includes investment life cycle data covering data domains such as trading, ABOR, IBOR, Security and fund reference.\nShould possess Problem Solving, Attention to detail, Critical thinking.\nTechnical Skills: Hands on SQL, Advanced Excel, Python, ML (optional) and knowledge of end-to-end tech solutions involving data platforms.\nKnowledge of data management, data governance, and data engineering practices\nHands on experience with data modelling techniques such as dimensional, data vault.\nWillingness to own and drive things, collaboration across business and tech stakeholders.",Industry Type: Investment Banking / Venture Capital / Private Equity,Department: Product Management,"Employment Type: Full Time, Permanent","['Data Transformation', 'ESG Framework', 'Snowflake', 'Asset Management', 'Product Owner', 'Product Manager', 'MIFID II', 'alphastate street', 'SQL', 'EMIR', 'Data Quality', 'Data Analysis', 'charles river', 'Agile', 'UK Regulatory Reporting', 'data roadmap', 'Capital Market Operations', 'Aladdin', 'SFDR.', 'Python', 'ML']",2025-06-11 05:33:31
Data Architect / Engagement Lead,Ignitho,7 - 10 years,Not Disclosed,['Chennai( Sholinganallur )'],"Job Title: Data Architect / Engagement Lead\nLocation: Chennai\nReports To: CEO\n\nAbout the Company:\nIgnitho Inc. is a leading AI and data engineering company with a global presence, including US, UK, India, and Costa Rica offices.\nVisit our website to learn more about our work and culture: www.ignitho.com.\nIgnitho is a portfolio company of Nuivio Ventures Inc., a venture builder dedicated to developing Enterprise AI product companies across various domains, including AI, Data Engineering, and IoT.\nLearn more about Nuivio at: www.nuivio.com.\n\nJob Summary:\nAs the Data Architect and Engagement Lead, you will define the data architecture strategy and lead client engagements, ensuring alignment between data solutions and business goals. This dual role blends technical leadership with client-facing responsibilities.\n\nKey Responsibilities:\nDesign scalable data architectures, including storage, processing, and integration layers.\nLead technical discovery and requirements gathering sessions with clients.\nProvide architectural oversight for data and AI solutions.\nAct as a liaison between technical teams and business stakeholders.\nDefine data governance, security, and compliance standards.\n\nRequired Qualifications:\nBachelors or Masters in computer science, Information Systems, or similar.\n7+ years of experience in data architecture, with client-facing experience.\nDeep knowledge of data modelling, cloud data platforms (Snowflake / BigQuery/ Redshift / Azure), and orchestration tools.\nExcellent communication, stakeholder management, and technical leadership skills.\nFamiliarity with AI/ML systems and their data requirements is a strong plus.",Industry Type: Emerging Technologies (AI/ML),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Aiml', 'Data Modeling', 'Azure Cloud', 'Bigquery', 'Redshift Aws', 'Artificial Intelligence', 'Snowflake', 'Machine Learning']",2025-06-11 05:33:33
Gen AI- Sr. Engineer/Lead,Iris Software,7 - 12 years,Not Disclosed,"['Pune', 'Chennai', 'Delhi / NCR']","Programming Skills: Advanced proficiency in Python, with experience in AI/ML frameworks.\nAzure DevOps: Expertise in version control systems (e.g., Git) and Azure DevOps tools for automation and monitoring.\nCI/CD Pipelines: Proven ability to design and implement continuous integration and deployment workflows.\nInfrastructure as Code: Experience with ARM templates or Terraform for provisioning cloud infrastructure.\nContainerization: Strong knowledge of Docker and Kubernetes for application deployment and orchestration.\nExperience:\n8+ years of experience in software development, with 3+ years in AI/ML or Generative AI projects.\nDemonstrated experience in deploying and managing AI applications in production environments.\nProven track record of implementing DevOps best practices and automation strategies.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Gen AI', 'Azure', 'Devops', 'Python', 'Sql']",2025-06-11 05:33:34
Lead Data Scientist - Media Mix Modelling (MMM),Blend360 India,7 - 10 years,Not Disclosed,['Hyderabad'],"We are seeking a highly skilled Data Scientist/Analyst with expertise in Media Mix Modeling (MMM) to join our dynamic analytics team. The ideal candidate will play a critical role in providing actionable insights and optimizing marketing spend across various channels by leveraging statistical models and data-driven techniques.\nKey Responsibilities:\nDevelop and implement Media Mix Models to optimize marketing spend across different channels (e.g., TV, digital, radio, print, etc.).\nAnalyze historical data to understand the impact of marketing efforts and determine the effectiveness of different media channels.\nCollaborate with marketing and business teams to translate business objectives into quantitative analyses and actionable insights.\nBuild predictive models to forecast the impact of future marketing activities and recommend budget allocation.\nPresent and communicate complex findings in a clear, concise, and actionable manner to both technical and non-technical stakeholders.\nPerform deep-dive analyses of marketing campaigns and customer data to identify trends, opportunities, and areas for improvement.\nEnsure data integrity, accuracy, and consistency in all analyses and models.\nStay up-to-date with the latest trends and advancements in media mix modeling, marketing analytics, and data science.\nCollaborate with cross-functional teams including Data Engineering, Marketing, and Business Intelligence to ensure seamless data flow and integration.\nCreate and maintain documentation for all models, methodologies, and analysis processes.\n\n\nBachelor s or Master s degree in Data Science, Statistics, Economics, Mathematics, or a related field.\nProven experience (6+ years) working in Media Mix Modeling (MMM) and/or marketing analytics.\nStrong proficiency in stati",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Analytical', 'Social media', 'Consulting', 'data integrity', 'Regression analysis', 'Business intelligence', 'CRM', 'SQL', 'Python']",2025-06-11 05:33:36
Senior Support Engineer,TIBCO,6 - 9 years,Not Disclosed,['Pune'],"Job Summary:\nWe are seeking a skilled and experienced Senior Support Engineer specializing in TIBCO Data Virtualization (TDV) to join our dynamic Customer Support team. In this role, you will serve as a key escalation point for complex technical issues, ensuring timely resolution and customer satisfaction. You will collaborate closely with cross-functional teams, including Engineering and Product Management, to improve product supportability and enhance customer experience.\nKey Responsibilities:\nTechnical Support Issue Resolution\n\nAct as the primary escalation point for complex technical issues related to TIBCO Data Virtualization (TDV).\n\nDiagnose, troubleshoot, and resolve product-related issues, ensuring minimal customer downtime.\n\nPerform in-depth root cause analysis and work with Engineering to drive long-term solutions.\n\nGuide customers in best practices for deploying, configuring, and optimizing TDV solutions.\n\nCustomer Engagement Communication\n\nProvide clear and effective technical guidance to customers, ensuring a smooth resolution process.\n\nEngage in high-priority customer interactions and incident reviews, delivering expert recommendations.\n\nCollaborate with customer teams to proactively identify and address potential issues before escalation.\n\nKnowledge Management Mentorship\n\nDocument solutions, troubleshooting steps, and best practices in the knowledge base.\n\nConduct internal training and mentorship programs to upskill junior support engineers.\n\nParticipate in technical workshops, webinars, and customer training sessions.\n\nProcess Improvement Collaboration\n\nWork closely with Product Management and Engineering to provide feedback on product improvements.\n\nContribute to automation and self-service initiatives to enhance customer experience.\n\nContinuously improve internal support processes to reduce issue resolution time.\n\nRequired Qualifications\n\nExperience: 5+ years in technical support, customer support, or escalation engineering roles.\n\nEducational Qualification: Bachelor s degree in Information Technology, Computer Science, Electronics and Telecommunication, or a related field.\n\n\nTechnical Skills:\n\n\nStrong expertise in TIBCO Data Virtualization (TDV), including configuration, troubleshooting, and optimization.\n\nSolid understanding of SQL, data modeling, and database management (Oracle, SQL Server, PostgreSQL, etc. ).\n\nKnowledge of cloud platforms (AWS, Azure, GCP) and virtualization technologies.\n\nExperience with REST APIs, authentication mechanisms, and security best practices.\n\n\n\nSoft Skills:\n\n\nExcellent problem-solving and analytical skills.\n\nStrong verbal and written communication, with the ability to simplify complex technical concepts.\n\nAbility to work independently and handle multiple high-priority cases simultaneously.\n\n\nPreferred Qualifications\n\nExperience with data integration, ETL tools, and performance tuning.\n\nFamiliarity with monitoring tools (Splunk, Datadog) for issue diagnosis.\n\nAuthentication (SSO, LDAP, AD, Kerberos, OpenID, X509, etc )\n\nAcademic knowledge or hands on experience with: RDBMS (Oracle, SQL Server, PostgreSQL)\n\nUnix/Linux\nApache Tomcat\n\nExperience with usage and administration of cloud platforms such as AWS, Azure, and GCP\n\nFamiliarity with related technologies such as Docker and Kubernetes.\n\nDemonstrated ability to handle demanding and difficult customers and their expectations.\n\nDemonstrated ability to facilitate Zoom and/or conference calls\n\nDemonstrated skill at mentoring other support staff.\n\nWhy Join Us\n\nWork in a fast-paced environment with a strong focus on innovation and customer success.\n\nGain hands-on experience with cutting-edge data virtualization technologies.\n\nEnjoy opportunities for career growth, training, and skill development.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Product management', 'Computer science', 'Automation', 'Linux', 'GCP', 'Process improvement', 'Telecommunication', 'Customer support', 'Technical support']",2025-06-11 05:33:38
Solution Architect Data Analysis - DATA Architect,Czone Software India,6 - 11 years,12-18 Lacs P.A.,['Bengaluru'],"Responsibilities include defining system architecture, integrating and ensuring compliance with industry standards.\nQualifications:\n- 8+ years in solution architecture\n- Expertise in cloud platforms, AI/ML integration, and big data processing.\n\n\nHealth insurance\nProvident fund",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['and big data processing.', 'Expertise in cloud platforms', 'AI/ML integration', 'Data Architecture']",2025-06-11 05:33:39
Sr. Software Engineer - SQL,Yash Technologies,5 - 8 years,Not Disclosed,['Pune'],"We are looking forward to hire SQL Professionals in the following areas :\n:\nWe are seeking a highly skilled Data Engineer with 5 to 8 years of experience to join our growing data team. The ideal candidate will have strong expertise in Azure technologies and advanced Python programming skills. This role involves designing, building, and optimizing data pipelines, ensuring data security, and enabling analytical capabilities across the organization.\nIn this Role, Your Responsibilities Will Be:",,,,"['Computer science', 'Manager Quality Assurance', 'Data modeling', 'Agile', 'microsoft', 'Information technology', 'Analytics', 'SQL', 'Python', 'Technical documentation']",2025-06-11 05:33:41
DevOps Engineer - Lead,Blend360 India,6 - 8 years,Not Disclosed,['Hyderabad'],"Job Description\nWe are seeking a highly skilled Lead Azure DevOps Engineer to join our team and drive the end-to-end deployment, scalability, and operationalization of machine learning models in production. You will collaborate closely with data scientists, data engineers, and DevOps teams to ensure seamless CI/CD, reproducibility, monitoring, and governance of ML pipelines\nKey Responsibilities\nDesign, implement, and maintain CI/CD pipelines for deploying and monitoring microservices efficiently.\nManage infrastructure as code using Terraform for repeatable and scalable provisioning.\nDeploy and optimize containerized applications using Docker and Azure services (Container Apps, Container Registry, Key Vault, Service Bus, Blob Storage).\nApply best practices for securing Docker images, including vulnerability scanning, reducing image size, and optimizing build efficiency.\nImplement and maintain Azure Monitor for logging, monitoring, and alerting to ensure system reliability.\nEnsure security best practices across cloud environments, including secrets management, access control, and compliance.\n(Nice to have) Design and manage multi-client architectures within shared pipelines and storage accounts in Azure Blob Storage\n\n\nQualifications\n6+ years of experience in DevOps or MLOps with a strong focus on production-grade ML solutions.\nStrong expertise in Azure, particularly with CI/CD, container orchestration, and cloud security. Profi",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'orchestration', 'cloud security', 'devops', 'Machine learning', 'Infrastructure', 'Vulnerability', 'Management', 'Monitoring', 'microservices']",2025-06-11 05:33:42
Azure Data Architect,Leading Client,10 - 15 years,Not Disclosed,"['Hyderabad', 'Coimbatore']","Azure+ SQL+ ADF+ Databricks +design+ Architecture( Mandate)\n\nTotal experience in data management area for 10 + years with Azure cloud data platform experience\n\nArchitect with Azure stack (ADLS, AALS, Azure Data Bricks, Azure Streaming Analytics Azure Data Factory, cosmos DB & Azure synapse) & mandatory expertise on Azure streaming Analytics, Data Bricks, Azure synapse, Azure cosmos DB\n\nMust have worked experience in large Azure Data platform and dealt with high volume Azure streaming Analytics\n\nExperience in designing cloud data platform architecture, designing large scale environments\n\n5 plus Years of experience architecting and building Cloud Data Lake, specifically Azure Data Analytics technologies and architecture is desired, Enterprise Analytics Solutions, and optimising real time 'big data' data pipelines, architectures and data sets.",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'azure databricks', 'architecting', 'azure data lake', 'data management', 'data analytics', 'ssas', 'azure synapse', 'microsoft azure', 'azure data factory', 'cosmos', 'azure cloud', 'sql', 'data bricks', 'ssrs', 'azure stack', 'azure stream analytics', 'azure cosmosdb', 'ssis', 'data lake', 'msbi']",2025-06-11 05:33:44
Software Engineer,Intelliswift software,3 - 8 years,Not Disclosed,['Bengaluru'],"Job Description:\nWe are looking for a skilled Palantir Foundry Developer with strong hands-on experience in data engineering using PySpark and SQL. The ideal candidate should be proficient in designing, building, and maintaining scalable data pipelines and integrating with Palantir Foundry environments.\nKey Skills:\nPalantir Foundry (Mandatory)\nPySpark,\nAdvanced SQL and Data Modelling\nData Pipeline Development and Optimization\nETL Processes, Data Transformation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Palantir Foundry', 'SQL']",2025-06-11 05:33:45
Opening For DevOps Engineer,Randstad Digital,2 - 6 years,8-10 Lacs P.A.,['Chennai'],"Job Type - Third Party Payroll\nPayroll Company Name - Randstad Digital\nPosition - DevOps Engineer\nExperience - 3+Years\nLocation - Chennai only\nNotice Period - Immediate to 15days\nInterview Type - Virtual (Final Round Face2Face Interview)\nWork Mode - 5 days office\n\n\nIf you're interested, kindly share your updated resume and the following details to this below email ID or LinkedIn\n\n\nEmail Id: gayathri.nambi@randstaddigital.com\nLinkedIn: https://www.linkedin.com/in/gayathri-alagia-nambi-0827921a5/\n\n\nFull Name:\nPan Number:\nExperience:\nRelevant Experience:\nNotice period:\nCTC:\nExpected CTC:\nCurrent company:\nPayroll Company:\nLocation:\nPreferred location:\nOffer in hand : (Y/N)\nReason for Job Change:\n\nKey Responsibilities\nCollaborate closely with Product teams, Software, and Data engineers to maintain and develop tools and infrastructure, driving innovation and optimising existing and new product value streams.\nLead and advise on the modernisation of end-to-end ETL and real-time streaming pipelines, refining development processes, and introducing relevant technologies and tools.\nWork extensively with AWS cloud platforms and tools, applying Infrastructure as Code (IaC) practicesparticularly with Terraformand prioritising both cost optimisation and secure network configurations.\nEnsure robust security by implementing and managing cybersecurity frameworks and tools, including [specific security tools currently in use], to protect systems against threats.\nApply network knowledge to optimise and secure infrastructure, ensuring efficient communication and data flow across systems.\nImplement a proactive patch management strategy to keep all systems updated with the latest security patches.\nAdhere to change management protocols and version control standards, ensuring a secure code repository with reliable backup strategies for key intellectual property. (maybe we can rephrase it better)\nEffectively present and explain advanced technical designs to both technical and non-technical stakeholders.\n\nWhat we are looking for:\nEssential\nBachelor’s degree in software, network, or cybersecurity (or demonstrable equal experience)\nAt least 2 years’ experience in a DevOps Engineer role\nProven experience or certification in AWS\nProven experience in troubleshooting and debugging SQL, SNS, SQS, Kinesis, C# (.net Core), S3, FTP, AWS Lambda. HTTP level REST API experience (and previous API standards, e.g SOAP)\nProven experience in operating system administration (on both Wintel and Linux).\nProven experience in networking components and systems, including webservers, firewalls and network routing.\nFamiliarity with automation tooling (such as Jenkins) and runtime integrated analysis tooling (such as New Relic)\nFamiliarity with the design, development and maintenance of best-in-class analytical capabilities, including data warehousing (Redshift, OpenSearch, Athena, SQL, etc)\nFamiliarity of architectural design patterns for micro-services leveraging relational and big data technologies\nDisciplined, self-starter attitude driven to improve systems and processes and a willingness to learn\nExcellent documentation of processes\nProficiency in written and spoken English\nDesirable\nProvisioning new technical assets (e.g EC2 build), Kubernetes, Docker and associated virtualisation or containerisation technology.\nComplete familiarity with Agile development process\nExcellent documentation of processes",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['DevOps', 'Terraform', 'Docker', 'AWS', 'Kubernetes']",2025-06-11 05:33:47
Data Scientist,Synergy Maritime,3 - 5 years,Not Disclosed,['Chennai'],Role & responsibilities\n\nDesign ML design and Ops stack considering the various trade-offs.\nStatistical Analysis and fundamentals\nMLOPS frameworks design and implementation \nModel Evaluation best practices -Train and retrain systems when necessary.\nExtend existing ML libraries and frameworks -Keep abreast of developments in the field.,,,,"['Data Science', 'Docker', 'Artificial Intelligence', 'Hadoop', 'Big Data Technologies', 'Machine Learning', 'Deep Learning', 'Kubernetes', 'SQL']",2025-06-11 05:33:50
Data Scientist,Qua Xigma IT Solutions Pvt Ltd,2 - 7 years,Not Disclosed,['Tirupati'],"Position Overview:\nWe are seeking a collaborative and analytical Data Scientist who can bridge the gap between business needs and data science capabilities. In this role, you will lead and support projects that apply machine learning, AI, and statistical modeling to generate actionable insights and drive business value.\n\nKey Responsibilities:\nCollaborate with stakeholders to define and translate business challenges into data science solutions.\nConduct in-depth data analysis on structured and unstructured datasets.\nBuild, validate, and deploy machine learning models to solve real-world problems.\nDevelop clear visualizations and presentations to communicate insights.\nDrive end-to-end project delivery, from exploration to production.\nContribute to team knowledge sharing and mentorship activities.\n\nMust-Have Skills:\n3+ years of progressive experience in data science, applied analytics, or a related quantitative role, demonstrating a proven track record of delivering impactful data-driven solutions.\nExceptional programming proficiency in Python, including extensive experience with core libraries such as Pandas, NumPy, Scikit-learn, NLTK and XGBoost.\nExpert-level SQL skills for complex data extraction, transformation, and analysis from various relational databases.\nDeep understanding and practical application of statistical modeling and machine learning techniques, including but not limited to regression, classification, clustering, time series analysis, and dimensionality reduction.\nProven expertise in end-to-end machine learning model development lifecycle, including robust feature engineering, rigorous model validation and evaluation (e.g., A/B testing), and model deployment strategies.\nDemonstrated ability to translate complex business problems into actionable analytical frameworks and data science solutions, driving measurable business outcomes.\nProficiency in advanced data analysis techniques, including Exploratory Data Analysis (EDA), customer segmentation (e.g., RFM analysis), and cohort analysis, to uncover actionable insights.\nExperience in designing and implementing data models, including logical and physical data modeling, and developing source-to-target mappings for robust data pipelines.\nExceptional communication skills, with the ability to clearly articulate complex technical findings, methodologies, and recommendations to diverse business stakeholders (both technical and non-technical audiences).\nExperience in designing and implementing data models, including logical and physical data modeling, and developing source-to-target mappings for robust data pipelines.\nExceptional communication skills, with the ability to clearly articulate complex technical findings, methodologies, and recommendations to diverse business stakeholders (both technical and non-technical audiences).\n\nGood-to-Have Skills:\nExperience with cloud platforms (Azure, AWS, GCP) and specific services like Azure ML, Synapse, Azure Kubernetes and Databricks.\nFamiliarity with big data processing tools like Apache Spark or Hadoop.\nExposure to MLOps tools and practices (e.g., MLflow, Docker, Kubeflow) for model lifecycle management.\nKnowledge of deep learning libraries (TensorFlow, PyTorch) or experience with Generative AI (GenAI) and Large Language Models (LLMs).\nProficiency with business intelligence and data visualization tools such as Tableau, Power BI, or Plotly.\nExperience working within Agile project delivery methodologies.\n\nCompetencies:\nTech Savvy - Anticipating and adopting innovations in business-building digital and technology applications.\nSelf-Development - Actively seeking new ways to grow and be challenged using both formal and informal development channels.\nAction Oriented - Taking on new opportunities and tough challenges with a sense of urgency, high energy, and enthusiasm.\nCustomer Focus - Building strong customer relationships and delivering customer-centric solutions.\nOptimizes Work Processes - Knowing the most effective and efficient processes to get things done, with a focus on continuous improvement.\n\nWhy Join Us?\nBe part of a collaborative and agile team driving cutting-edge AI and data engineering solutions.\nWork on impactful projects that make a difference across industries.\nOpportunities for professional growth and continuous learning.\nCompetitive salary and benefits package.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Python', 'SQL', 'End-to-End ML Development', 'Data Analysis', 'Data Modeling & ETL']",2025-06-11 05:33:51
Openings For QA Engineer,Randstad Digital,3 - 6 years,6-10 Lacs P.A.,['Chennai'],"Job Type - Third Party Payroll\nPayroll Company Name - Randstad Digital\nPosition - QA Engineer\nExperience - 3+Years\nLocation - Chennai only\nNotice Period - Immediate to 15days\nInterview Type - Virtual (Final Round Face2Face Interview)\nWork Mode - 5 days office\nKey Skills - Automation Testing, Manual Testing, AWS, WebDriverIO, Cucumber/Gherkin\n\n\nIf you're interested, kindly share your updated resume and the following details to this below email ID or LinkedIn\n\n\nEmail Id: gayathri.nambi@randstaddigital.com\nLinkedIn: https://www.linkedin.com/in/gayathri-alagia-nambi-0827921a5/\n\n\nFull Name:\nPan Number:\nExperience:\nRelevant Experience:\nNotice period:\nCTC:\nExpected CTC:\nCurrent company:\nPayroll Company:\nLocation:\nPreferred location:\nOffer in hand : (Y/N)\nReason for Job Change:\n\nKey Responsibilities\nCreate, execute, and review test cases in TestRail while actively participating in the testing process.\nAdhere to the QA standards, metrics, and quality gates across all product develop-ment stages.\nDrive improvements to our WebDriverIO/Cucumber test automation framework, aim-ing for consistent scalability.\nWork with AWS-based environments to integrate QA processes into CI/CD pipelines.\nOperate within 2-week sprint cycles, partnering closely with Product, Engineering, and DevOps teams in an Agile environment.\nWork directly with product owners to understand acceptance criteria for upcoming features, particularly on data-heavy and API-driven functionality.\nParticipate in sprint planning, refinement, and retrospectives, acting as an advocate for product quality.\nCollaborate with Engineering teams to ensure big data ingestion and processing pipelines meet testing standards.\nPerform tests at various stages of the production process.\nDeveloping and executing test plans and test cases.\nConducting root cause analysis for defects and recommending corrective actions.\nIdentifying, documenting, and reporting bugs, errors, and inconsistencies.\nStaying updated with new testing tools and strategies.\n\nWhat we are looking for:\nEssential\nAt least 3 years of QA experience both automation and manual.\nProven expertise in JavaScript/TypeScript test automation using WebDriverIO and Cucumber/Gherkin\nFamiliarity with AWS services and integrating QA processes into AWS-based CI/CD pipelines.\nDemonstrable experience testing APIs (RESTful or microservices-based), including end-to-end workflows.\nSelfdiscipline and willingness to learn\nAbility to make connections with technical and non-technical stakeholders\nStrong problem-solving abilities in fast-paced Agile environments.\nProficiency in written and spoken English\nDesirable\nExperience in big data projects (e.g., data-intensive applications, data lakes, or ana-lytics pipelines).\nExperience in maritime, logistics, or risk intelligence industries.\nKnowledge of performance testing tools (e.g., JMeter) and security testing methodol-ogies.\nExperience with API testing frameworks and microservices architectures.\nBackground in implementing QA processes across multi-team agile engineering or-ganizations.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation Testing', 'Manual Testing', 'Cucumber', 'AWS', 'Gherkin', 'Typescript', 'Javascript']",2025-06-11 05:33:52
Software Engineer,Pine Labs,2 - 6 years,Not Disclosed,['Noida'],"Front End Development.\n\nThose who share our core belief of 'Every Day is Game Day'\nWe bring our best selves to work each day to realise our mission of enriching the world through the power of digital commerce and financial services.\n\nROLE PURPOSE\n\nWe are looking for a talented and experienced JavaScript Developer to join our team to build, enhance, and support enterprise-grade applications and automation workflows. The ideal candidate will have strong JavaScript development skills and a passion for building robust, scalable systems. Experience with ServiceNow is a plus, but not mandatory\n\nTHE RESPONSIBILTIES WE ENTRUST YOU WITH.\n\nProduct knowledge, Problem solving, critical thinking\nTechnical understanding, coding skills\nAnalytical ability, sound algorithms and Data structures knowledge\nWHAT MATTERS IN THIS ROLE\nExperience & Portfolio\n2-6 years of experience in JavaScript (ES6+).\nBachelor/master's degree from good institutes\nSolid understanding of web technologies (HTML, CSS, JSON, AJAX)\nExperience working with RESTful APIs and asynchronous programming.\nFamiliarity with Git, CI/CD workflows and agile development practices.\nStrong problem-solving and communication skills\n\nTechnical Skills & Efficiency\nDevelop and maintain web-based applications and automation scripts using JavaScript.\nCollaborate with cross-functional teams to understand business requirements and deliver effective technical solutions.\nWrite clean, maintainable code and follow best practices for development and testing.\nWork with APIs (REST/SOAP) for data integration between systems.\nBuild custom UI components and tools for internal automation.\nDebug and troubleshoot application issues efficiently.\nContribute to architecture discussions and help improve coding standards.\n\nTHINGS YOU SHOULD BE COMFORTABLE WITH\nWorking from office\nAs of now, we work out of our offices, 5 days a week.\nPushing the boundaries\nHave a big idea? See something that you feel we should do but havent done? We will hustle hard to make it happen. We encourage out of the box thinking, and if you bring that with you, we will make sure you get a bag that fits all the energy you bring along.\nWHAT WE VALUE IN OUR PEOPLE\nYou take the shot\nYou decide fast and deliver right.\nYou are the CEO of what you do\nYou show ownership and make things happen.\nYou sign your work like an artist\nYou seek to learn and take pride in the work you do.",Industry Type: FinTech / Payments,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Servicenow', 'Javascript', 'Java Script']",2025-06-11 05:33:54
AI Engineer,HCLTech,10 - 14 years,Not Disclosed,['Noida'],"Seniority: Senior\nDescription & Requirements\nPosition Summary\nThe Senior AI Engineer with GenAI expertise is responsible for developing advanced technical solutions, integrating cutting-edge generative AI technologies. This role requires a deep understanding of modern technical and cloud-native practices, AI, DevOps, and machine learning technologies, particularly in generative models. You will support a wide range of customers through the Ideation to MVP journey, showcasing leadership and decision-making abilities while tackling complex challenges.\nKey Responsibilities\nTechnical & Engineering Leadership\nDevelop solutions leveraging GenAI technologies, integrating advanced AI capabilities into cloud-native architectures to enhance system functionality and scalability.\nLead the design and implementation of GenAI-driven applications, ensuring seamless integration with microservices and container-based environments.\nCreate solutions that fully leverage the capabilities of modern microservice and container-based environments running in public, private, and hybrid clouds.\nContribute to HCL thought leadership across the Cloud Native domain with an expert understanding of open-source technologies (e.g., Kubernetes/CNCF) and partner technologies.\nCollaborate on joint technical projects with partners, including Google, Microsoft, AWS, IBM, Red Hat, Intel, Cisco, and Dell/VMware.\nService Delivery\nEngineer innovative GenAI solutions from ideation to MVP, ensuring high performance and reliability within cloud-native frameworks.\nOptimize AI models for deployment in cloud environments, balancing efficiency and effectiveness to meet client requirements and industry standards.\nAssess existing complex solutions and recommend appropriate technical treatments to transform applications with cloud-native/12-factor characteristics.\nRefactor existing solutions to implement a microservices-based architecture.\nInnovation & Initiative\nDrive the adoption of cutting-edge GenAI technologies within cloud-native projects, spearheading initiatives that push the boundaries of AI integration in cloud services.\nEngage in technical innovation and support HCLs position as an industry leader.\nAuthor whitepapers, blogs, and speak at industry events.\nMaintain hands-on technical credibility, stay ahead of industry trends, and mentor others.\nClient Relationships\nProvide expert guidance to clients on incorporating GenAI and machine learning into their cloud-native systems, ensuring best practices and strategic alignment with business goals.\nConduct workshops and briefings to educate clients on the benefits and applications of GenAI, establishing strong, trust-based relationships.\nPerform a trusted advisor role, contributing to technical projects (PoCs and MVPs) with a strong focus on technical excellence and on-time delivery.\nMandatory Skills & Experience\nA passionate developer with 10+ years of experience in Java, Python, Node.js, and Spring programming, comfortable working as part of a paired/balanced team.\nExtensive experience in software development, with significant exposure to AI/ML technologies.\nExpertise in GenAI frameworks: Proficient in using GenAI frameworks and libraries such as LangChain, OpenAI API, Gemini, and Hugging Face Transformers.\nPrompt engineering: Experience in designing and optimizing prompts for various AI models to achieve desired outputs and improve model performance.\nStrong understanding of NLP techniques and tools, including tokenization, embeddings, transformers, and language models.\nProven experience developing complex solutions that leverage cloud-native technologiesfeaturing container-based, microservices-based approaches; based on applying 12-factor principles to application engineering.\nExemplary verbal and written communication skills (English).\nPositive and solution-oriented mindset.\nSolid experience delivering Agile and Scrum projects in a Jira-based project management environment.\nProven leadership skills and the ability to inspire and manage teams.\nDesired Skills & Experience\nMachine Learning Operations (MLOps): Experience in deploying, monitoring, and maintaining AI models in production environments using MLOps practices.\nData engineering for AI: Skilled in data preprocessing, feature engineering, and creating pipelines to feed AI models with high-quality data.\nAI model fine-tuning: Proficiency in fine-tuning pre-trained models on specific datasets to improve performance for specialized tasks.\nAI ethics and bias mitigation: Knowledgeable about ethical considerations in AI and experienced in implementing strategies to mitigate bias in AI models.\nKnowledgeable about vector databases, LLMs, and SMLs, and integrating with such models.\nProficient with Kubernetes and other cloud-native technologies, including experience with commercial Kubernetes distributions (e.g., Red Hat OpenShift, VMware Tanzu, Google Anthos, Azure AKS, Amazon EKS, Google GKE).\nDeep understanding of core practices including DevOps, SRE, Agile, Scrum, XP, Domain-Driven Design, and familiarity with the CNCF open-source community.\nRecognized with multiple cloud and technical certifications at a professional level, ideally including AI/ML specializations from providers like Google, Microsoft, AWS, Linux Foundation, IBM, or Red Hat.\nVerifiable Certification\nAt least one recognized cloud professional / developer certification (AWS/Google/Microsoft)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['AI engineering', 'VMware', 'Java', 'Azure', 'Data engineering', 'AI models', 'Node.js', 'NLP', 'Azure AKS', 'Machine Learning Operations', 'AWS', 'Kubernetes', 'Python']",2025-06-11 05:33:56
Data Engineer_ Immediate Joiner Required,Healthcare technology services,6 - 7 years,15-25 Lacs P.A.,"['Pune', 'Chennai']","Role: Data Engineer\nExperience: 6-9Years\nRelevant Experience in Data Engineer: 6+ Years\nNotice Period: Immediate Joiners Only\nJob Location: Pune and Chennai\n\nKey Responsibilities:\n\nMandatory Skill: Spark,SQL and Python\n\nMust Have:\nRelevant experience of 6-9 years as a Data Engineer\nExperience in programming language like Python\nGood Understanding of ETL (Extract, Transform, Load) concepts\nGood analytical and problem-solving skills\nKnowledge of a ticketing tool like JIRA/SNOW\nGood communication skills to interact with Customers on issues & requirements.\n\nReach us:If you are interested in this position and meet the above qualifications, please reach out to me directly at swati@cielhr.com and share your updated resume highlighting your relevant experience.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Spark', 'Python', 'SQL']",2025-06-11 05:33:57
Software Development Engineer,Accenture,3 - 5 years,Not Disclosed,['Chennai'],"Project Role :Software Development Engineer\n\n\n\nProject Role Description :Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.\n\nMust have skills :OneStream Extensive Finance SmartCPM\n\n\nGood to have skills :NAMinimum\n\n3 year(s) of experience is required\n\n\nEducational Qualification :Bachelor of EngineeringFinance Background MBA PG RecommendedKnowledge on any coding language will be plus\nSummary:As a Software Development Engineer, you will engage in a dynamic work environment where you will analyze, design, code, and test various components of application code across multiple clients. Your day will involve collaborating with team members to ensure the successful implementation of software solutions, performing maintenance and enhancements, and contributing to the overall development process. You will be responsible for delivering high-quality code while adhering to best practices and project timelines, ensuring that the applications meet client requirements and expectations. Key Responsibilities 1.Support the implementation, development, and configuration of OneStream XF applications across planning, budgeting, forecasting, and financial close areas.2.Collaborate with finance and IT teams to gather requirements and deliver scalable solutions aligned with business needs.3.Assist in building and maintaining business rules, workflows, dashboards, and data integrations within the OneStream platform.4.Participate in testing, troubleshooting, and documentation to ensure optimal performance and usability.5.Provide day-to-day support to end-users and assist with ongoing system enhancements.Maintain data integrity and help in automating key financial processes.\nTechnical Experience 6.3+ years of experience in EPM systems, including hands-on work with OneStream XF.7.Solid understanding of financial processes such as planning, budgeting, forecasting, and consolidations.8.Basic to intermediate skills in VB.NET, SQL, and/or scripting languages used in OneStream development.9.Familiarity with data integrations, source systems (ERP/GL), and financial reporting tools.10.Strong problem-solving, communication, and documentation skills.11.Bachelors degree in finance, Accounting, Information Systems, or related field.\nProfessional Attributes 1.Good Communication skills as candidate will be speaking (in calls) and writing mails directly to Client.2.Candidate should have good listening qualities and positive attitude to take the challenging task.3.Candidate should have good analytical and presentation skills.4.Strong sense of responsibility and positive attitude\nEducational Qualification Bachelor of EngineeringFinance Background (MBA/PG) RecommendedKnowledge on any coding language will be plus.\n\nQualification\n\nBachelor of EngineeringFinance Background MBA PG RecommendedKnowledge on any coding language will be plus",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'data visualization', 'epm', 'vb', 'finance', 'css', 'c++', 'dbms', 'jquery', 'java', 'git', 'asp.net', 'j2ee', 'json', 'html', 'mysql', 'scripting languages', 'c#', 'erp', 'python', 'software development', 'c', 'gl', 'javascript', 'sql server', 'microsoft windows', 'troubleshooting', '.net', 'data integration']",2025-06-11 05:33:59
Power BI Developer - Data Analytics,Leading Client,7 - 10 years,Not Disclosed,['Pune'],"About the Job :\n\nWe are seeking a highly skilled and experienced Power BI Developer to join our dynamic team. In this role, you will be instrumental in transforming raw data into insightful and actionable visualizations that drive business decisions. You will be responsible for the entire lifecycle of dashboard development, from understanding business needs and designing user-friendly interfaces in Figma to building robust data models and implementing sophisticated DAX calculations in Power BI. Collaboration with stakeholders and cross-functional teams is crucial to ensure the delivery of high-quality, impactful BI solutions.\n\nJob Summary :\n\nAs a Power BI Developer, you will leverage your deep expertise in data visualization, SQL/DAX, and UI/UX design principles using Figma to create interactive and visually compelling dashboards. You will work closely with business users to gather requirements, design intuitive interfaces, develop efficient data models, and implement robust reporting solutions within the Power BI ecosystem. Your ability to translate business needs into technical specifications and effectively communicate analytical findings will be key to your success in this role.\n\nKey Responsibilities :\n\n- Dashboard Design and Development : Design, develop, and deploy interactive dashboards and visual reports using Power BI Desktop and Power BI Service.\n\n- UI/UX Prototyping with Figma : Collaborate with business users to understand their reporting needs and translate them into user-friendly wireframes, mockups, and prototypes using Figma.\n\n- Figma to Power BI Implementation : Convert Figma designs into fully functional and aesthetically pleasing Power BI dashboards, ensuring adherence to UI/UX best practices.\n\n- DAX Development : Write and optimize complex DAX (Data Analysis Expressions) calculations to derive meaningful insights and implement business logic within Power BI.\n\n- SQL Querying and Optimization : Develop and optimize complex SQL queries to extract, transform, and load data from various data sources. This includes writing joins, window functions, Common Table Expressions (CTEs), and stored procedures.\n\n- Data Modeling : Design and implement efficient and scalable data models within Power BI, adhering to data warehousing best practices (e.g., star schema, snowflake schema).\n\n- Security Implementation : Implement and manage row-level security (RLS) and other security measures within Power BI to ensure data privacy and appropriate access control.\n\n- Performance Tuning : Identify and implement performance optimization techniques within Power BI reports and data models to ensure optimal responsiveness and efficiency.\n\n- Data Source Integration : Integrate Power BI with diverse data sources, including databases (e.g., SQL Server, Azure Synapse), cloud platforms, APIs, and other relevant systems.\n\n- Stakeholder Communication : Present analytical findings, dashboard designs, and technical solutions to stakeholders in a clear, concise, and compelling manner.\n\n- Requirements Gathering : Actively participate in gathering business requirements through user interviews, workshops, and documentation analysis.\n\n- Agile Collaboration : Work effectively within Agile/Scrum teams, contributing to sprint planning, daily stand-ups, and retrospectives, ensuring timely delivery of assigned tasks.\n\n- Documentation : Create and maintain comprehensive documentation for developed dashboards, data models, and processes.\n\n- Continuous Improvement : Stay updated with the latest Power BI features, Figma updates, and industry best practices to continuously improve the quality and efficiency of BI solutions.\n\nRequired Skills :\n\n- Experience : Minimum of 7 years of demonstrable experience in Business Intelligence and Data Analytics, with a strong focus on Power BI development.\n\n- Power BI Expertise : Proven hands-on expertise in Power BI Desktop and Power BI Service, including advanced DAX capabilities (e.g., CALCULATE, measures, calculated columns, time intelligence functions).\n\n- Figma Proficiency : Strong practical experience using Figma for UI/UX prototyping, wireframing, and creating visually appealing dashboard designs. Ability to translate design specifications into functional Power BI reports.\n\n- SQL Proficiency : Deep understanding and proficiency in SQL, with the ability to write complex queries involving multiple joins, window functions, CTEs, and stored procedures across various database systems.\n\n- Data Warehousing and Modeling : Solid understanding of data warehousing concepts, dimensional modeling (star and snowflake schemas), and ETL/ELT processes.\n\n- Cloud Data Experience (Preferred) : Experience working with cloud-based data sources such as Azure Synapse Analytics, SQL Server on Azure, or other cloud data platforms is a significant plus.\n\n- Requirements Elicitation : Demonstrated ability to effectively gather business requirements, conduct user interviews, and translate them into clear and actionable BI solutions.\n\n- Communication Skills : Excellent written and verbal communication skills, with the ability to effectively communicate technical concepts to both technical and non-technical audiences.\n\n- Problem-Solving : Strong analytical and problem-solving skills with the ability to troubleshoot issues and propose effective solutions.\n\n- Teamwork : Ability to work collaboratively within cross-functional teams and contribute positively to a team environment.\n\n- Agile Methodology : Experience working in Agile/Scrum development methodologies.\n\nEducation :\n\n- Bachelor's degree in Computer Science, Information Systems, Data Science, or a related quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analytics', 'Business Intelligence', 'DAX Power BI', 'Power BI', 'Dashboard Design', 'Data Modeling', 'Data Warehousing', 'ETL', 'SQL', 'Data Integration']",2025-06-11 05:34:00
Snowflake Data Architect,Kasmo Digital,10 - 16 years,Not Disclosed,['Hyderabad'],"Required Skills & Qualifications:\n10-12 years of experience in data architecture, data warehousing, and cloud technologies.\nStrong expertise in Snowflake architecture, data modeling, and optimization.\nSolid hands-on experience with cloud platforms: AWS, Azure, and GCP.\nIn-depth knowledge of SQL, Python, PySpark, and related data engineering tools.\nExpertise in data modeling (both dimensional and normalized models).\nStrong experience with data integration, ETL processes, and pipeline development.\nCertification in Snowflake, AWS, Azure, or related cloud technologies.\nExperience working with large-scale data processing frameworks and platforms.\nExperience in data visualization tools and BI platforms (e.g., Tableau, Power BI).\nExperience in Agile methodologies and project management.\nStrong problem-solving skills with the ability to address complex technical challenges.\nExcellent communication skills and ability to work collaboratively with cross-functional teams.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Data Visualization', 'Data Modeling', 'Data Warehousing', 'SQL', 'Data Architecture', 'Python']",2025-06-11 05:34:02
Data Architect,Opus Technologies,10 - 16 years,35-50 Lacs P.A.,['Pune'],"Role & responsibilities\nDefine and evolve data engineering & analytics offerings aligned with payments domain needs (e.g., transaction analytics, fraud detection, customer insights).\nLead reference architecture creation for data modernization, real-time analytics, and cloud-native data platforms (e.g., Azure Synapse, GCP BigQuery).\nBuild reusable components, PoCs, and accelerators for ingestion, transformation, data quality, and governance.\nSupport pre-sales engagements with solution design, estimation, and client workshops.\nGuide delivery teams on data platform implementation, optimization, and security.\nMentor and upskill talent pool through learning paths and certifications.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Kafka', 'Spark', 'Python', 'Azure Data Factory']",2025-06-11 05:34:04
ETL and BDX developer,"NTT DATA, Inc.",10 - 15 years,Not Disclosed,['Bengaluru'],"Req ID: 321918\n\nWe are currently seeking a ETL and BDX developer to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\n""¢ Develop and maintain Power BI dashboards, reports, and datasets.\n\n""¢ Collaborate with stakeholders to gather and analyse business requirements.\n\n""¢ Design robust and scalable data models using Power BI and underlying data sources.\n\n""¢ Write complex DAX expressions for calculated columns, measures, and KPIs.\n\n""¢ Optimize performance of Power BI reports and data models.\n\n""¢ Integrate Power BI with other data sources (SQL Server, Excel, Azure, SharePoint, etc.).\n\n""¢ Implement row-level security and data access control.\n\n""¢ Automate data refresh schedules and troubleshoot refresh failures.\n\n""¢ Mentor junior developers and conduct code reviews.\n\n""¢ Work closely with data engineering teams to ensure data accuracy and integrity.\n\n""¢ Exp working on Power Query and data flows.\n\n""¢ Strong in writing SQL queries\n\n\n\nTotal Exp7 ""“ 10 Yrs.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql queries', 'power bi', 'microsoft azure', 'sql server', 'power query', 'power bi reports', 'ssas', 'bi', 'sharepoint', 'data engineering', 'dashboards', 'sql', 'data modeling', 'power bi dashboards', 'ssrs', 'dax', 'code review', 'etl', 'ssis', 'etl development', 'data flow', 'msbi']",2025-06-11 05:34:06
LLMOps Engineer,HCLTech,8 - 10 years,Not Disclosed,['Noida'],"Position Summary\nLLMOps(Large language model operations) Engineer will play a pivotal role in building and maintaining the infrastructure and pipelines for our cutting-edge Generative AI applications, establishing efficient and scalable systems for LLM research, evaluation, training, and fine-tuning. Engineer will be responsible for managing and optimizing large language models (LLMs) across various platforms This position is uniquely tailored for those who excel in crafting pipelines, cloud infrastructure, environments, and workflows. Your expertise in automating and streamlining the ML lifecycle will be instrumental in ensuring the efficiency, scalability, and reliability of our Generative AI models and associated platform. LLMOps engineers expertise will ensure the smooth deployment, maintenance, and performance of these AI platforms and powerful large language models.\n\nYou will follow Site Reliability Engineering & MLOps principles and will be encouraged to contribute your own best practices and ideas to our ways of working.\nReporting to the Head of Cloud Native operations, you will be an experienced thought leader, and comfortable engaging senior managers and technologists. You will engage with clients, display technical leadership, and guide the creation of efficient and complex products/solutions.\nKey Responsibilities\nTechnical & Architectural Leadership\n\nContribute to the technical delivery of projects, ensuring a high quality of work that adheres to best practices, brings innovative approaches and meets client expectations. Project types include following (but not limited to):\nSolution architecture, Proof of concepts (PoCs), MVP, design, develop, and implementation of ML/LLM pipelines for generative AI models, data management & preparation for fine tuning, training, deployment, and monitoring.\nAutomate ML tasks across the model lifecycle.\nContribute to HCL thought leadership across the Cloud Native domain with an expert understanding of advanced AI solutions using Large Language Models (LLM) & Natural Language Processing (NLP) techniques and partner technologies.\nCollaborate with cross-functional teams to integrate LLM and NLP technologies into existing systems.\nEnsure the highest levels of governance and compliance are maintained in all ML and LLM operations.\nStay abreast of the latest developments in ML and LLM technologies and methodologies, integrating these innovations to enhance operational efficiency and model effectiveness.\nCollaborate with global peers from partner ecosystems on joint technical projects. This partner ecosystem includes Google, Microsoft, Nvidia, AWS, IBM, Red Hat, Intel, Cisco, and Dell VMware etc.\nService Delivery\n\nProvide a technical hands-on contribution. Create scalable infra to support enterprise loads (distributed GPU compute, foundation models, orchestrating across multiple cloud vendors, etc.)\nEnsuring the reliable and efficient platform operations.\nApply data science, machine learning, deep learning, and natural language processing methods to analyse, process, and improve the models data and performance.\nUnderstanding of Explainability & Biased Detection concepts.\nCreate and optimize prompts and queries for retrieval augmented generation and prompt engineering techniques to enhance the models capabilities and user experience w.r.t Operations & associated platforms.\nClient-facing influence and guidance, engaging in consultative client discussions and performing a Trusted Advisor role.\nProvide effective support to HCL Sales and Delivery teams.\nSupport sales pursuits and enable HCL revenue growth.\nDefine the modernization strategy for client platform and associated IT practices, create solution architecture and provide oversight of the client journey.\nInnovation & Initiative\n\nAlways maintain hands-on technical credibility, keep in front of the industry, and be prepared to show and lead the way forward to others.\nEngage in technical innovation and support HCLs position as an industry leader.\nActively contribute to HCL sponsorship of leading industry bodies such as the CNCF and Linux Foundation.\nContribute to thought leadership by writing Whitepapers, blogs, and speaking at industry events.\nBe a trusted, knowledgeable internal innovator driving success across our global workforce.\nClient Relationships\n\nAdvise on best practices related to platform & Operations engineering and cloud native operations, run client briefings and workshops, and engage technical leaders in a strategic dialogue.\nDevelop and maintain strong relationships with client stakeholders.\nPerform a Trusted Advisor role.\nContribute to technical projects with a strong focus on technical excellence and on-time delivery.\nMandatory Skills & Experience\nExpertise in designing and optimizing machine-learning operations, with a preference for LLMOps.\nProficient in Data Science, Machine Learning, Python, SQL, Linux/Unix shell scripting.\nExperience on Large Language Models and Natural Language Processing (NLP), and experience with researching, training, and fine-tuning LLMs. Contribute towards fine-tune Transformer models for optimal performance in NLP tasks, if required.\nImplement and maintain automated testing and deployment processes for machine learning models w.r.t LLMOps.\nImplement version control, CI/CD pipelines, and containerization techniques to streamline ML and LLM workflows.\nDevelop and maintain robust monitoring and alerting systems for generative AI models ensuring proactive identification and resolution of issues.\nResearch or engineering experience in deep learning with one or more of the following: generative models, segmentation, object detection, classification, model optimisations.\nExperience implementing RAG frameworks as part of available-ready products.\nExperience in setting up the infrastructure for the latest technology such as Kubernetes, Serverless, Containers, Microservices etc.\nExperience in scripting programming to automate deployments and testing, worked on tools like Terraform and Ansible. Scripting languages like Python, bash, YAML etc.\nExperience on CI/CD opensource and enterprise tool sets such as Argo CD, Jenkins.\nExperience with the GitHub/DevOps Lifecycle\nExperience in at least one of the Observability solutions (Prometheus, EFK stacks, ELK stacks, Grafana, Dynatrace, AppDynamics)\nExperience in at-least one of the clouds for example - Azure/AWS/GCP\nSignificant experience on microservices-based, container-based or similar modern approaches of applications and workloads.\nYou have exemplary verbal and written communication skills (English). Able to interact and influence at the highest level, you will be a confident presenter and speaker, able to command the respect of your audience.\nDesired Skills & Experience\nBachelor level technical degree or equivalent experience; Computer Science, Data Science, or Engineering background preferred; masters degree desired.\nExperience in LLMOps or related areas, such as DevOps, data engineering, or ML infrastructure.\nHands-on experience in deploying and managing machine learning and large language model pipelines in cloud platforms (e.g., AWS, Azure) for ML workloads.\nFamiliar with data science, machine learning, deep learning, and natural language processing concepts, tools, and libraries such as Python, TensorFlow, PyTorch, NLTK etc.\nExperience in using retrieval augmented generation and prompt engineering techniques to improve the models quality and diversity to improve operations efficiency. Proven experience in developing and fine-tuning Language Models (LLMs).\nStay up-to-date with the latest advancements in Generative AI, conduct research, and explore innovative techniques to improve model quality and efficiency.\nThe perfect candidate will already be working within a System Integrator, Consulting or Enterprise organisation with 8+ years of experience in a technical role within the Cloud domain.\nDeep understanding of core practices including SRE, Agile, Scrum, XP and Domain Driven Design. Familiarity with the CNCF open-source community.\nEnjoy working in a fast-paced environment using the latest technologies, love Labs dynamic and high-energy atmosphere, and want to build your career with an industry leader.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['LLMOps', 'Architectural Leadership', 'Machine Learning', 'Unix shell scripting', 'SQL', 'Data Science', 'NLP', 'Linux', 'Terraform', 'Ansible', 'CI/CD', 'technical delivery', 'AWS', 'Python']",2025-06-11 05:34:07
MDM Data Science Manager,Amgen Inc,10 - 14 years,Not Disclosed,['Hyderabad'],"We are seeking an accomplished and visionary Data Scientist/ GenAI Lead to join Amgens Enterprise Data Management team.\nAs MDM Data Science/Manager, you will lead the design, development, and deployment of Generative AI and ML models to power data-driven decisions across business domains.\nThis role is ideal for an AI practitioner who thrives in a collaborative environment and brings a strategic mindset to applying advanced AI techniques to solve real-world problems.To succeed in this role, the candidate must have strong AI/ML, Data Science, GenAI experience along with MDM knowledge, hence the candidates having only MDM experience are not eligible for this role. Candidate must have AI/ML, data science and GenAI experience on technologies like (PySpark/PyTorch, TensorFlow, LLM, Autogen, Hugging FaceVectorDB,Embeddings, RAGsetc), along with knowledge of MDM (Master Data Management)\nRoles & Responsibilities:\nDrive development of enterprise-level GenAI applications using LLM frameworks such as Langchain, Autogen, and Hugging Face.\nArchitect intelligent pipelines using PySpark, TensorFlow, and PyTorch within Databricks and AWS environments.\nImplement embedding models andmanage VectorStores for retrieval-augmented generation (RAG) solutions.\nIntegrate and leverage MDM platforms like Informatica and Reltio to supply high-quality structured data to ML systems.\nUtilize SQL and Python for data engineering, data wrangling, and pipeline automation.\nBuild scalable APIs and services to serve GenAI models in production.\nLead cross-functional collaboration with data scientists, engineers, and product teams to scope, design, and deploy AI-powered systems.\nEnsure model governance, version control, and auditability aligned with regulatory and compliance expectations.\nBasic Qualifications and Experience:\nMasters degree with 8 - 10 years of experience in Data Science, Artificial Intelligence, Computer Science, or related fields OR\nBachelors degree with 10 - 14 years of experience in Data Science, Artificial Intelligence, Computer Science, or related fields OR\nDiploma with 14 - 16 years of hands-on experience in Data Science, AI/ML technologies, or related technical domains\nFunctional Skills:\nMust-Have Skills:\n10+ years of experience working in AI/ML or Data Science roles, including designing and implementing GenAI solutions.\nExtensive hands-on experience with LLM frameworks and tools such as Langchain, Autogen, Hugging Face, OpenAI APIs, and embedding models.\nStrong programming background with Python, PySpark, and experience in building scalable solutions using TensorFlow, PyTorch, and SK-Learn.\nProven track record of building and deploying AI/ML applications in cloud environments such as AWS.\nExpertise in developing APIs, automation pipelines, and serving GenAI models using frameworks like Django, FastAPI, and DataBricks.\nSolid experience integrating and managing MDM tools (Informatica/Reltio) and applying data governance best practices.\nGuide the team on development activities and lead the solution discussions\nMust have core technical capabilities in GenAI, Data Science space\nGood-to-Have Skills:\nPrior experience in Data Modeling, ETL development, and data profiling to support AI/ML workflows.\nWorking knowledge of Life Sciences or Pharma industry standards and regulatory considerations.\nProficiency in tools like JIRA and Confluence for Agile delivery and project collaboration.\nFamiliarity with MongoDB, VectorStores, and modern architecture principles for scalable GenAI applications.\nProfessional Certifications:\nAny ETL certification (e.g. Informatica)\nAny Data Analysis certification (SQL)\nAny cloud certification (AWS or AZURE)\nData Science and ML Certification\nSoft Skills:\nStrong analytical abilities to assess and improve master data processes and solutions.\nExcellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders.\nEffective problem-solving skills to address data-related issues and implement scalable solutions.\nAbility to work effectively with global, virtual teams",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Science', 'AZURE', 'AI/ML', 'PyTorch', 'Data Analysis', 'ETL', 'AWS', 'SQL', 'TensorFlow']",2025-06-11 05:34:09
Security Managed Services Engineer (L2),"NTT DATA, Inc.",5 - 10 years,Not Disclosed,['Mumbai'],"Your day at NTT DATA\nThe Security Managed Services Engineer (L2) is a developing engineering role, responsible for providing a managed service to clients to ensure that their Security Infrastructures and systems remain operational.\n\nThrough the proactive monitoring, identifying, investigating, and resolving of technical incidents and problems, this role is able to restore service to clients.\n\nThe primary objective of this role is to proactively review client requests or tickets and apply technical/process knowledge to resolve them without breaching service level agreement (SLA) and focuses on second-line support for incidents and requests with a medium level of complexity.\n\nThe Security Managed Services Engineer (L2) may also contribute to support on project work as and when required.\nWhat you'll be doing\nKey Responsibilities:\nMin 5 Years exp\nCollaborate with Company to address challenging issues in cyber, analytics, machine learning, optimization, and computer networking to research solutions.\nPropose new research projects to tackle complex cyber, analytics, machine learning, optimization, and networking problems.\nPossess expertise in comprehending advanced persistent threats, emerging threats, and malware within a corporate environment.\nUnderstand attacks, attack vectors, and kill chain methodology.\nDemonstrate proficiency in working with big data and executing complex queries across multiple platforms.\nExhibit a strong grasp of malware analysis, threat taxonomy, and threat indicators.\nCompetently engage with various security technologies.\n\nAcademic Qualifications and Certifications:\nBachelor's degree or equivalent qualification in IT/Computing (or demonstrated equivalent work experience).\nCTIA/CEH/CSA certification in must.\nWorkplace type:\nOn-site Working",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Security engineering', 'malware analysis', 'networking', 'Security monitoring', 'Security analysis']",2025-06-11 05:34:11
Business Analyst- Cloud Data Platform,Rarr Technologies,4 - 6 years,Not Disclosed,[],"Job Title: Business Analyst (BA) Cloud Data Platform\n\nPrimary Skillset:\nBusiness Analysis expertise in cloud-based data platforms\nSQL\nWork closely with stakeholders to gather, analyze, and document business and technical requirements\nCollaborate with Azure Data Engineers and architects to translate business needs into scalable cloud-based data solutions\nConduct detailed analysis and support design decisions across data integration, transformation, and storage layers\nAssist in defining and validating data quality rules and governance processes\nDrive requirement sessions and ensure traceability throughout the project lifecycle\nManage timelines, risks, and dependencies across multiple priorities and projects\nRequired Experience and Qualifications:\nMinimum 4 years of experience as a Business Analyst, with at least 2+ years in Azure Data Engineering projects\nGood to have knowledge of:\nAzure Data Factory\nAzure Databricks\nAzure SQL Database\nOther Azure data services\nStrong analytical and problem-solving skills with the ability to work in fast-paced, dynamic environments\nFamiliarity with ETL pipelines, data modeling, and data warehousing concepts\nExcellent verbal and written communication skills\nDemonstrated ability to collaborate across business and technical teams\nUnderstanding of data governance and data quality best practices\nPreferred Skills:\nExposure to other cloud platforms\nFamiliarity with modern data engineering tools and frameworks\nSQL\nBusiness Analyst",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data services', 'Business Analyst', 'Business analysis', 'Data modeling', 'Analytical', 'Cloud', 'data governance', 'Data quality', 'Data warehousing', 'SQL']",2025-06-11 05:34:12
Lead - Master Data Management,VIBGYOR Group of Schools,8 - 13 years,Not Disclosed,['Mumbai (All Areas)'],"Role & responsibilities:\n\nThe Data Quality Lead for Central Master Data Management (MDM) is responsible for ensuring the quality, consistency, and accuracy of master data across the organization. This role involves developing and enforcing data quality standards, performing data profiling, and leading data quality initiatives for the companys central MDM system. The Data Quality Lead works closely with business units, IT teams, and data stewards to ensure the data used in key business processes is accurate, reliable, and fit for purpose.\nThe role is pivotal in driving data governance best practices and establishing a centralized approach to managing master data, ensuring compliance with regulatory requirements, and improving decision-making across the enterprise.\n\nData Quality Strategy and Framework:\nDevelop and implement a comprehensive data quality strategy for the Central Master Data Management (MDM) function.\nDefine and document data quality policies, standards, and procedures in alignment with business needs and data governance frameworks.\nEstablish metrics and KPIs to monitor and measure data quality and integrity across master data domains (e.g., customers, vendors, products, financial data).\n\nMaster Data Quality Assurance:\nLead data quality initiatives for central master data management, ensuring the accuracy, completeness, and consistency of master data across systems.\nConduct regular data quality assessments, data profiling, and audits to identify gaps,\nanomalies, or data integrity issues.\nDevelop and manage data validation processes and tools to ensure proper data entry, transformation, and integration.\n\nCollaboration with Business & IT:\nCollaborate with business stakeholders, data stewards, and IT teams to understand data quality requirements, challenges, and improvement opportunities.\nWork with data owners and stewards to resolve data quality issues and ensure the alignment of data definitions, business rules, and standards across the organization.\nProvide guidance and training to business users on data quality best practices and data governance principles.\n\nData Profiling & Root Cause Analysis:\nPerform data profiling and analysis to assess the current state of master data and identify areas for improvement.\n\nData Quality Tools & Technologies:\nSelect, implement, and manage data quality tools and solutions to automate data quality monitoring, reporting, and improvement efforts.\nWork closely with IT teams to ensure that data quality tools integrate effectively with MDM systems and other data sources.\n\nMaster Data Integration:\nEnsure proper integration of master data across different systems and departments, enforcing data standards during migration, transformation, and loading processes.\n\nReporting & Documentation:\nDevelop and maintain data quality dashboards, scorecards, and reports for senior management and key stakeholders.\nEnsure that all data quality rules, processes, and changes are thoroughly documented and maintained in a central repository.",Industry Type: Education / Training,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Master Data Management', 'Data Quality', 'Data Integrity', 'MDM', 'data governance', 'Data Cleansing', 'Data Quality Management', 'Data Warehousing', 'Master Data']",2025-06-11 05:34:14
Data QA - Python SQL Automation,Rarr Technologies,3 - 8 years,Not Disclosed,[],"Strong quality assurance experience with hands on experience in SQL, Python (postgres experience preferred) - Min 6-10 years\nStrong experience in performing manual and automation testing in validating data sources\nStrong ETL experience in any tool\nStrong demonstrated experience to work closely with data engineering functions to ensure a sustainable test approach\nStrong communication skills\nPython, Sql, Api, Automation, Selenium, Testing",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['QA', 'Automation', 'Automation testing', 'Manager Quality Assurance', 'selenium testing', 'MIN', 'Manual', 'SQL', 'Python', 'Testing']",2025-06-11 05:34:16
Master Data Management Data Architect,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will be responsible for designing, building, maintaining, analyzing, and interpreting data deliver actionable insights that drive business decisions. This role involves working with large datasets, developing reports, supporting and driving data governance initiatives, and visualizing data to ensure data is accessible, reliable, and efficiently managed. The ideal candidate has deep technical skills and provides administration support for Master Data Management (MDM) and Data Quality platform, including solution architecture, inbound/outbound data integration (ETL), Data Quality (DQ), and maintenance/tuning of match rules.\nRoles & Responsibilities:\nDesign, develop, and maintain data solutions for data generation, collection, and processing\nCollaborate and communicate with MDM Developers, Data Architects, Product teams, Business SMEs, and Data Scientists to design and develop end-to-end data pipelines to meet fast paced business needs across geographic regions\nIdentify and resolve complex data-related challenges\nAdhere to standard processes for coding, testing, and designing reusable code/component\nParticipate in sprint planning meetings and provide estimations on technical implementation\nAs a SME, work with the team on MDM related product installation, configuration, customization and optimization\nResponsible for the understanding, documentation, maintenance, and additional creation of master data related data-models (conceptual, logical, and physical) and database structures\nReview technical model specifications and participate in data quality testing\nCollaborate with Data Quality & Governance Analyst and Data Governance Organization to monitor and preserve the master data quality\nCreate and maintain system specific master data data-dictionaries for domains in scope\nArchitect MDM Solutions, including data modeling and data source integrations from proof-of-concept through development and delivery\nDevelop the architectural design for Master Data Management domain development, base object integration to other systems and general solutions as related to Master Data Management\nDevelop and deliver solutions individually or as part of a development team\nApproves code reviews and technical work\nMaintains compliance with change control, SDLC and development standards\nContribute to the design, development, and implementation of data pipelines, ETL/ELT processes, and data integration solutions\nCollaborate with multi-functional teams to understand data requirements and design solutions that meet business needs\nImplement data security and privacy measures to protect sensitive data\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions\n\nBasic Qualifications:\nMasters degree and 1 to 3 years of Computer Science, IT or related field experience OR\nBachelors degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience.\nPreferred Qualifications:\nExpertise in architecting and designing Master Data Management (MDM) solutions.\nPractical experience with AWS Cloud, Databricks, Apache Spark, workflow orchestration, and optimizing big data processing performance.\nFamiliarity with enterprise source systems and consumer systems for master and reference data, such as CRM, ERP, and Data Warehouse/Business Intelligence.\nAt least 2 to 3 years of experience as an MDM developer using Informatica MDM or Reltio MDM, along with strong proficiency in SQL.\n\n\nGood-to-Have Skills:\nExperience with ETL tools such as Apache Spark, and various Python packages related to data processing, machine learning model development.\nGood understanding of data modeling, data warehousing, and data integration concepts.\nExperience with development using Python, React JS, cloud data platforms.\nCertified Data Engineer / Data Analyst (preferred on Databricks or cloud environments).\n\n\nSoft Skills:\nExcellent critical-thinking and problem-solving skills\nGood communication and collaboration skills\nDemonstrated awareness of how to function in a team setting\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'Business Intelligence', 'Data Warehouse', 'cloud data platforms', 'Databricks', 'ETL', 'React JS', 'Python']",2025-06-11 05:34:17
Data Scientist,Top Rated Firm in IT Services Domain,3 - 8 years,5-7 Lacs P.A.,['Hyderabad( Nanakramguda )'],"Key Responsibilities:\nDesign and develop machine learning models and algorithms to solve business problems\nWrite clean, efficient, and reusable Python code for data processing and model deployment\nCollaborate with data engineers and product teams to integrate models into production systems\nAnalyze large datasets to derive insights, trends, and patterns\nEvaluate model performance and continuously improve through retraining and tuning\nCreate dashboards, reports, and data visualizations as needed\nMaintain documentation and ensure code quality and version control\n\nPreference\nMust have hands-on experience in building, training, and deploying AI/ML models using relevant frameworks and tools within a Linux environment.\nStrong proficiency in Python with hands-on experience in data science libraries (NumPy, Pandas, Scikit-learn, TensorFlow/PyTorch, etc.)\nExperience working with Hugging Face Transformers, spaCy, ChatGPT (OpenAI APIs), and DeepSeek LLMs for building NLP or generative AI solutions\nSolid understanding of machine learning, statistics, and data modeling\nExperience with data preprocessing, feature engineering, and model evaluation\nFamiliarity with SQL and working with structured/unstructured data\nKnowledge of APIs, data pipelines, and cloud platforms (AWS, GCP, or Azure) is a plus",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Python', 'Tensorflow', 'Pyspark', 'Artificial Intelligence', 'Natural Language Processing', 'Jupyter Notebook', 'Machine Learning', 'Scikit-Learn', 'Numpy', 'SQL', 'Pytorch', 'Pandas', 'AWS']",2025-06-11 05:34:19
Data Analyst / Data Scientist,Nybl,1 - 4 years,Not Disclosed,[],"nybl is looking for our next generation of data scientists. We pride ourselves on growing our team and are always looking for the brightest talent to join us. Attitude is the most important trait we are looking for above all else.\n\nYou will be working on transforming data into intelligence by developing innovative Artificial Intelligence (AI) solutions and integrating them with cutting-edge Internet of Things (IoT) technologies. Candidates must prove that they have the will, determination and ambition to be part of a team thats going to be the next Camel of the Middle East.\n\nresponsibilities\n\nwork closely with nybl to identify issues and use data to propose solutions for effective decision making\nbuild algorithms and design experiments to merge, manage, interrogate and extract data to supply tailored reports to colleagues, customers or the wider organisation\nuse machine learning tools and statistical techniques to produce solutions to problems\ntest data mining models to select the most appropriate ones for use on a project\nmaintain clear and coherent communication, both verbal and written, to understand data needs and report results\ncreate clear reports that tell compelling stories about how customers or clients work with the business\nassess the effectiveness of data sources and data-gathering techniques and improve data collection methods\nhorizon scan to stay up to date with the latest technology, techniques and methods\nconduct research from which youll develop prototypes and proof of concepts\nstay curious and enthusiastic about using algorithms to solve problems and enthuse others to see the benefit of your work.\n\nrequired skills, abilities, education, and experience\n\nExperience and knowledge in statistical and data mining techniques using (e.g., python, R, SQL)\nExperience and knowledge in applying advance Machine Learning techniques (e.g., Neural networks, supervised and unsupervised ML, computer vision and image processing, text analysis)\nExperience and knowledge in big data analysis and management and distributed computing tools (e.g., Hadoop, Hive, Spark)\nExperience and knowledge in one or more programming languages (C, C#, Java)\nExperience and knowledge in web development frameworks (javascript, React, node.js).\nExperience analyzing data from 3rd party providers: (e.g., Google Analytics, Site Catalyst, Facebook Insights)\nExperience visualizing/presenting data for stakeholders using: Periscope, Business Objects, D3, ggplot, etc.\nExperience working with and creating data architectures.\nKnowledge of a variety of machine learning techniques (clustering, decision tree learning, artificial neural networks, etc.) and their real-world advantages/drawbacks.\nKnowledge of advanced statistical techniques and concepts (regression, properties of distributions, statistical tests and proper usage, etc.) and experience with applications.\nExcellent written and verbal communication skills for coordinating across teams.\nA drive to learn and master new technologies and techniques.\nWillingness to learn new technology.\nAble to work independently on researching solutions and applying findings.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Business objects', 'Google Analytics', 'Image processing', 'Web development', 'Machine learning', 'Data collection', 'Data mining', 'SQL', 'Python']",2025-06-11 05:34:21
Data Analyst,Colruyt It Consultancy India,5 - 8 years,10-20 Lacs P.A.,['Coimbatore'],"We are seeking a highly skilled Senior Data Analyst with8-10 years of experience to join our dynamic team. The ideal candidate will utilize their expertise in data analysis, visualization, and business intelligence to support data-driven decision-making across the organization. You will be responsible for transforming data into actionable insights that drive business strategies and improve operational efficiency.\nKey Responsibilities:\nData Collection & Preparation:\n- Collect, clean, integrate, and manage data from multiple sources to build a comprehensive data landscape.\n- Ensure data accuracy, validation, and reliability through rigorous quality checks.\nData Analysis:\n- Conduct exploratory and in-depth data analysis to identify trends, patterns, and anomalies.\n- Provide solutions to specific business challenges by using statistical techniques and advanced data analysis.\nDashboard & Report Development:\n- Design, develop, and maintain interactive dashboards and reports using Power BI, making data accessible and actionable.\n- Customize reports based on business needs and ensure that visualizations are user-friendly and insightful.\nBusiness Insights & Recommendations:\n- Interpret data findings to generate actionable insights and recommendations that align with business objectives.\n- Communicate findings clearly to both technical and non-technical stakeholders.\nCollaboration & Stakeholder Management:\n- Collaborate with cross-functional teams, to meet data needs and drive business growth.\n- Serve as a data expert, guiding teams on best practices for data analysis and reporting.\nData Documentation & Quality Assurance:\n- Document data sources, transformations, and reporting processes.\n- Implement data quality checks and address data integrity issues by collaborating with relevant teams.\nMentoring & Leadership:\n- Mentor junior analysts and provide guidance on analytical tools, techniques, and best practices.\n- Leadcross-functional projects that require advanced analytical expertise.\nRequirements & Qualifications:\nEducation: Bachelors or Masters degree in Data Science, Statistics, Mathematics, Computer Science, or related field.\nExperience:\n8-10 year: Bachelors or Masters degree in Data Science, Statistics, Mathematics, Computer Science, or related field.\nProven experience with data visualization tools (Power BI, Tableau) and data querying languages(SQL, Python).\nTechnical Skills:\n- Strong proficiency in Power BI, including DAX and Power Query for creating interactive dashboards.\n- Experience inbuilding reports in Business Objects (SAP BO)\n- Advanced knowledge of SQL for querying and managing large datasets.\n- Experience with Python for data manipulation and advanced analytics.\n- Knowledge of ETL processes and tools for data integration and transformation.\n- Experience in Tableau reporting is an advantage.\n- Familiarity with cloud platforms like Azure, and experience with data warehouses and data lakes.\nSoft Skills:\n- Excellent analytical and problem-solving skills with attention to detail.\n- Strong communication and presentation skills to convey complex data insights to diverse audiences.\n- Ability to work independently, manage multiple projects, and collaborate across teams.\n\nPreferred Qualifications:\n- Experience in the retail industry.\n- Familiarity with machine learning techniques for predictive analytics is a plus.\nImpact & Contributions:\n- Decision-Making: Enhance decision-making processes by providing data-driven insights and guiding business strategies.\n- Data-Driven Culture: Contribute to fostering a data-driven culture by embedding insights into actions.\n- Stakeholder Collaboration: Effectively manage relationships with stakeholders across varying levels of data maturity, ensuring comprehensive understanding and application of insights.\nThis role offers an opportunity to influence key business decisions through advanced data analysis, reporting and a collaborative approach.\n\nJoin us in driving impactful changes and supporting the growth of our data-driven initiatives.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Report Development', 'Power Query', 'Data Analysis', 'Dax', 'ETL', 'Power Bi', 'Data Extraction', 'Data Collection', 'Tableau', 'SQL', 'Data Visualization', 'Dashboard Development', 'Data Analytics', 'Data Reporting']",2025-06-11 05:34:22
People Analytics Data and Systems Specialist,BP INCORPORATE INTERNATIONAL.,7 - 8 years,Not Disclosed,['Pune'],"Grade H - Office/ CoreResponsible for managing an HR Services team ensuring excellence in service delivery which may include ensuring effective employee query and problem/escalation resolution, resolving complex queries and problems raised by the customer, providing expertise on process and policy applicability or providing advice and guidance on reporting content and the reporting processes.\nEntity:\nPeople, Culture & Communications\n\nHR Group\n\nJob Description:\nHR Solution supports bp s People Analytics team to provide HR Strategic reporting, MI, analytics and insights to senior leadership and key collaborators across the business. The team manages a Data Lake environment to store and consolidate data ingested from systems of record from a range of domains including core HCM, Talent, Learning, Resourcing and Payroll. We are seeking an Analyst who will be collectively responsible for the ingestion, transformation and management of Data Lifecycle within the Data Lake to support reporting and analytics output and Data Governance. Candidate having experience with Workday reporting will be preferred.\nKey Responsibilities:\nData Process Management\nProvide domain expertise across all areas of operational activity to the GBS HR Services teams in Pune and Kuala Lumpur\nProvide support for all operational processes relating to ingestion, transformation and management of data within the Data Lake\nAct as a customer concern point to support the GBS HR Services team in Kuala Lumpur, including data investigation and solve\nEnsure all activities are completed in compliance with data privacy regulations\nLead on ad hoc data activities as required\nFormulating management techniques for quality data collection to ensure adequacy, accuracy and legitimacy of data\nCreating and enforcing policies for effective data management\nSupervise and analyze information and data systems and evaluate their performance to discover ways of enhancing them (new technologies, upgrades etc.)\nEnsure digital databases and archives are protected from security breaches and data losses\nChange Management Delivery & Continuous Improvement\nManage process improvements and technical changes to data models to support and develop the growth of bp s strategic People Analytics offering, including requirement gathering, impact assessment, liaising with DevOps team for solutions, testing, coordinating team members to complete large scale testing where required, managing cutover activities and providing collaborator updates\nManage the backlog, contribute to prioritization, support sprint planning, plan resources, run retrospective sessions\nIndependently see opportunities and lead activities to simplify existing processes, improve efficiency and increase value\nShare standard process:\nCompile clear and detailed solution documentation and process documentation\nDeliver and support knowledge transfer across the team\nProvide coaching to team members\nProject Delivery\nIndependently lead individual projects or work streams within larger programs where applicable to deliver improvements to bp s People Analytics platform and processes\nSupport all project related activities, including planning, resource identification and co-ordination, leading/participating in stand ups, setting success criteria, tracking and progress reporting\nLiaise with, and co-ordinate activities between, members of the People Analytics team, GBS teams, other teams within Services & Solutions and DevOps teams to meet project goals and respond to critical issue often dealing with ambiguity and requiring definition of new/complex procedures\nEnsure excellent and pro-active partner management (up to Senior Level Leaders)\nDeputize for the Data Lead where appropriate\ncrucial EDUCATION:\nDegree level education or equivalent experience with 7-8 yrs of strong data, systems or process background\nExperience of supporting reporting or analytics, ideally within data warehousing or big data solutions\nProficiency in SQL to query data sets, investigate and understand data problems\nExperience with Python scripts\nExperience of different data domains and working with large volumes of data\nExperience of application management, system design, change/release management\nProject delivery experience\nExperience of working as part of a remote team, especially within a service delivery environment\ncrucial EXPERIENCE AND JOB REQUIREMENTS:\nKnowledge/experience of Microsoft Azure\nCompetence in Power BI and Power App\nExperience of working with HR/people data and data privacy principles\nExperience of working within an AGILE framework\nAttitudes\nOwn your success - Demonstrates leadership and is accountable for driving team performance; Inspires teams to evaluate and enhance delivery of business outcomes; Models safe and ethical work practices and a culture of transparency\nThink big - Actively seeks opportunities to transform and enhance processes and systems; Constantly seeks ways to transform, improve and innovate; Builds the skills and knowledge of the team and promotes a collaborative team environment\nBe curious - Encourages a culture of curiosity across teams; Ensures the delivery and improvement of digital solutions to benefit customers; Supports the team to try new technologies, fail and learn fast and implement solutions at pace\nEffortless customer experiences - Drives team to understand customer needs and deliver digital seamless self-service customer experiences\nDigital first - Keeps up to date with digital innovation and seeks digital solutions for problems,\nEncourages the team to deliver creative digital solutions\nKey Competencies:\nAbility to support or manage multiple areas of activity simultaneously\nRigorous attention to detail, appetite to learn and hands on approach\nMethodical approach to work, strong problem-solving skills and innovative thinking\nAlignment to high standards and strong ownership of responsibilities\nAbility to quickly assimilate understanding of data and system architecture, especially including complex system logic\nGood communication skills and ability to work with both technical and non-technical stakeholders\nDESIRABLE CRITERIA:\nCandidate having experience with applications like Power BI and Power App will be preferred.\nFamiliarity with modern database and information system technologies\nAn analytical approach with problem-solving skills",Industry Type: Oil & Gas,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Payroll', 'Change management', 'Data management', 'Information security', 'Analytical', 'Data collection', 'Operations', 'Analytics', 'SQL', 'Process management']",2025-06-11 05:34:24
Devops AWS DATA Engineeer|| Technical Analyst || 12Lakhs CTC,Robotics Technologies,6 - 10 years,11-12 Lacs P.A.,['Hyderabad( Banjara hills )'],"We are seeking a highly skilled Devops Engineer to join our dynamic development team. In this role, you will be responsible for designing, developing, and maintaining both frontend and backend components of our applications using Devops and associated technologies.\nYou will collaborate with cross-functional teams to deliver robust, scalable, and high-performing software solutions that meet our business needs. The ideal candidate will have a strong background in devops, experience with modern frontend frameworks, and a passion for full-stack development.\n\nRequirements:\nBachelor's degree in Computer Science Engineering, or a related field.\n6 to 10+ years of experience in full-stack development, with a strong focus on DevOps.\n\nDevOps with AWS Data Engineer - Roles & Responsibilities:\nUse AWS services like EC2, VPC, S3, IAM, RDS, and Route 53.\nAutomate infrastructure using Infrastructure as Code (IaC) tools like Terraform or AWS CloudFormation.\nBuild and maintain CI/CD pipelines using tools AWS CodePipeline, Jenkins,GitLab CI/CD.\nCross-Functional Collaboration\nAutomate build, test, and deployment processes for Java applications.\nUse Ansible, Chef, or AWS Systems Manager for managing configurations across environments.\nContainerize Java apps using Docker.\nDeploy and manage containers using Amazon ECS, EKS (Kubernetes), or Fargate.\nMonitoring & Logging using Amazon CloudWatch,Prometheus + Grafana,E\nStack (Elasticsearch, Logstash, Kibana),AWS X-Ray for distributed tracing manage access with IAM roles/policies.\nUse AWS Secrets Manager / Parameter Store for managing credentials.\nEnforce security best practices, encryption, and audits.\nAutomate backups for databases and services using AWS Backup, RDS Snapshots, and S3 lifecycle rules.\nImplement Disaster Recovery (DR) strategies.\nWork closely with development teams to integrate DevOps practices.\nDocument pipelines, architecture, and troubleshooting runbooks.\nMonitor and optimize AWS resource usage.\nUse AWS Cost Explorer, Budgets, and Savings Plans.\n\nMust-Have Skills:\nExperience working on Linux-based infrastructure.\nExcellent understanding of Ruby, Python, Perl, and Java.\nConfiguration and managing databases such as MySQL, Mongo.\nExcellent troubleshooting.\nSelecting and deploying appropriate CI/CD tools\nWorking knowledge of various tools, open-source technologies, and cloud services.\nAwareness of critical concepts in DevOps and Agile principles.\nManaging stakeholders and external interfaces.\nSetting up tools and required infrastructure.\nDefining and setting development, testing, release, update, and support processes for DevOps operation.\nHave the technical skills to review, verify, and validate the software code developed in the project.\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Iac', 'Devops', 'Jenkins', 'AWS', 'Kubernetes', 'RDS', 'Aws Cloudformation', 'Amazon Cloudwatch', 'Prometheus', 'Ci/Cd', 'Grafana', 'DR', 'Cloud Trail', 'Docker', 'IAM', 'Ansible / Chef', 'fargate', 'Gitlab', 'Monitoring', 'Python']",2025-06-11 05:34:26
Devops AWS DATA Engineeer|| Technical Analyst || 12Lakhs CTC,Robotics Technologies,5 - 9 years,11-12 Lacs P.A.,['Hyderabad( Banjara hills )'],"We are seeking a highly skilled Devops Engineer to join our dynamic development team. In this role, you will be responsible for designing, developing, and maintaining both frontend and backend components of our applications using Devops and associated technologies.\nYou will collaborate with cross-functional teams to deliver robust, scalable, and high-performing software solutions that meet our business needs. The ideal candidate will have a strong background in devops, experience with modern frontend frameworks, and a passion for full-stack development.\n\nRequirements:\nBachelor's degree in Computer Science Engineering, or a related field.\n5 to 9+ years of experience in full-stack development, with a strong focus on DevOps.\n\nDevOps with AWS Data Engineer - Roles & Responsibilities:\nUse AWS services like EC2, VPC, S3, IAM, RDS, and Route 53.\nAutomate infrastructure using Infrastructure as Code (IaC) tools like Terraform or AWS CloudFormation.\nBuild and maintain CI/CD pipelines using tools AWS CodePipeline, Jenkins,GitLab CI/CD.\nCross-Functional Collaboration\nAutomate build, test, and deployment processes for Java applications.\nUse Ansible, Chef, or AWS Systems Manager for managing configurations across environments.\nContainerize Java apps using Docker.\nDeploy and manage containers using Amazon ECS, EKS (Kubernetes), or Fargate.\nMonitoring & Logging using Amazon CloudWatch,Prometheus + Grafana,E\nStack (Elasticsearch, Logstash, Kibana),AWS X-Ray for distributed tracing manage access with IAM roles/policies.\nUse AWS Secrets Manager / Parameter Store for managing credentials.\nEnforce security best practices, encryption, and audits.\nAutomate backups for databases and services using AWS Backup, RDS Snapshots, and S3 lifecycle rules.\nImplement Disaster Recovery (DR) strategies.\nWork closely with development teams to integrate DevOps practices.\nDocument pipelines, architecture, and troubleshooting runbooks.\nMonitor and optimize AWS resource usage.\nUse AWS Cost Explorer, Budgets, and Savings Plans.\n\nMust-Have Skills:\nExperience working on Linux-based infrastructure.\nExcellent understanding of Ruby, Python, Perl, and Java.\nConfiguration and managing databases such as MySQL, Mongo.\nExcellent troubleshooting.\nSelecting and deploying appropriate CI/CD tools\nWorking knowledge of various tools, open-source technologies, and cloud services.\nAwareness of critical concepts in DevOps and Agile principles.\nManaging stakeholders and external interfaces.\nSetting up tools and required infrastructure.\nDefining and setting development, testing, release, update, and support processes for DevOps operation.\nHave the technical skills to review, verify, and validate the software code developed in the project.\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Iac', 'Devops', 'Jenkins', 'AWS', 'Kubernetes', 'RDS', 'Aws Cloudformation', 'Amazon Cloudwatch', 'Prometheus', 'Ci/Cd', 'Grafana', 'DR', 'Cloud Trail', 'Docker', 'IAM', 'Ansible / Chef', 'fargate', 'Gitlab', 'Monitoring', 'Python']",2025-06-11 05:34:28
Data Tester,Creditsafe,3 - 6 years,Not Disclosed,['Hyderabad'],"Role\nWe are looking for a Test Engineer who will become part of our team building and testing the Creditsafe data. You will be working closely with the database teams and data engineering to build specific systems facilitating the extraction and\ntransformation of Creditsafe data. Based on the test strategy and approach you will develop, enhance and execute tests\nthat add value to Creditsafe data. You will act as a primary source of guidance to Junior Test Engineers and Test\nEngineers in all areas of data quality. You will contribute to the team using data quality best practices and techniques.\nYou can confidently communicate test results with your team members and stakeholders using evidence and reports. You\nact as a mentor and coach to the less experienced members of the test team. You will promote and coach leading practices in data test management, design, and implementation. You will be part of an Agile team and will effectively contribute to the ceremonies, acting as the quality specialist within that team. You are an influencer and will provide leadership in defining and implementing agreed standards and will actively promote this within your team and the wider\ndevelopment community.\nThe ideal candidate has extensive experience in mentorship and leading by example and is able to communicate values consistentwith the Creditsafe philosophy of engagement. You have critical thinking skills and can diplomatically communicate within, and outside their areas of responsibility, challenging assumptions where required. Required Skills\nProven working experience as a data test engineer or business data analyst or ETL tester.\nTechnical expertise regarding data models, database design development, data mining and segmentation\ntechniques\nStrong knowledge of and experience with SQL databases\nHands on experience of best engineering practices (handling and logging errors, system monitoring and building\nhuman-fault-tolerant applications)\nKnowledge of statistics and experience using statistical packages for analysing datasets (Excel, SPSS, SAS etc.) is an advantage.\nComfortable working with relational databases such as Redshift, Oracle, PostgreSQL, MySQL, and MariaDB (PostgreSQL preferred)\nRequired Skills\nProven working experience as a data test engineer or business data analyst or ETL tester.\nTechnical expertise regarding data models, database design development, data mining and segmentationtechniques\nStrong knowledge of and experience with SQL databases\nHands on experience of best engineering practices (handling and logging errors, system monitoring and buildinghuman-fault-tolerant applications)\nKnowledge of statistics and experience using statistical packages for analysing datasets (Excel, SPSS, SAS etc.) isan advantage.\nComfortable working with relational databases such as Redshift, Oracle, PostgreSQL, MySQL, and MariaDB(PostgreSQL preferred)Required Skills\nProven working experience as a data test engineer or business data analyst or ETL tester.\nTechnical expertise regarding data models, database design development, data mining and segmentationtechniques\nStrong knowledge of and experience with SQL databases\nHands on experience of best engineering practices (handling and logging errors, system monitoring and buildinghuman-fault-tolerant applications)\nKnowledge of statistics and experience using statistical packages for analysing datasets (Excel, SPSS, SAS etc.) isan advantage.\nComfortable working with relational databases such as Redshift, Oracle, PostgreSQL, MySQL, and MariaDB(PostgreSQL preferred)Strong analytical skills with the ability to collect, organise, analyse, and disseminate significant amounts ofinformation with attention to detail and accuracy\nAdept at queries, report writing and presenting findings.\nBS in Mathematics, Economics, Computer Science, Information Management or Statistics is desirable but notessential\nA good understanding of cloud technology, preferably AWS and/or Azure DevOps\nA practical understanding of programming JavaScript, Python\nExcellent communication skills\nPractical experience of testing in an Agile approachDesirable Skills\nAn understanding of version control systems\nPractical experience of conducting code reviews\nPractical experience of pair testing and pair programmingStrong analytical skills with the ability to collect, organise, analyse, and disseminate significant amounts ofinformation with attention to detail and accuracy\nAdept at queries, report writing and presenting findings.\nBS in Mathematics, Economics, Computer Science, Information Management or Statistics is desirable but notessential\nA good understanding of cloud technology, preferably AWS and/or Azure DevOps\nA practical understanding of programming JavaScript, Python\nExcellent communication skills\nPractical experience of testing in an Agile approachDesirable Skills\nAn understanding of version control systems\nPractical experience of conducting code reviews\nPractical experience of pair testing and pair programming",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['JavaScript', 'PostgreSQL', 'MySQL', 'MariaDB', 'Oracle', 'Redshift', 'Python']",2025-06-11 05:34:30
Data Analyst,Talent Hire It Solutions,5 - 10 years,Not Disclosed,"['Kochi', 'Thiruvananthapuram']","Collaborate with business stakeholders to understand data needs and translate them into analytical\nrequirements.\nAnalyze large datasets to uncover trends, patterns, and actionable insights.\nDesign and build dashboards and reports using Power BI.\nPerform ad-hoc analysis and develop data-driven narratives to support decision-making.\nEnsure data accuracy, consistency, and integrity through data validation and quality checks.\nBuild and maintain SQL queries, views, and data models for reporting purposes.\nCommunicate findings clearly through presentations, visualizations, and written summaries.\nPartner with data engineers and architects to improve data pipelines and architecture.\nContribute to the definition of KPIs, metrics, and data governance standards.\n\nJob Specification / Skills and Competencies\nBachelors or Master’s degree in Statistics, Mathematics, Computer Science,\nEconomics, or a related field.\n5+ years of experience in a data analyst or business intelligence role.\nAdvanced proficiency in SQL and experience working with relational databases (e.g.,\nSQL Server, Redshift, Snowflake).\n\nJob Description\n\n2\n\nHands-on experience in Power BI.\nProficiency in Python, Excel and data storytelling.\nUnderstanding of data modelling, ETL concepts, and basic data architecture.\nStrong analytical thinking and problem-solving skills.\nExcellent communication and stakeholder management skills\nTo adhere to the Information Security Management policies and procedures.\n\nSoft Skills Required\nMust be a good team player with good communication skills\nMust have good presentation skills\nMust be a pro-active problem solver and a leader by self\nManage & nurture a team of data engineersRole & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SQL', 'Power BI', 'Amazon Athena', 'Python']",2025-06-11 05:34:31
"Data Modeller, Healthcare",Digital Convergence Technologies,4 - 9 years,20-35 Lacs P.A.,['Pune'],"Data/ETL Architect/Data Modeler:\nDevelop conceptual, logical, and physical data models to ensure accurate, scalable, and optimized data structures aligned with business requirements.\nCollaborate with business and technical teams to define data flow, transformation rules, and ensure alignment with data governance and quality standards.\nDesign end-to-end ETL architecture and data integration solutions.\nTechnologies - SQL, ETL, Big Data, ER Studio Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: Miscellaneous,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Modelling', 'Healthcare Domain', 'ERwin']",2025-06-11 05:34:33
Data Architect,Quincy Compressor,6 - 12 years,Not Disclosed,['Pune'],"As a Data Architect, you'll design and optimize data architecture to ensure data is accurate, secure, and accessible. you'll collaborate across teams to shape the data strategy, implement governance, and promote best practices enabling the business to gain insights, innovate, and make data-driven decisions at scale.\n  Your responsibilites\nResponsible for defining the enterprise data architecture which streamlines, standardises, and enhances accessibility of organisational data.\nElicits data requirements from senior Business stakeholders and the broader IS function, translating their needs into conceptual, logical, and physical data models.\nOversees the effective integration of data from various sources, ensuring data quality and consistency.\nMonitors and optimises data performance, collaborating with Data Integration and Product teams to deliver changes that improve data performance.\nSupports the Business, Data Integration Platforms team and wider IS management to define a data governance framework that sets out how data will be governed, accessed, and secured across the organisation; supports the operation of the data governance model as a subject matter advisor.\nProvides advisory to Data Platform teams in defining the Data Platform architecture, providing advisory on metadata, data integration, business intelligence, and data storage needs.\nSupports the Data Integration Platforms team, and other senior IS stakeholders to define a data vision and strategy, setting out how the organisation will exploit its data for maximum Business value.\nBuilds and maintains a repository of data architecture artefacts (eg, data dictionary).\nWhat we're Looking For\nProven track record in defining enterprise data architectures, data models, and database/data warehouse solutions.\nEvidenced ability to advise on the use of key data platform architectural components (eg, Azure Lakehouse, Data Bricks, etc) to deliver and optimise the enterprise data architecture.\nExperience in data integration technologies, real-time data ingestion, and API-based integrations.\nExperience in SQL and other database management systems.\nStrong problem-solving skills for interpreting complex data requirements and translating them into feasible data architecture solutions and models.\nExperience in supporting the definition of an enterprise data vision and strategy, advising on implications and/or uplifts required to the enterprise data architecture.\nExperience designing and establishing data governance models and data management practices, ensuring data is correct and secure whilst still being accessible, in line with regulations and wider organisational policies.\nAble to present complex data-related initiatives and issues to senior non-data conversant audiences.\nProven experience working with AI and Machine Learning models preferred, but not essential.\nWhat We Can Offer You\nWe support your growth within the role, department, and across the company through internal opportunities.\nWe offer a hybrid working model, allowing you to combine remote work with the opportunity to connect with your team in modern, welcoming office spaces.\nWe encourage continuous learning with access to online platforms (eg, LinkedIn Learning), language courses, soft skills training, and various we'llbeing initiatives, including workshops and webinars.\nJoin a diverse and inclusive work environment where your ideas are valued and your contributions make a difference.",Industry Type: Industrial Equipment / Machinery,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['metadata', 'Architecture', 'Data management', 'Data Architect', 'data governance', 'Data quality', 'Business intelligence', 'Information technology', 'SQL', 'Data architecture']",2025-06-11 05:34:35
Master Data Analyst - Reporting,Heinz,3 - 5 years,Not Disclosed,['Ahmedabad'],"Responsibilities\nBe a team player\nDedicated to Source (Catalyst & Keystone) to GFIN (Target) Reconciliations, GFIN vs HFM Reconciliations, and research discrepancies between the systems\nWork with Data Engineering and Power BI developers to create recon dashboards and general maintenance of dashboards\nCreate and update process documentation\nSupport Internal Audit requests\nParticipate in ad hoc project requests\nManagement of Change Champions for MDG - to ensure the smooth flowing of new Data Objects via the MDG process\nGovernance of new Data Objects, and Values\nManagement of effective input capture and central overview data files\nExperience\n3 to 5 years experience\nProactive, self-starter - driving agenda and opportunities for improvement\nSimplification in comms and actions necessary to take to solve issues\nQuestion and call out when data look unusual\nAutonomy to review and research on their own and support findings\nExcel savvy - experience working with frequent and large data sets\nSome systems experience preferred (SAP, HFM, Oracle, Snowflake, Power BI)\nExperience of Data Governance and systems related to DG\nHas proven Networks in KH - knows who to speak to\nWorking knowledge of products, and how these fit together with supply elements\nUnderstanding of data rules by functional areas\nConfidence in using applications, some systems experience preferred - SAP, HFM, Oracle, Snowflake, Power BI\nHours - Hybrid\nUS hours - during close week\nHybrid hours - same schedule as current team during non close week\nLocation(s) Ahmedabad - Mondeal Heights - GBS Center",Industry Type: Food Processing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Manager Internal Audit', 'SAP', 'Excel', 'Process documentation', 'data governance', 'power bi', 'Data Analyst', 'Research', 'Management', 'Oracle']",2025-06-11 05:34:36
Urgent- Data Governance Analyst (Pharmaceuticals Industry)- Mumbai,-,6 - 11 years,Not Disclosed,['Mumbai (All Areas)'],"Hiring with a leading pharmaceuticals manufacturing industry for their office in Mumbai\n\nJob Title: Data Governance Analyst\n\nLocation: Mumbai, India (On-site)\nExperience: 5-14 Years in Data Governance, including SAP ECC/S4 HANA\n\nKey Responsibilities:\nSAP S4 HANA Implementation & Data Governance:\nWork closely with the SAP S4 HANA implementation team to ensure proper data governance during the brownfield conversion and subsequent functional transformation phases.\nDefine and implement data governance best practices within SAP ECC and SAP S4 HANA.\nSupport data migration strategies, ensuring accuracy, consistency, and completeness of data during the transition from SAP ECC6.0 to SAP S4 HANA.\nManage data cleansing, deduplication, validation, and enrichment processes specific to SAP Master Data (Material, Vendor, Customer, Finance).\nData Governance Policy & Framework Development:\nDevelop and implement data governance policies, standards, and procedures in compliance with industry standards (e.g., DAMA-DMBOK, GDPR, CCPA, Indian Data Protection Bill, GxP compliance).\nEstablish data governance frameworks, including data dictionaries, metadata repositories, and data lineage documentation within SAP and non-SAP ecosystems.\nAlign governance strategies with SAP best practices, ensuring regulatory compliance in healthcare and pharma sectors.\nData Quality Management:\nPerform data profiling, validation, and cleansing within SAP modules such as Material Master, Vendor Master, Customer Master, and Financial Data (FBL1N, FBL3N, FBL5N).\nIdentify and resolve data quality issues related to duplicate records, missing fields, and non-standardized data entries.\nDefine and monitor KPIs for data quality improvement.\nLeverage SAP MDG, Information Steward, and Data Services for data governance and quality enhancement.\nMaster Data Management (MDM):\nLead MDM initiatives within SAP S4 HANA by enforcing data standardization and deduplication rules.\nCollaborate with business stakeholders to maintain consistent master data across business units.\nImplement data lifecycle management strategies for customer, vendor, and product data.\nGovern material classifications, product hierarchies, and financial data structures.\nData Integration & Governance across Enterprise Systems:\nEnsure seamless data integration between SAP S4 HANA, legacy systems, and third-party applications.\nOversee governance of structured and unstructured data from multiple sources (SAP, CRM, external databases, cloud storage, etc.).\nDesign and implement a data governance framework that spans multiple SAP modules, ensuring consistency and compliance.\nCompliance & Risk Mitigation:\nEnsure adherence to data privacy and security standards applicable to the healthcare industry (HIPAA, GDPR, Indian Data Protection laws).\nMitigate risks related to regulatory non-compliance, data breaches, and financial misreporting.\nImplement access control and governance policies in SAP to prevent unauthorized data modifications.\nTraining & Change Management:\nProvide training and guidance to business users and data stewards on SAP data governance best practices.\nDrive user adoption of SAP Fiori apps and other governance tools for data management.\nSupport change management initiatives to promote awareness and compliance with data governance policies.\nMonitoring & Continuous Improvement:\nConduct regular audits to ensure compliance with governance standards.\nUse data governance dashboards (Power BI, SAP Analytics Cloud) to track data quality and performance.\nImplement real-time monitoring and alert mechanisms within SAP for proactive data quality management.\nRequirements:\nExperience in SAP Data Governance: Strong understanding of SAP ECC6.0 and SAP S4 HANA data structures, including material master, vendor master, finance master, and transaction data.\nProficiency in Data Management Tools: Experience with SAP MDG, Data Services, Information Steward, SQL, Power BI, and metadata management tools.\nStrong Analytical Skills: Ability to analyse complex data sets and identify patterns, inconsistencies, and improvement areas.\nKnowledge of Regulatory Frameworks: Familiarity with data privacy regulations (GDPR, CCPA, Indian data protection laws, HIPAA, GxP compliance for pharma).\nExcellent Communication & Collaboration Skills: Ability to engage with cross-functional teams including IT, business units, compliance, and SAP implementation teams.\nHands-on Experience in Data Migration & Quality Management: Proven track record of successful data migration projects within SAP environments.\nAbility to Lead Data Governance Initiatives: Experience in defining governance policies, setting up frameworks, and ensuring cross-functional adoption of best practices.\n\nPreferred Qualifications:\nSAP S4 HANA certification in Data Governance, MDM, or equivalent.\nExposure to healthcare and pharma industry data governance standards.\nExperience working with SAP Fiori-based applications and SAP Analytics Cloud.\n\nIf Interested, Kindly share your updated cv on shweta@topgearconsultants.com\nCurrent company name\nCurrent CTC\nExpected CTC\nNotice Period",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Governance framework', 'Data Governance Analyst', 'automation scripts', 'Data Quality', 'MDM', 'Data Governance', 'Python']",2025-06-11 05:34:38
Data & AI/ML Architect,"Milestone Technologies, Inc",14 - 19 years,Not Disclosed,['Kochi'],"Job Summary:\nWe are seeking a highly experienced and visionary Databricks Data Architect with over 14 years in data engineering and architecture, including deep hands-on experience in designing and scaling Lakehouse architectures using Databricks . The ideal candidate will possess deep expertise across data modeling, data governance, real-time and batch processing, and cloud-native analytics using the Databricks platform. You will lead the strategy, design, and implementation of modern data architecture to drive enterprise-wide data initiatives and maximize the value from the Databricks platform.\nKey Responsibilities:\nLead the architecture, design, and implementation of scalable and secure Lakehouse solutions using Databricks and Delta Lake .\nDefine and implement data modeling best practices , including medallion architecture (bronze/silver/gold layers).\nChampion data quality and governance frameworks leveraging Databricks Unity Catalog for metadata, lineage, access control, and auditing.\nArchitect real-time and batch data ingestion pipelines using Apache Spark Structured Streaming , Auto Loader , and Delta Live Tables (DLT) .\nDevelop reusable templates, workflows, and libraries for data ingestion, transformation, and consumption across various domains.\nCollaborate with enterprise data governance and security teams to ensure compliance with regulatory and organizational data standards.\nPromote self-service analytics and data democratization by enabling business users through Databricks SQL and Power BI/Tableau integrations .\nPartner with Data Scientists and ML Engineers to enable ML workflows using MLflow , Feature Store , and Databricks Model Serving .\nProvide architectural leadership for enterprise data platforms, including performance optimization , cost governance , and CI/CD automation in Databricks.\nDefine and drive the adoption of DevOps/MLOps best practices on Databricks using Databricks Repos , Git , Jobs , and Terraform .\nMentor and lead engineering teams on modern data platform practices, Spark performance tuning , and efficient Delta Lake optimizations (Z-ordering, OPTIMIZE, VACUUM, etc.) .\nTechnical Skills:\n10+ years in Data Warehousing, Data Architecture, and Enterprise ETL design .\n5+ years hands-on experience with Databricks on Azure/AWS/GCP , including advanced Apache Spark and Delta Lake .\nStrong command of SQL, PySpark, and Spark SQL for large-scale data transformation.\nProficiency with Databricks Unity Catalog , Delta Live Tables , Autoloader , DBFS , Jobs , and Workflows .\nHands-on experience with Databricks SQL and integration with BI tools (Power BI, Tableau, etc.).\nExperience implementing CI/CD on Databricks , using tools like Git , Azure DevOps , Terraform , and Databricks Repos .\nProficient with streaming architecture using Spark Structured Streaming , Kafka , or Event Hubs/Kinesis .\nUnderstanding of ML lifecycle management with MLflow , and experience in deploying MLOps solutions on Databricks.\nFamiliarity with cloud object stores (e.g., AWS S3, Azure Data Lake Gen2) and data lake architectures .\nExposure to data cataloging and metadata management using Unity Catalog or third-party tools.\nKnowledge of orchestration tools like Airflow , Databricks Workflows , or Azure Data Factory .\nExperience with Docker/Kubernetes for containerization (optional, for cross-platform knowledge).\nPreferred Certifications (a plus):\nDatabricks Certified Data Engineer Associate/Professional\nDatabricks Certified Lakehouse Architect\nMicrosoft Certified: Azure Data Engineer / Azure Solutions Architect\nAWS Certified Data Analytics - Specialty\nGoogle Professional Data Engineer",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Automation', 'metadata', 'GIT', 'Architecture', 'Data modeling', 'Data quality', 'microsoft', 'SQL', 'Data architecture']",2025-06-11 05:34:40
Data Modeller,Inspira Enterprise India,2 - 6 years,Not Disclosed,['Kolkata'],"Data Modeling Design:\n\nDesign and maintain conceptual, logical, and physical data models to support transactional, analytical, and data warehousing systems.\n\nDevelop data models to ensure data consistency, integrity, and quality across multiple banking functions and applications.\n\nDefine and implement the best practices for data modeling, data integration, and metadata management.\n\n\nData Architecture Collaboration:\n\nCollaborate with data architects to align models with enterprise data architecture and ensure optimal performance and scalability.\n\nWork with database administrators to translate logical models into physical database structures and optimize them for performance.\n\n\nData Quality Governance:\n\nEstablish data standards, definitions, and quality rules to ensure data accuracy, consistency, and compliance.\n\nCreate and maintain data dictionaries and metadata repositories to support data governance and facilitate efficient data access.\n\n\nStakeholder Engagement:\n\nEngage with business stakeholders, data scientists, and IT teams to understand data requirements and translate business needs into robust data models.\n\nEnsure data models support key business initiatives, such as regulatory reporting, analytics, and operational efficiency.\n\n\nDocumentation Best Practices:\n\nDevelop and maintain detailed documentation, including data models, entity-relationship diagrams, and mapping specifications.\n\nImplement data modeling standards and mentor team members to promote best practices in data management.\n\n\n\n\n\n\nRequirements:\n\n\nEducational Qualification:\n\nMBA / Engineering degree with relevant industry experience.\n\n\nExperience:\n\nMinimum of 10 years of experience in data modeling or as a data modeler within the banking industry.\n\nProven expertise in designing data models across at least three of the following areas:\n\nData Warehousing\n\nAnalytics BI\n\nData Mining Data Quality\n\nMetadata Management\n\n\n\nTechnical Skills:\n\nProficiency in data modeling tools such as Erwin, IBM InfoSphere, or SAP PowerDesigner.\n\nStrong SQL skills and experience in relational database systems like Oracle, SQL Server, or DB2.\n\nFamiliarity with big data technologies and NoSQL databases is a plus.\n\nKnowledge of ETL processes and tools (e. g. , Informatica, Talend) and experience working with BI tools (e. g. , Tableau, Power BI).\n\nKnowledge of SAS for analytics, data manipulation, and data management is a plus.\n\nStrong understanding of data governance frameworks, data quality management, and regulatory compliance.\n\n\nSoft Skills:\n\nStrong analytical and problem-solving skills, with attention to detail and accuracy.\n\nExcellent communication and interpersonal skills, with the ability to translate technical data concepts for business stakeholders.\n\nProven ability to work collaboratively in a cross-functional environment and manage multiple projects.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['metadata', 'SAP', 'SAS', 'Data management', 'Db2', 'Data modeling', 'Data quality', 'Informatica', 'Data mining', 'Analytics']",2025-06-11 05:34:41
