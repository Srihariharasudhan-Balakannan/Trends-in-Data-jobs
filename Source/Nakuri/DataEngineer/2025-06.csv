job_title,company,experience,salary,locations,description,industry,department,employment_type,skills,scraped_at
Data Engineer,PwC India,4 - 8 years,Not Disclosed,['Bengaluru'],"If Interested please fill the below application link : https://forms.office.com/r/Zc8wDfEGEH\n\n\nResponsibilities:\nDeliver projects integrating data flows within and across technology systems.\nLead data modeling sessions with end user groups, project stakeholders, and technology teams to produce logical and physical data models.",,,,"['Pyspark', 'Aws Cloud', 'Azure Cloud', 'Python']",2025-06-11 23:55:44
Data Engineer - Python/SQL,Wipro,3 - 5 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\n\n\n\nDo\n1. Instrumental in understanding the requirements and design of the product/ software\nDevelop software solutions by studying information needs, studying systems flow, data usage and work processes\nInvestigating problem areas followed by the software development life cycle\nFacilitate root cause analysis of the system issues and problem statement\nIdentify ideas to improve system performance and impact availability\nAnalyze client requirements and convert requirements to feasible design\nCollaborate with functional teams or systems analysts who carry out the detailed investigation into software requirements\nConferring with project managers to obtain information on software capabilities\n\n\n\n2. Perform coding and ensure optimal software/ module development\nDetermine operational feasibility by evaluating analysis, problem definition, requirements, software development and proposed software\nDevelop and automate processes for software validation by setting up and designing test cases/scenarios/usage cases, and executing these cases\nModifying software to fix errors, adapt it to new hardware, improve its performance, or upgrade interfaces.\nAnalyzing information to recommend and plan the installation of new systems or modifications of an existing system\nEnsuring that code is error free or has no bugs and test failure\nPreparing reports on programming project specifications, activities and status\nEnsure all the codes are raised as per the norm defined for project / program / account with clear description and replication patterns\nCompile timely, comprehensive and accurate documentation and reports as requested\nCoordinating with the team on daily project status and progress and documenting it\nProviding feedback on usability and serviceability, trace the result to quality risk and report it to concerned stakeholders\n\n\n\n3. Status Reporting and Customer Focus on an ongoing basis with respect to project and its execution\nCapturing all the requirements and clarifications from the client for better quality work\nTaking feedback on the regular basis to ensure smooth and on time delivery\nParticipating in continuing education and training to remain current on best practices, learn new programming languages, and better assist other team members.\nConsulting with engineering staff to evaluate software-hardware interfaces and develop specifications and performance requirements\nDocument and demonstrate solutions by developing documentation, flowcharts, layouts, diagrams, charts, code comments and clear code\nDocumenting very necessary details and reports in a formal way for proper understanding of software from client proposal to implementation\nEnsure good quality of interaction with customer w.r.t. e-mail content, fault report tracking, voice calls, business etiquette etc\nTimely Response to customer requests and no instances of complaints either internally or externally\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure\n1.Continuous Integration, Deployment & Monitoring of Software100% error free on boarding & implementation, throughput %, Adherence to the schedule/ release plan\n2.Quality & CSATOn-Time Delivery, Manage software, Troubleshoot queries,Customer experience, completion of assigned certifications for skill upgradation\n3.MIS & Reporting100% on time MIS & report generation\nMandatory Skills: Python for Insights.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'module development', 'Data Engineering', 'software development', 'software programs', 'SQL']",2025-06-12 00:39:28
Data Engineer,IT Services Company,2 - 3 years,6-7 Lacs P.A.,['Pune'],"Data Engineer\nJob Description :\nJash Data Sciences: Letting Data Speak!\nDo you love solving real-world data problems with the latest and best techniques? And having fun while solving them in a team! Then come and join our high-energy team of passionate data people. Jash Data Sciences is the right place for you.\nWe are a cutting-edge Data Sciences and Data Engineering startup based in Pune, India. We believe in continuous learning and evolving together. And we let the data speak!\nWhat will you be doing?\nYou will be discovering trends in the data sets and developing algorithms to transform\nraw data for further analytics\nCreate Data Pipelines to bring in data from various sources, with different formats,\ntransform it, and finally load it to the target database.\nImplement ETL/ ELT processes in the cloud using tools like AirFlow, Glue, Stitch, Cloud\nData Fusion, and DataFlow.\nDesign and implement Data Lake, Data Warehouse, and Data Marts in AWS, GCP, or\nAzure using Redshift, BigQuery, PostgreSQL, etc.\nCreating efficient SQL queries and understanding query execution plans for tuning\nqueries on engines like PostgreSQL.\nPerformance tuning of OLAP/ OLTP databases by creating indices, tables, and views.\nWrite Python scripts for the orchestration of data pipelines\nHave thoughtful discussions with customers to understand their data engineering\nrequirements. Break complex requirements into smaller tasks for execution.\nWhat do we need from you?\nStrong Python coding skills with basic knowledge of algorithms/data structures and\ntheir application.\nStrong understanding of Data Engineering concepts including ETL, ELT, Data Lake, Data\nWarehousing, and Data Pipelines.\nExperience designing and implementing Data Lakes, Data Warehouses, and Data Marts\nthat support terabytes of scale data.\nA track record of implementing Data Pipelines on public cloud environments\n(AWS/GCP/Azure) is highly desirable\nA clear understanding of Database concepts like indexing, query performance\noptimization, views, and various types of schemas.\nHands-on SQL programming experience with knowledge of windowing functions,\nsubqueries, and various types of joins.\nExperience working with Big Data technologies like PySpark/ Hadoop\nA good team player with the ability to communicate with clarity\nShow us your git repo/ blog!\nQualification\n1-2 years of experience working on Data Engineering projects for Data Engineer I\n2-5 years of experience working on Data Engineering projects for Data Engineer II\n1-5 years of Hands-on Python programming experience\nBachelors/Masters' degree in Computer Science is good to have\nCourses or Certifications in the area of Data Engineering will be given a higher preference.\nCandidates who have demonstrated a drive for learning and keeping up to date with technology by continuing to do various courses/self-learning will be given high preference.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Airflow', 'Elt', 'Data Mart', 'Data Pipeline', 'ETL', 'Pyspark', 'Hadoop', 'Data Bricks', 'SQL', 'Data Fusion', 'Glue', 'GCP', 'Data Flow', 'Data Warehousing', 'Azzure', 'AWS']",2025-06-12 00:39:30
Data Engineer,AMERICAN EXPRESS,2 - 4 years,13-17 Lacs P.A.,"['Gurugram', 'Delhi / NCR']","Role & responsibilities\nUnderstanding business use cases and be able to convert to technical design\nPart of a cross-disciplinary team, working closely with other data engineers, software engineers, data scientists, data managers and business partners.\nYou will be designing scalable, testable and maintainable data pipelines\nIdentify areas for data governance improvements and help to resolve data quality problems through the appropriate choice of error detection and correction, process control and improvement, or process design changes",,,,"['Spark', 'SQL', 'Python', 'Hadoop', 'Big Data']",2025-06-12 00:39:33
Senior Cloud Data Engineer,PwC India,10 - 15 years,12-22 Lacs P.A.,['Bengaluru'],"Role & responsibilities\n\nExperienced Senior Data Engineer utilizing Big Data & Google Cloud technologies to develop large scale, on-cloud data processing pipelines and data warehouses with Overall 12 to 15 years of experience\nHave 3 to 4 years of experience of leading Data Engineer teams developing enterprise grade data processing pipelines on multi Clouds like GCP and AWS",,,,"['Pyspark', 'Python', 'SQL', 'Datafactory', 'GCP', 'Microsoft Azure', 'ETL', 'AWS', 'Data Bricks']",2025-06-12 00:39:37
Data Engineer,AMERICAN EXPRESS,3 - 8 years,Not Disclosed,['Chennai'],"You Lead the Way. We've Got Your Back.\n\nWith the right backing, people and businesses have the power to progress in incredible ways. When you join Team Amex, you become part of a global and diverse community of colleagues with an unwavering commitment to back our customers, communities and each other. Here, youll learn and grow as we help you create a career journey thats unique and meaningful to you with benefits, programs, and flexibility that support you personally and professionally.\nAt American Express, you’ll be recognized for your contributions, leadership, and impact—every colleague has the opportunity to share in the company’s success. Together, we’ll win as a team, striving to uphold our company values and powerful backing promise to provide the world’s best customer experience every day. And we’ll do it with the utmost integrity, and in an environment where everyone is seen, heard and feels like they belong. As part of our diverse tech team, you can architect, code and ship software that makes us an essential part of our customers’ digital lives. Here, you can work alongside talented engineers in an open, supportive, inclusive environment where your voice is valued, and you make your own decisions on what tech to use to solve challenging problems. Amex offers a range of opportunities to work with the latest technologies and encourages you to back the broader engineering community through open source. And because we understand the importance of keeping your skills fresh and relevant, we give you dedicated time to invest in your professional development. Find your place in technology on #TeamAmex.",,,,"['Data Engineering', 'GCP', 'Airflow', 'Pyspark', 'Bigquery', 'Hadoop', 'Big Data', 'SQL', 'Python', 'Backend Development']",2025-06-12 00:39:40
Data Engineer,Bajaj Financial Securities,2 - 5 years,Not Disclosed,['Pune'],"We're Hiring: Data Engineer | 25 Years Experience | AWS + Real-time Focus\nJoin our fast-moving team as a Data Engineer where you'll build scalable, real-time data pipelines, own cloud infrastructure, and collaborate across teams to drive data-first decisions.\nIf you're strong in Python, experienced with streaming platforms like Kafka/Kinesis, and have shipped cloud-native data pipelines (preferably AWS) — we want to hear from you.\nMust-Haves:\n2–5 years of experience in Data Engineering\nPython (Pandas, PySpark, async), SQL, ETL/ELT\nStreaming experience (Kafka/Kinesis)\nAWS cloud stack (Glue, Lambda, S3, Athena)\nExperience in APIs, data warehousing, and data modelling\nBonus if you know: Docker, Kubernetes, Airflow/dbt, or have a background in MLOps.Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Aws Lambda', 'Docker', 'Cloud Platform', 'Data Warehousing', 'Python', 'Pyspark', 'Api Gateway', 'Kinesis', 'Kafka', 'ETL', 'SQL', 'Kubernetes']",2025-06-12 00:39:44
Data Engineer,"NTT DATA, Inc.",1 - 4 years,Not Disclosed,['Bengaluru'],"Req ID: 321498\n\nWe are currently seeking a Data Engineer to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\nJob Duties""¢ Work closely with Lead Data Engineer to understand business requirements, analyse and translate these requirements into technical specifications and solution design.\n""¢ Work closely with Data modeller to ensure data models support the solution design\n""¢ Develop , test and fix ETL code using Snowflake, Fivetran, SQL, Stored proc.\n""¢ Analysis of the data and ETL for defects/service tickets (for solution in production ) raised and service tickets.\n""¢ Develop documentation and artefacts to support projects\n\nMinimum Skills Required""¢ ADF\n""¢ Fivetran (orchestration & integration)\n""¢ SQL\n""¢ Snowflake DWH",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'data engineering', 'sql', 'oracle adf', 'etl', 'hive', 'python', 'data analysis', 'oracle', 'informatica powercenter', 'amazon redshift', 'talend', 'data warehousing', 'power bi', 'plsql', 'tableau', 'data modeling', 'spark', 'etl tool', 'technical specifications', 'hadoop', 'informatica', 'unix']",2025-06-12 00:39:47
Consultant-Data Engineer (Only from Pharma/Lifescience/Biotech domain),Chryselys,6 - 11 years,Not Disclosed,['Hyderabad'],"Job Description for Consultant - Data Engineer\nAbout Us:\nChryselys is a Pharma Analytics & Business consulting company that delivers data-driven insights leveraging AI-powered, cloud-native platforms to achieve high-impact transformations.\nWe specialize in digital technologies and advanced data science techniques that provide strategic and operational insights.\nWho we are:\nPeople - Our team of industry veterans, advisors and senior strategists have diverse backgrounds and have worked at top tier companies.\nQuality - Our goal is to deliver the value of a big five consulting company without the big five cost.\nTechnology - Our solutions are Business centric built on cloud native technologies.\nKey Responsibilities and Core Competencies:\n•      You will be responsible for managing and delivering multiple Pharma projects.\n•      Leading a team of atleast 8 members, resolving their technical and business related problems and other queries.\n•      Responsible for client interaction; requirements gathering, creating required documents, development, quality assurance of the deliverables.\n•      Good collaboration with onshore and Senior folks.\n•      Should have fair understanding of Data Capabilities (Data Management, Data Quality, Master and Reference Data).\n•      Exposure to Project management methodologies including Agile and Waterfall.\n•      Experience working in RFPs would be a plus.\nRequired Technical Skills:\n•      Proficient in Python, Pyspark, SQL\n•      Extensive hands-on experience in big data processing and cloud technologies like AWS and Azure services, Databricks etc.\n•      Strong experience working with cloud data warehouses like Snowflake, Redshift, Azure etc.\n•      Good experience in ETL, Data Modelling, building ETL Pipelines.\n•      Conceptual knowledge of Relational database technologies, Data Lake, Lake Houses etc.\n•      Sound knowledge in Data operations, quality and data governance.\nPreferred Qualifications:\n•      Bachelors or master’s Engineering/ MCA or equivalent degree.\n•      6-13 years of experience as Data Engineer, with atleast 2 years in managing medium to large scale programs.\n•      Minimum 5 years of Pharma and Life Science domain exposure in IQVIA, Veeva, Symphony, IMS etc.\n•      High motivation, good work ethic, maturity, self-organized and personal initiative.\n•      Ability to work collaboratively and providing the support to the team.\n•      Excellent written and verbal communication skills.\n•      Strong analytical and problem-solving skills.\nLocation\n•      Preferably Hyderabad, India",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'AWS', 'Data Bricks', 'Python', 'SQL', 'Data Engineering', 'Microsoft Azure', 'Data Lake', 'Data Warehousing']",2025-06-12 00:39:51
Data Engineer,Aqilea Softech,5 - 9 years,13-20 Lacs P.A.,"['Bangalore Rural', 'Bengaluru']","Job Title: Data Engineer\nCompany : Aqilea India(Client : H&M India)\nEmployment Type: Full Time\nLocation: Bangalore(Hybrid)\nExperience: 4.5 to 9 years\nClient : H&M India\n\nAt H&M, we welcome you to be yourself and feel like you truly belong. Help us reimagine the future of an entire industry by making everyone look, feel, and do good. We take pride in our history of making fashion accessible to everyone and led by our values we strive to build a more welcoming, inclusive, and sustainable industry. We are privileged to have more than 120,000 colleagues, in over 75 countries across the world. Thats 120 000 individuals with unique experiences, skills, and passions. At H&M, we believe everyone can make an impact, we believe in giving people responsibility and a strong sense of ownership. Our business is your business, and when you grow, we grow.\nWebsite : https://career.hm.com/\n\nWe are seeking a skilled and forward-thinking Data Engineer to join our Emerging Tech team. This role is designed for someone passionate about working with cutting-edge technologies such as AI, machine learning, IoT, and big data to turn complex data sets into actionable insights.\nAs the Data Engineer in Emerging Tech, you will be responsible for designing, implementing, and optimizing data architectures and processes that support the integration of next-generation technologies. Your role will involve working with large-scale datasets, building predictive models, and utilizing emerging tools to enable data-driven decision-making across the business. You ll collaborate with technical and business teams to uncover insights, streamline data pipelines, and ensure the best use of advanced analytics technologies.\n\nKey Responsibilities:\nDesign and build scalable data architectures and pipelines that support machine learning, analytics, and IoT initiatives.\nDevelop and optimize data models and algorithms to process and analyse large-scale, complex data sets.\nImplement data governance, security, and compliance measures to ensure high-quality\nCollaborate with cross-functional teams (engineering, product, and business) to translate business requirements into data-driven solutions.\nEvaluate, integrate, and optimize new data technologies to enhance analytics capabilities and drive business outcomes.\nApply statistical methods, machine learning models, and data visualization techniques to deliver actionable insights.\nEstablish best practices for data management, including data quality, consistency, and scalability.\nConduct analysis to identify trends, patterns, and correlations within data to support strategic business initiatives.\nStay updated on the latest trends and innovations in data technologies and emerging data management practices.\n\nSkills Required :\nBachelors or masters degree in data science, Computer Science, Engineering, Statistics, or a related field.\n4.5-9 years of experience in data engineering, data science, or a similar analytical role, with a focus on emerging technologies.\nProficiency with big data frameworks (e.g., Hadoop, Spark, Kafka) and experience with modern cloud platforms (AWS, Azure, or GCP).\nSolid skills in Python, SQL, and optionally R, along with experience using machine learning libraries such as Scikit-learn, TensorFlow, or PyTorch.\nExperience with data visualization tools (e.g., Tableau or Power BI or D3.js) to communicate insights effectively.\nFamiliarity with IoT and edge computing data architectures is a plus.\nUnderstanding of data governance, compliance, and privacy standards.\nAbility to work with both structured and unstructured data.\nExcellent problem-solving, communication, and collaboration skills, with the ability to work in a fast-paced, cross-functional team environment.\nA passion for emerging technologies and a continuous desire to learn and innovate.\nInterested Candidates can share your Resumes to mail id karthik.prakadish@aqilea.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Powerbi', 'Hadoop', 'Kafka', 'Tableau', 'Azure', 'GCP', 'Data Engineer', 'Spark', 'AWS', 'Python', 'SQL']",2025-06-12 00:39:54
Data Engineer,Infiniti Research,3 - 7 years,22.5-25 Lacs P.A.,['Bengaluru'],"Role & responsibilities\n3-6 years of experience in Data Engineering Pipeline Ownership and Quality Assurance, with hands-on expertise in building, testing, and maintaining data pipelines.\nProficiency with Azure Data Factory (ADF), Azure Databricks (ADB), and PySpark for data pipeline orchestration and processing large-scale datasets.\nStrong experience in writing SQL queries and performing data validation, data profiling, and schema checks.\nExperience with big data validation, including schema enforcement, data integrity checks, and automated anomaly detection.\nAbility to design, develop, and implement automated test cases to monitor and improve data pipeline efficiency.\nDeep understanding of Medallion Architecture (Raw, Bronze, Silver, Gold) for structured data flow management.\nHands-on experience with Apache Airflow for scheduling, monitoring, and managing workflows.\nStrong knowledge of Python for developing data quality scripts, test automation, and ETL validations.\nFamiliarity with CI/CD pipelines for deploying and automating data engineering workflows.\nSolid data governance and data security practices within the Azure ecosystem.\n\nAdditional Requirements:\nOwnership of data pipelines ensuring end-to-end execution, monitoring, and troubleshooting failures proactively.\nStrong stakeholder management skills, including follow-ups with business teams across multiple regions to gather requirements, address issues, and optimize processes.\nTime flexibility to align with global teams for efficient communication and collaboration.\nExcellent problem-solving skills with the ability to simulate and test edge cases in data processing environments.\nStrong communication skills to document and articulate pipeline issues, troubleshooting steps, and solutions effectively.\nExperience with Unity Catalog or willingness to learn.\n\nPreferred candidate profile\nImmediate Joiner's",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ADF', 'pyspark', 'Unity Catalog', 'ADB', 'SQL', 'Medallion Architecture']",2025-06-12 00:39:58
Senior Data Engineer,S&P Global Market Intelligence,6 - 11 years,Not Disclosed,['Gurugram'],"\n\nAbout the Role: \n\nGrade Level (for internal use):\n10\n \n\nPosition summary \n\n Our proprietary software-as-a-service helps automotive dealerships and sales teams better understand and predict exactly which customers are ready to buy, the reasons why, and the key offers and incentives most likely to close the sale. Its micro-marketing engine then delivers the right message at the right time to those customers, ensuring higher conversion rates and a stronger ROI. \n\n \n\nWhat You'll Do \n\n You will be part of our Data Platform & Product Insights data engineering team. As part of this agile team, you will work in our cloud native environment to \n\n Build & support data ingestion and processing pipelines in cloud. This will entail extraction, load and transformation of big data from a wide variety of sources, both batch & streaming, using latest data frameworks and technologies \n\n Partner with product team to assemble large, complex data sets that meet functional and non-functional business requirements, ensure build out of Data Dictionaries/Data Catalogue and detailed documentation and knowledge around these data assets, metrics and KPIs. \n\n Warehouse this data, build data marts, data aggregations, metrics, KPIs, business logic that leads to actionable insights into our product efficacy, marketing platform, customer behaviour, retention etc. \n\n Build real-time monitoring dashboards and alerting systems. \n\n Coach and mentor other team members. \n\n\n \n\nWho you are \n\n 6+ years of experience in Big Data and Data Engineering. \n\n Strong knowledge of advanced SQL, data warehousing concepts and DataMart designing. \n\n Have strong programming skills in SQL, Python/ PySpark etc. \n\n Experience in design and development of data pipeline, ETL/ELT process on-premises/cloud. \n\n Experience in one of the Cloud providers GCP, Azure, AWS. \n\n Experience with relational SQL and NoSQL databases, including Postgres and MongoDB. \n\n Experience workflow management toolsAirflow, AWS data pipeline, Google Cloud Composer etc. \n\n Experience with Distributed Versioning Control environments such as GIT, Azure DevOps \n\n Building Docker images and fetch/promote and deploy to Production. Integrate Docker container orchestration framework using Kubernetes by creating pods, config Maps, deployments using terraform. \n\n Should be able to convert business queries into technical documentation. \n\n Strong problem solving and communication skills. \n\n Bachelors or an advanced degree in Computer Science or related engineering discipline. \n\n\n \n\nGood to have some exposure to \n\n Exposure to any Business Intelligence (BI) tools like Tableau, Dundas, Power BI etc. \n\n Agile software development methodologies. \n\n Working in multi-functional, multi-location teams \n\n  \n \n\nGrade10  \n \n\nLocationGurugram \n \n\nHybrid Modeltwice a week work from office  \n \n\nShift Time12 pm to 9 pm IST  \n What You'll Love About Us Do ask us about these! \n\n \n\nTotal Rewards. Monetary, beneficial and developmental rewards! \n\n \n\nWork Life Balance. You can't do a good job if your job is all you do! \n\n \n\nPrepare for the Future.Academy we are all learners; we are all teachers! \n\n \n\nEmployee Assistance Program. Confidential and Professional Counselling and Consulting. \n\n \n\nDiversity & Inclusion. HeForShe! \n\n \n\nInternal Mobility.\n\nGrow with us! \n\n  \n  \n  \n  \n\nAbout automotiveMastermind\n\nWho we are:\n\nFounded in 2012, automotiveMastermind is a leading provider of predictive analytics and marketing automation solutions for the automotive industry and believes that technology can transform data, revealing key customer insights to accurately predict automotive sales. Through its proprietary automated sales and marketing platform, Mastermind, the company empowers dealers to close more deals by predicting future buyers and consistently marketing to them. automotiveMastermind is headquartered in New York City. For more information, visit automotivemastermind.com.\n\nAt automotiveMastermind, we thrive on high energy at high speed. Were an organization in hyper-growth mode and have a fast-paced culture to match. Our highly engaged teams feel passionately about both our product and our people. This passion is what continues to motivate and challenge our teams to be best-in-class. Our cultural values of Drive and Help have been at the core of what we do, and how we have built our culture through the years. This cultural framework inspires a passion for success while collaborating to win.\n\nWhat we do:\n\nThrough our proprietary automated sales and marketing platform, Mastermind, we empower dealers to close more deals by predicting future buyers and consistently marketing to them. In short, we help automotive dealerships generate success in their loyalty, service, and conquest portfolios through a combination of turnkey predictive analytics, proactive marketing, and dedicated consultative services.\n\nWhats In It For\n\nYou\n\nOur Purpose:\n\nProgress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technologythe right combination can unlock possibility and change the world.Our world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence, pinpointing risks and opening possibilities. We Accelerate Progress.\n\nOur People:\n\nOur Values:\n\nIntegrity, Discovery, Partnership\n\nAt S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of\n\nintegrity in all we do, bring a spirit of\n\ndiscovery to our work, and collaborate in close\n\npartnership with each other and our customers to achieve shared goals.\n\nBenefits:\n\nWe take care of you, so you cantake care of business. We care about our people. Thats why we provide everything youand your careerneed to thrive at S&P Global.\n\nHealth & WellnessHealth care coverage designed for the mind and body.\n\n\n\nContinuous LearningAccess a wealth of resources to grow your career and learn valuable new skills.\n\nInvest in Your FutureSecure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\n\nFamily Friendly PerksIts not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\n\nBeyond the BasicsFrom retail discounts to referral incentive awardssmall perks can make a big difference.\n\nFor more information on benefits by country visithttps://spgbenefits.com/benefit-summaries",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['advance sql', 'python', 'pyspark', 'sql', 'data warehousing concepts', 'kubernetes', 'microsoft azure', 'data warehousing', 'power bi', 'relational sql', 'elt', 'data engineering', 'business intelligence', 'azure devops', 'docker', 'nosql', 'tableau', 'git', 'postgresql', 'gcp', 'agile', 'big data', 'aws', 'mongodb']",2025-06-12 00:40:01
Data Engineer,Databeat,3 - 7 years,Not Disclosed,['Hyderabad( Rai Durg )'],"Experience Required: 3+ years\n\nTechnical knowledge: AWS, Python, SQL, S3, EC2, Glue, Athena, Lambda, DynamoDB, RedShift, Step Functions, Cloud Formation, CI/CD Pipelines, Github, EMR, RDS,AWS Lake Formation, GitLab, Jenkins and AWS CodePipeline.\n\n\n\nRole Summary: As a Senior Data Engineer,with over 3 years of expertise in Python, PySpark, SQL to design, develop and optimize complex data pipelines, support data modeling, and contribute to the architecture that supports big data processing and analytics to cutting-edge cloud solutions that drive business growth. You will lead the design and implementation of scalable, high-performance data solutions on AWS and mentor junior team members.This role demands a deep understanding of AWS services, big data tools, and complex architectures to support large-scale data processing and advanced analytics.\nKey Responsibilities:\nDesign and develop robust, scalable data pipelines using AWS services, Python, PySpark, and SQL that integrate seamlessly with the broader data and product ecosystem.\nLead the migration of legacy data warehouses and data marts to AWS cloud-based data lake and data warehouse solutions.\nOptimize data processing and storage for performance and cost.\nImplement data security and compliance best practices, in collaboration with the IT security team.\nBuild flexible and scalable systems to handle the growing demands of real-time analytics and big data processing.\nWork closely with data scientists and analysts to support their data needs and assist in building complex queries and data analysis pipelines.\nCollaborate with cross-functional teams to understand their data needs and translate them into technical requirements.\nContinuously evaluate new technologies and AWS services to enhance data capabilities and performance.\nCreate and maintain comprehensive documentation of data pipelines, architectures, and workflows.\nParticipate in code reviews and ensure that all solutions are aligned to pre-defined architectural specifications.\nPresent findings to executive leadership and recommend data-driven strategies for business growth.\nCommunicate effectively with different levels of management to gather use cases/requirements and provide designs that cater to those stakeholders.\nHandle clients in multiple industries at the same time, balancing their unique needs.\nProvide mentoring and guidance to junior data engineers and team members.\n\n\n\nRequirements:\n3+ years of experience in a data engineering role with a strong focus on AWS, Python, PySpark, Hive, and SQL.\nProven experience in designing and delivering large-scale data warehousing and data processing solutions.\nLead the design and implementation of complex, scalable data pipelines using AWS services such as S3, EC2, EMR, RDS, Redshift, Glue, Lambda, Athena, and AWS Lake Formation.\nBachelor's or Masters degree in Computer Science, Engineering, or a related technical field.\nDeep knowledge of big data technologies and ETL tools, such as Apache Spark, PySpark, Hadoop, Kafka, and Spark Streaming.\nImplement data architecture patterns, including event-driven pipelines, Lambda architectures, and data lakes.\nIncorporate modern tools like Databricks, Airflow, and Terraform for orchestration and infrastructure as code.\nImplement CI/CD using GitLab, Jenkins, and AWS CodePipeline.\nEnsure data security, governance, and compliance by leveraging tools such as IAM, KMS, and AWS CloudTrail.\nMentor junior engineers, fostering a culture of continuous learning and improvement.\nExcellent problem-solving and analytical skills, with a strategic mindset.\nStrong communication and leadership skills, with the ability to influence stakeholders at all levels.\nAbility to work independently as well as part of a team in a fast-paced environment.\nAdvanced data visualization skills and the ability to present complex data in a clear and concise manner.\nExcellent communication skills, both written and verbal, to collaborate effectively across teams and levels.\n\nPreferred Skills:\nExperience with Databricks, Snowflake, and machine learning pipelines.\nExposure to real-time data streaming technologies and architectures.\nFamiliarity with containerization and serverless computing (Docker, Kubernetes, AWS Lambda).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Aws Glue', 'SQL', 'Data Pipeline', 'Python', 'Amazon Ec2', 'Data Engineering', 'Data Bricks', 'Aws Lambda', 'Amazon Redshift', 'Azure Cloud', 'Data Lake', 'Data Modeling', 'Athena']",2025-06-12 00:40:04
Senior Data Engineer,Qualcomm,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Area: Information Technology Group, Information Technology Group > IT Data Engineer\n\nGeneral Summary:\n\nDeveloper will play an integral role in the PTEIT Machine Learning Data Engineering team. Design, develop and support data pipelines in a hybrid cloud environment to enable advanced analytics. Design, develop and support CI/CD of data pipelines and services. - 5+ years of experience with Python or equivalent programming using OOPS, Data Structures and Algorithms - Develop new services in AWS using server-less and container-based services. - 3+ years of hands-on experience with AWS Suite of services (EC2, IAM, S3, CDK, Glue, Athena, Lambda, RedShift, Snowflake, RDS) - 3+ years of expertise in scheduling data flows using Apache Airflow - 3+ years of strong data modelling (Functional, Logical and Physical) and data architecture experience in Data Lake and/or Data Warehouse - 3+ years of experience with SQL databases - 3+ years of experience with CI/CD and DevOps using Jenkins - 3+ years of experience with Event driven architecture specially on Change Data Capture - 3+ years of Experience in Apache Spark, SQL, Redshift (or) Big Query (or) Snowflake, Databricks - Deep understanding building the efficient data pipelines with data observability, data quality, schema drift, alerting and monitoring. - Good understanding of the Data Catalogs, Data Governance, Compliance, Security, Data sharing - Experience in building the reusable services across the data processing systems. - Should have the ability to work and contribute beyond defined responsibilities - Excellent communication and inter-personal skills with deep problem-solving skills.\n\nMinimum Qualifications:\n3+ years of IT-related work experience with a Bachelor's degree in Computer Engineering, Computer Science, Information Systems or a related field.\nOR\n5+ years of IT-related work experience without a Bachelors degree.\n\n2+ years of any combination of academic or work experience with programming (e.g., Java, Python).\n1+ year of any combination of academic or work experience with SQL or NoSQL Databases.\n1+ year of any combination of academic or work experience with Data Structures and algorithms.\n5 years of Industry experience and minimum 3 years experience in Data Engineering development with highly reputed organizations- Proficiency in Python and AWS- Excellent problem-solving skills- Deep understanding of data structures and algorithms- Proven experience in building cloud native software preferably with AWS suit of services- Proven experience in design and develop data models using RDBMS (Oracle, MySQL, etc.)\n\nDesirable - Exposure or experience in other cloud platforms (Azure and GCP) - Experience working on internals of large-scale distributed systems and databases such as Hadoop, Spark - Working experience on Data Lakehouse platforms (One House, Databricks Lakehouse) - Working experience on Data Lakehouse File Formats (Delta Lake, Iceberg, Hudi)\n\nBachelor's or Master's degree in Computer Science, Software Engineering, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['algorithms', 'python', 'data quality', 'data structures', 'aws', 'schema', 'continuous integration', 'glue', 'amazon redshift', 'event driven architecture', 'ci/cd', 'data engineering', 'sql', 'alerts', 'java', 'data modeling', 'spark', 'devops', 'data flow', 'nosql databases', 'sql database']",2025-06-12 00:40:06
Data Engineer Senior Consultant,"NTT DATA, Inc.",8 - 13 years,Not Disclosed,['Bengaluru'],"Req ID: 327863\n\nWe are currently seeking a Data Engineer Senior Consultant to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\nJob DutiesResponsibilitiesLead the development of backend systems using Django. Design and implement scalable and secure APIs. Integrate Azure Cloud services for application deployment and management. Utilize Azure Databricks for big data processing and analytics. Implement data processing pipelines using PySpark. Collaborate with front-end developers, product managers, and other stakeholders to deliver comprehensive solutions. Conduct code reviews and ensure adherence to best practices. Mentor and guide junior developers. Optimize database performance and manage data storage solutions. Ensure high performance and security standards for applications. Participate in architecture design and technical decision-making. Minimum Skills RequiredQualificationsBachelor's degree in Computer Science, Information Technology, or a related field. 8+ years of experience in backend development. 8+ years of experience with Django. Proven experience with Azure Cloud services. Experience with Azure Databricks and PySpark. Strong understanding of RESTful APIs and web services. Excellent communication and problem-solving skills. Familiarity with Agile methodologies. Experience with database management (SQL and NoSQL).\n\nSkills:\nDjango, Python, Azure Cloud, Azure Databricks, Delta Lake and Delta tables, PySpark, SQL/NoSQL databases, RESTful APIs, Git, and Agile methodologies",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['web services', 'rest', 'python', 'backend development', 'django', 'azure cloud services', 'pyspark', 'jquery', 'sql', 'git', 'asp.net', 'ssrs', 'web api', 'wcf', 'big data', 'agile methodology', 'azure databricks', 'c#', 'ssas', 'data engineering', 'azure cloud', 'sql server', 'javascript', 'nosql', 'linq', 'ssis']",2025-06-12 00:40:09
Senior GenAI Data Engineer,"NTT DATA, Inc.",2 - 5 years,Not Disclosed,"['New Delhi', 'Chennai', 'Bengaluru']","Your day at NTT DATA\nSenior GenAI Data Engineer\nWe are seeking an experienced Senior Data Engineer to join our team in delivering cutting-edge Generative AI (GenAI) solutions to clients. The successful candidate will be responsible for designing, developing, and deploying data pipelines and architectures that support the training, fine-tuning, and deployment of LLMs for various industries. This role requires strong technical expertise in data engineering, problem-solving skills, and the ability to work effectively with clients and internal teams.\nWhat you'll be doing\nKey Responsibilities:\nDesign, develop, and manage data pipelines and architectures to support GenAI model training, fine-tuning, and deployment\nData Ingestion and Integration: Develop data ingestion frameworks to collect data from various sources, transform, and integrate it into a unified data platform for GenAI model training and deployment.\nGenAI Model Integration: Collaborate with data scientists to integrate GenAI models into production-ready applications, ensuring seamless model deployment, monitoring, and maintenance.\nCloud Infrastructure Management: Design, implement, and manage cloud-based data infrastructure (e.g., AWS, GCP, Azure) to support large-scale GenAI workloads, ensuring cost-effectiveness, security, and compliance.\nWrite scalable, readable, and maintainable code using object-oriented programming concepts in languages like Python, and utilize libraries like Hugging Face Transformers, PyTorch, or TensorFlow\nPerformance Optimization: Optimize data pipelines, GenAI model performance, and infrastructure for scalability, efficiency, and cost-effectiveness.\nData Security and Compliance: Ensure data security, privacy, and compliance with regulatory requirements (e.g., GDPR, HIPAA) across data pipelines and GenAI applications.\nClient Collaboration: Collaborate with clients to understand their GenAI needs, design solutions, and deliver high-quality data engineering services.\nInnovation and R&D: Stay up to date with the latest GenAI trends, technologies, and innovations, applying research and development skills to improve data engineering services.\nKnowledge Sharing: Share knowledge, best practices, and expertise with team members, contributing to the growth and development of the team.\nRequirements:\nBachelors degree in computer science, Engineering, or related fields (Master's recommended)\nExperience with vector databases (e.g., Pinecone, Weaviate, Faiss, Annoy) for efficient similarity search and storage of dense vectors in GenAI applications\n5+ years of experience in data engineering, with a strong emphasis on cloud environments (AWS, GCP, Azure, or Cloud Native platforms)\nProficiency in programming languages like SQL, Python, and PySpark\nStrong data architecture, data modeling, and data governance skills\nExperience with Big Data Platforms (Hadoop, Databricks, Hive, Kafka, Apache Iceberg), Data Warehouses (Teradata, Snowflake, BigQuery), and lakehouses (Delta Lake, Apache Hudi)\nKnowledge of DevOps practices, including Git workflows and CI/CD pipelines (Azure DevOps, Jenkins, GitHub Actions)\nExperience with GenAI frameworks and tools (e.g., TensorFlow, PyTorch, Keras)\nNice to have:\nExperience with containerization and orchestration tools like Docker and Kubernetes\nIntegrate vector databases and implement similarity search techniques, with a focus on GraphRAG is a plus\nFamiliarity with API gateway and service mesh architectures\nExperience with low latency/streaming, batch, and micro-batch processing\nFamiliarity with Linux-based operating systems and REST APIs\nLocation: Delhi or Bangalore\nWorkplace type:\nHybrid Working",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GenAI', 'hive', 'continuous integration', 'kubernetes', 'ci/cd', 'pyspark', 'data architecture', 'sql', 'docker', 'tensorflow', 'git', 'data modeling', 'gcp', 'devops', 'linux', 'jenkins', 'pytorch', 'keras', 'hadoop', 'bigquery', 'python', 'microsoft azure', 'data engineering', 'data bricks', 'data governance', 'aws']",2025-06-12 00:40:12
Big Data Developer/Data Engineer,Grid Dynamics,5 - 10 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\nExperience: 5 - 8 years\nEmployment Type: Full-Time\n\nJob Summary:\nWe are looking for a highly skilled Scala and Spark Developer to join our data engineering team. The ideal candidate will have strong experience in building scalable data processing solutions using Apache Spark and writing robust, high-performance applications in Scala. You will work closely with data scientists, data analysts, and product teams to design, develop, and optimize large-scale data pipelines and ETL workflows.\n\nKey Responsibilities:\nDevelop and maintain scalable data processing pipelines using Apache Spark and Scala.\nWork on batch and real-time data processing using Spark (RDD/DataFrame/Dataset).\nWrite efficient and maintainable code following best practices and coding standards.\nCollaborate with cross-functional teams to understand data requirements and implement solutions.\nOptimize performance of Spark jobs and troubleshoot data-related issues.\nIntegrate data from multiple sources and ensure data quality and consistency.\nParticipate in design reviews, code reviews, and provide technical leadership when needed.\nContribute to data modeling, schema design, and architecture discussions.\nRequired Skills:\nStrong programming skills in Scala.\nExpertise in Apache Spark (Core, SQL, Streaming).\nHands-on experience with distributed computing and large-scale data processing.\nExperience with data formats like Parquet, Avro, ORC, and JSON.\nGood understanding of functional programming concepts.\nFamiliarity with data ingestion tools (Kafka, Flume, Sqoop, etc.).\nExperience working with Hadoop ecosystem (HDFS, Hive, YARN, etc.) is a plus.\nStrong SQL skills and experience working with relational and NoSQL databases.\nExperience with version control tools like Git.\nPreferred Qualifications:\nBachelor's or Masters degree in Computer Science, Engineering, or related field.\nExperience with cloud platforms like AWS, Azure, or GCP (especially EMR, Databricks, etc.).\nKnowledge of containerization (Docker, Kubernetes) is a plus.\nFamiliarity with CI/CD tools and DevOps practices.ndidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Scala', 'Pyspark', 'Spark']",2025-06-12 00:40:14
Data Engineer,"NTT DATA, Inc.",4 - 9 years,Not Disclosed,['Chennai'],"Req ID: 324631\n\nWe are currently seeking a Data Engineer to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\n\nKey Responsibilities:\n\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\n\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\n\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\n\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\n\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\n\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\n\nAdhere to Agile practices throughout the solution development process.\n\nDesign, build, and deploy databases and data stores to support organizational requirements.\n\nBasic Qualifications:\n\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\n\nExperience with Informatica, Python, Databricks, Azure Data Engineer\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\n\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\n\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\n\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\n\nUndergraduate or Graduate degree preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'nosql', 'elastic search', 'cassandra', 'solr', 'core data', 'cloudera', 'python', 'data analytics', 'scala', 'kudu', 'microsoft azure', 'data engineering', 'data bricks', 'gen', 'java', 'apache nifi', 'spark', 'gcp', 'kafka', 'software engineering', 'hadoop', 'aws', 'data integration']",2025-06-12 00:40:18
Data Engineer,"NTT DATA, Inc.",4 - 9 years,Not Disclosed,['Chennai'],"Req ID: 324632\n\nWe are currently seeking a Data Engineer to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\n\nKey Responsibilities:\n\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\n\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\n\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\n\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\n\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\n\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\n\nAdhere to Agile practices throughout the solution development process.\n\nDesign, build, and deploy databases and data stores to support organizational requirements.\n\nBasic Qualifications:\n\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\n\nExperience with Informatica, Python, Databricks, Azure Data Engineer\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\n\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\n\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\n\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\n\nUndergraduate or Graduate degree preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'nosql', 'elastic search', 'cassandra', 'solr', 'core data', 'cloudera', 'python', 'data analytics', 'scala', 'kudu', 'microsoft azure', 'data engineering', 'data bricks', 'gen', 'java', 'apache nifi', 'spark', 'gcp', 'kafka', 'software engineering', 'hadoop', 'aws', 'data integration']",2025-06-12 00:40:21
Data Engineer,"NTT DATA, Inc.",4 - 9 years,Not Disclosed,['Pune'],"Req ID: 324653\n\nWe are currently seeking a Data Engineer to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nKey Responsibilities:\n\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\n\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\n\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\n\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\n\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\n\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\n\nAdhere to Agile practices throughout the solution development process.\n\nDesign, build, and deploy databases and data stores to support organizational requirements.\n\nBasic Qualifications:\n\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\n\nExperience with Informatica, Python, Databricks, Azure Data Engineer\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\n\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\n\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\n\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\n\nUndergraduate or Graduate degree preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'nosql', 'elastic search', 'cassandra', 'solr', 'core data', 'cloudera', 'python', 'data analytics', 'scala', 'kudu', 'microsoft azure', 'data engineering', 'data bricks', 'gen', 'java', 'apache nifi', 'spark', 'gcp', 'kafka', 'software engineering', 'hadoop', 'aws', 'data integration']",2025-06-12 00:40:24
Data Engineer,"NTT DATA, Inc.",4 - 9 years,Not Disclosed,['Pune'],"Req ID: 324609\n\nWe are currently seeking a Data Engineer to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nKey Responsibilities:\n\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\n\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\n\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\n\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\n\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\n\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\n\nAdhere to Agile practices throughout the solution development process.\n\nDesign, build, and deploy databases and data stores to support organizational requirements.\n\nBasic Qualifications:\n\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\n\nExperience with Informatica, Python, Databricks, Azure Data Engineer\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\n\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\n\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\n\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\n\nUndergraduate or Graduate degree preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'nosql', 'elastic search', 'cassandra', 'solr', 'core data', 'cloudera', 'python', 'data analytics', 'scala', 'kudu', 'microsoft azure', 'data engineering', 'data bricks', 'gen', 'java', 'apache nifi', 'spark', 'gcp', 'kafka', 'software engineering', 'hadoop', 'aws', 'data integration']",2025-06-12 00:40:28
Data/ML Ops Engineer,"NTT DATA, Inc.",2 - 5 years,Not Disclosed,['Bengaluru'],"Additional Career Level Description:\n\n\nKnowledge and application:\nSeasoned, experienced professional; has complete knowledge and understanding of area of specialization.\nUses evaluation, judgment, and interpretation to select right course of action.\n\n\n\nProblem solving:\nWorks on problems of diverse scope where analysis of information requires evaluation of identifiable factors.\nResolves and assesses a wide range of issues in creative ways and suggests variations in approach.\n\n\n\nInteraction:\nEnhances relationships and networks with senior internal/external partners who are not familiar with the subject matter often requiring persuasion.\nWorks with others outside of own area of expertise, with the ability to adapt style to differing audiences and often advises others on difficult matters.\n\n\n\nImpact:\nImpacts short to medium term goals through personal effort or influence over team members.\n\n\n\nAccountability:\nAccountable for own targets with work reviewed at critical points.\nWork is done independently and is reviewed at critical points.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ML Ops', 'python', 'spark', 'big data', 'data engineering', 'artificial intelligence', 'ml', 'sql']",2025-06-12 00:40:31
Data Engineer,Emiza Supply Chain Services,2 - 6 years,Not Disclosed,['Mumbai (All Areas)( Vidya Vihar West )'],"Role & responsibilities\n\nKey Responsibilities\n\nDesign, build, and maintain scalable data pipelines and ETL/ELT processes.\nIntegrate data from various internal and external sources (e.g., ERP, WMS, APIs).\nOptimize and monitor data flows for performance and reliability.\nCollaborate with data analysts, software developers, and business teams to understand data requirements.\nEnsure data quality, consistency, and security across the data lifecycle.\nSupport reporting, dashboarding, and data science initiatives with clean and structured data.\nMaintain data documentation and metadata repositories.\n\nPreferred candidate profile\n\nBachelor's or Masters degree in Computer Science, Engineering, or a related field.\n2+ years of experience in a data engineering or similar role.\nStrong proficiency in SQL and working with relational databases (e.g., PostgreSQL, MySQL).\nExperience with big data technologies like Spark, Hadoop, or similar is a plus.\nHands-on experience with ETL tools (e.g., Apache Airflow, Talend, DBT).\nProficiency in Python or Scala for data processing.\nFamiliarity with cloud platforms (AWS, GCP, or Azure), especially with data services like S3, Redshift, BigQuery, etc.\nKnowledge of APIs and data integration concepts.",Industry Type: Courier / Logistics,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Etl Pipelines', 'Data Engineering', 'Hadoop', 'Cloud Platform', 'Elt', 'Data Pipeline', 'Spark', 'AWS', 'Python', 'SQL']",2025-06-12 00:40:34
Senior Big Data Engineer,Qualcomm,2 - 7 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Software Engineer, you will design, develop, create, modify, and validate embedded and cloud edge software, applications, and/or specialized utility programs that launch cutting-edge, world class products that meet and exceed customer needs. Qualcomm Software Engineers collaborate with systems, hardware, architecture, test engineers, and other teams to design system-level software solutions and obtain information on performance requirements and interfaces.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\n\nGeneral Summary\n\nPreferred Qualifications\n\n3+ years of experience as a Data Engineer or in a similar role\n\nExperience with\n\ndata modeling, data warehousing, and building ETL pipelines\n\nSolid working experience with\n\nPython, AWS analytical technologies and related resources (Glue, Athena, QuickSight, SageMaker, etc.,)\n\nExperience with\n\nBig Data tools, platforms and architecture with solid working experience with SQL\n\nExperience working in a very large data warehousing environment,\n\nDistributed System.\n\nSolid understanding on various data exchange formats and complexities\n\nIndustry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets\n\nStrong data visualization skills\n\nBasic understanding of Machine Learning; Prior experience in ML Engineering a plus\n\nAbility to manage on-premises data and make it inter-operate with AWS based pipelines\n\nAbility to interface with Wireless Systems/SW engineers and understand the Wireless ML domain; Prior experience in Wireless (5G) domain a plus\n\n\nEducation\n\nBachelor's degree in computer science, engineering, mathematics, or a related technical discipline\n\nPreferred QualificationsMasters in CS/ECE with a Data Science / ML Specialization\n\n\nMinimum Qualifications:\n\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\n\nOR\n\nMaster's degree in Engineering, Information Systems, Computer Science, or related field\n\nOR\n\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n3+ years of experience with Programming Language such as C, C++, Java, Python, etc.\n\n\nDevelops, creates, and modifies general computer applications software or specialized utility programs. Analyzes user needs and develops software solutions. Designs software or customizes software for client use with the aim of optimizing operational efficiency. May analyze and design databases within an application area, working individually or coordinating database development as part of a team. Modifies existing software to correct errors, allow it to adapt to new hardware, or to improve its performance. Analyzes user needs and software requirements to determine feasibility of design within time and cost constraints. Confers with systems analysts, engineers, programmers and others to design system and to obtain information on project limitations and capabilities, performance requirements and interfaces. Stores, retrieves, and manipulates data for analysis of system capabilities and requirements. Designs, develops, and modifies software systems, using scientific analysis and mathematical models to predict and measure outcome and consequences of design.\n\nPrincipal Duties and Responsibilities:\n\nCompletes assigned coding tasks to specifications on time without significant errors or bugs.\n\nAdapts to changes and setbacks in order to manage pressure and meet deadlines.\n\nCollaborates with others inside project team to accomplish project objectives.\n\nCommunicates with project lead to provide status and information about impending obstacles.\n\nQuickly resolves complex software issues and bugs.\n\nGathers, integrates, and interprets information specific to a module or sub-block of code from a variety of sources in order to troubleshoot issues and find solutions.\n\nSeeks others' opinions and shares own opinions with others about ways in which a problem can be addressed differently.\n\nParticipates in technical conversations with tech leads/managers.\n\nAnticipates and communicates issues with project team to maintain open communication.\n\nMakes decisions based on incomplete or changing specifications and obtains adequate resources needed to complete assigned tasks.\n\nPrioritizes project deadlines and deliverables with minimal supervision.\n\nResolves straightforward technical issues and escalates more complex technical issues to an appropriate party (e.g., project lead, colleagues).\n\nWrites readable code for large features or significant bug fixes to support collaboration with other engineers.\n\nDetermines which work tasks are most important for self and junior engineers, stays focused, and deals with setbacks in a timely manner.\n\nUnit tests own code to verify the stability and functionality of a feature.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'sql', 'software engineering', 'data visualization', 'aws', 'quicksight', 'c', 'software development', 'glue', 'aws sagemaker', 'data warehousing', 'machine learning', 'business intelligence', 'data engineering', 'java', 'data science', 'data modeling', 'athena', 'wireless', 'big data', 'etl', 'ml']",2025-06-12 00:40:38
Associate Data Engineer,"NTT DATA, Inc.",1 - 3 years,Not Disclosed,"['New Delhi', 'Chennai', 'Bengaluru']","Your day at NTT DATA\nWe are seeking an experienced Data Engineer to join our team in delivering cutting-edge Generative AI (GenAI) solutions to clients. The successful candidate will be responsible for designing, developing, and deploying data pipelines and architectures that support the training, fine-tuning, and deployment of LLMs for various industries. This role requires strong technical expertise in data engineering, problem-solving skills, and the ability to work effectively with clients and internal teams.\n\nWhat youll be doing\n\nKey Responsibilities:\nDesign, develop, and manage data pipelines and architectures to support GenAI model training, fine-tuning, and deployment\nData Ingestion and Integration: Develop data ingestion frameworks to collect data from various sources, transform, and integrate it into a unified data platform for GenAI model training and deployment.\nGenAI Model Integration: Collaborate with data scientists to integrate GenAI models into production-ready applications, ensuring seamless model deployment, monitoring, and maintenance.\nCloud Infrastructure Management: Design, implement, and manage cloud-based data infrastructure (e.g., AWS, GCP, Azure) to support large-scale GenAI workloads, ensuring cost-effectiveness, security, and compliance.\nWrite scalable, readable, and maintainable code using object-oriented programming concepts in languages like Python, and utilize libraries like Hugging Face Transformers, PyTorch, or TensorFlow\nPerformance Optimization: Optimize data pipelines, GenAI model performance, and infrastructure for scalability, efficiency, and cost-effectiveness.\nData Security and Compliance: Ensure data security, privacy, and compliance with regulatory requirements (e.g., GDPR, HIPAA) across data pipelines and GenAI applications.\nClient Collaboration: Collaborate with clients to understand their GenAI needs, design solutions, and deliver high-quality data engineering services.\nInnovation and R&D: Stay up to date with the latest GenAI trends, technologies, and innovations, applying research and development skills to improve data engineering services.\nKnowledge Sharing: Share knowledge, best practices, and expertise with team members, contributing to the growth and development of the team.\n\nBachelors degree in computer science, Engineering, or related fields (Masters recommended)\nExperience with vector databases (e.g., Pinecone, Weaviate, Faiss, Annoy) for efficient similarity search and storage of dense vectors in GenAI applications\n5+ years of experience in data engineering, with a strong emphasis on cloud environments (AWS, GCP, Azure, or Cloud Native platforms)\nProficiency in programming languages like SQL, Python, and PySpark\nStrong data architecture, data modeling, and data governance skills\nExperience with Big Data Platforms (Hadoop, Databricks, Hive, Kafka, Apache Iceberg), Data Warehouses (Teradata, Snowflake, BigQuery), and lakehouses (Delta Lake, Apache Hudi)\nKnowledge of DevOps practices, including Git workflows and CI/CD pipelines (Azure DevOps, Jenkins, GitHub Actions)\nExperience with GenAI frameworks and tools (e.g., TensorFlow, PyTorch, Keras)\nNice to have:\nExperience with containerization and orchestration tools like Docker and Kubernetes\nIntegrate vector databases and implement similarity search techniques, with a focus on GraphRAG is a plus\nFamiliarity with API gateway and service mesh architectures\nExperience with low latency/streaming, batch, and micro-batch processing\nFamiliarity with Linux-based operating systems and REST APIs",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Apache Iceberg', 'Faiss', 'PySpark', 'Kafka', 'Pinecone', 'GitHub Actions', 'Snowflake', 'Apache Hudi', 'AWS', 'Azure DevOps', 'Python', 'Azure', 'BigQuery', 'Hadoop', 'Annoy', 'Teradata', 'SQL', 'Jenkins', 'Hive', 'Cloud Native platforms', 'GCP', 'Delta Lake', 'Databricks', 'Weaviate']",2025-06-12 00:40:42
Data Engineer,7dxperts,5 - 8 years,15-20 Lacs P.A.,['Bengaluru'],"Role & responsibilities\n3+ years of experience in Spark, Databricks, Hadoop, Data and ML Engineering.\n3+ Years on experience in designing architectures using AWS cloud services & Databricks.\nArchitecture, design and build Big Data Platform (Data Lake / Data Warehouse / Lake house) using Databricks services and integrating with wider AWS cloud services.\nKnowledge & experience in infrastructure as code and CI/CD pipeline to build and deploy data platform tech stack and solution.\nHands-on spark experience in supporting and developing Data Engineering (ETL/ELT) and Machine learning (ML) solutions using Python, Spark, Scala or R languages.\nDistributed system fundamentals and optimising Spark distributed computing.\nExperience in setting up batch and streams data pipeline using Databricks DLT, jobs and streams.\nUnderstand the concepts and principles of data modelling, Database, tables and can produce, maintain, and update relevant data models across multiple subject areas.\nDesign, build and test medium to complex or large-scale data pipelines (ETL/ELT) based on feeds from multiple systems using a range of different storage technologies and/or access methods, implement data quality validation and to create repeatable and reusable pipelines\nExperience in designing metadata repositories, understanding range of metadata tools and technologies to implement metadata repositories and working with metadata.\nUnderstand the concepts of build automation, implementing automation pipelines to build, test and deploy changes to higher environments.\nDefine and execute test cases, scripts and understand the role of testing and how it works.\n\nPreferred candidate profile\nBig Data technologies Databricks, Spark, Hadoop, EMR or Hortonworks.\nSolid hands-on experience in programming languages Python, Spark, SQL, Spark SQL, Spark Streaming, Hive and Presto\nExperience in different Databricks components and API like notebooks, jobs, DLT, interactive and jobs cluster, SQL warehouse, policies, secrets, dbfs, Hive Metastore, Glue Metastore, Unity Catalog and ML Flow.\nKnowledge and experience in AWS Lambda, VPC, S3, EC2, API Gateway, IAM users, roles & policies, Cognito, Application Load Balancer, Glue, Redshift, Spectrum, Athena and Kinesis.\nExperience in using source control tools like git, bit bucket or AWS code commit and automation tools like Jenkins, AWS Code build and Code deploy.\nHands-on experience in terraform and Databricks API to automate infrastructure stack.\nExperience in implementing CI/CD pipeline and ML Ops pipeline using Git, Git actions or Jenkins.\nExperience in delivering project artifacts like design documents, test cases, traceability matrix and low-level design documents.\nBuild references architectures, how-tos, and demo applications for customers.\nReady to complete certifications",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'Data Bricks', 'Python', 'ML', 'ML Engineering', 'Pyspark', 'MLops', 'Ci Cd Pipeline', 'GIT', 'Machine Learning', 'SQL']",2025-06-12 00:40:44
Data Engineer - Streamsets,Wipro,4 - 6 years,Not Disclosed,['Pune'],"Role Purpose\n\nConsultants are expected to complete specific tasks as part of a consulting project with minimal supervision. They will start to build a core areas of expertise and will contribute to client projects typically involving in-depth analysis, research, supporting solution development and being a successful communicator. The Consultant must achieve high personal billability.\n\n\n\nResponsibilities\n\nAs aDeveloper, Analyze, design and develop components, tools and custom features using Databricks and streamsets as per business needs.\n\nAnalyze, create and develop technical design to determine business functional and non-functional requirements & processes and review them with the technology leads and architects.\n\nWork collaboratively with all the teams as required to build a data model and setup things in Databricks andstreamsets as appropriate to transform the data and transfer it as appropriate.\n\nDevelop solutions to publish/subscribe Kafka topics.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'data bricks', 'web services', 'asp.net', 'api', 'requirement analysis']",2025-06-12 00:40:47
Data Engineer,Bebo Technologies,4 - 9 years,Not Disclosed,['Chandigarh'],"Design, build, and maintain scalable and reliable data pipelines on Databricks, Snowflake, or equivalent cloud platforms.\nIngest and process structured, semi-structured, and unstructured data from a variety of sources including APIs, RDBMS, and file systems.\nPerform data wrangling, cleansing, transformation, and enrichment using PySpark, Pandas, NumPy, or similar libraries.\nOptimize and manage large-scale data workflows for performance, scalability, and cost-efficiency.\nWrite and optimize complex SQL queries for transformation, extraction, and reporting.\nDesign and implement efficient data models and database schemas with appropriate partitioning and indexing strategies for Data Warehouse or Data Mart.\nLeverage cloud services (e.g., AWS S3, Glue, Kinesis, Lambda) for storage, processing, and orchestration.\nUse orchestration tools like Airflow, Temporal, or AWS Step Functions to manage end-to-end workflows.\nBuild containerized solutions using Docker and manage deployment pipelines via CI/CD tools such as Azure DevOps, GitHub Actions, or Jenkins.\nCollaborate closely with data scientists, analysts, and business stakeholders to understand requirements and deliver data solutions.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'python', 'Snowflake', 'Data Bricks', 'sql']",2025-06-12 00:40:50
Senior Data Engineer,Impetus Technologies,5 - 10 years,Not Disclosed,['United Arab Emirates'],"The Opportunity:We are seeking a highly motivated and technically strong Module Lead Software Engineer with significant expertise in Python, PySpark, and Palantir Foundry. In this role, you will be responsible for the end-to-end technical ownership, design, and delivery of a specific module or component within our enterprise data platform. You will combine hands-on development with technical leadership, ensuring the highest standards of code quality, performance, and reliability.\n\nKey Responsibilities:\n\nModule Technical Leadership & Ownership: Take full technical ownership of a specific module or component within the data platform on Palantir Foundry. This includes defining its technical roadmap, architecture, design patterns, and ensuring its integration into the broader data ecosystem.\nHands-on Development and Complex Problem Solving: Act as a lead individual contributor, developing sophisticated data pipelines, transformations, and applications using Python and PySpark within Palantir Foundry's various tools (e.g., Code Workbook, Pipeline Builder). Tackle the most challenging technical problems and implement core functionalities for the module.\nQuality Assurance and Best Practices Advocacy: Drive and enforce high standards for code quality, test coverage, documentation, and operational excellence within your module. Conduct rigorous code reviews, provide constructive feedback, and mentor engineers within your immediate scope to elevate their technical skills.\nCross-Functional Collaboration and Module Integration: Collaborate extensively with other module leads, architects, data scientists, and business stakeholders to ensure seamless integration of your module's deliverables. Proactively identify and manage technical dependencies and ensure the module aligns with overall project goals and architectural vision.\n• Performance Optimization and Troubleshooting: Continuously monitor and optimize the performance of your module's data pipelines and applications. Efficiently troubleshoot and resolve complex technical issues, data quality concerns, and system failures specific to your module.\n\nRequired Qualifications:\n\nExperience: 6-8 years of progressive experience in software development with a strong focus on data engineering.\nPython Proficiency: Expert-level proficiency in Python, including advanced programming concepts, data structures, and performance optimization techniques.\nPySpark Expertise: Strong experience with PySpark for large-scale distributed data processing, transformations, and analytics.\nPalantir Foundry: Proven, hands-on experience designing, developing, and deploying solutions within Palantir Foundry is essential.\nDeep familiarity with Foundry's data integration capabilities, Code Workbook, Pipeline Builder, Data Health checks, and Ontology modeling.\nExperience with Foundry's approach to data governance and versioning.\nSQL Skills: Excellent SQL skills for complex data querying, manipulation, and optimization.\nData Warehousing/Lakes: Solid understanding of data warehousing concepts, data lake architectures, and ETL/ELT principles.\nCloud Platforms: Experience with at least one major cloud platform (AWS, Azure, GCP), particularly with data-related services.\nVersion Control: Strong experience with Git and collaborative development workflows.\n\nPreferred Qualifications (Nice-to-Have):\n\nExperience mentoring or leading small technical teams/pods.\nFamiliarity with containerization technologies (Docker, Kubernetes).\nExperience with streaming data technologies (e.g., Kafka, Kinesis).\nUnderstanding of CI/CD pipelines for data solutions.\nKnowledge of data governance, data quality, and metadata management best practices.\nExperience in [specific industry, e.g., Financial Services, Manufacturing, Healthcare].",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Palantir Foundry', 'Python', 'Spark', 'Data Warehousing', 'SQL']",2025-06-12 00:40:53
Consultant - Data Engineer,Affine Analytics,5 - 8 years,Not Disclosed,['Bengaluru'],"We are looking for a highly skilled API & Pixel Tracking Integration Engineer to lead the development and deployment of server-side tracking and attribution solutions across multiple platforms. The ideal candidate brings deep expertise in CAPI integrations (Meta, Google, and other platforms), secure data handling using cryptographic techniques, and experience working within privacy-first environments like Azure Clean Rooms.\nThis role requires strong hands-on experience in Azure cloud services, OCI (Oracle Cloud Infrastructure), and marketing technology stacks including Adobe Tag Management and Pixel Management. You will work closely with engineering, analytics, and marketing teams to deliver scalable, compliant, and secure data tracking solutions that drive business insights and performance.",,,,"['Python', 'Azure Cloud technologies', 'Azure Data Factory', 'Adobe Tag Management', 'Azure security', 'ADF', 'ADLS', 'Azure Functions', 'Key Vault']",2025-06-12 00:40:56
Data Engineer,Wissen Infotech,5 - 10 years,8-18 Lacs P.A.,"['Bengaluru', 'Mumbai (All Areas)']","Key Responsibilities\n• Design, develop, and optimize data pipelines using Python and AWS services such as Glue, Lambda, S3, EMR, Redshift, Athena, and Kinesis.\n• Implement ETL/ELT processes to extract, transform, and load data from various sources into centralized repositories (e.g., data lakes or data warehouses).\n• Collaborate with cross-functional teams to understand business requirements and translate them into scalable data solutions.\n• Monitor, troubleshoot, and enhance data workflows for performance and cost optimization.\n• Ensure data quality and consistency by implementing validation and governance practices.\n• Work on data security best practices in compliance with organizational policies and regulations.\n• Automate repetitive data engineering tasks using Python scripts and frameworks.\n• Leverage CI/CD pipelines for deployment of data workflows on AWS.\n\nRequired Skills and Qualifications\n\n• Professional Experience: 5+ years of experience in data engineering or a related field.\n• Programming: Strong proficiency in Python, with experience in libraries like pandas, pySpark, or boto3.\n• AWS Expertise: Hands-on experience with core AWS services for data engineering, such as AWS Glue for ETL/ELT, S3 for storage.\n• Redshift or Athena for data warehousing and querying.\n• Lambda for serverless compute.\n• Kinesis or SNS/SQS for data streaming.\n• IAM Roles for security. • Databases: Proficiency in SQL and experience with relational (e.g., PostgreSQL, MySQL) and NoSQL (e.g., DynamoDB) databases. • Data Processing: Knowledge of big data frameworks (e.g., Hadoop, Spark) is a plus. • DevOps: Familiarity with CI/CD pipelines and tools like Jenkins, Git, and CodePipeline. • Version Control: Proficient with Git-based workflows. • Problem Solving: Excellent analytical and debugging skills. Optional Skills • Knowledge of data modeling and data warehouse design principles. • Experience with data visualization tools (e.g., Tableau, Power BI). • Familiarity with containerization (e.g., Docker) and orchestration (e.g., Kubernetes).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'ETL', 'AWS', 'Python', 'SQL', 'Snowflake', 'Hadoop', 'SCALA', 'Big Data', 'Spark', 'Aws Glue']",2025-06-12 00:41:00
"Data Engineer - Hadoop, Spark, Python, Data bricks",Damco Solutions,4 - 9 years,Not Disclosed,['Coimbatore'],"Position Name: Data Engineer\nLocation: Coimbatore (Hybrid 3 days per week)\nWork Shift Timing: 1.30 pm to 10.30 pm (IST)\nMandatory Skills: Hadoop, Spark, Python, Data bricks\nGood to have: Java/Scala\n\nThe Role:\n• Designing and building optimized data pipelines using cutting-edge technologies in a cloud environment to drive analytical insights.\n• Constructing infrastructure for efficient ETL processes from various sources and storage systems.\n• Leading the implementation of algorithms and prototypes to transform raw data into useful information.\n• Architecting, designing, and maintaining database pipeline architectures, ensuring readiness for AI/ML transformations.\n• Creating innovative data validation methods and data analysis tools.\n• Ensuring compliance with data governance and security policies.\n• Interpreting data trends and patterns to establish operational alerts.\n• Developing analytical tools, programs, and reporting mechanisms.\n• Conducting complex data analysis and presenting results effectively.\n• Preparing data for prescriptive and predictive modeling.\n• Continuously exploring opportunities to enhance data quality and reliability.\n• Applying strong programming and problem-solving skills to develop scalable solutions.\n\nRequirements:\n• Experience in the Big Data technologies (Hadoop, Spark, Nifi, Impala).\n• Hands-on experience designing, building, deploying, testing, maintaining, monitoring, and owning scalable, resilient, and distributed data pipelines.\n• High proficiency in Scala/Java and Spark for applied large-scale data processing\n• Expertise with big data technologies, including Spark, Data Lake, and Hive.\n• Solid understanding of batch and streaming data processing techniques.\n• Proficient knowledge of the Data Lifecycle Management process, including data collection, access, use, storage, transfer, and deletion.\n• Expert-level ability to write complex, optimized SQL queries across extensive data volumes.\n• Experience on HDFS, Nifi, Kafka.\n• Experience on Apache Ozone, Delta Tables, Databricks, Axon(Kafka), Spring Batch, Oracle DB\n• Familiarity with Agile methodologies.\n• Obsession for service observability, instrumentation, monitoring, and alerting.\n• Knowledge or experience in architectural best practices for building data lakes\n\n\nInterested candidates can share their resume at Neesha1@damcogroup.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'Hadoop', 'Data bricks', 'Spark', 'Hdfs', 'Impala', 'Apache Nifi', 'date lake', 'Hive', 'java', 'SCALA', 'Big data', 'oracle DB']",2025-06-12 00:41:03
Data Engineer,Dun & Bradstreet,5 - 9 years,Not Disclosed,['Hyderabad'],"Key Responsibilities:\n1. Design, build, and deploy new data pipelines within our Big Data Eco-Systems using Streamsets/Talend/Informatica BDM etc. Document new/existing pipelines, Datasets.\n2. Design ETL/ELT data pipelines using StreamSets, Informatica or any other ETL processing engine. Familiarity with Data Pipelines, Data Lakes and modern Data Warehousing practices (virtual data warehouse, push down analytics etc.)\n3. Expert level programming skills on Python\n4. Expert level programming skills on Spark\n5. Cloud Based Infrastructure: GCP\n6. Experience with one of the ETL Informatica, StreamSets in creation of complex parallel loads, Cluster Batch Execution and dependency creation using Jobs/Topologies/Workflows etc.,\n7. Experience in SQL and conversion of SQL stored procedures into Informatica/StreamSets, Strong exposure working with web service origins/targets/processors/executors, XML/JSON Sources and Restful APIs.\n8. Strong exposure working with relation databases DB2, Oracle & SQL Server including complex SQL constructs and DDL generation.\n9. Exposure to Apache Airflow for scheduling jobs\n10. Strong knowledge of Big data Architecture (HDFS), Cluster installation, configuration, monitoring, cluster security, cluster resources management, maintenance, and performance tuning\n11. Create POCs to enable new workloads and technical capabilities on the Platform.\n12. Work with the platform and infrastructure engineers to implement these capabilities in production.\n13. Manage workloads and enable workload optimization including managing resource allocation and scheduling across multiple tenants to fulfill SLAs.\n14. Participate in planning activities, Data Science and perform activities to increase platform skills\n\nKey Requirements:\n1. Minimum 6 years of experience in ETL/ELT Technologies, preferably StreamSets/Informatica/Talend etc.,\n2. Minimum of 6 years hands-on experience with Big Data technologies e.g. Hadoop, Spark, Hive.\n3. Minimum 3+ years of experience on Spark\n4. Minimum 3 years of experience in Cloud environments, preferably GCP\n5. Minimum of 2 years working in a Big Data service delivery (or equivalent) roles focusing on the following disciplines:\n6. Any experience with NoSQL and Graph databases\n7. Informatica or StreamSets Data integration (ETL/ELT)\n8. Exposure to role and attribute based access controls\n9. Hands on experience with managing solutions deployed in the Cloud, preferably on GCP\n10. Experience working in a Global company, working in a DevOps model is a plus",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Airflow', 'GCP', 'Data engineer', 'Spark', 'ETL']",2025-06-12 00:41:07
Data Engineer,LTIMindtree,5 - 8 years,Not Disclosed,"['Noida', 'Chennai', 'Bengaluru']","1.Big Data Engineer:\n\nCompany: LTIMINDTREE\nMandotory skills - Python,Pyspark & AWS\nLocation : Noida & Pan India\nExperience : 5-8Years\nSalary: 19LPA\n\nShare your cv at Muktai.S@alphacom.in",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'AWS', 'Python']",2025-06-12 00:41:10
Data Engineer,Nemetschek,5 - 10 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']","Role & responsibilities\n5+ years in software development, with a focus on data-intensive applications, cloud solutions, and scalable data architectures.\nDevelopment experience in GoLang for building scalable and efficient data applications.\nExperience with Snowflake, Redshift, or similar data platforms including architecture, data modeling, performance optimization, and integrations.\nExperience designing and building data lakes and data warehouses, ensuring data integrity, scalability, and performance.\nProficient in developing and managing ETL pipelines, using modern tools and techniques to transform, load, and integrate data efficiently.\nExperience with high-volume event streams (such as Kafka, Kinesis) and near real-time data processing solutions for fast and accurate reporting.\nHands-on experience with Terraform for automating infrastructure deployment and configuration management in cloud environments.\nExperience with containerization technologies (Docker, Kubernetes) and orchestration.\nSolid grasp of database fundamentals (SQL, NoSQL, data modeling, performance tuning)\nExperience with CI/CD pipelines and automation tools for testing, deployment, and continuous improvement.\nExperience working in AWS cloud environments, specifically with big data solutions and serverless architectures\nAbility to mentor and guide junior engineers, fostering a culture of learning and innovation\nStrong communication skills to articulate technical concepts clearly to non-technical stakeholders.\nWHAT WE OFFER\nA young, dynamic, and innovation-oriented environment\nA wide variety of projects within different industries\nA very open and informal culture where knowledge sharing, and employee development are key.\nRoom for personal initiative, development, and growth\nRealistic career opportunities\nCompetitive package and fringe benefits.\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Golang', 'Snowflake', 'Javascript', 'ETL', 'AWS']",2025-06-12 00:41:13
Data Engineer,Forbes Global 2000 IT Services Firm,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Title: Big Data Engineer Java & Spark\nLocation: Hyderabad\nWork Mode: Onsite (5 days a week)\nExperience: 5 to 10 Years\nJob Summary:\nWe are hiring an experienced Big Data Engineer with strong expertise in Java, Apache Spark, and Big Data technologies. You will be responsible for designing and implementing scalable data pipelines that support real-time and batch processing for data-driven applications.\nKey Responsibilities:\nDevelop and maintain scalable batch and streaming data pipelines using Java and Apache Spark\nWork with Hadoop, Hive, Kafka, and HDFS to manage and process large datasets\nCollaborate with data analysts, scientists, and other engineering teams to understand data requirements\nOptimize Spark jobs and ensure performance and reliability in production\nMaintain data quality, governance, and security best practices\nRequired Skills:\n510 years of hands-on experience in data engineering or related roles\nStrong programming skills in both Java\nExpertise in Apache Spark for data processing and transformation\nGood understanding of Big Data frameworks: Hadoop, Hive, Kafka, HDFS\nExperience with distributed systems and large-scale data processing\nFamiliarity with cloud platforms such as AWS, GCP, or Azure\nGood to Have:\nExperience with workflow orchestration tools like Airflow or NiFi\nKnowledge of containerization (Docker, Kubernetes)\nExposure to CI/CD pipelines and version control (e.g., Git)\nEducation:\nBachelors or Masters degree in Computer Science, Engineering, or related field\nWhy Join Us:\nBe part of a high-impact data engineering team\nWork on modern data platforms with the latest open-source tools\nStrong tech culture with career growth opportunities",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Spark', 'Hive', 'Hadoop']",2025-06-12 00:41:16
Data Engineer Advisor,"NTT DATA, Inc.",8 - 13 years,Not Disclosed,['Bengaluru'],"Req ID: 327859\n\nWe are currently seeking a Data Engineer Advisor to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\nJob DutiesResponsibilitiesLead the development of backend systems using Django. Design and implement scalable and secure APIs. Integrate Azure Cloud services for application deployment and management. Utilize Azure Databricks for big data processing and analytics. Implement data processing pipelines using PySpark. Collaborate with front-end developers, product managers, and other stakeholders to deliver comprehensive solutions. Conduct code reviews and ensure adherence to best practices. Mentor and guide junior developers. Optimize database performance and manage data storage solutions. Ensure high performance and security standards for applications. Participate in architecture design and technical decision-making. Minimum Skills RequiredQualificationsBachelor's degree in Computer Science, Information Technology, or a related field. 8+ years of experience in backend development. 8+ years of experience with Django. Proven experience with Azure Cloud services. Experience with Azure Databricks and PySpark. Strong understanding of RESTful APIs and web services. Excellent communication and problem-solving skills. Familiarity with Agile methodologies. Experience with database management (SQL and NoSQL).\n\nSkills:\nDjango, Python, Azure Cloud, Azure Databricks, Delta Lake and Delta tables, PySpark, SQL/NoSQL databases, RESTful APIs, Git, and Agile methodologies",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['web services', 'rest', 'python', 'backend development', 'django', 'azure cloud services', 'css', 'pyspark', 'jquery', 'sql', 'git', 'asp.net', 'html', 'web api', 'mvc', 'wcf', 'agile methodology', 'azure databricks', 'c#', 'entity framework', 'azure cloud', 'javascript', 'sql server', 'nosql', 'angular', 'linq', '.net']",2025-06-12 00:41:21
Data Engineer,Lance Labs,7 - 12 years,Not Disclosed,"['Noida', 'Chennai']","Deployment, configuration & maintenance of Databricks clusters & workspaces\nSecurity & Access Control\nAutomate administrative task using tools like Python, PowerShell &Terraform\nIntegrations with Azure Data Lake, Key Vault & implement CI/CD pipelines\n\nRequired Candidate profile\nAzure, AWS, or GCP; Azure experience is preferred\nStrong skills in Python, PySpark, PowerShell & SQL\nExperience with Terraform\nETL processes, data pipeline &big data technologies\nSecurity & Compliance",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Azure Databricks', 'SQL', 'Terraform', 'Python', 'Powershell', 'Ci/Cd', 'Data Pipeline', 'GCP', 'Azure Cloud', 'Azure Data Lake', 'ETL', 'AWS', 'Data Governance', 'Azure Devops']",2025-06-12 00:41:23
Senior Data Engineer,Talentien Global Solutions,4 - 8 years,12-18 Lacs P.A.,"['Hyderabad', 'Chennai', 'Coimbatore']","We are seeking a skilled and motivated Data Engineer to join our dynamic team. The ideal candidate will have experience in designing, developing, and maintaining scalable data pipelines and architectures using Hadoop, PySpark, ETL processes, and Cloud technologies.\n\nResponsibilities:\nDesign, develop, and maintain data pipelines for processing large-scale datasets.\nBuild efficient ETL workflows to transform and integrate data from multiple sources.\nDevelop and optimize Hadoop and PySpark applications for data processing.\nEnsure data quality, governance, and security standards are met across systems.\nImplement and manage Cloud-based data solutions (AWS, Azure, or GCP).\nCollaborate with data scientists and analysts to support business intelligence initiatives.\nTroubleshoot performance issues and optimize query executions in big data environments.\nStay updated with industry trends and advancements in big data and cloud technologies.\nRequired Skills:\nStrong programming skills in Python, Scala, or Java.\nHands-on experience with Hadoop ecosystem (HDFS, Hive, Spark, etc.).\nExpertise in PySpark for distributed data processing.\nProficiency in ETL tools and workflows (SSIS, Apache Nifi, or custom pipelines).\nExperience with Cloud platforms (AWS, Azure, GCP) and their data-related services.\nKnowledge of SQL and NoSQL databases.\nFamiliarity with data warehousing concepts and data modeling techniques.\nStrong analytical and problem-solving skills.\n\nInterested can reach us at +91 7305206696/ saranyadevib@talentien.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Hadoop', 'Spark', 'ETL', 'Airflow', 'Etl Pipelines', 'Big Data', 'EMR', 'Gcp Cloud', 'Data Bricks', 'Azure Cloud', 'Data Pipeline', 'SCALA', 'Snowflake', 'Data Lake', 'Data Warehousing', 'Data Modeling', 'AWS', 'Python']",2025-06-12 00:41:26
Data Engineer,Kanini Software Solutions,12 - 20 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","We are looking for a skilled Data Engineer to join our growing data team. The ideal candidate will be responsible for designing, building, and maintaining data pipelines and infrastructure to support data analytics and business intelligence needs. A strong foundation in cloud data platforms, data transformation tools, and programming is essential.\nKey Responsibilities:\nDesign and implement scalable data pipelines using Azure Data Lake and dbt.\nIngest and transform data from various sources including databases, APIs, flat files, JSON, and XML.",,,,"['Pyspark', 'Azure', 'Snowflake']",2025-06-12 00:41:29
Consultant - Data Engineer (with Fabric),Affine Analytics,5 - 8 years,Not Disclosed,['Bengaluru'],"We are looking for a highly skilled API & Pixel Tracking Integration Engineer to lead the development and deployment of server-side tracking and attribution solutions across multiple platforms. The ideal candidate brings deep expertise in CAPI integrations (Meta, Google, and other platforms), secure data handling using cryptographic techniques, and experience working within privacy-first environments like Azure Clean Rooms.\nThis role requires strong hands-on experience in C# development, Azure cloud services, OCI (Oracle Cloud Infrastructure), and marketing technology stacks including Adobe Tag Management and Pixel Management. You will work closely with engineering, analytics, and marketing teams to deliver scalable, compliant, and secure data tracking solutions that drive business insights and performance.",,,,"['Adobe Tag Management', 'Data Engineering', 'Azure Data Factory', 'Azure security', 'ADF', 'ADLS', 'Azure Functions', 'Meta CAPI', 'Google Enhanced Conversions', 'Key Vault', 'Cosmos DB']",2025-06-12 00:41:33
Cloud Data Engineer,PwC India,3 - 8 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Job Description:\n\nWe are seeking skilled and dynamic Cloud Data Engineers specializing in AWS, Azure, Databricks. The ideal candidate will have a strong background in data engineering, with a focus on data ingestion, transformation, and warehousing. They should also possess excellent knowledge of PySpark or Spark, and a proven ability to optimize performance in Spark job executions.\n\nKey Responsibilities:",,,,"['AWS OR Azure', 'Azure Data Engineer OR AWS Data Engineer', 'Azure', 'AWS']",2025-06-12 00:41:36
Cloud Data Engineer,PwC India,5 - 8 years,10-20 Lacs P.A.,['Bengaluru'],"Role & responsibilities\nStrong hands-on experience with multi cloud (AWS, Azure, GCP)  services such as GCP BigQuery, Dataform AWS Redshift, \nProficient in PySpark and SQL for building scalable data processing pipelines\nKnowledge of utilizing serverless technologies like AWS Lambda, and Google Cloud Functions \nExperience in orchestration frameworks like Apache Airflow, Kubernetes, and Jenkins to manage and orchestrate data pipelines",,,,"['Pyspark', 'Python', 'SQL', 'Datafactory', 'GCP', 'Microsoft Azure', 'AWS', 'Data Bricks']",2025-06-12 00:41:40
Data Engineer,Careerzgraph,2 - 4 years,4.8-7.2 Lacs P.A.,['Bengaluru( Electronic City )'],"Hands On* Experience Required (Code Level) *\nSQL Query Writing - Query Optimization -\nPython Scripting - Programming\n* Design, develop & maintain data pipelines using Python, Azure & Power BI\n* Collaborate with cross-functional teams on ETL projects\n\n\nAssistive technologies\nProvident fund\nHealth insurance",Industry Type: Recruitment / Staffing,"Department: Production, Manufacturing & Engineering","Employment Type: Full Time, Permanent","['SQL Queries', 'Optimization', 'Python', 'Azure', 'Power Bi', 'Stored Procedures', 'Optimization Techniques', 'Programming', 'ETL', 'SQL Scripting']",2025-06-12 00:41:43
Data Engineer,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"Role Description:\nAs part of the cybersecurity organization, In this vital role you will be responsible for designing, building, and maintaining data infrastructure to support data-driven decision-making. This role involves working with large datasets, developing reports, executing data governance initiatives, and ensuring data is accessible, reliable, and efficiently managed. The role sits at the intersection of data infrastructure and business insight delivery, requiring the Data Engineer to design and build robust data pipelines while also translating data into meaningful visualizations for stakeholders across the organization. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture, ETL processes, and cybersecurity data frameworks.\nRoles & Responsibilities:\nDesign, develop, and maintain data solutions for data generation, collection, and processing.\nBe a key team member that assists in design and development of the data pipeline.\nBuild data pipelines and ensure data quality by implementing ETL processes to migrate and deploy data across systems.\nDevelop and maintain interactive dashboards and reports using tools like Tableau, ensuring data accuracy and usability\nSchedule and manage workflows the ensure pipelines run on schedule and are monitored for failures.\nCollaborate with multi-functional teams to understand data requirements and design solutions that meet business needs.\nDevelop and maintain data models, data dictionaries, and other documentation to ensure data accuracy and consistency.\nImplement data security and privacy measures to protect sensitive data.\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nCollaborate and communicate effectively with product teams.\nCollaborate with data scientists to develop pipelines that meet dynamic business needs.\nShare and discuss findings with team members practicing SAFe Agile delivery model.\n\n\nBasic Qualifications:\nMasters degree and 1 to 3 years of experience of Computer Science, IT or related field experience OR\nBachelors degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience\nPreferred Qualifications:\nHands on experience with data practices, technologies, and platforms, such as Databricks, Python, GitLab, LucidChart, etc.\nHands-on experience with data visualization and dashboarding toolsTableau, Power BI, or similar is a plus\nProficiency in data analysis tools (e.g. SQL) and experience with data sourcing tools\nExcellent problem-solving skills and the ability to work with large, complex datasets\nUnderstanding of data governance frameworks, tools, and best practices\nKnowledge of and experience with data standards (FAIR) and protection regulations and compliance requirements (e.g., GDPR, CCPA)\n\nGood-to-Have Skills:\nExperience with ETL tools and various Python packages related to data processing, machine learning model development\nStrong understanding of data modeling, data warehousing, and data integration concepts\nKnowledge of Python/R, Databricks, cloud data platforms\nExperience working in Product team's environment\nExperience working in an Agile environment\n\nProfessional Certifications:\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\n\nSoft Skills:\nInitiative to explore alternate technology and approaches to solving problems\nSkilled in breaking down problems, documenting problem statements, and estimating efforts\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to handle multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data engineering', 'data analysis', 'data modeling', 'analysis tools', 'data warehousing', 'troubleshooting', 'data architecture', 'data integration', 'etl process']",2025-06-12 00:41:48
ETL Developer,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\n\nResponsibilities:\nDesign and implement the data modeling, data ingestion and data processing for various datasets\nDesign, develop and maintain ETL Framework for various new data source\nDevelop data ingestion using AWS Glue/ EMR, data pipeline using PySpark, Python and Databricks.\nBuild orchestration workflow using Airflow & databricks Job workflow\nDevelop and execute adhoc data ingestion to support business analytics.\nProactively interact with vendors for any questions and report the status accordingly\nExplore and evaluate the tools/service to support business requirement\nAbility to learn to create a data-driven culture and impactful data strategies.\nAptitude towards learning new technologies and solving complex problem.\nQualifications:\nMinimum of bachelors degree. Preferably in Computer Science, Information system, Information technology.\nMinimum 5 years of experience on cloud platforms such as AWS, Azure, GCP.\nMinimum 5 year of experience in Amazon Web Services like VPC, S3, EC2, Redshift, RDS, EMR, Athena, IAM, Glue, DMS, Data pipeline & API, Lambda, etc.\nMinimum of 5 years of experience in ETL and data engineering using Python, AWS Glue, AWS EMR /PySpark and Airflow for orchestration.\nMinimum 2 years of experience in Databricks including unity catalog, data engineering Job workflow orchestration and dashboard generation based on business requirements\nMinimum 5 years of experience in SQL, Python, and source control such as Bitbucket, CICD for code deployment.\nExperience in PostgreSQL, SQL Server, MySQL & Oracle databases.\nExperience in MPP such as AWS Redshift, AWS EMR, Databricks SQL warehouse & compute cluster.\nExperience in distributed programming with Python, Unix Scripting, MPP, RDBMS databases for data integration\nExperience building distributed high-performance systems using Spark/PySpark, AWS Glue and developing applications for loading/streaming data into Databricks SQL warehouse & Redshift.\nExperience in Agile methodology\nProven skills to write technical specifications for data extraction and good quality code.\nExperience with big data processing techniques using Sqoop, Spark, hive is additional plus\nExperience in data visualization tools including PowerBI, Tableau.\nNice to have experience in UI using Python Flask framework anglular\n\n\nMandatory Skills: Python for Insights. Experience: 5-8 Years.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ETL', 'data bricks', 'aws glue', 'amazon ec2', 'python', 'spark', 'glue', 'amazon redshift', 'cloud platforms', 'aws', 'data engineering', 'sql']",2025-06-12 00:41:51
Data Engineer KL-BL,PureSoftware Pvt Ltd,7 - 12 years,Not Disclosed,"['Bengaluru', 'Malaysia']","Core Competences Required and Desired Attributes:\n  Bachelor's degree in computer science, Information Technology, or a related field.\nProficiency in Azure Data Factory, Azure Databricks and Unity Catalog, Azure SQL Database, and other Azure data services.\nStrong programming skills in SQL, Python and PySpark languages.\nExperience in the Asset Management domain would be preferable.\nStrong proficiency in data analysis and data modelling, with the ability to extract insights from complex data sets.\nHands-on experience in Power BI, including creating custom visuals, DAX expressions, and data modelling.\nFamiliarity with Azure Analysis Services, data modelling techniques, and optimization.\nExperience with data quality and data governance frameworks with an ability to debug, fine tune and optimise large scale data processing jobs.\nStrong analytical and problem-solving skills, with a keen eye for detail.\nExcellent communication and interpersonal skills, with the ability to work collaboratively in a team environment.\nProactive and self-motivated, with the ability to manage multiple tasks and deliver high-quality results within deadlines.\n\nRoles and Responsibilities\nCore Competences Required and Desired Attributes:\n  Bachelor's degree in computer science, Information Technology, or a related field.\nProficiency in Azure Data Factory, Azure Databricks and Unity Catalog, Azure SQL Database, and other Azure data services.\nStrong programming skills in SQL, Python and PySpark languages.\nExperience in the Asset Management domain would be preferable.\nStrong proficiency in data analysis and data modelling, with the ability to extract insights from complex data sets.\nHands-on experience in Power BI, including creating custom visuals, DAX expressions, and data modelling.\nFamiliarity with Azure Analysis Services, data modelling techniques, and optimization.\nExperience with data quality and data governance frameworks with an ability to debug, fine tune and optimise large scale data processing jobs.\nStrong analytical and problem-solving skills, with a keen eye for detail.\nExcellent communication and interpersonal skills, with the ability to work collaboratively in a team environment.\nProactive and self-motivated, with the ability to manage multiple tasks and deliver high-quality results within deadlines.",Industry Type: Not mentioned,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['azure databricks', 'python', 'data services', 'data analysis', 'modeling', 'analytical', 'languages', 'catalog', 'pyspark', 'datafactory', 'interpersonal skills', 'microsoft azure', 'power bi', 'azure data factory', 'data engineering', 'sql', 'sql azure', 'data modeling', 'azure analysis', 'programming', 'communication skills']",2025-06-12 00:41:54
Senior Azure Data Engineer (Only Immediate Join),Adecco,7 - 12 years,15-30 Lacs P.A.,['Bengaluru'],"Position : Senior Azure Data Engineer (Only Immediate Joiner)\nLocation : Bangalore\nMode of Work : Work from Office\nExperience : 7 years relevant experience\nJob Type : Full Time (On Roll)\n\nJob Description\nRoles and Responsibilities:\nThe Data Engineer will work on data engineering projects for various business units, focusing on delivery of complex data management solutions by leveraging industry best practices. They work with the project team to build the most efficient data pipelines and data management solutions that make data easily available for consuming applications and analytical solutions. A Data engineer is expected to possess strong technical skills.\nKey Characteristics\nTechnology champion who constantly pursues skill enhancement and has inherent curiosity to understand work from multiple dimensions.\nInterest and passion in Big Data technologies and appreciates the value that can be brought in with an effective data management solution.\nHas worked on real data challenges and handled high volume, velocity, and variety of data.\nExcellent analytical & problem-solving skills, willingness to take ownership and resolve technical challenges.\nContributes to community building initiatives like CoE, CoP.\nMandatory skills:\nAzure - Master\nELT - Skill\nData Modeling - Skill\nData Integration & Ingestion - Skill\nData Manipulation and Processing - Skill\nGITHUB, Action, Azure DevOps - Skill\nData factory, Databricks, SQL DB, Synapse, Stream Analytics, Glue, Airflow, Kinesis, Redshift, SonarQube, PyTest - Skill\nOptional skills:\nExperience in project management, running a scrum team.\nExperience working with BPC, Planning.\nExposure to working with external technical ecosystem.\nMKDocs documentation\n\nInterested candidates kindly share your CV and below details to usha.sundar@adecco.com\n1) Present CTC (Fixed + VP) -\n2) Expected CTC -\n3) No. of years experience -\n4) Notice Period -\n5) Offer-in hand -\n6) Reason of Change -\n7) Present Location -",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Pyspark', 'SCALA', 'Azure Data Lake', 'Databricks', 'Stream Analytics', 'Azure Data Engineering', 'Azure Databricks', 'Data factory', 'Streaming data', 'Data Bricks', 'SQL']",2025-06-12 00:41:58
Sr. Executive Data Engineering Analytics,IndiGo,5 - 10 years,Not Disclosed,['Gurugram'],"Role & responsibilities\nDevelop in Python, create responsive dashboards, and manage large datasets.\nDesign and deploy Power BI reports based on business needs.\nApply machine learning, deep learning, and statistical analysis (e.g., classification, regression, sentiment analysis, time series).\nTranslate technical concepts for non-technical stakeholders.\nDesign and implement Big Data platform components (batch/stream processing, memory cache, SQL query layer, rule engine).\nBuild scalable data solutions.\nConduct root cause analysis, troubleshoot applications, and support configurations.\nAutomate processes and reporting within the Operations Control Center (OCC).\nCollaborate with leadership to solve business problems and define objectives.\nRecommend and implement automated solutions.\nPerform data analysis and apply statistical methods for decision-making.\n\nPreferred candidate profile\n\nEducation: Bachelors or Master’s in Computer Science, Engineering, or related field. (Mathematics/Statistics)\nExperience: 5–10 years in data engineering & analytics.\nSkills:\nPython (hands-on)\nPower BI (dashboarding)\nSQL/SSMS (data storage and extraction)\nPower Automate / Power Apps (nice to have)",Industry Type: Travel & Tourism,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Python', 'SQL', 'SSMS', 'Power Bi', 'Power Automate']",2025-06-12 00:42:01
Lead Data Engineer,Acuity Knowledge Partners,8 - 13 years,20-25 Lacs P.A.,"['Pune', 'Bangalore Rural', 'Gurugram']","Desired Skills and experience\n9+ years of experience in software development with a focus on data projects using Python, PySpark, and associated frameworks.\nProven experience as a Data Engineer with experience in Azure cloud.\nExperience implementing solutions using Azure cloud services, Azure Data Factory, Azure Lake Gen 2, Azure Databases, Azure Data Fabric, API Gateway management, Azure Functions.",,,,"['Data Engineering', 'Python', 'Pyspark', 'ETL', 'SQL']",2025-06-12 00:42:06
Data Engineer,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nAs part of the cybersecurity organization, the Data Engineer is responsible for designing, building, and maintaining data infrastructure to support data-driven decision-making. This role involves working with large datasets, developing reports, executing data governance initiatives, and ensuring data is accessible, reliable, and efficiently managed. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture, ETL processes, and cybersecurity data frameworks.\nRoles & Responsibilities:\nDesign, develop, and maintain data solutions for data generation, collection, and processing.\nBe a key team member that assists in design and development of the data pipeline.\nCreate data pipelines and ensure data quality by implementing ETL processes to migrate and deploy data across systems.\nSchedule and manage workflows the ensure pipelines run on schedule and are monitored for failures.\nCollaborate with cross-functional teams to understand data requirements and design solutions that meet business needs.\nDevelop and maintain data models, data dictionaries, and other documentation to ensure data accuracy and consistency.\nImplement data security and privacy measures to protect sensitive data.\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nCollaborate and communicate effectively with product teams.\nCollaborate with data scientists to develop pipelines that meet dynamic business needs.\nShare and discuss findings with team members practicing SAFe Agile delivery model.\nFunctional Skills:\nBasic Qualifications:\nMasters degree and 1 to 3 years of Computer Science, IT or related field experience OR\nBachelors degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience\nPreferred Qualifications:\nHands on experience with data practices, technologies, and platforms, such as Databricks, Python, Gitlab, LucidChart,etc.\nProficiency in data analysis tools (e.g. SQL) and experience with data sourcing tools\nExcellent problem-solving skills and the ability to work with large, complex datasets\nUnderstanding of data governance frameworks, tools, and best practices\nKnowledge of and experience with data standards (FAIR) and protection regulations and compliance requirements (e.g., GDPR, CCPA)\nGood-to-Have Skills:\nExperience with ETL tools and various Python packages related to data processing, machine learning model development\nStrong understanding of data modeling, data warehousing, and data integration concepts\nKnowledge of Python/R, Databricks, cloud data platforms\nExperience working in Product team's environment\nExperience working in an Agile environment\nProfessional Certifications:\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nSoft Skills:\nInitiative to explore alternate technology and approaches to solving problems\nSkilled in breaking down problems, documenting problem statements, and estimating efforts\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data engineering', 'data security', 'Agile', 'cloud data platforms', 'Databricks', 'data governance frameworks', 'ETL', 'AWS', 'SQL', 'Python']",2025-06-12 00:42:09
DataBricks - Data Engineering Professional,Wipro,3 - 5 years,Not Disclosed,['Pune'],"Role Purpose\nThe purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\n\nDo\n1. Instrumental in understanding the requirements and design of the product/ software\nDevelop software solutions by studying information needs, studying systems flow, data usage and work processes\nInvestigating problem areas followed by the software development life cycle\nFacilitate root cause analysis of the system issues and problem statement\nIdentify ideas to improve system performance and impact availability\nAnalyze client requirements and convert requirements to feasible design\nCollaborate with functional teams or systems analysts who carry out the detailed investigation into software requirements\nConferring with project managers to obtain information on software capabilities\n\n\n\n2. Perform coding and ensure optimal software/ module development\nDetermine operational feasibility by evaluating analysis, problem definition, requirements, software development and proposed software\nDevelop and automate processes for software validation by setting up and designing test cases/scenarios/usage cases, and executing these cases\nModifying software to fix errors, adapt it to new hardware, improve its performance, or upgrade interfaces.\nAnalyzing information to recommend and plan the installation of new systems or modifications of an existing system\nEnsuring that code is error free or has no bugs and test failure\nPreparing reports on programming project specifications, activities and status\nEnsure all the codes are raised as per the norm defined for project / program / account with clear description and replication patterns\nCompile timely, comprehensive and accurate documentation and reports as requested\nCoordinating with the team on daily project status and progress and documenting it\nProviding feedback on usability and serviceability, trace the result to quality risk and report it to concerned stakeholders\n\n\n\n3. Status Reporting and Customer Focus on an ongoing basis with respect to project and its execution\nCapturing all the requirements and clarifications from the client for better quality work\nTaking feedback on the regular basis to ensure smooth and on time delivery\nParticipating in continuing education and training to remain current on best practices, learn new programming languages, and better assist other team members.\nConsulting with engineering staff to evaluate software-hardware interfaces and develop specifications and performance requirements\nDocument and demonstrate solutions by developing documentation, flowcharts, layouts, diagrams, charts, code comments and clear code\nDocumenting very necessary details and reports in a formal way for proper understanding of software from client proposal to implementation\nEnsure good quality of interaction with customer w.r.t. e-mail content, fault report tracking, voice calls, business etiquette etc\nTimely Response to customer requests and no instances of complaints either internally or externally\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure1.Continuous Integration, Deployment & Monitoring of Software100% error free on boarding & implementation, throughput %, Adherence to the schedule/ release plan2.Quality & CSATOn-Time Delivery, Manage software, Troubleshoot queries,Customer experience, completion of assigned certifications for skill upgradation3.MIS & Reporting100% on time MIS & report generation\nMandatory Skills: DataBricks - Data Engineering. Experience: 3-5 Years.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'data bricks', 'software development life cycle', 'continuous integration', 'software development', 'mis', 'software management', 'root cause analysis']",2025-06-12 00:42:12
Azure Data Engineer,Big4,7 - 12 years,18-30 Lacs P.A.,['Bengaluru'],"Urgently Hiring for Senior Azure Data Engineer\n\nJob Location- Bangalore\nMinimum exp - Total 7+yrs with min 4 years relevant exp\n\nKeywords Databricks, Pyspark, SCALA, SQL, Live / Streaming data, batch processing data\n\nShare CV siddhi.pandey@adecco.com\nOR Call 6366783349\n\nRoles and Responsibilities:\nThe Data Engineer will work on data engineering projects for various business units, focusing on delivery of complex data management solutions by leveraging industry best practices. They work with the project team to build the most efficient data pipelines and data management solutions that make data easily available for consuming applications and analytical solutions. A Data engineer is expected to possess strong technical skills\n\nKey Characteristics\nTechnology champion who constantly pursues skill enhancement and has inherent curiosity to understand work from multiple dimensions\nInterest and passion in Big Data technologies and appreciates the value that can be brought in with an effective data management solution\nHas worked on real data challenges and handled high volume, velocity, and variety of data.\nExcellent analytical & problem-solving skills, willingness to take ownership and resolve technical challenges.\nContributes to community building initiatives like CoE, CoP.\nMandatory skills:\nAzure - Master\nELT - Skill\nData Modeling - Skill\nData Integration & Ingestion - Skill\nData Manipulation and Processing - Skill\nGITHUB, Action, Azure DevOps - Skill\nData factory, Databricks, SQL DB, Synapse, Stream Analytics, Glue, Airflow, Kinesis, Redshift, SonarQube, PyTest - Skill\nOptional skills:\nExperience in project management, running a scrum team.\nExperience working with BPC, Planning.\nExposure to working with external technical ecosystem.\nMKDocs documentation\n\nShare CV siddhi.pandey@adecco.com\nOR Call 6366783349",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['databricks', 'Azure Synapse', 'Pyspark', 'Stream Analytics', 'SCALA', 'SQL Azure', 'Data Bricks', 'SQL']",2025-06-12 00:42:15
Data Engineer,Prohr Strategies,9 - 11 years,Not Disclosed,['Bengaluru'],"Hands-on Data Engineer with strong Databricks expertise in Git/DevOps integration, Unity Catalog governance, and performance tuning of data transformation workloads. Skilled in optimizing pipelines and ensuring secure, efficient data operations.",Industry Type: Emerging Technologies (AI/ML),Department: Data Science & Analytics,"Employment Type: Full Time, Temporary/Contractual","['Data Transformation', 'GIT', 'Azure Databricks', 'Databricks', 'Devops', 'Data Engineering', 'Governance', 'Catalog', 'Code Versioning Tools']",2025-06-12 00:42:18
Azure Data Engineer,Our client is a multinational professio...,7 - 12 years,Not Disclosed,['Bengaluru'],"Urgently Hiring for Senior Azure Data Engineer\n\nJob Location- Bangalore\nMinimum exp - 7yrs- 11yrs\n\nKeywords Databricks, Pyspark, SCALA, SQL, Live / Streaming data, batch processing data\n\nShare CV Mohini.sharma@adecco.com\nOR Call 9740521948\n\nRoles and Responsibilities:\nThe Data Engineer will work on data engineering projects for various business units, focusing on delivery of complex data management solutions by leveraging industry best practices. They work with the project team to build the most efficient data pipelines and data management solutions that make data easily available for consuming applications and analytical solutions. A Data engineer is expected to possess strong technical skills.\nKey Characteristics\nTechnology champion who constantly pursues skill enhancement and has inherent curiosity to understand work from multiple dimensions.\nInterest and passion in Big Data technologies and appreciates the value that can be brought in with an effective data management solution.\nHas worked on real data challenges and handled high volume, velocity, and variety of data.\nExcellent analytical & problem-solving skills, willingness to take ownership and resolve technical challenges.\nContributes to community building initiatives like CoE, CoP.\nMandatory skills:\nAzure - Master\nELT - Skill\nData Modeling - Skill\nData Integration & Ingestion - Skill\nData Manipulation and Processing - Skill\nGITHUB, Action, Azure DevOps - Skill\nData factory, Databricks, SQL DB, Synapse, Stream Analytics, Glue, Airflow, Kinesis, Redshift, SonarQube, PyTest - Skill\nOptional skills:\nExperience in project management, running a scrum team.\nExperience working with BPC, Planning.\nExposure to working with external technical ecosystem.\nMKDocs documentation\n\nShare CV Mohini.sharma@adecco.com\nOR Call 9740521948",Industry Type: Financial Services (Asset Management),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Databricks', 'Pyspark', 'SCALA', 'SQL']",2025-06-12 00:42:21
Data Engineer,Talent Aspire,2 - 7 years,Not Disclosed,"['Chandigarh', 'Bengaluru', 'Remote']","As the Data Engineer, you will play a pivotal role in shaping our data infrastructure and\nexecuting against our strategy. You will ideate alongside engineering, data and our clients to\ndeploy data products with an innovative and meaningful impact to clients. You will design, build,\nand maintain scalable data pipelines and workflows on AWS. Additionally, your expertise in AI\nand machine learning will enhance our ability to deliver smarter, more predictive solutions.\nKey Responsibilities\nCollaborate with other engineers, customers to brainstorm and develop impactful data\nproducts tailored to our clients.\nLeverage AI and machine learning techniques to integrate intelligent features into our\nofferings.\nDevelop, and optimize end-to-end data pipelines on AWS\nFollow best practices in software architecture and development.\nImplement effective cost management and performance optimization strategies.\nDevelop and maintain systems using Python, SQL, PySpark, and Django for front-end\ndevelopment.\nWork directly with clients and end-users and address their data needs\nUtilize databases and tools including and not limited to, Postgres, Redshift, Airflow, and\nMongoDB to support our data ecosystem.\nLeverage AI frameworks and libraries to integrate advanced analytics into our solutions.\nQualifications\n\nExperience:\nMinimum of 3 years of experience in data engineering, software development, or\nrelated roles.\nProven track record in designing and deploying AWS cloud infrastructure\nsolutions\nAt least 2 years in data analysis and mining techniques to aid in descriptive and\ndiagnostic insights\nExtensive hands-on experience with Postgres, Redshift, Airflow, MongoDB, and\nreal-time data workflows.\n\nTechnical Skills:\nExpertise in Python, SQL, and PySpark\nStrong background in software architecture and scalable development practices.\nTableau, Metabase or similar viz tools experience\nWorking knowledge of AI frameworks and libraries is a plus.\nLeadership & Communication:\nDemonstrates ownership and accountability for delivery with a strong\ncommitment to quality.\nExcellent communication skills with a history of effective client and end-user\nengagement.\nStartup & Fintech Mindset:\nAdaptability and agility to thrive in a fast-paced, early-stage startup environment.\nPassion for fintech innovation and a strong desire to make a meaningful impact\non the future of finance.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'and PySpark', 'Django', 'AI frameworks', 'Python', 'SQL']",2025-06-12 00:42:24
Data Engineer,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nDesign, develop, and maintain complex ETL/ELT data pipelines in Databricks using PySpark, Scala, and SQL to process large-scale datasets\nUnderstand the biotech/pharma or related domains & build highly efficient data pipelines to migrate and deploy complex data across systems\nDesign and Implement solutions to enable unified data access, governance, and interoperability across hybrid cloud environments\nIngest and transform structured and unstructured data from databases (PostgreSQL, MySQL, SQL Server, MongoDB etc.), APIs, logs, event streams, images, pdf, and third-party platforms\nEnsuring data integrity, accuracy, and consistency through rigorous quality checks and monitoring\nExpert in data quality, data validation and verification frameworks\nInnovate, explore and implement new tools and technologies to enhance efficient data processing\nProactively identify and implement opportunities to automate tasks and develop reusable frameworks\nWork in an Agile and Scaled Agile (SAFe) environment, collaborating with cross-functional teams, product owners, and Scrum Masters to deliver incremental value\nUse JIRA, Confluence, and Agile DevOps tools to manage sprints, backlogs, and user stories\nSupport continuous improvement, test automation, and DevOps practices in the data engineering lifecycle\nCollaborate and communicate effectively with the product teams, with cross-functional teams to understand business requirements and translate them into technical solutions\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. We are looking for highly motivated expert Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks.\nBasic Qualifications:\nMasters degree and 1 to 3 years of Computer Science, IT or related field experience OR\nBachelors degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies\nProficiency in workflow orchestration, performance tuning on big data processing\nStrong understanding of AWS services\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices\nPreferred Qualifications:\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nData Engineering experience in Biotechnology or pharma industry\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and DevOps\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data engineering', 'Maven', 'data validation', 'PySpark', 'Scala', 'APIs', 'SQL Server', 'SQL', 'Jenkins', 'Git', 'MySQL', 'troubleshooting', 'MongoDB', 'ETL']",2025-06-12 00:42:27
Data Engineer,Society Managers,3 - 5 years,Not Disclosed,['Mumbai (All Areas)'],"We are seeking a skilled and driven SDE-II (Data Engineering) to join our dynamic team. In this role, you will design, develop, and maintain scalable data pipelines, working with large, complex datasets. Youll collaborate closely with cross-functional teams to gather data requirements and contribute to the architecture of our data systems, leveraging your expertise in tools like Databricks, Spark, and SQL.\n\nRoles and responsibilities\nData Pipeline Development: Design,build, and maintain scalable data pipelines using Databricks, Python,and Spark.\nData Processing & Transformation: Handle large, complex datasets to ensure efficient data processing and transformations.\nCollaboration: Work with cross-functional teams to gather, understand, and implement data requirements.\nSQL & ETL: Write and optimize SQL queries for data extraction, transformation, and loading (ETL) processes.\nData Quality & Security: Ensure data accuracy, integrity, and security across all stages of the data lifecycle.\nSystem Design & Architecture: Contribute to the design and architecture of scalable data systems and solutions.\nRequired Skills and Qualification\nExperience: 3+ years of experience in data engineering or a related field.\nDatabricks & Spark: Strong expertise in Databricks and distributed data processing with Spark.\nProgramming: Proficiency in Python for data engineering tasks.\nSQL Optimization: Solid experience in writing and optimizing complex SQL queries.\nData Systems Knowledge: Hands-on experience with large-scale data systems and tools.\nDomain Knowledge: Familiarity with Capital Market/Private Equity is a plus (relaxation may apply).\nData Visualization: Experience with Tableau for creating insightful data visualizations and reports.\nPreferred skills\nCloud Platforms: Familiarity with cloud services like AWS, Azure, or GCP.\nData Warehousing & ETL:Experience with data warehousing concepts and ETL processes.\nAnalytical Skills: Strong problem-solving and analytical capabilities Analytics Tools: Hands-on experience with tools like Amplitude, PostHog, Google Analytics, or Mixpanel.\nAdditional Tools: Knowledge of Python for web scraping and frameworks like Django (good to have).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Microsoft Azure', 'Data Bricks', 'Spark', 'ETL', 'Python', 'SQL']",2025-06-12 00:42:30
Data Engineer,Cloud Angles Digital Transformation,3 - 5 years,Not Disclosed,['Noida'],"Essential Functions/Responsibilities/Duties\n•       Work closely with Senior Business Intelligence engineer and BI architect to understand the schema objects and build BI reports and Dashboards\n•       Participation in sprint refinement, planning, and kick-off to understand the Agile process and Sprint priorities\n•       Develop necessary transformations and aggregate tables required for the reporting\\Dashboard needs\n•       Understand the Schema layer in MicroStrategy and business requirements\n•       Develop complex reports and Dashboards in MicroStrategy\n•       Investigate and troubleshoot issues with Dashboard and reports\n•       Proactively researching new technologies and proposing improvements to processes and tech stack\n•       Create test cases and scenarios to validate the dashboards and maintain data accuracy\nEducation and Experience\n•       3 years of experience in Business Intelligence and Data warehousing\n•       3+ years of experience in MicroStrategy Reports and Dashboard development\n•       2 years of experience in SQL\n•       Bachelors or masters degree in IT or Computer Science or ECE.\n•       Nice to have – Any MicroStrategy certifications\nRequired Knowledge, Skills, and Abilities\n•       Good in writing complex SQL, including aggregate functions, subqueries and complex date calculations and able to teach these concepts to others.\n•       Detail oriented and able to examine data and code for quality and accuracy.\n•       Self-Starter – taking initiative when inefficiencies or opportunities are seen.\n•       Good understanding of modern relational and non-relational models and differences between them\n•       Good understanding of Datawarehouse concepts, snowflake & star schema architecture and SCD concepts\n•       Good understanding of MicroStrategy Schema objects\n•       Develop Public objects such as metrics, filters, prompts, derived objects, custom groups and consolidations in MicroStrategy\n•       Develop complex reports and dashboards using OLAP and MTDI cubes\n•       Create complex dashboards with data blending\n•       Understand VLDB settings and report optimization\n•       Understand security filters and connection mappings in MSTR\nWork Environment\nAt Personify Health, we value and celebrate diversity and are committed to creating an inclusive environment for all employees. We believe in creating teams made up of individuals with various backgrounds, experiences, and perspectives. Diversity inspires innovation and collaboration and challenges us to produce better solutions. But more than this, diversity is our strength and a catalyst in our ability to change lives for the good. \nPhysical Requirements\n•       Constantly operates a computer and other office productivity machinery, such as copy machine, computer printer, calculator, etc.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Microstrategy', 'SQL', 'Dashboards']",2025-06-12 00:42:32
Azure Data Engineer,HTC Global Services,4 - 8 years,Not Disclosed,['Bengaluru( Murugeshpalya )'],"Job Summary:\nWe are looking for a highly skilled Azure Data Engineer with experience in building and managing scalable data pipelines using Azure Data Factory, Synapse, and Databricks. The ideal candidate should be proficient in big data tools and Azure services, with strong programming knowledge and a solid understanding of data architecture and cloud platforms.\n\nKey Responsibilities:",,,,"['Power Bi', 'Azure Databricks', 'Azure Data Factory', 'Synapse', 'Python', 'Java', 'Scala', 'Kafka', 'big data tools', 'SQL', 'EventHub', 'Azure cloud services', 'Spark']",2025-06-12 00:42:36
"Data engineer with Gen AI- Balewadi, pune- hybrid",Indian MNC,5 - 10 years,15-30 Lacs P.A.,['Pune( Balewadi )'],"Role & responsibilities\nWe are seeking a skilled Data Engineer with advanced expertise in Python, PySpark, Databricks, and Machine Learning, along with a working knowledge of Generative and Agentic AI. This role is critical in ensuring data integrity and driving innovation across enterprise systems. You will design and implement ML-driven solutions to enhance Data Governance & Data Privacy initiatives through automation, self-service capabilities, and scalable, AI-enabled innovation.\nKey Responsibilities:\nImplement ML and Generative/Agentic AI solutions to optimize Data Governance processes.\nDesign, develop, and maintain scalable data pipelines using Python, PySpark, and Databricks.\nDevelop automation frameworks to support data quality, lineage, classification, and access control.\nDevelop and deploy machine learning models to uncover data patterns, detect anomalies, and enhance data governance and privacy compliance\nCollaborate with data stewards, analysts, and governance teams to build self-service data capabilities.\nWork with Databricks, Azure Data Lake, AWS, and other cloud-based data platforms for data engineering.\nBuild, configure, and integrate APIs for seamless system interoperability.\nEnsure data integrity, consistency, and compliance across systems and workflows.\nIntegrate AI models to support data discovery, metadata enrichment, and intelligent recommendations.\nOptimize data architecture to support analytics, reporting, and governance use cases.\nMonitor and improve the performance of ML/AI components in production environments.\nStay updated with emerging AI and data engineering technologies to drive continuous innovation.\nTechnical Skills:\nStrong programming skills in Python, PySpark, SQL for data processing and automation.\nExperience with Databricks and Snowflake (preferred) for building and maintaining data pipelines.\nExperience with Machine Learning model development and Generative/Agentic AI frameworks (e.g. LLMs, Transformers, LangChain) especially in the Data Management space\nExperience working with REST APIs & JSON for service integration\nExperience working with cloud-based platforms such as Azure, AWS, or GCP\nPower BI dashboard development experience is a plus.\nSoft Skills:\nStrong problem-solving skills and attention to detail.\nExcellent communication and collaboration abilities, with experience working across technical and business teams",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Generative Ai', 'Azure Databricks', 'Ml']",2025-06-12 00:42:39
"Data Engineer Openings at Advantum Health, Hyderabad",Advantum Health,3 - 5 years,Not Disclosed,['Hyderabad'],"Data Engineer openings at Advantum Health Pvt Ltd, Hyderabad.\nOverview:\nWe are looking for a Data Engineer to build and optimize robust data pipelines that support AI and RCM analytics. This role involves integrating structured and unstructured data from diverse healthcare systems into scalable, AI-ready datasets.\nKey Responsibilities:\nDesign, implement, and optimize data pipelines for ingesting and transforming healthcare and RCM data.\nBuild data marts and warehouses to support analytics and machine learning.\nEnsure data quality, lineage, and governance across AI use cases.\nIntegrate data from EMRs, billing platforms, claims databases, and third-party APIs.\nSupport data infrastructure in a HIPAA-compliant cloud environment.\nQualifications:\nBachelors in Computer Science, Data Engineering, or related field.\n3+ years of experience with ETL/ELT pipelines using tools like Apache Airflow, dbt, or Azure Data Factory.\nStrong SQL and Python skills.\nExperience with healthcare data standards (HL7, FHIR, X12) preferred.\nFamiliarity with data lake house architectures and AI integration best practices\nPh: 9177078628\nEmail id: jobs@advantumhealth.com\nAddress: Advantum Health Private Limited, Cyber gateway, Block C, 4th floor Hitech City, Hyderabad.\nDo follow us on LinkedIn, Facebook, Instagram, YouTube and Threads\nAdvantum Health LinkedIn Page:\nhttps://lnkd.in/gVcQAXK3\n\nAdvantum Health Facebook Page:\nhttps://lnkd.in/g7ARQ378\n\nAdvantum Health Instagram Page:\nhttps://lnkd.in/gtQnB_Gc\n\nAdvantum Health India YouTube link:\nhttps://lnkd.in/g_AxPaPp\n\nAdvantum Health Threads link:\nhttps://lnkd.in/gyq73iQ6",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'SQL', 'Python', 'Airflow', 'ETL', 'Elt']",2025-06-12 00:42:42
Data Engineer,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will responsible for designing, building, maintaining, analyzing, and interpreting data to provide actionable insights that drive business decisions. This role involves working with large datasets, developing reports, supporting and executing data governance initiatives, and visualizing data to ensure data is accessible, reliable, and efficiently managed. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture and ETL processes.\nRoles & Responsibilities:\nDesign, develop, and maintain data solutions for data generation, collection, and processing.\nBe a key team member that assists in the design and development of the data pipeline.\nCreate data pipelines and ensure data quality by implementing ETL processes to migrate and deploy data across systems.\nContribute to the design, development, and implementation of data pipelines, ETL/ELT processes, and data integration solutions.\nTake ownership of data pipeline projects from inception to deployment, manage scope, timelines, and risks.\nCollaborate with cross-functional teams to understand data requirements and design solutions that meet business needs.\nDevelop and maintain data models, data dictionaries, and other documentation to ensure data accuracy and consistency.\nImplement data security and privacy measures to protect sensitive data.\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nCollaborate and communicate effectively with product teams.\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients.\nBasic Qualifications and Experience\nMasters degree and 1 to 3 years of experience in Computer Science, IT, or related field OR\nBachelors degree and 3 to 5 years of experience in Computer Science, IT, or related field OR\nDiploma and 7 to 9 years of experience in Computer Science, IT, or related field\nMust-Have Skills:\nHands-on experience with big data technologies and platforms, such as Databricks, Apache Spark (PySpark, SparkSQL), workflow orchestration, performance tuning on big data processing.\nProficiency in data analysis tools (e.g., SQL) and experience with data visualization tools.\nExcellent problem-solving skills and the ability to work with large, complex datasets.\nPreferred Qualifications:\nGood-to-Have Skills:\nExperience with ETL tools such as Apache Spark, and various Python packages related to data processing, machine learning model development.\nStrong understanding of data modeling, data warehousing, and data integration concepts.\nKnowledge of Python/R, Databricks, SageMaker, cloud data platforms.\nProfessional Certifications:\nCertified Data Engineer / Data Analyst (preferred on Databricks or cloud environments).\nCertified Data Scientist (preferred on Databricks or Cloud environments).\nMachine Learning Certification (preferred on Databricks or Cloud environments).\nSoft Skills:\nExcellent critical-thinking and problem-solving skills.\nStrong communication and collaboration skills.\nDemonstrated awareness of how to function in a team setting.\nDemonstrated presentation skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'SageMaker', 'R', 'data modeling', 'data warehousing', 'cloud data platforms', 'Databricks', 'ETL', 'data integration', 'Python']",2025-06-12 00:42:45
Azure Data Engineer,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of the role is to support process delivery by ensuring daily performance of the Production Specialists, resolve technical escalations and develop technical capability within the Production Specialists.\n\nDo\nOversee and support process by reviewing daily transactions on performance parameters\nReview performance dashboard and the scores for the team\nSupport the team in improving performance parameters by providing technical support and process guidance\nRecord, track, and document all queries received, problem-solving steps taken and total successful and unsuccessful resolutions\nEnsure standard processes and procedures are followed to resolve all client queries\nResolve client queries as per the SLAs defined in the contract\nDevelop understanding of process/ product for the team members to facilitate better client interaction and troubleshooting\nDocument and analyze call logs to spot most occurring trends to prevent future problems\nIdentify red flags and escalate serious client issues to Team leader in cases of untimely resolution\nEnsure all product information and disclosures are given to clients before and after the call/email requests\nAvoids legal challenges by monitoring compliance with service agreements\n\nHandle technical escalations through effective diagnosis and troubleshooting of client queries\nManage and resolve technical roadblocks/ escalations as per SLA and quality requirements\nIf unable to resolve the issues, timely escalate the issues to TA & SES\nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers and clients business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract SLAs\nBuild people capability to ensure operational excellence and maintain superior customer service levels of the existing account/client\nMentor and guide Production Specialists on improving technical knowledge\nCollate trainings to be conducted as triage to bridge the skill gaps identified through interviews with the Production Specialist\nDevelop and conduct trainings (Triages) within products for production specialist as per target\nInform client about the triages being conducted\nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nIdentify and document most common problems and recommend appropriate resolutions to the team\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\nDeliver\n\nNoPerformance ParameterMeasure1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback, NSAT/ ESAT2Team ManagementProductivity, efficiency, absenteeism3Capability developmentTriages completed, Technical Test performance\nMandatory Skills: Azure Data Factory. Experience: 5-8 Years.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'azure databricks', 'azure data lake', 'ssas', 'ssrs', 'microsoft azure', 'azure data factory', 'ssis', 'msbi', 'sql server', 'sql']",2025-06-12 00:42:47
"Tcs is hiring For Azure Data Engineer (ADF, Python, Pyspark,DataBricks",Tata Consultancy Services,5 - 10 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Role & responsibilities\nStrong understanding of Azure environment (PaaS, IaaS) and experience in working with Hybrid model\nAt least 1 project experience in Azure Data Stack that involves components like Azure Data Lake, Azure Synapse Analytics, Azure Data Factory, Azure Data Bricks, Azure Analysis Service, Azure SQL DWH\nStrong hands-on SQL/T-SQL/Spark SQL and database concepts",,,,"['Azure Databricks', 'Azure Data Factory', 'Pyspark', 'Python Data', 'Microsoft Azure', 'Devops', 'Python', 'SQL']",2025-06-12 00:42:50
Senior Data Engineer,Epsilon,5 - 9 years,Not Disclosed,['Bengaluru'],"This position in the Engineering team under the Digital Experience organization. We drive the first mile of the customer experience through personalization of offers and content. We are currently on the lookout for a smart, highly driven engineer.\nYou will be part of a team that is focused on building & managing solutions, pipelines using marketing technology stacks. You will also be expected to Identify and implement improvements including for optimizing data delivery and automate processes/pipelines.\nThe incumbent is also expected to partner with various stakeholders, bring scientific rigor to design and develop high quality solutions.\nCandidate must have excellent verbal and written communication skills and be comfortable working in an entrepreneurial, startup environment within a larger company.\nClick here to view how Epsilon transforms marketing with 1 View, 1 Vision and 1 Voice.\n\nBrief Description of Role:\nExperience with both structured and unstructured data\nExperience working on AdTech or MarTech technologies.\nExperience in relational and non-relational databases and SQL (NoSQL is a plus).\nUnderstanding of Data Modeling, Data Catalog concepts and tools\nAbility to deal with data imperfections such as missing values, outliers, inconsistent formatting, etc.\nCollaborate with other members of the team to ensure high quality deliverables\nLearning and implementing the latest design patterns in data engineering\n\nData Management\nExperience with both structured and unstructured data\nExperience building Data and CI/CD pipelines\nExperience working on AdTech or MarTech technologies is added advantage\nExperience in relational and non-relational databases and SQL (NoSQL is a plus).\nHands on experience building ETL workflows/pipelines on large volumes of data\nGood understanding of Data Modeling, Data Warehouse, Data Catalog concepts and tools\nAble to identify, join, explore, and examine data from multiple disparate sources and formats\nAbility to reduce large quantities of unstructured or formless data and get it into a form in which it can be analyzed\nAbility to deal with data imperfections such as missing values, outliers, inconsistent formatting, etc.\nDevelopment\nAbility to write code in programming languages such as Python and shell script on Linux\nFamiliarity with development methodology such as Agile/Scrum\nLove to learn new technologies, keep abreast of the latest technologies within the cloud architecture, and drive your organization to adapt to emerging best practices\nGood knowledge of working in UNIX/LINUX systems\nQualifications\nBachelors degree in computer science with 5+ years of similar experience\nTech Stack: Python, SQL, Scripting language (preferably JavaScript)\nExperience or knowledge on Adobe Experience Platform (RT-CDP/AEP)\nExperience working in Cloud Platforms (GCP or AWS)\nFamiliarity with automated unit/integration test frameworks\nGood written and spoken communication skills, team player.\nStrong analytic thought process and ability to interpret findings",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Data Bricks', 'Python', 'SQL', 'Azure Aws', 'AWS']",2025-06-12 00:42:54
NLP Data Engineer - Risk Insights & Monitoring,MNC IT,10 - 12 years,25-30 Lacs P.A.,"['Pune', 'Mumbai (All Areas)']","Design and implement state-of-the-art NLP models, including but not limited to text classification, semantic search, sentiment analysis, named entity recognition, and summary generation.\nconduct data preprocessing, and feature engineering to improve model accuracy and performance.\nStay updated with the latest developments in NLP and ML, and integrate cutting-edge techniques into our solutions.\ncollaborate with Cross-Functional Teams: Work closely with data scientists, software engineers, and product managers to align NLP projects with business objectives.\ndeploy models into production environments and monitor their performance to ensure robustness and reliability.\nmaintain comprehensive documentation of processes, models, and experiments, and report findings to stakeholders.\nimplement and deliver high quality software solutions / components for the Credit Risk monitoring platform.\nleverage his/her expertise to mentor developers; review code and ensure adherence to standards.\napply a broad range of software engineering practices, from analyzing user needs and developing new features to automated testing and deployment\nensure the quality, security, reliability, and compliance of our solutions by applying our digital principles and implementing both functional and non-functional requirements\nbuild observability into our solutions, monitor production health, help to resolve incidents, and remediate the root cause of risks and issues\nunderstand, represent, and advocate for client needs\nshare knowledge and expertise with colleagues , help with hiring, and contribute regularly to our engineering culture and internal communities.\nExpertise -\nBachelor of Engineering or equivalent.\nIdeally 8-10Yrs years of experience in NLP based applications focused on Banking / Finance sector.\nPreference for experience in financial data extraction and classification.\nInterested in learning new technologies and practices, reuse strategic platforms and standards, evaluate options, and make decisions with long-term sustainability in mind.\nProficiency in programming languages such as Python & Java. Experience with frameworks like TensorFlow, PyTorch, or Keras.\nIn-depth knowledge of NLP techniques and tools, including spaCy, NLTK, and Hugging Face.\nExperience with data handling and processing tools like Pandas, NumPy, and SQL.\nPrior experience in agentic AI, LLMs ,prompt engineering and generative AI is a plus.\nBackend development and microservices using Java Spring Boot, J2EE, REST for implementing projects with high SLA of data availability and data quality.\nExperience of building cloud ready and migrating applications using Azure and understanding of the Azure Native Cloud services, software design and enterprise integration patterns.\nKnowledge of SQL and PL/SQL (Oracle) and UNIX, writing queries, packages, working with joins, partitions, looking at execution plans, and tuning queries.\nA real passion for and experience of Agile working practices, with a strong desire to work with baked in quality subject areas such as TDD, BDD, test automation and DevOps principles\nExperience in Azure development including Databricks , Azure Services , ADLS etc.\nExperience using DevOps toolsets like GitLab, Jenkins",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['data engineer', 'Natural Language Processing', 'Python', 'Java']",2025-06-12 00:42:57
Azure Data Engineer,Infosys,5 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Job description\nWe are looking for Azure Data Engineer's resources having minimum 5 to 9 years of Experience.\n\nRole & responsibilities\nBlend of technical expertise with 5 to 9 year of experience, analytical problem-solving, and collaboration with cross-functional teams. Design and implement Azure data engineering solutions (Ingestion & Curation)\nCreate and maintain Azure data solutions including Azure SQL Database, Azure Data Lake, and Azure Blob Storage.\nDesign, implement, and maintain data pipelines for data ingestion, processing, and transformation in Azure.\nUtilizing Azure Data Factory or comparable technologies, create and maintain ETL (Extract, Transform, Load) operations\nUse Azure Data Factory and Databricks to assemble large, complex data sets\nImplementing data validation and cleansing procedures will ensure the quality, integrity, and dependability of the data.\nEnsure data quality / security and compliance.\nOptimize Azure SQL databases for efficient query performance.\nCollaborate with data engineers, and other stakeholders to understand requirements and translate them into scalable and reliable data platform architectures.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Azure Data Factory', 'Azure Synapse', 'Pyspark', 'Azure Data Lake']",2025-06-12 00:43:00
Data Engineer 2,Uplers,3 - 8 years,Not Disclosed,['Bengaluru'],"About the Role:\nAs a Data Engineer, you will be part of the Data Engineering team with this role being inherently multi-functional, and the ideal candidate will work with Data Scientist, Analysts, Application teams across the company, as well as all other Data Engineering squads at Wayfair. We are looking for someone with a love for data, understanding requirements clearly and the ability to iterate quickly. Successful candidates will have strong engineering skills and communication and a belief that data-driven processes lead to phenomenal products.\n\nWhat you'll do:\nBuild and launch data pipelines, and data products focussed on SMART Org.\nHelping teams push the boundaries of insights, creating new product features using data, and powering machine learning models.\nBuild cross-functional relationships to understand data needs, build key metrics and standardize their usage across the organization.\nUtilize current and leading edge technologies in software engineering, big data, streaming, and cloud infrastructure\n\nWhat You'll Need:\nBachelor/Master degree in Computer Science or related technical subject area or equivalent combination of education and experience 3+ years relevant work experience in the Data Engineering field with web scale data sets.\nDemonstrated strength in data modeling, ETL development and data lake architecture.\nData Warehousing Experience with Big Data Technologies (Hadoop, Spark, Hive, Presto, Airflow etc.).\nCoding proficiency in at least one modern programming language (Python, Scala, etc)\nExperience building/operating highly available, distributed systems of data extraction, ingestion, and processing and query performance tuning skills of large data sets.\nIndustry experience as a Big Data Engineer and working along cross functional teams such as Software Engineering, Analytics, Data Science with a track record of manipulating, processing, and extracting value from large datasets.\nStrong business acumen. Experience leading large-scale data warehousing and analytics projects, including using GCP technologies Big Query, Dataproc, GCS, Cloud Composer, Dataflow or related big data technologies in other cloud platforms like AWS, Azure etc.\nBe a team player and introduce/follow the best practices on the data engineering space.\nAbility to effectively communicate (both written and verbally) technical information and the results of engineering design at all levels of the organization.\n\nGood to have :\nUnderstanding of NoSQL Database exposure and Pub-Sub architecture setup.\nFamiliarity with Bl tools like Looker, Tableau, AtScale, PowerBI, or any similar tools.\n\nPS: This role is with one of our clients who is a leading name in Retail Industry.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Data Engineering', 'Cloud Platform', 'Hive', 'GCP', 'Bigquery', 'Hadoop', 'SCALA', 'Big Data Technologies', 'Etl Development', 'Spark', 'Python']",2025-06-12 00:43:03
Senior Data Engineer,SPAN.IO,5 - 10 years,Not Disclosed,['Bengaluru( Indira Nagar )'],"Senior Data Engineer\n\nOur Mission\n\nSPAN is enabling electrification for all\nWe are a mission-driven company designing, building, and deploying products that electrify the built environment, reduce carbon emissions, and slow the effects of climate change.\nDecarbonization is the process to reduce or remove greenhouse gas emissions, especially carbon dioxide, from entering our atmosphere.\nElectrification is the process of replacing fossil fuel appliances that run on gas or oil with all-electric upgrades for a cleaner way to power our lives.\n\nAt SPAN, we believe in:\nEnabling homes and vehicles powered by clean energy\nMaking electrification upgrades possible\nBuilding more resilient homes with reliable backup\nDesigning a flexible and distributed electrical grid\n\nThe Role\nAs a Data Engineer you would be working to design, build, test and create infrastructure necessary for real time analytics and batch analytics pipelines. You will work with multiple teams within the org to provide analysis, insights on the data. You will also be involved in writing ETL processes that support data ingestion. You will also guide and enforce best practices for data management, governance and security. You will build infrastructure to monitor these data pipelines / ETL jobs / tasks and create tooling/infrastructure for providing visibility into these.\n\nResponsibilities\nWe are looking for a Data Engineer with passion for building data pipelines, working with product, data science and business intelligence teams and delivering great solutions. As a part of the team you:-\nAcquire deep business understanding on how SPAN data flows from IoT device to cloud through the system and build scalable and optimized data solutions that impact many stakeholders.\nBe an advocate for data quality and excellence of our platform.\nBuild tools that help streamline the management and operation of our data ecosystem.\nEnsure best practices and standards in our data ecosystem are shared across teams.\nWork with teams within the company to build close relationships with our partners to understand the value our platform can bring and how we can make it better.\nImprove data discovery by creating data exploration processes and promoting adoption of data sources across the company.\nHave a desire to write tools and applications to automate work rather than do everything by hand.\nAssist internal teams in building out data logging, alerting and monitoring for their applications\nAre passionate about CI/CD process.\nDesign, develop and establish KPIs to monitor analysis and provide strategic insights to drive growth and performance.\n\nAbout You\n\nRequired Qualifications\nBachelor's Degree in a quantitative discipline: computer science, statistics, operations research, informatics, engineering, applied mathematics, economics, etc.\n5+ years of relevant work experience in data engineering, business intelligence, research or related fields.\nExpert level production-grade, programming experience in at least one of these languages (Python, Kotlin, or other JVM based languages)\nExperience in writing clean, concise and well structured code in one of the above languages.\nExperience working with Infrastructure-as-code tools: Pulumi, Terraform, etc.\nExperience working with CI/CD systems: Circle-CI, Github Actions, Argo-CD, etc.\nExperience managing data engineering infrastructure through Docker and Kubernetes\nExperience working with latency data processing solutions like Flink, Prefect, AWS Kinesis, Kafka, Spark Stream processing etc.\nExperience with SQL/Relational databases, OLAP databases like Snowflake.\nExperience working in AWS: S3, Glue, Athena, MSK, EMR, ECR etc.\n\nBonus Qualifications\nExperience with the Energy industry\nExperience with building IoT and/or hardware products\nUnderstanding of electrical systems and residential loads\nExperience with data visualization using Tableau.\nExperience in Data loading tools like FiveTran as well as data debugging tools such as DataDog\n\nLife at SPAN\nOur Bengaluru team plays a pivotal role in SPANs continued growth and expansion. Together, were driving engineering, product development, and operational excellence to shape the future of home energy solutions.\nAs part of our team in India, youll have the opportunity to collaborate closely with our teams in the US and across the globe. This international collaboration fosters innovation, learning, and growth, while helping us achieve our bold mission of electrifying homes and advancing clean energy solutions worldwide.\nOur in-office culture offers the chance for dynamic interactions and hands-on teamwork, making SPAN a truly collaborative environment where every team members contribution matters.\nOur climate-focused culture is driven by a team of forward-thinkers, engineers, and problem-solvers who push boundaries every day.\nDo mission-driven work: Every role at SPAN directly advances clean energy adoption.\nBring powerful ideas to life: We encourage diverse ideas and perspectives to drive stronger products.\nNurture an innovation-first mindset: We encourage big thinking and bold action.\nDeliver exceptional customer value: We value hard work, and the ability to deliver exceptional customer value.\n\nBenefits at SPAN India\nGenerous paid leave\nComprehensive Insurance & Health Benefits\nCentrally located office in Bengaluru with easy access to public transit, dining, and city amenities\n\nInterested in joining our team? Apply today and well be in touch with the next steps!",Industry Type: Electronics Manufacturing,"Department: Production, Manufacturing & Engineering","Employment Type: Full Time, Permanent","['Terraform', 'Snowflake', 'AWS', 'Python', 'SQL', 'Java', 'Apache Flink', 'Kotlin']",2025-06-12 00:43:05
Data Engineer,reycruit,7 - 12 years,35-40 Lacs P.A.,['Hyderabad'],"Looking for 8+ years\nPython+Azure/Aws cloud is mandatory\n1st round- virtual\n2nd round- F2F\nMust have 7+ years of relevant experience- should have hands-on experience with ETL/ELT processes, cloud-based data solutions, and big data technologies.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Pipeline', 'Big Data', 'Elt', 'ETL']",2025-06-12 00:43:08
Technical Lead - Sr. Data Engineer,Edgematics Consulting,8 - 13 years,Not Disclosed,['Pune'],"About This Role :\n\nWe are looking for a talented and experienced Data Engineer with Tech Lead with hands-on expertise in any ETL Tool with full knowledge about CI/CD practices with leading a team technically more than 5 and client facing and create Data Engineering, Data Quality frameworks. As a tech lead must ensure to build ETL jobs, Data Quality Jobs, Big Data Jobs performed performance optimization by understanding the requirements, create re-usable assets and able to perform production deployment and preferably worked in DWH appliances Snowflake / redshift / Synapse\n\nResponsibilities\nWork with a team of engineers in designing, developing, and maintaining scalable and efficient data solutions using Any Data Integration (any ETL tool like Talend / Informatica) and any Big Data technologies.\nDesign, develop, and maintain end-to-end data pipelines using Any ETL Data Integration (any ETL tool like Talend / Informatica) to ingest, process, and transform large volumes of data from heterogeneous sources.\nHave good experience in designing cloud pipelines using Azure Data Factory or AWS Glues/Lambda.\nImplemented Data Integration end to end with any ETL technologies.\nImplement database solutions for storing, processing, and querying large volumes of structured and unstructured and semi-structured data\nImplement Job Migrations of ETL Jobs from Older versions to New versions.\nImplement and write advanced SQL scripts in SQL Database at medium to expert level. \nWork with technical team with client and provide guidance during technical challenges.\nIntegrate and optimize data flows between various databases, data warehouses, and Big Data platforms.\nCollaborate with cross-functional teams to gather data requirements and translate them into scalable and efficient data solutions.\nOptimize ETL, Data Load performance, scalability, and cost-effectiveness through optimization techniques.\nInteract with Client on a daily basis and provide technical progress and respond to technical questions.\nImplement best practices for data integration.\nImplement complex ETL data pipelines or similar frameworks to process and analyze massive datasets.\nEnsure data quality, reliability, and security across all stages of the data pipeline.\nTroubleshoot and debug data-related issues in production systems and provide timely resolution.\nStay current with emerging technologies and industry trends in data engineering technologies, CI/CD, and incorporate them into our data architecture and processes.\nOptimize data processing workflows and infrastructure for performance, scalability, and cost-effectiveness.\nProvide technical guidance and foster a culture of continuous learning and improvement.\nImplement and automate CI/CD pipelines for data engineering workflows, including testing, deployment, and monitoring.\nPerform migration to production deployment from lower environments, test & validate\n\nMust Have Skills\nMust be certified in any ETL tools, Database, Cloud.(Snowflake certified is more preferred)\nMust have implemented at least 3 end-to-end projects in Data Engineering.\nMust have worked on performance management optimization and tuning for data loads, data processes, data transformation in big data\nMust be flexible to write code using JAVA/Scala/Python etc. as required\nMust have implemented CI/CD pipelines using tools like Jenkins, GitLab CI, or AWS CodePipeline.\nMust have managed a team technically of min 5 members and guided the team technically.\nMust have the Technical Ownership capability of Data Engineering delivery.\nStrong communication capabilities with client facing.\nBachelor's or Master's degree in Computer Science, Engineering, or a related field.\n5 years of experience in software engineering or a related role, with a strong focus on Any ETL Tool, database, integration.\nProficiency in Any ETL tools like Talend , Informatica etc for Data Integration for building and orchestrating data pipelines.\nHands-on experience with relational databases such as MySQL, PostgreSQL, or Oracle, and NoSQL databases such as MongoDB, Cassandra, or Redis.\nSolid understanding of database design principles, data modeling, and SQL query optimization.\nExperience with data warehousing, Data Lake , Delta Lake concepts and technologies, data modeling, and relational databases.",Industry Type: IT Services & Consulting,"Department: Production, Manufacturing & Engineering","Employment Type: Full Time, Permanent","['Data Engineering', 'Snowflake', 'ETL', 'Azure Aws', 'Data Management', 'Big Data', 'Ci/Cd', 'Data Integration', 'Data Quality', 'Data Pipeline', 'Data Warehousing', 'Data Modeling', 'Data Governance']",2025-06-12 00:43:11
Azure Data Engineer,Hexaware Technologies,6 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","Hi\n\nWork Location : Chennai AND Bangalore\nWork location : Imm - 30 days\n\nPrimary: Azure Databricks,ADF, Pyspark SQL",,,,"['Pyspark', 'Azure Databricks', 'SQL', 'Azure Data Factory']",2025-06-12 00:43:15
Sr. AWS Databricks Data Engineer,Tata Consultancy Services,6 - 11 years,Not Disclosed,"['Kolkata', 'Pune', 'Chennai']","Role & responsibilities\n\nData Engineer, Expertise in AWS, Databricks and Pyspark",,,,"['AWS', 'Data Bricks', 'Pyspark', 'Data engineer', 'Aws Databricks']",2025-06-12 00:43:18
Lead Data Engineer,Conduent,8 - 13 years,Not Disclosed,['Noida'],"Job Overview \n\nWe are looking for a Data Engineer who will be part of our Analytics Practice and will be expected to actively work in a multi-disciplinary fast paced environment. This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project; its primary responsibility is the acquisition, transformation, loading and processing of data from a multitude of disparate data sources, including structured and unstructured data for advanced analytics and machine learning in a big data environment.\n\n\n Responsibilities: \nEngineer a modern data pipeline to collect, organize, and process data from disparate sources.\nPerforms data management tasks, such as conduct data profiling, assess data quality, and write SQL queries to extract and integrate data\nDevelop efficient data collection systems and sound strategies for getting quality data from different sources\nConsume and analyze data from the data pool to support inference, prediction and recommendation of actionable insights to support business growth.\nDesign and develop ETL processes using tools and scripting. Troubleshoot and debug ETL processes. Performance tuning and opitimization of the ETL processes.\nProvide support to new of existing applications while recommending best practices and leading projects to implement new functionality.\nCollaborate in design reviews and code reviews to ensure standards are met. Recommend new standards for visualizations.\nLearn and develop new ETL techniques as required to keep up with the contemporary technologies.\nReviews the solution requirements and architecture to ensure selection of appropriate technology, efficient use of resources and integration of multiple systems and technology.\nSupport presentations to Customers and Partners\nAdvising on new technology trends and possible adoption to maintain competitive advantage\n\n\n Experience Needed: \n8+ years of related experience is required.\nA BS or Masters degree in Computer Science or related technical discipline is required\nETL experience with data integration to support data marts, extracts and reporting\nExperience connecting to varied data sources\nExcellent SQL coding experience with performance optimization for data queries.\nUnderstands different data models like normalized, de-normalied, stars, and snowflake models. Worked with transactional, temporarl, time series, and structured and unstructured data.\nExperience on Azure Data Factory and Azure Synapse Analytics\nWorked in big data environments, cloud data stores, different RDBMS and OLAP solutions.\nExperience in cloud-based ETL development processes.\nExperience in deployment and maintenance of ETL Jobs.\nIs familiar with the principles and practices involved in development and maintenance of software solutions and architectures and in service delivery.\nHas strong technical background and remains evergreen with technology and industry developments.\nAt least 3 years of demonstrated success in software engineering, release engineering, and/or configuration management.\nHighly skilled in scripting languages like PowerShell.\nSubstantial experience in the implementation and exectuion fo CI/CD processes.\n\n\n Additional  \nDemonstrated ability to have successfully completed multiple, complex technical projects\nPrior experience with application delivery using an Onshore/Offshore model\nExperience with business processes across multiple Master data domains in a services based company\nDemonstrates a rational and organized approach to the tasks undertaken and an awareness of the need to achieve quality.\nDemonstrates high standards of professional behavior in dealings with clients, colleagues and staff.\nIs able to make sound and far reaching decisions alone on major issues and to take full responsibility for them on a technical basis.\nStrong written communication skills. Is effective and persuasive in both written and oral communication.\nExperience with gathering end user requirements and writing technical documentation\nTime management and multitasking skills to effectively meet deadlines under time-to-market pressure",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql coding', 'sql', 'configuration management', 'software engineering', 'release engineering', 'continuous integration', 'rdbms', 'sql queries', 'performance tuning', 'azure synapse', 'ci/cd', 'azure data factory', 'machine learning', 'data engineering', 'powershell', 'olap', 'etl', 'big data']",2025-06-12 00:43:22
Big Data Engineer,Client of Hiresquad Resources,5 - 8 years,22.5-30 Lacs P.A.,"['Noida', 'Hyderabad', 'Bengaluru']","Role: Data Engineer\nExp: 5 to 8 Years\nLocation: Bangalore, Noida, and Hyderabad (Hybrid, weekly 2 Days office must)\nNP: Immediate to 15 Days (Try to find only immediate joiners)\n\n\nNote:\nCandidate must have experience in Python, Kafka Streams, Pyspark, and Azure Databricks.\nNot looking for candidates who have only Exp in Pyspark and not in Python.\n\n\nJob Title: SSE Kafka, Python, and Azure Databricks (Healthcare Data Project)\nExperience:  5 to 8 years\n\nRole Overview:\nWe are looking for a highly skilled with expertise in Kafka, Python, and Azure Databricks (preferred) to drive our healthcare data engineering projects. The ideal candidate will have deep experience in real-time data streaming, cloud-based data platforms, and large-scale data processing. This role requires strong technical leadership, problem-solving abilities, and the ability to collaborate with cross-functional teams.\n\nKey Responsibilities:\nLead the design, development, and implementation of real-time data pipelines using Kafka, Python, and Azure Databricks.\nArchitect scalable data streaming and processing solutions to support healthcare data workflows.\nDevelop, optimize, and maintain ETL/ELT pipelines for structured and unstructured healthcare data.\nEnsure data integrity, security, and compliance with healthcare regulations (HIPAA, HITRUST, etc.).\nCollaborate with data engineers, analysts, and business stakeholders to understand requirements and translate them into technical solutions.\nTroubleshoot and optimize Kafka streaming applications, Python scripts, and Databricks workflows.\nMentor junior engineers, conduct code reviews, and ensure best practices in data engineering.\nStay updated with the latest cloud technologies, big data frameworks, and industry trends.\n\n\nRequired Skills & Qualifications:\n4+ years of experience in data engineering, with strong proficiency in Kafka and Python.\nExpertise in Kafka Streams, Kafka Connect, and Schema Registry for real-time data processing.\nExperience with Azure Databricks (or willingness to learn and adopt it quickly).\nHands-on experience with cloud platforms (Azure preferred, AWS or GCP is a plus).\nProficiency in SQL, NoSQL databases, and data modeling for big data processing.\nKnowledge of containerization (Docker, Kubernetes) and CI/CD pipelines for data applications.\nExperience working with healthcare data (EHR, claims, HL7, FHIR, etc.) is a plus.\nStrong analytical skills, problem-solving mindset, and ability to lead complex data projects.\nExcellent communication and stakeholder management skills.\n\n\n\nEmail: Sam@hiresquad.in",Industry Type: Medical Services / Hospital,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Kafka', 'Azure Databricks', 'Python', 'Etl Pipelines', 'Pyspark', 'spark architecture', 'Data Engineering', 'opps concepts', 'Data Streaming', 'Medallion Architecture', 'python scripts', 'schema registry', 'SQL Database', 'Nosql Databases', 'spark tuning', 'Kafka Streams', 'kafka connect']",2025-06-12 00:43:25
Big Data Engineer - Python+ PySpark + Spark,Hexaware Technologies,9 - 12 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","Experience - 9 years - 12 years\nLocation - Mumbai / Chennai / Bangalore / Pune\n\nDevelop and maintain scalable data pipelines using PySpark and Spark SQL for processing large datasets efficiently.\nWrite clean, reusable, and optimized code in Python for data manipulation, analysis, and automation tasks.",,,,"['PySpark', 'Spark', 'Python', 'SQL']",2025-06-12 00:43:28
Data Engineer,MNC,4 - 8 years,Not Disclosed,['Hyderabad'],"Job Title: Software Engineer -Data Engineer\nPosition: Software Engineer\nExperience: 4-6 years (Less YOE will be Rejected)\nCategory: Software Development/ Engineering\nShift Timings: 1:00 pm to 10:00 pm\nMain location: Hyderabad\nWork Type: Work from office\nNotice Period: 0-30 Days\nSkill: Python, Pyspark, Data Bricks\nEmployment Type: Full Time\n\n• Bachelor's in Computer Science, Computer Engineering or related field\nRequired qualifications to be successful in this role\nMust have Skills:\n• 3+ yrs. Development experience with Spark (PySpark), Python and SQL.\n• Extensive knowledge building data pipelines\n• Hands on experience with Databricks Devlopment\n• Strong experience with\n• Strong experience developing on Linux OS.\n• Experience with scheduling and orchestration (e.g. Databricks Workflows,airflow, prefect, control-m).\n\nGood to have skills:\n• Solid understanding of distributed systems, data structures, design principles.\n• Agile Development Methodologies (e.g. SAFe, Kanban, Scrum).\n• Comfortable communicating with teams via showcases/demos.\n• Play key role in establishing and implementing migration patterns for the Data Lake Modernization project.\n• Actively migrate use cases from our on premises Data Lake to Databricks on GCP.\n• Collaborate with Product Management and business partners to understand use case requirements and reporting.\n• Adhere to internal development best practices/lifecycle (e.g. Testing, Code Reviews, CI/CD, Documentation) .\n• Document and showcase feature designs/workflows.\n• Participate in team meetings and discussions around product development.\n• Stay up to date on industry latest industry trends and design patterns.\n• 3+ years experience with GIT.\n• 3+ years experience with CI/CD (e.g. Azure Pipelines).\n• Experience with streaming technologies, such as Kafka, Spark.\n• Experience building applications on Docker and Kubernetes.\n• Cloud experience (e.g. Azure, Google).\n\nInterested Candidates can drop your Resume on Mail id :- "" tarun.k@talent21.in """,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Bricks', 'Python', 'Pyspark', 'SQL', 'ETL', 'Airflow', 'Azure Pipelines', 'Kafka', 'Design', 'Docker', 'Azure Cloud', 'Control-M', 'Cicd Pipeline', 'Modernization', 'testing', 'Documentation', 'Workflow', 'Code Review', 'Gcp Cloud', 'Product Development', 'Agile Methodology', 'GIT', 'Linux', 'GCP', 'Data Lake', 'Kubernetes']",2025-06-12 00:43:31
Data Engineer,MNC,4 - 9 years,Not Disclosed,['Hyderabad'],"Job Title: Software Engineer -Data Engineer\nPosition: Software Engineer\nExperience: 4-9 years\nCategory: Software Development/ Engineering\nShift Timings: 1:00 pm to 10:00 pm\nMain location: Hyderabad\nWork Type: Work from office\nNotice Period: 0-30 Days\nSkill: Python, Pyspark, Data Bricks\nEmployment Type: Full Time\n\n• Bachelor's in Computer Science, Computer Engineering or related field\nRequired qualifications to be successful in this role\nMust have Skills:\n• 3+ yrs. Development experience with Spark (PySpark), Python and SQL.\n• Extensive knowledge building data pipelines\n• Hands on experience with Databricks Devlopment\n• Strong experience with\n• Strong experience developing on Linux OS.\n• Experience with scheduling and orchestration (e.g. Databricks Workflows,airflow, prefect, control-m).\n\nGood to have skills:\n• Solid understanding of distributed systems, data structures, design principles.\n• Agile Development Methodologies (e.g. SAFe, Kanban, Scrum).\n• Comfortable communicating with teams via showcases/demos.\n• Play key role in establishing and implementing migration patterns for the Data Lake Modernization project.\n• Actively migrate use cases from our on premises Data Lake to Databricks on GCP.\n• Collaborate with Product Management and business partners to understand use case requirements and reporting.\n• Adhere to internal development best practices/lifecycle (e.g. Testing, Code Reviews, CI/CD, Documentation) .\n• Document and showcase feature designs/workflows.\n• Participate in team meetings and discussions around product development.\n• Stay up to date on industry latest industry trends and design patterns.\n• 3+ years experience with GIT.\n• 3+ years experience with CI/CD (e.g. Azure Pipelines).\n• Experience with streaming technologies, such as Kafka, Spark.\n• Experience building applications on Docker and Kubernetes.\n• Cloud experience (e.g. Azure, Google).\n\nInterested Candidates can drop your Resume on Mail id :- "" kalyan.v@talent21.in """,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Bricks', 'Python', 'Pyspark', 'SQL', 'ETL', 'Airflow', 'Azure Pipelines', 'Kafka', 'Design', 'Docker', 'Azure Cloud', 'Control-M', 'Cicd Pipeline', 'Modernization', 'testing', 'Documentation', 'Workflow', 'Code Review', 'Gcp Cloud', 'Product Development', 'Agile Methodology', 'GIT', 'Linux', 'GCP', 'Data Lake', 'Kubernetes']",2025-06-12 00:43:33
"Senior Data Engineer (Snowflake, DBT)",Allegis Global Solutions (AGS),5 - 10 years,Not Disclosed,[],"Senior Data Engineer (Snowflake, DBT, Azure)\nJob Location: Hyderabad / Bangalore / Chennai / Kolkata / Noida/ Gurgaon / Pune / Indore / Mumbai\n\nJob Details\nTechnical Expertise:\nStrong proficiency in Snowflake architecture, including data sharing, partitioning, clustering, and materialized views.\nAdvanced experience with DBT for data transformations and workflow management.\nExpertise in Azure services, including Azure Data Factory, Azure Data Lake, Azure Synapse, and Azure Functions.\nData Engineering:\nProficiency in SQL, Python, or other relevant programming languages.\nStrong understanding of data modeling concepts, including star schema and normalization.\nHands-on experience with ETL/ELT pipelines and data integration tools.\n\nPreferred Qualifications:\nCertifications in Snowflake, DBT, or Azure Data Engineering.\nFamiliar with data visualization tools like Power BI or Tableau.\nKnowledge of CI/CD pipelines and DevOps practices for data workflows.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Data Build Tool', 'Azure']",2025-06-12 00:43:37
Data Engineer- MS Fabric,InfoCepts,5 - 9 years,Not Disclosed,['India'],"Position: Data Engineer – MS Fabric\n  Purpose of the Position: As an MS Fabric Data engineer you will be responsible for designing, implementing, and managing scalable data pipelines. Strong experience in implementation and management of lake House using MS Fabric Azure Tech stack (ADLS Gen2, ADF, Azure SQL) .\nProficiency in data integration techniques, ETL processes and data pipeline architectures. Well versed in Data Quality rules, principles and implementation.\n",,,,"['components', 'data', 'scala', 'delta', 'pyspark', 'data warehousing', 'rules', 'azure data factory', 'sql', 'parquet', 'analytics', 'sql azure', 'spark', 'oracle adf', 'data pipeline architecture', 'etl', 'python', 'azure synapse', 'microsoft azure', 'power bi', 'data bricks', 'data quality', 'system', 't', 'fabric', 'data integration', 'etl process']",2025-06-12 00:43:40
Immediate Joiner- Data Engineer,Healthedge,1 - 4 years,Not Disclosed,['Bengaluru'],"Data Engineer\nYou will be working with agile cross functional software development teams developing cutting age software to solve a significant problem in the Provider Data Management space. This hire will have experience building large scale complex data systems involving multiple cross functional data sets and teams. The ideal candidate will be excited about working on new product development, is comfortable pushing the envelope and challenging the status quo, sets high standards for him/herself and the team, and works well with ambiguity.\nWhat you will do:\nBuild data pipelines to assemble large, complex sets of data that meet non-functional and functional business requirements.\nWork closely with data architect, SMEs and other technology partners to develop & execute data architecture and product roadmap.\nBuild analytical tools to utilize the data pipeline, providing actionable insight into key business performance including operational efficiency and business metrics.\nWork with stakeholders including the leadership, product, customer teams to support their data infrastructure needs while assisting with data-related technical issues.\nAct as a subject matter expert to other team members for technical guidance, solution design and best practices within the customer organization.\nKeep current on big data and data visualization technology trends, evaluate, work on proof-of-concept and make recommendations on cloud technologies.\nWhat you bring:\n2+ years of data engineering experience working in partnership with large data sets (preferably terabyte scale)\nExperience in building data pipelines using any of the ETL tools such as Glue, ADF, Notebooks, Stored Procedures, SQL/Python constructs or similar.\nDeep experience working with industry standard RDBMS such Postgres, SQL Server, Oracle, MySQL etc. and any of the analytical cloud databases such as Big Query, Redshift, Snowflake or similar\nAdvanced SQL expertise and solid programming experience with Python and/or Spark\nExperience working with orchestration tools such as Airflow and building complex dependency workflows.\nExperience, developing and implementing Data Warehouse or Data Lake Architectures, OLAP technologies, data modeling with star/snowflake-schemas to enable analytics & reporting.\nGreat problem-solving capabilities, troubleshooting data issues and experience in stabilizing big data systems.\nExcellent communication and presentation skills as youll be regularly interacting with stakeholders and engineering leadership.\nBachelors or master's in quantitative disciplines such as Computer Science, Computer Engineering, Analytics, Mathematics, Statistics, Information Systems, or other scientific fields.\nBonus points:\nHands-on deep experience with cloud data migration, and experience working with analytic platforms like Fabric, Databricks on the cloud.\nCertification in one of the cloud platforms (AWS/GCP/Azure)\nExperience or demonstrated understanding with real-time data streaming tools like Kafka, Kinesis or any similar tools.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ETL', 'SQL', 'Pyspark', 'Cloud', 'Python']",2025-06-12 00:43:42
Data Engineer - ETL/ Python,Meritus Management Service,5 - 7 years,10-14 Lacs P.A.,['Indore'],"Focus on Python, you'll play a crucial role in designing, developing, and maintaining data pipelines and ETL processes. Python to manage large datasets, automate data workflows, and ensure data accuracy and efficiency across our organization.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pandas', 'MySQL', 'Sqlalchemy', 'Numpy', 'Python', 'Azure Synapse', 'Postgresql', 'Etl Process', 'SQL']",2025-06-12 00:43:45
Data Engineer,Lenskart,1 - 4 years,Not Disclosed,['Bengaluru'],"Key Responsibilities\nBuild and maintain scalable ETL/ELT data pipelines using Python and cloud-native tools.\nDesign and optimize data models and queries on Google BigQuery for analytical workloads.\nDevelop, schedule, and monitor workflows using orchestration tools like Apache Airflow or Cloud Composer.\nIngest and integrate data from multiple structured and semi-structured sources, including MySQL, MongoDB, APIs, and cloud storage.",,,,"['GCP', 'Bigquery', 'MySQL', 'MongoDB', 'Python']",2025-06-12 00:43:47
Data Engineer,Xenonstack,2 - 5 years,Not Disclosed,['Mohali( Phase 8B Mohali )'],"At XenonStack, We committed to become the Most Value Driven Cloud Native, Platform Engineering and Decision Driven Analytics Company. Our Consulting Services and Solutions towards the Neural Company and its Key Drivers.\nXenonStacks DataOps team is looking for a Data Engineer who will be responsible for employing techniques to create and sustain structures that allow for the analysis of data while remaining familiar with dominant programming and deployment strategies in the field.\nYou should demonstrate flexibility, creativity, and the capacity to receive and utilize constructive criticism. The ideal candidate should be highly skilled in all aspects of Python, Java/Scala, SQL and analytical skills.\nJob Responsibilities:\nDevelop, construct, test and maintain Data Platform Architectures\nAlign Data Architecture with business requirements\nLiaising with co-workers and clients to elucidate the requirements for each task.\nScalable and High Performant Data Platform Infrastructure that allows big data to be accessed and analysed quickly by BI & AI Teams.\nReformulating existing frameworks to optimize their functioning.\nTransforming Raw Data into InSights for manipulation by Data Scientists.\nEnsuring that your work remains backed up and readily accessible to relevant co-workers.\nRemaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.\nRequirements:\nTechnical Requirements\nExperience of Python, Java/Scala\nGreat Statistical / SQL based Analytical Skills\nExperience of Data Analytics Architectural Design Patterns for Batch, Event Driven and Real-Time Analytics Use Cases\nUnderstanding of Data warehousing, ETL tools, machine learning, Data EPIs\nExcellent in Algorithms and Data Systems\nUnderstanding of Distributed System for Data Processing and Analytics\nFamiliarity with Popular Data Analytics Framework like Hadoop , Spark , Delta Lake , Time Series / Analytical Stores Stores.\nProfessional Attributes:\nExcellent communication skills & Attention to detail.\nAnalytical mind and problem-solving Aptitude with Strong Organizational skills & Visual Thinking.\nBenefits:\nDiscover the benefits of joining our team:\nDynamic and purposeful work culture in a people-oriented organization contributing to multi-million-dollar projects with guaranteed job security.\nOpen, authentic, and transparent communication fostering a warm work environment.\nRegular constructive feedback and exposure to diverse technologies.\nRecognition and rewards for exceptional performance achievements.\nAccess to certification courses & Skill Sessions to develop continually and refine your skills.\nAdditional allowances for team members assigned to specific projects.\nSpecial skill allowances to acknowledge and compensate for unique expertise.\nComprehensive medical insurance policy for your health and well-being.\nTo Learn more about the company -\nWebsite - http://www.xenonstack.com/",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hadoop', 'Spark', 'ETL', 'Python', 'SQL', 'Java', 'Data Processing', 'Machine Learning']",2025-06-12 00:43:50
AWS Data Engineer,Infosys,5 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Significant 5 to 9 years of experience in designing and implementing scalable data engineering solutions on AWS.\nStrong proficiency in Python programming language.\nExpertise in serverless architecture and AWS services such as Lambda, Glue, Redshift, Kinesis, SNS, SQS, and CloudFormation.\nExperience with Infrastructure as Code (IaC) using AWS CDK for defining and provisioning AWS resources.\nProven leadership skills with the ability to mentor and guide junior team members.\nExcellent understanding of data modeling concepts and experience with tools like ERStudio.\nStrong communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.\nExperience with Apache Airflow for orchestrating data pipelines is a plus.\nKnowledge of Data Lakehouse, dbt, or Apache Hudi data format is a plus.\nRoles and Responsibilities\nDesign, develop, test, deploy, and maintain large-scale data pipelines using AWS services such as S3, Glue, Lambda, Redshift.\nCollaborate with cross-functional teams to gather requirements and design solutions that meet business needs.\nDesired Candidate Profile\n5-9 years of experience in an IT industry setting with expertise in Python programming language (Pyspark).\nStrong understanding of AWS ecosystem including S3, Glue, Lambda, Redshift.\nBachelor's degree in Any Specialization (B.Tech/B.E.).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Aws Glue', 'Pyspark', 'Aws Lambda', 'Amazon Redshift', 'Athena', 'Python']",2025-06-12 00:43:53
Data Ingest Engineer,"NTT DATA, Inc.",6 - 10 years,Not Disclosed,['Pune'],"Req ID: 323909\n\nWe are currently seeking a Data Ingest Engineer to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nJob DutiesThe Applications Development Technology Lead Analyst is a senior level position responsible for establishing and implementing new or revised application systems and programs in coordination with the Technology team.\nThis is a position within the Ingestion team of the DRIFT data ecosystem. The focus is on ingesting data in a timely , complete, and comprehensive fashion while using the latest technology available to Citi. The ability to leverage new and creative methods for repeatable data ingestion from a variety of data sources while always questioning ""is this the best way to solve this problem"" and ""am I providing the highest quality data to my downstream partners"" are the questions we are trying to solve.\n\nResponsibilities:\n""¢ Partner with multiple management teams to ensure appropriate integration of functions to meet goals as well as identify and define necessary system enhancements to deploy new products and process improvements\n""¢ Resolve variety of high impact problems/projects through in-depth evaluation of complex business processes, system processes, and industry standards\n""¢ Provide expertise in area and advanced knowledge of applications programming and ensure application design adheres to the overall architecture blueprint\n""¢ Utilize advanced knowledge of system flow and develop standards for coding, testing, debugging, and implementation\n""¢ Develop comprehensive knowledge of how areas of business, such as architecture and infrastructure, integrate to accomplish business goals\n""¢ Provide in-depth analysis with interpretive thinking to define issues and develop innovative solutions\n""¢ Serve as advisor or coach to mid-level developers and analysts, allocating work as necessary\n""¢ Appropriately assess risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding Citigroup, its clients and assets, by driving compliance with applicable laws, rules and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct and business practices, and escalating, managing and reporting control issues with transparency.\n\nMinimum Skills Required""¢ 6-10 years of relevant experience in Apps Development or systems analysis role\n""¢ Extensive experience system analysis and in programming of software applications\n""¢ Application Development using JAVA, Scala, Spark\n""¢ Familiarity with event driven applications and streaming data\n""¢ Experience with Confluent Kafka, HDFS, HIVE, structured and unstructured database systems (SQL and NoSQL)\n""¢ Experience with various schema and data types -> JSON, AVRO, Parquet, etc.\n""¢ Experience with various ELT methodologies and formats -> JDBC, ODBC, API, Web hook, SFTP, etc.\n""¢ Experience working within the Agile and version control tool sets (JIRA, Bitbucket, Git, etc.)\n""¢ Ability to adjust priorities quickly as circumstances dictate\n""¢ Demonstrated leadership and project management skills\n""¢ Consistently demonstrates clear and concise written and verbal communication",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['application software', 'system analysis', 'java', 'project management', 'applications programming', 'hive', 'scala', 'jdbc', 'bitbucket', 'sql', 'parquet', 'git', 'spark', 'data ingestion', 'json', 'debugging', 'api', 'jira', 'avro', 'odbc', 'elt', 'data engineering', 'nosql', 'app development', 'kafka', 'sftp', 'agile']",2025-06-12 00:43:57
AWS Data Engineer,Infosys,5 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","AWS Data Engineer\n\nTo Apply, use the below link:\nhttps://career.infosys.com/jobdesc?jobReferenceCode=INFSYS-EXTERNAL-210775&rc=0\n\nJOB Profile:\nSignificant 5 to 9 years of experience in designing and implementing scalable data engineering solutions on AWS.\nStrong proficiency in Python programming language.\nExpertise in serverless architecture and AWS services such as Lambda, Glue, Redshift, Kinesis, SNS, SQS, and CloudFormation.\nExperience with Infrastructure as Code (IaC) using AWS CDK for defining and provisioning AWS resources.\nProven leadership skills with the ability to mentor and guide junior team members.\nExcellent understanding of data modeling concepts and experience with tools like ERStudio.\nStrong communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.\nExperience with Apache Airflow for orchestrating data pipelines is a plus.\nKnowledge of Data Lakehouse, dbt, or Apache Hudi data format is a plus.\n\n\nRoles and Responsibilities\nDesign, develop, test, deploy, and maintain large-scale data pipelines using AWS services such as S3, Glue, Lambda, Redshift.\nCollaborate with cross-functional teams to gather requirements and design solutions that meet business needs.\nDesired Candidate Profile\n5-9 years of experience in an IT industry setting with expertise in Python programming language (Pyspark).\nStrong understanding of AWS ecosystem including S3, Glue, Lambda, Redshift.\nBachelor's degree in Any Specialization (B.Tech/B.E.).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Aws Glue', 'AWS Data Engineer', 'Pyspark', 'Aws Lambda', 'Redshift Aws', 'Python']",2025-06-12 00:44:00
Data Engineer,Meritus Management Service,5 - 10 years,10-20 Lacs P.A.,"['Nagpur', 'Pune', 'Gurugram']","We are looking for a skilled Data Engineer to design, build, and manage scalable data pipelines and ensure high-quality, secure, and reliable data infrastructure across our cloud and on-prem platforms.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Synapse Analytics', 'SQL', 'Azure Data Factory', 'Python', 'API Integration', 'Postgresql', 'Data Bricks', 'Scripting', 'SCALA', 'Data Lake', 'MongoDB', 'Data Warehousing', 'Data Modeling', 'ETL', 'Azure Devops']",2025-06-12 00:44:03
Data Engineer _Technology Lead,Broadridge,6 - 10 years,Not Disclosed,['Bengaluru'],"Key Responsibilities:\nAnalyzes and solve problems using technical experience, judgment and precedents\nProvides informal guidance to new team members\nExplains complex information to others in straightforward situations\n1. Data Engineering and Modelling:\nDesign & Develop Scalable Data Pipelines: Leverage AWS technologies to design, develop, and manage end-to-end data pipelines with services like .",,,,"['Star Schema', 'Snowflake', 'AWS', 'Apache Airflow']",2025-06-12 00:44:07
"Senior Data Engineer (Exp into Azure Databricks,Pyspark, SQL)",Adecco India,7 - 12 years,22.5-30 Lacs P.A.,"['Pune', 'Bengaluru']","Job Role & responsibilities:-\n\nUnderstanding operational needs by collaborating with specialized teams\nSupporting key business operations. This involves architecture designing, building and deploying data systems, pipelines etc\nDesigning and implementing agile, scalable, and cost efficiency solution on cloud data services.\nLead a team of developers, implement Sprint planning and executions to ensure timely deliveries\n\nTechnical Skill, Qualification & experience required:-\n\n7-10 years of experience in Azure Cloud Data Engineering, Azure Databricks, datafactory , Pyspark, SQL,Python\nHands on experience in Data Engineer, Azure Databricks, Data factory, Pyspark, SQL\nProficient in Cloud Services Azure\nArchitect and implement ETL and data movement solutions.\nMigrate data from traditional database systems to Cloud environment\nStrong hands-on experience for working with Streaming dataset\nBuilding Complex Notebook in Databricks to achieve business Transformations.\nHands-on Expertise in Data Refinement using Pyspark and Spark SQL\nFamiliarity with building dataset using Scala.\nFamiliarity with tools such as Jira and GitHub\nExperience leading agile scrum, sprint planning and review sessions\nGood communication and interpersonal skills\nComfortable working in a multidisciplinary team within a fast-paced environment\n\n* Immediate Joiners will be preferred only",Industry Type: Insurance,Department: Other,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineer', 'Azure data engineer', 'Data Bricks', 'SQL', 'Data Engineering', 'Python']",2025-06-12 00:44:10
Data Engineer,Meritus Management Service,4 - 9 years,9-18 Lacs P.A.,"['Pune', 'Gurugram']","The first Data Engineer specializes in traditional ETL with SAS DI and Big Data (Hadoop, Hive). The second is more versatile, skilled in modern data engineering with Python, MongoDB, and real-time processing.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Etl Pipelines', 'Big Data', 'Informatica', 'SAS DI', 'SQL', 'Hive', 'Hadoop', 'Talend', 'ETL Tool', 'Python']",2025-06-12 00:44:13
AWS Databricks Data Engineer,Tata Consultancy Services,6 - 11 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']","Role & responsibilities\n\nData Engineer, Expertise in AWS, Databricks and Pyspark",,,,"['Pyspark', 'databricks', 'AWS', 'data engineer', 'Aws Databricks']",2025-06-12 00:44:16
Data Engineer,Trantor,5 - 10 years,Not Disclosed,[],"We are looking for a skilled and motivated Data Engineer with deep expertise in GCP,\nBigQuery, Apache Airflow to join our data platform team. The ideal candidate should have hands-on experience building scalable data pipelines, automating workflows, migrating large-scale datasets, and optimizing distributed systems. The candidate should have experience with building Web APIs using Python. This role will play a key part in designing and maintaining robust data engineering solutions across cloud and on-prem environments.\nKey Responsibilities\nBigQuery & Cloud Data Pipelines:\nDesign and implement scalable ETL pipelines for ingesting large-scale datasets.\nBuild solutions for efficient querying of tables in BigQuery.\nAutomated scheduled data ingestion using Google Cloud services and scheduled\nApache Airflow DAGs",,,,"['Airflow', 'Etl Pipelines', 'GCP', 'Bigquery', 'Python', 'SFTP', 'ETL', 'SQL']",2025-06-12 00:44:19
Senior Data Engineer (Data Architect),Adastra Corp,8 - 13 years,Not Disclosed,[],"Join our innovative team and architect the future of data solutions on Azure, Synapse, and Databricks!\nSenior Data Engineer (Data Architect)\nAdditional Details:\nNotice Period: 30 days (maximum)\nLocation: Remote\nAbout the Role\nDesign and implement scalable data pipelines, data warehouses, and data lakes that drive business growth. Collaborate with stakeholders to deliver data-driven insights and shape the data landscape.\nRequirements\n8+ years of experience in data engineering and data architecture\nStrong expertise in Azure services (Synapse Analytics, Databricks, Storage, Active Directory)\nProven experience in designing and implementing data pipelines, data warehouses, and data lakes\nStrong understanding of data governance, data quality, and data security\nExperience with infrastructure design and implementation, including DevOps practices and tools",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Azure', 'DESIGN', 'Architecture', 'Synapse Analytics']",2025-06-12 00:44:21
Lead AWS Glue Data Engineer,Allegis Group,8 - 13 years,Not Disclosed,[],"Lead AWS Glue Data Engineer\nJob Location : Hyderabad / Bangalore / Chennai / Noida/ Gurgaon / Pune / Indore / Mumbai/ Kolkata\n\nWe are seeking a skilled Lead AWS Data Engineer with 8+ years of strong programming and SQL skills to join our team. The ideal candidate will have hands-on experience with AWS Data Analytics services and a basic understanding of general AWS services. Additionally, prior experience with Oracle and Postgres databases and secondary skills in Python and Azure DevOps will be an advantage.\n\nKey Responsibilities:\nDesign, develop, and optimize data pipelines using AWS Data Analytics services such as RDS, DMS, Glue, Lambda, Redshift, and Athena.\nImplement data migration and transformation processes using AWS DMS and Glue.\nWork with SQL (Oracle & Postgres) to query, manipulate, and analyse large datasets.\nDevelop and maintain ETL/ELT workflows for data ingestion and transformation.\nUtilize AWS services like S3, IAM, CloudWatch, and VPC to ensure secure and efficient data operations.\nWrite clean and efficient Python scripts for automation and data processing.\nCollaborate with DevOps teams using Azure DevOps for CI/CD pipelines and infrastructure management.\nMonitor and troubleshoot data workflows to ensure high availability and performance.\n\nPreferred Qualifications:\nAWS certifications in Data Analytics, Solutions Architect, or DevOps.\nExperience with data warehousing concepts and data lake implementations.\nHands-on experience with Infrastructure as Code (IaC) tools like Terraform or CloudFormation.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['RDS', 'Glue', 'DMS', 'Lambda', 'Redshift', 'Athena']",2025-06-12 00:44:24
Data Engineer - SAS Migration,Crisil,2 - 4 years,Not Disclosed,['Mumbai'],"The SAS to Databricks Migration Developer will be responsible for migrating existing SAS code, data processes, and workflows to the Databricks platform. This role requires expertise in both SAS and Databricks, with a focus on converting SAS logic into scalable PySpark and Python code. The developer will design, implement, and optimize data pipelines, ensuring seamless integration and functionality within the Databricks environment. Collaboration with various teams is essential to understand data requirements and deliver solutions that meet business needs",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'scala', 'pyspark', 'data warehousing', 'data migration', 'sql', 'spark', 'gcp', 'mysql', 'hadoop', 'bigquery', 'big data', 'etl', 'python', 'sas', 'teradata', 'airflow', 'microsoft azure', 'data engineering', 'sql server', 'dataproc', 'data bricks', 'cloud data flow', 'kafka', 'migration', 'sqoop', 'data flow']",2025-06-12 00:44:27
"Data Engineer (Python, Kafka Stream, Pyspark, and Azure Databricks.)",Hire Squad,5 - 8 years,20-30 Lacs P.A.,"['Noida', 'Hyderabad', 'Bengaluru']","Looking for Data Engineers, immediate joiners only, for Hyderabad, Bengaluru and Noida Location.\n\n*Must have experience in Python, Kafka Stream, Pyspark, and Azure Databricks.*\n\nRole and responsibilities:\nLead the design, development, and implementation of real-time data pipelines using Kafka, Python, and Azure Databricks.\nArchitect scalable data streaming and processing solutions to support healthcare data workflows.\nDevelop, optimize, and maintain ETL/ELT pipelines for structured and unstructured healthcare data.\nEnsure data integrity, security, and compliance with healthcare regulations (HIPAA, HITRUST, etc.).\nCollaborate with data engineers, analysts, and business stakeholders to understand requirements and translate them into technical solutions.\nTroubleshoot and optimize Kafka streaming applications, Python scripts, and Databricks workflows.\nMentor junior engineers, conduct code reviews, and ensure best practices in data engineering.\nStay updated with the latest cloud technologies, big data frameworks, and industry trends.\n\n\nPreferred candidate profile :\n5+ years of experience in data engineering, with strong proficiency in Kafka and Python.\nExpertise in Kafka Streams, Kafka Connect, and Schema Registry for real-time data processing.\nExperience with Azure Databricks (or willingness to learn and adopt it quickly).\nHands-on experience with cloud platforms (Azure preferred, AWS or GCP is a plus).\nProficiency in SQL, NoSQL databases, and data modeling for big data processing.\nKnowledge of containerization (Docker, Kubernetes) and CI/CD pipelines for data applications.\nExperience working with healthcare data (EHR, claims, HL7, FHIR, etc.) is a plus.\nStrong analytical skills, problem-solving mindset, and ability to lead complex data projects.\nExcellent communication and stakeholder management skills.\n\n\n\nInterested, call:\nRose (9873538143 / WA : 8595800635)\nrose2hiresquad@gmail.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Azure Databricks', 'Kafka Streams', 'Python', 'Etl Pipelines', 'development', 'Data Engineering', 'cloud-based data platforms', 'implementation', 'Data Streaming', 'Lead the design', 'processing solutions', 'Hitrust', 'large-scale data processing', 'ELT pipelines', 'HIPAA', 'real-time data streaming']",2025-06-12 00:44:30
Data Engineer,DATA ENGINEER,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Title: Data Engineer\nExperience: 5+ Years\nLocation: Hyderabad (Onsite)\nAvailability: Immediate Joiners Preferred\nJob Description:\nWe are seeking an experienced Data Engineer with a strong background in Java, Spark, and Scala to join our dynamic team in Hyderabad. The ideal candidate will be responsible for building scalable data pipelines, optimizing data processing workflows, and supporting data-driven solutions for enterprise-grade applications. This is a full-time onsite role.\nKey Responsibilities:\nDesign, develop, and maintain robust and scalable data processing pipelines.\nWork with large-scale data using distributed computing technologies like Apache Spark.\nDevelop applications and data integration workflows using Java and Scala.\nCollaborate with cross-functional teams including Data Scientists, Analysts, and Product Managers.\nEnsure data quality, integrity, and security in all data engineering solutions.\nMonitor and troubleshoot performance and data issues in production systems.\nMust-Have Skills:\nStrong hands-on experience with Java, Apache Spark, and Scala.\nProven experience working on large-scale data processing systems.\nSolid understanding of distributed systems and performance tuning.\nGood-to-Have Skills:\nExperience with Hadoop, Hive, and HDFS.\nFamiliarity with data warehousing concepts and ETL processes.\nExposure to cloud data platforms is a plus.\nDesired Candidate Profile:\n5+ years of relevant experience in data engineering or big data technologies.\nStrong problem-solving and analytical skills.\nExcellent communication and collaboration skills.\nAbility to work independently in a fast-paced environment.\nAdditional Details:\nWork Mode: Onsite (Hyderabad)\nEmployment Type: Full-time\nNotice Period: Immediate joiners highly preferred, candidates serving notice period.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'SCALA', 'Spark', 'Hive', 'Hadoop', 'Kafka']",2025-06-12 00:44:33
Lead Data Engineer,"NTT DATA, Inc.",5 - 10 years,Not Disclosed,['Bengaluru'],"Req ID: 306669\n\nWe are currently seeking a Lead Data Engineer to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\n Position Overview  We are seeking a highly skilled and experienced Lead Data/Product Engineer to join our dynamic team. The ideal candidate will have a strong background in streaming services and AWS cloud technology, leading teams and directing engineering workloads. This is an opportunity to work on the core systems supporting multiple secondary teams, so a history in software engineering and interface design would be an advantage.\n\n\n\n Key Responsibilities  \n\nLead and direct a small team of engineers engaged in\n\n- Engineering reuseable assets for the later build of data products\n\n- Building foundational integrations with Kafka, Confluent Cloud and AWS\n\n- Integrating with a large number of upstream and downstream technologies\n\n- Providing best in class documentation for downstream teams to develop, test and run data products built using our tools\n\n- Testing our tooling, and providing a framework for downstream teams to test their utilisation of our products\n\n- Helping to deliver CI, CD and IaC for both our own tooling, and as templates for downstream teams\n\n\n\n Required Skills and Qualifications  \n\n\n\n- Bachelor's degree in Computer Science, Engineering, or related field\n\n- 5+ years of experience in data engineering\n\n- 3+ years of experience with real time (or near real time) streaming systems\n\n- 2+ years of experience leading a team of data engineers\n\n- A willingness to independently learn a high number of new technologies and to lead a team in learning new technologies\n\n- Experience in AWS cloud services, particularly Lambda, SNS, S3, and EKS, API Gateway\n\n- Strong experience with Python\n\n- Strong experience in Kafka\n\n- Excellent understanding of data streaming architectures and best practices\n\n- Strong problem-solving skills and ability to think critically\n\n- Excellent communication skills to convey complex technical concepts both directly and through documentation\n\n- Strong use of version control and proven ability to govern a team in the best practice use of version control\n\n- Strong understanding of Agile and proven ability to govern a team in the best practice use of Agile methodologies\n\n\n\n Preferred Skills and Qualifications  \n\n   \n\n- An understanding of cloud networking patterns and practises\n\n- Experience with working on a library or other long term product\n\n- Knowledge of the Flink ecosystem\n\n- Experience with terraform\n\n- Experience with CI pipelines\n\n- Ability to code in a JVM language\n\n- Understanding of GDPR and the correct handling of PII\n\n- Knowledge of technical interface design\n\n- Basic use of Docker",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'version control', 'kafka', 'cloud formation aws', 'agile', 'jvm', 'cloud services', 'api gateway', 'eks', 'data engineering', 'docker', 'sql', 'java', 'lambda expressions', 'aws cloud', 'devops', 'real time pcr', 'sns', 'terraform', 'software engineering', 'aws', 'agile methodology', 'interface design']",2025-06-12 00:44:36
"Senior Data Engineer Databricks, ADF, PySpark",Suzva Software Technologies,6 - 11 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","Senior Data Engineer (Remote, Contract 6 Months) Databricks, ADF, and PySpark.\nWe are hiring a Senior Data Engineer for a 6-month remote contract position. The ideal candidate is highly skilled in building scalable data pipelines and working within the Azure cloud ecosystem, especially Databricks, ADF, and PySpark. You'll work closely with cross-functional teams to deliver enterprise-level data engineering solutions.\n\nKeyResponsibilities\nBuild scalable ETL pipelines and implement robust data solutions in Azure.\n\nManage and orchestrate workflows using ADF, Databricks, ADLS Gen2, and Key Vaults.\n\nDesign and maintain secure and efficient data lake architecture.\n\nWork with stakeholders to gather data requirements and translate them into technical specs.\n\nImplement CI/CD pipelines for seamless data deployment using Azure DevOps.\n\nMonitor data quality, performance bottlenecks, and scalability issues.\n\nWrite clean, organized, reusable PySpark code in an Agile environment.\n\nDocument pipelines, architectures, and best practices for reuse.\n\nMustHaveSkills\nExperience: 6+ years in Data Engineering\n\nTech Stack: SQL, Python, PySpark, Spark, Azure Databricks, ADF, ADLS Gen2, Azure DevOps, Key Vaults\n\nCore Expertise: Data Warehousing, ETL, Data Pipelines, Data Modelling, Data Governance\n\nAgile, SDLC, Containerization (Docker), Clean coding practices\n\nGoodToHaveSkills\nEvent Hubs, Logic Apps\n\nPower BI\n\nStrong logic building and competitive programming background\n\nLocation : - Remote,Mumbai, Delhi / NCR, Bengaluru , Kolkata, Chennai, Hyderabad, Ahmedabad, Pune",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Databricks', 'ADF', 'PySpark', 'ADLS Gen2', 'Azure Databricks', 'Key Vaults', 'Spark', 'Azure DevOps', 'SQL', 'Python']",2025-06-12 00:44:40
