title,company,experience,salary,locations,description,industry,department,employment_type,skills,scraped_at
Data Engineer - Python/SQL,Wipro,3 - 5 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\n\n\n\nDo\n1. Instrumental in understanding the requirements and design of the product/ software\nDevelop software solutions by studying information needs, studying systems flow, data usage and work processes\nInvestigating problem areas followed by the software development life cycle\nFacilitate root cause analysis of the system issues and problem statement\nIdentify ideas to improve system performance and impact availability\nAnalyze client requirements and convert requirements to feasible design\nCollaborate with functional teams or systems analysts who carry out the detailed investigation into software requirements\nConferring with project managers to obtain information on software capabilities\n\n\n\n2. Perform coding and ensure optimal software/ module development\nDetermine operational feasibility by evaluating analysis, problem definition, requirements, software development and proposed software\nDevelop and automate processes for software validation by setting up and designing test cases/scenarios/usage cases, and executing these cases\nModifying software to fix errors, adapt it to new hardware, improve its performance, or upgrade interfaces.\nAnalyzing information to recommend and plan the installation of new systems or modifications of an existing system\nEnsuring that code is error free or has no bugs and test failure\nPreparing reports on programming project specifications, activities and status\nEnsure all the codes are raised as per the norm defined for project / program / account with clear description and replication patterns\nCompile timely, comprehensive and accurate documentation and reports as requested\nCoordinating with the team on daily project status and progress and documenting it\nProviding feedback on usability and serviceability, trace the result to quality risk and report it to concerned stakeholders\n\n\n\n3. Status Reporting and Customer Focus on an ongoing basis with respect to project and its execution\nCapturing all the requirements and clarifications from the client for better quality work\nTaking feedback on the regular basis to ensure smooth and on time delivery\nParticipating in continuing education and training to remain current on best practices, learn new programming languages, and better assist other team members.\nConsulting with engineering staff to evaluate software-hardware interfaces and develop specifications and performance requirements\nDocument and demonstrate solutions by developing documentation, flowcharts, layouts, diagrams, charts, code comments and clear code\nDocumenting very necessary details and reports in a formal way for proper understanding of software from client proposal to implementation\nEnsure good quality of interaction with customer w.r.t. e-mail content, fault report tracking, voice calls, business etiquette etc\nTimely Response to customer requests and no instances of complaints either internally or externally\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure\n1.Continuous Integration, Deployment & Monitoring of Software100% error free on boarding & implementation, throughput %, Adherence to the schedule/ release plan\n2.Quality & CSATOn-Time Delivery, Manage software, Troubleshoot queries,Customer experience, completion of assigned certifications for skill upgradation\n3.MIS & Reporting100% on time MIS & report generation\nMandatory Skills: Python for Insights.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'module development', 'Data Engineering', 'software development', 'software programs', 'SQL']",2025-06-12 13:40:38
Data Engineer,AMERICAN EXPRESS,2 - 4 years,13-17 Lacs P.A.,"['Gurugram', 'Delhi / NCR']","Role & responsibilities\nUnderstanding business use cases and be able to convert to technical design\nPart of a cross-disciplinary team, working closely with other data engineers, software engineers, data scientists, data managers and business partners.\nYou will be designing scalable, testable and maintainable data pipelines\nIdentify areas for data governance improvements and help to resolve data quality problems through the appropriate choice of error detection and correction, process control and improvement, or process design changes",,,,"['Spark', 'SQL', 'Python', 'Hadoop', 'Big Data']",2025-06-12 13:40:40
Data Engineer,Visa,10 - 13 years,Not Disclosed,['Bengaluru'],"Translate business requirements and source system understanding into technical solutions using Opensource Tech Stack, Big Data, Java.\nWork with business partners directly to seek clarity on requirements.\nDefine solutions in terms of components, modules, and algorithms.\nDesign, develop, document, and implement new programs and subprograms, as we'll as enhancements, modifications and corrections to existing software.\nCreate technical documentation and procedures for installation and maintenance.\nWrite Unit Tests covering known use cases using appropriate tools.\nIntegrate test frameworks in the development process.\nWork with operations to get the solutions deployed.\nTake ownership of production deployment of code.\nCome up with Coding and Design best practices.\nThrive in a self-motivated, internal-innovation driven environment.\nAdapt quickly to new application knowledge and changes.\nThis is a hybrid position. Expectation of days in office will be confirmed by your Hiring Manager.\n\n\nBachelor degree in Computer Science, Electrical Engineering, Information Systems or other technical discipline.\nMinimum of 1 plus years of software development experience in Hadoop using Spark, Scala, Hive.\nExpertise in Object Oriented Programming Language Java, Python.\nExperience using CI CD Process, version control and bug tracking tools.\nResult-oriented with strong analytical and problem-solving skills.\nExperience with automation of job execution, validation and comparison of data files on Hadoop Environment at the field level.\nExperience in working with a small team and being a team player.\nStrong communication skills with proven ability to present complex ideas and document them in a clear and concise way.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Electrical engineering', 'Automation', 'Version control', 'Coding', 'Analytical', 'SCALA', 'Object oriented programming', 'Python', 'Technical documentation']",2025-06-12 13:40:43
Data Engineer,PwC India,4 - 8 years,Not Disclosed,['Bengaluru'],"If Interested please fill the below application link : https://forms.office.com/r/Zc8wDfEGEH\n\n\nResponsibilities:\nDeliver projects integrating data flows within and across technology systems.\nLead data modeling sessions with end user groups, project stakeholders, and technology teams to produce logical and physical data models.",,,,"['Pyspark', 'Aws Cloud', 'Azure Cloud', 'Python']",2025-06-12 13:40:46
Data Engineer,AMERICAN EXPRESS,3 - 8 years,Not Disclosed,['Chennai'],"You Lead the Way. We've Got Your Back.\n\nWith the right backing, people and businesses have the power to progress in incredible ways. When you join Team Amex, you become part of a global and diverse community of colleagues with an unwavering commitment to back our customers, communities and each other. Here, youll learn and grow as we help you create a career journey thats unique and meaningful to you with benefits, programs, and flexibility that support you personally and professionally.\nAt American Express, you’ll be recognized for your contributions, leadership, and impact—every colleague has the opportunity to share in the company’s success. Together, we’ll win as a team, striving to uphold our company values and powerful backing promise to provide the world’s best customer experience every day. And we’ll do it with the utmost integrity, and in an environment where everyone is seen, heard and feels like they belong. As part of our diverse tech team, you can architect, code and ship software that makes us an essential part of our customers’ digital lives. Here, you can work alongside talented engineers in an open, supportive, inclusive environment where your voice is valued, and you make your own decisions on what tech to use to solve challenging problems. Amex offers a range of opportunities to work with the latest technologies and encourages you to back the broader engineering community through open source. And because we understand the importance of keeping your skills fresh and relevant, we give you dedicated time to invest in your professional development. Find your place in technology on #TeamAmex.",,,,"['Data Engineering', 'GCP', 'Airflow', 'Pyspark', 'Bigquery', 'Hadoop', 'Big Data', 'SQL', 'Python', 'Backend Development']",2025-06-12 13:40:48
Data Engineer,HCLTech,11 - 14 years,16-27.5 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Roles and Responsibilities\nDesign, develop, and maintain large-scale data pipelines using Azure Data Factory (ADF) to extract, transform, and load data from various sources into Snowflake Data Warehouse.\nDevelop complex SQL queries to optimize database performance and troubleshoot issues in Snowflake tables.\nCollaborate with cross-functional teams to gather requirements for reporting needs and design scalable solutions using Power BI.\nEnsure high-quality data modeling by creating logical and physical models for large datasets.\nTroubleshoot technical issues related to ETL processes, data quality, and performance tuning.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Power Bi', 'Snowflake', 'Azure Data Lake', 'SQL', 'Data Modeling', 'Data Warehousing']",2025-06-12 13:40:51
Data Engineer,HARMAN,5 - 10 years,Not Disclosed,['Bengaluru'],"-Strong analytical thinking and problem-solving skills, with the ability to translate complex data into actionable insights\n-Excellent communication skills, with the ability to effectively convey complex findings to both technical and non-technical stakeholders.\nCandidate to work form SRIB Bangalore with 3 days working from office is mandatory\n  What You Will Do",,,,"['Digital media', 'CTV', 'Analytical', 'Machine learning', 'Agile', 'Data processing', 'Automotive', 'Python']",2025-06-12 13:40:54
Data Engineer - Hadoop,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Wipro Limited (NYSEWIT, BSE507685, NSEWIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients’ most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.\n About The Role  _x000D_\n\n \n\nRole Purpose  \nThe purpose of the role is to support process delivery by ensuring daily performance of the Production Specialists, resolve technical escalations and develop technical capability within the Production Specialists.\n\n\n ? _x000D_\n\n \n\nDo  \n\n\nOversee and support process by reviewing daily transactions on performance parameters\nReview performance dashboard and the scores for the team\nSupport the team in improving performance parameters by providing technical support and process guidance\nRecord, track, and document all queries received, problem-solving steps taken and total successful and unsuccessful resolutions\nEnsure standard processes and procedures are followed to resolve all client queries\nResolve client queries as per the SLA’s defined in the contract\nDevelop understanding of process/ product for the team members to facilitate better client interaction and troubleshooting\nDocument and analyze call logs to spot most occurring trends to prevent future problems\nIdentify red flags and escalate serious client issues to Team leader in cases of untimely resolution\nEnsure all product information and disclosures are given to clients before and after the call/email requests\nAvoids legal challenges by monitoring compliance with service agreements\n\n\n ? _x000D_\n\n\nHandle technical escalations through effective diagnosis and troubleshooting of client queries\nManage and resolve technical roadblocks/ escalations as per SLA and quality requirements\nIf unable to resolve the issues, timely escalate the issues to TA & SES\nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers’ and clients’ business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract SLA’s\n\n\n ? _x000D_\n\n\nBuild people capability to ensure operational excellence and maintain superior customer service levels of the existing account/client\nMentor and guide Production Specialists on improving technical knowledge\nCollate trainings to be conducted as triage to bridge the skill gaps identified through interviews with the Production Specialist\nDevelop and conduct trainings (Triages) within products for production specialist as per target\nInform client about the triages being conducted\nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nIdentify and document most common problems and recommend appropriate resolutions to the team\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\n\n ? _x000D_\n\nDeliver\nNoPerformance ParameterMeasure1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback, NSAT/ ESAT2Team ManagementProductivity, efficiency, absenteeism3Capability developmentTriages completed, Technical Test performance\n\nMandatory\n\nSkills:\nHadoop_x000D_.\n\nExperience5-8 Years_x000D_.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'data engineering', 'spark', 'troubleshooting', 'hadoop', 'cloudera', 'python', 'scala', 'big data analytics', 'oozie', 'airflow', 'pyspark', 'data warehousing', 'apache pig', 'machine learning', 'sql', 'mapreduce', 'sqoop', 'big data', 'aws', 'etl', 'hbase']",2025-06-12 13:40:56
Data Engineer,"NTT DATA, Inc.",1 - 4 years,Not Disclosed,['Bengaluru'],"Req ID: 321498\n\nWe are currently seeking a Data Engineer to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\nJob Duties""¢ Work closely with Lead Data Engineer to understand business requirements, analyse and translate these requirements into technical specifications and solution design.\n""¢ Work closely with Data modeller to ensure data models support the solution design\n""¢ Develop , test and fix ETL code using Snowflake, Fivetran, SQL, Stored proc.\n""¢ Analysis of the data and ETL for defects/service tickets (for solution in production ) raised and service tickets.\n""¢ Develop documentation and artefacts to support projects\n\nMinimum Skills Required""¢ ADF\n""¢ Fivetran (orchestration & integration)\n""¢ SQL\n""¢ Snowflake DWH",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'data engineering', 'sql', 'oracle adf', 'etl', 'hive', 'python', 'data analysis', 'oracle', 'informatica powercenter', 'amazon redshift', 'talend', 'data warehousing', 'power bi', 'plsql', 'tableau', 'data modeling', 'spark', 'etl tool', 'technical specifications', 'hadoop', 'informatica', 'unix']",2025-06-12 13:40:59
Data Engineer,IBM,2 - 4 years,Not Disclosed,['Bengaluru'],"Ingest new data from relational and non-relational source database systems into our warehouse. Connect data from various sources.\n\nIntegrate data from external sources to warehouse by building facts and dimensions based on the EPM data model requirements.\n\n\n\nAutomate data exchange and processing through serverless data pipelines.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience in data analysis and integration.\nExperience in data building and consuming fact and dimension tables.\nExperience in automating data integration through data pipelines.\nExperience with object-oriented programing languages such as Python.\nExperience with structured data processing languages such as SQL and Spark SQL.\nExperience with REST APIs and JSON\nExperience in IBM Cloud data processing services such as IBM Code Engine, IBM Event Streams (Apache Kafka).\nStrong understanding of Datawarehouse concepts and various data warehouse architectures\n\n\nPreferred technical and professional experience\nExperience with IBM Cloud architecture\nExperience with DevOps.\nKnowledge of Agile development methodologies\nExperience with building containerized applications and running them in serverless environments on the Cloud such as IBM Code Engine, Kubernetes, or Satellite.\nExperience with IBM Cognitive Enterprise Data Platform and CodeHub.\nExperience with data integration tools such as IBM DataStage or Informatica",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data warehousing', 'sql', 'spark', 'data warehousing concepts', 'hive', 'kubernetes', 'rest', 'data analysis', 'ibm cloud', 'datastage', 'data processing', 'data engineering', 'apache', 'cloud architecture', 'cognitive', 'devops', 'ibm datastage', 'kafka', 'json', 'agile', 'hadoop', 'informatica']",2025-06-12 13:41:01
Hadoop Data Engineer,Info Origin,0 - 2 years,Not Disclosed,['Gurugram'],"Job Overview:\nWe are looking for experienced Data Engineers proficient in Hadoop, Hive, Python, SQL, and Pyspark/Spark to join our dynamic team. Candidates will be responsible for designing, developing, and maintaining scalable big data solutions.\nKey Responsibilities:\nDevelop and optimize data pipelines for large-scale data processing.\nWork with structured and unstructured datasets to derive actionable insights.\nCollaborate with cross-functional teams to enhance data-driven decision-making.\nEnsure the performance, scalability, and reliability of data architectures.\nImplement best practices for data security and governance.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'Scalability', 'data security', 'spark', 'Hadoop', 'Data processing', 'big data', 'SQL', 'Python']",2025-06-12 13:41:04
Data Engineer,DXC Technology,3 - 5 years,Not Disclosed,['Bengaluru'],"Job Description:\nSr. Snowflake, DBT, SQL Developer:\nWe are looking for an experienced senior Backend developer (Snowflake, DBT and SQL) The person should have proven hands-on experience in Snowflake, DBT and SQL. Azure and DBT is always an added advantage as Nestle is using DBT for Transformation.\nThe overall experience of 7+ years in SQL and with minimum 3-5 years of experience in Snowflake.\nExperience with SnowFlake data warehouse\nExperience with Data Ingestion into SnowFlake such as Snowpipe and DBT\nDBT Development: Design, develop, and maintain DBT models, transformations, and SQL code to build efficient data pipelines for analytics and reporting.\nGood understanding of SnowFlake architecture and processing\nHands-on experience in Snowflake Cloud Development\nExperience in writing complicated SQLs, analyzing query performance, query tuning, database indexes partitions, and stored procedure development.\nImplementing ETL pipelines within and outside of a data warehouse using Python and Snowflakes Snow SQL\nQuerying Snowflake using SQL\ngood knowledge of SQL language and data warehousing concepts,\no Experience with technologies such as SQL Server 2008, as well as with newer ones like SSIS and stored procedures\no Designing database tables and structures.\no Creating views, functions, and stored procedures.\no Writing optimized SQL queries for integration with other applications.\no Creating database triggers for use in automation.\no Maintaining data quality and overseeing database security.\no Exceptional experience developing codes, testing for quality assurance, administering RDBMS, and monitoring of database\no Strong knowledge and experience with relational databases and database administration (indexes, optimization, etc. )\no Querying Snowflake using SQL, Experience with SQL and PLSQL, SQL query tuning, database performance tuning and data warehousing concepts, Knowledge of DBT and Azure (ADF) is desirable.\nRecruitment fraud is a scheme in which fictitious job opportunities are offered to job seekers typically through online services, such as false websites, or through unsolicited emails claiming to be from the company. These emails may request recipients to provide personal information or to make payments as part of their illegitimate recruiting process. DXC does not make offers of employment via social media networks and DXC never asks for any money or payments from applicants at any point in the recruitment process, nor ask a job seeker to purchase IT or other equipment on our behalf. More information on employment scams is available here .",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Automation', 'Manager Quality Assurance', 'RDBMS', 'Database administration', 'Stored procedures', 'SSIS', 'Analytics', 'Monitoring', 'Python']",2025-06-12 13:41:06
Big Data Developer/Data Engineer,Grid Dynamics,5 - 10 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\nExperience: 5 - 8 years\nEmployment Type: Full-Time\n\nJob Summary:\nWe are looking for a highly skilled Scala and Spark Developer to join our data engineering team. The ideal candidate will have strong experience in building scalable data processing solutions using Apache Spark and writing robust, high-performance applications in Scala. You will work closely with data scientists, data analysts, and product teams to design, develop, and optimize large-scale data pipelines and ETL workflows.\n\nKey Responsibilities:\nDevelop and maintain scalable data processing pipelines using Apache Spark and Scala.\nWork on batch and real-time data processing using Spark (RDD/DataFrame/Dataset).\nWrite efficient and maintainable code following best practices and coding standards.\nCollaborate with cross-functional teams to understand data requirements and implement solutions.\nOptimize performance of Spark jobs and troubleshoot data-related issues.\nIntegrate data from multiple sources and ensure data quality and consistency.\nParticipate in design reviews, code reviews, and provide technical leadership when needed.\nContribute to data modeling, schema design, and architecture discussions.\nRequired Skills:\nStrong programming skills in Scala.\nExpertise in Apache Spark (Core, SQL, Streaming).\nHands-on experience with distributed computing and large-scale data processing.\nExperience with data formats like Parquet, Avro, ORC, and JSON.\nGood understanding of functional programming concepts.\nFamiliarity with data ingestion tools (Kafka, Flume, Sqoop, etc.).\nExperience working with Hadoop ecosystem (HDFS, Hive, YARN, etc.) is a plus.\nStrong SQL skills and experience working with relational and NoSQL databases.\nExperience with version control tools like Git.\nPreferred Qualifications:\nBachelor's or Masters degree in Computer Science, Engineering, or related field.\nExperience with cloud platforms like AWS, Azure, or GCP (especially EMR, Databricks, etc.).\nKnowledge of containerization (Docker, Kubernetes) is a plus.\nFamiliarity with CI/CD tools and DevOps practices.ndidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Scala', 'Pyspark', 'Spark']",2025-06-12 13:41:08
Data Engineer,"NTT DATA, Inc.",4 - 9 years,Not Disclosed,['Chennai'],"Req ID: 324631\n\nWe are currently seeking a Data Engineer to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\n\nKey Responsibilities:\n\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\n\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\n\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\n\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\n\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\n\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\n\nAdhere to Agile practices throughout the solution development process.\n\nDesign, build, and deploy databases and data stores to support organizational requirements.\n\nBasic Qualifications:\n\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\n\nExperience with Informatica, Python, Databricks, Azure Data Engineer\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\n\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\n\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\n\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\n\nUndergraduate or Graduate degree preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'nosql', 'elastic search', 'cassandra', 'solr', 'core data', 'cloudera', 'python', 'data analytics', 'scala', 'kudu', 'microsoft azure', 'data engineering', 'data bricks', 'gen', 'java', 'apache nifi', 'spark', 'gcp', 'kafka', 'software engineering', 'hadoop', 'aws', 'data integration']",2025-06-12 13:41:11
Data Engineer,"NTT DATA, Inc.",4 - 9 years,Not Disclosed,['Chennai'],"Req ID: 324632\n\nWe are currently seeking a Data Engineer to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\n\nKey Responsibilities:\n\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\n\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\n\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\n\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\n\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\n\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\n\nAdhere to Agile practices throughout the solution development process.\n\nDesign, build, and deploy databases and data stores to support organizational requirements.\n\nBasic Qualifications:\n\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\n\nExperience with Informatica, Python, Databricks, Azure Data Engineer\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\n\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\n\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\n\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\n\nUndergraduate or Graduate degree preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'nosql', 'elastic search', 'cassandra', 'solr', 'core data', 'cloudera', 'python', 'data analytics', 'scala', 'kudu', 'microsoft azure', 'data engineering', 'data bricks', 'gen', 'java', 'apache nifi', 'spark', 'gcp', 'kafka', 'software engineering', 'hadoop', 'aws', 'data integration']",2025-06-12 13:41:13
Starburst Data Engineer/ Architect,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Starburst Data Engineer/Architect \nExpertise in Starburst and policy management like Ranger or equivalent.\nIn-depth knowledge of data modelling principles and techniques, including relational and dimensional.\nExcellent problem solving skills and the ability to troubleshoot and debug complex data related issues.\nStrong awareness of data tools and platforms like: Starburst, Snowflakes, Databricks and programming languages like SQL.\nIn-depth knowledge of data management principles, methodologies, and best practices with excellent analytical, problem-solving and decision making skills.\nDevelop, implement and maintain database systems using SQL.\nWrite complex SQL queries for integration with applications.\nDevelop and maintain data models (Conceptual, physical and logical) to meet organisational needs.\n\n\n ? \n\nDo\n\n1. Managing the technical scope of the project in line with the requirements at all stages\n\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\n\nb. Develop record management process and policies\n\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\n\nd. Providing sales data, proposals, data insights and account reviews to the client base\n\ne. Identify areas to increase efficiency and automation of processes\n\nf. Set up and maintain automated data processes\n\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\n\nh. Produce and track key performance indicators\n\n\n\n2. Analyze the data sets and provide adequate information\n\na. Liaise with internal and external clients to fully understand data content\n\nb. Design and carry out surveys and analyze survey data as per the customer requirement\n\nc. Analyze and interpret complex data sets relating to customer??s business and prepare reports for internal and external audiences using business analytics reporting tools\n\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\n\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\n\nf. Develop predictive models and share insights with the clients as per their requirement\n\n ? \n\nDeliver\nNoPerformance ParameterMeasure1.Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\n\n ? \n\n ? \nMandatory\n\nSkills:\nStartburst.\n\nExperience5-8 Years.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data management', 'sql', 'data bricks', 'data modeling', 'policy management', 'hive', 'snowflake', 'python', 'data mining', 'data warehousing', 'power bi', 'dbms', 'data architecture', 'sql server', 'plsql', 'tableau', 'unix shell scripting', 'spark', 'hadoop', 'etl', 'ssis', 'data integration', 'informatica']",2025-06-12 13:41:16
Data Engineer,"NTT DATA, Inc.",4 - 9 years,Not Disclosed,['Pune'],"Req ID: 324653\n\nWe are currently seeking a Data Engineer to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nKey Responsibilities:\n\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\n\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\n\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\n\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\n\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\n\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\n\nAdhere to Agile practices throughout the solution development process.\n\nDesign, build, and deploy databases and data stores to support organizational requirements.\n\nBasic Qualifications:\n\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\n\nExperience with Informatica, Python, Databricks, Azure Data Engineer\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\n\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\n\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\n\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\n\nUndergraduate or Graduate degree preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'nosql', 'elastic search', 'cassandra', 'solr', 'core data', 'cloudera', 'python', 'data analytics', 'scala', 'kudu', 'microsoft azure', 'data engineering', 'data bricks', 'gen', 'java', 'apache nifi', 'spark', 'gcp', 'kafka', 'software engineering', 'hadoop', 'aws', 'data integration']",2025-06-12 13:41:18
Data Engineer,"NTT DATA, Inc.",4 - 9 years,Not Disclosed,['Pune'],"Req ID: 324609\n\nWe are currently seeking a Data Engineer to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nKey Responsibilities:\n\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\n\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\n\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\n\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\n\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\n\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\n\nAdhere to Agile practices throughout the solution development process.\n\nDesign, build, and deploy databases and data stores to support organizational requirements.\n\nBasic Qualifications:\n\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\n\nExperience with Informatica, Python, Databricks, Azure Data Engineer\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\n\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\n\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\n\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\n\nUndergraduate or Graduate degree preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'nosql', 'elastic search', 'cassandra', 'solr', 'core data', 'cloudera', 'python', 'data analytics', 'scala', 'kudu', 'microsoft azure', 'data engineering', 'data bricks', 'gen', 'java', 'apache nifi', 'spark', 'gcp', 'kafka', 'software engineering', 'hadoop', 'aws', 'data integration']",2025-06-12 13:41:21
Data Engineer - Scala,Capco,5 - 8 years,Not Disclosed,['Bengaluru'],"The Senior Data Engineer will be responsible for designing, developing, and maintaining scalable data pipelines and building out new API integrations to support continuing increases in data volume and complexity. They will collaborate with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nResponsibilities:\nDesign, construct, install, test and maintain highly scalable data management systems & Data Pipeline.\nEnsure systems meet business requirements and industry practices.\nBuild high-performance algorithms, prototypes, predictive models, and proof of concepts.\nResearch opportunities for data acquisition and new uses for existing data.\nDevelop data set processes for data modeling, mining and production.\nIntegrate new data management technologies and software engineering tools into existing structures.\nCreate custom software components and analytics applications.\nInstall and update disaster recovery procedures.\nCollaborate with data architects, modelers, and IT team members on project goals.\nProvide senior level technical consulting to peer data engineers during data application design and development for highly complex and critical data projects.\nQualifications:\nBachelors degree in computer science, Engineering, or related field, or equivalent work experience.\nProven 5-8 years of experience as a Senior Data Engineer or similar role.\nExperience with big data tools: Hadoop, Spark, Kafka, Ansible, chef, Terraform, Airflow, and Protobuf RPC etc.\nExpert level SQL skills for data manipulation (DML) and validation (DB2).\nExperience with data pipeline and workflow management tools.\nExperience with object-oriented/object function scripting languages: Python, Java, Go lang etc.\nStrong problem solving and analytical skills.\nExcellent verbal communication skills.\nGood interpersonal skills.\nAbility to provide technical leadership for the team.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data management', 'Db2', 'Data modeling', 'Consulting', 'Disaster recovery', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-12 13:41:23
Data Engineer (C2H),First Mile Consulting,4 - 8 years,Not Disclosed,"['Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Very strong in python, pyspark and SQL. Good experience in any cloud . They use AWS but any cloud experience is ok. They will train on other things but if candidates have experience with ETL (like AWS Airflow), datalakes like Snowflake",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Pyspark', 'Cloud', 'Data engineer', 'Python', 'Sql', 'Airflow']",2025-06-12 13:41:26
Data Engineer,Lenskart,1 - 4 years,Not Disclosed,['Bengaluru'],"Key Responsibilities\nBuild and maintain scalable ETL/ELT data pipelines using Python and cloud-native tools.\nDesign and optimize data models and queries on Google BigQuery for analytical workloads.\nDevelop, schedule, and monitor workflows using orchestration tools like Apache Airflow or Cloud Composer.\nIngest and integrate data from multiple structured and semi-structured sources, including MySQL, MongoDB, APIs, and cloud storage.",,,,"['GCP', 'Bigquery', 'MySQL', 'MongoDB', 'Python']",2025-06-12 13:41:28
Data Engineer,Xenonstack,2 - 5 years,Not Disclosed,['Mohali( Phase 8B Mohali )'],"At XenonStack, We committed to become the Most Value Driven Cloud Native, Platform Engineering and Decision Driven Analytics Company. Our Consulting Services and Solutions towards the Neural Company and its Key Drivers.\nXenonStacks DataOps team is looking for a Data Engineer who will be responsible for employing techniques to create and sustain structures that allow for the analysis of data while remaining familiar with dominant programming and deployment strategies in the field.\nYou should demonstrate flexibility, creativity, and the capacity to receive and utilize constructive criticism. The ideal candidate should be highly skilled in all aspects of Python, Java/Scala, SQL and analytical skills.\nJob Responsibilities:\nDevelop, construct, test and maintain Data Platform Architectures\nAlign Data Architecture with business requirements\nLiaising with co-workers and clients to elucidate the requirements for each task.\nScalable and High Performant Data Platform Infrastructure that allows big data to be accessed and analysed quickly by BI & AI Teams.\nReformulating existing frameworks to optimize their functioning.\nTransforming Raw Data into InSights for manipulation by Data Scientists.\nEnsuring that your work remains backed up and readily accessible to relevant co-workers.\nRemaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.\nRequirements:\nTechnical Requirements\nExperience of Python, Java/Scala\nGreat Statistical / SQL based Analytical Skills\nExperience of Data Analytics Architectural Design Patterns for Batch, Event Driven and Real-Time Analytics Use Cases\nUnderstanding of Data warehousing, ETL tools, machine learning, Data EPIs\nExcellent in Algorithms and Data Systems\nUnderstanding of Distributed System for Data Processing and Analytics\nFamiliarity with Popular Data Analytics Framework like Hadoop , Spark , Delta Lake , Time Series / Analytical Stores Stores.\nProfessional Attributes:\nExcellent communication skills & Attention to detail.\nAnalytical mind and problem-solving Aptitude with Strong Organizational skills & Visual Thinking.\nBenefits:\nDiscover the benefits of joining our team:\nDynamic and purposeful work culture in a people-oriented organization contributing to multi-million-dollar projects with guaranteed job security.\nOpen, authentic, and transparent communication fostering a warm work environment.\nRegular constructive feedback and exposure to diverse technologies.\nRecognition and rewards for exceptional performance achievements.\nAccess to certification courses & Skill Sessions to develop continually and refine your skills.\nAdditional allowances for team members assigned to specific projects.\nSpecial skill allowances to acknowledge and compensate for unique expertise.\nComprehensive medical insurance policy for your health and well-being.\nTo Learn more about the company -\nWebsite - http://www.xenonstack.com/",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hadoop', 'Spark', 'ETL', 'Python', 'SQL', 'Java', 'Data Processing', 'Machine Learning']",2025-06-12 13:41:30
Data Engineer - Databricks,KPI Partners,2 - 5 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","About KPI Partners.\nKPI Partners is a leading provider of data analytics solutions, dedicated to helping organizations transform data into actionable insights. Our innovative approach combines advanced technology with expert consulting, allowing businesses to leverage their data for improved performance and decision-making.\n\nJob Description.\nWe are seeking a skilled and motivated Data Engineer with experience in Databricks to join our dynamic team. The ideal candidate will be responsible for designing, building, and maintaining scalable data pipelines and data processing solutions that support our analytics initiatives. You will collaborate closely with data scientists, analysts, and other engineers to ensure the consistent flow of high-quality data across our platforms.",,,,"['python', 'data analytics', 'analytical', 'scala', 'pyspark', 'microsoft azure', 'data warehousing', 'data pipeline', 'data architecture', 'data engineering', 'sql', 'data bricks', 'cloud', 'analytics', 'data quality', 'data modeling', 'gcp', 'teamwork', 'integration', 'aws', 'etl', 'programming', 'communication skills', 'etl scripts']",2025-06-12 13:41:33
Data Engineer,Luxoft,5 - 8 years,Not Disclosed,['Pune'],"Help Group Enterprise Architecture team to develop our suite of EA tools and workbenches\nWork in the development team to support the development of portfolio health insights\nBuild data applications from cloud infrastructure to visualization layer\nProduce clear and commented code\nProduce clear and comprehensive documentation\nPlay an active role with technology support teams and ensure deliverables are completed or escalated on time\nProvide support on any related presentations, communications, and trainings\nBe a team player, working across the organization with skills to indirectly manage and influence\nBe a self-starter willing to inform and educate others\nSkills\nMust have\nB.Sc./M.Sc. degree in computing or similar\n5-8+ years experience as a Data Engineer, ideally in a large corporate environment\nIn-depth knowledge of SQL and data modelling/data processing\nStrong experience working with Microsoft Azure\nExperience with visualisation tools like PowerBI (or Tableau, QlikView or similar)\nExperience working with Git, JIRA, GitLab\nStrong flair for data analytics\nStrong flair for IT architecture and IT architecture metrics\nExcellent stakeholder interaction and communication skills\nUnderstanding of performance implications when making design decisions to deliver performant and maintainable software.\nExcellent end-to-end SDLC process understanding.\nProven track record of delivering complex data apps on tight timelines\nFluent in English both written and spoken.\nPassionate about development with focus on data and cloud\nAnalytical and logical, with strong problem solving skills\nA team player, comfortable with taking the lead on complex tasks\nAn excellent communicator who is adept in, handling ambiguity and communicating with both technical and non-technical audiences\nComfortable with working in cross-functional global teams to effect change\nPassionate about learning and developing your hard and soft professional skills\nNice to have\nExperience working in the financial industry\nExperience in complex metrics design and reporting\nExperience in using artificial intelligence for data analytics\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Power BI Developer\nBI Engineering\nIndia\nBengaluru\nSenior Power BI Developer\nBI Engineering\nIndia\nChennai\nSenior Power BI Developer\nBI Engineering\nIndia\nGurugram\nPune, India\nReq. VR-114797\nBI Engineering\nBCM Industry\n02/06/2025\nReq. VR-114797\nApply for Data Engineer in Pune\n*",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['GIT', 'Enterprise architecture', 'Analytical', 'Artificial Intelligence', 'Data processing', 'Data analytics', 'QlikView', 'JIRA', 'SDLC', 'SQL']",2025-06-12 13:41:35
Data Engineer _Technology Lead,Broadridge,6 - 10 years,Not Disclosed,['Bengaluru'],"Key Responsibilities:\nAnalyzes and solve problems using technical experience, judgment and precedents\nProvides informal guidance to new team members\nExplains complex information to others in straightforward situations\n1. Data Engineering and Modelling:\nDesign & Develop Scalable Data Pipelines: Leverage AWS technologies to design, develop, and manage end-to-end data pipelines with services like .",,,,"['Star Schema', 'Snowflake', 'AWS', 'Apache Airflow']",2025-06-12 13:41:37
Data Engineer,Prohr Strategies,9 - 11 years,Not Disclosed,['Bengaluru'],"Hands-on Data Engineer with strong Databricks expertise in Git/DevOps integration, Unity Catalog governance, and performance tuning of data transformation workloads. Skilled in optimizing pipelines and ensuring secure, efficient data operations.",Industry Type: Emerging Technologies (AI/ML),Department: Data Science & Analytics,"Employment Type: Full Time, Temporary/Contractual","['Data Transformation', 'GIT', 'Azure Databricks', 'Databricks', 'Devops', 'Data Engineering', 'Governance', 'Catalog', 'Code Versioning Tools']",2025-06-12 13:41:39
Consultant - Data Engineer,Affine Analytics,5 - 8 years,Not Disclosed,['Bengaluru'],"We are looking for a highly skilled API & Pixel Tracking Integration Engineer to lead the development and deployment of server-side tracking and attribution solutions across multiple platforms. The ideal candidate brings deep expertise in CAPI integrations (Meta, Google, and other platforms), secure data handling using cryptographic techniques, and experience working within privacy-first environments like Azure Clean Rooms.\nThis role requires strong hands-on experience in Azure cloud services, OCI (Oracle Cloud Infrastructure), and marketing technology stacks including Adobe Tag Management and Pixel Management. You will work closely with engineering, analytics, and marketing teams to deliver scalable, compliant, and secure data tracking solutions that drive business insights and performance.",,,,"['Python', 'Azure Cloud technologies', 'Azure Data Factory', 'Adobe Tag Management', 'Azure security', 'ADF', 'ADLS', 'Azure Functions', 'Key Vault']",2025-06-12 13:41:41
Big Data Engineer - Hadoop,Info Origin Technologies Pvt Ltd,3 - 7 years,Not Disclosed,"['Hyderabad', 'Gurugram']","Role: Hadoop Data Engineer\nLocation: Gurgaon / Hyderabad\nWork Mode: Hybrid\nEmployment Type: Full-Time\nInterview Mode: First Video then In Person\nJob Description\nJob Overview:\nWe are looking for experienced Data Engineers proficient in Hadoop, Hive, Python, SQL, and Pyspark/Spark to join our dynamic team. Candidates will be responsible for designing, developing, and maintaining scalable big data solutions.\nKey Responsibilities:\nDevelop and optimize data pipelines for large-scale data processing.\nWork with structured and unstructured datasets to derive actionable insights.\nCollaborate with cross-functional teams to enhance data-driven decision-making.\nEnsure the performance, scalability, and reliability of data architectures.\nImplement best practices for data security and governance.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hive', 'Hadoop', 'Pyspark', 'Big Data', 'Python', 'SQL']",2025-06-12 13:41:43
Senior Data Engineer - Azure,Blend360 India,3 - 6 years,Not Disclosed,['Hyderabad'],"Job Description\nAs a Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\nQualifications\n3+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar fiel",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-12 13:41:45
Microsoft Fabrics Data Engineer,Swits Digital,5 - 10 years,Not Disclosed,['Bengaluru'],"Job TItle: Microsoft Fabric Data Engineer\nLocation: Bangalore\nJob Type: Conract (24 Months)\nJob Description:\nWe are seeking a highly skilled and experienced Microsoft Fabric Data Engineer/Architect to design, develop, and maintain robust, scalable, and secure data solutions within the Microsoft Fabric ecosystem. This role will leverage the full suite of Microsoft Azure data services, including Azure Data Bricks, Azure Data Factory, and Azure Data Lake, to build end-to-end data pipelines, data warehouses, and data lakehouses that enable advanced analytics and business intelligence.\nRequired Skills & Qualifications:\nBachelors degree in Computer Science, Engineering, or a related field.\n5+ years of experience in data architecture and engineering, with a strong focus on Microsoft Azure data platforms.\nProven hands-on expertise with Microsoft Fabric and its components, including:\nOneLake\nData Factory (Pipelines, Dataflows Gen2)\nSynapse Analytics (Data Warehousing, SQL analytics endpoint)\nLakehouses and Warehouses\nNotebooks (PySpark)\nExtensive experience with Azure Data Bricks, including Spark development (PySpark, Scala, SQL).\nStrong proficiency in Azure Data Factory for building and orchestrating ETL/ELT pipelines.\nDeep understanding and experience with Azure Data Lake Storage Gen2.\nProficiency in SQL (T-SQL, Spark SQL), Python, and/or other relevant scripting languages.\nSolid understanding of data warehousing concepts, dimensional modeling, and data lakehouse architectures.\nExperience with data governance principles and tools (e.g., Microsoft Purview).\nFamiliarity with CI/CD practices, version control (Git), and DevOps for data pipelines.\nExcellent problem-solving, analytical, and communication skills.\nAbility to work independently and collaboratively in a fast-paced, agile environment.\nPreferred Qualifications:\nMicrosoft certifications in Azure Data Engineering (e.g., DP-203, DP-600: Microsoft Fabric Analytics Engineer Associate).\nExperience with Power BI for data visualization and reporting.\nFamiliarity with real-time analytics and streaming data processing.\nExposure to machine learning workflows and integrating ML models with data solutions",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['GIT', 'Analytical', 'microsoft azure', 'data visualization', 'microsoft', 'Business intelligence', 'Data warehousing', 'Analytics', 'Data architecture', 'Python']",2025-06-12 13:41:48
Data Engineer,Nemetschek,5 - 10 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']","Role & responsibilities\n5+ years in software development, with a focus on data-intensive applications, cloud solutions, and scalable data architectures.\nDevelopment experience in GoLang for building scalable and efficient data applications.\nExperience with Snowflake, Redshift, or similar data platforms including architecture, data modeling, performance optimization, and integrations.\nExperience designing and building data lakes and data warehouses, ensuring data integrity, scalability, and performance.\nProficient in developing and managing ETL pipelines, using modern tools and techniques to transform, load, and integrate data efficiently.\nExperience with high-volume event streams (such as Kafka, Kinesis) and near real-time data processing solutions for fast and accurate reporting.\nHands-on experience with Terraform for automating infrastructure deployment and configuration management in cloud environments.\nExperience with containerization technologies (Docker, Kubernetes) and orchestration.\nSolid grasp of database fundamentals (SQL, NoSQL, data modeling, performance tuning)\nExperience with CI/CD pipelines and automation tools for testing, deployment, and continuous improvement.\nExperience working in AWS cloud environments, specifically with big data solutions and serverless architectures\nAbility to mentor and guide junior engineers, fostering a culture of learning and innovation\nStrong communication skills to articulate technical concepts clearly to non-technical stakeholders.\nWHAT WE OFFER\nA young, dynamic, and innovation-oriented environment\nA wide variety of projects within different industries\nA very open and informal culture where knowledge sharing, and employee development are key.\nRoom for personal initiative, development, and growth\nRealistic career opportunities\nCompetitive package and fringe benefits.\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Golang', 'Snowflake', 'Javascript', 'ETL', 'AWS']",2025-06-12 13:41:50
Senior Data Engineer,Qualcomm,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Area: Information Technology Group, Information Technology Group > IT Data Engineer\n\nGeneral Summary:\n\nDeveloper will play an integral role in the PTEIT Machine Learning Data Engineering team. Design, develop and support data pipelines in a hybrid cloud environment to enable advanced analytics. Design, develop and support CI/CD of data pipelines and services. - 5+ years of experience with Python or equivalent programming using OOPS, Data Structures and Algorithms - Develop new services in AWS using server-less and container-based services. - 3+ years of hands-on experience with AWS Suite of services (EC2, IAM, S3, CDK, Glue, Athena, Lambda, RedShift, Snowflake, RDS) - 3+ years of expertise in scheduling data flows using Apache Airflow - 3+ years of strong data modelling (Functional, Logical and Physical) and data architecture experience in Data Lake and/or Data Warehouse - 3+ years of experience with SQL databases - 3+ years of experience with CI/CD and DevOps using Jenkins - 3+ years of experience with Event driven architecture specially on Change Data Capture - 3+ years of Experience in Apache Spark, SQL, Redshift (or) Big Query (or) Snowflake, Databricks - Deep understanding building the efficient data pipelines with data observability, data quality, schema drift, alerting and monitoring. - Good understanding of the Data Catalogs, Data Governance, Compliance, Security, Data sharing - Experience in building the reusable services across the data processing systems. - Should have the ability to work and contribute beyond defined responsibilities - Excellent communication and inter-personal skills with deep problem-solving skills.\n\nMinimum Qualifications:\n3+ years of IT-related work experience with a Bachelor's degree in Computer Engineering, Computer Science, Information Systems or a related field.\nOR\n5+ years of IT-related work experience without a Bachelors degree.\n\n2+ years of any combination of academic or work experience with programming (e.g., Java, Python).\n1+ year of any combination of academic or work experience with SQL or NoSQL Databases.\n1+ year of any combination of academic or work experience with Data Structures and algorithms.\n5 years of Industry experience and minimum 3 years experience in Data Engineering development with highly reputed organizations- Proficiency in Python and AWS- Excellent problem-solving skills- Deep understanding of data structures and algorithms- Proven experience in building cloud native software preferably with AWS suit of services- Proven experience in design and develop data models using RDBMS (Oracle, MySQL, etc.)\n\nDesirable - Exposure or experience in other cloud platforms (Azure and GCP) - Experience working on internals of large-scale distributed systems and databases such as Hadoop, Spark - Working experience on Data Lakehouse platforms (One House, Databricks Lakehouse) - Working experience on Data Lakehouse File Formats (Delta Lake, Iceberg, Hudi)\n\nBachelor's or Master's degree in Computer Science, Software Engineering, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['algorithms', 'python', 'data quality', 'data structures', 'aws', 'schema', 'continuous integration', 'glue', 'amazon redshift', 'event driven architecture', 'ci/cd', 'data engineering', 'sql', 'alerts', 'java', 'data modeling', 'spark', 'devops', 'data flow', 'nosql databases', 'sql database']",2025-06-12 13:41:52
Data Engineer with Neo4j,Luxoft,3 - 5 years,Not Disclosed,['Gurugram'],"Graph Data Modeling & Implementation.\nDesign and implement complex graph data models using Cypher and Neo4j best practices.\nLeverage APOC procedures, custom plugins, and advanced graph algorithms to solve domain-specific problems.\nOversee integration of Neo4j with other enterprise systems, microservices, and data platforms.\nDevelop and maintain APIs and services in Java, Python, or JavaScript to interact with the graph database.\nMentor junior developers and review code to maintain high-quality standards.\nEstablish guidelines for performance tuning, scalability, security, and disaster recovery in Neo4j environments.\nWork with data scientists, analysts, and business stakeholders to translate complex requirements into graph-based solutions.\nSkills\nMust have\n12+ years in software/data engineering, with at least 3-5 years hands-on experience with Neo4j.\nLead the technical strategy, architecture, and delivery of Neo4j-based solutions.\nDesign, model, and implement complex graph data structures using Cypher and Neo4j best practices.\nGuide the integration of Neo4j with other data platforms and microservices.\nCollaborate with cross-functional teams to understand business needs and translate them into graph-based models.\nMentor junior developers and ensure code quality through reviews and best practices.\nDefine and enforce performance tuning, security standards, and disaster recovery strategies for Neo4j.\nStay up-to-date with emerging technologies in the graph database and data engineering space.\nStrong proficiency in Cypher query language, graph modeling, and data visualization tools (e.g., Bloom, Neo4j Browser).\nSolid background in Java, Python, or JavaScript and experience integrating Neo4j with these languages.\nExperience with APOC procedures, Neo4j plugins, and query optimization.\nFamiliarity with cloud platforms (AWS) and containerization tools (Docker, Kubernetes).\nProven experience leading engineering teams or projects.\nExcellent problem-solving and communication skills.\nNice to have\nN/A\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Flexera Data Analyst\nData Science\nIndia\nBengaluru\nSenior Flexera Data Analyst\nData Science\nIndia\nChennai\nData Scientist\nData Science\nIndia\nBengaluru\nGurugram, India\nReq. VR-114556\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114556\nApply for Data Engineer with Neo4j in Gurugram\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'query optimization', 'neo4j', 'data science', 'Data modeling', 'Disaster recovery', 'Javascript', 'Data structures', 'data visualization', 'Python']",2025-06-12 13:41:54
Data Engineer with Neo4j,Luxoft,3 - 5 years,Not Disclosed,['Chennai'],"Graph Data Modeling & Implementation.\nDesign and implement complex graph data models using Cypher and Neo4j best practices.\nLeverage APOC procedures, custom plugins, and advanced graph algorithms to solve domain-specific problems.\nOversee integration of Neo4j with other enterprise systems, microservices, and data platforms.\nDevelop and maintain APIs and services in Java, Python, or JavaScript to interact with the graph database.\nMentor junior developers and review code to maintain high-quality standards.\nEstablish guidelines for performance tuning, scalability, security, and disaster recovery in Neo4j environments.\nWork with data scientists, analysts, and business stakeholders to translate complex requirements into graph-based solutions.\nSkills\nMust have\n12+ years in software/data engineering, with at least 3-5 years hands-on experience with Neo4j.\nLead the technical strategy, architecture, and delivery of Neo4j-based solutions.\nDesign, model, and implement complex graph data structures using Cypher and Neo4j best practices.\nGuide the integration of Neo4j with other data platforms and microservices.\nCollaborate with cross-functional teams to understand business needs and translate them into graph-based models.\nMentor junior developers and ensure code quality through reviews and best practices.\nDefine and enforce performance tuning, security standards, and disaster recovery strategies for Neo4j.\nStay up-to-date with emerging technologies in the graph database and data engineering space.\nStrong proficiency in Cypher query language, graph modeling, and data visualization tools (e.g., Bloom, Neo4j Browser).\nSolid background in Java, Python, or JavaScript and experience integrating Neo4j with these languages.\nExperience with APOC procedures, Neo4j plugins, and query optimization.\nFamiliarity with cloud platforms (AWS) and containerization tools (Docker, Kubernetes).\nProven experience leading engineering teams or projects.\nExcellent problem-solving and communication skills.\nNice to have\nN/A\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Flexera Data Analyst\nData Science\nIndia\nGurugram\nSenior Flexera Data Analyst\nData Science\nIndia\nBengaluru\nData Scientist\nData Science\nIndia\nBengaluru\nChennai, India\nReq. VR-114556\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114556\nApply for Data Engineer with Neo4j in Chennai\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'query optimization', 'neo4j', 'data science', 'Data modeling', 'Disaster recovery', 'Javascript', 'Data structures', 'data visualization', 'Python']",2025-06-12 13:41:57
Data Engineer with Neo4j,Luxoft,3 - 5 years,Not Disclosed,['Bengaluru'],"Graph Data Modeling & Implementation.\nDesign and implement complex graph data models using Cypher and Neo4j best practices.\nLeverage APOC procedures, custom plugins, and advanced graph algorithms to solve domain-specific problems.\nOversee integration of Neo4j with other enterprise systems, microservices, and data platforms.\nDevelop and maintain APIs and services in Java, Python, or JavaScript to interact with the graph database.\nMentor junior developers and review code to maintain high-quality standards.\nEstablish guidelines for performance tuning, scalability, security, and disaster recovery in Neo4j environments.\nWork with data scientists, analysts, and business stakeholders to translate complex requirements into graph-based solutions.\nSkills\nMust have\n12+ years in software/data engineering, with at least 3-5 years hands-on experience with Neo4j.\nLead the technical strategy, architecture, and delivery of Neo4j-based solutions.\nDesign, model, and implement complex graph data structures using Cypher and Neo4j best practices.\nGuide the integration of Neo4j with other data platforms and microservices.\nCollaborate with cross-functional teams to understand business needs and translate them into graph-based models.\nMentor junior developers and ensure code quality through reviews and best practices.\nDefine and enforce performance tuning, security standards, and disaster recovery strategies for Neo4j.\nStay up-to-date with emerging technologies in the graph database and data engineering space.\nStrong proficiency in Cypher query language, graph modeling, and data visualization tools (e.g., Bloom, Neo4j Browser).\nSolid background in Java, Python, or JavaScript and experience integrating Neo4j with these languages.\nExperience with APOC procedures, Neo4j plugins, and query optimization.\nFamiliarity with cloud platforms (AWS) and containerization tools (Docker, Kubernetes).\nProven experience leading engineering teams or projects.\nExcellent problem-solving and communication skills.\nNice to have\nN/A\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Flexera Data Analyst\nData Science\nIndia\nGurugram\nSenior Flexera Data Analyst\nData Science\nIndia\nChennai\nBusiness Analyst\nData Science\nPoland\nRemote Poland\nBengaluru, India\nReq. VR-114556\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114556\nApply for Data Engineer with Neo4j in Bengaluru\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'query optimization', 'neo4j', 'data science', 'Data modeling', 'Disaster recovery', 'Javascript', 'Data structures', 'data visualization', 'Python']",2025-06-12 13:42:00
Senior Data Engineer - AWS,Blend360 India,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Description\nWe are looking for an experienced Senior Data Engineer with a strong foundation in Python, SQL, and Spark , and hands-on expertise in AWS, Databricks . In this role, you will build and maintain scalable data pipelines and architecture to support analytics, data science, and business intelligence initiatives. You ll work closely with cross-functional teams to drive data reliability, quality, and performance.\nResponsibilities:\nDesign, develop, and optimize scalable data pipelines using Databricks in AWS such as Glue, S3, Lambda, EMR, Databricks notebooks, workflows and jobs.\nBuilding data lake in WS Databricks.\nBuild and maintain robust ETL/ELT workflows using Python and SQL to handle structured and semi-structured data.\nDevelop distributed data processing solutions using Apache Spark or PySpark .\nPartner with data scientists and analysts to provide high-quality, accessible, and well-structured data.\nEnsure data quality, governance, security, and compliance across pipelines and data stores.\nMonitor, troubleshoot, and improve the performance of data systems and pipelines.\nParticipate in code reviews and help establish engineering best practices.\nMentor junior data engineers and support their technical development.\n\n\nQualifications\nRequirements\nBachelors or masters degree in computer science, Engineering, or a related field.\n5+ years of hands-on experience in data",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Version control', 'GIT', 'Workflow', 'Data quality', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-12 13:42:03
Data Engineer,Amakshar Technology,4 - 7 years,Not Disclosed,"['Mumbai', 'Pune', 'Chennai', 'Bengaluru']","Job Category: IT\nJob Type: Full Time\nJob Location: Bangalore Chennai Mumbai Pune\nLocation- Mumbai, Pune, Bangalore, Chennai\nExperience- 5+\nData Engineer: Expertise in Python Language is MUST. SQL (should be able to write complex SQL Queries) is MUST Data Lake Development experience. Orchestration (Apache Airflow is preferred). Spark and Hive: Optimization of Spark/PySpark and Hive apps is MUST Trino/(AWS Athena) (Good to have) Snowflake (good to have). Data Quality (good to have). File Storage (S3 is good to have)\nKind Note: Please apply or share your resume only if it matches the above criteria",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'SQL queries', 'orchestration', 'spark', 'Data quality', 'Apache', 'AWS', 'Python']",2025-06-12 13:42:06
Consultant - Data Engineer (with Fabric),Affine Analytics,5 - 8 years,Not Disclosed,['Bengaluru'],"We are looking for a highly skilled API & Pixel Tracking Integration Engineer to lead the development and deployment of server-side tracking and attribution solutions across multiple platforms. The ideal candidate brings deep expertise in CAPI integrations (Meta, Google, and other platforms), secure data handling using cryptographic techniques, and experience working within privacy-first environments like Azure Clean Rooms.\nThis role requires strong hands-on experience in C# development, Azure cloud services, OCI (Oracle Cloud Infrastructure), and marketing technology stacks including Adobe Tag Management and Pixel Management. You will work closely with engineering, analytics, and marketing teams to deliver scalable, compliant, and secure data tracking solutions that drive business insights and performance.",,,,"['Adobe Tag Management', 'Data Engineering', 'Azure Data Factory', 'Azure security', 'ADF', 'ADLS', 'Azure Functions', 'Meta CAPI', 'Google Enhanced Conversions', 'Key Vault', 'Cosmos DB']",2025-06-12 13:42:08
Senior Data Engineer,Fractal Analytics,8 - 10 years,Not Disclosed,['Mumbai'],"Job Description:\nAs a Backend (Java) Engineer, you would be part of the team consisting of Scrum Master, Cloud Engineers, AI/ML Engineers, and UI/UX Engineers to build end-to-end Data to Decision Systems.\nMandatory:\n8+ years of demonstrable experience designing, building, and working as a Java Developer for enterprise web applications\nIdeally, this would include the following:\no Expert-level proficiency with Java\no Expert-level proficiency with SpringBoot\nFamiliarity with common databases (RDBMS such as MySQL & NoSQL such as MongoDB) and data warehousing concepts (OLAP, OLTP)\nUnderstanding of REST concepts and building/interacting with REST APIs\nDeep understanding of core backend concepts:\no Develop and design RESTful services and APIs\no Develop functional databases, applications, and servers to support websites on the back end\no Performance optimization and multithreading concepts\no Experience with deploying and maintaining high traffic infrastructure (performance testing is a plus)\nIn addition, the ideal candidate would have great problem-solving skills, and familiarity with code versioning tools such as GitHub",,,,"['Backend', 'Multithreading', 'RDBMS', 'MySQL', 'Performance testing', 'OLAP', 'Scrum', 'MongoDB', 'Apache', 'OLTP']",2025-06-12 13:42:11
Data Engineer,IT Services Company,2 - 3 years,6-7 Lacs P.A.,['Pune'],"Data Engineer\nJob Description :\nJash Data Sciences: Letting Data Speak!\nDo you love solving real-world data problems with the latest and best techniques? And having fun while solving them in a team! Then come and join our high-energy team of passionate data people. Jash Data Sciences is the right place for you.\nWe are a cutting-edge Data Sciences and Data Engineering startup based in Pune, India. We believe in continuous learning and evolving together. And we let the data speak!\nWhat will you be doing?\nYou will be discovering trends in the data sets and developing algorithms to transform\nraw data for further analytics\nCreate Data Pipelines to bring in data from various sources, with different formats,\ntransform it, and finally load it to the target database.\nImplement ETL/ ELT processes in the cloud using tools like AirFlow, Glue, Stitch, Cloud\nData Fusion, and DataFlow.\nDesign and implement Data Lake, Data Warehouse, and Data Marts in AWS, GCP, or\nAzure using Redshift, BigQuery, PostgreSQL, etc.\nCreating efficient SQL queries and understanding query execution plans for tuning\nqueries on engines like PostgreSQL.\nPerformance tuning of OLAP/ OLTP databases by creating indices, tables, and views.\nWrite Python scripts for the orchestration of data pipelines\nHave thoughtful discussions with customers to understand their data engineering\nrequirements. Break complex requirements into smaller tasks for execution.\nWhat do we need from you?\nStrong Python coding skills with basic knowledge of algorithms/data structures and\ntheir application.\nStrong understanding of Data Engineering concepts including ETL, ELT, Data Lake, Data\nWarehousing, and Data Pipelines.\nExperience designing and implementing Data Lakes, Data Warehouses, and Data Marts\nthat support terabytes of scale data.\nA track record of implementing Data Pipelines on public cloud environments\n(AWS/GCP/Azure) is highly desirable\nA clear understanding of Database concepts like indexing, query performance\noptimization, views, and various types of schemas.\nHands-on SQL programming experience with knowledge of windowing functions,\nsubqueries, and various types of joins.\nExperience working with Big Data technologies like PySpark/ Hadoop\nA good team player with the ability to communicate with clarity\nShow us your git repo/ blog!\nQualification\n1-2 years of experience working on Data Engineering projects for Data Engineer I\n2-5 years of experience working on Data Engineering projects for Data Engineer II\n1-5 years of Hands-on Python programming experience\nBachelors/Masters' degree in Computer Science is good to have\nCourses or Certifications in the area of Data Engineering will be given a higher preference.\nCandidates who have demonstrated a drive for learning and keeping up to date with technology by continuing to do various courses/self-learning will be given high preference.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Airflow', 'Elt', 'Data Mart', 'Data Pipeline', 'ETL', 'Pyspark', 'Hadoop', 'Data Bricks', 'SQL', 'Data Fusion', 'Glue', 'GCP', 'Data Flow', 'Data Warehousing', 'Azzure', 'AWS']",2025-06-12 13:42:13
Cloud Data Engineer,PwC India,3 - 8 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Job Description:\n\nWe are seeking skilled and dynamic Cloud Data Engineers specializing in AWS, Azure, Databricks. The ideal candidate will have a strong background in data engineering, with a focus on data ingestion, transformation, and warehousing. They should also possess excellent knowledge of PySpark or Spark, and a proven ability to optimize performance in Spark job executions.\n\nKey Responsibilities:",,,,"['AWS OR Azure', 'Azure Data Engineer OR AWS Data Engineer', 'Azure', 'AWS']",2025-06-12 13:42:15
Data Engineer,XL India Business Services Pvt. Ltd,1 - 7 years,Not Disclosed,['Gurugram'],"Senior Engineer, Data Modeling Gurgaon/Bangalore, India AXA XL recognizes data and information as critical business assets, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained industrious advantage\n\nOur Chief Data Office also known as our Innovation, Data Intelligence & Analytics team (IDA) is focused on driving innovation through optimizing how we leverage data to drive strategy and create a new business model - disrupting the insurance market\n\nAs we develop an enterprise-wide data and digital strategy that moves us toward greater focus on the use of data and data-driven insights, we are seeking a Data Engineer\n\nThe role will support the team s efforts towards creating, enhancing, and stabilizing the Enterprise data lake through the development of the data pipelines\n\nThis role requires a person who is a team player and can work well with team members from other disciplines to deliver data in an efficient and strategic manner\n\nWhat you ll be doing What will your essential responsibilities include? Act as a data engineering expert and partner to Global Technology and data consumers in controlling complexity and cost of the data platform, whilst enabling performance, governance, and maintainability of the estate\n\nUnderstand current and future data consumption patterns, architecture (granular level), partner with Architects to make sure optimal design of data layers\n\nApply best practices in Data architecture\n\nFor example, balance between materialization and virtualization, optimal level of de-normalization, caching and partitioning strategies, choice of storage and querying technology, performance tuning\n\nLeading and hands-on execution of research into new technologies\n\nFormulating frameworks for assessment of new technology vs business benefit, implications for data consumers\n\nAct as a best practice expert, blueprint creator of ways of working such as testing, logging, CI/CD, observability, release, enabling rapid growth in data inventory and utilization of Data Science Platform\n\nDesign prototypes and work in a fast-paced iterative solution delivery model\n\nDesign, Develop and maintain ETL pipelines using Py spark in Azure Databricks using delta tables\n\nUse Harness for deployment pipeline\n\nMonitor Performance of ETL Jobs, resolve any issue that arose and improve the performance metrics as needed\n\nDiagnose system performance issue related to data processing and implement solution to address them\n\nCollaborate with other teams to make sure successful integration of data pipelines into larger system architecture requirement\n\nMaintain integrity and quality across all pipelines and environments\n\nUnderstand and follow secure coding practice to make sure code is not vulnerable\n\nYou will report to the Application Manager\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Effective Communication skills\n\nBachelor s degree in computer science, Mathematics, Statistics, Finance, related technical field, or equivalent work experience\n\nRelevant years of extensive work experience in various data engineering & modeling techniques (relational, data warehouse, semi-structured, etc), application development, advanced data querying skills\n\nRelevant years of programming experience using Databricks\n\nRelevant years of experience using Microsoft Azure suite of products (ADF, synapse and ADLS)\n\nSolid knowledge on network and firewall concepts\n\nSolid experience writing, optimizing and analyzing SQL\n\nRelevant years of experience with Python\n\nAbility to break complex data requirements and architect solutions into achievable targets\n\nRobust familiarity with Software Development Life Cycle (SDLC) processes and workflow, especially Agile\n\nExperience using Harness\n\nTechnical lead responsible for both individual and team deliveries\n\nDesired Skills and Abilities: Worked in big data migration projects\n\nWorked on performance tuning both at database and big data platforms\n\nAbility to interpret complex data requirements and architect solutions\n\nDistinctive problem-solving and analytical skills combined with robust business acumen\n\nExcellent basics on parquet files and delta files\n\nEffective Knowledge of Azure cloud computing platform\n\nFamiliarity with Reporting software - Power BI is a plus\n\nFamiliarity with DBT is a plus\n\nPassion for data and experience working within a data-driven organization\n\nYou care about what you do, and what we do\n\nWho WE are AXA XL, the P&C and specialty risk division of AXA, is known for solving complex risks\n\nFor mid-sized companies, multinationals and even some inspirational individuals we don t just provide re/insurance, we reinvent it\n\nHow? By combining a comprehensive and efficient capital platform, data-driven insights, leading technology, and the best talent in an agile and inclusive workspace, empowered to deliver top client service across all our lines of business property, casualty, professional, financial lines and specialty\n\nWith an innovative and flexible approach to risk solutions, we partner with those who move the world forward\n\nLearn more at axaxl\n\ncom What we OFFER Inclusion AXA XL is committed to equal employment opportunity and will consider applicants regardless of gender, sexual orientation, age, ethnicity and origins, marital status, religion, disability, or any other protected characteristic\n\nAt AXA XL, we know that an inclusive culture and a diverse workforce enable business growth and are critical to our success\n\nThat s why we have made a strategic commitment to attract, develop, advance and retain the most diverse workforce possible, and create an inclusive culture where everyone can bring their full selves to work and can reach their highest potential\n\nIt s about helping one another and our business to move forward and succeed\n\nFive Business Resource Groups focused on gender, LGBTQ+, ethnicity and origins, disability and inclusion with 20 Chapters around the globe Robust support for Flexible Working Arrangements Enhanced family friendly leave benefits Named to the Diversity Best Practices Index Signatory to the UK Women in Finance Charter Learn more at axaxl\n\ncom / about-us / inclusion-and-diversity\n\nAXA XL is an Equal Opportunity Employer\n\nTotal Rewards AXA XL s Reward program is designed to take care of what matters most to you, covering the full picture of your health, wellbeing, lifestyle and financial security\n\nIt provides dynamic compensation and personalized, inclusive benefits that evolve as you do\n\nWe re committed to rewarding your contribution for the long term, so you can be your best self today and look forward to the future with confidence\n\nSustainability At AXA XL, Sustainability is integral to our business strategy\n\nIn an ever-changing world, AXA XL protects what matters most for our clients and communities\n\nWe know that sustainability is at the root of a more resilient future\n\nOur 2023-26 Sustainability strategy, called Roots of resilience , focuses on protecting natural ecosystems, addressing climate change, and embedding sustainable practices across our operations\n\nOur Pillars: Valuing nature: How we impact nature affects how nature impacts us\n\nResilient ecosystems - the foundation of a sustainable planet and society - are essential to our future\n\nWe re committed to protecting and restoring nature - from mangrove forests to the bees in our backyard - by increasing biodiversity awareness and inspiring clients and colleagues to put nature at the heart of their plans\n\nAddressing climate change: The effects of a changing climate are far reaching and significant\n\nUnpredictable weather, increasing temperatures, and rising sea levels cause both social inequalities and environmental disruption\n\nWere building a net zero strategy, developing insurance products and services, and mobilizing to advance thought leadership and investment in societal-led solutions\n\nIntegrating ESG: All companies have a role to play in building a more resilient future\n\nIncorporating ESG considerations into our internal processes and practices builds resilience from the roots of our business\n\nWe re training our colleagues, engaging our external partners, and evolving our sustainability governance and reporting\n\nAXA Hearts in Action: We have established volunteering and charitable giving programs to help colleagues support causes that matter most to them, known as AXA XL s Hearts in Action programs\n\nThese include our Matching Gifts program, Volunteering Leave, and our annual volunteering day - the Global Day of Giving\n\nFor more information, please see axaxl\n\ncom/sustainability",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Performance tuning', 'Data modeling', 'Coding', 'Agile', 'Workflow', 'Application development', 'SDLC', 'SQL', 'Python', 'Firewall']",2025-06-12 13:42:18
Data Engineer,Infoobjects Inc.,3 - 6 years,Not Disclosed,['Jaipur'],"Role & responsibilities:\nDesign, develop, and maintain robust ETL/ELT pipelines to ingest and process data from multiple sources.\nBuild and maintain scalable and reliable data warehouses, data lakes, and data marts.\nCollaborate with data scientists, analysts, and business stakeholders to understand data needs and deliver solutions.\nEnsure data quality, integrity, and security across all data systems.\nOptimize data pipeline performance and troubleshoot issues in a timely manner.\nImplement data governance and best practices in data management.\nAutomate data validation, monitoring, and reporting processes.\n\n\n\nPreferred candidate profile:\nBachelor's or Masters degree in Computer Science, Engineering, Information Systems, or related field.\nProven experience (X+ years) as a Data Engineer or similar role.\nStrong programming skills in Python, Java, or Scala.\nProficiency with SQL and working knowledge of relational databases (e.g., PostgreSQL, MySQL).\nHands-on experience with big data technologies (e.g., Spark, Hadoop).\nFamiliarity with cloud platforms such as AWS, GCP, or Azure (e.g., S3, Redshift, BigQuery, Data Factory).\nExperience with orchestration tools like Airflow or Prefect.\nKnowledge of data modeling, warehousing, and architecture design principles.\nStrong problem-solving skills and attention to detail.\n\nPerks and benefits\nFree Meals\nPF and Gratuity\nMedical and Term Insurance",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SCALA', 'Kafka', 'AWS', 'Python', 'Pyspark', 'Java', 'Postgresql', 'Hadoop', 'Spark', 'ETL', 'SQL']",2025-06-12 13:42:20
Data Engineer II,Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Amazon strives to be the worlds most customer-centric company, where customers can research and purchase anything they might want online\nWe set big goals and are looking for people who can help us reach and exceed them\nThe CPT Data Engineering & Analytics (DEA) team builds and maintains critical data infrastructure that enhances seller experience and protects the privacy of Amazon business partners throughout their lifecycle\nWe are looking for a strong Data Engineer to join our team\n\nThe Data Engineer I will work with well-defined requirements to develop and maintain data pipelines that help internal teams gather required insights for business decisions timely and accurately\nYou will collaborate with a team of Data Scientists, Business Analysts and other Engineers to build solutions that reduce investigation defects and assess the health of our Operations business while ensuring data quality and regulatory compliance\n\nThe ideal candidate must be passionate about building reliable data infrastructure, detail-oriented, and driven to help protect Amazons customers and business partners\nThey will be an individual contributor who works effectively with guidance from senior team members to successfully implement data solutions\nThe candidate must be proficient in SQL and at least one scripting language (e\ng\nPython, Perl, Scala), with strong understanding of data management fundamentals and distributed systems concepts\n\n\nBuild and optimize physical data models and data pipelines for simple datasets\nWrite secure, stable, testable, maintainable code with minimal defects\nTroubleshoot existing datasets and maintain data quality\nParticipate in team design, scoping, and prioritization discussions\nDocument solutions to ensure ease of use and maintainability\nHandle data in accordance with Amazon policies and security requirements Masters degree in computer science, engineering, analytics, mathematics, statistics, IT or equivalent\n3+ years of data engineering experience\nExperience with SQL\nExperience with data modeling, warehousing and building ETL pipelines\nKnowledge of distributed systems concepts from data storage and compute perspective\nAbility to work effectively in a team environment Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions\nFamiliarity with big data technologies (Hadoop, Spark, etc\n)\nKnowledge of data security and privacy best practices\nStrong problem-solving and analytical skills\nExcellent written and verbal communication skills",Industry Type: Internet,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data management', 'Data modeling', 'data security', 'Perl', 'Data quality', 'Distribution system', 'Analytics', 'SQL', 'Python']",2025-06-12 13:42:23
Data Engineer,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"Role Description:\nAs part of the cybersecurity organization, In this vital role you will be responsible for designing, building, and maintaining data infrastructure to support data-driven decision-making. This role involves working with large datasets, developing reports, executing data governance initiatives, and ensuring data is accessible, reliable, and efficiently managed. The role sits at the intersection of data infrastructure and business insight delivery, requiring the Data Engineer to design and build robust data pipelines while also translating data into meaningful visualizations for stakeholders across the organization. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture, ETL processes, and cybersecurity data frameworks.\nRoles & Responsibilities:\nDesign, develop, and maintain data solutions for data generation, collection, and processing.\nBe a key team member that assists in design and development of the data pipeline.\nBuild data pipelines and ensure data quality by implementing ETL processes to migrate and deploy data across systems.\nDevelop and maintain interactive dashboards and reports using tools like Tableau, ensuring data accuracy and usability\nSchedule and manage workflows the ensure pipelines run on schedule and are monitored for failures.\nCollaborate with multi-functional teams to understand data requirements and design solutions that meet business needs.\nDevelop and maintain data models, data dictionaries, and other documentation to ensure data accuracy and consistency.\nImplement data security and privacy measures to protect sensitive data.\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nCollaborate and communicate effectively with product teams.\nCollaborate with data scientists to develop pipelines that meet dynamic business needs.\nShare and discuss findings with team members practicing SAFe Agile delivery model.\n\n\nBasic Qualifications:\nMasters degree and 1 to 3 years of experience of Computer Science, IT or related field experience OR\nBachelors degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience\nPreferred Qualifications:\nHands on experience with data practices, technologies, and platforms, such as Databricks, Python, GitLab, LucidChart, etc.\nHands-on experience with data visualization and dashboarding toolsTableau, Power BI, or similar is a plus\nProficiency in data analysis tools (e.g. SQL) and experience with data sourcing tools\nExcellent problem-solving skills and the ability to work with large, complex datasets\nUnderstanding of data governance frameworks, tools, and best practices\nKnowledge of and experience with data standards (FAIR) and protection regulations and compliance requirements (e.g., GDPR, CCPA)\n\nGood-to-Have Skills:\nExperience with ETL tools and various Python packages related to data processing, machine learning model development\nStrong understanding of data modeling, data warehousing, and data integration concepts\nKnowledge of Python/R, Databricks, cloud data platforms\nExperience working in Product team's environment\nExperience working in an Agile environment\n\nProfessional Certifications:\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\n\nSoft Skills:\nInitiative to explore alternate technology and approaches to solving problems\nSkilled in breaking down problems, documenting problem statements, and estimating efforts\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to handle multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data engineering', 'data analysis', 'data modeling', 'analysis tools', 'data warehousing', 'troubleshooting', 'data architecture', 'data integration', 'etl process']",2025-06-12 13:42:25
Data Engineer,Luxoft,5 - 10 years,Not Disclosed,['Pune'],"Are you passionate about data and analytics? Are you keen to be part of the journey to modernize a data warehouse/ analytics suite of application(s). Do you take pride in the quality of software delivered for each development iteration?\nWere looking for someone like that to join us and\nbe a part of a high-performing team on a high-profile project.\nsolve challenging problems in an elegant way\nmaster state-of-the-art technologies\nbuild a highly responsive and fast updating application in an Agile & Lean environment\napply best development practices and effectively utilize technologies\nwork across the full delivery cycle to ensure high-quality delivery\nwrite high-quality code and adhere to coding standards\nwork collaboratively with diverse team(s) of technologists\nYou are:\nCurious and collaborative, comfortable working independently, as well as in a team\nFocused on delivery to the business\nStrong in analytical skills. For example, the candidate must understand the key dependencies among existing systems in terms of the flow of data among them. It is essential that the candidate learns to understand the big picture of how IB industry/business functions.\nAble to quickly absorb new terminology and business requirements\nAlready strong in analytical tools, technologies, platforms, etc. The candidate must also demonstrate a strong desire for learning and self-improvement.\nOpen to learning home-grown technologies, support current state infrastructure and help drive future state migrations. imaginative and creative with newer technologies\nAble to accurately and pragmatically estimate the development effort required for specific objectives\nYou will have the opportunity to work under minimal supervision to understand local and global system requirements, design and implement the required functionality/bug fixes/enhancements. You will be responsible for components that are developed across the whole team and deployed globally.\nYou will also have the opportunity to provide third-line support to the applications global user community, which will include assisting dedicated support staff and liaising with the members of other development teams directly, some of which will be local and some remote.\nSkills\nMust have\nA bachelors or masters degree, preferably in Information Technology or a related field (computer science, mathematics, etc.), focusing on data engineering.\n5+ years of relevant experience as a data engineer in Big Data is required.\nStrong Knowledge of programming languages (Python / Scala) and Big Data technologies (Spark, Databricks or equivalent) is required.\nStrong experience in executing complex data analysis and running complex SQL/Spark queries.\nStrong experience in building complex data transformations in SQL/Spark.\nStrong knowledge of Database technologies is required.\nStrong knowledge of Azure Cloud is advantageous.\nGood understanding and experience with Agile methodologies and delivery.\nStrong communication skills with the ability to build partnerships with stakeholders.\nStrong analytical, data management and problem-solving skills.\nNice to have\nExperience working on the QlikView tool\nUnderstanding of QlikView scripting and data model\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nBig Data Engineer (Scala/Java/Python)\nBigData Development\nUnited States of America\nStamford, US\nBig Data Engineer (Scala/Java/Python)\nBigData Development\nUnited States of America\nWeehawken\nData Engineer - PostgreSQL\nBigData Development\nPoland\nRemote Poland\nPune, India\nReq. VR-114879\nBigData Development\nBCM Industry\n05/06/2025\nReq. VR-114879\nApply for Data Engineer in Pune\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data management', 'Coding', 'Postgresql', 'Agile', 'Information technology', 'Analytics', 'SQL', 'Python']",2025-06-12 13:42:27
Data Engineer,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nAs part of the cybersecurity organization, the Data Engineer is responsible for designing, building, and maintaining data infrastructure to support data-driven decision-making. This role involves working with large datasets, developing reports, executing data governance initiatives, and ensuring data is accessible, reliable, and efficiently managed. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture, ETL processes, and cybersecurity data frameworks.\nRoles & Responsibilities:\nDesign, develop, and maintain data solutions for data generation, collection, and processing.\nBe a key team member that assists in design and development of the data pipeline.\nCreate data pipelines and ensure data quality by implementing ETL processes to migrate and deploy data across systems.\nSchedule and manage workflows the ensure pipelines run on schedule and are monitored for failures.\nCollaborate with cross-functional teams to understand data requirements and design solutions that meet business needs.\nDevelop and maintain data models, data dictionaries, and other documentation to ensure data accuracy and consistency.\nImplement data security and privacy measures to protect sensitive data.\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nCollaborate and communicate effectively with product teams.\nCollaborate with data scientists to develop pipelines that meet dynamic business needs.\nShare and discuss findings with team members practicing SAFe Agile delivery model.\nFunctional Skills:\nBasic Qualifications:\nMasters degree and 1 to 3 years of Computer Science, IT or related field experience OR\nBachelors degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience\nPreferred Qualifications:\nHands on experience with data practices, technologies, and platforms, such as Databricks, Python, Gitlab, LucidChart,etc.\nProficiency in data analysis tools (e.g. SQL) and experience with data sourcing tools\nExcellent problem-solving skills and the ability to work with large, complex datasets\nUnderstanding of data governance frameworks, tools, and best practices\nKnowledge of and experience with data standards (FAIR) and protection regulations and compliance requirements (e.g., GDPR, CCPA)\nGood-to-Have Skills:\nExperience with ETL tools and various Python packages related to data processing, machine learning model development\nStrong understanding of data modeling, data warehousing, and data integration concepts\nKnowledge of Python/R, Databricks, cloud data platforms\nExperience working in Product team's environment\nExperience working in an Agile environment\nProfessional Certifications:\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nSoft Skills:\nInitiative to explore alternate technology and approaches to solving problems\nSkilled in breaking down problems, documenting problem statements, and estimating efforts\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data engineering', 'data security', 'Agile', 'cloud data platforms', 'Databricks', 'data governance frameworks', 'ETL', 'AWS', 'SQL', 'Python']",2025-06-12 13:42:30
Data Engineer - SSIS - 5+ Years - Gurugram (Hybrid),Crescendo Global,5 - 10 years,Not Disclosed,['Gurugram'],"Data Engineer - SSIS - 5+ Years - Gurugram (Hybrid)\n\nAre you a skilled Data Engineer with expertise in SSIS and 5+ years of experience? Do you have a passion for analytics and want to work in a hybrid setup in Gurugram? Our client is seeking a talented individual to join their team and contribute to their data engineering projects.\n\nLocation : Gurugram (Hybrid)\n\nYour Future EmployerOur client is a leading organization in the analytics domain, known for fostering an inclusive and diverse work environment. They are committed to providing their employees with opportunities for growth and development.\n\nResponsibilities\nDesign, develop, and maintain data pipelines using SSIS for efficient data processing\nCollaborate with cross-functional teams to understand data requirements and provide effective data solutions\nOptimize data pipelines for performance and scalability\nEnsure data quality and integrity throughout the data engineering process\n\nRequirements\n5+ years of experience in data engineering with a strong focus on SSIS\nProficiency in data warehousing concepts and ETL processes\nHands-on experience with SQL databases and data modeling4.Strong analytical and problem-solving skills\nBachelor's degree in Computer Science, Engineering, or related field\n\nWhat's in it for you : In this role, you will have the opportunity to work on challenging projects and enhance your expertise in data engineering. The organization offers a competitive compensation package and a supportive work environment where your contributions are valued.\n\nReach us : If you feel this opportunity is well aligned with your career progression plans, please feel free to reach me with your updated profile at rohit.kumar@crescendogroup.in\n\nDisclaimer : Crescendo Global specializes in Senior to C-level niche recruitment. We are passionate about empowering job seekers and employers with an engaging memorable job search and leadership hiring experience. Crescendo Global does not discriminate on the basis of race, religion, color, origin, gender, sexual orientation, age, marital status, veteran status or disability status.\n\nNote : We receive a lot of applications on a daily basis so it becomes a bit difficult for us to get back to each candidate. Please assume that your profile has not been shortlisted in case you don't hear back from us in 1 week. Your patience is highly appreciated.\n\nScammers can misuse Crescendo Globals name for fake job offers. We never ask for money, purchases, or system upgrades. Verify all opportunities at www.crescendo-global.com and report fraud immediately. Stay alert!\n\nProfile keywords : Data Engineer, SSIS, Data Warehousing, ETL, SQL, Analytics",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'SSIS', 'SQL', 'Analytics']",2025-06-12 13:42:32
Data Engineer,Konrad Group,3 - 7 years,15-30 Lacs P.A.,['Gurugram( Sector 42 Gurgaon )'],"Who We Are\n\nKonrad is a next generation digital consultancy. We are dedicated to solving complex business problems for our global clients with creative and forward-thinking solutions. Our employees enjoy a culture built on innovation and a commitment to creating best-in-class digital products in use by hundreds of millions of consumers around the world. We hire exceptionally smart, analytical, and hard working people who are lifelong learners.\nAbout The Role\nAs a Data Engineer youll be tasked with designing, building, and maintaining scalable data platforms and pipelines. Your deep knowledge of data platforms such as Azure Fabric, Databricks, and Snowflake will be essential as you collaborate closely with data analysts, scientists, and other engineers to ensure reliable, secure, and efficient data solutions.\n\nWhat Youll Do\n\nDesign, build, and manage robust data pipelines and data architectures.\nImplement solutions leveraging platforms such as Azure Fabric, Databricks, and Snowflake.\nOptimize data workflows, ensuring reliability, scalability, and performance.\nCollaborate with internal stakeholders to understand data needs and deliver tailored solutions.\nEnsure data security and compliance with industry standards and best practices.\nPerform data modelling, data extraction, transformation, and loading (ETL/ELT).\nIdentify and recommend innovative solutions to enhance data quality and analytics capabilities.\n\nQualifications\n\nBachelors degree or higher in Computer Science, Data Engineering, Information Technology, or a related field.\nAt least 3 years of professional experience as a Data Engineer or similar role.\nProficiency in data platforms such as Azure Fabric, Databricks, and Snowflake.\nHands-on experience with data pipeline tools, cloud services, and storage solutions.\nStrong programming skills in SQL, Python, or related languages.\nExperience with big data technologies and concepts (Spark, Hadoop, Kafka).\nExcellent analytical, troubleshooting, and problem-solving skills.\nAbility to effectively communicate technical concepts clearly to non-technical stakeholders.\nAdvanced English\n\nNice to have\n\nCertifications related to Azure Data Engineering, Databricks, or Snowflake.\nFamiliarity with DevOps practices and CI/CD pipelines.\n\nPerks and Benefits\n\nComprehensive Health & Wellness Benefits Package \nSocials, Outings & Retreats\nCulture of Learning & Development\nFlexible Working Hours\nWork from Home Flexibility\nService Recognition Programs\n\nKonrad is committed to maintaining a diverse work environment and is proud to be an equal opportunity employer. All qualified applicants, regardless of race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status will receive consideration for employment. If you have any accessibility requirements or concerns regarding the hiring process or employment with us, please notify us so we can provide suitable accommodation.\nWhile we sincerely appreciate all applications, only those candidates selected for an interview will be contacted.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hadoop', 'Azure Data Factory', 'Azure Databricks', 'Spark', 'Fabric', 'Python']",2025-06-12 13:42:34
Data Engineer,Atyeti,2 - 4 years,Not Disclosed,['Pune'],"Role & responsibilities\n\nDevelop and Maintain Data Pipelines: Design, develop, and manage scalable ETL pipelines to process large datasets using PySpark, Databricks, and other big data technologies.\nData Integration and Transformation: Work with various structured and unstructured data sources to build efficient data workflows and integrate them into a central data warehouse.\nCollaborate with Data Scientists & Analysts: Work closely with the data science and business intelligence teams to ensure the right data is available for advanced analytics, machine learning, and reporting.",,,,"['Azure Synapse', 'Pyspark', 'ETL', 'Python']",2025-06-12 13:42:37
Data Engineer,zocket,4 - 5 years,Not Disclosed,['Chennai'],"About Zocket: AI Assistant for Marketers\nZocket empowers businesses to scale social media advertising seamlessly with advanced AI . From search to conversions, Zocket automates the entire workflow effortlessly.\n\nHeadquartered in Chennai and San Francisco , with 500+ customers across 25+ countries.\n\nOur GenAI Product Suite:\nCreative Studio: Make ad creatives 10x faster, 20x better\n\nAudience Assistant: Precision targeting, platform-wide\n\nSnoop AI: Competitive & industry insights in seconds\n\nInsights AI: 360 performance dashboard across channels\n\nGrowth Infra: Whitelisted infra for unbounded scale\n\nTrusted and Recognized:\nAWS GenAI Accelerator 2024 (Top 80 global startups)\n\nGoogle for Startups (GFSA Class VIII Ai-first)\n\nNASSCOM Gen AI Foundry\n\n4.9 Trustpilot | #1 Product of the Day & Week on Product Hunt\n\n\nKey Responsibilities\n\nDesign, build, and maintain robust ETL pipelines to process large-scale structured and unstructured data.\n\nWork with GraphQL databases and Vector DBs (e.g., Pinecone, Chroma) to power intelligent, AI-driven features.\n\nBuild and optimize scalable ML pipelines and contribute to model lifecycle management .\n\nImplement real-time and batch data streaming solutions using Kafka and Spark .\n\nWork with Airflow to schedule and orchestrate data workflows.\n\nDevelop efficient data models in PostgreSQL and ensure high-performance data access.\n\nIntegrate and optimize Elasticsearch for fast search and analytics.\n\nEnsure high data quality and reliability through monitoring and automation.\n\nCollaborate with Product, Data Science, and Engineering teams to deliver scalable data solutions.\n\nOwn the data infrastructure on AWS - from data lake to warehouse and analytics stack.\n\nDrive internal data tooling improvements and support analytics for business decision-making.\n\n\nKey Requirements\n\n4-5 years of experience as a Data Engineer or in a similar backend/data-intensive role.\n\nStrong programming experience with Python and familiarity with ML model development workflows .\n\nHands-on experience with GraphQL and Vector DBs (Pinecone, Chroma).\n\nProficiency in PostgreSQL and working knowledge of NoSQL and search systems like Elasticsearch .\n\nSolid understanding of data engineering on AWS (EC2, S3, RDS, Redshift, etc.).\n\nExperience with Apache Kafka , Apache Spark , and Airflow is essential.\n\nFamiliarity with modern ML Ops and data science integration is a plus.\n\nStrong analytical thinking and problem-solving skills.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'Backend', 'data science', 'Analytical', 'Social media', 'Workflow', 'Data quality', 'AWS', 'Analytics', 'Monitoring']",2025-06-12 13:42:39
Data Engineer,Trantor,5 - 10 years,Not Disclosed,[],"We are looking for a skilled and motivated Data Engineer with deep expertise in GCP,\nBigQuery, Apache Airflow to join our data platform team. The ideal candidate should have hands-on experience building scalable data pipelines, automating workflows, migrating large-scale datasets, and optimizing distributed systems. The candidate should have experience with building Web APIs using Python. This role will play a key part in designing and maintaining robust data engineering solutions across cloud and on-prem environments.\nKey Responsibilities\nBigQuery & Cloud Data Pipelines:\nDesign and implement scalable ETL pipelines for ingesting large-scale datasets.\nBuild solutions for efficient querying of tables in BigQuery.\nAutomated scheduled data ingestion using Google Cloud services and scheduled\nApache Airflow DAGs",,,,"['Airflow', 'Etl Pipelines', 'GCP', 'Bigquery', 'Python', 'SFTP', 'ETL', 'SQL']",2025-06-12 13:42:41
Data Engineer,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nDesign, develop, and maintain complex ETL/ELT data pipelines in Databricks using PySpark, Scala, and SQL to process large-scale datasets\nUnderstand the biotech/pharma or related domains & build highly efficient data pipelines to migrate and deploy complex data across systems\nDesign and Implement solutions to enable unified data access, governance, and interoperability across hybrid cloud environments\nIngest and transform structured and unstructured data from databases (PostgreSQL, MySQL, SQL Server, MongoDB etc.), APIs, logs, event streams, images, pdf, and third-party platforms\nEnsuring data integrity, accuracy, and consistency through rigorous quality checks and monitoring\nExpert in data quality, data validation and verification frameworks\nInnovate, explore and implement new tools and technologies to enhance efficient data processing\nProactively identify and implement opportunities to automate tasks and develop reusable frameworks\nWork in an Agile and Scaled Agile (SAFe) environment, collaborating with cross-functional teams, product owners, and Scrum Masters to deliver incremental value\nUse JIRA, Confluence, and Agile DevOps tools to manage sprints, backlogs, and user stories\nSupport continuous improvement, test automation, and DevOps practices in the data engineering lifecycle\nCollaborate and communicate effectively with the product teams, with cross-functional teams to understand business requirements and translate them into technical solutions\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. We are looking for highly motivated expert Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks.\nBasic Qualifications:\nMasters degree and 1 to 3 years of Computer Science, IT or related field experience OR\nBachelors degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies\nProficiency in workflow orchestration, performance tuning on big data processing\nStrong understanding of AWS services\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices\nPreferred Qualifications:\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nData Engineering experience in Biotechnology or pharma industry\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and DevOps\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data engineering', 'Maven', 'data validation', 'PySpark', 'Scala', 'APIs', 'SQL Server', 'SQL', 'Jenkins', 'Git', 'MySQL', 'troubleshooting', 'MongoDB', 'ETL']",2025-06-12 13:42:44
Data Engineer,Cloud Angles Digital Transformation,3 - 5 years,Not Disclosed,['Noida'],"Essential Functions/Responsibilities/Duties\n•       Work closely with Senior Business Intelligence engineer and BI architect to understand the schema objects and build BI reports and Dashboards\n•       Participation in sprint refinement, planning, and kick-off to understand the Agile process and Sprint priorities\n•       Develop necessary transformations and aggregate tables required for the reporting\\Dashboard needs\n•       Understand the Schema layer in MicroStrategy and business requirements\n•       Develop complex reports and Dashboards in MicroStrategy\n•       Investigate and troubleshoot issues with Dashboard and reports\n•       Proactively researching new technologies and proposing improvements to processes and tech stack\n•       Create test cases and scenarios to validate the dashboards and maintain data accuracy\nEducation and Experience\n•       3 years of experience in Business Intelligence and Data warehousing\n•       3+ years of experience in MicroStrategy Reports and Dashboard development\n•       2 years of experience in SQL\n•       Bachelors or masters degree in IT or Computer Science or ECE.\n•       Nice to have – Any MicroStrategy certifications\nRequired Knowledge, Skills, and Abilities\n•       Good in writing complex SQL, including aggregate functions, subqueries and complex date calculations and able to teach these concepts to others.\n•       Detail oriented and able to examine data and code for quality and accuracy.\n•       Self-Starter – taking initiative when inefficiencies or opportunities are seen.\n•       Good understanding of modern relational and non-relational models and differences between them\n•       Good understanding of Datawarehouse concepts, snowflake & star schema architecture and SCD concepts\n•       Good understanding of MicroStrategy Schema objects\n•       Develop Public objects such as metrics, filters, prompts, derived objects, custom groups and consolidations in MicroStrategy\n•       Develop complex reports and dashboards using OLAP and MTDI cubes\n•       Create complex dashboards with data blending\n•       Understand VLDB settings and report optimization\n•       Understand security filters and connection mappings in MSTR\nWork Environment\nAt Personify Health, we value and celebrate diversity and are committed to creating an inclusive environment for all employees. We believe in creating teams made up of individuals with various backgrounds, experiences, and perspectives. Diversity inspires innovation and collaboration and challenges us to produce better solutions. But more than this, diversity is our strength and a catalyst in our ability to change lives for the good. \nPhysical Requirements\n•       Constantly operates a computer and other office productivity machinery, such as copy machine, computer printer, calculator, etc.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Microstrategy', 'SQL', 'Dashboards']",2025-06-12 13:42:46
Data Engineer,Centrilogic,15 - 20 years,Not Disclosed,['Hyderabad'],"Data Engineer\n\nPurpose:\n\nOver 15 years, we have become a premier global provider of multi-cloud management, cloud-native application development solutions, and strategic end-to-end digital transformation services.\nHeadquartered in Canada and with regional headquarters in the U.S. and the United Kingdom, Centrilogic delivers smart, streamlined solutions to clients worldwide.\n\nWe are looking for a passionate and experienced Data Engineer to work with our other 70 Software, Data and DevOps engineers to guide and assist our clients data modernization journey.\n\nOur team works with companies with ambitious missions - clients who are creating new, innovative products, often in uncharted markets. We work as embedded members and leaders of our clients development and data teams. We bring experienced senior engineers, leading-edge technologies and mindsets, and creative thinking. We show our clients how to move to the modern frameworks of data infrastructures and processing, and we help them reach their full potential with the power of data.\n\nIn this role, youll be the day-to-day primary point of contact with our clients to modernize their data infrastructures, architecture, and pipelines.\n\nPrincipal Responsibilities:\n\nConsulting clients on cloud-first strategies for core bet-the-company data initiatives\nProviding thought leadership on both process and technical matters\nBecoming a real champion and trusted advisor to our clients on all facets of Data Engineering\nDesigning, developing, deploying, and supporting the modernization and transformation of our client s end-to-end data strategy, including infrastructure, collection, transmission, processing, and analytics\nMentoring and educating clients teams to keep them up to speed with the latest approaches, tools and skills, and setting them up for continued success post-delivery\n\nRequired Experience and Skills:\n\nMust have either Microsoft Certified Azure Data Engineer Associate or Fabric Data Engineer Associate certification.\nMust have experience working in a consulting or contracting capacity on large data management and modernization programs.\nExperience with SQL Servers, data engineering, on platforms such as Azure Data Factory, Databricks, Data Lake, and Synapse.\nStrong knowledge and demonstrated experience with Delta Lake and Lakehouse Architecture.\nStrong knowledge of securing Azure environment, such as RBAC, Key Vault, and Azure Security Center.\nStrong knowledge of Kafka and Spark and extensive experience using them in a production environment.\nStrong and demonstrable experience as DBA in large-scale MS SQL environments deployed in Azure.\nStrong problem-solving skills, with the ability to get to the route of an issue quickly.\nStrong knowledge of Scala or Python.\nStrong knowledge of Linux administration and networking.\nScripting skills and Infrastructure as Code (IaC) experience using PowerShell, Bash, and ARM templates.\nUnderstanding of security and corporate governance issues related with cloud-first data architecture, as well as accepted industry solutions.\nExperience in enabling continuous delivery for development teams using scripted cloud provisioning and automated tooling.\nExperience working with Agile development methodology that is fit for purpose.\nSound business judgment and demonstrated leadership",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['MS SQL', 'Networking', 'Data management', 'Powershell', 'Consulting', 'Application development', 'microsoft', 'Analytics', 'Python', 'Data architecture']",2025-06-12 13:42:48
Data Engineer,Databeat,3 - 7 years,Not Disclosed,['Hyderabad( Rai Durg )'],"Experience Required: 3+ years\n\nTechnical knowledge: AWS, Python, SQL, S3, EC2, Glue, Athena, Lambda, DynamoDB, RedShift, Step Functions, Cloud Formation, CI/CD Pipelines, Github, EMR, RDS,AWS Lake Formation, GitLab, Jenkins and AWS CodePipeline.\n\n\n\nRole Summary: As a Senior Data Engineer,with over 3 years of expertise in Python, PySpark, SQL to design, develop and optimize complex data pipelines, support data modeling, and contribute to the architecture that supports big data processing and analytics to cutting-edge cloud solutions that drive business growth. You will lead the design and implementation of scalable, high-performance data solutions on AWS and mentor junior team members.This role demands a deep understanding of AWS services, big data tools, and complex architectures to support large-scale data processing and advanced analytics.\nKey Responsibilities:\nDesign and develop robust, scalable data pipelines using AWS services, Python, PySpark, and SQL that integrate seamlessly with the broader data and product ecosystem.\nLead the migration of legacy data warehouses and data marts to AWS cloud-based data lake and data warehouse solutions.\nOptimize data processing and storage for performance and cost.\nImplement data security and compliance best practices, in collaboration with the IT security team.\nBuild flexible and scalable systems to handle the growing demands of real-time analytics and big data processing.\nWork closely with data scientists and analysts to support their data needs and assist in building complex queries and data analysis pipelines.\nCollaborate with cross-functional teams to understand their data needs and translate them into technical requirements.\nContinuously evaluate new technologies and AWS services to enhance data capabilities and performance.\nCreate and maintain comprehensive documentation of data pipelines, architectures, and workflows.\nParticipate in code reviews and ensure that all solutions are aligned to pre-defined architectural specifications.\nPresent findings to executive leadership and recommend data-driven strategies for business growth.\nCommunicate effectively with different levels of management to gather use cases/requirements and provide designs that cater to those stakeholders.\nHandle clients in multiple industries at the same time, balancing their unique needs.\nProvide mentoring and guidance to junior data engineers and team members.\n\n\n\nRequirements:\n3+ years of experience in a data engineering role with a strong focus on AWS, Python, PySpark, Hive, and SQL.\nProven experience in designing and delivering large-scale data warehousing and data processing solutions.\nLead the design and implementation of complex, scalable data pipelines using AWS services such as S3, EC2, EMR, RDS, Redshift, Glue, Lambda, Athena, and AWS Lake Formation.\nBachelor's or Masters degree in Computer Science, Engineering, or a related technical field.\nDeep knowledge of big data technologies and ETL tools, such as Apache Spark, PySpark, Hadoop, Kafka, and Spark Streaming.\nImplement data architecture patterns, including event-driven pipelines, Lambda architectures, and data lakes.\nIncorporate modern tools like Databricks, Airflow, and Terraform for orchestration and infrastructure as code.\nImplement CI/CD using GitLab, Jenkins, and AWS CodePipeline.\nEnsure data security, governance, and compliance by leveraging tools such as IAM, KMS, and AWS CloudTrail.\nMentor junior engineers, fostering a culture of continuous learning and improvement.\nExcellent problem-solving and analytical skills, with a strategic mindset.\nStrong communication and leadership skills, with the ability to influence stakeholders at all levels.\nAbility to work independently as well as part of a team in a fast-paced environment.\nAdvanced data visualization skills and the ability to present complex data in a clear and concise manner.\nExcellent communication skills, both written and verbal, to collaborate effectively across teams and levels.\n\nPreferred Skills:\nExperience with Databricks, Snowflake, and machine learning pipelines.\nExposure to real-time data streaming technologies and architectures.\nFamiliarity with containerization and serverless computing (Docker, Kubernetes, AWS Lambda).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Aws Glue', 'SQL', 'Data Pipeline', 'Python', 'Amazon Ec2', 'Data Engineering', 'Data Bricks', 'Aws Lambda', 'Amazon Redshift', 'Azure Cloud', 'Data Lake', 'Data Modeling', 'Athena']",2025-06-12 13:42:51
Data Engineer 4,Comcast,5 - 11 years,Not Disclosed,['Chennai'],".\nResponsible for designing, building and overseeing the deployment and operation of technology architecture, solutions and software to capture, manage, store and utilize structured and unstructured data from internal and external sources. Establishes and builds processes and structures based on business and technical requirements to channel data from multiple inputs, route appropriately and store using any combination of distributed (cloud) structures, local databases, and other applicable storage forms as required. Develops technical tools and programming that leverage artificial intelligence, machine learning and big-data techniques to cleanse, organize and transform data and to maintain, defend and update data structures and integrity on an automated basis. Creates and establishes design standards and assurance processes for software, systems and applications development to ensure compatibility and operability of data connections, flows and storage requirements. Reviews internal and external business and product requirements for data operations and activity and suggests changes and upgrades to systems and storage to accommodate ongoing needs. Work with data modelers/analysts to understand the business problems they are trying to solve then create or augment data assets to feed their analysis. Integrates knowledge of business and functional priorities. Acts as a key contributor in a complex and crucial environment. May lead teams or projects and shares expertise.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBachelors Degree\nWhile possessing the stated degree is preferred, Comcast also may consider applicants who hold some combination of coursework and experience, or who have extensive related professional experience.\n7-10 Years\nComcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, genetic information, or any other basis protected by applicable law.",,,,"['Engineering services', 'Assurance', 'Process optimization', 'MySQL', 'Machine learning', 'Data structures', 'Data quality', 'Troubleshooting', 'Downstream', 'Python']",2025-06-12 13:42:53
Data Engineer,Fortune India 500 Chemicals Firm,12 - 18 years,Not Disclosed,['Mumbai (All Areas)'],"Skills:\nData Management: Expertise in data warehousing, SQL/NoSQL, cloud platforms (AWS, Azure, GCP)\nETL Tools: Proficient in Informatica, Talend, Azure Data Factory\nModelling: Strong in dimensional modelling, star/snowflake schema\nGovernance & Compliance: Knowledge of GDPR, HIPAA, data governance frameworks\nLanguages: T-SQL, PL/SQL\nSoft Skills: Effective communicator, strong analytical and problem-solving skills\nKey Responsibilities:\nArchitecture: Designed scalable, high-performance data warehouse architectures and data models\nETL & Integration: Led ETL design/development for structured/unstructured data across platforms\nGovernance: Defined data quality standards and collaborated on data governance policy implementation\nCollaboration: Interfaced with BI, data science, and business teams to align data strategies\nPerformance & Security: Optimized queries/ETL jobs and ensured data security and compliance\nDocumentation: Maintained standards and documentation for architecture, ETL, and workflows",Industry Type: Chemicals,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Warehousing', 'GCP', 'Snowflake', 'Microsoft Azure', 'Dimensional Modeling', 'Data Modeling', 'ETL', 'AWS']",2025-06-12 13:42:56
Data Engineer- MS Fabric,InfoCepts,5 - 9 years,Not Disclosed,['India'],"Position: Data Engineer – MS Fabric\n  Purpose of the Position: As an MS Fabric Data engineer you will be responsible for designing, implementing, and managing scalable data pipelines. Strong experience in implementation and management of lake House using MS Fabric Azure Tech stack (ADLS Gen2, ADF, Azure SQL) .\nProficiency in data integration techniques, ETL processes and data pipeline architectures. Well versed in Data Quality rules, principles and implementation.\n",,,,"['components', 'data', 'scala', 'delta', 'pyspark', 'data warehousing', 'rules', 'azure data factory', 'sql', 'parquet', 'analytics', 'sql azure', 'spark', 'oracle adf', 'data pipeline architecture', 'etl', 'python', 'azure synapse', 'microsoft azure', 'power bi', 'data bricks', 'data quality', 'system', 't', 'fabric', 'data integration', 'etl process']",2025-06-12 13:42:58
Cloud Data Engineer,PwC India,5 - 8 years,10-20 Lacs P.A.,['Bengaluru'],"Role & responsibilities\nStrong hands-on experience with multi cloud (AWS, Azure, GCP)  services such as GCP BigQuery, Dataform AWS Redshift, \nProficient in PySpark and SQL for building scalable data processing pipelines\nKnowledge of utilizing serverless technologies like AWS Lambda, and Google Cloud Functions \nExperience in orchestration frameworks like Apache Airflow, Kubernetes, and Jenkins to manage and orchestrate data pipelines",,,,"['Pyspark', 'Python', 'SQL', 'Datafactory', 'GCP', 'Microsoft Azure', 'AWS', 'Data Bricks']",2025-06-12 13:43:00
Data Engineer - ETL/ Python,Meritus Management Service,5 - 7 years,10-14 Lacs P.A.,['Indore'],"Focus on Python, you'll play a crucial role in designing, developing, and maintaining data pipelines and ETL processes. Python to manage large datasets, automate data workflows, and ensure data accuracy and efficiency across our organization.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pandas', 'MySQL', 'Sqlalchemy', 'Numpy', 'Python', 'Azure Synapse', 'Postgresql', 'Etl Process', 'SQL']",2025-06-12 13:43:03
"Data Engineer Openings at Advantum Health, Hyderabad",Advantum Health,3 - 5 years,Not Disclosed,['Hyderabad'],"Data Engineer openings at Advantum Health Pvt Ltd, Hyderabad.\nOverview:\nWe are looking for a Data Engineer to build and optimize robust data pipelines that support AI and RCM analytics. This role involves integrating structured and unstructured data from diverse healthcare systems into scalable, AI-ready datasets.\nKey Responsibilities:\nDesign, implement, and optimize data pipelines for ingesting and transforming healthcare and RCM data.\nBuild data marts and warehouses to support analytics and machine learning.\nEnsure data quality, lineage, and governance across AI use cases.\nIntegrate data from EMRs, billing platforms, claims databases, and third-party APIs.\nSupport data infrastructure in a HIPAA-compliant cloud environment.\nQualifications:\nBachelors in Computer Science, Data Engineering, or related field.\n3+ years of experience with ETL/ELT pipelines using tools like Apache Airflow, dbt, or Azure Data Factory.\nStrong SQL and Python skills.\nExperience with healthcare data standards (HL7, FHIR, X12) preferred.\nFamiliarity with data lake house architectures and AI integration best practices\nPh: 9177078628\nEmail id: jobs@advantumhealth.com\nAddress: Advantum Health Private Limited, Cyber gateway, Block C, 4th floor Hitech City, Hyderabad.\nDo follow us on LinkedIn, Facebook, Instagram, YouTube and Threads\nAdvantum Health LinkedIn Page:\nhttps://lnkd.in/gVcQAXK3\n\nAdvantum Health Facebook Page:\nhttps://lnkd.in/g7ARQ378\n\nAdvantum Health Instagram Page:\nhttps://lnkd.in/gtQnB_Gc\n\nAdvantum Health India YouTube link:\nhttps://lnkd.in/g_AxPaPp\n\nAdvantum Health Threads link:\nhttps://lnkd.in/gyq73iQ6",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'SQL', 'Python', 'Airflow', 'ETL', 'Elt']",2025-06-12 13:43:06
Data Engineer,Conversehr Business Solutions,4 - 7 years,15-30 Lacs P.A.,['Hyderabad'],"What are the ongoing responsibilities of Data Engineer responsible for?\nWe are building a growing Data and AI team. You will play a critical role in the efforts to centralize structured and unstructured data for the firm. We seek a candidate with skills in data modeling, data management and data governance, and can contribute first-hand towards firms data strategy. The ideal candidate is a self-starter with a strong technical foundation, a collaborative mindset, and the ability to navigate complex data challenges #ASSOCIATE\nWhat ideal qualifications, skills & experience would help someone to be successful?\nBachelors degree in computer science or computer applications; or equivalent experience in lieu of degree with 3 years of industry experience.\nStrong expertise in data modeling and data management concepts. Experience in implementing master data management is preferred.\nSound knowledge on Snowflake and data warehousing techniques.\nExperience in building, optimizing, and maintaining data pipelines and data management frameworks to support business needs.\nProficiency in at least one programming language, preferably python.\nCollaborate with cross-functional teams to translate business needs into scalable data and AI-driven solutions.\nTake ownership of projects from ideation to production, operating in a startup-like culture within an enterprise environment. Excellent communication, collaboration, and ownership mindset.\nFoundational Knowledge of API development and integration.\nKnowledge of Tableau, Alteryx is good-to-have.\nWork Shift Timings - 2:00 PM - 11:00 PM IST",Industry Type: Financial Services (Asset Management),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Master Data Management', 'Python', 'Etl Pipelines', 'Alteryx', 'ai', 'Data Modeling', 'Tableau', 'ETL']",2025-06-12 13:43:08
Azure Data Engineer,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of the role is to support process delivery by ensuring daily performance of the Production Specialists, resolve technical escalations and develop technical capability within the Production Specialists.\n\nDo\nOversee and support process by reviewing daily transactions on performance parameters\nReview performance dashboard and the scores for the team\nSupport the team in improving performance parameters by providing technical support and process guidance\nRecord, track, and document all queries received, problem-solving steps taken and total successful and unsuccessful resolutions\nEnsure standard processes and procedures are followed to resolve all client queries\nResolve client queries as per the SLAs defined in the contract\nDevelop understanding of process/ product for the team members to facilitate better client interaction and troubleshooting\nDocument and analyze call logs to spot most occurring trends to prevent future problems\nIdentify red flags and escalate serious client issues to Team leader in cases of untimely resolution\nEnsure all product information and disclosures are given to clients before and after the call/email requests\nAvoids legal challenges by monitoring compliance with service agreements\n\nHandle technical escalations through effective diagnosis and troubleshooting of client queries\nManage and resolve technical roadblocks/ escalations as per SLA and quality requirements\nIf unable to resolve the issues, timely escalate the issues to TA & SES\nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers and clients business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract SLAs\nBuild people capability to ensure operational excellence and maintain superior customer service levels of the existing account/client\nMentor and guide Production Specialists on improving technical knowledge\nCollate trainings to be conducted as triage to bridge the skill gaps identified through interviews with the Production Specialist\nDevelop and conduct trainings (Triages) within products for production specialist as per target\nInform client about the triages being conducted\nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nIdentify and document most common problems and recommend appropriate resolutions to the team\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\nDeliver\n\nNoPerformance ParameterMeasure1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback, NSAT/ ESAT2Team ManagementProductivity, efficiency, absenteeism3Capability developmentTriages completed, Technical Test performance\nMandatory Skills: Azure Data Factory. Experience: 5-8 Years.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'azure databricks', 'azure data lake', 'ssas', 'ssrs', 'microsoft azure', 'azure data factory', 'ssis', 'msbi', 'sql server', 'sql']",2025-06-12 13:43:10
AWS Data Engineer,Infosys,5 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Significant 5 to 9 years of experience in designing and implementing scalable data engineering solutions on AWS.\nStrong proficiency in Python programming language.\nExpertise in serverless architecture and AWS services such as Lambda, Glue, Redshift, Kinesis, SNS, SQS, and CloudFormation.\nExperience with Infrastructure as Code (IaC) using AWS CDK for defining and provisioning AWS resources.\nProven leadership skills with the ability to mentor and guide junior team members.\nExcellent understanding of data modeling concepts and experience with tools like ERStudio.\nStrong communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.\nExperience with Apache Airflow for orchestrating data pipelines is a plus.\nKnowledge of Data Lakehouse, dbt, or Apache Hudi data format is a plus.\nRoles and Responsibilities\nDesign, develop, test, deploy, and maintain large-scale data pipelines using AWS services such as S3, Glue, Lambda, Redshift.\nCollaborate with cross-functional teams to gather requirements and design solutions that meet business needs.\nDesired Candidate Profile\n5-9 years of experience in an IT industry setting with expertise in Python programming language (Pyspark).\nStrong understanding of AWS ecosystem including S3, Glue, Lambda, Redshift.\nBachelor's degree in Any Specialization (B.Tech/B.E.).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Aws Glue', 'Pyspark', 'Aws Lambda', 'Amazon Redshift', 'Athena', 'Python']",2025-06-12 13:43:12
Senior Data Engineer,Grid Dynamics,8 - 13 years,15-25 Lacs P.A.,['Bengaluru'],"We are looking for an enthusiastic and technology-proficient Big Data Engineer, who is eager to participate in the design and implementation of a top-notch Big Data solution to be deployed at massive scale.\nOur customer is one of the world's largest technology companies based in Silicon Valley with operations all over the world. On this project we are working on the bleeding-edge of Big Data technology to develop high performance data analytics platform, which handles petabytes datasets.\nEssential functions\nParticipate in design and development of Big Data analytical applications.\nDesign, support and continuously enhance the project code base, continuous integration pipeline, etc.\nWrite complex ETL processes and frameworks for analytics and data management.\nImplement large-scale near real-time streaming data processing pipelines.\nWork inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale.\nQualifications\nStrong coding experience with Scala, Spark,Hive, Hadoop.\nIn-depth knowledge of Hadoop and Spark, experience with data mining and stream processing technologies (Kafka, Spark Streaming, Akka Streams).\nUnderstanding of the best practices in data quality and quality engineering.\nExperience with version control systems, Git in particular.\nDesire and ability for quick learning of new tools and technologies.\nWould be a plus\nKnowledge of Unix-based operating systems (bash/ssh/ps/grep etc.).\nExperience with Github-based development processes.\nExperience with JVM build systems (SBT, Maven, Gradle).\nWe offer\nOpportunity to work on bleeding-edge projects\nWork with a highly motivated and dedicated team\nCompetitive salary\nFlexible schedule\nBenefits package - medical insurance, sports\nCorporate social events\nProfessional development opportunities\nWell-equipped office\nAbout us\nGrid Dynamics (NASDAQ: GDYN) is a leading provider of technology consulting, platform and product engineering, AI, and advanced analytics services. Fusing technical vision with business acumen, we solve the most pressing technical challenges and enable positive business outcomes for enterprise companies undergoing business transformation. A key differentiator for Grid Dynamics is our 8 years of experience and leadership in enterprise AI, supported by profound expertise and ongoing investment in data, analytics, cloud & DevOps, application modernization and customer experience. Founded in 2006, Grid Dynamics is headquartered in Silicon Valley with offices across the Americas, Europe, and India.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hive', 'SCALA', 'Hadoop', 'Big Data', 'Spark']",2025-06-12 13:43:14
Big Data Engineer,Rarr Technologies,6 - 8 years,Not Disclosed,['Bengaluru'],Job description\n\nProven experience working with data pipelines ETL BI regardless of the technology\nProven experience working with AWS including at least 3 of RedShift S3 EMR Cloud Formation DynamoDB RDS lambda\nBig Data technologies and distributed systems one of Spark Presto or Hive\nPython language scripting and object oriented\nFluency in SQL for data warehousing RedShift in particular is a plus\nGood understanding on data warehousing and Data modelling concepts\nFamiliar with GIT Linux CICD pipelines is a plus\nStrong systems process orientation with demonstrated analytical thinking organization skills and problem solving skills\nAbility to self manage prioritize and execute tasks in a demanding environment\nStrong consultancy orientation and experience with the ability to form collaborative\nproductive working relationships across diverse teams and cultures is a must\nWillingness and ability to train and teach others\nAbility to facilitate meetings and follow up with resulting action items,Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['python scripting', 'Big Data Technologies', 'ETL', 'AWS']",2025-06-12 13:43:17
AWS Data Engineer,Infosys,5 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","AWS Data Engineer\n\nTo Apply, use the below link:\nhttps://career.infosys.com/jobdesc?jobReferenceCode=INFSYS-EXTERNAL-210775&rc=0\n\nJOB Profile:\nSignificant 5 to 9 years of experience in designing and implementing scalable data engineering solutions on AWS.\nStrong proficiency in Python programming language.\nExpertise in serverless architecture and AWS services such as Lambda, Glue, Redshift, Kinesis, SNS, SQS, and CloudFormation.\nExperience with Infrastructure as Code (IaC) using AWS CDK for defining and provisioning AWS resources.\nProven leadership skills with the ability to mentor and guide junior team members.\nExcellent understanding of data modeling concepts and experience with tools like ERStudio.\nStrong communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.\nExperience with Apache Airflow for orchestrating data pipelines is a plus.\nKnowledge of Data Lakehouse, dbt, or Apache Hudi data format is a plus.\n\n\nRoles and Responsibilities\nDesign, develop, test, deploy, and maintain large-scale data pipelines using AWS services such as S3, Glue, Lambda, Redshift.\nCollaborate with cross-functional teams to gather requirements and design solutions that meet business needs.\nDesired Candidate Profile\n5-9 years of experience in an IT industry setting with expertise in Python programming language (Pyspark).\nStrong understanding of AWS ecosystem including S3, Glue, Lambda, Redshift.\nBachelor's degree in Any Specialization (B.Tech/B.E.).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Aws Glue', 'AWS Data Engineer', 'Pyspark', 'Aws Lambda', 'Redshift Aws', 'Python']",2025-06-12 13:43:19
Azure Data Engineer,Infosys,5 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Job description\nWe are looking for Azure Data Engineer's resources having minimum 5 to 9 years of Experience.\n\nRole & responsibilities\nBlend of technical expertise with 5 to 9 year of experience, analytical problem-solving, and collaboration with cross-functional teams. Design and implement Azure data engineering solutions (Ingestion & Curation)\nCreate and maintain Azure data solutions including Azure SQL Database, Azure Data Lake, and Azure Blob Storage.\nDesign, implement, and maintain data pipelines for data ingestion, processing, and transformation in Azure.\nUtilizing Azure Data Factory or comparable technologies, create and maintain ETL (Extract, Transform, Load) operations\nUse Azure Data Factory and Databricks to assemble large, complex data sets\nImplementing data validation and cleansing procedures will ensure the quality, integrity, and dependability of the data.\nEnsure data quality / security and compliance.\nOptimize Azure SQL databases for efficient query performance.\nCollaborate with data engineers, and other stakeholders to understand requirements and translate them into scalable and reliable data platform architectures.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Azure Data Factory', 'Azure Synapse', 'Pyspark', 'Azure Data Lake']",2025-06-12 13:43:21
Data Engineer,Reputed Client,5 - 10 years,18-25 Lacs P.A.,[],"Data Engineer\n(Python, PySpark, SQL and Spark SQL)\n\nExperience - 5-10 Years\nMandate Skills: Python, PySpark, SQL and SparkSQL\nWorking Hours: 11:00 am to 8 pm\n\n(Candidate has to be flexible. 4-hour overlap with US business hours)\n\nSalary : 1.50 LPM to 2 LPM + Tax (Rate is not fixed, negotiable depending upon the candidate feedback)\n\nRemote / Hybrid (3 Days in a week WFO) (Pune, Bangalore, Noida, Mumbai, Hyderabad)\n\nNOTE: Need candidates within these cities, they have to collect assets from the office / need to be available for meetings - if they are working remotely)\n\nIt's a 6 months (C2H role).",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Python', 'PySpark', 'Spark', 'SQL']",2025-06-12 13:43:24
Data Engineer,Meritus Management Service,4 - 9 years,9-18 Lacs P.A.,"['Pune', 'Gurugram']","The first Data Engineer specializes in traditional ETL with SAS DI and Big Data (Hadoop, Hive). The second is more versatile, skilled in modern data engineering with Python, MongoDB, and real-time processing.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Etl Pipelines', 'Big Data', 'Informatica', 'SAS DI', 'SQL', 'Hive', 'Hadoop', 'Talend', 'ETL Tool', 'Python']",2025-06-12 13:43:26
Architect (Data Engineering),Amgen Inc,9 - 12 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\n\nRole Description:\n\nWe are seeking a Data Solutions Architect with deep expertise in Biotech/Pharma to design, implement, and optimize scalable and high-performance data solutions that support enterprise analytics, AI-driven insights, and digital transformation initiatives. This role will focus on data strategy, architecture, governance, security, and operational efficiency, ensuring seamless data integration across modern cloud platforms. The ideal candidate will work closely with engineering teams, business stakeholders, and leadership to establish a future-ready data ecosystem, balancing performance, cost-efficiency, security, and usability. This position requires expertise in modern cloud-based data architectures, data engineering best practices, and Scaled Agile methodologies.\n\nRoles & Responsibilities:\nDesign and implement scalable, modular, and future-proof data architectures that initiatives in enterprise.\nDevelop enterprise-wide data frameworks that enable governed, secure, and accessible data across various business domains.\nDefine data modeling strategies to support structured and unstructured data, ensuring efficiency, consistency, and usability across analytical platforms.\nLead the development of high-performance data pipelines for batch and real-time data processing, integrating APIs, streaming sources, transactional systems, and external data platforms.\nOptimize query performance, indexing, caching, and storage strategies to enhance scalability, cost efficiency, and analytical capabilities.\nEstablish data interoperability frameworks that enable seamless integration across multiple data sources and platforms.\nDrive data governance strategies, ensuring security, compliance, access controls, and lineage tracking are embedded into enterprise data solutions.\nImplement DataOps best practices, including CI/CD for data pipelines, automated monitoring, and proactive issue resolution, to improve operational efficiency.\nLead Scaled Agile (SAFe) practices, facilitating Program Increment (PI) Planning, Sprint Planning, and Agile ceremonies, ensuring iterative delivery of enterprise data capabilities.\nCollaborate with business stakeholders, product teams, and technology leaders to align data architecture strategies with organizational goals.\nAct as a trusted advisor on emerging data technologies and trends, ensuring that the enterprise adopts cutting-edge data solutions that provide competitive advantage and long-term scalability.\nMust-Have\n\nSkills:\nExperience in data architecture, enterprise data management, and cloud-based analytics solutions.\nWell versed in domain of Biotech/Pharma industry and has been instrumental in solving complex problems for them using data strategy.\nExpertise in Databricks, cloud-native data platforms, and distributed computing frameworks.\nStrong proficiency in modern data modeling techniques, including dimensional modeling, NoSQL, and data virtualization.\nExperience designing high-performance ETL/ELT pipelines and real-time data processing solutions.\nDeep understanding of data governance, security, metadata management, and access control frameworks.\nHands-on experience with CI/CD for data solutions, DataOps automation, and infrastructure as code (IaC).\nProven ability to collaborate with cross-functional teams, including business executives, data engineers, and analytics teams, to drive successful data initiatives.\nStrong problem-solving, strategic thinking, and technical leadership skills.\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with Apache Spark, Apache Airflow\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nGood-to-Have\n\nSkills:\nExperience with Data Mesh architectures and federated data governance models.\nCertification in cloud data platforms or enterprise architecture frameworks.\nKnowledge of AI/ML pipeline integration within enterprise data architectures.\nFamiliarity with BI & analytics platforms for enabling self-service analytics and enterprise reporting.\nEducation and Professional Certifications\n9 to 12 years of experience in Computer Science, IT or related field\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nSoft\n\nSkills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'continuous integration', 'technical leadership', 'metadata management', 'presentation skills', 'ci/cd', 'distributed computing', 'sql', 'data bricks', 'git', 'data modeling', 'spark', 'devops', 'data governance', 'jenkins', 'troubleshooting', 'access control', 'etl']",2025-06-12 13:43:29
Data Engineer III,Expedia Group,5 - 10 years,Not Disclosed,['Bengaluru'],"Why Join Us?\nTo shape the future of travel, people must come first. Guided by our Values and Leadership Agreements, we foster an open culture where everyone belongs, differences are celebrated and know that when one of us wins, we all win.\nWe provide a full benefits package, including exciting travel perks, generous time-off, parental leave, a flexible work model (with some pretty cool offices), and career development resources, all to fuel our employees passion for travel and ensure a rewarding career journey. We re building a more open world. Join us.\nData Engineer III\nIntroduction to the Team\nExpedia Technology teams partner with our Product teams to create innovative products, services, and tools to deliver high-quality experiences for travelers, partners, and our employees. A singular technology platform powered by data and machine learning provides secure, differentiated, and personalized experiences that drive loyalty and traveler satisfaction.\nExpedia Group is seeking a skilled and motivated Data Engineer III to join our Finance Business Intelligence team supporting the Product & Technology Finance organization. In this role, you will help drive data infrastructure and analytics solutions that support strategic financial planning, reporting, and operational decision-making across the Global Finance community. You ll work closely with Finance and Technology partners to ensure data accuracy, accessibility, and usability in support of Expedia s business objectives.\nAs a Data Engineer III, you have strong experience working with a variety of datasets, data environments, tools, and analytical techniques. You enjoy a fun, collaborative and stimulating team environment. Successful candidates should be able to own projects end-to-end, including identifying problems and solutions, building and maintain data pipelines and dashboards, distilling key insights and communicate to stakeholders.\nIn this role, you will:\nDevelop new and improve existing end to end Business Intelligence products (data pipelines, Tableau dashboards, and Machine Learning predictive forecasting models).\nDrive internal efficiencies through streamline code/documentation/Tableau development to maintain high data integrity.\nTroubleshoot and resolve production issues with the team products (automation opportunities, optimizations, back-end data issues, data reconciliations).\nProactively reach out to subject matter experts /stakeholders and collaborate to solve problems.\nRespond to ad hoc data requests and conduct analysis to provide valuable insights to stakeholders.\nCollaborate and coordinate with team members/stakeholders to translate complex data into meaningful insights, that improve the analytical capabilities of the business.\nApply knowledge of database design to support migration of data pipelines from on prem to cloud environment (including data extraction, ingestion, processing of large data sets)\nSupport dashboard development on cloud environment to enable self-service reporting.\nCommunicate clearly on current work status and design considerations\nThink broadly and comprehend the how, why, and what behind data architecture designs\nExperience & Qualifications:\nBachelor s in Computer Science, Mathematics, Statistics, Information Systems, or related field\n5+ years experience in a Data Analyst, Data Engineer or Business Analyst role\nProven expertise in SQL, with practical experience utilizing query engines including SQL Server, Starburst, Trino, Querybook and data science tools such as Python/R, SparkSQL.\nProficient visualization skills (Tableau, Looker, or similar) and excel modeling/report automation.\nExceptional understanding of relational and dimensional datasets, data warehouse and data mining and applies database design principles to solve data requirements\nExperience building robust data extract, load and transform (ELT) processes, that source data from multiple databases.\nDemonstrated record of defining and executing key analysis and solving problems with minimal supervision.\nDynamic individual contributor who consistently enhances operational playbooks to address business problems.\n3+ year working in a hybrid environment that uses both on-premise and cloud technologies is preferred.\nExperience working in an environment that manipulates large datasets on the cloud platform preferred.\nBackground in analytics, finance or a comparable reporting and analytics role preferred.\nAccommodation requests\nIf you need assistance with any part of the application or recruiting process due to a disability, or other physical or mental health conditions, please reach out to our Recruiting Accommodations Team through the Accommodation Request .\nWe are proud to be named as a Best Place to Work on Glassdoor in 2024 and be recognized for award-winning culture by organizations like Forbes, TIME, Disability:IN, and others.\nExpedia Groups family of brands includes: Brand Expedia , Hotels.com , Expedia Partner Solutions, Vrbo , trivago , Orbitz , Travelocity , Hotwire , Wotif , ebookers , CheapTickets , Expedia Group Media Solutions, Expedia Local Expert , CarRentals.com , and Expedia Cruises . 2024 Expedia, Inc. All rights reserved. Trademarks and logos are the property of their respective owners. . Never provide sensitive, personal information to someone unless you re confident who the recipient is. Expedia Group does not extend job offers via email or any other messaging tools to individuals with whom we have not made prior contact. Our email domain is @expediagroup.com. The official website to find and apply for job openings at Expedia Group is careers.expediagroup.com/jobs .\nExpedia is committed to creating an inclusive work environment with a diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, gender, sexual orientation, national origin, disability or age.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Database design', 'Machine learning', 'Business intelligence', 'Data mining', 'Analytics', 'SQL', 'Python', 'Data architecture']",2025-06-12 13:43:32
Azure Data Engineer,Hexaware Technologies,6 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","Hi\n\nWork Location : Chennai AND Bangalore\nWork location : Imm - 30 days\n\nPrimary: Azure Databricks,ADF, Pyspark SQL",,,,"['Pyspark', 'Azure Databricks', 'SQL', 'Azure Data Factory']",2025-06-12 13:43:34
Data Engineer,Society Managers,3 - 5 years,Not Disclosed,['Mumbai (All Areas)'],"We are seeking a skilled and driven SDE-II (Data Engineering) to join our dynamic team. In this role, you will design, develop, and maintain scalable data pipelines, working with large, complex datasets. Youll collaborate closely with cross-functional teams to gather data requirements and contribute to the architecture of our data systems, leveraging your expertise in tools like Databricks, Spark, and SQL.\n\nRoles and responsibilities\nData Pipeline Development: Design,build, and maintain scalable data pipelines using Databricks, Python,and Spark.\nData Processing & Transformation: Handle large, complex datasets to ensure efficient data processing and transformations.\nCollaboration: Work with cross-functional teams to gather, understand, and implement data requirements.\nSQL & ETL: Write and optimize SQL queries for data extraction, transformation, and loading (ETL) processes.\nData Quality & Security: Ensure data accuracy, integrity, and security across all stages of the data lifecycle.\nSystem Design & Architecture: Contribute to the design and architecture of scalable data systems and solutions.\nRequired Skills and Qualification\nExperience: 3+ years of experience in data engineering or a related field.\nDatabricks & Spark: Strong expertise in Databricks and distributed data processing with Spark.\nProgramming: Proficiency in Python for data engineering tasks.\nSQL Optimization: Solid experience in writing and optimizing complex SQL queries.\nData Systems Knowledge: Hands-on experience with large-scale data systems and tools.\nDomain Knowledge: Familiarity with Capital Market/Private Equity is a plus (relaxation may apply).\nData Visualization: Experience with Tableau for creating insightful data visualizations and reports.\nPreferred skills\nCloud Platforms: Familiarity with cloud services like AWS, Azure, or GCP.\nData Warehousing & ETL:Experience with data warehousing concepts and ETL processes.\nAnalytical Skills: Strong problem-solving and analytical capabilities Analytics Tools: Hands-on experience with tools like Amplitude, PostHog, Google Analytics, or Mixpanel.\nAdditional Tools: Knowledge of Python for web scraping and frameworks like Django (good to have).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Microsoft Azure', 'Data Bricks', 'Spark', 'ETL', 'Python', 'SQL']",2025-06-12 13:43:36
Big Data Engineer,Apexon,11 - 16 years,Not Disclosed,['Bengaluru'],"We enable #HumanFirstDigital\n\nJob Summary:\nWe are looking for a highly experienced and strategic Data Engineer to drive the design, development, and optimization of our enterprise data platform. This role requires deep technical expertise in AWS, StreamSets, and Snowflake, along with solid experience in Kubernetes, Apache Airflow, and unit testing. The ideal candidate will lead a team of data engineers and play a key role in delivering scalable, secure, and high-performance data solutions for both historical and incremental data loads.\nKey Responsibilities:\nLead the architecture, design, and implementation of end-to-end data pipelines using StreamSets and Snowflake.\nOversee the development of scalable ETL/ELT processes for historical data migration and incremental data ingestion.\nGuide the team in leveraging AWS services (S3, Lambda, Glue, IAM, etc.) to build cloud-native data solutions.\nProvide technical leadership in deploying and managing containerized applications using Kubernetes.\nDefine and implement workflow orchestration strategies using Apache Airflow.\nEstablish best practices for unit testing, code quality, and data validation.\nCollaborate with data architects, analysts, and business stakeholders to align data solutions with business goals.\nMentor junior engineers and foster a culture of continuous improvement and innovation.\nMonitor and optimize data workflows for performance, scalability, and cost-efficiency.\nRequired Skills & Qualifications:\nHigh proficiency in AWS, including hands-on experience with core services (S3, Lambda, Glue, IAM, CloudWatch).\nExpert-level experience with StreamSets, including Data Collector, Transformer, and Control Hub.\nStrong Snowflake expertise, including data modeling, SnowSQL, and performance tuning.\nMedium-level experience with Kubernetes, including container orchestration and deployment.\nWorking knowledge of Apache Airflow for workflow scheduling and monitoring.\nExperience with unit testing frameworks and practices in data engineering.\nProven experience in building and managing ETL pipelines for both batch and real-time data.\nStrong command of SQL and scripting languages such as Python or Shell.\nExperience with CI/CD pipelines and version control tools (e.g., Git, Jenkins).\nPreferred Qualifications:\nAWS certification (e.g., AWS Certified Data Analytics, Solutions Architect).\nExperience with data governance, security, and compliance frameworks.\nFamiliarity with Agile methodologies and tools like Jira and Confluence.\nPrior experience in a leadership or mentoring role within a data engineering team.\nOur Commitment to Diversity & Inclusion:\nOur Perks and Benefits:\nOur benefits and rewards program has been thoughtfully designed to recognize your skills and contributions, elevate your learning/upskilling experience and provide care and support for you and your loved ones. As an Apexon Associate, you get continuous skill-based development, opportunities for career advancement, and access to comprehensive health and well-being benefits and assistance.\nWe also offer:\no Group Health Insurance covering family of 4\no Term Insurance and Accident Insurance\no Paid Holidays & Earned Leaves\no Paid Parental LeaveoLearning & Career Development\no Employee Wellness\nJob Location : Bengaluru, India",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Data modeling', 'Agile', 'Wellness', 'Workflow', 'Healthcare', 'Unit testing', 'Apache', 'SQL', 'Python']",2025-06-12 13:43:38
Big Data Engineer,Client of Hiresquad Resources,5 - 8 years,22.5-30 Lacs P.A.,"['Noida', 'Hyderabad', 'Bengaluru']","Role: Data Engineer\nExp: 5 to 8 Years\nLocation: Bangalore, Noida, and Hyderabad (Hybrid, weekly 2 Days office must)\nNP: Immediate to 15 Days (Try to find only immediate joiners)\n\n\nNote:\nCandidate must have experience in Python, Kafka Streams, Pyspark, and Azure Databricks.\nNot looking for candidates who have only Exp in Pyspark and not in Python.\n\n\nJob Title: SSE Kafka, Python, and Azure Databricks (Healthcare Data Project)\nExperience:  5 to 8 years\n\nRole Overview:\nWe are looking for a highly skilled with expertise in Kafka, Python, and Azure Databricks (preferred) to drive our healthcare data engineering projects. The ideal candidate will have deep experience in real-time data streaming, cloud-based data platforms, and large-scale data processing. This role requires strong technical leadership, problem-solving abilities, and the ability to collaborate with cross-functional teams.\n\nKey Responsibilities:\nLead the design, development, and implementation of real-time data pipelines using Kafka, Python, and Azure Databricks.\nArchitect scalable data streaming and processing solutions to support healthcare data workflows.\nDevelop, optimize, and maintain ETL/ELT pipelines for structured and unstructured healthcare data.\nEnsure data integrity, security, and compliance with healthcare regulations (HIPAA, HITRUST, etc.).\nCollaborate with data engineers, analysts, and business stakeholders to understand requirements and translate them into technical solutions.\nTroubleshoot and optimize Kafka streaming applications, Python scripts, and Databricks workflows.\nMentor junior engineers, conduct code reviews, and ensure best practices in data engineering.\nStay updated with the latest cloud technologies, big data frameworks, and industry trends.\n\n\nRequired Skills & Qualifications:\n4+ years of experience in data engineering, with strong proficiency in Kafka and Python.\nExpertise in Kafka Streams, Kafka Connect, and Schema Registry for real-time data processing.\nExperience with Azure Databricks (or willingness to learn and adopt it quickly).\nHands-on experience with cloud platforms (Azure preferred, AWS or GCP is a plus).\nProficiency in SQL, NoSQL databases, and data modeling for big data processing.\nKnowledge of containerization (Docker, Kubernetes) and CI/CD pipelines for data applications.\nExperience working with healthcare data (EHR, claims, HL7, FHIR, etc.) is a plus.\nStrong analytical skills, problem-solving mindset, and ability to lead complex data projects.\nExcellent communication and stakeholder management skills.\n\n\n\nEmail: Sam@hiresquad.in",Industry Type: Medical Services / Hospital,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Kafka', 'Azure Databricks', 'Python', 'Etl Pipelines', 'Pyspark', 'spark architecture', 'Data Engineering', 'opps concepts', 'Data Streaming', 'Medallion Architecture', 'python scripts', 'schema registry', 'SQL Database', 'Nosql Databases', 'spark tuning', 'Kafka Streams', 'kafka connect']",2025-06-12 13:43:41
Lead Big Data Engineer - Python & Spark,Hubnex,7 - 12 years,Not Disclosed,['Bengaluru'],"Job Title: Lead Big Data Engineer - Python & Spark\nLocation: Gurgaon, India\nExperience: 7+ Years\nEmployment Type: Full-Time | Onsite\nDepartment: Data Engineering\nAbout Hubnex Labs:\nHubnex Labs is a forward-looking IT consulting and software services company, building next-generation data platforms, AI systems, and enterprise-grade applications. We re looking for a Lead Big Data Engineer to drive the development and deployment of high-performance data processing solutions.\nRole Overview:\nAs a Lead Big Data Engineer , you will be responsible for architecting and implementing scalable big data solutions using Spark, Python, Hive, and related technologies. You will mentor a team of developers and work closely with cross-functional stakeholders to ensure on-time, error-free software delivery.\nKey Responsibilities:\nLead and mentor a team of Python and big data developers to deliver robust data-driven applications\nDesign, develop, and maintain scalable data processing pipelines using Spark (Scala/PySpark), Hive, and Hadoop ecosystems\nWrite efficient, reusable, and well-documented code in Python, SQL, and shell scripting\nOptimize Spark applications for performance and scalability; tune existing Hadoop-based systems\nCollaborate with QA, DevOps, and business teams to ensure high-quality software delivery\nPerform code reviews , enforce coding standards, and contribute to overall architectural decisions\nActively participate in daily Scrum meetings , sprint planning, retrospectives, and release cycles\nTroubleshoot complex issues across the full data pipeline\nRequired Qualifications:\n7+ years of hands-on experience in software development, with a strong background in big data frameworks\nDeep expertise in Hadoop ecosystem including HDFS, Hive, HQL, Spark (Scala/PySpark), Sqoop\n4+ years of programming experience using Python , SQL , and Unix shell scripting\nProven experience in leading development teams and delivering enterprise-level solutions\nStrong grasp of Spark architecture , performance tuning, and data frame APIs\nExcellent understanding of database concepts , data structures , and distributed systems\nBachelors degree in Computer Science , Engineering , or a related field\nExceptional communication, problem-solving, and leadership skills\nPreferred Skills:\nExperience with CI/CD pipelines for data projects\nFamiliarity with cloud-based data platforms (AWS EMR, GCP DataProc, or Azure HDInsight)\nWorking knowledge of Kafka , Airflow , or other data orchestration tools\nWhy Join Hubnex Labs?\nWork on mission-critical big data solutions that impact businesses globally\nBe part of an innovative, agile, and supportive team\nLeadership opportunities and exposure to latest technologies in AI and cloud computing\nCompetitive salary, performance incentives, and professional growth support",Industry Type: Internet,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Performance tuning', 'Cloud computing', 'Coding', 'Agile', 'Data structures', 'Scrum', 'Unix shell scripting', 'SQL', 'Python']",2025-06-12 13:43:43
"Senior Data Engineer (Snowflake, DBT)",Allegis Global Solutions (AGS),5 - 10 years,Not Disclosed,[],"Senior Data Engineer (Snowflake, DBT, Azure)\nJob Location: Hyderabad / Bangalore / Chennai / Kolkata / Noida/ Gurgaon / Pune / Indore / Mumbai\n\nJob Details\nTechnical Expertise:\nStrong proficiency in Snowflake architecture, including data sharing, partitioning, clustering, and materialized views.\nAdvanced experience with DBT for data transformations and workflow management.\nExpertise in Azure services, including Azure Data Factory, Azure Data Lake, Azure Synapse, and Azure Functions.\nData Engineering:\nProficiency in SQL, Python, or other relevant programming languages.\nStrong understanding of data modeling concepts, including star schema and normalization.\nHands-on experience with ETL/ELT pipelines and data integration tools.\n\nPreferred Qualifications:\nCertifications in Snowflake, DBT, or Azure Data Engineering.\nFamiliar with data visualization tools like Power BI or Tableau.\nKnowledge of CI/CD pipelines and DevOps practices for data workflows.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Data Build Tool', 'Azure']",2025-06-12 13:43:46
Senior Data Engineer,Epsilon,5 - 9 years,Not Disclosed,['Bengaluru'],"This position in the Engineering team under the Digital Experience organization. We drive the first mile of the customer experience through personalization of offers and content. We are currently on the lookout for a smart, highly driven engineer.\nYou will be part of a team that is focused on building & managing solutions, pipelines using marketing technology stacks. You will also be expected to Identify and implement improvements including for optimizing data delivery and automate processes/pipelines.\nThe incumbent is also expected to partner with various stakeholders, bring scientific rigor to design and develop high quality solutions.\nCandidate must have excellent verbal and written communication skills and be comfortable working in an entrepreneurial, startup environment within a larger company.\nClick here to view how Epsilon transforms marketing with 1 View, 1 Vision and 1 Voice.\n\nBrief Description of Role:\nExperience with both structured and unstructured data\nExperience working on AdTech or MarTech technologies.\nExperience in relational and non-relational databases and SQL (NoSQL is a plus).\nUnderstanding of Data Modeling, Data Catalog concepts and tools\nAbility to deal with data imperfections such as missing values, outliers, inconsistent formatting, etc.\nCollaborate with other members of the team to ensure high quality deliverables\nLearning and implementing the latest design patterns in data engineering\n\nData Management\nExperience with both structured and unstructured data\nExperience building Data and CI/CD pipelines\nExperience working on AdTech or MarTech technologies is added advantage\nExperience in relational and non-relational databases and SQL (NoSQL is a plus).\nHands on experience building ETL workflows/pipelines on large volumes of data\nGood understanding of Data Modeling, Data Warehouse, Data Catalog concepts and tools\nAble to identify, join, explore, and examine data from multiple disparate sources and formats\nAbility to reduce large quantities of unstructured or formless data and get it into a form in which it can be analyzed\nAbility to deal with data imperfections such as missing values, outliers, inconsistent formatting, etc.\nDevelopment\nAbility to write code in programming languages such as Python and shell script on Linux\nFamiliarity with development methodology such as Agile/Scrum\nLove to learn new technologies, keep abreast of the latest technologies within the cloud architecture, and drive your organization to adapt to emerging best practices\nGood knowledge of working in UNIX/LINUX systems\nQualifications\nBachelors degree in computer science with 5+ years of similar experience\nTech Stack: Python, SQL, Scripting language (preferably JavaScript)\nExperience or knowledge on Adobe Experience Platform (RT-CDP/AEP)\nExperience working in Cloud Platforms (GCP or AWS)\nFamiliarity with automated unit/integration test frameworks\nGood written and spoken communication skills, team player.\nStrong analytic thought process and ability to interpret findings",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Data Bricks', 'Python', 'SQL', 'Azure Aws', 'AWS']",2025-06-12 13:43:48
Data Engineer III,Expedia Group,6 - 11 years,Not Disclosed,['Gurugram'],"Why Join Us?\nTo shape the future of travel, people must come first. Guided by our Values and Leadership Agreements, we foster an open culture where everyone belongs, differences are celebrated and know that when one of us wins, we all win.\nWe provide a full benefits package, including exciting travel perks, generous time-off, parental leave, a flexible work model (with some pretty cool offices), and career development resources, all to fuel our employees passion for travel and ensure a rewarding career journey. We re building a more open world. Join us.\nData Engineer III\nExpedia Group s CTO Enablement team is looking for a highly motivated Data Engineer III to lead the design, delivery, and stewardship of business-critical data infrastructure that powers our Capitalization program and Business Operations functions . This role is at the intersection of finance, strategy, and engineering , where data precision and operational rigor directly support the company s financial integrity and execution effectiveness.\nYou will collaborate with stakeholders across Finance, BizOps, and Technology to build scalable data solutions that ensure capitalization accuracy, enable deep operational analytics, and streamline financial and business reporting at scale.\nWhat you will do:\nDesign, build, and maintain high-scale data pipelines and transformation logic to support CapEx/OpEx classification, capitalization tracking, and operational data modeling.\nDeliver clean, well-documented, governed datasets that drive finance reporting, strategic planning, and key operational dashboards.\nPartner with cross-functional teams (Finance, Engineering, Strategy) to translate business and compliance requirements into technical solutions.\nLead the development of data models and ETL processes to support performance monitoring, workforce utilization, project financials, and business KPIs.\nEstablish and enforce data quality, lineage, and access control standards to ensure trust in business-critical data.\nProactively identify and resolve data reliability issues related to financial close processes, budget tracking, and capitalization rules.\nServe as a technical advisor to BizOps and Finance stakeholders, recommending improvements in tooling, architecture, and process automation.\nMentor other engineers and contribute to the growth of a high-performance data team culture.\nWho you are:\n6+ years of experience in data engineering , analytics engineering , or data infrastructure roles with a focus on operational and financial data.\nExpertise in SQL and Python , and experience with data pipeline orchestration tools such as Airflow , dbt , or equivalent.\nStrong understanding of cloud-based data platforms (e.g., Snowflake, BigQuery, Redshift, or Databricks).\nDeep familiarity with capitalization standards , CapEx/OpEx distinction, and operational reporting in a tech-driven environment.\nDemonstrated ability to build scalable, reliable ETL/ELT workflows that serve diverse analytical and reporting needs.\nExperience working cross-functionally in complex organizations with multiple stakeholder groups.\nPassion for operational excellence, data governance, and driving actionable business insights from data.\nPreferred qualifications:\n- Experience supporting BizOps , FP&A , or Product Finance teams with data tooling and reporting.\n- Familiarity with BI platforms like Looker , Power BI , or Tableau .\n- Exposure to agile delivery frameworks and enterprise-level operational rhythms.\nAccommodation requests\nIf you need assistance with any part of the application or recruiting process due to a disability, or other physical or mental health conditions, please reach out to our Recruiting Accommodations Team through the Accommodation Request .\nWe are proud to be named as a Best Place to Work on Glassdoor in 2024 and be recognized for award-winning culture by organizations like Forbes, TIME, Disability:IN, and others.\nExpedia Groups family of brands includes: Brand Expedia , Hotels.com , Expedia Partner Solutions, Vrbo , trivago , Orbitz , Travelocity , Hotwire , Wotif , ebookers , CheapTickets , Expedia Group Media Solutions, Expedia Local Expert , CarRentals.com , and Expedia Cruises . 2024 Expedia, Inc. All rights reserved. Trademarks and logos are the property of their respective owners. . Never provide sensitive, personal information to someone unless you re confident who the recipient is. Expedia Group does not extend job offers via email or any other messaging tools to individuals with whom we have not made prior contact. Our email domain is @expediagroup.com. The official website to find and apply for job openings at Expedia Group is careers.expediagroup.com/jobs .\nExpedia is committed to creating an inclusive work environment with a diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, gender, sexual orientation, national origin, disability or age.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Operational excellence', 'Data modeling', 'Talent acquisition', 'Analytical', 'Strategic planning', 'Data quality', 'Operations', 'Analytics', 'SQL', 'Business operations']",2025-06-12 13:43:51
Senior Data Engineer,Wavicle Data Solutions,6 - 11 years,15-25 Lacs P.A.,"['Chennai', 'Coimbatore', 'Bengaluru']","Hi Professionals,\n\nWe are looking for Senior Data Engineer for Permanent Role\n\nWork Location: Hybrid Chennai, Coimbatore or Bangalore\n\nExperience: 6 to 12 Years\n\nNotice Period: 0 TO 15 Days or Immediate Joiner.\n\nSkills:\n1. Python\n2. Pyspark\n3. SQL\n4. AWS\n5. GCP\n6. MLOps\n\nInterested can send your resume to gowtham.veerasamy@wavicledata.com.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'GCP', 'AWS', 'Ml']",2025-06-12 13:43:53
Cloud Data Engineer - GCP,Synechron,2 - 3 years,Not Disclosed,"['Hyderabad', 'Gachibowli']","Job Summary\nSynechron is seeking a highly motivated and skilled Senior Cloud Data Engineer GCP to join our cloud solutions team. In this role, you will collaborate closely with clients and internal stakeholders to design, implement, and manage scalable, secure, and high-performance cloud-based data solutions on Google Cloud Platform (GCP). You will leverage your technical expertise to ensure the integrity, security, and efficiency of cloud data architectures, enabling the organization to derive maximum value from cloud data assets. This role contributes directly to our mission of delivering innovative digital transformation solutions and supports the organizations strategic objectives of scalable and sustainable cloud infrastructure.\nSoftware Requirements\nRequired Skills:\nProficiency with Google Cloud Platform (GCP) services (Compute Engine, Cloud Storage, BigQuery, Cloud Pub/Sub, Dataflow, etc.)\nBasic scripting skills with Python, Bash, or similar languages\nFamiliarity with virtualization and cloud networking concepts\nUnderstanding of cloud security best practices and compliance standards\nExperience with infrastructure as code tools (e.g., Terraform, Deployment Manager)\nStrong knowledge of data management, data pipelines, and ETL processes\nPreferred Skills:\nExperience with other cloud platforms (AWS, Azure)\nKnowledge of SQL and NoSQL databases\nFamiliarity with containerization (Docker, GKE)\nExperience with data visualization tools\nOverall Responsibilities\nDesign, implement, and operate cloud data solutions that are secure, scalable, and optimized for performance\nCollaborate with clients and internal teams to identify infrastructure and data architecture requirements\nManage and monitor cloud infrastructure and ensure operational reliability\nResolve technical issues related to cloud data workflows and storage solutions\nParticipate in project planning, timelines, and technical documentation\nContribute to best practices and continuous improvement initiatives within the organization\nEducate and support clients in adopting cloud data services and best practices\nTechnical Skills (By Category)\nProgramming Languages:\nEssential: Python, Bash scripts\nPreferred: SQL, Java, or other data processing languages\nDatabases & Data Management:\nEssential: BigQuery, Cloud SQL, Cloud Spanner, Cloud Storage\nPreferred: NoSQL databases like Firestore, MongoDB\nCloud Technologies:\nEssential: Google Cloud Platform core services (Compute, Storage, BigQuery, Dataflow, Pub/Sub)\nPreferred: Cloud monitoring, logging, and security tools\nFrameworks & Libraries:\nEssential: Data pipeline frameworks, Cloud SDKs, APIs\nPreferred: Apache Beam, Data Studio\nDevelopment Tools & Methodologies:\nEssential: Infrastructure as Code (Terraform, Deployment Manager)\nPreferred: CI/CD tools (Jenkins, Cloud Build)\nSecurity Protocols:\nEssential: IAM policies, data encryption, network security best practices\nPreferred: Compliance frameworks such as GDPR, HIPAA\nExperience Requirements\n2-3 years of experience in cloud data engineering, cloud infrastructure, or related roles\nHands-on experience with GCP is preferred; experience with AWS or Azure is a plus\nBackground in designing and managing cloud data pipelines, storage, and security solutions\nProven ability to deliver scalable data solutions in cloud environments\nExperience working with cross-functional teams on cloud deployments\nAlternative experience pathways: academic projects, certifications, or relevant internships demonstrating cloud data skills\nDay-to-Day Activities\nDevelop and deploy cloud data pipelines, databases, and analytics solutions\nCollaborate with clients and team members to plan and implement infrastructure architecture\nPerform routine monitoring, maintenance, and performance tuning of cloud data systems\nTroubleshoot technical issues affecting data workflows and resolve performance bottlenecks\nDocument system configurations, processes, and best practices\nEngage in continuous learning on new cloud features and data management tools\nParticipate in project meetings, code reviews, and knowledge sharing sessions\nQualifications\nBachelors or Masters degree in computer science, engineering, information technology, or a related field\nRelevant certifications (e.g., Google Cloud Professional Data Engineer, Cloud Architect) are preferred\nTraining in cloud security, data management, or infrastructure design is advantageous\nCommitment to professional development and staying updated with emerging cloud technologies\nProfessional Competencies\nCritical thinking and problem-solving skills to resolve complex cloud architecture challenges\nAbility to work collaboratively with multidisciplinary teams and clients\nStrong communication skills for technical documentation and stakeholder engagement\nAdaptability to evolving cloud technologies and project priorities\nOrganized with a focus on quality and detail-oriented delivery\nProactive learner with a passion for innovation in cloud data solutions\nAbility to manage multiple tasks effectively and prioritize in a fast-paced environment",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GCP', 'Jenkins', 'Java', 'NoSQL', 'Bash scripts', 'Data Studio', 'Data Management', 'CI/CD', 'Apache Beam', 'MongoDB', 'Cloud Build']",2025-06-12 13:43:55
Senior Data Engineer,SPAN.IO,5 - 10 years,Not Disclosed,['Bengaluru( Indira Nagar )'],"Senior Data Engineer\n\nOur Mission\n\nSPAN is enabling electrification for all\nWe are a mission-driven company designing, building, and deploying products that electrify the built environment, reduce carbon emissions, and slow the effects of climate change.\nDecarbonization is the process to reduce or remove greenhouse gas emissions, especially carbon dioxide, from entering our atmosphere.\nElectrification is the process of replacing fossil fuel appliances that run on gas or oil with all-electric upgrades for a cleaner way to power our lives.\n\nAt SPAN, we believe in:\nEnabling homes and vehicles powered by clean energy\nMaking electrification upgrades possible\nBuilding more resilient homes with reliable backup\nDesigning a flexible and distributed electrical grid\n\nThe Role\nAs a Data Engineer you would be working to design, build, test and create infrastructure necessary for real time analytics and batch analytics pipelines. You will work with multiple teams within the org to provide analysis, insights on the data. You will also be involved in writing ETL processes that support data ingestion. You will also guide and enforce best practices for data management, governance and security. You will build infrastructure to monitor these data pipelines / ETL jobs / tasks and create tooling/infrastructure for providing visibility into these.\n\nResponsibilities\nWe are looking for a Data Engineer with passion for building data pipelines, working with product, data science and business intelligence teams and delivering great solutions. As a part of the team you:-\nAcquire deep business understanding on how SPAN data flows from IoT device to cloud through the system and build scalable and optimized data solutions that impact many stakeholders.\nBe an advocate for data quality and excellence of our platform.\nBuild tools that help streamline the management and operation of our data ecosystem.\nEnsure best practices and standards in our data ecosystem are shared across teams.\nWork with teams within the company to build close relationships with our partners to understand the value our platform can bring and how we can make it better.\nImprove data discovery by creating data exploration processes and promoting adoption of data sources across the company.\nHave a desire to write tools and applications to automate work rather than do everything by hand.\nAssist internal teams in building out data logging, alerting and monitoring for their applications\nAre passionate about CI/CD process.\nDesign, develop and establish KPIs to monitor analysis and provide strategic insights to drive growth and performance.\n\nAbout You\n\nRequired Qualifications\nBachelor's Degree in a quantitative discipline: computer science, statistics, operations research, informatics, engineering, applied mathematics, economics, etc.\n5+ years of relevant work experience in data engineering, business intelligence, research or related fields.\nExpert level production-grade, programming experience in at least one of these languages (Python, Kotlin, or other JVM based languages)\nExperience in writing clean, concise and well structured code in one of the above languages.\nExperience working with Infrastructure-as-code tools: Pulumi, Terraform, etc.\nExperience working with CI/CD systems: Circle-CI, Github Actions, Argo-CD, etc.\nExperience managing data engineering infrastructure through Docker and Kubernetes\nExperience working with latency data processing solutions like Flink, Prefect, AWS Kinesis, Kafka, Spark Stream processing etc.\nExperience with SQL/Relational databases, OLAP databases like Snowflake.\nExperience working in AWS: S3, Glue, Athena, MSK, EMR, ECR etc.\n\nBonus Qualifications\nExperience with the Energy industry\nExperience with building IoT and/or hardware products\nUnderstanding of electrical systems and residential loads\nExperience with data visualization using Tableau.\nExperience in Data loading tools like FiveTran as well as data debugging tools such as DataDog\n\nLife at SPAN\nOur Bengaluru team plays a pivotal role in SPANs continued growth and expansion. Together, were driving engineering, product development, and operational excellence to shape the future of home energy solutions.\nAs part of our team in India, youll have the opportunity to collaborate closely with our teams in the US and across the globe. This international collaboration fosters innovation, learning, and growth, while helping us achieve our bold mission of electrifying homes and advancing clean energy solutions worldwide.\nOur in-office culture offers the chance for dynamic interactions and hands-on teamwork, making SPAN a truly collaborative environment where every team members contribution matters.\nOur climate-focused culture is driven by a team of forward-thinkers, engineers, and problem-solvers who push boundaries every day.\nDo mission-driven work: Every role at SPAN directly advances clean energy adoption.\nBring powerful ideas to life: We encourage diverse ideas and perspectives to drive stronger products.\nNurture an innovation-first mindset: We encourage big thinking and bold action.\nDeliver exceptional customer value: We value hard work, and the ability to deliver exceptional customer value.\n\nBenefits at SPAN India\nGenerous paid leave\nComprehensive Insurance & Health Benefits\nCentrally located office in Bengaluru with easy access to public transit, dining, and city amenities\n\nInterested in joining our team? Apply today and well be in touch with the next steps!",Industry Type: Electronics Manufacturing,"Department: Production, Manufacturing & Engineering","Employment Type: Full Time, Permanent","['Terraform', 'Snowflake', 'AWS', 'Python', 'SQL', 'Java', 'Apache Flink', 'Kotlin']",2025-06-12 13:43:57
Sr Data Engineer,Lowes Services India Private limited,5 - 10 years,Not Disclosed,['Bengaluru'],"We are seeking a seasoned Senior Data Engineer to join our Marketing Data Platform team. This role is pivotal in designing, building, and optimizing scalable data pipelines and infrastructure that support our marketing analytics and customer engagement strategies. The ideal candidate will have extensive experience with big data technologies, cloud platforms, and a strong understanding of marketing data dynamics.\n\nData Pipeline Development & Optimization\nDesign, develop, and maintain robust ETL/ELT pipelines using Apache PySpark on GCP services like Dataproc and Cloud Composer.\nEnsure data pipelines are scalable, efficient, and reliable to handle large volumes of marketing data.\nData Warehousing & Modeling\nImplement and manage data warehousing solutions using BigQuery, ensuring optimal performance and cost-efficiency.\nDevelop and maintain data models that support marketing analytics and reporting needs.\nCollaboration & Stakeholder Engagement\nWork closely with marketing analysts, data scientists, and cross-functional teams to understand data requirements and deliver solutions that drive business insights.\nTranslate complex business requirements into technical specifications and data architecture.\nData Quality & Governance\nImplement data quality checks and monitoring to ensure the accuracy and integrity of marketing data.\nAdhere to data governance policies and ensure compliance with data privacy regulations.\nContinuous Improvement & Innovation\nStay abreast of emerging technologies and industry trends in data engineering and marketing analytics.\nPropose and implement improvements to existing data processes and infrastructure\n  Years of Experience\n5 Years in Data Engineer space\n  Education Qualification & Certifications\nB.Tech or MCA\n  Experience\nProven experience with Apache PySpark, GCP (including Dataproc, BigQuery, Cloud Composer), and data pipeline orchestration.\nTechnical Skills\nProficiency in SQL and Python.\nExperience with data modeling, ETL/ELT processes, and data warehousing concepts.",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['orchestration', 'Data modeling', 'data governance', 'Data quality', 'Apache', 'Continuous improvement', 'Monitoring', 'SQL', 'Python', 'Data architecture']",2025-06-12 13:43:59
Lead AWS Glue Data Engineer,Allegis Group,8 - 13 years,Not Disclosed,[],"Lead AWS Glue Data Engineer\nJob Location : Hyderabad / Bangalore / Chennai / Noida/ Gurgaon / Pune / Indore / Mumbai/ Kolkata\n\nWe are seeking a skilled Lead AWS Data Engineer with 8+ years of strong programming and SQL skills to join our team. The ideal candidate will have hands-on experience with AWS Data Analytics services and a basic understanding of general AWS services. Additionally, prior experience with Oracle and Postgres databases and secondary skills in Python and Azure DevOps will be an advantage.\n\nKey Responsibilities:\nDesign, develop, and optimize data pipelines using AWS Data Analytics services such as RDS, DMS, Glue, Lambda, Redshift, and Athena.\nImplement data migration and transformation processes using AWS DMS and Glue.\nWork with SQL (Oracle & Postgres) to query, manipulate, and analyse large datasets.\nDevelop and maintain ETL/ELT workflows for data ingestion and transformation.\nUtilize AWS services like S3, IAM, CloudWatch, and VPC to ensure secure and efficient data operations.\nWrite clean and efficient Python scripts for automation and data processing.\nCollaborate with DevOps teams using Azure DevOps for CI/CD pipelines and infrastructure management.\nMonitor and troubleshoot data workflows to ensure high availability and performance.\n\nPreferred Qualifications:\nAWS certifications in Data Analytics, Solutions Architect, or DevOps.\nExperience with data warehousing concepts and data lake implementations.\nHands-on experience with Infrastructure as Code (IaC) tools like Terraform or CloudFormation.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['RDS', 'Glue', 'DMS', 'Lambda', 'Redshift', 'Athena']",2025-06-12 13:44:02
Azure Data Engineer,HTC Global Services,4 - 8 years,Not Disclosed,['Bengaluru( Murugeshpalya )'],"Job Summary:\nWe are looking for a highly skilled Azure Data Engineer with experience in building and managing scalable data pipelines using Azure Data Factory, Synapse, and Databricks. The ideal candidate should be proficient in big data tools and Azure services, with strong programming knowledge and a solid understanding of data architecture and cloud platforms.\n\nKey Responsibilities:",,,,"['Power Bi', 'Azure Databricks', 'Azure Data Factory', 'Synapse', 'Python', 'Java', 'Scala', 'Kafka', 'big data tools', 'SQL', 'EventHub', 'Azure cloud services', 'Spark']",2025-06-12 13:44:04
Azure Cloud Data Engineering Consultant,Optum,7 - 10 years,17-27.5 Lacs P.A.,['Gurugram'],"Primary Responsibilities:\nDesign and develop applications and services running on Azure, with a strong emphasis on Azure Databricks, ensuring optimal performance, scalability, and security.\nBuild and maintain data pipelines using Azure Databricks and other Azure data integration tools.\nWrite, read, and debug Spark, Scala, and Python code to process and analyze large datasets.\nWrite extensive query in SQL and Snowflake\nImplement security and access control measures and regularly audit Azure platform and infrastructure to ensure compliance.\nCreate, understand, and validate design and estimated effort for given module/task, and be able to justify it.\nPossess solid troubleshooting skills and perform troubleshooting of issues in different technologies and environments.\nImplement and adhere to best engineering practices like design, unit testing, functional testing automation, continuous integration, and delivery.\nMaintain code quality by writing clean, maintainable, and testable code.\nMonitor performance and optimize resources to ensure cost-effectiveness and high availability.\nDefine and document best practices and strategies regarding application deployment and infrastructure maintenance.\nProvide technical support and consultation for infrastructure questions.\nHelp develop, manage, and monitor continuous integration and delivery systems.\nTake accountability and ownership of features and teamwork.\nComply with the terms and conditions of the employment contract, company policies and procedures, and any directives.\nRequired Qualifications:\nB.Tech/MCA (Minimum 16 years of formal education)\nOverall 7+ years of experience.\nMinimum of 3 years of experience in Azure (ADF), Databricks and DevOps.\n5 years of experience in writing advanced level SQL.\n2-3 years of experience in writing, reading, and debugging Spark, Scala, and Python code.\n3 or more years of experience in architecting, designing, developing, and implementing cloud solutions on Azure.\nProficiency in programming languages and scripting tools.\nUnderstanding of cloud data storage and database technologies such as SQL and NoSQL.\nProven ability to collaborate with multidisciplinary teams of business analysts, developers, data scientists, and subject-matter experts.\nFamiliarity with DevOps practices and tools, such as continuous integration and continuous deployment (CI/CD) and Teraform.\nProven proactive approach to spotting problems, areas for improvement, and performance bottlenecks.\nProven excellent communication, writing, and presentation skills.\nExperience in interacting with international customers to gather requirements and convert them into solutions using relevant skills.\nPreferred Qualifications:\nKnowledge of AI/ML or LLM (GenAI).\nKnowledge of US Healthcare domain and experience with healthcare data.\nExperience and skills with Snowflake.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Databricks', 'ETL', 'SQL', 'Python', 'Airflow', 'Pyspark', 'Snowflake', 'SCALA', 'Spark', 'Data Bricks']",2025-06-12 13:44:06
Senior GCP Data Engineer,Swits Digital,6 - 9 years,Not Disclosed,['Bengaluru'],"Job Title: Senior GCP Data Engineer\nLocation: Chennai, Bangalore, Hyderabad\nExperience: 6-9 Years\nJob Summary:\nWe are seeking a GCP Data & Cloud Engineer with strong expertise in Google Cloud Platform services, including BigQuery, Cloud Run, Cloud Storage , and Pub/Sub . The ideal candidate will have deep experience in SQL coding , data pipeline development, and deploying cloud-native solutions.\nKey Responsibilities:\nDesign, implement, and optimize scalable data pipelines and services using GCP\nBuild and manage cloud-native applications deployed via Cloud Run\nDevelop complex and performance-optimized SQL queries for analytics and data transformation\nManage and automate data storage, retrieval, and archival using Cloud Storage\nImplement event-driven architectures using Google Pub/Sub\nWork with large datasets in BigQuery , including ETL/ELT design and query optimization\nEnsure security, monitoring, and compliance of cloud-based systems\nCollaborate with data analysts, engineers, and product teams to deliver end-to-end cloud solutions\nRequired Skills & Experience:\n3+ years of experience working with Google Cloud Platform (GCP)\nStrong proficiency in SQL coding , query tuning, and handling complex data transformations\nHands-on experience with:\nBigQuery\nCloud Run\nCloud Storage\nPub/Sub\nUnderstanding of data pipeline and ETL/ELT workflows in cloud environments\nFamiliarity with containerized services and CI/CD pipelines\nExperience in scripting languages (e.g., Python, Shell) is a plus\nStrong analytical and problem-solving skills",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SUB', 'query optimization', 'GCP', 'Analytical', 'Cloud', 'query', 'cloud storage', 'Security monitoring', 'SQL coding', 'Python']",2025-06-12 13:44:09
Lead Data Engineer,Conduent,8 - 13 years,Not Disclosed,['Noida'],"Job Overview \n\nWe are looking for a Data Engineer who will be part of our Analytics Practice and will be expected to actively work in a multi-disciplinary fast paced environment. This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project; its primary responsibility is the acquisition, transformation, loading and processing of data from a multitude of disparate data sources, including structured and unstructured data for advanced analytics and machine learning in a big data environment.\n\n\n Responsibilities: \nEngineer a modern data pipeline to collect, organize, and process data from disparate sources.\nPerforms data management tasks, such as conduct data profiling, assess data quality, and write SQL queries to extract and integrate data\nDevelop efficient data collection systems and sound strategies for getting quality data from different sources\nConsume and analyze data from the data pool to support inference, prediction and recommendation of actionable insights to support business growth.\nDesign and develop ETL processes using tools and scripting. Troubleshoot and debug ETL processes. Performance tuning and opitimization of the ETL processes.\nProvide support to new of existing applications while recommending best practices and leading projects to implement new functionality.\nCollaborate in design reviews and code reviews to ensure standards are met. Recommend new standards for visualizations.\nLearn and develop new ETL techniques as required to keep up with the contemporary technologies.\nReviews the solution requirements and architecture to ensure selection of appropriate technology, efficient use of resources and integration of multiple systems and technology.\nSupport presentations to Customers and Partners\nAdvising on new technology trends and possible adoption to maintain competitive advantage\n\n\n Experience Needed: \n8+ years of related experience is required.\nA BS or Masters degree in Computer Science or related technical discipline is required\nETL experience with data integration to support data marts, extracts and reporting\nExperience connecting to varied data sources\nExcellent SQL coding experience with performance optimization for data queries.\nUnderstands different data models like normalized, de-normalied, stars, and snowflake models. Worked with transactional, temporarl, time series, and structured and unstructured data.\nExperience on Azure Data Factory and Azure Synapse Analytics\nWorked in big data environments, cloud data stores, different RDBMS and OLAP solutions.\nExperience in cloud-based ETL development processes.\nExperience in deployment and maintenance of ETL Jobs.\nIs familiar with the principles and practices involved in development and maintenance of software solutions and architectures and in service delivery.\nHas strong technical background and remains evergreen with technology and industry developments.\nAt least 3 years of demonstrated success in software engineering, release engineering, and/or configuration management.\nHighly skilled in scripting languages like PowerShell.\nSubstantial experience in the implementation and exectuion fo CI/CD processes.\n\n\n Additional  \nDemonstrated ability to have successfully completed multiple, complex technical projects\nPrior experience with application delivery using an Onshore/Offshore model\nExperience with business processes across multiple Master data domains in a services based company\nDemonstrates a rational and organized approach to the tasks undertaken and an awareness of the need to achieve quality.\nDemonstrates high standards of professional behavior in dealings with clients, colleagues and staff.\nIs able to make sound and far reaching decisions alone on major issues and to take full responsibility for them on a technical basis.\nStrong written communication skills. Is effective and persuasive in both written and oral communication.\nExperience with gathering end user requirements and writing technical documentation\nTime management and multitasking skills to effectively meet deadlines under time-to-market pressure",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql coding', 'sql', 'configuration management', 'software engineering', 'release engineering', 'continuous integration', 'rdbms', 'sql queries', 'performance tuning', 'azure synapse', 'ci/cd', 'azure data factory', 'machine learning', 'data engineering', 'powershell', 'olap', 'etl', 'big data']",2025-06-12 13:44:11
Senior Data Engineer,The Main Stage Productions,4 - 6 years,Not Disclosed,['Bengaluru'],"Design and implement cloud-native data architectures on AWS, including data lakes, data warehouses, and streaming pipelines using services like S3, Glue, Redshift, Athena, EMR, Lake Formation, and Kinesis.\nDevelop and orchestrate ETL/ELT pipelines\n\nRequired Candidate profile\nParticipate in pre-sales and consulting activities such as:\nEngaging with clients to gather requirements and propose AWS-based data engineering solutions.\nSupporting RFPs/RFIs, technical proposals",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AWS Glue', 'GitHub Actions', 'PySpark', 'Scala', 'CodePipeline', 'Step Functions', 'data engineering']",2025-06-12 13:44:13
Collibra Data Governance Engineer,Allegis Group,6 - 11 years,Not Disclosed,[],"Collibra Data Governance Engineer\nJob Location : Hyderabad / Bangalore / Chennai / Noida/ Gurgaon / Pune / Indore / Mumbai/ Kolkata\nRequired Skills\n5+ years of experience in data governance and/or metadata management.\nHands-on experience with Collibra Data Governance Center (Collibra DGC), including workflow configuration, cataloging, and operating model customization.\nStrong knowledge of metadata management, data lineage, and data quality principles.\nHands-on experience with Snowflake\nFamiliarity with data integration tools and AWS cloud platform\nExperience with SQL and working knowledge of relational databases.\nUnderstanding of data privacy regulations (e.g., GDPR, CCPA) and compliance frameworks.\nPreferred Skills\nCertifications such as Collibra Certified Solution Architect.\nExperience integrating Collibra with tools like Snowflake, Tableau or other BI/analytics platforms.\nExposure to DataOps, MDM (Master Data Management), and data governance frameworks like DAMA-DMBOK.\nStrong communication and stakeholder management skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Collibra', 'Metadata', 'Data Governance']",2025-06-12 13:44:15
Senior Software Engineer - Data Engineering,Zendesk,2 - 6 years,Not Disclosed,['Bengaluru'],"Design, build, and maintain data quality systems and pipelines.\nWork with tools such as Snowflake, Docker/Kubernetes, and Kafka to enable scalable, observable data movement.\nCollaborate cross-functionally to close skill gaps in DQ and data platform tooling.\nContribute to building internal tooling that supports schema validation, data experimentation, and automated checks.\nCollaborate cross-functionally with data producers, analytics engineers, platform teams, and business stakeholders.\nOwn the reliability, scalability, and performance of ingestion systems deployed on AWS\nArchitect and build core components of our real-time ingestion platform using Kafka, Snowpipe Streaming.\nChampion software engineering excellence including testing, observability, CI/CD, and automation\nDrive the development of platform tools that ensure data quality, observability, and lineage through Protobuf-based schema management..\nParticipate in the implementation of ingestion best practices and reusable frameworks across data and software engineering teams.\nCore Skills:\nSolid programming experience (preferably in Java )\nExperience with distributed data systems ( Kafka, Snowflake )\nFamiliarity with Data Quality tooling and concepts\nGood working knowledge of SQL (especially for diagnostics and DQ workflows)\nExperience with containerization (Docker, Kubernetes )\nStrong debugging, observability, and pipeline reliability practices\n\nWhat You Bring:\nA systems mindset with strong software engineering fundamentals.\nPassion for building resilient, high-throughput, real-time platforms.\nAbility to influence technical direction across teams and drive alignment.\nStrong communication and mentoring skills.\nA bias toward automation, continuous improvement, and platform thinking.\n\nNice to Haves:\nExperience with GenAI tools or supporting ML/AI data workflows\nFamiliarity with cloud-native data platforms (e.g., AWS, GCP)\nExposure to dbt or ELT frameworks",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'GCP', 'Quality systems', 'Debugging', 'Data quality', 'Customer service', 'Continuous improvement', 'Monitoring', 'Analytics', 'SQL']",2025-06-12 13:44:17
Senior Data Engineer,Talentien Global Solutions,4 - 8 years,12-18 Lacs P.A.,"['Hyderabad', 'Chennai', 'Coimbatore']","We are seeking a skilled and motivated Data Engineer to join our dynamic team. The ideal candidate will have experience in designing, developing, and maintaining scalable data pipelines and architectures using Hadoop, PySpark, ETL processes, and Cloud technologies.\n\nResponsibilities:\nDesign, develop, and maintain data pipelines for processing large-scale datasets.\nBuild efficient ETL workflows to transform and integrate data from multiple sources.\nDevelop and optimize Hadoop and PySpark applications for data processing.\nEnsure data quality, governance, and security standards are met across systems.\nImplement and manage Cloud-based data solutions (AWS, Azure, or GCP).\nCollaborate with data scientists and analysts to support business intelligence initiatives.\nTroubleshoot performance issues and optimize query executions in big data environments.\nStay updated with industry trends and advancements in big data and cloud technologies.\nRequired Skills:\nStrong programming skills in Python, Scala, or Java.\nHands-on experience with Hadoop ecosystem (HDFS, Hive, Spark, etc.).\nExpertise in PySpark for distributed data processing.\nProficiency in ETL tools and workflows (SSIS, Apache Nifi, or custom pipelines).\nExperience with Cloud platforms (AWS, Azure, GCP) and their data-related services.\nKnowledge of SQL and NoSQL databases.\nFamiliarity with data warehousing concepts and data modeling techniques.\nStrong analytical and problem-solving skills.\n\nInterested can reach us at +91 7305206696/ saranyadevib@talentien.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Hadoop', 'Spark', 'ETL', 'Airflow', 'Etl Pipelines', 'Big Data', 'EMR', 'Gcp Cloud', 'Data Bricks', 'Azure Cloud', 'Data Pipeline', 'SCALA', 'Snowflake', 'Data Lake', 'Data Warehousing', 'Data Modeling', 'AWS', 'Python']",2025-06-12 13:44:20
Lead Data Engineer - Azure,Blend360 India,7 - 12 years,Not Disclosed,['Hyderabad'],"Job Description\nAs a Sr Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\nQualifications\n7+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar fiel",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-12 13:44:22
"Lead Engineer, Data Engineering (J2EE/Angular/React/React Full Stack)",S&P Global Market Intelligence,10 - 15 years,Not Disclosed,"['Mumbai', 'Maharastra']","About the Role:\nGrade Level (for internal use): 11\nThe Team\nYou will be an expert contributor and part of the Rating Organizations Data Services Product Engineering Team. This team, who has a broad and expert knowledge on Ratings organizations critical data domains, technology stacks and architectural patterns, fosters knowledge sharing and collaboration that results in a unified strategy. All Data Services team members provide leadership, innovation, timely delivery, and the ability to articulate business value. Be a part of a unique opportunity to build and evolve S&P Ratings next gen analytics platform.\nResponsibilities:\nArchitect, design, and implement innovative software solutions to enhance S&P Ratings' cloud-based analytics platform.\nMentor a team of engineers (as required), fostering a culture of trust, continuous growth, and collaborative problem-solving.\nCollaborate with business partners to understand requirements, ensuring technical solutions align with business goals.\nManage and improve existing software solutions, ensuring high performance and scalability.\nParticipate actively in all Agile scrum ceremonies, contributing to the continuous improvement of team processes.\nProduce comprehensive technical design documents and conduct technical walkthroughs.\nExperience & Qualifications:\nBachelors degree in computer science, Information Systems, Engineering, equivalent or more is required\nProficient with software development lifecycle (SDLC) methodologies like Agile, Test-driven development\n10+ years of experience with 4+ years designing/developing enterprise products, modern tech stacks and data platforms\n4+ years of hands-on experience contributing to application architecture & designs, proven software/enterprise integration design patterns and full-stack knowledge including modern distributed front end and back-end technology stacks\n5+ years full stack development experience in modern web development technologies, Java/J2EE, UI frameworks like Angular, React, SQL, Oracle, NoSQL Databases like MongoDB\nExperience designing transactional/data warehouse/data lake and data integrations with Big data eco system leveraging AWS cloud technologies\nThorough understanding of distributed computing\nPassionate, smart, and articulate developer\nQuality first mindset with a strong background and experience with developing products for a global audience at scale\nExcellent analytical thinking, interpersonal, oral and written communication skills with strong ability to influence both IT and business partners\nSuperior knowledge of system architecture, object-oriented design, and design patterns.\nGood work ethic, self-starter, and results-oriented\nExcellent communication skills are essential, with strong verbal and writing proficiencies\nExp. with Delta Lake systems like Databricks using AWS cloud technologies and PySpark is a plus\nAdditional Preferred Qualifications:\nExperience working AWS\nExperience with SAFe Agile Framework\nBachelor's/PG degree in Computer Science, Information Systems or equivalent.\nHands-on experience contributing to application architecture & designs, proven software/enterprise integration design principles\nAbility to prioritize and manage work to critical project timelines in a fast-paced environment\nExcellent Analytical and communication skills are essential, with strong verbal and writing proficiencies\nAbility to train and mentor",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'AWS cloud technologies', 'PySpark', 'J2EE', 'React Full Stack', 'Databricks', 'React', 'Angular']",2025-06-12 13:44:24
"Senior Data Engineer - Airflow, PLSQL",Relanto Global,5 - 10 years,Not Disclosed,['Bengaluru'],"PositionSenior Data Engineer - Airflow, PLSQL \n\n Experience5+ Years \n\n LocationBangalore/Hyderabad/Pune \n\n\n\nSeeking a Senior Data Engineer with strong expertise in Apache Airflow and Oracle PL/SQL, along with working experience in Snowflake and Agile methodologies. The ideal candidate will also take up Scrum Master responsibilities and lead a data engineering scrum team to deliver robust, scalable data solutions.\n\n\n Key Responsibilities: \nDesign, develop, and maintain scalable data pipelines using Apache Airflow.\nWrite and optimize complex PL/SQL queries, procedures, and packages on Oracle databases.\nCollaborate with cross-functional teams to design efficient data models and integration workflows.\nWork with Snowflake for data warehousing and analytics use cases.\nOwn the delivery of sprint goals, backlog grooming, and facilitation of agile ceremonies as the Scrum Master.\nMonitor pipeline health and troubleshoot production data issues proactively.\nEnsure code quality, documentation, and best practices across the team.\nMentor junior data engineers and promote a culture of continuous improvement.\n\n\n Required Skills and Qualifications: \n5+ years of experience as a Data Engineer in enterprise environments.\nStrong expertise in  Apache Airflow  for orchestrating workflows.\nExpert in  Oracle PL/SQL  - stored procedures, performance tuning, debugging.\nHands-on experience with  Snowflake  - data modeling, SQL, optimization.\nWorking knowledge of version control (Git) and CI/CD practices.\nPrior experience or certification as a  Scrum Master  is highly desirable.\nStrong analytical and problem-solving skills with attention to detail.\nExcellent communication and leadership skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'plsql', 'stored procedures', 'oracle pl', 'performance tuning', 'hive', 'continuous integration', 'ci/cd', 'data warehousing', 'pyspark', 'git', 'apache', 'data modeling', 'spark', 'debugging', 'hadoop', 'big data', 'snowflake', 'python', 'oracle', 'sql queries', 'airflow', 'data engineering', 'agile', 'sqoop']",2025-06-12 13:44:27
"Senior Data Engineer Databricks, ADF, PySpark",Suzva Software Technologies,6 - 11 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","Senior Data Engineer (Remote, Contract 6 Months) Databricks, ADF, and PySpark.\nWe are hiring a Senior Data Engineer for a 6-month remote contract position. The ideal candidate is highly skilled in building scalable data pipelines and working within the Azure cloud ecosystem, especially Databricks, ADF, and PySpark. You'll work closely with cross-functional teams to deliver enterprise-level data engineering solutions.\n\nKeyResponsibilities\nBuild scalable ETL pipelines and implement robust data solutions in Azure.\n\nManage and orchestrate workflows using ADF, Databricks, ADLS Gen2, and Key Vaults.\n\nDesign and maintain secure and efficient data lake architecture.\n\nWork with stakeholders to gather data requirements and translate them into technical specs.\n\nImplement CI/CD pipelines for seamless data deployment using Azure DevOps.\n\nMonitor data quality, performance bottlenecks, and scalability issues.\n\nWrite clean, organized, reusable PySpark code in an Agile environment.\n\nDocument pipelines, architectures, and best practices for reuse.\n\nMustHaveSkills\nExperience: 6+ years in Data Engineering\n\nTech Stack: SQL, Python, PySpark, Spark, Azure Databricks, ADF, ADLS Gen2, Azure DevOps, Key Vaults\n\nCore Expertise: Data Warehousing, ETL, Data Pipelines, Data Modelling, Data Governance\n\nAgile, SDLC, Containerization (Docker), Clean coding practices\n\nGoodToHaveSkills\nEvent Hubs, Logic Apps\n\nPower BI\n\nStrong logic building and competitive programming background\n\nLocation : - Remote,Mumbai, Delhi / NCR, Bengaluru , Kolkata, Chennai, Hyderabad, Ahmedabad, Pune",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Databricks', 'ADF', 'PySpark', 'ADLS Gen2', 'Azure Databricks', 'Key Vaults', 'Spark', 'Azure DevOps', 'SQL', 'Python']",2025-06-12 13:44:29
Data Engineer IV - Big Data / Spark,Sadup Soft,5 - 7 years,Not Disclosed,['Chennai'],"Must have skills :\n\n- Minimum of 5-7 years of experience in software development, with a focus on Java and infrastructure tools.\n\n- Min 6+ years of experience as a Data Engineer.\n\n- Good Experience in handling Big Data Spark, Hive SQL, BigQuery, SQL.\n\n- Candidate worked on cloud platforms and GCP would be an added advantage.\n\n- Good understanding of Hadoop based ecosystem including hard sequel, HDFS would be very essential.\n\n- Very good professional knowledge of PySpark or using Scala\n\nResponsibilities :\n\n- Collaborate with cross-functional teams such as Data Scientists, Product Partners and Partner Team Developers to identify opportunities for Big Data, Query ( Spark, Hive SQL, BigQuery, SQL ) tuning opportunities that can be solved using machine learning and generative AI.\n\n- Write clean, high-performance, high-quality, maintainable code.\n\n- Design and develop Big Data Engineering Solutions Applications for above ensuring scalability, efficiency, and maintainability of such solutions.\n\nRequirements :\n\n- A Bachelor or Master's degree in Computer Science or a related field.\n\n- Proven experience working as a Big Data & MLOps Engineer, with a focus on Spark, Scala Spark or PySpark, Spark SQL, BigQuery, Python, Google Cloud,.\n\n- Deep understanding and experience in tuning Dataproc, BigQuery, Spark Applications.\n\n- Solid knowledge of software engineering best practices, including version control systems (e.g Git), code reviews, and testing methodologies.\n\n- Strong communication skills to effectively collaborate and present findings to both technical and non-technical stakeholders.\n\n- Proven ability to adapt and learn new technologies and frameworks quickly.\n\n- A proactive mindset with a passion for continuous learning and research.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Data Engineering', 'BigQuery', 'GCP', 'Spark', 'Machine Learning', 'Python', 'SQL']",2025-06-12 13:44:31
"Senior Data Engineer II, Business Intelligence & Reporting",XL India Business Services Pvt. Ltd,3 - 7 years,Not Disclosed,['Gurugram'],"Senior Data Engineer II, Business Intelligence & Reporting Gurgaon, Haryana, India AXA XL recognizes digital, data, and information assets are critical for the business, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained dynamic advantage\n\nOur Data, Intelligence & Analytics function is focused on driving innovation by optimizing how we leverage digital, data, and AI to drive strategy and differentiate ourselves from the competition\n\nAs we develop an enterprise-wide data and digital strategy that moves us toward a greater focus on the use of data and strengthening our digital, AI capabilities, we are seeking a Deputy Manager, BI and Reporting\n\nIn this role, you will support/manage BI & reporting\n\nWhat you ll be DOING Your essential responsibilities include: BI & Reporting Management: Oversee and support Business Intelligence (BI) and Reporting products, ensuring their effectiveness and alignment with organizational goals\n\nStakeholder Engagement: Manage Business as Usual (BAU) activities for BI and Reporting, fostering effective communication and relationships with stakeholders to understand their needs and expectations\n\nModel Integration: Energize and synergize various Business Intelligence models and reporting systems to enhance data insights and reporting capabilities\n\nStrategic Initiative Support: Collaborate with the Data Intelligence and Analytics (DIA) team on various strategic initiatives, enabling the development of BI and Reporting functions and related capabilities\n\nTalent Development: Foster the growth of BI and Reporting talent across AXA XL by promoting an inclusive and diverse environment that encourages the utilization and value creation of our strategic digital, data, and analytics assets\n\nCustomer-Centric Culture: Instill a customer-first mindset within the team, prioritizing exceptional service for business stakeholders and ensuring their needs are met\n\nTeam Development: Contribute to the enhancement of the Business Intelligence teams tools, skills, and culture, driving positive impacts on team performance and outcomes\n\nYou will report to Senior Manager, Business Intelligence & Reporting\n\nWhat you will BRING At AXA XL, we view individuals holistically through their People, Business, and Technical Skills\n\nWe re interested in what you bring, how you think, and your potential for growth\n\nWe value diverse backgrounds and perspectives, recognizing that each person contributes uniquely to our teams success\n\nWe value relevant education and experience in a related field\n\nAdditionally, we encourage candidates with diverse educational backgrounds or equivalent experience to apply\n\nHere are some of the key skills important for the role: People Skills Customer Centricity: Brings a collaborative spirit, a can-do attitude, and a Customer First mindset, ensuring that stakeholder needs are prioritized\n\nCross-Functional Collaboration: Ability to communicate effectively with teams, peers, and stakeholders across the globe, fostering collaboration and understanding\n\nAble to help and guide team members on technical issues, fostering their development and promoting self-directed problem-solving\n\nGrowth Mindset: Passion for digital, data, and AI, along with a commitment to personal and team development in a digital and data-driven organization\n\nResilience: Ability to lead a project or team, demonstrating adaptability and leadership under various circumstances\n\nAnalytical & Strategic Mindset: Ability to analyze data effectively and develop strategic insights that drive decision-making and improve business outcomes\n\nPerformance Excellence: Commitment to delivering high-quality results and continuously improving processes and performance metrics within the team\n\nBUSINESS Skills Business & Insurance Acumen: Ability to showcase relevant industry knowledge supporting multiple specialty areas of Data and Analytics\n\nStakeholder Management: Ability to manage stakeholders effectively, understanding their needs and ensuring clear communication and support\n\nSimplifies Complexity: Ability to distill complex data concepts and analyses into clear, actionable insights for stakeholders\n\nEnsuring that technical information is accessible, enabling informed decision-making and fostering collaboration between technical and non-technical teams\n\nTECHNICAL Skills Data Visualization: Experience with end-user BI tools like Power BI, enabling effective presentation and visualization of data insights\n\nReporting Tools: Proficincy in SQL, Advanced Excel, MS Access, and VBA, allowing for effective data manipulation and reporting\n\nData Analytics: Ability to help and guide team members on technical issues, fostering skill development within the team to self-directedly manage data analytics tasks",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Stakeholder Engagement', 'MS Access', 'Analytical', 'Agile', 'digital strategy', 'Business strategy', 'data visualization', 'Stakeholder management', 'Reporting tools', 'SQL']",2025-06-12 13:44:33
IT Manager - Data Engineering & Analytics,ZS,12 - 15 years,Not Disclosed,['Pune'],"IT MANAGER, DATA ENGINEERING AND ANALYTICS will lead a team of data engineers and analysts responsible for designing, developing, and maintaining robust data systems and integrations. This role is critical for ensuring the smooth collection, transformation, integration and visualization of data, making it easily accessible for analytics and decision-making across the organization. The Manager will collaborate closely with analysts, developers, business leaders and other stakeholders to ensure that the data infrastructure meets business needs and is scalable, reliable, and efficient.\n",,,,"['Data modeling', 'Project management', 'Analytical', 'Financial planning', 'Management consulting', 'Data quality', 'Troubleshooting', 'Stakeholder management', 'Analytics', 'SQL']",2025-06-12 13:44:36
Data Lineage Engineers,Altimetrik,5 - 10 years,15-30 Lacs P.A.,"['Pune', 'Chennai', 'Bengaluru']","Role & responsibilities\nSkill need :  Data Lineage with Ab-initio - Metadata hub\n\nSkills & Experience:\nExpertise in mHub or similar tools, data pipelines, and cloud platforms.\nProficiency in Python, Oracle, SQL, Java, and ETL tools.\n5-10 years of experience in data engineering and governance.",,,,"['Ab-initio', 'Metadata hub', 'Python', 'mhub', 'ETL', 'Oracle', 'SQL']",2025-06-12 13:44:38
Senior Data Engineer (Identity),Kargo,6 - 11 years,Not Disclosed,[],"Success takes all kinds. Diversity describes our workforce. Inclusion defines our culture. We do not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, marital status, age, national origin, protected veteran status, disability or other legally protected status. Individuals with disabilities are provided reasonable accommodation to participate in the job application process, perform essential job functions, and receive other benefits and privileges of employment.\nTitle: Senior Data Engineer\nJob Type: Permanent\n\nJob Location: Remote\nThe Opportunity\nAt Kargo, we are rapidly evolving our data infrastructure and capabilities to address challenges of data scale, new methodologies for onboarding and targeting, and rigorous privacy standards. Were looking for an experienced Senior Data Engineer to join our team, focusing on hands-on implementation, creative problem-solving, and exploring new technical approaches. Youll work collaboratively with our technical leads and peers, actively enhancing and scaling the data processes that drive powerful targeting systems.\nThe Daily To-Do\nIndependently implement, optimize, and maintain robust ETL/ELT pipelines using Python, Airflow, Spark, Iceberg, Snowflake, Aerospike, Docker, Kubernetes (EKS), AWS, and real-time streaming technologies like Kafka and Flink.\nEngage proactively in collaborative design and brainstorming sessions, contributing technical insights and innovative ideas for solving complex data engineering challenges.\nSupport the definition and implementation of robust testing strategies, and guide the team in adopting disciplined CI/CD practices using ArgoCD to enable efficient and reliable deployments.\nMonitor and optimize data systems and infrastructure to ensure operational reliability, performance efficiency, and cost-effectiveness.\nActively contribute to onboarding new datasets, enhancing targeting capabilities, and exploring modern privacy-compliant methodologies.\nMaintain thorough documentation of technical implementations, operational procedures, and best practices for effective knowledge sharing and onboarding.\nQualifications:\nStrong expertise in implementing, maintaining, and optimizing large-scale data systems with minimal oversight.\nDeep proficiency in Python, Spark, and Iceberg, with a clear understanding of data structuring for efficiency and performance.\nExperience with Airflow for building robust data workflows is strongly preferred.\nExtensive DevOps experience, particularly with AWS (including EKS), Docker, Kubernetes, CI/CD automation using ArgoCD, and monitoring via Prometheus.\nFamiliarity with Snowflake, including writing and optimizing SQL queries and understanding Snowflakes performance and cost dynamics.\nComfort with Agile methodologies, including regular use of Jira and Confluence for task management and documentation.\nProven ability to independently drive implementation and problem-solving, turning ambiguity into clearly defined actions.\nExcellent communication skills to effectively engage in discussions with technical teams and stakeholders.\nFamiliarity with identity, privacy, and targeting methodologies in AdTech is required.\nFollow Our Lead\nBig Picture: kargo.com\nThe Latest: Instagram ( @kargomobile ) and LinkedIn ( Kargo )",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL queries', 'Automation', 'CTV', 'spark', 'Agile', 'JIRA', 'Operations', 'Cost', 'AWS', 'Python']",2025-06-12 13:44:40
Sr Data Engineering Manager,Amgen Inc,12 - 15 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nWe are seeking a Senior Data Engineering Manager with a strong background in Regulatory or Integrated Product Teams within the Biotech or Pharmaceutical domain. This role will lead the end-to-end data strategy and execution for regulatory product submissions, lifecycle management, and compliance reporting, ensuring timely and accurate delivery of regulatory data assets across global markets.You will be embedded in a cross-functional Regulatory Integrated Product Team (IPT) and serve as the data and technology lead, driving integration between scientific, regulatory, and engineering functions to support submission-ready data and regulatory intelligence solutions.\nRoles & Responsibilities:\nFunctional Skills:\nLead the engineering strategy and implementation for end-to-end regulatory operations, including data ingestion, transformation, integration, and delivery across regulatory systems.\nServe as the data engineering SME in the Integrated Product Team (IPT) to support regulatory submissions, agency interactions, and lifecycle updates.\nCollaborate with global regulatory affairs, clinical, CMC, quality, safety, and IT teams to gather submission data requirements and translate them into data engineering solutions.\nManage and oversee the development of data pipelines, data models, and metadata frameworks that support submission data standards (e.g., eCTD, IDMP, SPL, xEVMPD).\nEnable integration and reporting across regulatory information management systems (RIMS), EDMS, clinical trial systems, and lab data platforms.\nImplement data governance, lineage, validation, and audit trails for regulatory data workflows, ensuring GxP and regulatory compliance.\nGuide the development of automation solutions, dashboards, and analytics that improve visibility into submission timelines, data quality, and regulatory KPIs.\nEnsure interoperability between regulatory data platforms and enterprise data lakes or lakehouses for cross-functional reporting and insights.\nCollaborate with IT, data governance, and enterprise architecture teams to ensure alignment with overall data strategy and compliance frameworks.\nDrive innovation by evaluating emerging technologies in data engineering, graph data, knowledge management, and AI for regulatory intelligence.\nLead, mentor, and coach a small team of data engineers and analysts, fostering a culture of excellence, innovation, and delivery.\nDrive Agile and Scaled Agile (SAFe) methodologies, managing sprint backlogs, prioritization, and iterative improvements to enhance team velocity and project delivery.\nStay up-to-date with emerging data technologies, industry trends, and best practices, ensuring the organization leverages the latest innovations in data engineering and architecture.\nMust-Have Skills:\n812 years of experience in data engineering or data architecture, with 3+ years in a senior or managerial capacity, preferably within the biotech or pharmaceutical industry.\nProven experience supporting regulatory functions, including submissions, tracking, and reporting for FDA, EMA, and other global authorities.\nExperience with ETL/ELT tools, data pipelines, and cloud-based data platforms (e.g., Databricks, AWS, Azure, or GCP).\nFamiliarity with regulatory standards and data models such as eCTD, IDMP, HL7, CDISC, and xEVMPD.\nDeep understanding of GxP data compliance, audit requirements, and regulatory submission processes.\nExperience with tools like Power BI, Tableau, or Qlik for regulatory dashboarding and visualization is a plus.\nStrong project management, stakeholder communication, and leadership skills, especially in matrixed, cross-functional environments.\nAbility to translate technical capabilities into regulatory and business outcomes.Prepare team members for stakeholder discussions by helping assess data costs, access requirements, dependencies, and availability for business scenarios.\nGood-to-Have Skills:\nPrior experience working on integrated product teams or regulatory transformation programs.\nKnowledge of Regulatory Information Management Systems (RIMS), Veeva Vault RIM, or Master Data Management (MDM) in regulated environments.\nFamiliarity with Agile/SAFe methodologies and DevOps/DataOps best practices.\nEducation and Professional Certifications\n12 to 15 years of experience in Computer Science, IT or related field\nScaled Agile SAFe certification preferred\nProject Management certifications preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'engineering strategy', 'DevOps', 'Project Management', 'DataOps', 'Agile', 'data strategy']",2025-06-12 13:44:42
Senior Data Engineer (Data Architect),Adastra Corp,8 - 13 years,Not Disclosed,[],"Join our innovative team and architect the future of data solutions on Azure, Synapse, and Databricks!\nSenior Data Engineer (Data Architect)\nAdditional Details:\nNotice Period: 30 days (maximum)\nLocation: Remote\nAbout the Role\nDesign and implement scalable data pipelines, data warehouses, and data lakes that drive business growth. Collaborate with stakeholders to deliver data-driven insights and shape the data landscape.\nRequirements\n8+ years of experience in data engineering and data architecture\nStrong expertise in Azure services (Synapse Analytics, Databricks, Storage, Active Directory)\nProven experience in designing and implementing data pipelines, data warehouses, and data lakes\nStrong understanding of data governance, data quality, and data security\nExperience with infrastructure design and implementation, including DevOps practices and tools",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Azure', 'DESIGN', 'Architecture', 'Synapse Analytics']",2025-06-12 13:44:44
"Walk-In Senior Data Engineer - DataStage, Azure & Power BI",Net Connect,6 - 10 years,5-11 Lacs P.A.,['Hyderabad( Madhapur )'],"Greetings from NCG!\n\nWe have a opening for Snowflake Developer role in Hyderabad office!\nBelow JD for your reference\n\nJob Description:\n\nWe are hiring an experienced Senior Data Engineer with strong expertise in IBM DataStage, , and . The ideal candidate will be responsible for end-to-end data integration, transformation, and reporting solutions that drive business decisions.",,,,"['Azure Data Factory', 'Datastage', 'Etl Datastage']",2025-06-12 13:44:46
"Sr. Data Engineer, R&D Data Catalyst Team",Amgen Inc,7 - 9 years,Not Disclosed,['Hyderabad'],"The R&D Data Catalyst Team is responsible for buildingData Searching, Cohort Building, and Knowledge Management tools that provide the Amgen scientific community with visibility to Amgens wealth of human datasets, projects and study histories, and knowledge over various scientific findings. These solutions are pivotal tools in Amgens goal to accelerate the speed of discovery, and speed to market of advanced precision medications.\nThe Data Engineer will be responsible for the end-to-end development of an enterprise analytics and data mastering solution leveraging Databricks and Power BI. This role requiresexpertise in both data architecture and analytics, with the ability to create scalable, reliable, and high-performing enterprise solutions that research cohort-building and advanced research pipeline.The ideal candidate will have experience creating and surfacing large unifiedrepositories of human data, based on integrations from multiple repositories and solutions, and be exceptionally skilled with data analysis and profiling.\nYou will collaborate closely with stakeholders, product team members, and related IT teams, to design and implement data models, integrate data from various sources, and ensure best practices for data governance and security. The ideal candidate will have a strong background in data warehousing, ETL, Databricks, Power BI, and enterprise data mastering.\nRoles & Responsibilities:\nDesign and build scalable enterprise analytics solutions using Databricks, Power BI, and other modern data tools.\nLeverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation\nBreak down features into work that aligns with the architectural direction runway\nParticipate hands-on in pilots and proofs-of-concept for new patterns\nCreate robust documentation from data analysis and profiling, and proposed designs and data logic\nDevelop advanced sql queries to profile, and unify data\nDevelop data processing code in sql, along with semantic views to prepare data for reporting\nDevelop PowerBI Models and reporting packages\nDesign robust data models, and processing layers, that support both analytical processing and operational reporting needs.\nDesign and develop solutions based on best practices for data governance, security, and compliance within Databricks and Power BI environments.\nEnsure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.\nDevelop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.\nCollaborate with stakeholders to define data requirements, functional specifications, and project goals.\nContinuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions.\nBasic Qualifications and Experience:\nMasters degree with 1 to 3years of experience in Data Engineering OR\nBachelors degree with 4 to 5 years of experience in Data Engineering\nDiploma and 7 to 9 years of experience in Data Engineering.\nFunctional Skills:\nMust-Have Skills:\nMinimum of 3 years of hands-on experience with BI solutions (Preferrable Power BI or Business Objects) including report development, dashboard creation, and optimization.\nMinimum of 3years of hands-on experience building Change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management.\nHands-on experience with Databricks, including data engineering, optimization, and analytics workloads.\nDeep understanding of Power BI, including model design, DAX, and Power Query.\nProven experience designing and implementing data mastering solutions and data governance frameworks.\nExpertise in cloud platforms (AWS), data lakes, and data warehouses.\nStrong knowledge of ETL processes, data pipelines, and integration technologies.\nStrong communication and collaboration skills to work with cross-functional teams and senior leadership.\nAbility to assess business needs and design solutions that align with organizational goals.\nExceptional hands-on capabilities with data profiling, data transformation, data mastering\nSuccess in mentoring and training team members\nGood-to-Have Skills:\nExperience in developing differentiated and deliverable solutions\nExperience with human data, ideally human healthcare data\nFamiliarity with laboratory testing, patient data from clinical care, HL7, FHIR, and/or clinical trial data management\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nITIL Foundation or other relevant certifications (preferred)\nSAFe Agile Practitioner (6.0)\nMicrosoft Certified: Data Analyst Associate (Power BI) or related certification.\nDatabricks Certified Professional or similar certification.\nSoft Skills:\nExcellent analytical and troubleshooting skills\nDeep intellectual curiosity\nHighest degree of initiative and self-motivation\nStrong verbal and written communication skills, including presentation to varied audiences of complex technical/business topics\nConfidence technical leader\nAbility to work effectively with global, virtual teams, specifically including leveraging of tools and artifacts to assure clear and efficient collaboration across time zones\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong problem solving, analytical skills;\nAbility to learn quickly and retain and synthesize complex information from diverse sources",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'data analysis', 'ETL processes', 'DAX', 'Business Objects', 'data warehouse design', 'ETL', 'PowerBI Models', 'AWS', 'Power Query']",2025-06-12 13:44:49
Data Engineering - Senior Developer with Salesforce,Job Delights,5 - 10 years,25-27.5 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Data Engineering with SQL, Python, ETL & SalesForce Marketing Cloud (Must)",Industry Type: Software Product,Department: Other,"Employment Type: Full Time, Permanent","['Salesforce Marketing Cloud', 'SQL', 'Marketing Cloud', 'Salesforce', 'Python']",2025-06-12 13:44:51
Data Engineer - SAS Migration,Crisil,2 - 4 years,Not Disclosed,['Mumbai'],"The SAS to Databricks Migration Developer will be responsible for migrating existing SAS code, data processes, and workflows to the Databricks platform\n\nThis role requires expertise in both SAS and Databricks, with a focus on converting SAS logic into scalable PySpark and Python code\n\nThe developer will design, implement, and optimize data pipelines, ensuring seamless integration and functionality within the Databricks environment\n\nCollaboration with various teams is essential to understand data requirements and deliver solutions that meet business needs",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['hive', 'scala', 'pyspark', 'data warehousing', 'data migration', 'azure data factory', 'sql', 'sql azure', 'java', 'spark', 'mysql', 'hadoop', 'big data', 'etl', 'python', 'sas', 'microsoft azure', 'power bi', 'machine learning', 'sql server', 'data bricks', 'migration', 'sqoop', 'aws', 'ssis']",2025-06-12 13:44:53
"Data Engineering : Sr Software Engineer, Tech Lead & Sr Tech Lead",Reflion Tech,7 - 12 years,22.5-37.5 Lacs P.A.,"['Mumbai( Ghansoli )', 'Navi Mumbai', 'Mumbai (All Areas)']","Hiring: Data Engineering Senior Software Engineer / Tech Lead / Senior Tech Lead\n\n- Hybrid (3 Days from office) | Shift: 2 PM 11 PM IST\n- Experience: 5 to 12+ years (based on role & grade)\n\nOpen Grades/Roles:\nSenior Software Engineer: 58 Years\nTech Lead: 7–10 Years\nSenior Tech Lead: 10–12+ Years\n\nJob Description – Data Engineering Team\n\nCore Responsibilities (Common to All Levels):\n\nDesign, build and optimize ETL/ELT pipelines using tools like Pentaho, Talend, or similar\nWork on traditional databases (PostgreSQL, MSSQL, Oracle) and MPP/modern systems (Vertica, Redshift, BigQuery, MongoDB)\nCollaborate cross-functionally with BI, Finance, Sales, and Marketing teams to define data needs\nParticipate in data modeling (ER/DW/Star schema), data quality checks, and data integration\nImplement solutions involving messaging systems (Kafka), REST APIs, and scheduler tools (Airflow, Autosys, Control-M)\nEnsure code versioning and documentation standards are followed (Git/Bitbucket)\n\nAdditional Responsibilities by Grade\n\nSenior Software Engineer (5–8 Yrs):\nFocus on hands-on development of ETL pipelines, data models, and data inventory\nAssist in architecture discussions and POCs\nGood to have: Tableau/Cognos, Python/Perl scripting, GCP exposure\n\nTech Lead (7–10 Yrs):\nLead mid-sized data projects and small teams\nDecide on ETL strategy (Push Down/Push Up) and performance tuning\nStrong working knowledge of orchestration tools, resource management, and agile delivery\n\nSenior Tech Lead (10–12+ Yrs):\nDrive data architecture, infrastructure decisions, and internal framework enhancements\nOversee large-scale data ingestion, profiling, and reconciliation across systems\nMentoring junior leads and owning stakeholder delivery end-to-end\nAdvantageous: Experience with AdTech/Marketing data, Hadoop ecosystem (Hive, Spark, Sqoop)\n\n- Must-Have Skills (All Levels):\n\nETL Tools: Pentaho / Talend / SSIS / Informatica\nDatabases: PostgreSQL, Oracle, MSSQL, Vertica / Redshift / BigQuery\nOrchestration: Airflow / Autosys / Control-M / JAMS\nModeling: Dimensional Modeling, ER Diagrams\nScripting: Python or Perl (Preferred)\nAgile Environment, Git-based Version Control\nStrong Communication and Documentation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'SQL', 'ETL', 'Orchestration', 'Postgresql', 'Peri', 'Informatica', 'ETL Tool', 'SSIS', 'Elt', 'Modeling', 'MongoDB', 'Data Architecture', 'Talend', 'Pentaho', 'Python']",2025-06-12 13:44:56
"Senior data engineer - Python, Pyspark, AWS - 5+ years Gurgaon",One of the largest insurance providers.,5 - 10 years,Not Disclosed,['Gurugram'],"Senior data engineer - Python, Pyspark, AWS - 5+ years Gurgaon\n\nSummary: An excellent opportunity for someone having a minimum of five years of experience with expertise in building data pipelines. A person must have experience in Python, Pyspark and AWS.\n\nLocation- Gurgaon (Hybrid)\n\nYour Future Employer- One of the largest insurance providers.\n\nResponsibilities-\nTo design, develop, and maintain large-scale data pipelines that can handle large datasets from multiple sources.\nReal-time data replication and batch processing of data using distributed computing platforms like Spark, Kafka, etc.\nTo optimize the performance of data processing jobs and ensure system scalability and reliability.\nTo collaborate with DevOps teams to manage infrastructure, including cloud environments like AWS.\nTo collaborate with data scientists, analysts, and business stakeholders to develop tools and platforms that enable advanced analytics and reporting.\n\nRequirements-\nHands-on experience with AWS services such as S3, DMS, Lambda, EMR, Glue, Redshift, RDS (Postgres) Athena, Kinesics, etc.\nExpertise in data modeling and knowledge of modern file and table formats.\nProficiency in programming languages such as Python, PySpark, and SQL/PLSQL for implementing data pipelines and ETL processes.\nExperience data architecting or deploying Cloud/Virtualization solutions (Like Data Lake, EDW, Mart ) in the enterprise.\nCloud/hybrid cloud (preferably AWS) solution for data strategy for Data lake, BI and Analytics.\nWhat is in for you-\nA stimulating working environment with equal employment opportunities.\nGrowing of skills while working with industry leaders and top brands.\nA meritocratic culture with great career progression.\n\nReach us- If you feel that you are the right fit for the role please share your updated CV at randhawa.harmeen@crescendogroup.in\n\nDisclaimer- Crescendo Global specializes in Senior to C-level niche recruitment. We are passionate about empowering job seekers and employers with an engaging memorable job search and leadership hiring experience. Crescendo Global does not discriminate on the basis of race, religion, color, origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Pipeline', 'AWS', 'Data Ingestion', 'Data Engineering', 'Data Processing']",2025-06-12 13:44:58
Linux Kernel Engineer Senior For Data Center SoC,Qualcomm,2 - 7 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nAs a Senior Software Engineer, you will play a pivotal role in designing, developing, optimizing, and commercializing software solutions for Qualcomms next-generation data center platforms. You will collaborate closely with cross-functional teams to advance critical technologies such as virtualization, memory management, scheduling, and the Linux Kernel.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\nCollaborate within the team and across teams to design, develop, and release our software, tooling, and practices to meet community standards and internal and external requirements.\nBring up platform solutions across the Qualcomm chipset portfolio.\nTriage software build, tooling, packaging, functional, or stability failures.\nGuide and support development teams inside and outside the Linux organization, focusing on Linux userspace software functionality, integration, and maintenance.\nWork with development and product teams as necessary for issue resolution.\n\n\nPreferred Qualifications:\nMaster's Degree in Engineering, Information Systems, Computer Science, or a related field.\nStrong background in Computer Science and software fundamentals.\nWorking knowledge of C, C++, and proficiency in scripting languages (Bash, Python, etc.).\nExperience using git/gerrit.\nStrong understanding of the Linux kernel, configuration techniques like ACPI and device tree, system services, and various components that make up a Linux distribution.\nExperience with Linux distributions such as Debian, Ubuntu, RedHat, Yocto, etc.\nFamiliarity with package managers and their workings is crucial.\nFamiliarity with CI/CD tools.\nProven ability and interest in debugging complex compute and data center systems.\nStrong ability to solve problems in a non-linear fashion.\nQuick learner; able to grasp concepts with only basic training and the initiative to ask questions and investigate new areas and concepts as needed.\nPrior experience with Qualcomm software platforms is a plus.\nMature interpersonal skills with an ability to collaboratively work within the team and with many varied teams to resolve problems spanning many disciplines.\nProven ability to work in a dynamic, multi-tasked environment.\nExcellent written and verbal communication skills are required.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['computer science', 'linux', 'software engineering', 'scripting languages', 'linux kernel', 'continuous integration', 'c++', 'redhat linux', 'ci/cd', 'gerrit', 'git', 'java', 'yocto', 'embedded systems', 'debian', 'html', 'mysql', 'python', 'c', 'ubuntu', 'javascript', 'data center', 'embedded c', 'bash', 'aws']",2025-06-12 13:45:00
Senior Azure Data Engineer,Cloud Angles Digital Transformation,8 - 12 years,Not Disclosed,['Hyderabad'],"Job Summary:\nWe are seeking a highly skilled Data Engineer with expertise in leveraging Data Lake architecture and the Azure cloud platform to develop, deploy, and optimise data-driven solutions. . You will play a pivotal role in transforming raw data into actionable insights, supporting strategic decision-making across the organisation.\nResponsibilities\nDesign and implement scalable data science solutions using Azure Data Lake, Azure Data Bricks, Azure Data Factory and related Azure services.\nDevelop, train, and deploy machine learning models to address business challenges.\nCollaborate with data engineering teams to optimise data pipelines and ensure seamless data integration within Azure cloud infrastructure.\nConduct exploratory data analysis (EDA) to identify trends, patterns, and insights.\nBuild predictive and prescriptive models to support decision-making processes.\nExpertise in developing end-to-end Machine learning lifecycle utilizing crisp-DM which includes of data collection, cleansing, visualization, preprocessing, model development, model validation and model retraining\nProficient in building and implementing RAG systems that enhance the accuracy and relevance of model outputs by integrating retrieval mechanisms with generative models.\nEnsure data security, compliance, and governance within the Azure cloud ecosystem.\nMonitor and optimise model performance and scalability in production environments.\nPrepare clear and concise documentation for developed models and workflows.\nSkills Required:\nGood experience in using Pyspark, Python, MLops (Optional), ML flow (Optional), Azure Data Lake Storage. Unity Catalog\nWorked and utilized data from various RDBMS like MYSQL, SQL Server, Postgres and NoSQL databases like MongoDB, Cassandra, Redis and graph DB like Neo4j, Grakn.\nProven experience as a Data Engineer with a strong focus on Azure cloud platform and Data Lake architecture.\nProficiency in Python, Pyspark,\nHands-on experience with Azure services such as Azure Data Lake, Azure Synapse Analytics, Azure Machine Learning, Azure Databricks, and Azure Functions.\nStrong knowledge of SQL and experience in querying large datasets from Data Lakes.\nFamiliarity with data engineering tools and frameworks for data ingestion and transformation in Azure.\nExperience with version control systems (e.g., Git) and CI/CD pipelines for machine learning projects.\nExcellent problem-solving skills and the ability to work collaboratively in a team environment.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Data Engineering', 'Azure Databricks', 'Pyspark', 'Azure Data Lake', 'Python']",2025-06-12 13:45:03
Data Streaming Engineer,Data Streaming Engineer,5 - 10 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","Hello Candidates,\n\nWe are Hiring !!\n\nJob Position - Data Streaming Engineer\nExperience - 5+ years\nLocation - Mumbai, Pune , Chennai , Bangalore\nWork mode - Hybrid ( 3 days WFO)\n\nJOB DESCRIPTION\n\nRequest for Data Streaming Engineer Data Streaming @ offshore :\n• Flink , Python Language.\n• Data Lake Systems. (OLAP Systems).\n• SQL (should be able to write complex SQL Queries)\n• Orchestration (Apache Airflow is preferred).\n• Hadoop (Spark and Hive: Optimization of Spark and Hive apps).\n• Snowflake (good to have).\n• Data Quality (good to have).\n• File Storage (S3 is good to have)\n\nNOTE - Candidates can share their resume on - shrutia.talentsketchers@gmail.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Flink', 'Apache Airflow', 'Data Quality', 'Hadoop', 'Snowflake', 'Data Lake', 'orchastration', 'Python', 'SQL']",2025-06-12 13:45:05
Senior Data Engineer,Amgen Inc,3 - 8 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nDesign, develop, and maintain scalable ETL/ELT pipelines to support structured, semi-structured, and unstructured data processing across the Enterprise Data Fabric.\nImplement real-time and batch data processing solutions, integrating data from multiple sources into a unified, governed data fabric architecture.\nOptimize big data processing frameworks using Apache Spark, Hadoop, or similar distributed computing technologies to ensure high availability and cost efficiency.\nWork with metadata management and data lineage tracking tools to enable enterprise-wide data discovery and governance.\nEnsure data security, compliance, and role-based access control (RBAC) across data environments.\nOptimize query performance, indexing strategies, partitioning, and caching for large-scale data sets.\nDevelop CI/CD pipelines for automated data pipeline deployments, version control, and monitoring.\nImplement data virtualization techniques to provide seamless access to data across multiple storage systems.\nCollaborate with cross-functional teams, including data architects, business analysts, and DevOps teams, to align data engineering strategies with enterprise goals.\nStay up to date with emerging data technologies and best practices, ensuring continuous improvement of Enterprise Data Fabric architectures.\nMust-Have Skills:\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies.\nProficiency in workflow orchestration, performance tuning on big data processing.\nStrong understanding of AWS services\nExperience with Data Fabric, Data Mesh, or similar enterprise-wide data architectures.\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and DevOps\nEducation and Professional Certifications\nMasters degree and 3 to 4 + years of Computer Science, IT or related field experience\nOR\nBachelors degree and 5 to 8 + years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Git', 'PySpark', 'CI/CD', 'Databricks', 'ETL', 'NOSQL', 'AWS', 'data integration', 'SQL', 'Apache Spark', 'Python']",2025-06-12 13:45:07
Consultant - Lead Data Engineer,Affine Analytics,5 - 8 years,Not Disclosed,['Bengaluru'],"Strong experience with Python, SQL, pySpark, AWS Glue. Good to have - Shell Scripting, Kafka\nGood knowledge of DevOps pipeline usage (Jenkins, Bitbucket, EKS, Lightspeed)\nExperience of AWS tools (AWS S3, EC2, Athena, Redshift, Glue, EMR, Lambda, RDS, Kinesis, DynamoDB, QuickSight etc.).\nOrchestration using Airflow\nGood to have - Streaming technologies and processing engines, Kinesis, Kafka, Pub/Sub and Spark Streaming\nGood debugging skills",,,,"['Python', 'RDS', 'Shell Scripting', 'Kafka', 'AWS Glue', 'DynamoDB', 'Lightspeed', 'EMR', 'EKS', 'pySpark', 'Redshift', 'SQL', 'Jenkins', 'QuickSight', 'Glue', 'EC2', 'Kinesis', 'AWS S3', 'Bitbucket', 'Athena', 'Lambda']",2025-06-12 13:45:10
Senior Data Engineer,Amgen Inc,3 - 7 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nDesign, develop, and maintain scalable ETL/ELT pipelines to support structured, semi-structured, and unstructured data processing across the Enterprise Data Fabric.\nImplement real-time and batch data processing solutions, integrating data from multiple sources into a unified, governed data fabric architecture.\nOptimize big data processing frameworks using Apache Spark, Hadoop, or similar distributed computing technologies to ensure high availability and cost efficiency.\nWork with metadata management and data lineage tracking tools to enable enterprise-wide data discovery and governance.\nEnsure data security, compliance, and role-based access control (RBAC) across data environments.\nOptimize query performance, indexing strategies, partitioning, and caching for large-scale data sets.\nDevelop CI/CD pipelines for automated data pipeline deployments, version control, and monitoring.\nImplement data virtualization techniques to provide seamless access to data across multiple storage systems.\nCollaborate with cross-functional teams, including data architects, business analysts, and DevOps teams, to align data engineering strategies with enterprise goals.\nStay up to date with emerging data technologies and best practices, ensuring continuous improvement of Enterprise Data Fabric architectures.\nMust-Have Skills:\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies.\nProficiency in workflow orchestration, performance tuning on big data processing.\nStrong understanding of AWS services\nExperience with Data Fabric, Data Mesh, or similar enterprise-wide data architectures.\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\n9 to 12 years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Maven', 'SparkSQL Apache Spark', 'PySpark', 'Subversion', 'OLAP', 'Scaled Agile methodologies', 'SQL', 'Scaled Agile Framework', 'Jenkins', 'NOSQL database', 'Git', 'Databricks', 'Data Fabric', 'Data Mesh', 'AWS', 'Python']",2025-06-12 13:45:12
Senior Data Engineer,Amgen Inc,9 - 12 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nDesign, develop, and maintain scalable ETL/ELT pipelines to support structured, semi-structured, and unstructured data processing across the Enterprise Data Fabric.\nImplement real-time and batch data processing solutions, integrating data from multiple sources into a unified, governed data fabric architecture.\nOptimize big data processing frameworks using Apache Spark, Hadoop, or similar distributed computing technologies to ensure high availability and cost efficiency.\nWork with metadata management and data lineage tracking tools to enable enterprise-wide data discovery and governance.\nEnsure data security, compliance, and role-based access control (RBAC) across data environments.\nOptimize query performance, indexing strategies, partitioning, and caching for large-scale data sets.\nDevelop CI/CD pipelines for automated data pipeline deployments, version control, and monitoring.\nImplement data virtualization techniques to provide seamless access to data across multiple storage systems.\nCollaborate with cross-functional teams, including data architects, business analysts, and DevOps teams, to align data engineering strategies with enterprise goals.\nStay up to date with emerging data technologies and best practices, ensuring continuous improvement of Enterprise Data Fabric architectures.\nMust-Have Skills:\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies.\nProficiency in workflow orchestration, performance tuning on big data processing.\nStrong understanding of AWS services\nExperience with Data Fabric, Data Mesh, or similar enterprise-wide data architectures.\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\n9 to 12 years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data engineering', 'performance tuning', 'data security', 'data processing', 'Hadoop', 'Apache Spark', 'SQL', 'CI/CD', 'troubleshooting', 'big data', 'aws', 'ETL', 'Python']",2025-06-12 13:45:15
Senior Data Engineer,Amgen Inc,3 - 7 years,Not Disclosed,['Hyderabad'],"What you will do\nRole Description:\nWe are seeking a Senior Data Engineer with expertise in Graph Data technologies to join our data engineering team and contribute to the development of scalable, high-performance data pipelines and advanced data models that power next-generation applications and analytics. This role combines core data engineering skills with specialized knowledge in graph data structures, graph databases, and relationship-centric data modeling, enabling the organization to leverage connected data for deep insights, pattern detection, and advanced analytics use cases. The ideal candidate will have a strong background in data architecture, big data processing, and Graph technologies and will work closely with data scientists, analysts, architects, and business stakeholders to design and deliver graph-based data engineering solutions.\nRoles & Responsibilities:\nDesign, build, and maintain robust data pipelines using Databricks (Spark, Delta Lake, PySpark) for complex graph data processing workflows.\nOwn the implementation of graph-based data models, capturing complex relationships and hierarchies across domains.\nBuild and optimize Graph Databases such as Stardog, Neo4j, Marklogic or similar to support query performance, scalability, and reliability.\nImplement graph query logic using SPARQL, Cypher, Gremlin, or GSQL, depending on platform requirements.\nCollaborate with data architects to integrate graph data with existing data lakes, warehouses, and lakehouse architectures.\nWork closely with data scientists and analysts to enable graph analytics, link analysis, recommendation systems, and fraud detection use cases.\nDevelop metadata-driven pipelines and lineage tracking for graph and relational data processing.\nEnsure data quality, governance, and security standards are met across all graph data initiatives.\nMentor junior engineers and contribute to data engineering best practices, especially around graph-centric patterns and technologies.\nStay up to date with the latest developments in graph technology, graph ML, and network analytics.\nWhat we expect of you\nMust-Have Skills:\nHands-on experience in Databricks, including PySpark, Delta Lake, and notebook-based development.\nHands-on experience with graph database platforms such as Stardog, Neo4j, Marklogic etc.\nStrong understanding of graph theory, graph modeling, and traversal algorithms\nProficiency in workflow orchestration, performance tuning on big data processing\nStrong understanding of AWS services\nAbility to quickly learn, adapt and apply new technologies with strong problem-solving and analytical skills\nExcellent collaboration and communication skills, with experience working with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\nMasters degree and 3 to 4 + years of Computer Science, IT or related field experience\nBachelors degree and 5 to 8 + years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'SPARQL', 'Maven', 'PySpark', 'GSQL', 'Subversion', 'AWS services', 'Stardog', 'Cypher', 'SAFe', 'Jenkins', 'DevOps', 'Git', 'Neo4j', 'Delta Lake', 'Graph Databases', 'Spark', 'Marklogic', 'Gremlin']",2025-06-12 13:45:17
Immediate Joiner- Data Engineer,Healthedge,1 - 4 years,Not Disclosed,['Bengaluru'],"Data Engineer\nYou will be working with agile cross functional software development teams developing cutting age software to solve a significant problem in the Provider Data Management space. This hire will have experience building large scale complex data systems involving multiple cross functional data sets and teams. The ideal candidate will be excited about working on new product development, is comfortable pushing the envelope and challenging the status quo, sets high standards for him/herself and the team, and works well with ambiguity.\nWhat you will do:\nBuild data pipelines to assemble large, complex sets of data that meet non-functional and functional business requirements.\nWork closely with data architect, SMEs and other technology partners to develop & execute data architecture and product roadmap.\nBuild analytical tools to utilize the data pipeline, providing actionable insight into key business performance including operational efficiency and business metrics.\nWork with stakeholders including the leadership, product, customer teams to support their data infrastructure needs while assisting with data-related technical issues.\nAct as a subject matter expert to other team members for technical guidance, solution design and best practices within the customer organization.\nKeep current on big data and data visualization technology trends, evaluate, work on proof-of-concept and make recommendations on cloud technologies.\nWhat you bring:\n2+ years of data engineering experience working in partnership with large data sets (preferably terabyte scale)\nExperience in building data pipelines using any of the ETL tools such as Glue, ADF, Notebooks, Stored Procedures, SQL/Python constructs or similar.\nDeep experience working with industry standard RDBMS such Postgres, SQL Server, Oracle, MySQL etc. and any of the analytical cloud databases such as Big Query, Redshift, Snowflake or similar\nAdvanced SQL expertise and solid programming experience with Python and/or Spark\nExperience working with orchestration tools such as Airflow and building complex dependency workflows.\nExperience, developing and implementing Data Warehouse or Data Lake Architectures, OLAP technologies, data modeling with star/snowflake-schemas to enable analytics & reporting.\nGreat problem-solving capabilities, troubleshooting data issues and experience in stabilizing big data systems.\nExcellent communication and presentation skills as youll be regularly interacting with stakeholders and engineering leadership.\nBachelors or master's in quantitative disciplines such as Computer Science, Computer Engineering, Analytics, Mathematics, Statistics, Information Systems, or other scientific fields.\nBonus points:\nHands-on deep experience with cloud data migration, and experience working with analytic platforms like Fabric, Databricks on the cloud.\nCertification in one of the cloud platforms (AWS/GCP/Azure)\nExperience or demonstrated understanding with real-time data streaming tools like Kafka, Kinesis or any similar tools.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ETL', 'SQL', 'Pyspark', 'Cloud', 'Python']",2025-06-12 13:45:19
Senior Data Engineer,Conversehr Business Solutions,7 - 12 years,30-45 Lacs P.A.,['Hyderabad'],"What is the Data team responsible for?\nAs a Senior Data Engineer, youll be a key member of the Data & AI team. This team is responsible for designing and delivering data engineering, analytics, and generative AI solutions that drive meaningful business impact. Were looking for a pragmatic, results-driven problem solver who thrives in a fast-paced environment and is passionate about building solutions on a scale #MID_SENIOR_LEVEL\nThe ideal candidate has a strong technical foundation, a collaborative mindset, and the ability to navigate complex challenges. You should be comfortable working in a fast-moving, startup-like environment within an established enterprise, and should bring strong skill sets to adapt to new solutions fast. You will play a pivotal role in optimizing data infrastructure, enabling data-driven decision-making and integrating AI across the organization.\nWhat is the Lead Software Engineer (Senior Data Engineer) responsible for?\nServe as a hands-on technical lead, driving project execution and delivery in our growing data team based in the Hyderabad office.\nCollaborate closely with the U.S.-based team and cross-functional stakeholders to understand business needs and deliver scalable solutions.\nLead the initiative to build firmwide data models and master data management solutions for structured data (in Snowflake) and manage unstructured data using vector embeddings.\nBuild, maintain, and optimize robust data pipelines and frameworks to support business intelligence and operational workflows.\nDevelop dashboards and data visualizations that support strategic business decisions.\nStay current with emerging trends in data engineering and help implement best practices within the team.\nMentor and support junior engineers, fostering a culture of learning and technical excellence.\nWhat ideal qualifications, skills & experience would help someone to be successful?\nBachelors or master’s degree in computer science, data science, engineering, or a related field.\n7+ years of experience in data engineering including 3+ years in a technical leadership role.\nStrong SQL skills and hands-on experience with modern data pipeline technologies (e.g., Spark, Flink).\nDeep expertise in the Snowflake ecosystem, including data modeling, data warehousing, and master data management.\nProficiency in at least one programming language - Python preferred.\nExperience with Tableau and Alteryx is a plus.\nSelf-starter with a passion for learning new tools and technologies.\nStrong communication skills and a collaborative, ownership-driven mindset.\nWork Shift Timings - 2:00 PM - 11:00 PM IST",Industry Type: Investment Banking / Venture Capital / Private Equity,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'Snowflake', 'Python', 'flink', 'Data Pipeline', 'Spark', 'SQL']",2025-06-12 13:45:21
Data Engineer KL-BL,Puresoftware,5 - 12 years,Not Disclosed,['Bengaluru'],"Core Competences Required and Desired Attributes:\nBachelors degree in computer science, Information Technology, or a related field.\nProficiency in Azure Data Factory, Azure Databricks and Unity Catalog, Azure SQL Database, and other Azure data services.\nStrong programming skills in SQL, Python and PySpark languages.\nExperience in the Asset Management domain would be preferable.\nStrong proficiency in data analysis and data modelling, with the ability to extract insights from complex data sets.\nHands-on experience in Power BI, including creating custom visuals, DAX expressions, and data modelling.\nFamiliarity with Azure Analysis Services, data modelling techniques, and optimization.\nExperience with data quality and data governance frameworks with an ability to debug, fine tune and optimise large scale data processing jobs.\nStrong analytical and problem-solving skills, with a keen eye for detail.\nExcellent communication and interpersonal skills, with the ability to work collaboratively in a team environment.\nProactive and self-motivated, with the ability to manage multiple tasks and deliver high-quality results within deadlines.",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Interpersonal skills', 'Data modeling', 'Analytical', 'data governance', 'Data quality', 'Asset management', 'Information technology', 'SQL', 'Python']",2025-06-12 13:45:23
Data Engineering Specialist,Overture Rede,10 - 15 years,Not Disclosed,"['Mumbai', 'Hyderabad', 'Gurugram', 'Bengaluru']","Job Title: Sales Excellence COE Data Engineering Specialist\nLocations: Mumbai / Bangalore / Gurgaon / Hyderabad\nExperience: 1012 Years Level: Team Lead / Specialist (Level 9)\n\nJob Role\nLead data engineering efforts to support sales insights through scalable pipelines, statistical modeling, and ML workflows across cloud platforms.\n\nRequired Skills\nProficiency in Python\nAdvanced SQL (Views, Functions, Procedures)\nExperience with Google Cloud Platform (GCP) ML workflow setup\nStrong in Data Modeling and ETL Development\nExcel skills including VBA, Power Pivot, Cube Functions\nSolid understanding of Sales Processes\n\n\n",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Statistical modeling', 'Sales', 'Excel', 'VBA', 'Data modeling', 'GCP', 'Cloud', 'Workflow', 'SQL', 'Python']",2025-06-12 13:45:26
Data Engineer II - Marketplace (Experimentation Track),Booking Holdings,5 - 10 years,Not Disclosed,['Bengaluru'],"We are looking for a Data Engineer to join our team and help us to improve the platform that supports one of the best experimentation tools in the world.\nYou will work side by side with other data engineers and site reliability engineers to improve the reliability, scalability, maintenance and operations of all the data products that are part of the experimentation tool at Booking.com.\nYour day to day work includes but is not limited to: maintenance and operations of data pipelines and products that handles data at big scale; the development of capabilities for monitoring, alerting, testing and troubleshooting of the data ecosystem of the experiment platform; and the delivery of data products that produce metrics for experimentation at scale. You will collaborate with colleagues in Amsterdam to achieve results the right way. This will include engineering managers, product managers, engineers and data scientists.\nKey Responsibilities and Duties\nTake ownership of multiple data pipelines and products and provide innovative solutions to reduce the operational workload required to maintain them\nRapidly developing next-generation scalable, flexible, and high-performance data pipelines.\nContribute to the development of data platform capabilities such as testing, monitoring, debugging and alerting to improve the development environment of data products\nSolve issues with data and data pipelines, prioritizing based on customer impact.\nEnd-to-end ownership of data quality in complex datasets and data pipelines.\nExperiment with new tools and technologies, driving innovative engineering solutions to meet business requirements regarding performance, scaling, and data quality.\nProvide self-organizing tools that help the analytics community discover data, assess quality, explore usage, and find peers with relevant expertise.\nServe as the main point of contact for technical and business stakeholders regarding data engineering issues, such as pipeline failures and data quality concerns\nRole requirements\nMinimum 5 years of hands-on experience in data engineering as a Data Engineer or as a Software Engineer developing data pipelines and products.\nBachelors degree in Computer Science, Computer or Electrical Engineering, Mathematics, or a related field or 5 years of progressively responsible experience in the specialty as equivalent\nSolid experience in at least one programming language. We use Java and Python\nExperience building production data pipelines in the cloud, setting up data-lakes and server-less solutions\nHands-on experience with schema design and data modeling\nExperience designing systems E2E and knowledge of basic concepts (lb, db, caching, NoSQL, etc)\nKnowledge of Flink, CDC, Kafka, Airflow, Snowflake, DBT or equivalent tools\nPractical experience building data platform capabilities like testing, alerting, monitoring, debugging, security\nExperience working with big data.\nExperience working with teams located in different timezones is a plus\nExperience with experimentation, statistics and A/B testing is a plus",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Airflow', 'Java', 'CDC', 'NoSQL', 'Snowflake', 'DBT', 'Kafka', 'Python']",2025-06-12 13:45:28
GCP Data Engineer,TVS Next,3 - 5 years,Not Disclosed,['Bengaluru'],"What you’ll be doing:\nAssist in developing machine learning models based on project requirements\nWork with datasets by preprocessing, selecting appropriate data representations, and ensuring data quality.\nPerforming statistical analysis and fine-tuning using test results.\nSupport training and retraining of ML systems as needed.\nHelp build data pipelines for collecting and processing data efficiently.",,,,"['kubernetes', 'pyspark', 'data pipeline', 'sql', 'docker', 'cloud', 'tensorflow', 'java', 'spark', 'gcp', 'pytorch', 'bigquery', 'programming', 'ml', 'cloud sql', 'cd', 'python', 'airflow', 'cloud spanner', 'cloud pubsub', 'application engine', 'machine learning', 'apache flink', 'data engineering', 'dataproc', 'kafka', 'cloud storage', 'terraform', 'bigtable']",2025-06-12 13:45:31
Azure Data Engineer,Big4,7 - 12 years,18-30 Lacs P.A.,['Bengaluru'],"Urgently Hiring for Senior Azure Data Engineer\n\nJob Location- Bangalore\nMinimum exp - Total 7+yrs with min 4 years relevant exp\n\nKeywords Databricks, Pyspark, SCALA, SQL, Live / Streaming data, batch processing data\n\nShare CV siddhi.pandey@adecco.com\nOR Call 6366783349\n\nRoles and Responsibilities:\nThe Data Engineer will work on data engineering projects for various business units, focusing on delivery of complex data management solutions by leveraging industry best practices. They work with the project team to build the most efficient data pipelines and data management solutions that make data easily available for consuming applications and analytical solutions. A Data engineer is expected to possess strong technical skills\n\nKey Characteristics\nTechnology champion who constantly pursues skill enhancement and has inherent curiosity to understand work from multiple dimensions\nInterest and passion in Big Data technologies and appreciates the value that can be brought in with an effective data management solution\nHas worked on real data challenges and handled high volume, velocity, and variety of data.\nExcellent analytical & problem-solving skills, willingness to take ownership and resolve technical challenges.\nContributes to community building initiatives like CoE, CoP.\nMandatory skills:\nAzure - Master\nELT - Skill\nData Modeling - Skill\nData Integration & Ingestion - Skill\nData Manipulation and Processing - Skill\nGITHUB, Action, Azure DevOps - Skill\nData factory, Databricks, SQL DB, Synapse, Stream Analytics, Glue, Airflow, Kinesis, Redshift, SonarQube, PyTest - Skill\nOptional skills:\nExperience in project management, running a scrum team.\nExperience working with BPC, Planning.\nExposure to working with external technical ecosystem.\nMKDocs documentation\n\nShare CV siddhi.pandey@adecco.com\nOR Call 6366783349",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['databricks', 'Azure Synapse', 'Pyspark', 'Stream Analytics', 'SCALA', 'SQL Azure', 'Data Bricks', 'SQL']",2025-06-12 13:45:33
Tech. PM - Data Engineering-Data Analytics@ Gurgaon/Blore_Urgent,A global leader in delivering innovative...,5 - 10 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Job Title - Technical Project Manager\n\nLocation - Gurgaon/ Bangalore\n\nNature of Job - Permanent\n\nDepartment - data analytics\n\nWhat you will be doing\n\n\nDemonstrated client servicing and business analytics skills with at least 5 - 9 years of experience as data engineer, BI developer, data analyst, technical project manager, program manager etc.\nTechnical project management- drive BRD, project scope, resource allocation, team\ncoordination, stakeholder communication, UAT, Prod fix, change requests, project governance\nSound knowledge of banking industry (payments, retail operations, fraud etc.)\nStrong ETL experience or experienced Teradata developer\nManaging team of business analysts, BI developers, ETL developers to ensure that projects are completed on time\nResponsible for providing thought leadership and technical advice on business issues\nDesign methodological frameworks and solutions.\n\n\nWhat were looking for\n\n\nBachelors/masters degree in computer science/data science/AI/statistics, Certification in Gen AI. Masters degree Preferred.\nManage multiple projects, at a time, from inception to delivery\nSuperior problem-solving, analytical, and quantitative skills\nEntrepreneurial mindset, coupled with a “can do” attitude\nDemonstrated ability to collaborate with cross-functional, cross-border teams and coach / mentor colleagues.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Technical Project Manager', 'Data Engineering', 'multiple projects', 'Technical project management', 'Data Analytics', 'project scope', 'ETL Pipeline', 'team coordination', 'resource allocation', 'Prod fix', 'drive BRD', 'program manager', 'Big data']",2025-06-12 13:45:35
Data Engineering Manager,NOVARTIS,6 - 8 years,Not Disclosed,['Hyderabad'],"Summary\nWe are seeking a highly skilled and motivated GCP Data Engineering Manager to join our dynamic team. As a Data Engineering manager specializing in Google Cloud Platform (GCP), you will play a crucial role in designing, implementing, and maintaining scalable data pipelines and\nsystems. You will leverage your expertise in Google Big Query, SQL, Python, and analytical skills to drive data-driven decision-making processes and support various business functions.\nAbout the Role\nKey Responsibilities:\nData Pipeline Development: Design, develop, and maintain robust data pipelines using GCP services like Dataflow, Dataproc, ensuring high performance and scalability.\nGoogle Big Query Expertise: Utilize your hands-on experience with Google Big Query to manage and optimize data storage, retrieval, and processing.\nSQL Proficiency: Write and optimize complex SQL queries to transform and analyze large datasets, ensuring data accuracy and integrity.\nPython Programming: Develop and maintain Python scripts for data processing, automation, and integration with other systems and tools.\nData Integration: Collaborate with data analysts, and other stakeholders to integrate data from various sources, ensuring seamless data flow and consistency.\nData Quality and Governance: Implement data quality checks, validation processes, and governance frameworks to maintain high data standards.\nPerformance Tuning: Monitor and optimize the performance of data pipelines, queries, and storage solutions to ensure efficient data processing.\nDocumentation: Create comprehensive documentation for data pipelines, processes, and best practices to facilitate knowledge sharing and team collaboration.\nMinimum Qualifications:\nProven experience (minimum 6 - 8 yrs) in Data Engineer, with significant hands-on experience in Google Cloud Platform (GCP) and Google Big Query.\nProficiency in SQL for data transformation, analysis and performance optimization.\nStrong programming skills in Python, with experience in developing data processing scripts and automation.\nProven analytical skills with the ability to interpret complex data and provide actionable insights.\nExcellent problem-solving abilities and attention to detail.\nStrong communication and collaboration skills, with the ability to work effectively in a team enviro\nDesired Skills :\nExperience with Google Analytics data and understanding of digital marketing data.\nFamiliarity with other GCP services such as Cloud Storage, Dataflow, Pub/Sub, and Dataproc.\nKnowledge of data visualization tools such as Looker, Tableau, or Data Studio.\nExperience with machine learning frameworks and libraries.\nWhy Novartis: Helping people with disease and their families takes more than innovative science. It takes a community of smart, passionate people like you. Collaborating, supporting and inspiring each other. Combining to achieve breakthroughs that change patients lives. Ready to create a brighter future together? https://www. novartis. com / about / strategy / people-and-culture\nJoin our Novartis Network: Not the right Novartis role for you? Sign up to our talent community to stay connected and learn about suitable career opportunities as soon as they come up: https://talentnetwork. novartis. com/network\nBenefits and Rewards: Read our handbook to learn about all the ways we ll help you thrive personally and professionally:",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Performance tuning', 'Automation', 'Google Analytics', 'Machine learning', 'Data processing', 'Data quality', 'data visualization', 'Digital marketing', 'SQL', 'Python']",2025-06-12 13:45:38
GCP Data Engineer,Swits Digital,4 - 6 years,Not Disclosed,['Bengaluru'],"Job Title: GCP Data Engineer\nLocation: Chennai, Bangalore, Hyderabad\nExperience: 4-6 Years\nJob Summary:\nWe are seeking a GCP Data & Cloud Engineer with strong expertise in Google Cloud Platform services, including BigQuery, Cloud Run, Cloud Storage , and Pub/Sub . The ideal candidate will have deep experience in SQL coding , data pipeline development, and deploying cloud-native solutions.\nKey Responsibilities:\nDesign, implement, and optimize scalable data pipelines and services using GCP\nBuild and manage cloud-native applications deployed via Cloud Run\nDevelop complex and performance-optimized SQL queries for analytics and data transformation\nManage and automate data storage, retrieval, and archival using Cloud Storage\nImplement event-driven architectures using Google Pub/Sub\nWork with large datasets in BigQuery , including ETL/ELT design and query optimization\nEnsure security, monitoring, and compliance of cloud-based systems\nCollaborate with data analysts, engineers, and product teams to deliver end-to-end cloud solutions\nRequired Skills & Experience:\n4 years of experience working with Google Cloud Platform (GCP)\nStrong proficiency in SQL coding , query tuning, and handling complex data transformations\nHands-on experience with:\nBigQuery\nCloud Run\nCloud Storage\nPub/Sub\nUnderstanding of data pipeline and ETL/ELT workflows in cloud environments\nFamiliarity with containerized services and CI/CD pipelines\nExperience in scripting languages (e.g., Python, Shell) is a plus\nStrong analytical and problem-solving skills",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SUB', 'query optimization', 'GCP', 'Analytical', 'Cloud', 'query', 'cloud storage', 'Analytics', 'SQL coding', 'Python']",2025-06-12 13:45:40
Senior Data Engineer,Jeavio,5 - 10 years,Not Disclosed,[],"We are seeking an experienced Senior Data Engineer to join our team. The ideal candidate will have a strong background in data engineering and AWS infrastructure, with hands-on experience in building and maintaining data pipelines and the necessary infrastructure components. The role will involve using a mix of data engineering tools and AWS services to design, build, and optimize data architecture.\n\nKey Responsibilities:\nDesign, develop, and maintain data pipelines using Airflow and AWS services.\nImplement and manage data warehousing solutions with Databricks and PostgreSQL.\nAutomate tasks using GIT / Jenkins.\nDevelop and optimize ETL processes, leveraging AWS services like S3, Lambda, AppFlow, and DMS.\nCreate and maintain visual dashboards and reports using Looker.\nCollaborate with cross-functional teams to ensure smooth integration of infrastructure components.\nEnsure the scalability, reliability, and performance of data platforms.\nWork with Jenkins for infrastructure automation.\n\nTechnical and functional areas of expertise:\nWorking as a senior individual contributor on a data intensive project\nStrong experience in building high performance, resilient & secure data processing pipelines preferably using Python based stack.\nExtensive experience in building data intensive applications with a deep understanding of querying and modeling with relational databases preferably on time-series data.\nIntermediate proficiency in AWS services (S3, Airflow)\nProficiency in Python and PySpark\nProficiency with ThoughtSpot or Databricks.\nIntermediate proficiency in database scripting (SQL)\nBasic experience with Jenkins for task automation\n\nNice to Have :\nIntermediate proficiency in data analytics tools (Power BI / Tableau / Looker / ThoughSpot)\nExperience working with AWS Lambda, Glue, AppFlow, and other AWS transfer services.\nExposure to PySpark and data automation tools like Jenkins or CircleCI.\nFamiliarity with Terraform for infrastructure-as-code.\nExperience in data quality testing to ensure the accuracy and reliability of data pipelines.\nProven experience working directly with U.S. client stakeholders.\nAbility to work independently and take the lead on tasks.\n\nEducation and experience:\nBachelors or masters in computer science or related fields.\n5+ years of experience\n\nStack/Skills needed:\nDatabricks\nPostgreSQL\nPython & Pyspark\nAWS Stack\nPower BI / Tableau / Looker / ThoughSpot\nFamiliarity with GIT and/or CI/CD tools",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'AWS', 'Data Bricks', 'Python', 'Etl Pipelines', 'Airflow', 'Database Scripting', 'Postgresql', 'Looker', 'SQL']",2025-06-12 13:45:42
Data Engineer (AWS),Neoware Technology Solutions,4 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","Data Engineer (AWS) - Neoware Technology Solutions Private Limited Data Engineer (AWS)\nRequirements\n4 - 10 years of hands-on experience in designing, developing and implementing data engineering solutions.\nStrong SQL development skills, including performance tuning and query optimization.\nGood understanding of data concepts.\nProficiency in Python and a solid understanding of programming concepts.\nHands-on experience with PySpark or Spark Scala for building data pipelines.\nUnderstanding of streaming data pipelines for near real-time analytics.\nExperience with Azure services including Data Factory, Functions, Databricks, Synapse Analytics, Event Hub, Stream Analytics and Data Lake Storage.\nFamiliarity with at least one NoSQL database.\nKnowledge of modern data architecture patterns and industry trends in data engineering.\nUnderstanding of data governance concepts for data platforms and analytical solutions.\nExperience with Git for managing version control for source code.\nExperience with DevOps processes, including experience implementing CI/CD pipelines for data engineering solutions.\nStrong analytical and problem-solving skills.\nExcellent communication and teamwork skills.\nResponsibilities\nAzure Certifications related to Data Engineering are highly preferred.\nExperience with Amazon AppFlow, EKS, API Gateway, NoSQL database services.\nStrong understanding and experience with BI/visualization tools like Power BI.\nChennai, Bangalore\nFull time\n4+ years\nOther positions Principal Architect (Data and Cloud) Development",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Performance tuning', 'Version control', 'GIT', 'NoSQL', 'query optimization', 'Analytical', 'data governance', 'Analytics', 'Python', 'Data architecture']",2025-06-12 13:45:45
Gcp Data Engineer,Saama Technologies,3 - 8 years,Not Disclosed,"['Pune', 'Chennai', 'Coimbatore']","We are looking for immediate joiners only.\nPosition: GCP Data Engineer\nWe are seeking a skilled and experienced GCP Data Engineer to join our dynamic team. The ideal candidate will have a strong background in Google Cloud Platform (GCP), BigQuery, Dataform, and data warehouse concepts. Experience with Airflow/Cloud Composer and cloud computing knowledge will be a significant advantage.\nResponsibilities:\n- Designing, developing, and maintaining data pipelines and workflows on the Google Cloud Platform.",,,,"['Pyspark', 'GCP', 'Python', 'SQL', 'Google Cloud Platforms']",2025-06-12 13:45:48
Azure Data Engineer,JRD Systems,7 - 12 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","Cloud Data Engineer\n\nThe Cloud Data Engineer will be responsible for developing the data lake platform and all applications on Azure cloud. Proficiency in data engineering, data modeling, SQL, and Python programming is essential. The Data Engineer will provide design and development solutions for applications in the cloud.\nEssential Job Functions:\nUnderstand requirements and collaborate with the team to design and deliver projects.\nDesign and implement data lake house projects within Azure.\nDevelop application lifecycle utilizing Microsoft Azure technologies.\nParticipate in design, planning, and necessary documentation.\nEngage in Agile ceremonies including daily standups, scrum, retrospectives, demos, and code reviews.\nHands-on experience with Python/SQL development and Azure data pipelines.\nCollaborate with the team to develop and deliver cross-functional products.\nKey Skills:\na. Data Engineering and SQL\nb. Python\nc. PySpark\nd. Azure Data Lake and ADF\ne. Databricks\nf. CI/CD\ng. Strong communication\nOther Responsibilities:\nDocument and maintain project artifacts.\nMaintain comprehensive knowledge of industry standards, methodologies, processes, and best practices.\nComplete training as required for Privacy, Code of Conduct, etc.\nPromptly report any known or suspected loss, theft, or unauthorized disclosure or use of PI to the General Counsel/Chief Compliance Officer or Chief Information Officer.\nAdhere to the company's compliance program.\nSafeguard the company's intellectual property, information, and assets.\nOther duties as assigned.\nMinimum Qualifications and Job Requirements:\nBachelor's degree in Computer Science.\n7 years of hands-on experience in designing and developing distributed data pipelines.\n5 years of hands-on experience in Azure data service technologies.\n5 years of hands-on experience in Python, SQL, Object-oriented programming, ETL, and unit testing.\nExperience with data integration with APIs, Web services, Queues.\nExperience with Azure DevOps and CI/CD as well as agile tools and processes including JIRA, Confluence.\n*Required: Azure data engineering associate and databricks data engineering certification",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Delta Table', 'Azure Databricks', 'SQL', 'Python', 'SCALA', 'Big Data', 'Kafka', 'Azure Data Lake', 'Spark', 'ETL', 'Data Bricks']",2025-06-12 13:45:50
Azure Data Engineer ( Azure Databricks),Apex One,4 - 8 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","Job Summary\nWe are seeking a skilled Azure Data Engineer with 4 years of overall experience, including at least 2 years of hands-on experience with Azure Databricks (Must). The ideal candidate will have strong expertise in building and maintaining scalable data pipelines and working across cloud-based data platforms.\nKey Responsibilities\nDesign, develop, and optimize large-scale data pipelines using Azure Data Factory, Azure Databricks, and Azure Synapse.\nImplement data lake solutions and work with structured and unstructured datasets in Azure Data Lake Storage (ADLS).\nCollaborate with data scientists, analysts, and engineering teams to design and deliver end-to-end data solutions.\nDevelop ETL/ELT processes and integrate data from multiple sources.\nMonitor, debug, and optimize workflows for performance and cost-efficiency.\nEnsure data governance, quality, and security best practices are maintained.\nMust-Have Skills\n4+ years of total experience in data engineering.\n2+ years of experience with Azure Databricks (PySpark, Notebooks, Delta Lake).\nStrong experience with Azure Data Factory, Azure SQL, and ADLS.\nProficient in writing SQL queries and Python/Scala scripting.\nUnderstanding of CI/CD pipelines and version control systems (e.g., Git).\nSolid grasp of data modeling and warehousing concepts.",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'Azure Data Factory', 'SQL queries', 'PySpark', 'Delta Lake', 'Azure Databricks', 'Notebooks', 'Azure SQL']",2025-06-12 13:45:52
Senior Data Engineer - Azure,Blend360 India,6 - 11 years,Not Disclosed,['Hyderabad'],"As a Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\n3+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar field\nMust have experience e",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-12 13:45:54
Senior Data Engineer : 7+ Years,Jayam Solutions Pvt Ltd - CMMI Level III Company,5 - 9 years,Not Disclosed,['Hyderabad( Madhapur )'],"Job Description:\nPosition: Sr.Data Engineer\nExperience: Minimum 7 years\nLocation: Hyderabad\nJob Summary:\n\nWhat Youll Do\n\nDesign and build efficient, reusable, and reliable data architecture leveraging technologies like Apache Flink, Spark, Beam and Redis to support large-scale, real-time, and batch data processing.\nParticipate in architecture and system design discussions, ensuring alignment with business objectives and technology strategy, and advocating for best practices in distributed data systems.\nIndependently perform hands-on development and coding of data applications and pipelines using Java, Scala, and Python, including unit testing and code reviews.\nMonitor key product and data pipeline metrics, identify root causes of anomalies, and provide actionable insights to senior management on data and business health.\nMaintain and optimize existing datalake infrastructure, lead migrations to lakehouse architectures, and automate deployment of data pipelines and machine learning feature engineering requests.\nAcquire and integrate data from primary and secondary sources, maintaining robust databases and data systems to support operational and exploratory analytics.\nEngage with internal stakeholders (business teams, product owners, data scientists) to define priorities, refine processes, and act as a point of contact for resolving stakeholder issues.\nDrive continuous improvement by establishing and promoting technical standards, enhancing productivity, monitoring, tooling, and adopting industry best practices.\n\nWhat Youll Bring\n\nBachelors degree or higher in Computer Science, Engineering, or a quantitative discipline, or equivalent professional experience demonstrating exceptional ability.\n7+ years of work experience in data engineering and platform engineering, with a proven track record in designing and building scalable data architectures.\nExtensive hands-on experience with modern data stacks, including datalake, lakehouse, streaming data (Flink, Spark), and AWS or equivalent cloud platforms.\nCloud - AWS\nApache Flink/Spark , Redis\nDatabase platform- Databricks.\nProficiency in programming languages such as Java, Scala, and Python(Good to have) for data engineering and pipeline development.\nExpertise in distributed data processing and caching technologies, including Apache Flink, Spark, and Redis.\nExperience with workflow orchestration, automation, and DevOps tools (Kubernetes,git,Terraform, CI/CD).\nAbility to perform under pressure, managing competing demands and tight deadlines while maintaining high-quality deliverables.\nStrong passion and curiosity for data, with a commitment to data-driven decision making and continuous learning.\nExceptional attention to detail and professionalism in report and dashboard creation.\nExcellent team player, able to collaborate across diverse functional groups and communicate complex technical concepts clearly.\nOutstanding verbal and written communication skills to effectively manage and articulate the health and integrity of data and systems to stakeholders.\n\nPlease feel free to contact us: 9440806850\nEmail ID : careers@jayamsolutions.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Apache Flink', 'Redis', 'Spark', 'Python', 'SCALA', 'Ci/Cd', 'Devops', 'AWS']",2025-06-12 13:45:56
Data Engineer - AWS,Happiest Minds Technologies,6 - 10 years,Not Disclosed,"['Pune', 'Bengaluru']","Role & responsibilities\nEssential Skills: Experience: 6 to 10 yrs\n- Technical Expertise: Proficiency in AWS services such as Amazon S3, Redshift, EMR, Glue, Lambda, and Kinesis. Strong skills in SQL and experience with scripting languages like Python or Java.\n- Data Engineering Experience: Hands on experience in building and maintaining data pipelines, data modeling, and working with big data technologies.\n- Problem-Solving Skills: Ability to analyze complex data issues and develop effective solutions to optimize data processing and storage.",,,,"['Data Engineering', 'Pyspark', 'Aws Lambda', 'Amazon Redshift', 'Aws Glue', 'Athena', 'AWS', 'Python', 'SQL']",2025-06-12 13:45:59
Senior Data Engineer,Neal Analytics,10 - 15 years,Not Disclosed,['Mumbai'],"Its fun to work in a company where people truly BELIEVE in what they are doing!\nWere committed to bringing passion and customer focus to the business.\nJob Description:\nAs a Backend (Java) Engineer, you would be part of the team consisting of Scrum Master, Cloud Engineers, AI/ML Engineers, and UI/UX Engineers to build end-to-end Data to Decision Systems.\nMandatory:\n8+ years of demonstrable experience designing, building, and working as a Java Developer for enterprise web applications\nIdeally, this would include the following:\no Expert-level proficiency with Java\no Expert-level proficiency with SpringBoot\nFamiliarity with common databases (RDBMS such as MySQL & NoSQL such as MongoDB) and data warehousing concepts (OLAP, OLTP)\nUnderstanding of REST concepts and building/interacting with REST APIs\nDeep understanding of core backend concepts:\no Develop and design RESTful services and APIs\no Develop functional databases, applications, and servers to support websites on the back end\no Performance optimization and multithreading concepts\no Experience with deploying and maintaining high traffic infrastructure (performance testing is a plus)\nIn addition, the ideal candidate would have great problem-solving skills, and familiarity with code versioning tools such as GitHub\nGood to have:\nFamiliarity with Microsoft Azure Cloud Services (particularly Azure Web App, Storage and VM), or familiarity with AWS (EC2 containers) or GCP Services.\nExperience with Microservices, Messaging Brokers (e.g., RabbitMQ)\nExperience with fine-tuning reverse proxy engines such as Nginx, Apache HTTPD\nIf you like wild growth and working with happy, enthusiastic over-achievers, youll enjoy your career with us!\nNot the right fit? Let us know youre interested in a future opportunity by clicking Introduce Yourself in the top-right corner of the page or create an account to set up email alerts as new job postings become available that meet your interest!",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Backend', 'Multithreading', 'RDBMS', 'MySQL', 'Performance testing', 'OLAP', 'Scrum', 'MongoDB', 'Apache', 'OLTP']",2025-06-12 13:46:01
Senior Data Engineer - AWS,Blend360 India,6 - 10 years,Not Disclosed,['Hyderabad'],"We are looking for an experienced Senior Data Engineer with a strong foundation in Python, SQL, and Spark , and hands-on expertise in AWS, Databricks . In this role, you will build and maintain scalable data pipelines and architecture to support analytics, data science, and business intelligence initiatives. You ll work closely with cross-functional teams to drive data reliability, quality, and performance.\nResponsibilities:\nDesign, develop, and optimize scalable data pipelines using Databricks in AWS such as Glue, S3, Lambda, EMR, Databricks notebooks, workflows and jobs.\nBuilding data lake in WS Databricks.\nBuild and maintain robust ETL/ELT workflows using Python and SQL to handle structured and semi-structured data.\nDevelop distributed data processing solutions using Apache Spark or PySpark .\nPartner with data scientists and analysts to provide high-quality, accessible, and well-structured data.\nEnsure data quality, governance, security, and compliance across pipelines and data stores.\nMonitor, troubleshoot, and improve the performance of data systems and pipelines.\nParticipate in code reviews and help establish engineering best practices.\nMentor junior data engineers and support their technical development.\n\n\nRequirements\nBachelors or masters degree in computer science, Engineering, or a related field.\n5+ years of hands-on experience in data engineering , with at lea",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Version control', 'GIT', 'Workflow', 'Data quality', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-12 13:46:03
Associate Data Engineer,Amgen Inc,0 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role We are seeking a Associate Data Engineer to design, build, and maintain scalable data solutions that drive business insights. You will work with large datasets, cloud platforms (AWS preferred), and big data technologies to develop ETL pipelines, ensure data quality, and support data governance initiatives.\nDevelop and maintain data pipelines, ETL/ELT processes, and data integration solutions.\nDesign and implement data models, data dictionaries, and documentation for accuracy and consistency.\nEnsure data security, privacy, and governance standard processes.\nUse Databricks, Apache Spark (PySpark, SparkSQL), AWS, Redshift, for scalable data processing.\nCollaborate with cross-functional teams to understand data needs and deliver actionable insights.\nOptimize data pipeline performance and explore new tools for efficiency.\nFollow best practices in coding, testing, and infrastructure-as-code (CI/CD, version control, automated testing).\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. Strong problem-solving, critical thinking, and communication skills.\nAbility to collaborate effectively in a team setting.\nProficiency in SQL, data analysis tools, and data visualization.\nHands-on experience with big data technologies (Databricks, Apache Spark, AWS, Redshift ).\nExperience with ETL tools, workflow orchestration, and performance tuning for big data.\nBasic Qualifications:\nBachelors degree and 0 to 3 years of experience OR Diploma and 4 to 7 years of experience in Computer science, IT or related field.\nPreferred Qualifications:\nKnowledge of data modeling, warehousing, and graph databases\nExperience with Python, SageMaker, and cloud data platforms.\nAWS Certified Data Engineer or Databricks certification preferred.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'data analysis', 'data modeling', 'data warehousing', 'data visualization', 'Databricks', 'ETL', 'AWS', 'SQL', 'Apache Spark', 'Python']",2025-06-12 13:46:05
Graph Engineer- Data Science,HARMAN,4 - 9 years,Not Disclosed,['Bengaluru'],"Job Description\nIntroduction: Digital Transformation Solutions (DTS)\n.\nExtensive experience in defining, developing, and implementing security software, ideally with a strong embedded firmware development background\nAbout the Role\nThis position offers an opportunity to work in a globally distributed team where you will get a unique opportunity of personal development in a multi-cultural environment. You will also get a challenging environment to develop expertise in the technologies useful in the industry.",,,,"['Computer science', 'Product quality', 'UML', 'XML', 'Relationship', 'Javascript', 'HTML', 'Oracle', 'Automotive', 'Python']",2025-06-12 13:46:07
"Associate Director, Data Science/Software Engineering",ATT Communication Services,10 - 15 years,Not Disclosed,['Bengaluru'],"Associate Director, Data Science/Software Engineering:\nAT&T is one of the leading service providers in the telecommunication sector and propelling it into the data and AI driven era is powered by CDO (Chief Data Office) . CDO is empowering AT&T, through execution, self-service, and as a data and AI center of excellence, to unlock transformative insights and actions that drive value for the company and its customers.\nEmployees in CDO imagine, innovate, and unlock data & AI driven insights and actions that create value for our customers and the enterprise. Part of the work, we govern data collection and use, mitigate for potential bias in machine learning models, and encourage an enterprise culture of responsible AI.\nAT&T s Chief Data Office (CDO) is harnessing data and making AT&T s data assets and ground-breaking AI functionality accessible to employees across the firm. In addition, our talented employees are a significant component that contributes to AT&T s place as the U.S. company with the sixth most AI-related patents. CDO also maintains academic and tech partnerships to cultivate the next generation of experts in statistics and machine learning, statistical computing, data visualization, text mining, time series modelling, data stream and database management, data quality and anomaly detection, data privacy, and more.\nWe are looking for an accomplished and visionary professional for the role of Associate Director, Data Science/Software Engineering to join our team and lead the development of cutting-edge software solutions. This is a hands-on leadership position that requires the fine balance of supervising and leading people while providing significant technical contributions to the projects you will be responsible for. As a key technical leader, you will leverage your expertise in full-stack development, DevOps best practices, Data analysis, AI/ML and Generative AI to lead your team in creating scalable, reliable, and efficient systems.\nThis role demands a strategic thinker and hands-on contributor who can work across multiple teams, drive innovation, and ensure technical excellence. You will be instrumental in shaping the technical roadmap, mentoring teams, and delivering transformative solutions that align with business objectives.\nKey Responsibilities:\nTechnical Leadership:\nDefine and drive the technical vision and architecture for scalable, resilient, and secure full-stack applications utilizing data powered insights.\nLead end-to-end software development projects from concept to deployment and maintenance.\nCollaborate with cross-functional teams to translate business requirements into technical solutions.\nServe as a mentor and technical advisor to engineering teams, fostering a culture of innovation and excellence.\nFull-Stack Development:\nDesign and implement scalable and high-performance web applications using modern front-end and back-end frameworks (e.g., React, Angular, Node.js, Python, Java).\nDevelop modular and reusable APIs (RESTful or GraphQL) with an emphasis on maintainability and performance.\nEnsure seamless integration of front-end and back-end systems while maintaining best practices for UI/UX design.\nOptimize database structures and queries for both relational (e.g., MySQL, PostgreSQL) and non-relational (e.g., MongoDB, DynamoDB) databases.\nDevOps and Automation:\nArchitect and implement CI/CD pipelines to streamline build, test, and deployment processes.\nEnsure seamless deployment and scalability of applications through containerization tools (e.g., Docker) and orchestration platforms (e.g., Kubernetes).\nLeverage infrastructure-as-code solutions (e.g., Terraform, Ansible) to automate infrastructure provisioning and management.\nMonitor application performance, troubleshoot issues, and ensure high availability through tools like Prometheus, Grafana, or New Relic.\nShell Scripting and Automation:\nDevelop and maintain shell scripts to automate routine tasks, system monitoring, and application deployments.\nDebug and troubleshoot production issues using scripting techniques to ensure minimal downtime.\nEnhance system efficiency by automating log analysis, error detection, and reporting.\nStrategic Contribution:\nCollaborate with stakeholders to align technical priorities with business goals.\nEvaluate emerging technologies and tools to recommend and implement solutions that advance the organization s technical capabilities.\nEstablish and enforce software engineering best practices, ensuring robust security, scalability, and maintainability.\nQualifications:\nEducation:\nBachelor s or Master s degree in Computer Science, Software Engineering, or a related field. A Ph.D. is a plus.\nExperience:\n13+ years of experience in software engineering, including hands-on experience with full-stack development and DevOps practices.\nProven track record of delivering large-scale, high-impact software solutions in a leadership capacity.\nTechnical Expertise:\nAdvanced proficiency in front-end frameworks (React, Angular, or Vue.js) and back-end technologies (Node.js, Python, Java, Go, etc.).\nStrong experience with DevOps tools (Jenkins, GitLab CI/CD, Docker, Kubernetes).\nDeep understanding of cloud platforms (AWS, Azure, GCP), including architecture and deployment strategies.\nSolid grasp of database technologies (SQL and NoSQL) and optimization techniques.\nProficiency in writing, debugging, and maintaining shell scripts for automation and system monitoring.\nStrong knowledge of microservices architecture, API gateways, and distributed systems.\nSoft Skills:\nExceptional problem-solving and critical-thinking abilities.\nStrong leadership and mentoring skills, with the ability to inspire and guide teams.\nExcellent communication skills, both written and verbal, to collaborate effectively with technical and non-technical stakeholders.\nStrategic mindset, capable of balancing technical depth with business impact.\nPreferred Qualifications:\nExperience with serverless computing frameworks (e.g., AWS Lambda).\nCertifications in cloud platforms (e.g., AWS Certified Solutions Architect, Azure DevOps Engineer Expert).\nKnowledge of security best practices in software development and DevOps.\n#DataEngineering\nLocation:\nIND:KA:Bengaluru / Innovator Building, Itpb, Whitefield Rd - Adm: Intl Tech Park, Innovator Bldg\nJob ID R-66889 Date posted 05/14/2025",Industry Type: Telecom / ISP,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Data analysis', 'Front end', 'Postgresql', 'MySQL', 'Shell scripting', 'Telecommunication', 'SQL', 'Python']",2025-06-12 13:46:09
Associate Data Engineer,Amgen Inc,0 - 2 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for an Associate Data Engineer with deep expertise in writing data pipelines to build scalable, high-performance data solutions. The ideal candidate will be responsible for developing, optimizing and maintaining complex data pipelines, integration frameworks, and metadata-driven architectures that enable seamless access and analytics. This role prefers deep understanding of the big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nData Engineer who owns development of complex ETL/ELT data pipelines to process large-scale datasets\nContribute to the design, development, and implementation of data pipelines, ETL/ELT processes, and data integration solutions\nEnsuring data integrity, accuracy, and consistency through rigorous quality checks and monitoring\nExploring and implementing new tools and technologies to enhance ETL platform and performance of the pipelines\nProactively identify and implement opportunities to automate tasks and develop reusable frameworks\nEager to understand the biotech/pharma domains & build highly efficient data pipelines to migrate and deploy complex data across systems\nWork in an Agile and Scaled Agile (SAFe) environment, collaborating with cross-functional teams, product owners, and Scrum Masters to deliver incremental value\nUse JIRA, Confluence, and Agile DevOps tools to manage sprints, backlogs, and user stories.\nSupport continuous improvement, test automation, and DevOps practices in the data engineering lifecycle\nCollaborate and communicate effectively with the product teams, with cross-functional teams to understand business requirements and translate them into technical solutions\nMust-Have Skills:\nExperience in Data Engineering with a focus on Databricks, AWS, Python, SQL, and Scaled Agile methodologies\nProficiency & Strong understanding of data processing and transformation of big data frameworks (Databricks, Apache Spark, Delta Lake, and distributed computing concepts)\nStrong understanding of AWS services and can demonstrate the same\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery, and DevOps practices\nGood-to-Have Skills:\nData Engineering experience in Biotechnology or pharma industry\nExposure to APIs, full stack development\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\nBachelors degree and 2 to 5 + years of Computer Science, IT or related field experience\nOR\nMasters degree and 1 to 4 + years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Maven', 'test automation', 'data engineering lifecycle', 'Scaled Agile methodologies', 'JIRA', 'SQL', 'Apache Spark', 'Jenkins', 'Agile DevOps tools', 'ETL platform', 'Confluence', 'Scaled Agile', 'Delta Lake', 'Agile', 'Databricks', 'AWS', 'Python']",2025-06-12 13:46:12
Snowflake Data Engineer,Tredence,3 - 8 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\n\nDesign, build, and maintain scalable data pipelines using DBT and Airflow.\nDevelop and optimize SQL queries and data models in Snowflake.\nImplement ETL/ELT workflows, ensuring data quality, performance, and reliability.\nWork with Python for data processing, automation, and integration tasks.\nHandle JSON data structures for data ingestion, transformation, and APIs.\nLeverage AWS services (e.g., S3, Lambda, Glue, Redshift) for cloud-based data solutions. Collaborate with data analysts, engineers, and business teams to deliver high-quality data products.",,,,"['Snowflake', 'DBT', 'SQL']",2025-06-12 13:46:14
Urgent hiring For Cloud Data Engineer,Wowjobs,7 - 10 years,30-45 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities\n*  Design, Build, and Maintain ETL Pipelines: Develop robust, scalable, and efficient ETL workflows to ingest, transform, and load data into distributed data products within the Data Mesh architecture.\n*   Data Transformation with dbt: Use dbt to build modular, reusable transformation workflows that align with the principles of Data Products.\n*   Cloud Expertise: Leverage Google Cloud Platform (GCP) services such as BigQuery, Cloud Storage, Pub/Sub, and Dataflow to implement highly scalable data solutions.\n*   Data Quality & Governance: Enforce strict data quality standards by implementing validation checks, anomaly detection mechanisms, and monitoring frameworks.\n*   Performance Optimization: Continuously optimize ETL pipelines for speed, scalability, and cost efficiency.\n*   Collaboration & Ownership: Work closely with data product owners, BI developers, and stakeholders to understand requirements and deliver on expectations. Take full ownership of your deliverables.\n*   Documentation & Standards: Maintain detailed documentation of ETL workflows, enforce coding standards, and adhere to best practices in data engineering.\n*   Troubleshooting & Issue Resolution: Proactively identify bottlenecks or issues in pipelines and resolve them quickly with minimal disruption.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python coding', 'Cloud data engineer', 'ETL Workflow']",2025-06-12 13:46:16
AM Client Workflows & Ref Data - Associate - Software Engineering,Goldman Sachs,2 - 5 years,Not Disclosed,['Bengaluru'],"Work with a global team of highly motivated platform engineers and software developers building integrated architectures for secure, scalable infrastructure services serving a diverse set of use cases.\nPartner with colleagues from across technology and risk to ensure an outstanding platform is delivered.\nHelp to provide frictionless integration with the firm s runtime, deployment and SDLC technologies.\nCollaborate on feature design and problem solving.\nHelp to ensure reliability, define, measure, and meet service level objectives.\nQuality coding & integration, testing, release, and demise of software products supporting AWM functions.\nEngage in quality assurance and production troubleshooting.\nHelp to communicate and promote best practices for software engineering across the Asset Management tech stack.\nBasic Qualifications\nA strong grounding in software engineering concepts and implementation of architecture design patterns.\nA good understanding of multiple aspects of software development in microservices architecture, full stack development experience, Identity / access management and technology risk.\nSound SDLC and practices and tooling experience - version control, CI/CD and configuration management tools.\nAbility to communicate technical concepts effectively, both written and orally, as we'll as interpersonal skills required to collaborate effectively with colleagues across diverse technology teams.\nExperience meeting demands for high availability and scalable system requirements.\nAbility to reason about performance, security, and process interactions in complex distributed systems.\nAbility to understand and effectively debug both new and existing software.\nExperience with metrics and monitoring tooling, including the ability to use metrics to rationally derive system health and availability information.\nExperience in auditing and supporting software based on sound SRE principles.\nPreferred Qualifications\n3+ Years of Experience using and/or supporting Java based frameworks & SQL / NOSQL data stores.\nExperience with deploying software to containerized environments - Kubernetes/Docker.\nScripting skills using Python, Shell or bash.\nExperience with Terraform or similar infrastructure-as-code platforms.\nExperience building services using public cloud providers such as AWS, Azure or GCP.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Manager Quality Assurance', 'Coding', 'Configuration management', 'Investment banking', 'Asset management', 'Troubleshooting', 'SDLC', 'Monitoring', 'SQL', 'Python']",2025-06-12 13:46:18
Director - Data Engineering,Blend360 India,10 - 15 years,Not Disclosed,['Bengaluru'],"We are seeking a strategic Director of Data & AI Engineering to lead the growth and evolution of our data engineering function. This role will play a pivotal part in designing scalable platforms, enabling advanced AI and ML applications, and building a world-class engineering team. The ideal candidate is both a seasoned technical leader and a visionary strategist who thrives at the intersection of innovation, execution, and impact. You will collaborate with internal and client teams to develop systems and infrastructure that power intelligent products and data-driven decision-making. You will also build and mentor a high-performing team of Data and AI Engineers, foster a modern data culture, and champion best practices for scalable architecture, AI integration, and engineering excellence.\nResponsibilities:\nSolution Design & Engineering Leadership\nArchitect and build scalable, high-performance data and AI pipelines using tools such as Spark, PySpark, SQL, Python, DBT, Airflow, and cloud-native platforms (AWS, GCP, or Azure).\nLead the design of hybrid/on-prem data platforms, incorporating security, governance, and performance optimization.\nStrategic Client & Stakeholder Engagement\nServe as a trusted technical advisor to internal and external stakeholders.\nTranslate complex business needs into practical engineering solutions and oversee the end-to-end delivery lifecycle.\nAI-Driven Productivity & Innovation\nIntroduce and scale AI-driven tools and practices to accelerate development and enhance data quality, resilience, and maintainability.\nChampion the adoption of generative AI and foundation models to enable intelligent automation and insight generation.\nGrowth & Team Leadership\nBuild, lead, and inspire a diverse team of Data Engineers, ML Engineers, and AI Specialists.\nSet a clear vision and goals, provide mentorship, and cultivate a strong engineering culture and standards.\nPlatform and Data Strategy\nLead initiatives to modernize data infrastructure, improve data discoverability, and support real-time analytics and experimentation.\nCollaborate cross-functionally to shape product data strategies and influence the overall AI roadmap.\nPresales & Business Development Support\nPartner with sales and solution teams to craft compelling proposals, technical solutions, and client presentations.\nRepresent the engineering function in client discussions, workshops, and RFP responses to articulate value and differentiation.\nSupport opportunity scoping, estimation, and roadmap planning for prospective engagements.\n\n\n10+ years of experience in data engineering, AI/ML engineering, or product/platform engineering.\nProven track record of leading high-performing teams and managing senior engineers and managers.\nBachelor s or masters degre",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'GCP', 'Business Development Manager', 'Social media', 'Data quality', 'RFP', 'Analytics', 'SQL', 'Python']",2025-06-12 13:46:20
Sr Data Engineer - Fully Remote & Immediate Opportunity,Zealogics.com,10 - 15 years,Not Disclosed,[],"10 yrs of exp working in cloud-native data (Azure Preferred),Databricks, SQL,PySpark, migrating from Hive Metastore to Unity Catalog, Unity Catalog, implementing Row-Level Security (RLS), metadata-driven ETL design patterns,Databricks certifications",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Azure', 'Metadata', 'Data Bricks', 'Unity Catalog', 'ETL design', 'SQL']",2025-06-12 13:46:22
Specialist Data/AI Engineering,ATT Communication Services,4 - 9 years,Not Disclosed,['Bengaluru'],"Key Roles and Responsibilities\nDevelop, enhance, and support assigned applications, ensuring seamless functionality and high performance.\nProvide subject matter expertise, acting as a go-to resource for application-specific knowledge.\nCollaborate and communicate effectively with team members, stakeholders, and end-users.\nTroubleshoot issues, monitor performance, and optimize processes for continuous improvement.\nStay current with industry trends and share best practices with the team.\nAdhere to company policies, procedures, and security requirements while maintaining compliance standards.\nFlexible with shifts and occasional weekend support.\nKey Competencies\nFull life-cycle experience on enterprise software development projects.\nExperience in relational databases/ data marts/data warehouses and complex SQL programming.\nExtensive experience in ETL, shell or python scripting, data modelling, analysis, and preparation\nExperience in Unix/Linux system, files systems, shell scripting.\nGood to have knowledge on any cloud platforms like Azure, Databricks etc.\nGood to have experience in BI Reporting tools Power BI\nGood problem-solving and analytical skills used to resolve technical problems.\nMust possess a good understanding of Compliance and Security Standards (preferably US standards).\nAbility to work independently but must be a team player. Should be able to drive business decisions and take ownership of their work.\nExperience in presentation design, development, delivery, and good communication skills to present analytical results and recommendations for action-oriented data driven decisions and associated operational and financial impacts.\nRequired/Desired Skills\nApplication: Databricks\nLanguages: Python, Shell Scripting, PLSQL, SQL\nCloud Technologies: Azure.\nTools: Jupyter Notebooks, SQL Developer, Postman, IDE.\nDatabase: Oracle, Snowflake and SQL Server.\nDevops: Jenkins, Kubernetes and Docker.\nBachelor s degree in computer science, Information Systems or related field.\n4+ years of experience in working in Engineering or Development roles with Compliance Standards\n4+ years of experience building high transaction defensive web applications and Python applications\n2+ years of experience in cloud technologies: AWS, Azure, OpenStack, Ansible, Chef or Terraform\n2+ years of experience in creating application for container services Docker, Kubernetes,\n2+ years of experience in build and CICD technologies: GitHub, Maven, Jenkins, Nexus or Sonar\nProficiency in Unix/Linux command line\n1+ years of experience in building applications using Large Language Models.\n1+ years of experience in building intelligent automation using Power Automate\n1+ years of experience in using Agentic framework\n1+ years of experience in building accurate prompts for AI tools.\nExpert knowledge and experience working with asynchronous message processing, stream processing and event driven computing.\nExperience working within Agile/Scrum/Kanban development team\nFamiliarity with HTML5, JavaScript frameworks, and CSS3\nCertified in Java, Spring or Azure technologies\nExcellent written and verbal communication skills with demonstrated ability to present complex technical information in a clear manner to peers, developers, and senior leaders\nEducation & Qualifications\nUniversity Degree in Computer Science and/or Analytics\nMinimum Experience required: 6-9 years in relational database design & development, ETL development, GenAI\nAdditional Details\nShift timing (if any): 12.30 to 9.30 IST(Bangalore)\nWork mode: Hybrid (3 days mandatory in office)\nLocation: Bangalore\n#DataPlatform\n#DataScience\nJob ID R-61633 Date posted 06/03/2025",Industry Type: Telecom / ISP,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Computer science', 'Automation', 'Linux', 'Database design', 'Shell scripting', 'Javascript', 'Design development', 'SQL', 'Python']",2025-06-12 13:46:25
Senior Data Engineer,Binary Infoways,5 - 9 years,12-19.2 Lacs P.A.,['Hyderabad'],"Responsibilities:\n* Design, develop & maintain data pipelines using Airflow, Python & SQL.\n* Optimize performance through Spark & Splunk analytics.\n* Collaborate with cross-functional teams on big data initiatives.\n* AWS",Industry Type: BPM / BPO,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Airflow', 'Big Data Technologies', 'ETL', 'AWS', 'Python', 'Glue', 'Snowflake', 'Spark', 'Splunk', 'SQL']",2025-06-12 13:46:27
Data Engineer-Having Stratup-Mid-Size company Exp.@ Bangalore_Urgent,"As a leader in this space, we deliver wo...",8 - 13 years,Not Disclosed,['Bengaluru'],"Data Engineer\n\nLocation: Bangalore - Onsite\nExperience: 8 - 15 years\nType: Full-time\n\nRole Overview\n\nWe are seeking an experienced Data Engineer to build and maintain scalable, high-performance data pipelines and infrastructure for our next-generation data platform. The platform ingests and processes real-time and historical data from diverse industrial sources such as airport systems, sensors, cameras, and APIs. You will work closely with AI/ML engineers, data scientists, and DevOps to enable reliable analytics, forecasting, and anomaly detection use cases.\nKey Responsibilities\nDesign and implement real-time (Kafka, Spark/Flink) and batch (Airflow, Spark) pipelines for high-throughput data ingestion, processing, and transformation.\nDevelop data models and manage data lakes and warehouses (Delta Lake, Iceberg, etc) to support both analytical and ML workloads.\nIntegrate data from diverse sources: IoT sensors, databases (SQL/NoSQL), REST APIs, and flat files.\nEnsure pipeline scalability, observability, and data quality through monitoring, alerting, validation, and lineage tracking.\nCollaborate with AI/ML teams to provision clean and ML-ready datasets for training and inference.\nDeploy, optimize, and manage pipelines and data infrastructure across on-premise and hybrid environments.\nParticipate in architectural decisions to ensure resilient, cost-effective, and secure data flows.\nContribute to infrastructure-as-code and automation for data deployment using Terraform, Ansible, or similar tools.\n\n\nQualifications & Required Skills\n\nBachelors or Master’s in Computer Science, Engineering, or related field.\n6+ years in data engineering roles, with at least 2 years handling real-time or streaming pipelines.\nStrong programming skills in Python/Java and SQL.\nExperience with Apache Kafka, Apache Spark, or Apache Flink for real-time and batch processing.\nHands-on with Airflow, dbt, or other orchestration tools.\nFamiliarity with data modeling (OLAP/OLTP), schema evolution, and format handling (Parquet, Avro, ORC).\nExperience with hybrid/on-prem and cloud platforms (AWS/GCP/Azure) deployments.\nProficient in working with data lakes/warehouses like Snowflake, BigQuery, Redshift, or Delta Lake.\nKnowledge of DevOps practices, Docker/Kubernetes, Terraform or Ansible.\nExposure to data observability, data cataloging, and quality tools (e.g., Great Expectations, OpenMetadata).\nGood-to-Have\nExperience with time-series databases (e.g., InfluxDB, TimescaleDB) and sensor data.\nPrior experience in domains such as aviation, manufacturing, or logistics is a plus.\n\nRole & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['aviation', 'Data Modeling', 'Python', 'OLAP', 'Cloud', 'ORC', 'logistics', 'Avro', 'Terraform', 'Snowflake', 'manufacturing', 'AWS', 'Parquet', 'Java', 'Azure', 'BigQuery', 'Data', 'Redshift', 'SQL', 'TimescaleDB', 'GCP', 'InfluxDB', 'dbt', 'Ansible', 'OLTP', 'Kubernetes']",2025-06-12 13:46:30
Data Engineer (Azure),Neoware Technology Solutions,4 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","Data Engineer (Azure) - Neoware Technology Solutions Private Limited Data Engineer (Azure)\nRequirements\n4 - 10 years of hands-on experience in designing, developing and implementing data engineering solutions.\nStrong SQL development skills, including performance tuning and query optimization.\nGood understanding of data concepts.\nProficiency in Python and a solid understanding of programming concepts.\nHands-on experience with PySpark or Spark Scala for building data pipelines.\nEnsure data consistency and address ambiguities or inconsistencies across datasets.\nUnderstanding of streaming data pipelines for near real-time analytics.\nExperience with Azure services including Data Factory, Functions, Databricks, Synapse Analytics, Event Hub, Stream Analytics and Data Lake Storage.\nFamiliarity with at least one NoSQL database.\nKnowledge of modern data architecture patterns and industry trends in data engineering.\nUnderstanding of data governance concepts for data platforms and analytical solutions.\nExperience with Git for managing version control for source code.\nExperience with DevOps processes, including experience implementing CI/CD pipelines for data engineering solutions.\nStrong analytical and problem-solving skills.\nExcellent communication and teamwork skills.\nResponsibilities\nAzure Certifications related to Data Engineering are highly preferred.\nExperience with Azure Kubernetes Service (AKS), Container Apps and API Management.\nStrong understanding and experience with BI/visualization tools like Power BI.\nChennai, Bangalore\nFull time\n4+ years\nOther positions\nChennai / Bangalore / Mumbai\n3+ years\nPrincipal Architect (Data and Cloud) Development",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Performance tuning', 'Version control', 'GIT', 'query optimization', 'NoSQL', 'Analytical', 'SCALA', 'Analytics', 'Python', 'Data architecture']",2025-06-12 13:46:33
Azure Data Engineer/Lead/Architect (5 - 20 Years) (Pan India Location),Allegis Group,5 - 10 years,Not Disclosed,[],"Azure Data Engineer/Lead/Architect (5 - 20 Years) (Pan India Location)\nJob Location : Hyderabad / Bangalore / Chennai / Kolkata / Noida/ Gurgaon / Pune / Indore / Mumbai\n\n\n5 -20 years of relevant hands on development experience. And 4+ years as Azure Data Engineering role\nProficient in Azure technologies like ADB, ADF, SQL(capability of writing complex SQL queries), ADB, PySpark, Python, Synapse, Delta Tables, Unity Catalog\nHands on in Python, PySpark or Spark SQL\nHands on in Azure Analytics and DevOps\nTaking part in Proof of Concepts (POCs) and pilot solutions preparation\nAbility to conduct data profiling, cataloguing, and mapping for technical design and construction of technical data flows\nExperience in business processing mapping of data and analytics solutions",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Analytics', 'Azure Data Engineering', 'Azure Databricks', 'Devops', 'Python', 'Azure Data Factory', 'Pyspark', 'Azure', 'Adb']",2025-06-12 13:46:35
Senior Data Engineer,Conviction HR,8 - 10 years,Not Disclosed,"['Kolkata', 'Hyderabad', 'Pune( Malad )']","Must have -Azure Data Factory (Mandatory). Azure Databricks, Pyspark and Python and advance SQL Azure eco-system. 1) Advanced SQL Skills. 2)Data Analysis. 3) Data Models. 4) Python (Desired). 5) Automation - Experience required : 8 to 10 years.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Data Engineering', 'Python', 'Azure Databricks', 'Data Modeling', 'Data Bricks', 'SQL']",2025-06-12 13:46:38
Gcp Data Engineer,Royal Cyber,9 - 14 years,Not Disclosed,[],"Minimum 7+ years in data engineering with 5+ years of hands-on experience on GCP.\nProven track record with tools and services like BigQuery, Cloud Composer (Apache Airflow), Cloud Functions, Pub/Sub, Cloud Storage, Dataflow, and IAM/VPC.\nDemonstrated expertise in Apache Spark (batch and streaming), PySpark, and building scalable API integrations.\nAdvanced Airflow skills including custom operators, dynamic DAGs, and workflow performance tuning.\nCertifications\nGoogle Cloud Professional Data Engineer certification preferred.\nKey Skills\nMandatory Technical Skills\nAdvanced Python (PySpark, Pandas, pytest) for automation and data pipelines.\nStrong SQL with experience in window functions, CTEs, partitioning, and optimization.\nProficiency in GCP services including BigQuery, Dataflow, Cloud Composer, Cloud Functions, and Cloud Storage.\nHands-on with Apache Airflow, including dynamic DAGs, retries, and SLA enforcement.\nExpertise in API data ingestion, Postman collections, and REST/GraphQL integration workflows.\nFamiliarity with CI/CD workflows using Git, Jenkins, or Bitbucket.\nExperience with infrastructure security and governance using IAM and VPC.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Part Time, Temporary/Contractual","['GCP', 'Bigquery', 'Google Cloud Platforms', 'Cloud Storage', 'Data Flow']",2025-06-12 13:46:40
"Data Engineer - Snowflake, Azure Data Factory (ADF)",Suzva Software Technologies,0 - 1 years,Not Disclosed,['Mumbai'],"We are seeking an experienced Data Engineer to join our team for a 6-month contract assignment. The ideal candidate will work on data warehouse development, ETL pipelines, and analytics enablement using Snowflake, Azure Data Factory (ADF), dbt, and other tools.\n\nThis role requires strong hands-on experience with data integration platforms, documentation, and pipeline optimizationespecially in cloud environments such as Azure and AWS.\n\n#KeyResponsibilities\nBuild and maintain ETL pipelines using Fivetran, dbt, and Azure Data Factory\n\nMonitor and support production ETL jobs\n\nDevelop and maintain data lineage documentation for all systems\n\nDesign data mapping and documentation to aid QA/UAT testing\n\nEvaluate and recommend modern data integration tools\n\nOptimize shared data workflows and batch schedules\n\nCollaborate with Data Quality Analysts to ensure accuracy and integrity of data flows\n\nParticipate in performance tuning and improvement recommendations\n\nSupport BI/MDM initiatives including Data Vault and Data Lakes\n\n#RequiredSkills\n7+ years of experience in data engineering roles\n\nStrong command of SQL, with 5+ years of hands-on development\n\nDeep experience with Snowflake, Azure Data Factory, dbt\n\nStrong background with ETL tools (Informatica, Talend, ADF, dbt, etc.)\n\nBachelor's in CS, Engineering, Math, or related field\n\nExperience in healthcare domain (working with PHI/PII data)\n\nFamiliarity with scripting/programming (Python, Perl, Java, Linux-based environments)\n\nExcellent communication and documentation skills\n\nExperience with BI tools like Power BI, Cognos, etc.\n\nOrganized, self-starter with strong time-management and critical thinking abilities\n\n#NiceToHave\nExperience with Data Lakes and Data Vaults\n\nQA & UAT alignment with clear development documentation\nMulti-cloud experience (especially Azure, AWS)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Java', 'Azure', 'Power BI', 'UAT', 'Perl', 'QA', 'Azure Data Factory', 'Linux', 'Cognos', 'Snowflake', 'ETL', 'AWS', 'Python']",2025-06-12 13:46:43
"Data Engineer( Python, AWS, Databricks, EKS, Airflow)",Banking,5 - 9 years,Not Disclosed,['Bengaluru'],"Exprence 5-8 Years\nLocation - Bangalore\nMode C2H\n\nHands on data engineering experience.\nHands on experience with Python programming\nHands-on Experience with AWS & EKS\nWorking knowledge of Unix, Databases, SQL\nWorking Knowledge on Databricks\nWorking Knowledge on Airflow and DBT",Industry Type: Investment Banking / Venture Capital / Private Equity,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Airflow', 'Data Engineering', 'AWS', 'Python', 'SQL', 'Databricks', 'Eks']",2025-06-12 13:46:45
Aws Data Engineer,Hiring for Leading MNC Company!!,8 - 13 years,Not Disclosed,['Bengaluru'],"Warm Greetings from SP Staffing!!\n\nRole:AWS Data Engineer\nExperience Required :8 to 15 yrs\nWork Location :Bangalore\n\nRequired Skills,\n\nTechnical knowledge of data engineering solutions and practices. Implementation of data pipelines using tools like EMR, AWS Glue, AWS Lambda, AWS Step Functions, API Gateway, Athena\nProficient in Python and Spark, with a focus on ETL data processing and data engineering practices.\n\nInterested candidates can send resumes to nandhini.spstaffing@gmail.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ETL', 'AWS', 'Pyspark', 'python', 'EMR', 'Aws Glue', 'Aws Emr', 'AWS Data Engineer', 'Aws Lambda', 'Lakehouse', 'spark', 'Data Engineer', 'Athena', 'gateway']",2025-06-12 13:46:48
Software Engineer (Java + Big Data),Impetus Technologies,5 - 7 years,Not Disclosed,['Chennai'],"Job: Software Developer (Java + Big Data)\nLocation: Indore\nYears of experience: 5-7 years\n\nRequisition Description\n1. Problem solving and analytical skills\n2. Good verbal and written communication skills\n\nRoles and Responsibilities\n\n1. Design and develop high performance, scale-able applications with Java + Bigdata  as minimum required skill .\nJava, Microservices , Spring boot, API ,Bigdata-Hive, Spark, Pyspark\n2. Build and maintain efficient data pipelines to process large volumes of structured and unstructured data.\n3. Develop micro-services, API and distributed systems\n4. Worked on Spark, HDFS, CEPh, Solr/Elastic search, Kafka, Deltalake\n5. Mentor and Guide junior members",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Java', 'Hive', 'Big Data', 'Spring Boot', 'Microservices', 'Hdfs', 'Spark Streaming']",2025-06-12 13:46:50
"AI/ML Engineer (Specializing in NLP/ML, Large Data Processing,",Synechron,8 - 10 years,Not Disclosed,"['Pune', 'Hinjewadi']","job requisition idJR1027361\n\nJob Summary\nSynechron seeks a highly skilled AI/ML Engineer specializing in Natural Language Processing (NLP), Large Language Models (LLMs), Foundation Models (FMs), and Generative AI (GenAI). The successful candidate will design, develop, and deploy advanced AI solutions, contributing to innovative projects that transform monolithic systems into scalable microservices integrated with leading cloud platforms such as Azure, Amazon Bedrock, and Google Gemini. This role plays a critical part in advancing Synechrons capabilities in cutting-edge AI technologies, enabling impactful business insights and product innovations.\n\nSoftware\n\nRequired Proficiency:\nPython (core librariesTensorFlow, PyTorch, Hugging Face transformers, etc.)\nCloud platformsAzure, AWS, Google Cloud (familiarity with AI/ML services)\nContainerizationDocker, Kubernetes\nVersion controlGit\nData management toolsSQL, NoSQL databases (e.g., MongoDB)\nModel deployment and MLOps toolsMLflow, CI/CD pipelines, monitoring tools\nPreferred\n\nSkills:\nExperience with cloud-native AI frameworks and SDKs\nFamiliarity with AutoML tools\nAdditional programming languages (e.g., Java, Scala)\nOverall Responsibilities\nDesign, develop, and optimize NLP models, including advanced LLMs and Foundation Models, for diverse business use cases.\nLead the development of large data pipelines for training, fine-tuning, and deploying models on big data platforms.\nArchitect, implement, and maintain scalable AI solutions in line with MLOps best practices.\nTransition legacy monolithic AI systems into modular, microservices-based architectures for scalability and maintainability.\nBuild end-to-end AI applications from scratch, including data ingestion, model training, deployment, and integration.\nImplement retrieval-augmented generation techniques for enhanced context understanding and response accuracy.\nConduct thorough testing, validation, and debugging of AI/ML models and pipelines.\nCollaborate with cross-functional teams to embed AI capabilities into customer-facing and enterprise products.\nSupport ongoing maintenance, monitoring, and scaling of deployed AI systems.\nDocument system designs, workflows, and deployment procedures for compliance and knowledge sharing.\nPerformance Outcomes:\nProduction-ready AI solutions delivering high accuracy and efficiency.\nRobust data pipelines supporting training and inference at scale.\nSeamless integration of AI models with cloud infrastructure.\nEffective collaboration leading to innovative AI product deployment.\nTechnical Skills (By Category)\n\nProgramming Languages:\nEssential: Python (TensorFlow, PyTorch, Hugging Face, etc.)\nPreferred: Java, Scala\nDatabases/Data Management:\nSQL (PostgreSQL, MySQL), NoSQL (MongoDB, DynamoDB)\nCloud Technologies:\nAzure AI, AWS SageMaker, Bedrock, Google Cloud Vertex AI, Gemini\nFrameworks and Libraries:\nTransformers, Keras, scikit-learn, XGBoost, Hugging Face engines\nDevelopment Tools & Methodologies:\nDocker, Kubernetes, Git, CI/CD pipelines (Jenkins, Azure DevOps)\nSecurity & Compliance:\nKnowledge of data security standards and privacy policies (GDPR, HIPAA as applicable)\nExperience\n8 to 10 years of hands-on experience in AI/ML development, especially NLP and Generative AI.\nDemonstrated expertise in designing, fine-tuning, and deploying LLMs, FMs, and GenAI solutions.\nProven ability to develop end-to-end AI applications within cloud environments.\nExperience transforming monolithic architectures into scalable microservices.\nStrong background with big data processing pipelines.\nPrior experience working with cloud-native AI tools and frameworks.\nIndustry experience in finance, healthcare, or technology sectors is advantageous.\nAlternative Experience:\nCandidates with extensive research or academic experience in AI/ML, especially in NLP and large-scale data processing, are eligible if they have practical deployment experience.\n\nDay-to-Day Activities\nDevelop and optimize sophisticated NLP/GenAI models fulfilling business requirements.\nLead data pipeline construction for training and inference workflows.\nCollaborate with data engineers, architects, and product teams to ensure scalable deployment.\nConduct model testing, validation, and performance tuning.\nImplement and monitor model deployment pipelines, troubleshoot issues, and improve system robustness.\nDocument models, pipelines, and deployment procedures for audit and knowledge sharing.\nStay updated with emerging AI/ML trends, integrating best practices into projects.\nPresent findings, progress updates, and technical guidance to stakeholders.\nQualifications\nBachelors degree in Computer Science, Data Science, or related field; Masters or PhD preferred.\nCertifications in AI/ML, Cloud (e.g., AWS, Azure, Google Cloud), or Data Engineering are a plus.\nProven professional experience with advanced NLP and Generative AI solutions.\nCommitment to continuous learning to keep pace with rapidly evolving AI technologies.\nProfessional Competencies\nStrong analytical and problem-solving capabilities.\nExcellent communication skills, capable of translating complex technical concepts.\nCollaborative team player with experience working across global teams.\nAdaptability to rapidly changing project scopes and emerging AI trends.\nInnovation-driven mindset with a focus on delivering impactful solutions.\nTime management skills to prioritize and manage multiple projects effectively.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'data management', 'data processing', 'pipeline', 'big data', 'continuous integration', 'kubernetes', 'deploying models', 'natural language processing', 'ci/cd', 'fms', 'artificial intelligence', 'docker', 'sql', 'microservices', 'tensorflow', 'java', 'pytorch', 'jenkins', 'keras', 'aws']",2025-06-12 13:46:52
Data Engineer - R&D Data Catalyst Team,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role, you will be responsible for the end-to-end development of an enterprise analytics and data mastering solution using Databricks and Power BI. This role requires expertise in both data architecture and analytics, with the ability to create scalable, reliable, and impactful enterprise solutions that research cohort-building and advanced research pipeline. The ideal candidate will have experience creating and surfacing large unified repositories of human data, based on integrations from multiple repositories and solutions, and be extraordinarily skilled with data analysis and profiling.\nYou will collaborate closely with key customers, product team members, and related IT teams, to design and implement data models, integrate data from various sources, and ensure best practices for data governance and security. The ideal candidate will have a good background in data warehousing, ETL, Databricks, Power BI, and enterprise data mastering.\nDesign and build scalable enterprise analytics solutions using Databricks, Power BI, and other modern data tools.\nLeverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation\nBreak down features into work that aligns with the architectural direction runway\nParticipate hands-on in pilots and proofs-of-concept for new patterns\nCreate robust documentation from data analysis and profiling, and proposed designs and data logic\nDevelop advanced sql queries to profile, and unify data\nDevelop data processing code in sql, along with semantic views to prepare data for reporting\nDevelop PowerBI Models and reporting packages\nDesign robust data models, and processing layers, that support both analytical processing and operational reporting needs.\nDesign and develop solutions based on best practices for data governance, security, and compliance within Databricks and Power BI environments.\nEnsure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.\nDevelop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.\nCollaborate with key customers to define data requirements, functional specifications, and project goals.\nContinuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions.\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The R&D Data Catalyst Team is responsible for building Data Searching, Cohort Building, and Knowledge Management tools that provide the Amgen scientific community with visibility to Amgens wealth of human datasets, projects and study histories, and knowledge over various scientific findings. These solutions are pivotal tools in Amgens goal to accelerate the speed of discovery, and speed to market of advanced precision medications.\nBasic Qualifications:\nMasters degree and 1 to 3 years of Data Engineering experience OR\nBachelors degree and 3 to 5 years of Data Engineering experience OR\nDiploma and 7 to 9 years of Data Engineering experience\nMust Have Skills:\nMinimum of 3 years of hands-on experience with BI solutions (Preferable Power BI or Business Objects) including report development, dashboard creation, and optimization.\nMinimum of 3 years of hands-on experience building Change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management.\nHands-on experience with Databricks, including data engineering, optimization, and analytics workloads.\nDeep understanding of Power BI, including model design, DAX, and Power Query.\nProven experience designing and implementing data mastering solutions and data governance frameworks.\nExpertise in cloud platforms (AWS), data lakes, and data warehouses.\nStrong knowledge of ETL processes, data pipelines, and integration technologies.\nGood communication and collaboration skills to work with cross-functional teams and senior leadership.\nAbility to assess business needs and design solutions that align with organizational goals.\nExceptional hands-on capabilities with data profiling, data transformation, data mastering\nSuccess in mentoring and training team members\nGood to Have Skills:\nITIL Foundation or other relevant certifications (preferred)\nSAFe Agile Practitioner (6.0)\nMicrosoft Certified: Data Analyst Associate (Power BI) or related certification.\nDatabricks Certified Professional or similar certification.\nSoft Skills:\nExcellent analytical and troubleshooting skills\nDeep intellectual curiosity\nThe highest degree of initiative and self-motivation\nStrong verbal and written communication skills, including presentation to varied audiences of complex technical/business topics\nConfidence technical leader\nAbility to work effectively with global, remote teams, specifically including using of tools and artifacts to assure clear and efficient collaboration across time zones\nAbility to handle multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong problem solving, analytical skills;\nAbility to learn quickly and retain and synthesize complex information from diverse sources.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'data management', 'Power BI', 'data governance', 'data warehousing', 'Databricks', 'ETL', 'AWS']",2025-06-12 13:46:55
DBA DATA Engineer || 12 Lakhs CTC,Robotics Technologies,6 - 10 years,12 Lacs P.A.,['Hyderabad( Banjara hills )'],"Dear Candidate,\nWe are seeking a skilled and experienced DBA Data Engineer to join our growing data team. The ideal candidate will play a key role in designing, implementing, and maintaining our databases and data pipeline architecture. You will collaborate with software engineers, data analysts, and DevOps teams to ensure efficient data flow, data integrity, and optimal database performance across all systems. This role requires a strong foundation in database administration, SQL performance tuning, data modeling, and experience with both on-prem and cloud-based environments.\n\nRequirements:\nBachelors degree in Computer Science, Information Systems, or a related field.\n6+ years of experience in database administration and data engineering.\nProven expertise in RDBMS (Oracle, MySQL, and PostgreSQL) and NoSQL systems (MongoDB, Cassandra).\nExperience managing databases in cloud environments (AWS, Azure, or GCP).\nProficiency in ETL processes and tools (e.g., Apache NiFi, Talend, Informatica, AWS Glue).\nStrong experience with scripting languages such as Python, Bash, or PowerShell.\n\nDBA Data Engineer Roles & Responsibilities:\nDesign and maintain scalable and high-performance database architectures.\nMonitor and optimize database performance using tools like CloudWatch, Oracle Enterprise Manager, pgAdmin, Mongo Compass, or Dynatrace.\nDevelop and manage ETL/ELT pipelines to support business intelligence and analytics.\nEnsure data integrity and security through best practices in backup, recovery, and encryption.\nAutomate regular database maintenance tasks using scripting and scheduled jobs.\nImplement high availability, failover, and disaster recovery strategies.\nConduct performance tuning of queries, stored procedures, indexes, and table structures.\nCollaborate with DevOps to automate database deployments using CI/CD and IaC tools (e.g., Terraform, AWS CloudFormation).\nDesign and implement data models, including star/snowflake schemas for data warehousing.\nDocument data flows, data dictionaries, and database configurations.\nManage user access and security policies using IAM roles or database-native permissions.\nAnalyze existing data systems and propose modernization or migration plans (on-prem to cloud, SQL to NoSQL, etc.).\nUse AWS RDS, Amazon Redshift, Azure SQL Database, or Google BigQuery as needed.\nStay up-to-date with emerging database technologies and make recommendations.\n\nMust-Have Skills:\nDeep knowledge of SQL and database performance tuning.\nHands-on experience with database migrations and replication strategies.\nFamiliarity with data governance, data quality, and compliance frameworks (GDPR, HIPAA, etc.).\nStrong problem-solving and troubleshooting skills.\nExperience with data streaming platforms such as Apache Kafka, AWS Kinesis, or Apache Flink is a plus.\nExperience with data lake and data warehouse architectures.\nExcellent communication and documentation skills.\n\nSoft Skills:\nProblem-Solving: Ability to analyze complex problems and develop effective solutions.\nCommunication Skills: Strong verbal and written communication skills to effectively collaborate with cross-functional teams.\nAnalytical Thinking: Ability to think critically and analytically to solve technical challenges.\nTime Management: Capable of managing multiple tasks and deadlines in a fast-paced environment.\nAdaptability: Ability to quickly learn and adapt to new technologies and methodologies.\n\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database Administration', 'Database Migration', 'Dba Skills', 'High Availability', 'Db Administration', 'Hadr', 'Database Security', 'Disaster Recovery', 'Dr Testing', 'DR', 'Luw', 'Db Upgrade', 'Brtools', 'UDDI', 'Os Migration', 'Dbase', 'DBMS', 'IBM DB2', 'Db Migration', 'Disaster Recovery Planning', 'Db Dba', 'Backup And Recovery']",2025-06-12 13:46:57
Data Engineering Lead,Yotta Techports,10 - 15 years,30-35 Lacs P.A.,['Hyderabad'],"Responsibilities:\nLead and manage an offshore team of data engineers, providing strategic guidance, mentorship, and support to ensure the successful delivery of projects and the development of team members.\nCollaborate closely with onshore stakeholders to understand project requirements, allocate resources efficiently, and ensure alignment with client expectations and project timelines.\nDrive the technical design, implementation, and optimization of data pipelines, ETL processes, and data warehouses, ensuring scalability, performance, and reliability.\nDefine and enforce engineering best practices, coding standards, and data quality standards to maintain high-quality deliverables and mitigate project risks.\nStay abreast of emerging technologies and industry trends in data engineering, and provide recommendations for tooling, process improvements, and skill development.\nAssume a data architect role as needed, leading the design and implementation of data architecture solutions, data modeling, and optimization strategies.\nDemonstrate proficiency in AWS services such as:\nExpertise in cloud data services, including AWS services like Amazon Redshift, Amazon EMR, and AWS Glue, to design and implement scalable data solutions.\nExperience with cloud infrastructure services such as AWS EC2, AWS S3, to optimize data processing and storage.\nKnowledge of cloud security best practices, IAM roles, and encryption mechanisms to ensure data privacy and compliance.\nProficiency in managing or implementing cloud data warehouse solutions, including data modeling, schema design, performance tuning, and optimization techniques.\nDemonstrate proficiency in modern data platforms such as Snowflake and Databricks, including:\nDeep understanding of Snowflake's architecture, capabilities, and best practices for designing and implementing data warehouse solutions.\nHands-on experience with Databricks for data engineering, data processing, and machine learning tasks, leveraging Spark clusters for scalable data processing.\nAbility to optimize Snowflake and Databricks configurations for performance, scalability, and cost-effectiveness.\nManage the offshore team's performance, including resource allocation, performance evaluations, and professional development, to maximize team productivity and morale.\n\nQualifications:\nBachelor's degree in Computer Science, Engineering, or a related field; advanced degree preferred.\n10+ years of experience in data engineering, with a proven track record of leadership and technical expertise in managing complex data projects.\nProficiency in programming languages such as Python, Java, or Scala, as well as expertise in SQL and relational databases (e.g., PostgreSQL, MySQL).\nStrong understanding of distributed computing, cloud technologies (e.g., AWS), and big data frameworks (e.g., Hadoop, Spark).\nExperience with data architecture design, data modeling, and optimization techniques.\nExcellent communication, collaboration, and leadership skills, with the ability to effectively manage remote teams and engage with onshore stakeholders.\nProven ability to adapt to evolving project requirements and effectively prioritize tasks in a fast-paced environment.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Team Handling', 'Snowflake', 'Data Services', 'Cloud Infrastructure', 'Data Bricks']",2025-06-12 13:46:59
DBA DATA Engineer || 12 Lakhs CTC,Robotics Technologies,8 - 9 years,12 Lacs P.A.,['Hyderabad( Banjara hills )'],"Dear Candidate,\nWe are seeking a skilled and experienced DBA Data Engineer to join our growing data team. The ideal candidate will play a key role in designing, implementing, and maintaining our databases and data pipeline architecture. You will collaborate with software engineers, data analysts, and DevOps teams to ensure efficient data flow, data integrity, and optimal database performance across all systems. This role requires a strong foundation in database administration, SQL performance tuning, data modeling, and experience with both on-prem and cloud-based environments.\n\nRequirements:\nBachelors degree in Computer Science, Information Systems, or a related field.\n8+ years of experience in database administration and data engineering.\nProven expertise in RDBMS (Oracle, MySQL, and PostgreSQL) and NoSQL systems (MongoDB, Cassandra).\nExperience managing databases in cloud environments (AWS, Azure, or GCP).\nProficiency in ETL processes and tools (e.g., Apache NiFi, Talend, Informatica, AWS Glue).\nStrong experience with scripting languages such as Python, Bash, or PowerShell.\n\nDBA Data Engineer Roles & Responsibilities:\nDesign and maintain scalable and high-performance database architectures.\nMonitor and optimize database performance using tools like CloudWatch, Oracle Enterprise Manager, pgAdmin, Mongo Compass, or Dynatrace.\nDevelop and manage ETL/ELT pipelines to support business intelligence and analytics.\nEnsure data integrity and security through best practices in backup, recovery, and encryption.\nAutomate regular database maintenance tasks using scripting and scheduled jobs.\nImplement high availability, failover, and disaster recovery strategies.\nConduct performance tuning of queries, stored procedures, indexes, and table structures.\nCollaborate with DevOps to automate database deployments using CI/CD and IaC tools (e.g., Terraform, AWS CloudFormation).\nDesign and implement data models, including star/snowflake schemas for data warehousing.\nDocument data flows, data dictionaries, and database configurations.\nManage user access and security policies using IAM roles or database-native permissions.\nAnalyze existing data systems and propose modernization or migration plans (on-prem to cloud, SQL to NoSQL, etc.).\nUse AWS RDS, Amazon Redshift, Azure SQL Database, or Google BigQuery as needed.\nStay up-to-date with emerging database technologies and make recommendations.\n\nMust-Have Skills:\nDeep knowledge of SQL and database performance tuning.\nHands-on experience with database migrations and replication strategies.\nFamiliarity with data governance, data quality, and compliance frameworks (GDPR, HIPAA, etc.).\nStrong problem-solving and troubleshooting skills.\nExperience with data streaming platforms such as Apache Kafka, AWS Kinesis, or Apache Flink is a plus.\nExperience with data lake and data warehouse architectures.\nExcellent communication and documentation skills.\n\nSoft Skills:\nProblem-Solving: Ability to analyze complex problems and develop effective solutions.\nCommunication Skills: Strong verbal and written communication skills to effectively collaborate with cross-functional teams.\nAnalytical Thinking: Ability to think critically and analytically to solve technical challenges.\nTime Management: Capable of managing multiple tasks and deadlines in a fast-paced environment.\nAdaptability: Ability to quickly learn and adapt to new technologies and methodologies.\n\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database Administration', 'Database Migration', 'Dba Skills', 'High Availability', 'Db Administration', 'Hadr', 'Database Security', 'Disaster Recovery', 'Dr Testing', 'DR', 'Luw', 'Db Upgrade', 'Brtools', 'UDDI', 'Os Migration', 'Dbase', 'DBMS', 'IBM DB2', 'Db Migration', 'Disaster Recovery Planning', 'Db Dba', 'Backup And Recovery']",2025-06-12 13:47:02
MDM Associate Data Engineer,Amgen Inc,1 - 4 years,Not Disclosed,['Hyderabad'],"We are seeking an MDM Associate Data Engineerwith 25 years of experience to support and enhance our enterprise MDM (Master Data Management) platforms using Informatica/Reltio. This role is critical in delivering high-quality master data solutions across the organization, utilizing modern tools like Databricks and AWS to drive insights and ensure data reliability. The ideal candidate will have strong SQL, data profiling, and experience working with cross-functional teams in a pharma environment.To succeed in this role, the candidate must have strong data engineering experience along with MDM knowledge, hence the candidates having only MDM experience are not eligible for this role. Candidate must have data engineering experience on technologies like (SQL, Python, PySpark, Databricks, AWS etc), along with knowledge of MDM (Master Data Management)\nRoles & Responsibilities:\nAnalyze and manage customer master data using Reltio or Informatica MDM solutions.\nPerform advanced SQL queries and data analysis to validate and ensure master data integrity.\nLeverage Python, PySpark, and Databricks for scalable data processing and automation.\nCollaborate with business and data engineering teams for continuous improvement in MDM solutions.\nImplement data stewardship processes and workflows, including approval and DCR mechanisms.\nUtilize AWS cloud services for data storage and compute processes related to MDM.\nContribute to metadata and data modeling activities.\nTrack and manage data issues using tools such as JIRA and document processes in Confluence.\nApply Life Sciences/Pharma industry context to ensure data standards and compliance.\nBasic Qualifications and Experience:\nMasters degree with 1 - 3 years of experience in Business, Engineering, IT or related field OR\nBachelors degree with 2 - 5 years of experience in Business, Engineering, IT or related field OR\nDiploma with 6 - 8 years of experience in Business, Engineering, IT or related field\nFunctional Skills:\nMust-Have Skills:\nAdvanced SQL expertise and data wrangling.\nStrong experience in Python and PySpark for data transformation workflows.\nStrong experience with Databricks and AWS architecture.\nMust have knowledge of MDM, data governance, stewardship, and profiling practices.\nIn addition to above, candidates having experience with Informatica or Reltio MDM platforms will be preferred.\nGood-to-Have Skills:\nExperience with IDQ, data modeling and approval workflow/DCR.\nBackground in Life Sciences/Pharma industries.\nFamiliarity with project tools like JIRA and Confluence.\nStrong grip on data engineering concepts.\nProfessional Certifications:\nAny ETL certification (e.g. Informatica)\nAny Data Analysis certification (SQL, Python, Databricks)\nAny cloud certification (AWS or AZURE)\nSoft Skills:\nStrong analytical abilities to assess and improve master data processes and solutions.\nExcellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders.\nEffective problem-solving skills to address data-related issues and implement scalable solutions.\nAbility to work effectively with global, virtual teams",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MDM', 'PySpark', 'AWS architecture', 'Jira', 'Reltio', 'SQL', 'Informatica MDM', 'data modeling', 'Confluence', 'IDQ', 'Databricks', 'data stewardship processes', 'Python']",2025-06-12 13:47:04
Senior Engineer - Data Science,Sasken Technologies,2 - 5 years,Not Disclosed,['Bengaluru'],"Job Summary\nPerson at this position has gained significant work experience to be able to apply their knowledge effectively and deliver results. Person at this position is also able to demonstrate the ability to analyse and interpret complex problems and improve change or adapt existing methods to solve the problem.\nPerson at this position regularly interacts with interfacing groups / customer on technical issue clarification and resolves the issues. Also participates actively in important project/ work related activities and contributes towards identifying important issues and risks. Reaches out for guidance and advice to ensure high quality of deliverables.\nPerson at this position consistently seek opportunities to enhance their existing skills, acquire more complex skills and work towards enhancing their proficiency level in their field of specialisation.\nWorks under limited supervision of Team Lead/ Project Manager.\n\n\nRoles & Responsibilities\nResponsible for design, coding, testing, bug fixing, documentation and technical support in the assigned area. Responsible for on time delivery while adhering to quality and productivity goals. Responsible for adhering to guidelines and checklists for all deliverable reviews, sending status report to team lead and following relevant organizational processes. Responsible for customer collaboration and interactions and support to customer queries. Expected to enhance technical capabilities by attending trainings, self-study and periodic technical assessments. Expected to participate in technical initiatives related to project and organization and deliver training as per plan and quality.\n\n\nEducation and Experience Required\nEngineering graduate, MCA, etc Experience: 2-5 years\n\nCompetencies Description\nData Science TCB is applicable to one who\n1) Analyses data to arrive at patterns/Insights/models\n2) Come up with models based on the data to provide recommendations, predictive analytics etc\n3) Provides implementation of the models in R, Matlab etc\n4) Can understand and apply machine learning/AI techniques\nPlatforms-\nUnix\nTechnology Standard-\nNA\nTools-\nR, Matlab, Spark Machine Learning, Python-ML, SPSS, SAS\nLanguages-\nR, Perl, Python, Scala\nSpecialization-\nCOGNITIVE ANALYTICS INCLUDING COMPUTER VISION, AI and ML, STATISTICS.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'R', 'SAS', 'Scala', 'Perl', 'SPSS', 'Spark', 'machine learning', 'Python']",2025-06-12 13:47:07
Assoc. Data Engineer - R&D Precision Medicine Team,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\n\n\nThe R&D Precision Medicine team is responsible for Data Standardization, Data Searching, Cohort Building, and Knowledge Management tools that provide the Amgen scientific community with access to Amgens wealth of human datasets, projects and study histories, and knowledge over various scientific findings. These data include clinical data, omics, and images. These solutions are pivotal tools in Amgens goal to accelerate the speed of discovery, and speed to market of advanced precision medications.\n\nThe Data Engineer will be responsible for full stack development of enterprise analytics and data mastering solutions leveraging Databricks and Power BI. This role requires expertise in both data architecture and analytics, with the ability to create scalable, reliable, and high-performing enterprise solutions that support research cohort-building and advanced AI pipelines. The ideal candidate will have experience creating and surfacing large unified repositories of human data, based on integrations from multiple repositories and solutions, and be exceptionally skilled with data analysis and profiling.\n\nYou will collaborate closely with partners, product team members, and related IT teams, to design and implement data models, integrate data from various sources, and ensure best practices for data governance and security. The ideal candidate will have a solid background in data warehousing, ETL, Databricks, Power BI, and enterprise data mastering.\n\nRoles & Responsibilities\nDesign and build scalable enterprise analytics solutions using Databricks, Power BI, and other modern data management tools.\nLeverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation\nBreak down features into work that aligns with the architectural direction runway\nParticipate hands-on in pilots and proofs-of-concept for new patterns\nCreate robust documentation from data analysis and profiling, and proposed designs and data logic\nDevelop advanced sql queries to profile, and unify data\nDevelop data processing code in sql, along with semantic views to prepare data for reporting\nDevelop PowerBI Models and reporting packages\nDesign robust data models, and processing layers, that support both analytical processing and operational reporting needs.\nDesign and develop solutions based on best practices for data governance, security, and compliance within Databricks and Power BI environments.\nEnsure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.\nDevelop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.\nCollaborate with partners to define data requirements, functional specifications, and project goals.\nContinuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions.\n\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The professional we seek is someone with these qualifications.\n\nBasic Qualifications:\nMasters degree with 1 to 3 years of experience in Data Engineering OR\nBachelors degree with 1 to 3 years of experience in Data Engineering\nMust-Have\n\nSkills:\nMinimum of 1 year of hands-on experience with BI solutions (Preferrable Power BI or Business Objects) including report development, dashboard creation, and optimization.\nMinimum of 1 year of hands-on experience building Change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management.\nHands-on experience with Databricks, including data engineering, optimization, and analytics workloads.\nExperience using cloud platforms (AWS), data lakes, and data warehouses.\nWorking knowledge of ETL processes, data pipelines, and integration technologies.\nGood communication and collaboration skills to work with cross-functional teams and senior leadership.\nAbility to assess business needs and design solutions that align with organizational goals.\nExceptional hands-on capabilities with data profiling and data anlysis\nGood-to-Have\n\nSkills:\nExperience with human data, ideally human healthcare data\nFamiliarity with laboratory testing, patient data from clinical care, HL7, FHIR, and/or clinical trial data management\nProfessional Certifications:\nITIL Foundation or other relevant certifications (preferred)\nSAFe Agile Practitioner (6.0)\nMicrosoft CertifiedData Analyst Associate (Power BI) or related certification.\nDatabricks Certified Professional or similar certification.\nSoft\n\nSkills:\nExcellent analytical and troubleshooting skills\nDeep intellectual curiosity\nHighest degree of initiative and self-motivation\nStrong verbal and written communication skills, including presentation to varied audiences of complex technical/business topics\nConfidence technical leader\nAbility to work effectively with global, virtual teams, specifically including using of tools and artifacts to assure clear and efficient collaboration across time zones\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong problem solving, analytical skills;\nAbility to learn quickly and retain and synthesize complex information from diverse sources",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'data lakes', 'data pipelines', 'ETL processes', 'AWS', 'data warehouses', 'BI solutions']",2025-06-12 13:47:10
Azure Data Engineer,CODERZON Technologies Pvt Ltd,3 - 8 years,6-18 Lacs P.A.,['Kochi'],"Looking for a Data Engineer with 3+ yrs exp in Azure Data Factory, Synapse, Data Lake, Databricks, SQL, Python, Spark, CI/CD. Preferred: DP-203 cert, real-time data tools (Kafka, Stream Analytics), data governance (Purview), Power BI.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Synapse', 'Pyspark', 'Azure Databricks', 'Azure Data Lake', 'SQL Azure', 'Python']",2025-06-12 13:47:12
Azure Data Engineer,Arges Global,2 - 5 years,8-18 Lacs P.A.,['Pune( Baner )'],"Scope of Work:\nCollaborate with the lead Business / Data Analyst to gather and analyse business requirements for data processing and reporting solutions.\nMaintain and run existing Python code, ensuring smooth execution and troubleshooting any issues that arise.\nDevelop new features and enhancements for data processing, ingestion, transformation, and report building.\nImplement best coding practices to improve code quality, maintainability, and efficiency.\nWork within Microsoft Fabric to manage data integration, warehousing, and analytics, ensuring optimal performance and reliability.\nSupport and maintain CI/CD workflows using Git-based deployments or other automated deployment tools, preferably in Fabric.\nDevelop complex business rules and logic in Python to meet functional specifications and reporting needs.\nParticipate in an agile development environment, providing feedback, iterating on improvements, and supporting continuous integration and delivery processes.\nRequirements:\nThis person will be an individual contributor responsible for programming, maintenance support, and troubleshooting tasks related to data movement, processing, ingestion, transformation, and report building.\nAdvanced-level Python developer.\nModerate-level experience in working in Microsoft Fabric environment (at least one and preferably two or more client projects in Fabric).\nWell-versed with understanding of modelling, databases, data warehousing, data integration, and technical elements of business intelligence technologies.\nAbility to understand business requirements and translate them into functional specifications for reporting applications.\nExperience in GIT-based deployments or other CI/CD workflow options, preferably in Fabric.\nStrong verbal and written communication skills.\nAbility to perform in an agile environment where continual development is prioritized.\nWorking experience in the financial industry domain and familiarity with financial accounting terms and statements like general ledger, balance sheet, and profit & loss statements would be a plus.\nAbility to create Power BI dashboards, KPI scorecards, and visual reports would be a plus.\nDegree in Computer Science or Information Systems, along with a good understanding of financial terms or working experience in banking/financial institutions, is preferred.",Industry Type: Financial Services (Asset Management),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power Bi', 'Microsoft Azure', 'Python', 'Azure Data Factory', 'Microsoft Fabric', 'Azure Databricks', 'Azure Data Lake']",2025-06-12 13:47:14
Software Engineer Data Privacy,Bajaj Finserv Health,4 - 6 years,Not Disclosed,['Pune( Viman Nagar )'],"Job Description:\nKnowledgeable and proactive Data Privacy engineer to manage privacy and data protection initiatives in compliance with Indian data privacy regulations, including the Digital Personal Data Protection Act, 2023 (DPDP Act).\nThis role will work closely with legal, IT, security, HR, and business teams to implement privacy by design and foster a strong culture of privacy data protection\n\nKey Objectives/Responsibilities of this Role:\nImplement and oversee the data privacy program implementation in accordance with the Privacy Laws and requirements (DPDP Act, 2023 and other applicable Indian laws)\nDocument and maintain records of processing activities\nDraft, review, and update privacy notices, policies, consent mechanisms, and contractual clauses for data processing.\nConduct and review Data Protection Impact Assessments (DPIAs) and help identify and mitigate privacy risks.\nTrain and educate employees on Data privacy requirements and best practices.\nPerform regular compliance audits and assessments across internal departments and third-party vendors.\nDevelop and deliver training and awareness programs to ensure employees understand their responsibilities under the DPDP Act and internal privacy policies.\nHandle and ensure timely response to data principal rights requests (access, correction, erasure, grievance redressal) as per Privacy Law requirements\nCollaborate with legal and IT/security teams to ensure privacy by design and default in new processes, technologies, and products.\nStay updated with latest developments in data protection laws and regulations.\nSupport incident response and breach notification processes as per regulatory timelines and requirements.\nAssist the Data Privacy Officer in preparing reports for internal leadership or regulators, as required.\nServe as a subject matter expert on Indian data privacy regulations and provide guidance to cross-functional teams.\n\nMandatory Skillset & Experience:\nMinimum 4+ years of relevant experience in information security, compliance, or legal roles with focus on data privacy\nIn-depth knowledge of the Data Protection and Privacy Regulations, and awareness of related Indian IT regulations.\nExperienced in using Data protection management tools and softwares\nUnderstanding of global data privacy frameworks (GDPR, etc.) is a plus.\nStrong analytical and problem-solving skills with a practical approach to risk and compliance.\nAbility to communicate complex privacy topics in a clear and business-friendly manner.\nExperience working with privacy compliance tools and risk management systems.\n\nBehavioral Skills:\nSelf-driven and proactive.\nPlan and execute projects in a timely fashion meeting the project timelines\nExperienced in coordinating activities across different teams and stakeholders\nContinuous process improvement / quality assurance experience is a plus\nProactively identify potential data privacy issues and problems in business processes\n\nPreferred Qualification:\nBachelors or Masters degree in law, Technology, Compliance, Risk Management, or a related field.\n\nPreferred Certifications:\nDCPP (Data Protection Certified Professional - India)\nCIPP/A, CIPM (IAPP certifications)\nISO/IEC 27701, ISO 27001 (Good to have)",Industry Type: Medical Services / Hospital,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Protection Manager', 'Privacy Regulation', 'Data Privacy Law', 'Data Privacy', 'Gdpr']",2025-06-12 13:47:16
Data Engineering Lead Microsoft Power BI,Client of Techs To Suit,8 - 12 years,25-32.5 Lacs P.A.,"['Indore', 'Hyderabad', 'Ahmedabad']","Poistion - Data Engineering Lead\nExp - 8 to 12 Years\nJob Location: Hyderabad, Ahmedabad, Indore, India.\n\nMust be Able to join in 30 days\nJob Summary:\nAs a Data Engineering Lead, your role will involve designing, developing, and implementing\ninteractive dashboards and reports using data engineering tools. You will work closely with\nstakeholders to gather requirements and translate them into effective data visualizations that\nprovide valuable insights. Additionally, you will be responsible for extracting, transforming, and\nloading data from multiple sources into Power BI, ensuring its accuracy and integrity. Your\nexpertise in Power BI and data analytics will contribute to informed decision-making and\nsupport the organization in driving data-centric strategies and initiatives.\nWe are looking for you!\nAs an ideal candidate for the Data Engineering Lead position, you embody the qualities of a\nteam player with a relentless get-it-done attitude. Your intellectual curiosity and customer\nfocus drive you to continuously seek new ways to add value to your job accomplishments. You\nthrive under pressure, maintaining a positive attitude and understanding that your career is a\njourney. You are willing to make the right choices to support your growth. In addition to your\nexcellent communication skills, both written and verbal, you have a proven ability to create\nvisually compelling designs using tools like Power BI and Tableau that effectively\ncommunicate our core values.\nYou build high-performing, scalable, enterprise-grade applications and teams. Your creativity\nand proactive nature enable you to think differently, find innovative solutions, deliver high-\nquality outputs, and ensure customers remain referenceable. With over eight years of\nexperience in data engineering, you possess a strong sense of self-motivation and take\nownership of your responsibilities. You prefer to work independently with little to no\nsupervision.\nYou are process-oriented, adopt a methodical approach, and demonstrate a quality-first\nmindset. You have led mid to large-size teams and accounts, consistently using constructive\nfeedback mechanisms to improve productivity, accountability, and performance within the\nteam. Your track record showcases your results-driven approach, as you have consistently\ndelivered successful projects with customer case studies published on public platforms.\nOverall, you possess a unique combination of skills, qualities, and experiences that make you\nan ideal fit to lead our data engineering team(s). You value inclusivity and want to join a culture\nthat empowers you to show up as your authentic self.\nYou know that success hinges on commitment, our differences make us stronger, and the\nfinish line is always sweeter when the whole team crosses together. In your role, you shouldbe driving the team using data, data, and more data. You will manage multiple teams, oversee\nagile stories and their statuses, handle escalations and mitigations, plan ahead, identify hiring\nneeds, collaborate with recruitment teams for hiring, enable sales with pre-sales teams, and\nwork closely with development managers/leads for solutioning and delivery statuses, as well\nas architects for technology research and solutions.\nWhat You Will Do:\nAnalyze Business Requirements.\nAnalyze the Data Model and do GAP analysis with Business Requirements and Power\nBI. Design and Model Power BI schema.\nTransformation of Data in Power BI/SQL/ETL Tool.\nCreate DAX Formula, Reports, and Dashboards. Able to write DAX formulas.\nExperience writing SQL Queries and stored procedures.\nDesign effective Power BI solutions based on business requirements.\nManage a team of Power BI developers and guide their work.\nIntegrate data from various sources into Power BI for analysis.\nOptimize performance of reports and dashboards for smooth usage.\nCollaborate with stakeholders to align Power BI projects with goals.\nKnowledge of Data Warehousing(must), Data Engineering is a plus",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power Bi Dashboards', 'Microsoft Power Bi', 'SQL Queries', 'Azure Databricks', 'Dax', 'Azure Data Factory']",2025-06-12 13:47:19
Data Engineering Manager,Amgen Inc,8 - 12 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\n\n\nRole Description:\n\nWe are seeking a seasoned Engineering Manager (Data Engineering) to lead the end-to-end management of enterprise data assets and operational data workflows. This role is critical in ensuring the availability, quality, consistency, and timeliness of data across platforms and functions, supporting analytics, reporting, compliance, and digital transformation initiatives. You will be responsible for the day-to-day data operations, manage a team of data professionals, and drive process excellence in data intake, transformation, validation, and delivery. You will work closely with cross-functional teams including data engineering, analytics, IT, governance, and business stakeholders to align operational data capabilities with enterprise needs.\n\nRoles & Responsibilities:\nLead and manage the enterprise data operations team, responsible for data ingestion, processing, validation, quality control, and publishing to various downstream systems.\nDefine and implement standard operating procedures for data lifecycle management, ensuring accuracy, completeness, and integrity of critical data assets.\nOversee and continuously improve daily operational workflows, including scheduling, monitoring, and troubleshooting data jobs across cloud and on-premise environments.\nEstablish and track key data operations metrics (SLAs, throughput, latency, data quality, incident resolution) and drive continuous improvements.\nPartner with data engineering and platform teams to optimize pipelines, support new data integrations, and ensure scalability and resilience of operational data flows.\nCollaborate with data governance, compliance, and security teams to maintain regulatory compliance, data privacy, and access controls.\nServe as the primary escalation point for data incidents and outages, ensuring rapid response and root cause analysis.\nBuild strong relationships with business and analytics teams to understand data consumption patterns, prioritize operational needs, and align with business objectives.\nDrive adoption of best practices for documentation, metadata, lineage, and change management across data operations processes.\nMentor and develop a high-performing team of data operations analysts and leads.\nFunctional\n\nSkills:\nMust-Have Skills:\nExperience managing a team of data engineers in biotech/pharma domain companies.\nExperience in designing and maintaining data pipelines and analytics solutions that extract, transform, and load data from multiple source systems.\nDemonstrated hands-on experience with cloud platforms (AWS) and the ability to architect cost-effective and scalable data solutions.\nExperience managing data workflows in cloud environments such as AWS, Azure, or GCP.\nStrong problem-solving skills with the ability to analyze complex data flow issues and implement sustainable solutions.\nWorking knowledge of SQL, Python, or scripting languages for process monitoring and automation.\nExperience collaborating with data engineering, analytics, IT operations, and business teams in a matrixed organization.\nFamiliarity with data governance, metadata management, access control, and regulatory requirements (e.g., GDPR, HIPAA, SOX).\nExcellent leadership, communication, and stakeholder engagement skills.\nWell versed with full stack development & DataOps automation, logging frameworks, and pipeline orchestration tools.\nStrong analytical and problem-solving skills to address complex data challenges.\nEffective communication and interpersonal skills to collaborate with cross-functional teams.\nGood-to-Have\n\nSkills:\nData Engineering Management experience in Biotech/Life Sciences/Pharma\nExperience using graph databases such as Stardog or Marklogic or Neo4J or Allegrograph, etc.\nEducation and Professional Certifications\nDoctorate Degree with 3-5 + years of experience in Computer Science, IT or related field\nOR\nMasters degree with 6 - 8 + years of experience in Computer Science, IT or related field\nOR\nBachelors degree with 10 - 12 + years of experience in Computer Science, IT or related field\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft\n\nSkills:\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'fullstack development', 'logging framework', 'stakeholder engagement', 'troubleshooting', 'cloud platforms']",2025-06-12 13:47:21
Hadoop Data Engineer,Envision Technology Solutions,3 - 8 years,5-15 Lacs P.A.,"['New Delhi', 'Hyderabad', 'Gurugram']","Primary Skill – Hadoop, Hive, Python, SQL, Pyspark/Spark.\nLocation –Hyderabad / Gurgaon;",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Hadoop', 'Hive', 'Spark', 'Python', 'SQL']",2025-06-12 13:47:24
Manager Data Engineer – Research Data and Analytics,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will create and develop data lake solutions for scientific data that drive business decisions for Research. You will build scalable and high-performance data engineering solutions for large scientific datasets and collaborate with Research collaborators. You will also provide technical leadership to junior team members. The ideal candidate possesses experience in the pharmaceutical or biotech industry, demonstrates deep technical skills, is proficient with big data technologies, and has a deep understanding of data architecture and ETL processes.\nRoles & Responsibilities:\nLead, manage, and mentor a high-performing team of data engineers\nDesign, develop, and implement data pipelines, ETL processes, and data integration solutions\nTake ownership of data pipeline projects from inception to deployment, manage scope, timelines, and risks\nDevelop and maintain data models for biopharma scientific data, data dictionaries, and other documentation to ensure data accuracy and consistency\nOptimize large datasets for query performance\nCollaborate with global multi-functional teams including research scientists to understand data requirements and design solutions that meet business needs\nImplement data security and privacy measures to protect sensitive data\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions\nCollaborate with Data Architects, Business SMEs, Software Engineers and Data Scientists to design and develop end-to-end data pipelines to meet fast paced business needs across geographic regions\nIdentify and resolve data-related challenges\nAdhere to best practices for coding, testing, and designing reusable code/component\nExplore new tools and technologies that will help to improve ETL platform performance\nParticipate in sprint planning meetings and provide estimations on technical implementation\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The [vital attribute] professional we seek is a [type of person] with these qualifications.\nBasic Qualifications:\nDoctorate Degree OR\nMasters degree with 4 - 6 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nBachelors degree with 6 - 8 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nDiploma with 10 - 12 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field\nPreferred Qualifications:\n3+ years of experience in implementing and supporting biopharma scientific research data analytics (software platforms)\n\n\nFunctional Skills:\nMust-Have Skills:\nProficiency in SQL and Python for data engineering, test automation frameworks (pytest), and scripting tasks\nHands on experience with big data technologies and platforms, such as Databricks, Apache Spark (PySpark, SparkSQL), workflow orchestration, performance tuning on big data processing\nExcellent problem-solving skills and the ability to work with large, complex datasets\nAble to engage with business collaborators and mentor team to develop data pipelines and data models\n\n\nGood-to-Have Skills:\nA passion for tackling complex challenges in drug discovery with technology and data\nGood understanding of data modeling, data warehousing, and data integration concepts\nGood experience using RDBMS (e.g. Oracle, MySQL, SQL server, PostgreSQL)\nKnowledge of cloud data platforms (AWS preferred)\nExperience with data visualization tools (e.g. Dash, Plotly, Spotfire)\nExperience with diagramming and collaboration tools such as Miro, Lucidchart or similar tools for process mapping and brainstorming\nExperience writing and maintaining technical documentation in Confluence\nUnderstanding of data governance frameworks, tools, and best practices\n\n\nProfessional Certifications:\nDatabricks Certified Data Engineer Professional preferred\n\n\nSoft Skills:\nExcellent critical-thinking and problem-solving skills\nGood communication and collaboration skills\nDemonstrated awareness of how to function in a team setting\nDemonstrated presentation skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Spotfire', 'PySpark', 'PostgreSQL', 'Plotly', 'SparkSQL', 'SQL server', 'SQL', 'process mapping', 'Dash', 'MySQL', 'ETL', 'Oracle', 'data governance frameworks', 'Python']",2025-06-12 13:47:26
Snowflake Data Engineer,Epam Systems,5 - 10 years,Not Disclosed,['Chennai'],"Key Skills:\nSnowflake (Snow SQL, Snow PLSQL and Snowpark)\nStrong Python\nAirflow/DBT\nAny DevOps tools\nAWS/Azure Cloud Skills\n\nRequirements:\nLooking for engineer for information warehouse\nWarehouse is based on AWS/Azure, DBT, Snowflake.\nStrong programming experience with Python.\nExperience with workflow management tools like Argo/Oozie/Airflow.\nExperience in Snowflake modelling - roles, schema, databases\nExperience in data Modeling (Data Vault).\nExperience in design and development of data transformation pipelines using the DBT framework.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Python', 'Azure Cloud', 'AWS', 'SQL']",2025-06-12 13:47:28
Lead Data Engineer,Prolegion,8 - 12 years,Not Disclosed,['Hyderabad'],"Job Summary:\nWe are seeking a highly skilled Lead Data Engineer/Associate Architect to lead the design, implementation, and optimization of scalable data architectures. The ideal candidate will have a deep understanding of data modeling, ETL processes, cloud data solutions, and big data technologies. You will work closely with cross-functional teams to build robust, high-performance data pipelines and infrastructure to enable data-driven decision-making.\n\nExperience: 8 - 12+ years\nWork Location: Hyderabad (Hybrid)\nMandatory skills: Python, SQL, Snowflake\n\nResponsibilities:\nDesign and Develop scalable and resilient data architectures that support business needs, analytics, and AI/ML workloads.\nData Pipeline Development: Design and implement robust ETL/ELT processes to ensure efficient data ingestion, transformation, and storage.\nBig Data & Cloud Solutions: Architect data solutions using cloud platforms like AWS, Azure, or GCP, leveraging services such as Snowflake, Redshift, BigQuery, and Databricks.\nDatabase Optimization: Ensure performance tuning, indexing strategies, and query optimization for relational and NoSQL databases.\nData Governance & Security: Implement best practices for data quality, metadata management, compliance (GDPR, CCPA), and security.\nCollaboration & Leadership: Work closely with data engineers, analysts, and business stakeholders to translate business requirements into scalable solutions.\nTechnology Evaluation: Stay updated with emerging trends, assess new tools and frameworks, and drive innovation in data engineering.\n\nRequired Skills:\nEducation: Bachelors or Masters degree in Computer Science, Data Engineering, or a related field.\nExperience: 8 - 12+ years of experience in data engineering\nCloud Platforms: Strong expertise in AWS data services.\nBig Data Technologies: Experience with Hadoop, Spark, Kafka, and related frameworks.\nDatabases: Hands-on experience with SQL, NoSQL, and columnar databases such as PostgreSQL, MongoDB, Cassandra, and Snowflake.\nProgramming: Proficiency in Python, Scala, or Java for data processing and automation.\nETL Tools: Experience with tools like Apache Airflow, Talend, DBT, or Informatica.\nMachine Learning & AI Integration (Preferred): Understanding of how to architect data solutions for AI/ML applications\n\n,",Industry Type: Defence & Aerospace,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Performance tuning', 'Automation', 'Data modeling', 'Postgresql', 'Informatica', 'Apache', 'Analytics', 'SQL', 'Python']",2025-06-12 13:47:30
Lead Data Engineer - Azure,Blend360 India,7 - 12 years,Not Disclosed,['Hyderabad'],"As a Sr Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\n7+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar field\nMust have experience e",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-12 13:47:32
Azure Data Engineer (Azure Databricks must),Fortune India 500 IT Services Firm,5 - 8 years,Not Disclosed,['Hyderabad'],"We are looking for an experienced Azure Data Engineer with strong expertise in Azure Databricks to join our data engineering team.\n\nMandatory skill- Azure Databricks\nExperience- 5 to 8 years\nLocation- Hyderabad\nKey Responsibilities:\nDesign and build data pipelines and ETL/ELT workflows using Azure Databricks and Azure Data Factory\nIngest, clean, transform, and process large datasets from diverse sources (structured and unstructured)\nImplement Delta Lake solutions and optimize Spark jobs for performance and reliability\nIntegrate Azure Databricks with other Azure services including Data Lake Storage, Synapse Analytics, and Event Hubs\n\n\n\nInterested candidates share your CV at himani.girnar@alikethoughts.com with below details\n\nCandidate's name-\nEmail and Alternate Email ID-\nContact and Alternate Contact no-\nTotal exp-\nRelevant experience-\nCurrent Org-\nNotice period-\nCCTC-\nECTC-\nCurrent Location-\nPreferred Location-\nPancard No-",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Azure Data Factory', 'Pyspark', 'Azure Data Lake', 'SQL']",2025-06-12 13:47:34
Lead Data Engineer (Immediate joiner),Decision Point,4 - 9 years,15-30 Lacs P.A.,"['Gurugram', 'Chennai']","Role & responsibilities\n• Assume ownership of Data Engineering projects from inception to completion.\nImplement fully operational Unified Data Platform solutions in production environments using technologies like Databricks, Snowflake, Azure Synapse etc.\nShowcase proficiency in Data Modelling and Data Architecture\nUtilize modern data transformation tools such as DBT (Data Build Tool) to streamline and automate data pipelines (nice to have).",,,,"['Pyspark', 'Azure Databricks', 'SQL', 'Azure Synapse', 'Python', 'Etl Pipelines', 'Airflow', 'Bigquery', 'Advance Sql', 'Azure Cloud', 'GCP', 'Data Modeling', 'Data Architecture', 'AWS']",2025-06-12 13:47:37
"Associate Engineer, Digital Data Development",XL India Business Services Pvt. Ltd,1 - 4 years,Not Disclosed,['Gurugram'],"Engineer, Digital Data Development Gurgaon/ Bangalore, India AXA XL offers risk transfer and risk management solutions to clients globally\n\nWe offer worldwide capacity, flexible underwriting solutions, a wide variety of client-focused loss prevention services, and a team-based account management approach\n\nAXA XL recognizes data and information as critical business assets, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained dynamic advantage\n\nOur Innovation, Data, and Analytics (IDA) organization is focused on driving innovation by optimizing how we leverage data to drive strategy and create a new business model - disrupting the insurance market\n\nThis role is part of the Digital Data Dev Division within the Digital Transformation vertical of IDA\n\nIt will be responsible for different aspects of Data Product development lifecycle activities, including but not limited to Data Production Support, business stakeholders engagement for usage & problem resolutions, Product migrations, and platform/data product rollouts, performance stability & reliability\n\nWhat you ll be DOING What will your essential responsibilities include? Hands-on experience with CI/CD tools: Harness, Azure DevOps\n\nImplement and manage DevSecOps tools and CI/CD pipelines with security controls\n\nAutomate security scanning and compliance checks (SAST, DAST, container scanning, etc)\n\nCollaborate with development, operations, and security teams to embed security best practices\n\nConduct threat modeling, vulnerability assessments, and risk\n\nBuild, Release Management & DevSecOps support for various data solutions owned and managed by IDA organization\n\nExperience with cloud platforms like Azure is preferred\n\nProficiency in scripting languages: Python, Bash, PowerShell\n\nFamiliarity with containerization and orchestration: Docker, Kubernetes, OpenShift\n\nExperience using Tools like Git, JIRA, Confluence etc Knowledge of Artifactory like JFrog / X-Ray\n\nExperience of working with Agile methodologies\n\nGood knowledge of OOP concepts & Microservice-based architecture\n\nAnalyze and mitigate risks (technical or otherwise) about Data Solution build & release delivery timelines\n\nProvide top-class DevSecOps functionalities and support\n\nPartner with the Product & Production Support team(s) as a Data/DevSecOps/Technical SME for migration of re-architected Product/Product functionalities to the new Cloud Platform\n\nDemonstrate proactive communication with Business users, Development, Technology, Production Support, and Delivery Teams, and Senior Management\n\nProvide day-to-day management of the DevSecOps services and ensure smooth operation of the Release pipelines to various Environments\n\nWork in the Follow the Sun support model providing cross-team support coverage across Digital Data Dev division responsibilities\n\nBuild/Setup/Maintain various critical monitoring processes, alerts, and overall health reports (performance and functional) of production, and pre-production environments to be used by the Production Support Teams\n\nWork with Product Teams to build deployment pipelines for various Data Science Products used within IDA/Pricing & Analytics Teams\n\nOversee the development and maintenance of Build & Release Management processes and their documentation\n\nEnsure that all policies, standards, and best practices are followed and kept up to date\n\nTimely and accurate completion of emergency Release pipelines/processes in a manner that is auditable, testable, and maintainable\n\nEnsure any builds are consistent with Solution design, Security recommendations and business specifications\n\nAchieve & maintain the highest business customer confidence and net promoter score (NPS)\n\nGood grasp of Azure fundamentals (Microsoft AZ-900)\n\nRobust understanding of Designing and Implementing DevOps/DevSecOps Solutions (Microsoft AZ-400)\n\nKnowledge of Python or R Programming Language is a plus\n\nYou will report to Senior Delivery Lead\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Excellent understanding of DevOps principles with integrated security practices\n\nA minimum of an Undergraduate University Degree in Computer Science or related fields\n\nExtensive experience in data-focused roles (analytics, specialist, or engineer) and one or more areas of Build, Release & Data Management\n\nDistinctive problem-solving and analytical skills combined with robust business acumen\n\nExperience/knowledge of Microservices, Dot Net, R Programming Language, Python, Azure, and Kibana\n\nExperience with SQL, HIVE, ADLS, and Document Databases like Cosmos, SQL Databases & SQL DW Analytics\n\nExperience/Understanding of systems integration, and developer support tools Azure DevOps/DevSecOps, CI/CD pipelines, Release Management, Configuration Management, and Automation\n\nData Engineering background or working experience with ETL and big data platforms (HDInsight / ADLS / Data Bricks) a plus\n\nDesired Skills and Abilities: Demonstrates a level of experience/ability to influence and understand business problems in technical terminology and able to liaise with staff at all levels in the organization\n\nExcellent writing skills, with the ability to create clear requirements, specifications, and documentation for data systems\n\nExperience with multiple software delivery models (Waterfall, Agile, etc) is a plus\n\nPrevious experience leading small teams with a mix of onsite/offshore developers",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Production support', 'Configuration management', 'Agile', 'microsoft', 'Risk management', 'Release management', 'Analytics', 'SQL', 'Python']",2025-06-12 13:47:39
ML Engineer/Data Scientist,Altimetrik,6 - 8 years,15-30 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities\nData Scientist /ML engineers : ML Engineer with Python, SQL, Machine Learning, Azure skills(Good to have)",Industry Type: IT Services & Consulting,,,"['Machine Learning', 'Python', 'SQL', 'Data Science', 'Ml', 'azure']",2025-06-12 13:47:41
Senior Technical Support Engineer - SQL & Data Analytics,Insightsoftware,6 - 11 years,Not Disclosed,['Hyderabad'],"insightsoftware is a global provider of comprehensive solutions for the Office of the CFO. We believe an actionable business strategy begins and ends with accessible financial data. With solutions across financial planning and analysis (FP&A), accounting, and operations, we transform how teams operate, empowering leaders to make timely and informed decisions. With data at the heart of everything we do, insightsoftware enables automated processes, delivers trusted insights, boosts predictability, and increases productivity. Learn more at insightsoftware.com\nWorking Timings:\nReady to work as per eastern shift timings, (5:30 PM to 2:30 AM IST)\nWork from office - Hyderabad location.(Hybrid)\nResponsibilities\n\nManage large amounts of incoming emails regarding software support\n\nInteract with customers, partners, and internal teams to provide advice and assistance and achieve customer satisfaction.\n\nLogically deduce root cause and find workarounds and solutions to issues\n\nIdentify, analyze, and document product bugs and fixes relating to the Product functionality, databases, application servers, and new technologies for product management and engineering teams\n\nComplete or assist with customer product installs as needed\n\nMeet personal Productivity, Efficiency, and Quality metrics\n\nPrioritize and resolve issues of the highest technical and business severity\n\nHandle customer complaints and provide appropriate solutions and alternatives within time limits; follow up to ensure resolution including identifying workarounds and communicating those to customers\n\nKeep accurate records of customer interactions by documenting them in Salesforce\n\nCommunicate with Product Management and Development Teams in JIRA\n\nQualifications/ Requirements\nTechnical Requirements:\n\nPrevious technical support /Product Support experience.\n\nHigh proficiency in Microsoft Word, Excel, and PowerPoint\nCandidate should have skills/knowledge on SQL (ex: MS SQL Server, MySQL, Postgre SQL),\nGeneral experience in **Data Analytics** (e.g., Data Visualization, BI tools, Data Cubes)\nAbility to understand and troubleshoot complex systems\n\nAbility to extract meaningful information from Customer communications to understand customer intent and identify the customer s technical issue.\n\nBasic Networking knowledge (TCP/IP, DNS, SSL etc.)\n\nUnderstanding of Windows client and server environments\n\nNice to have:\n\nPrevious experience working with Salesforce.\n\nPrevious experience working with JIRA.\n\nBasic technical SQL knowledge.\n\nBasic Oracle and/or SAP knowledge.\n\nExperience with software installations, network operations, and software support\n\nKnowledge of SQL Server (i.e. able to do admin tasks such as backup/restore, understanding SQL/triggers/stored procedures etc.).\n\nUnderstanding and experience with Microsoft IIS.\n\nUnderstanding and experience with SQL Server Analysis Services\n\nWindows general troubleshooting - understanding Event Viewer logs, Windows Installer errors and logs, using the Registry Editor etc.\n\nCommunication skills:\n\nAbility to communicate correctly and clearly with both internal team members and external customers.\n\nNative or equivalent English proficiency\n\nExcellent written communication skills.\n\nValued Traits:\n\nHighly motivated and driven to perform at the highest level.\n\nNatural curiosity and willingness to learn and understand issues\n\nShows pride in producing quality deliverables.\n\nAlways being punctual and professional internally and with customers.\nEducation Requirements:\nBachelor s Degree (CSE, IT) or MCA",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SAP', 'Networking', 'Financial planning', 'MySQL', 'Windows', 'Business strategy', 'Oracle', 'Technical support', 'Product support', 'Salesforce']",2025-06-12 13:47:43
Cloud SRE - Big Data Platform,ANZ,6 - 11 years,Not Disclosed,['Bengaluru'],"As a Cloud Big Data SRE in our Cloud Infrastructure Team, you ll play a key role in hprimarily lead the engineering and operational activities on an enterprise big data platform (EBD) on GCP. The platform is vast with Petabytes of data mostly retail and commercial bank customers which is used for banks critical customer remediation program and is significant in regulatory compliance. Additionally, the role involves providing platform engineering leadership and mentorship within the EDAE team, significantly influencing the direction and success of ANZs data capabilities on a broad scale.\nBanking is changing and we re changing with it, giving our people great opportunities to try new things, learn and grow. Whatever your role at ANZ, you ll be building your future, while helping to build ours.\nRole Type:Permanent\nRole Location:Bengaluru\n\nWhat will your day look like?\nLead SRE principles adoption in squad through monitoring of reliability of platform and applications, automate operations.\nEnsure services meet defined uptime, performance and latency targets.\nSolving ambiguous and complex data platform engineering problems.\nWork collaboratively within and across teams, Tech Areas and Domains.\nUtilise tools and practices to maintain, verify and deploy solutions in the most efficient ways, we place a high emphasis on Software Fundamentals.\nImplements a culture within the Tribe and the Chapter, encouraging best practices around reviews, quality, and documentation.\nProvide ongoing support for platforms as required e.g. problem and incident management troubleshoot.\nWhat will you bring?\n\nTo grow and be successful in this role, you will ideally bring the following:\nExperienced in managing enterprise-scale big data lake platforms on cloud, with a strong focus on platform engineering, service hosting, and dependency management across distributed systems.\nSpecialize in SRE practices including reliability engineering, observability implementation, proactive monitoring, and incident response operations to ensure platform stability and operational excellence.\nProven experience in devops, automation, manage and version codebases , configurations and infrastructure automation using Terraform.\nProficiency in Python, with experience in a secondary language like Golang or Java.\n6+ years of experience as a Cloud SRE/Engineer (preferably GCP) with applications utilizing services such as:\nBig Data Ecosystem : Hadoop cluster and technologies like Spark, Hive, and HBase deployed on cloud platforms\nWorkflow Orchestration : Airflow (or equivalent managed services) for job scheduling and pipeline orchestration\nContainerization & Orchestration : Kubernetes for scalable deployment and orchestration of containerized workloads\nObservability: Centralized logging and metrics-based monitoring using cloud-native or open-source solutions\nCloud Object Storage: Scalable storage solutions such as GCS, S3, or Azure Blob Storage\nDistributed SQL Stores: Managed relational databases like Cloud SQL, Amazon RDS, or Azure SQL\nIdentity & Access Management: Role-based access control and policy enforcement via IAM across cloud environments\nYou re not expected to have 100% of these skills. At ANZ a growth mindset is at the heart of our culture, so if you have most of these things in your toolbox, we d love to hear from you.\nSo why join us?\nANZ is a place where big things happen as we work together to provide banking and financial services across more than 30 markets. With more than 7,500 people, our Bengaluru team is the banks largest technology, data and operations centre outside Australia. In operation for over 33 years, the centre is critical in delivering the banks strategy and making an impact for our millions of customers around the world. Our Bengaluru team not only drives the transformation initiatives of the bank, it also drives a culture that makes ANZ a great place to be. Were proud that people feel they can be themselves at ANZ and 90 percent of our people feel they belong.\nWe want to continue building a diverse workplace and welcome applications from everyone. Please talk to us about any adjustments you may require to our recruitment process or the role itself. If you are a candidate with a disability or access requirements, let us know how we can provide you with additional support.\nTo find out more about working at ANZ visit https://www.anz.com/careers/ . You can apply for this role by visiting ANZ Careers and searching for reference number 98658\n.\nJob Posting End Date\n18/06/2025 , 11.59pm, (Melbourne Australia)",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Access management', 'Workflow', 'Incident management', 'Open source', 'Distribution system', 'Monitoring', 'Financial services', 'SQL', 'Python']",2025-06-12 13:47:46
Software Data Operations Engineer (BS+2),MAQ Software,2 - 5 years,Not Disclosed,['Noida'],"MAQ LLC d.b.a MAQ Software hasmultiple openings at Redmond, WA for:\nSoftware Data Operations Engineer (BS+2)\n\nResponsible for gathering & analyzing business requirements from customers. Implement,test and integrate software applications for use by customers. Develop &review cost effective data architecture to ensure appropriateness with currentindustry advances in data management, cloud & user experience. Automateuser test scenarios, debug & fix errors in cloud-based data infrastructure,reporting applications to meet customer needs. Must be able to traveltemporarily to client sites and or relocate throughout the United States.\n\nRequirements:Bachelors Degree or foreign equivalent in Computer Science, ComputerApplications, Computer Information Systems, Information Technology or relatedfield with two years of work experience in job offered, software engineer, systemsanalyst or related job.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Software Data Operations', 'software applications', 'data management', 'Data Operations', 'data architecture', 'data infrastructure']",2025-06-12 13:47:48
Big Data Lead,IQVIA,8 - 13 years,25-40 Lacs P.A.,['Bengaluru'],"Job Title / Primary Skill: Big Data Developer (Lead/Associate Manager)\nManagement Level: G150\nYears of Experience: 8 to 13 years\nJob Location: Bangalore (Hybrid)\nMust Have Skills: Big data, Spark, Scala, SQL, Hadoop Ecosystem.\nEducational Qualification: BE/BTech/ MTech/ MCA, Bachelor or masters degree in Computer Science,\n\nJob Overview\nOverall Experience 8+ years in IT, Software Engineering or relevant discipline.\nDesigns, develops, implements, and updates software systems in accordance with the needs of the organization.\nEvaluates, schedules, and resources development projects; investigates user needs; and documents, tests, and maintains computer programs.\nJob Description:\nWe look for developers to have good knowledge of Scala programming skills and Knowledge of SQL\nTechnical Skills:\nScala, Python -> Scala is often used for Hadoop-based projects, while Python and Scala are choices for Apache Spark-based projects.\nSQL -> Knowledge of SQL (Structured Query Language) is important for querying and manipulating data\nShell Script -> Shell scripts are used for batch processing of data, it can be used for scheduling the jobs and shell scripts are often used for deploying applications\nSpark Scala -> Spark Scala allows you to write Spark applications using the Spark API in Scala\nSpark SQL -> It allows to work with structured data using SQL-like queries and Data Frame APIs.\nWe can execute SQL queries against Data Frames, enabling easy data exploration, transformation, and analysis.\n\nThe typical tasks and responsibilities of a Big Data Developer include:\n1. Data Ingestion: Collecting and importing data from various sources, such as databases, logs, APIs into the Big Data infrastructure.\n2. Data Processing: Designing data pipelines to clean, transform, and prepare raw data for analysis. This often involves using technologies like Apache Hadoop, Apache Spark.\n3. Data Storage: Selecting appropriate data storage technologies like Hadoop Distributed File System (HDFS), HIVE, IMPALA, or cloud-based storage solutions (Snowflake, Databricks).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SCALA', 'Big Data', 'Spark', 'Hive', 'Apache Pig', 'Hadoop', 'Hadoop Development', 'Mapreduce', 'Hdfs', 'Impala', 'YARN']",2025-06-12 13:47:51
Big Data Developer-STG(P),Infosys,5 - 10 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities\n\nA day in the life of an Infoscion\nAs part of the Infosys delivery team, your primary role would be to ensure effective Design, Development, Validation and Support activities, to assure that our clients are satisfied with the high levels of service in the technology domain.\nYou will gather the requirements and specifications to understand the client requirements in a detailed manner and translate the same into system requirements.\nYou will play a key role in the overall estimation of work requirements to provide the right information on project estimations to Technology Leads and Project Managers.\nYou would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\nIf you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\n\nPrimary skills:Technology->Functional Programming->Scala\n\nAdditional information(Optional)\nKnowledge of design principles and fundamentals of architecture\nUnderstanding of performance engineering\nKnowledge of quality processes and estimation techniques\nBasic understanding of project domain\nAbility to translate functional / nonfunctional requirements to systems requirements\nAbility to design and code complex programs\nAbility to write test cases and scenarios based on the specifications\nGood understanding of SDLC and agile methodologies\nAwareness of latest technologies and trends\nLogical thinking and problem solving skills along with an ability to collaborate\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SCALA', 'Big Data', 'Spark', 'Hive', 'Kafka']",2025-06-12 13:47:54
Big Data Developer - N,Infosys,5 - 10 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Job description\n\nHiring for Bigdata Developer with experience range 5 to 15 years.\n\nMandatory Skills: Bigdata, Scala, Spark, Hive, Kafka\n\nEducation: BE/B.Tech/MCA/M.Tech/MSc./MSts",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hive', 'SCALA', 'Big Data', 'Kafka', 'Spark', 'Bigdata Technologies']",2025-06-12 13:47:56
"Quant Data Specialist, Aladdin Financial Engineering - Associate",Primetrace Technologies,3 - 6 years,Not Disclosed,['Gurugram'],"About this role\nAbout Aladdin Financial Engineering (AFE):\nJoin a diverse and collaborative team of over 3 00 modelers and technologists in Aladdin Financial Engineering (AFE) within BlackRock Solutions, the business responsible for the research and development of Aladdin s financial models. This group is also accountable for analytics production, enhancing the infrastructure platform and delivering analytics content to portfolio and risk management professionals (both within BlackRock and across the Aladdin client community). The models developed and supported by AFE span a wide array of financial products covering equities, fixed income, commodities, derivatives, and private markets. AFE provides investment insights that range from an analysis of cash flows on a single bond, to the overall financial risk associated with an entire portfolio, balance sheet, or enterprise.\nRole Description:\nWe are looking for a person to join the Advanced Data Analytics team with AFE Single Security . Advanced Data Analytics is a team of Quantitative Data and Product Specialists, focused on delivering Single Security Data Content, Governance and Product Solutions and Research Platform. The team leverages data, cloud, and emerging technologies in building an innovative data platform, with the focus on business and research use cases in the S ingle S ecurity space. The team uses various statistical/mathematical methodologies to derive insights and generate content to help develop predictive models, clustering, and classification solutions and enable Governance . The team works on Mortgage, Structured & Credit Products.\nWe are looking for a person to help build and expand Data & Analytics Content in the Credit space . The person will be responsible for building, enhancing, and maintaining the Credit Content Suite . The person will work on the below -\nCredit Derived Data Content\nModel & Data Governance\nCredit Model & Analytics\nExperience\nExperience on Scala\nKnowledge of ETL, data curation and analytical jobs using distributed computing framework with Spark\nKnowledge and Experience of working with large enterprise databases like Snowflake, Cassandra & Cloud manged services like Dataproc , Databricks\nKnowledge of financial instruments like Corporate Bonds, Derivatives etc.\nKnowledge of regression methodologies\nAptitude for design and building tools for D ata Governance\nPython knowledge is a plus\nQualifications\nBachelors / masters in computer science with a major in Math, Econ, or related field\n3 - 6 years of relevant experience\nOur benefits\n\n.\nOur hybrid work model\n.\nAbout BlackRock\n.\nThis mission would not be possible without our smartest investment - the one we make in our employees. It s why we re dedicated to creating an environment where our colleagues feel welcomed, valued and supported with networks, benefits and development opportunities to help them thrive.\nFor additional information on BlackRock, please visit @blackrock | Twitter: @blackrock | LinkedIn: www.linkedin.com / company / blackrock\nBlackRock is proud to be an Equal Opportunity Employer. We evaluate qualified applicants without regard to age, disability, family status, gender identity, race, religion, sex, sexual orientation and other protected attributes at law.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Analytical', 'Fixed income', 'Financial risk', 'Finance', 'Healthcare', 'Data analytics', 'Risk management', 'Analytics', 'Balance Sheet', 'Financial engineering']",2025-06-12 13:47:58
AI Python Data Science Engineer,Probeseven,4 - 5 years,Not Disclosed,['Coimbatore'],"AI Python Data Science Engineer\nHot Openings\nAs a data science and analytics engineer, you will be involved in developing computer visions and data algorithms. Artificial intelligence development and deep machine learning implementations will be part of your development and deployment to the cloud.\n\nExperience for senior positions: 4 - 5+ years\nExperience for junior positions: 2 - 3 years\n\nRequired Tech Skills\nExperience in Python (must have) and/or R language.\nExperience with computer vision algorithms and data science.\nExperience in deep machine learning models.\nWell-versed in data visualization techniques.\nTroubleshoot and resolve code issues.\nCollaborate with data engineers to design and integrate the data sources.\nExperience in handling multiple priorities with Agile development.\nExperience with Git and working in a collaborative and distributive team environment.\nRequired Soft Skills\nExcellent listening, verbal, and written communication skills.\nStrong interpersonal & customer relationship skills.\nStrong analytical, problem solving, and decision-making skills.\nDocumentation skills.\nApply now\nHot Openings\nPHP + Node.js Developers\nFull Time\nTech Development\nExperience 4 - 6+ years",Industry Type: Film / Music / Entertainment,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer vision', 'GIT', 'data science', 'Analytical', 'Artificial Intelligence', 'Machine learning', 'PHP', 'Customer relationship', 'Analytics', 'Python']",2025-06-12 13:48:01
"Sr Validation Engineer, R&D Data Catalyst Team",Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"Role Description:\nThe Validation and Testing Leadis responsible forleading the testing activities for software applications and solutions that meet business needs and ensuring the availability of critical systems and applications. This roleis for a lead tester with experience testing data management solutions and data analytics products, and with experience with designing and executing testing for GxP validated systems. The role involves working closely with product managers, designers, and other engineers to test high-quality, scalable software solutions.\nRoles & Responsibilities:\nParticipate inrequirementdiscussionsin order to create test scripts.\nBuild test scriptsper implementation project plan by working with various members of the product team and business partners.\nConduct informal and formal testing, consolidate all the findings and coordinate with developer(s) and business partners to resolve all the issues\nPerform regression testing to verify the changes do not negatively impact existing system functionality\nCommunicate potential risks and contingency plans with project management to ensure process compliance with all regulatory and Amgen procedural requirements\nIdentify and resolve technical challenges/bugs effectively\nWork closely with cross-functional teams, including product management, design, and QA, to deliver high-quality software on time\nSupport the creating and implementation of automated testing frameworks to improve efficiency and consistency\nBasic Qualifications and Experience:\nMasters degree with 4 - 6 years of experience in Computer Science, IT or related field OR\nBachelors degree with 6 - 8 years of experience in Computer Science, IT or related field\nDiploma and 10 to 12 years of experience in Computer Science, IT or related field\nFunctional Skills:\nMust-Have Skills:\nKnowledge of GxPsoftware validation processes\nExperience with test management in Jiraand test automation practices\nExpert sql skills for data profiling and creation of test data\nStrong experience testing analytics and data platforms\nGood Problem-solving skills - Identifying and fixing bugs, adapting to changes\nExcellent communication skills - Explaining design decisions, collaborating with teams\nExperienced in Agile methodology\nGood-to-Have Skills:\nExperience in Risk-based Approach to Change Management of Validated GxP Systems\nExperience with cloud-based technologies\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nSAFE for Teams certification (Preferred)\nSoft Skills:\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Validation engineering', 'project management', 'data analytics', 'data validation', 'troubleshooting', 'agile', 'change management', 'sql', 'data profiling', 'jira']",2025-06-12 13:48:04
Senior Engineering Manager - Data Operations,Amgen Inc,12 - 15 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nWe are seeking a seasoned Senior Engineering Manager(Data Engineering) to lead the end-to-end management of enterprise data assets and operational data workflows. This role is critical in ensuring the availability, quality, consistency, and timeliness of data across platforms and functions, supporting analytics, reporting, compliance, and digital transformation initiatives.As a senior leader in the data organization, you will oversee the day-to-day data operations, manage a team of data professionals, and drive process excellence in data intake, transformation, validation, and delivery. You will work closely with cross-functional teams including data engineering, analytics, IT, governance, and business stakeholders to align operational data capabilities with enterprise needs.\nRoles & Responsibilities:\nLead and manage the enterprise data operations team, responsible for data ingestion, processing, validation, quality control, and publishing to various downstream systems.\nDefine and implement standard operating procedures for data lifecycle management, ensuring availability, accuracy, completeness, and integrity of critical data assets.\nOversee and continuously improve daily operational workflows, including scheduling, monitoring, and troubleshooting data jobs across cloud and on-premise environments.\nEstablish and track key data operations metrics (SLAs, throughput, latency, data quality, incident resolution) and drive continuous improvements.\nPartner with data engineering and platform teams to optimize pipelines, support new data integrations, and ensure scalability and resilience of operational data flows.\nCollaborate with data governance, compliance, and security teams to maintain regulatory compliance, data privacy, and access controls.\nServe as the primary escalation point for data incidents and outages, ensuring rapid response and root cause analysis.\nBuild strong relationships with business and analytics teams to understand data consumption patterns, prioritize operational needs, and align with business objectives.\nDrive adoption of best practices for documentation, metadata, lineage, and change management across data operations processes.\nMentor and develop a high-performing team of data operations analysts and leads.\nFunctional Skills:\nMust-Have Skills:\nExperience managing a team of data engineers in biotech/pharmadomain companies.\nExperience in designing and maintainingdata pipelines and analytics solutions that extract, transform, and load data from multiple source systems.\nDemonstrated hands-on experience with cloud platforms (AWS) and the ability to architect cost-effective and scalable data solutions.\nExperience managing data workflows on Databricks in cloud environments such as AWS, Azure, or GCP.\nStrong problem-solving skills with the ability to analyze complex data flow issues and implement sustainable solutions.\nWorking knowledge of SQL, Python, PySparkor scripting languages for process monitoring and automation.\nExperience collaborating with data engineering, analytics, IT operations, and business teams in a matrixed organization.\nFamiliarity with data governance, metadata management, access control, and regulatory requirements (e.g., GDPR, HIPAA, SOX).\nExcellent leadership, communication, and stakeholder engagement skills.\nWell versed with full stack development& DataOps automation, logging & observability frameworks, and pipeline orchestration tools.\nStrong analytical and problem-solving skills to address complex data challenges.\nEffective communication and interpersonal skills to collaborate with cross-functional teams.\nGood-to-Have Skills:\nData Engineering Management experience in Biotech/Life Sciences/Pharma\nExperience using graph databases such as Stardog or Marklogic or Neo4J or Allegrograph, etc.\nEducation and Professional Certifications\n12 to 15 years of experience in Computer Science, IT or related field\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nExperience in life sciences, healthcare, or other regulated industries with large-scale operational data environments.\nFamiliarity with incident and change management processes (e.g., ITIL).\nSoft Skills:\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Operations', 'Azure', 'Data Engineering', 'Neo4J', 'GCP', 'Engineering Management', 'troubleshooting', 'Stardog', 'Marklogic', 'AWS']",2025-06-12 13:48:06
"Sr. Staff Engineer, Data Frameworks",NetSkope Software,10 - 15 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","As a Sr. Staff Engineer on the Data Engineering Team you'll be working on some of the hardest problems in the field of Data, Cloud and Security with a mission to achieve the highest standards of customer success. You will be building blocks of technology that will define Netskope s future. You will leverage open source Technologies around OLAP, OLTP, Streaming, Big Data and ML models. You will help design, and build an end-to-end system to manage the data and infrastructure used to improve security insights for our global customer base.\nYou will be part of a growing team of renowned industry experts in the exciting space of Data and Cloud Analytics\nYour contributions will have a major impact on our global customer-base and across the industry through our market-leading products\nYou will solve complex, interesting challenges, and improve the depth and breadth of your technical and business skills.\nWhat you will be doing\nConceiving and building services used by Netskope products to validate, transform, load and perform analytics of large amounts of data using distributed systems with cloud scale and reliability.\nHelping other teams architect their applications using services from the Data team wile using best practices and sound designs.\nEvaluating many open source technologies to find the best fit for our needs, and contributing to some of them.\nWorking with the Application Development and Product Management teams to scale their underlying services\nProviding easy-to-use analytics of usage patterns, anticipating capacity issues and helping with long term planning\nLearning about and designing large-scale, reliable enterprise services.\nWorking with great people in a fun, collaborative environment.\nCreating scalable data mining and data analytics frameworks using cutting edge tools and techniques\nRequired skills and experience\n10+ years of industry experience building highly scalable distributed Data systems\nProgramming experience in Python, Java or Golang\nExcellent data structure and algorithm skills\nProven good development practices like automated testing, measuring code coverage.\nProven experience developing complex Data Platforms and Solutions using Technologies like Kafka, Kubernetes, MySql, Hadoop, Big Query and other open source databases\nExperience designing and implementing large, fault-tolerant and distributed systems around columnar data stores.\nExcellent written and verbal communication skills\nBonus points for contributions to the open source community\nEducation\nBSCS or equivalent required, MSCS or equivalent strongly preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'data security', 'MySQL', 'OLAP', 'Application development', 'Open source', 'Data mining', 'OLTP', 'Distribution system', 'Python']",2025-06-12 13:48:08
Data Science & AI Engineer,Blue Altair,5 - 8 years,Not Disclosed,['Pune'],"Greetings from Blue Altair!\nJob Overview:\nWe are seeking an experienced and highly skilled Data Science and AI Engineer to join our dynamic team. The ideal candidate will have 5+ years of experience working on cutting-edge data science and AI technologies across various cloud platforms with a strong focus to work on LLMs and SLMs. The role demands a professional capable of performing in a client-facing environment, as well as mentoring and guiding junior team members.\n\nTitle: Consultant/Sr. Consultant - Data Science Engineer\nExperience: 5-8 years\nLocation: Pune/Bangalore (Hybrid)\n\nRoles and responsibilities:\nDevelop, implement, and optimize machine learning models and AI algorithms to solve complex business problems.\nDesign, build, and fine-tune AI models, particularly focusing on LLMs and SLMs, using state-of-the-art techniques and architectures.\nApply advanced techniques in prompt engineering, model fine-tuning, and optimization to tailor models for specific business needs.\nDeploy and manage machine learning models and pipelines on cloud platforms (AWS, GCP, Azure, etc.).\nWork closely with clients to understand their data and AI needs and provide tailored solutions.\nCollaborate with cross-functional teams to integrate AI solutions into broader software architectures.\nMentor junior team members and provide guidance in implementing best practices in data science and AI development.\nStay up-to-date with the latest trends and advancements in data science, AI, and cloud technologies.\nPrepare technical documentation and present insights to both technical and non-technical stakeholders.\n\nRequirement:\n5+ years of experience in data science, machine learning, and AI technologies.\nProven experience working with cloud platforms such as Google Cloud, Microsoft Azure, or AWS.\nExpertise in programming languages such as Python, R, Julia, and AI frameworks like TensorFlow, PyTorch, Scikit-learn, Hugging face Transformers.\nKnowledge of data visualization tools (e.g., Matplotlib, Seaborn, Tableau)\nSolid understanding of data engineering concepts including ETL, data pipelines, and databases (SQL, NoSQL).\nExperience with MLOps practices and deployment of models in production environments.\nFamiliarity with NLP (Natural Language Processing) tasks and working with large-scale datasets.\nHands-on experience with generative AI models like GPT, Gemini, Claude, Mistral etc.\nClient-facing experience with strong communication skills to manage and engage stakeholders.\nStrong problem-solving skills and analytical mindset.\nAbility to work independently and as part of a team and mentor and provide technical leadership to junior team members.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['LLMs', 'Artificial Intelligence', 'MLOps', 'RAG', 'Natural Language Processing', 'Neural Networks', 'LLM', 'Machine Learning', 'AI Models', 'Data Science', 'PyTorch', 'SLM', 'AI Automation']",2025-06-12 13:48:11
Data Analytics Engineer,Automotive Industry,5 - 6 years,Not Disclosed,['Chennai'],"Position: Data Analytics Engineer\nExp: 5 -6 years\nNP: Immediate - 30 days\nQualification: B.tech\nLocation: Chennai Hybrid\nPrimary: Google Cloud Platform ,Python skills and Big Data pipeline\nSecondary: Big Query SQ, coding, testing, implementing, debugging workflows and apps\nKindly share your updated resume to aishwarya_s@onwardgroup.com\nKindly fill the below details\nTotal Exp:\nRelevant Exp:\nNotice Period: CTC:\nECTC:\nIf servicing NP, Last working Day, offered location & CTC:\nAvailable for Video modes interview on Weekdays (Y/N) :\nPAN Number:\nName as Per PAN Card:\nDate of Birth:\nAlternative Contact No:\nReason for Job Change:",Industry Type: Automobile,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['GCP', 'Bigquery', 'Google Cloud Platform', 'Query SQ', 'Python']",2025-06-12 13:48:13
Senior Associate Data Security Engineer,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role the Senior Associate Data Security Engineer role will cover Data Loss Prevention (DLP) and Data Security Posture Management (DSPM) technologies. This role will report to the Manager, Data Security. This position will provide essential services that enable us to better pursue our mission.\nSr. Associate Data Security Engineers operate, manage, and improve Amgens DLP and DSPM solutions. In our Data Security team, they will operate data protection security technologies in a rapidly changing global security sector. They will work with other engineers and business units to help craft, build, coordinate, configure, and implement critical preventive and detective security controls related to the protection of Amgen data.\nThis engineer will play a key role in designing, deploying, and maintaining solutions to build our rapidly growing operations.\nRoles & Responsibilities:\nMaintain the service delivery and working order of Amgen data security solutions across Amgens global enterprise\nExecute Amgen service management processes such as Incident Management, change processes, and service improvements for Amgens data security technologies\nAssist in the design and improvement of Amgens data security technologies and solutions. Build scripts for the configuration and the testing of the solution\nManage and perform analysis of escalated DLP events, engage with the business, fulfill legal hold requests, and provide executive reporting\nWork with business domain specialists to collect, analyze, build, tune and automate DLP policy sets\nAnalyze events and logs for suspicious activity and opportunities to improve posture, processes, procedures, and protections.\nConsult to the Incident Response team on investigations\nDevelop automation solutions in increase response times and reduce risk of identified incidents\nParticipate in regular meetings and conference calls with the client, IT, business partners and vendors to help ensure technical coverage for new or existing projects across the business\n\n\nFunctional Skills:\nMust-Have Skills:\nKnowledge of Cloud Access Security Platforms (Elastica, Netskope, SkyHigh, etc)\nUnderstanding of cloud and SAAS environments (AWS, O365, Box, Salesforce, etc)\nSolid experience with potential to grow knowledge in Linux/Windows OS and other infrastructure systems\nExperience with DLP and data protection technologies for a large global enterprise\nDemonstrated understanding on how emerging security technologies and data flows interoperate across complex, multi-cloud systems.\n\nBasic Qualifications:\nMasters degree and 1 to 3 years of experience OR\nBachelors degree and 3 to 5 years of experience OR\nDiploma and 7 to 9 years of experience.\nPreferred Qualifications:\nGood-to-Have Skills:\nComfort with scripting (PowerShell, Python, etc) and expression development (SQL, Regex)\nAbility to develop documentation for Infrastructure Security implementations\nBasic experience with ITIL processes such as Incident, Problem, and configuration management\nExperience in complex enterprise environments with competing business priorities\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nSystems Security Certified Practitioner (SSCP) or Security+\nSANS Certifications\nRelevant vendor-specific certifications\n\n\nSoft Skills:\nEstablished analytical and gap/fit assessment skills.\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals\nEffective presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Security', 'PowerShell', 'configuration management', 'Cloud Access Security', 'Linux', 'Incident management', 'ITIL', 'Python', 'SQL']",2025-06-12 13:48:15
Big Data Developer/ Senior Big Data Developer,Grid Dynamics,5 - 10 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","About us\nGrid Dynamics (NASDAQ: GDYN) is a leading provider of technology consulting, platform and product engineering, AI, and advanced analytics services. Fusing technical vision with business acumen, we solve the most pressing technical challenges and enable positive business outcomes for enterprise companies undergoing business transformation. A key differentiator for Grid Dynamics is our 8 years of experience and leadership in enterprise AI, supported by profound expertise and ongoing investment in data, analytics, cloud & DevOps, application modernization and customer experience. Founded in 2006, Grid Dynamics is headquartered in Silicon Valley with offices across the Americas, Europe, and India.\n\nRole & responsibilities\nWe are looking for an enthusiastic and technology-proficient Big Data Engineer, who is eager to participate in the design and implementation of a top-notch Big Data solution to be deployed at massive scale.\nOur customer is one of the world's largest technology companies based in Silicon Valley with operations all over the world. On this project we are working on the bleeding-edge of Big Data technology to develop high performance data analytics platform, which handles petabytes datasets.\nEssential functions\nParticipate in design and development of Big Data analytical applications.\nDesign, support and continuously enhance the project code base, continuous integration pipeline, etc.\nWrite complex ETL processes and frameworks for analytics and data management.\nImplement large-scale near real-time streaming data processing pipelines.\nWork inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale.\nQualifications\nStrong coding experience with Scala, Spark,Hive, Hadoop.\nIn-depth knowledge of Hadoop and Spark, experience with data mining and stream processing technologies (Kafka, Spark Streaming, Akka Streams).\nUnderstanding of the best practices in data quality and quality engineering.\nExperience with version control systems, Git in particular.\nDesire and ability for quick learning of new tools and technologies.\nWould be a plus\nKnowledge of Unix-based operating systems (bash/ssh/ps/grep etc.).\nExperience with Github-based development processes.\nExperience with JVM build systems (SBT, Maven, Gradle).\nWe offer\nOpportunity to work on bleeding-edge projects\nWork with a highly motivated and dedicated team\nCompetitive salary\nFlexible schedule\nBenefits package - medical insurance, sports\nCorporate social events\nProfessional development opportunities\nWell-equipped office",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SCALA', 'Big Data', 'Spark']",2025-06-12 13:48:17
Avaloq Software Engineer (Data),Luxoft,3 - 5 years,Not Disclosed,[],"Your expertise in Avaloq technologies, database management, and software development combined with a solid understanding of the Avaloq Enterprise-Wide Object Model will be critical in shaping and enhancing our data capabilities. This is a fantastic opportunity to bring your passion for innovation into a fast-paced, dynamic environment where your impact will be tangible.\nSkills\nMust have\nYou will have between 3-5 years experience working as an Avaloq Developer.\nMapping physical data from Avaloq and other platforms to the Enterprise Data Model, ensuring adherence to reference data standards and clear conceptual definitions.\nPerforming data quality assessments, developing strategies to enhance data integrity, and creating Avaloq system functionalities to mitigate the risk of poor data at source.\nSupporting option analysis, data profiling, and interfacing with external rule/result repositories.\nReviewing and optimising Avaloq APDM outcomes, defining treatment strategies, and improving system performance.\nIntegrating the Avaloq APDM with external data minimisation orchestration tools.\nDesigning and delivering data quality improvement solutions, including mass-data manipulation scripts and associated testing and reconciliation processes.\nAnalysing business requirements and developing tailored software solutions.\nSupporting technical analysis and enhancements for Avaloq change requests and incidents.\nCollaborating with stakeholders to build alignment around technology change initiatives.\nBuilding and maintaining synthetic data delivery routines for test environments.\nManaging market data ingestion into the Avaloq Core Platform (ACP) via third-party tools.\nNice to have\nACCP certification\nOther\nLanguages\nEnglish: C2 Proficient\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nData Modelling Expert (with Avaloq experience)\nAvaloq\nIndia\nBengaluru\nAvaloq Technical Lead\nAvaloq\nAustralia\nSydney\nSenior Avaloq Engineer\nAvaloq\nAustralia\nSydney\nRemote India, India\nReq. VR-114137\nAvaloq\nBCM Industry\n21/05/2025\nReq. VR-114137\nApply for Avaloq Software Engineer (Data) in Remote India\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Technical analysis', 'Quality improvement', 'orchestration', 'Data modeling', 'Reconciliation', 'Technical Lead', 'Data quality', 'market data', 'data integrity', 'Software solutions']",2025-06-12 13:48:19
Lead Engineer - Data Science,Sasken Technologies,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Summary\nPerson at this position takes ownership of a module and associated quality and delivery. Person at this position provides instructions, guidance and advice to team members to ensure quality and on time delivery.\nPerson at this position is expected to be able to instruct and review the quality of work done by technical staff.\nPerson at this position should be able to identify key issues and challenges by themselves, prioritize the tasks and deliver results with minimal direction and supervision.\nPerson at this position has the ability to investigate the root cause of the problem and come up alternatives/ solutions based on sound technical foundation gained through in-depth knowledge of technology, standards, tools and processes.\nPerson has the ability to organize and draw connections among ideas and distinguish between those which are implementable.\nPerson demonstrates a degree of flexibility in resolving problems/ issues that atleast to in-depth command of all techniques, processes, tools and standards within the relevant field of specialisation.\n\n\nRoles & Responsibilities\nResponsible for requirement analysis and feasibility study including system level work estimation while considering risk identification and mitigation.\nResponsible for design, coding, testing, bug fixing, documentation and technical support in the assigned area. Responsible for on time delivery while adhering to quality and productivity goals.\nResponsible for traceability of the requirements from design to delivery Code optimization and coverage.\nResponsible for conducting reviews, identifying risks and ownership of quality of deliverables.\nResponsible for identifying training needs of the team.\nExpected to enhance technical capabilities by attending trainings, self-study and periodic technical assessments.\nExpected to participate in technical initiatives related to project and organization and deliver training as per plan and quality.\nExpected to be a technical mentor for junior members.\nPerson may be given additional responsibility of managing people based on discretion of Project Manager.\n\nEducation and Experience Required\nEngineering graduate, MCA, etc Experience: 5-8 years\n\n\nCompetencies Description\nData Science TCB is applicable to one who\n1) Analyses data to arrive at patterns/Insights/models\n2) Come up with models based on the data to provide recommendations, predictive analytics etc\n3) Provides implementation of the models in R, Matlab etc\n4) Can understand and apply machine learning/AI techniques\nPlatforms-\nUnix\nTools-\nR, Matlab, Spark Machine Learning, Python-ML, SPSS, SAS\nLanguages-\nR, Perl, Python, Scala\nSpecialization-\nCOGNITIVE ANALYTICS INCLUDING COMPUTER VISION, AI and ML, STATISTICS",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Unix', 'R', 'SAS', 'Scala', 'Perl', 'SPSS', 'Machine Learning', 'Python']",2025-06-12 13:48:22
Big Data Developer,Techstar Group,7 - 10 years,Not Disclosed,['Hyderabad'],"Responsibilities of the Candidate :\n\n- Be responsible for the design and development of big data solutions. Partner with domain experts, product managers, analysts, and data scientists to develop Big Data pipelines in Hadoop\n\n- Be responsible for moving all legacy workloads to a cloud platform\n\n- Work with data scientists to build Client pipelines using heterogeneous sources and provide engineering services for data PySpark science applications\n\n- Ensure automation through CI/CD across platforms both in cloud and on-premises\n\n- Define needs around maintainability, testability, performance, security, quality, and usability for the data platform\n\n- Drive implementation, consistent patterns, reusable components, and coding standards for data engineering processes\n\n- Convert SAS-based pipelines into languages like PySpark, and Scala to execute on Hadoop and non-Hadoop ecosystems\n\n- Tune Big data applications on Hadoop and non-Hadoop platforms for optimal performance\n\n- Apply an in-depth understanding of how data analytics collectively integrate within the sub-function as well as coordinate and contribute to the objectives of the entire function.\n\n- Produce a detailed analysis of issues where the best course of action is not evident from the information available, but actions must be recommended/taken.\n\n- Assess risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding Citigroup, its clients, and assets, by driving compliance with applicable laws, rules, and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct, and business practices, and escalating, managing and reporting control issues with transparency\n\nRequirements :\n\n- 6+ years of total IT experience\n\n- 3+ years of experience with Hadoop (Cloudera)/big data technologies\n\n- Knowledge of the Hadoop ecosystem and Big Data technologies Hands-on experience with the Hadoop eco-system (HDFS, MapReduce, Hive, Pig, Impala, Spark, Kafka, Kudu, Solr)\n\n- Experience in designing and developing Data Pipelines for Data Ingestion or Transformation using Java Scala or Python.\n\n- Experience with Spark programming (Pyspark, Scala, or Java)\n\n- Hands-on experience with Python/Pyspark/Scala and basic libraries for machine learning is required.\n\n- Proficient in programming in Java or Python with prior Apache Beam/Spark experience a plus.\n\n- Hand on experience in CI/CD, Scheduling and Scripting\n\n- Ensure automation through CI/CD across platforms both in cloud and on-premises\n\n- System level understanding - Data structures, algorithms, distributed storage & compute\n\n- Can-do attitude on solving complex business problems, good interpersonal and teamwork skills",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Hive', 'Data Engineering', 'Data Pipeline', 'PySpark', 'Hadoop', 'Kafka', 'HDFS', 'Spark', 'Python']",2025-06-12 13:48:25
Advanced Data Science Associate,ZS,0 - 2 years,Not Disclosed,['Bengaluru'],"Develop advanced and efficient statistically effective algorithms that solve problems of high dimensionality .\nUtilize technical skills such as hypothesis testing, machine learning and retrieval processes to apply statistical and data mining techniques to identify trends, create figures, and analyze other relevant information.\nCollaborate with clients and other stakeholders at ZS to integrate and effectively communicate analysis findings.\nContribute to the assessment of emerging datasets and technologies that impact our analytical",,,,"['Text mining', 'Analytical', 'Management consulting', 'Financial planning', 'Machine learning', 'Hypothesis Testing', 'Predictive modeling', 'Data mining', 'big data']",2025-06-12 13:48:27
"Data Analyst, Staff",Qualcomm,4 - 7 years,Not Disclosed,['Bengaluru'],"Job Area: Miscellaneous Group, Miscellaneous Group > Data Analyst\n \n\nQualcomm Overview: \nQualcomm is a company of inventors that unlocked 5G ushering in an age of rapid acceleration in connectivity and new possibilities that will transform industries, create jobs, and enrich lives. But this is just the beginning. It takes inventive minds with diverse skills, backgrounds, and cultures to transform 5Gs potential into world-changing technologies and products. This is the Invention Age - and this is where you come in.\n\nGeneral Summary:\n\nAbout the Team\n\nQualcomm's People Analytics team plays a crucial role in transforming data into strategic workforce insights that drive HR and business decisions. As part of this lean but high-impact team, you will have the opportunity to analyze workforce trends, ensure data accuracy, and collaborate with key stakeholders to enhance our data ecosystem. This role is ideal for a generalist who thrives in a fast-paced, evolving environment""”someone who can independently conduct data analyses, communicate insights effectively, and work cross-functionally to enhance our People Analytics infrastructure.\n\nWhy Join Us\n\n\nEnd-to-End ImpactWork on the full analytics cycle""”from data extraction to insight generation""”driving meaningful HR and business decisions.\n\n\nCollaboration at ScalePartner with HR leaders, IT, and other analysts to ensure seamless data integration and analytics excellence.\n\n\nData-Driven CultureBe a key player in refining our data lake, ensuring data integrity, and influencing data governance efforts.\n\n\nProfessional GrowthGain exposure to multiple areas of people analytics, including analytics, storytelling, and stakeholder engagement.\n\n\nKey Responsibilities\n\n\nPeople Analytics & Insights\nAnalyze HR and workforce data to identify trends, generate insights, and provide recommendations to business and HR leaders.\nDevelop thoughtful insights to support ongoing HR and business decision-making.\nPresent findings in a clear and compelling way to stakeholders at various levels, including senior leadership.\n\n\nData Quality & Governance\nEnsure accuracy, consistency, and completeness of data when pulling from the data lake and other sources.\nIdentify and troubleshoot data inconsistencies, collaborating with IT and other teams to resolve issues.\nDocument and maintain data definitions, sources, and reporting standards to drive consistency across analytics initiatives.\n\n\nCollaboration & Stakeholder Management\nWork closely with other analysts on the team to align methodologies, share best practices, and enhance analytical capabilities.\nAct as a bridge between People Analytics, HR, and IT teams to define and communicate data requirements.\nPartner with IT and data engineering teams to improve data infrastructure and expand available datasets.\n\n\nQualifications\n\nRequired4-7 years experience in a People Analytics focused role\n\n\nAnalytical & Technical Skills\nStrong ability to analyze, interpret, and visualize HR and workforce data to drive insights.\nExperience working with large datasets and ensuring data integrity.\nProficiency in Excel and at least one data visualization tool (e.g., Tableau, Power BI).\n\n\nCommunication & Stakeholder Management\nAbility to communicate data insights effectively to both technical and non-technical audiences.\nStrong documentation skills to define and communicate data requirements clearly.\nExperience collaborating with cross-functional teams, including HR, IT, and business stakeholders.\n\n\nPreferred:\n\n\nTechnical Proficiency\nExperience with SQL, Python, or R for data manipulation and analysis.\nFamiliarity with HR systems (e.g., Workday) and cloud-based data platforms.\n\n\nPeople Analytics Expertise\nPrior experience in HR analytics, workforce planning, or related fields.\nUnderstanding of key HR metrics and workforce trends (e.g., turnover, engagement, diversity analytics).\n\n\nAdditional Information\nThis is an office-based position (4 days a week onsite) with possible locations that may include India and Mexico",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['sql', 'people analytics', 'documentation', 'tableau', 'data integration tools', 'hiring', 'data warehousing', 'data architecture', 'sourcing', 'jquery', 'staffing', 'plsql', 'oracle 10g', 'java', 'etl tool', 'html', 'etl', 'mongodb', 'python', 'oracle', 'power bi', 'hrsd', 'r', 'node.js', 'hr analytics', 'angularjs']",2025-06-12 13:48:30
Analyst - Data Analytics,AMERICAN EXPRESS,0 - 4 years,Not Disclosed,['Gurugram'],The American Express Enterprise Digital Experimentation & Analytics (EDEA) leads the Enterprise Product Analytics and Experimentation charter for Brand & Performance Marketing and Digital Acquisition & Membership experiences as we'll as Enterprise Platforms. The focus of this collaborative team is to drive growth by enabling efficiencies in paid performance channels & evolve our digital experiences with actionable insights & analytics. The team specializes in using data around digital product usage to drive improvements in the acquisition customer experience to deliver higher satisfaction and business value.\n,,,,"['Mining', 'Career development', 'Finance', 'Analytical', 'Data processing', 'Analytics', 'SQL']",2025-06-12 13:48:32
Big Data Engineer_ Info Edge _Noida,Info Edge,1 - 4 years,14-19 Lacs P.A.,['Noida'],"About Info Edge India Ltd.\nInfo Edge: Info Edge (India) Limited (NSE: NAUKRI) is among the leading internet companies in India. Info Edge, Indias premier online classifieds company is fundamentally in the matching business. With a network of 62 offices located in 43 cities throughout India, Info Edge has 5000 plus employees engaged in innovation, product development, integration with mobile and social media, technology and technology updation, research and development, quality assurance, sales, marketing and payment collection.\nThe umbrella brand has an online recruitment classifieds, www.naukri.com– India’s No. 1 Jobsite with over 75% traffic share, a matrimony classifieds, www.jeevansathi.com, a real estate classifieds, www.99acres.com– India’s largest property marketplace and an education classifieds, www.shiksha.com. Find out more about the Company at",,,,"['Data Modeling', 'Python', 'SCALA']",2025-06-12 13:48:35
Data Bricks,PwC India,7 - 12 years,Not Disclosed,['Bengaluru'],"Job Summary:\n\nWe are seeking a talented Data Engineer with strong expertise in Databricks, specifically in Unity Catalog, PySpark, and SQL, to join our data team. Youll play a key role in building secure, scalable data pipelines and implementing robust data governance strategies using Unity Catalog.\n\nKey Responsibilities:",,,,"['DataBricks', 'Data Bricks', 'Pyspark', 'Delta Lake', 'Databricks Engineer', 'Unity Catalog', 'SQL']",2025-06-12 13:48:37
AVP Data Management Analyst,Wells Fargo,2 - 7 years,Not Disclosed,['Bengaluru'],"About this role:\nWells Fargo is seeking a Data Management Analyst\n\nIn this role, you will:\nParticipate in less complex analysis to identify and remediate data quality or integrity issues and to identify and remediate process or control gaps\nAdhere to data governance standards and procedures",,,,"['Data Management', 'Agile Methodology', 'Funds Transfer Pricing', 'Financial Data Mapping', 'Big Data Query Techniques', 'Lineage Tracing', 'Data warehousing', 'Data Governance', 'Jira', 'Market Risks', 'SQL']",2025-06-12 13:48:39
Data Governance Engineers,Meritus Management Service,4 - 9 years,14-17 Lacs P.A.,"['Pune', 'Gurugram']","Define, implement, & enforce data governance policies & standards to ensure data quality, consistency, & compliance across the organization\nCollaborate with data stewards, business users, & IT teams to maintain metadata, lineage, & data catalog tools",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Stewardship', 'Metadata', 'Data Governance', 'Metadata Management', 'Data Lineage', 'Data Modeling', 'SQL']",2025-06-12 13:48:41
Data Analyst - Gurugram,Infosys,5 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru', 'PAN INDIA']","Responsibilities:\nUnderstand architecture requirements and ensure effective design, development, validation, and support activities.\nAnalyze user requirements, envisioning system features and functionality.\nIdentify bottlenecks and bugs, and recommend system solutions by comparing advantages and disadvantages of custom development.\nContribute to team meetings, troubleshooting development and production problems across multiple environments and operating platforms.\nEnsure effective design, development, validation, and support activities for Big Data solutions.\nTechnical and Professional Requirements:\nSkills:\nProficiency in Scala, Spark, Hive, and Kafka.\nIn-depth knowledge of design issues and best practices.\nSolid understanding of object-oriented programming.\nFamiliarity with various design, architectural patterns, and software development processes.\nExperience with both external and embedded databases.\nCreating database schemas that represent and support business processes.\nImplementing automated testing platforms and unit tests.\nPreferred Skills:\nTechnology -> Big Data -> Scala, Spark, Hive, Kafka\nAdditional Responsibilities:\nCompetencies:\nGood verbal and written communication skills.\nAbility to communicate with remote teams effectively.\nHigh flexibility to travel.\nEducational Requirements:Master of Computer Applications, Master of Technology, Master of Engineering, MSc, Bachelor of Technology, Bachelor of Computer Applications, Bachelor of Computer Science, Bachelor of Engineering",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Spark', 'Hive', 'Hadoop', 'Big Data', 'Kafka']",2025-06-12 13:48:44
Data Scientist,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nThe Data Scientist is responsible for developing and implementing AI-driven solutions to enhance cybersecurity measures within the organization. This role involves leveraging data science techniques to analyze security data, detect threats, and automate security processes. The Data Scientist will work closely with cybersecurity teams to identify data-driven automation opportunities, strengthening the organizations security posture.\nRoles & Responsibilities:\nDevelop analytics to address security concerns, enhancements, and capabilities to improve the organization's security posture.\nCollaborate with Data Engineers to translate security-focused algorithms into effective solutions.\nWork in technical teams in development, deployment, and application of applied analytics, predictive analytics, and prescriptive analytics.\nPerform exploratory and targeted data analyses using descriptive statistics and other methods to identify security patterns and anomalies.\nDesign and implement security-focused analytics pipelines leveraging MLOps practices.\nCollaborate with data engineers on data quality assessment, data cleansing, and the development of security-related data pipelines.\nContribute to data engineering efforts to refine data infrastructure and ensure scalable, efficient security analytics.\nGenerate reports, annotated code, and other projects artifacts to document, archive, and communicate your work and outcomes.\nShare and discuss findings with team members practicing SAFe Agile delivery model.\nFunctional Skills:\nBasic Qualifications:\nMasters degree and 1 to 3 years of experience with one or more analytic software tools or languages (e.g., SAS, SPSS, R, Python) OR\nBachelors degree and 3 to 5 years of experience with one or more analytic software tools or languages (e.g., SAS, SPSS, R, Python) OR\nDiploma and 7 to 9 years of experience with one or more analytic software tools or languages (e.g., SAS, SPSS, R, Python)\nPreferred Qualifications:\nExperience with one or more analytic software tools or languages (e.g., SAS, SPSS, R, Python)\nDemonstrated skill in the use of applied analytics, descriptive statistics, feature extraction and predictive analytics on industrial datasets\nStrong foundation in machine learning algorithms and techniques\nExperience in statistical techniques and hypothesis testing, experience with regression analysis, clustering and classification\nGood-to-Have Skills:\nProficiency in Python and relevant ML libraries (e.g., TensorFlow, PyTorch, Scikit-learn)\nOutstanding analytical and problem-solving skills; Ability to learn quickly; Excellent communication and interpersonal skills\nExperience with data engineering and pipeline development\nExperience in analyzing time-series data for forecasting and trend analysis\nExperience with AWS, Azure, or Google Cloud\nExperience with Databricks platform for data analytics and MLOps\nExperience with Generative AI models (e.g., GPT, DALLE, Stable Diffusion) and their applications in cybersecurity and data analysis\nExperience working in Product team's environment\nExperience working in an Agile environment\nProfessional Certifications:\nAny AWS Developer certification (preferred)\nAny Python and ML certification (preferred)\nAny SAFe Agile certification (preferred)\nSoft Skills:\nInitiative to explore alternate technology and approaches to solving problems\nSkilled in breaking down problems, documenting problem statements, and estimating efforts\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data science', 'R', 'PyTorch', 'SAS', 'predictive analytics', 'Scikit-learn', 'SPSS', 'machine learning', 'data engineering', 'Python', 'TensorFlow']",2025-06-12 13:48:46
"Delivery Head - Infrastructure Engineering, Data Center",Bajaj Allianz General Insurance Company Limited,15 - 20 years,Not Disclosed,['Pune'],"The role requires strong leadership, strategic thinking, and the ability to drive innovation and efficiency within the technology department. It demands extensive experience in leading complex data center infrastructures, focusing on servers, SAN storage, high availability, disaster recovery, and hybrid environments, including data center operations and physical servers (blade and rack). Responsibilities include designing and testing backup strategies, maintaining documentation, ensuring compliance with regulations, and conducting product and vendor evaluations. Collaboration with various IT teams, the security team, and business stakeholders is essential\n",,,,"['process setting', 'document management', 'vmware', 'center', 'microsoft azure', 'itil service management', 'storage', 'shuttering', 'analysis', 'problem management', 'change management', 'cloud', 'data center', 'operations', 'service delivery', 'incident management', 'leadership', 'it infrastructure management', 'itil']",2025-06-12 13:48:48
Data Techology Senior Associate,MSCI Services,4 - 7 years,Not Disclosed,['Pune'],"Overview\nThe Data Technology team at MSCI is responsible for meeting the data requirements across various business areas, including Index, Analytics, and Sustainability. Our team collates data from multiple sources such as vendors (e.g., Bloomberg, Reuters), website acquisitions, and web scraping (e.g., financial news sites, company websites, exchange websites, filings). This data can be in structured or semi-structured formats. We normalize the data, perform quality checks, assign internal identifiers, and release it to downstream applications.\nResponsibilities\nAs data engineers, we build scalable systems to process data in various formats and volumes, ranging from megabytes to terabytes. Our systems perform quality checks, match data across various sources, and release it in multiple formats. We leverage the latest technologies, sources, and tools to process the data. Some of the exciting technologies we work with include Snowflake, Databricks, and Apache Spark.\nQualifications\nCore Java, Spring Boot, Apache Spark, Spring Batch, Python. Exposure to sql databases like Oracle, Mysql, Microsoft Sql is a must. Any experience/knowledge/certification on Cloud technology preferrably Microsoft Azure or Google cloud platform is good to have. Exposures to non sql databases like Neo4j or Document database is again good to have.\n What we offer you\nTransparent compensation schemes and comprehensive employee benefits, tailored to your location, ensuring your financial security, health, and overall wellbeing.\nFlexible working arrangements, advanced technology, and collaborative workspaces.\nA culture of high performance and innovation where we experiment with new ideas and take responsibility for achieving results.\nA global network of talented colleagues, who inspire, support, and share their expertise to innovate and deliver for our clients.\nGlobal Orientation program to kickstart your journey, followed by access to our Learning@MSCI platform, LinkedIn Learning Pro and tailored learning opportunities for ongoing skills development.\nMulti-directional career paths that offer professional growth and development through new challenges, internal mobility and expanded roles.\nWe actively nurture an environment that builds a sense of inclusion belonging and connection, including eight Employee Resource Groups. All Abilities, Asian Support Network, Black Leadership Network, Climate Action Network, Hola! MSCI, Pride & Allies, Women in Tech, and Women’s Leadership Forum.\nAt MSCI we are passionate about what we do, and we are inspired by our purpose – to power better investment decisions. You’ll be part of an industry-leading network of creative, curious, and entrepreneurial pioneers. This is a space where you can challenge yourself, set new standards and perform beyond expectations for yourself, our clients, and our industry.\nMSCI is a leading provider of critical decision support tools and services for the global investment community. With over 50 years of expertise in research, data, and technology, we power better investment decisions by enabling clients to understand and analyze key drivers of risk and return and confidently build more effective portfolios. We create industry-leading research-enhanced solutions that clients use to gain insight into and improve transparency across the investment process.\nMSCI Inc. is an equal opportunity employer. It is the policy of the firm to ensure equal employment opportunity without discrimination or harassment on the basis of race, color, religion, creed, age, sex, gender, gender identity, sexual orientation, national origin, citizenship, disability, marital and civil partnership/union status, pregnancy (including unlawful discrimination on the basis of a legally protected parental leave), veteran status, or any other characteristic protected by law. MSCI is also committed to working with and providing reasonable accommodations to individuals with disabilities. If you are an individual with a disability and would like to request a reasonable accommodation for any part of the application process, please email Disability.Assistance@msci.com and indicate the specifics of the assistance needed. Please note, this e-mail is intended only for individuals who are requesting a reasonable workplace accommodation; it is not intended for other inquiries.\n To all recruitment agencies\nMSCI does not accept unsolicited CVs/Resumes. Please do not forward CVs/Resumes to any MSCI employee, location, or website. MSCI is not responsible for any fees related to unsolicited CVs/Resumes.\n Note on recruitment scams\nWe are aware of recruitment scams where fraudsters impersonating MSCI personnel may try and elicit personal information from job seekers. Read our full note on careers.msci.com",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'access', 'scala', 'pyspark', 'data warehousing', 'hibernate', 'research', 'sql', 'analytics', 'spring', 'java', 'spring batch', 'spark', 'gcp', 'mysql', 'html', 'hadoop', 'big data', 'etl', 'snowflake', 'python', 'oracle', 'data analysis', 'microsoft azure', 'power bi', 'sql server', 'javascript', 'data bricks', 'spring boot', 'tableau', 'neo4j', 'aws', 'sql database']",2025-06-12 13:48:51
"Senior Python Developer (Machine Learning,Data Analysis,Visualization)",Synechron,3 - 5 years,Not Disclosed,"['Pune', 'Hinjewadi']","Software Requirements\nRequired Skills:\nProficiency in Python (version 3.6+) with experience in data analysis, manipulation, and scripting\nKnowledge of SQL for data extraction, transformation, and database querying\nExperience with data visualization tools such as PowerBI, Tableau, or QlikView\nFamiliarity with AI and Machine Learning frameworks such as TensorFlow, Keras, PyTorch, or equivalent\nHands-on experience in developing, deploying, and optimizing machine learning models\nPreferred Skills:\nExperience with R for data analysis\nFamiliarity with cloud platforms like AWS, Azure, or GCP for deploying AI solutions\nKnowledge of version control systems such as Git\nOverall Responsibilities\nAnalyze, interpret, and visualize large and complex datasets to extract actionable insights\nDesign, develop, and implement machine learning and AI models for predictive and prescriptive analytics\nCollaborate with cross-functional teams to understand business requirements and translate them into data-driven solutions\nCommunicate findings, insights, and recommendations via reports, dashboards, and presentations to stakeholders\nEvaluate and refine models and algorithms to maximize accuracy, efficiency, and impact\nStay informed on emerging AI, Data Science, and analytics trends and incorporate best practices into projects\nSupport automation efforts, optimize data pipelines, and enhance existing analytical workflows\nContribute to organizational learning by sharing knowledge and mentoring team members\nStrategic objectives:\nDrive innovation through the application of AI and machine learning\nEnable data-driven decision-making across business units\nImprove operational efficiencies and business outcomes\nPerformance outcomes:\nAccurate, robust, and scalable AI models\nHigh-quality insights delivered on time and aligned with business needs\nWell-documented solutions and knowledge-sharing artifacts\nTechnical Skills (By Category)\nProgramming Languages (Essential):\nPython (required); experience with R is a plus\nSQL (required); experience with data manipulation and querying\nData Analysis & Visualization Tools (Essential):\nPowerBI, Tableau, or QlikView\nFrameworks & Libraries (Essential):\nTensorFlow, Keras, PyTorch, or similar frameworks for AI/ML development\nData Management & Databases (Essential):\nRelational databases (e.g., MySQL, PostgreSQL, Oracle)\nData extraction and transformation (ETL processes)\nCloud & Deployment (Preferred):\nExperience deploying models on cloud platforms such as AWS, Azure, GCP\nDevelopment & Version Control (Preferred):\nGit for code versioning\nOther Skills:\nStrong statistical knowledge and experience with data preprocessing, feature engineering\nFamiliarity with agile development methodologies\nExperience Requirements\n3 to 5 years of relevant experience in AI, Data Science, or Data Analytics roles\nProven track record applying machine learning techniques to real-world problems\nExperience working with large datasets and scalable data pipelines\nExperience collaborating with cross-functional teams to deliver analytics-driven solutions\nIndustry experience in finance, healthcare, retail, or similar data-rich sectors is preferred\nAlternative pathways:\nCandidates with extensive AI & ML project experience, strong programming skills, and relevant certifications can be considered with slightly varied years of experience\nDay-to-Day Activities\nCollect, clean, and explore large datasets to identify patterns and insights\nDevelop and tune machine learning models to address business problems\nCollaborate with business analysts, data engineers, and product owners to align technical solutions with organizational goals\nDocument methodologies, code, and analytical findings to ensure reproducibility and knowledge sharing\nCreate dashboards, visualizations, and reports to communicate insights effectively\nEvaluate model performance regularly and optimize models for accuracy and efficiency\nParticipate in team meetings, project planning, and review sessions\nKeep abreast of advancements in AI/ML technologies, tools, and best practices\nQualifications\nBachelors degree in Computer Science, Data Science, Statistics, or related field\nMasters degree or higher in AI, Data Science, or related disciplines is a plus\nProfessional certifications in AI/ML (e.g., TensorFlow Developer, AWS Machine Learning Specialty) are advantageous\nWilling to learn new tools and stay updated with emerging AI trends\nAbility to work independently and collaborate effectively in a dynamic environment\nProfessional Competencies\nAnalytical and problem-solving mindset with a focus on actionable insights\nExcellent verbal and written communication skills for diverse audiences\nStrong interpersonal skills and stakeholder management\nAdaptability to fast-changing technology landscapes\nGrowth mindset with continuous learning enthusiasm\nOrganizational skills to handle multiple projects and priorities simultaneously\nInnovation-driven approach and proactive problem resolution",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'PostgreSQL', 'MySQL', 'Data Analysis', 'Data Visualization', 'Oracle', 'ETL', 'Machine Learning']",2025-06-12 13:48:54
"Senior Data Scientist (AI/ML, Data Analysis, Cloud (AWS), and Model",Synechron,8 - 13 years,Not Disclosed,['Pune'],"job requisition idJR1027352\n\nJob Summary\nSynechron is seeking an analytical and innovative Senior Data Scientist to support and advance our data-driven initiatives. The ideal candidate will have a solid understanding of data science principles, hands-on experience with AI/ML tools and techniques, and the ability to interpret complex data sets to deliver actionable insights. This role contributes to the organizations strategic decision-making and technology innovation by applying advanced analytics and machine learning models in a collaborative environment.\n\nSoftware\n\nRequired\n\nSkills:\nPython (including libraries such as pandas, scikit-learn, TensorFlow, PyTorch) proficiency in developing and deploying models\nR (optional, but preferred)\nData management tools (SQL, NoSQL databases)\nCloud platforms (preferably AWS or Azure) for data storage and ML deployment\nJupyter Notebooks or similar interactive development environments\nVersion control tools such as Git\nPreferred\n\nSkills:\nBig data technologies (Spark, Hadoop)\nModel deployment tools (MLflow, Docker, Kubernetes)\nData visualization tools (Tableau, Power BI)\nOverall Responsibilities\nAnalyze and interpret large and complex data sets to generate insights for business and technology initiatives.\nAssist in designing, developing, and implementing AI/ML models and algorithms to solve real-world problems.\nCollaborate with cross-functional teams including data engineers, software developers, and business analysts to integrate models into production systems.\nStay current with emerging trends, research, and best practices in AI/ML/Data Science and apply them to ongoing projects.\nDocument methodologies, modeling approaches, and insights clearly for technical and non-technical stakeholders.\nSupport model validation, testing, and performance monitoring to ensure accuracy and reliability.\nContribute to the development of data science workflows and standards within the organization.\nPerformance Outcomes:\nAccurate and reliable data models that support strategic decision-making.\nClear documentation and communication of findings and recommendations.\nEffective collaboration with technical teams to deploy scalable models.\nContinuous adoption of best practices in AI/ML and data management.\nTechnical Skills (By Category)\n\nProgramming Languages:\nEssential: Python (best practices in ML development), SQL\nPreferred: R, Java (for integration purposes)\nDatabases/Data Management:\nSQL databases, NoSQL (MongoDB, Cassandra)\nCloud data storage solutions (AWS S3, Azure Blob Storage)\nCloud Technologies:\nAWS (S3, EC2, SageMaker, Lambda)\nAzure Machine Learning (preferred)\nFrameworks & Libraries:\nTensorFlow, PyTorch, scikit-learn, Keras, XGBoost\nDevelopment Tools & Methodologies:\nJupyter Notebooks, Git, CI/CD pipelines\nAgile and Scrum processes\nSecurity Protocols:\nBest practices in data security and privacy, GDPR compliance\nExperience\n8+ years of professional experience in AI, ML, or Data Science roles.\nProven hands-on experience designing and deploying ML models in real-world scenarios.\nDemonstrated ability to analyze complex data sets and translate findings into business insights.\nPrevious experience working with cloud-based data science solutions is preferred.\nStrong portfolio showcasing data science projects, models developed, and practical impact.\nAlternative Pathways:\nCandidates with extensive research or academic experience in AI/ML can be considered, provided they demonstrate practical application of skills.\n\nDay-to-Day Activities\nConduct data exploration, cleaning, feature engineering, and model development.\nCollaborate with data engineers to prepare data pipelines for model training.\nBuild, validate, and refine machine learning models.\nPresent insights, models, and recommendations to technical and business stakeholders.\nSupport deployment of models into production environments.\nMonitor model performance and iterate to improve effectiveness.\nParticipate in team meetings, project planning, and reviewing progress.\nDocument methodologies and maintain version control of codebase.\nQualifications\nBachelors degree in Computer Science, Mathematics, Statistics, Data Science, or a related field; Masters or PhD highly desirable.\nEvidence of relevant coursework, certifications, or professional training in AI/ML.\nProfessional certifications (e.g., AWS Certified Machine Learning Specialty, Microsoft Certified Data Scientist) are a plus.\nCommitment to ongoing professional development in AI/ML methodologies.\nProfessional Competencies\nStrong analytical and critical thinking to solve complex problems.\nEffective communication skills for technical and non-technical audiences.\nDemonstrated ability to work collaboratively in diverse teams.\nAptitude for learning new tools, techniques, and technologies rapidly.\nInnovation mindset with a focus on applying emerging research.\nStrong organizational skills to manage multiple projects and priorities.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['java', 'data science', 'python', 'deploying models', 'aws', 'continuous integration', 'kubernetes', 'scikit-learn', 'ci/cd', 'artificial intelligence', 'sql', 'docker', 'tensorflow', 'spark', 'pytorch', 'keras', 'hadoop', 'big data', 'mongodb', 'microsoft azure', 'nosql', 'pandas', 'amazon ec2', 'r', 'cassandra', 'agile']",2025-06-12 13:48:56
Data Loss Prevention Engineer,Credent Infotech Solutions Llp,2 - 3 years,Not Disclosed,['Mumbai( kandivali )'],"Daily Monitoring and Investigation\n\nMonitor DLP alerts across email, endpoint, web, and cloud.\nPerform triage to determine false positives, true positives, and actual incidents.\nDocument findings and escalate critical violations per SOPs.\nIncident Response Support\n\nSupport incident response by providing evidence, logs, and context around DLP policy violations.\nCoordinate with IT, HR, and Legal teams for user engagement, awareness, and disciplinary action if necessary.\nParticipate in Root Cause Analysis (RCA) for recurring or high-severity incidents.\nPolicy Tuning and Optimization\n\nAnalyse alert trends and false positive patterns to suggest and implement policy refinements.\nWork with business and security teams to validate policy changes and test updated rulesets before production deployment.\nMaintain documentation of policy changes, rationales, and approvals.\nLifecycle Management\n\nSupport onboarding business units, or geographies into DLP coverage.\nMaintain and update DLP dashboards and reporting structures.\nStakeholder Communication\n\nProvide regular reports to CISO on DLP violations\nInterface with Data Owners, Business Units, and Compliance teams for policy alignment and exception management.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Endpoint Protection', 'Incident Response', 'Symantec Dlp', 'SIEM', 'Data Classification', 'Risk Assessment', 'Compliance', 'Powershell', 'Information Security', 'Dlp', 'Casb Netskope', 'Data Protection Manager', 'Microsoft Purview']",2025-06-12 13:49:00
Engineer- Data Support,Powerica,3 - 5 years,Not Disclosed,['Mumbai'],Job profile\na. Co-ordinate with project team\nb. Upload BVQ in oracle system\nc. BOQ revision and making entries.\nd. Data Accuracy\ne. Process management.\n\nDiploma in Engineering\nExp 3-5 Years’ experience.,Industry Type: Power,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Boq Preparation', 'Oracle', 'BVQ']",2025-06-12 13:49:02
Specialist Data Security Engineer,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role the Specialist Data Security Engineer covering Data Loss Prevention (DLP) and Cloud Access Security Broker (CASB) technologies. This role will report to the Manager, Data Security. This position will provide essential services that enable us to better pursue our mission.\nSpecialist Data Security Engineers operate, manage, and improve Amgens DLP and Cloud Access Security Broker (CASB) solutions. In our Data Security team, they will identify emerging risks related to changes in cloud technologies, advise management, and develop technical remediations to address those risks. Specialists lead the development of processes and procedures for multiple solutions which enable business units to remediate identify cloud data exposures. They run multiple projects simultaneously to implement and improve the cloud data security protection and use advanced analytics to demonstrate success.\nThis engineer will play a key role in educating and evangelizing to technologists and business leaders the security strategies that both protect and enable business processes related to cloud data handling.\nRoles & Responsibilities:\nDesigns, operates, maintains, and enhances capabilities for the technical systems that ensure protection of data for all Amgen global operations.\nIdentifies new risk areas for data and plans controls to mitigate those risks.\nResearches new technologies, processes, and approaches based on industry practices and recommends future plans for data protection.\nAuthors procedures and guidelines and advises on policies related to data protection requirements and remediation or investigation of violations.\nDevelops and conducts training on data protection technologies for operations staff. Educates business leadership about data risk.\nAdvises other technology groups on data protection strategies and recommends appropriate points of both technical and process integration.\nPartners with the Manager of Data Security to liaise to legal and human resources leadership on violation remediations.\nCollaborates with cloud strategy leaders and business unit leadership to ensure that cloud data protection is incorporated by design into new business projects.\nCollaborates with Cloud Security Engineers to integrate cloud data protection technology into the operations of traditional Data Loss Prevention operations.\n\nBasic Qualifications:\nMasters degree and 4 to 6 years of experience OR\nBachelors degree and 6 to 8 years of experience OR\nDiploma and 10 to 12 years of experience.\nFunctional Skills:\nMust-Have Skills\nFamiliarity with one or more security frameworks, especially in regulated environments.\nProficiency specifying requirements for technical systems, as well as designing, implementing, and operating those systems.\nExpertise in global IT operations, including an understanding of regulatory and cultural differences encountered when dealing with international peers and customers.\nDemonstrated competence maintaining applications on Windows and Linux based operating systems, and basic understanding of one or more programming or scripting languages.\nDemonstrated proficiency with one or more Cloud Access Security Platforms (Elastica, Netskope, SkyHigh,etc)\nTrack record of project management leadership, preferably using Agile methodology.\nDeep knowledge of the principles of Data Protection, including availability, integrity, and confidentiality of data.\n\n\nGood-to-Have Skills:\nProficiency with communications focused on both the development of written technical processes and the ability to convey complex ideas clearly in front of an audience.\nExperience with data analytics focused on building executive reports\nReputation of successfully navigating large enterprise environments, understanding both ITIL driven processes and business relationship building\nAbility to self-direct work on multiple priorities with little to no oversight, based on critical initiatives.\nProfessional Certifications (please mention if the certification is preferred or required for the role):\nSystems Security Certified Practitioner (SSCP) or Security+\nSANS Certifications\nCloud security certifications\nRelevant vendor-specific certifications\n\n\nSoft Skills:\nEstablished analytical and gap/fit assessment skills.\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals\nEffective presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Security Engineering', 'SkyHigh', 'Data Protection', 'Cloud Access Security Platforms', 'Netskope', 'security frameworks', 'Agile methodology', 'IT operations', 'ITIL', 'Elastica']",2025-06-12 13:49:05
Data Science,Global Banking Organization,5 - 10 years,Not Disclosed,['Bengaluru'],"Key Skills: Machine Learning, Data Science, Azure, Python, Hadoop.\nRoles and Responsibilities:\nStrong understanding of Math, Statistics, and the theoretical foundations of Statistical & Machine Learning, including Parametric and Non-parametric models.\nApply advanced data mining techniques to curate, process, and transform raw data into reliable datasets.\nUse various statistical techniques and ML methods to perform predictive modeling/classification for problems related to clients, distribution, sales, client profiles, and segmentation, and provide actionable insights for business decision-making.\nDemonstrate expertise in the full Machine Learning lifecycle--feature engineering, training, validation, scaling, deployment, scoring, monitoring, and feedback loops.\nProficiency in Python visualization libraries such as matplotlib and seaborn.\nExperience with cloud computing infrastructure like Azure, including Machine Learning Studio, Azure Data Factory, Synapse, Python, and PySpark.\nAbility to develop, test, and deploy models on cloud/web platforms.\nExcellent knowledge of Deep Learning Architectures, including Convolutional Neural Networks and Transformer/LLM Foundation Models.\nStrong expertise in supervised and adversarial learning techniques.\nRobust working knowledge of deep learning frameworks such as TensorFlow, Keras, and PyTorch.\nExcellent Python coding skills.\nExperience with version control tools (Git, GitHub/GitLab) and data version control.\nExperience in end-to-end model deployment and productionization.\nDemonstrated proficiency in deploying, scaling, and optimizing ML models in production environments with low latency, high availability, and cost efficiency.\nSkilled in model interpretability and CI/CD for ML using tools like MLflow and Kubeflow, with the ability to implement automated monitoring, logging, and retraining strategies.\nExperience Requirement:\n5-12 years of experience in designing and deploying deep learning and machine learning solutions.\nProven track record of delivering AI/ML solutions in real-world business applications at scale.\nHands-on experience working in cross-functional teams including data engineers, product managers, and business stakeholders.\nExperience mentoring junior data scientists and providing technical leadership within a data science team.\nExperience working with big data tools and environments such as Hadoop, Spark, or Databricks is a plus.\nPrior experience in managing model lifecycle in enterprise production environments including drift detection and retraining pipelines.\nEducation: B.Tech.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Azure', 'Hadoop.', 'Machine Learning', 'Python']",2025-06-12 13:49:07
Data Center Critical Facilities Engineer (Electrical),Equinix,3 - 4 years,Not Disclosed,['Mumbai'],"Who are we?\nEquinix is the world s digital infrastructure company , operating over 260 data centers across the globe. Digital leaders harness Equinixs trusted platform to bring together and interconnect foundational infrastructure at software speed. Equinix enables organizations to access all the right places, partners and possibilities to scale with agility, speed the launch of digital services, deliver world-class experiences and multiply their value, while supporting their sustainability goals.\nJoining our operations team means that you will be at the forefront of all we do, maintaining critical facilities infrastructure as part of a close-knit team delivering best-in-class service to our data center customers. We embrace diversity in thought and contribution and are committed to providing an equitable work environment that is foundational to our core values as a company and is vital to our success.",,,,"['Capacity management', 'Access control', 'Protection system', 'Compliance', 'Infrastructure', 'Corrective maintenance', 'Site administration', 'Vendor', 'Fire protection', 'Electricals']",2025-06-12 13:49:09
Senior Data Analyst-Azure Data Factory,Lumen Technologies,8 - 12 years,Not Disclosed,['Bengaluru'],"Were looking for a Senior Data Analyst with a strong foundation in Azure-based data engineering and Machine Learning to design, develop, and optimize robust data pipelines, applications, and analytics infrastructure. This role demands deep technical expertise, cross-functional collaboration, and the ability to align data solutions with dynamic business needs.\nKey Responsibilities:\nData Pipeline Development:\nDesign and implement efficient data pipelines using Azure Databricks with PySpark to transform and process large datasets.\nOptimize data workflows for scalability, reliability, and performance.\nApplication Integration:\nCollaborate with cross-functional teams to develop APIs using the .NET Framework for Azure Web Application integration.\nEnsure smooth data exchange between applications and downstream systems.\nData Warehousing and Analytics:\nBuild and manage data warehousing solutions using Synapse Analytics and Azure Data Factory (ADF).\nDevelop and maintain reusable and scalable data models to support business intelligence needs.\nAutomation and Orchestration:\nUtilize Azure Logic Apps, Function Apps, and Azure DevOps to automate workflows and streamline deployments.\nImplement CI/CD pipelines for efficient code deployment and testing.\nInfrastructure Management:\nOversee Azure infrastructure management and maintenance, ensuring a secure and optimized environment.\nProvide support for performance tuning and capacity planning.\nBusiness Alignment:\nGain a deep understanding of AMO data sources and their business implications.\nWork closely with stakeholders to provide customized solutions aligning with business needs.\nBAU Support:\nMonitor and support data engineering workflows and application functionality in BAU mode.\nTroubleshoot and resolve production issues promptly to ensure business continuity.\nTechnical Expertise:\nProficiency in Microsoft SQL for complex data queries and database management.\nAdvanced knowledge of Azure Databricks and PySpark for data engineering and ETL processes.\nExperience with Azure Data Factory (ADF) for orchestrating data workflows.\nExpertise in Azure Synapse Analytics for data integration and analytics.\nProficiency in .NET Framework for API development and integration.\nCloud and DevOps Skills:\nStrong experience in Azure Infrastructure Management and optimization.\nHands-on knowledge of Azure Logic Apps, Function Apps, and Azure DevOps for CI/CD automation.\n""We are an equal opportunity employer committed to fair and ethical hiring practices. We do not charge any fees or accept any form of payment from candidates at any stage of the recruitment process. If anyone claims to offer employment opportunities in our company in exchange for money or any other benefit, please treat it as fraudulent and report it immediately.""\n#LI-BS1",Industry Type: Telecom / ISP,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Automation', 'orchestration', 'Infrastructure management', 'Machine learning', 'Business intelligence', 'Business continuity', 'Analytics', 'Downstream', 'Capacity planning']",2025-06-12 13:49:11
"ETL Developer ( SSIS, Informatica ,Talend, Big Data)Group Manager",Vuram,5 - 10 years,Not Disclosed,['Bengaluru'],"Design, develop, and maintain relational and non-relational database systems.\nDefine analytical architecture including datalakes, lakehouse, data mesh and medallion patterns\nAbility to understand & analyze business requirements and translate them into analytical or relational database designAble to design for non-SQL datastores\nOptimize SQL queries, stored procedures, and database performance.Create and maintain ETL processes for data integration from various sources.\nWork closely with application teams to design database schemas and support integration.Monitor, troubleshoot, and resolve database issues related to performance, storage, and replication.Implement data security, backup, recovery, and disaster recovery procedures.\nEnsure data integrity and enforce best practices in database development.\nParticipate in code reviews and mentor junior developers.Collaborate with business and analytics teams for reporting and data warehousing needs.Must Have: Strong expertise in SQL and PL/SQL\nHands-on experience with at least one RDBMS: SQL Server, Oracle, PostgreSQL, or MySQL\nExperience with NoSQL databases: MongoDB, Cassandra, or Redis (at least one)\nETL Development Tools: SSIS, Informatica, Talend, or equivalent\nExperience in performance tuning and optimization\nDatabase design and modeling tools: Erwin, dbForge, or similar\nCloud platforms: Experience with AWS RDS, Azure SQL, or Google Cloud SQL\nBackup and Recovery Management, High Availability, and Disaster Recovery Planning\nUnderstanding of indexing, partitioning, replication, and sharding\nKnowledge of CI/CD pipelines and DevOps practices for database deployments\nExperience with Big Data technologies (Hadoop, Spark)Experience working in Agile/Scrum environments\n\n\nQualifications\nBachelor s or Master s degree in Computer Science, Information Technology, or a related field.8+ years of relevant experience in database design and development",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'RDBMS', 'Database design', 'MySQL', 'PLSQL', 'Informatica', 'Stored procedures', 'Oracle', 'SSIS', 'SQL']",2025-06-12 13:49:14
Advanced Data Science Associate,ZS,0 - 2 years,Not Disclosed,['Pune'],"ZSs Insights & Analytics group partners with clients to design and deliver solutions to help them tackle a broad range of business challenges. Our teams work on multiple projects simultaneously, leveraging advanced data analytics and problem-solving techniques. Our recommendations and solutions are based on rigorous research and analysis underpinned by deep expertise and thought leadership.\nWhat you'll Do\nDevelop advanced and efficient statistically effective algorithms that solve problems of high",,,,"['Text mining', 'Analytical', 'Management consulting', 'Financial planning', 'Machine learning', 'Hypothesis Testing', 'Predictive modeling', 'Data mining', 'big data']",2025-06-12 13:49:16
Advanced Data Science Associate,ZS,0 - 2 years,Not Disclosed,"['Noida', 'Gurugram']","Develop advanced and efficient statistically effective algorithms that solve problems of high dimensionality .\nUtilize technical skills such as hypothesis testing, machine learning and retrieval processes to apply statistical and data mining techniques to identify trends, create figures, and analyze other relevant information.\nCollaborate with clients and other stakeholders at ZS to integrate and effectively communicate analysis findings.\nContribute to the assessment of emerging datasets and technologies that impact our analytical",,,,"['Text mining', 'Analytical', 'Management consulting', 'Financial planning', 'Machine learning', 'Hypothesis Testing', 'Predictive modeling', 'Data mining', 'big data']",2025-06-12 13:49:18
Data & Analytics Specialist,Hoffmann La Roche,5 - 10 years,Not Disclosed,['Pune'],"At Roche you can show up as yourself, embraced for the unique qualities you bring. Our culture encourages personal expression, open dialogue, and genuine connections, where you are valued, accepted and respected for who you are, allowing you to thrive both personally and professionally. This is how we aim to prevent, stop and cure diseases and ensure everyone has access to healthcare today and for generations to come. Join Roche, where every voice matters.\nThe Position The Position\nWe are looking for a Data & Analytics Specialist/ Business Analyst who will join us in the newly setup Integrated Informatics for a journey to drive transformation with data and foster automated and efficient decision making throughout the organisation\nThe Data and Analytics Specialist must be the big-picture thinker who understands the value of data to the organisation, has a strong focus on delivering high value, connecting the dots, investing in right initiatives with reusability at the heart of it.\nIn this position you will be acting as squad lead, have end to end ownership of Product delivery with setting up teams from multiple teams/areas with focus on Lifecycle management of the product\nResponsibilities\nYou will work on various aspects of Analytics Solution Development, Data Management, Governance and Information Architecture including but not limited to:\nCollaborate with business stakeholders to understand their data and analytics needs and develop a product roadmap that aligns with business goals.\nDefine and prioritise product requirements, user stories, and acceptance criteria for data and analytics products and ensure what was agreed gets delivered.\nWork with data engineers and data scientists to develop data pipelines, analytical models, and visualisations that meet business requirements.\nCollaborate with Infrastructure Teams and software developers to ensure that data and analytics products are integrated into existing systems and platforms in a sustainable way that still meets the needs of business to generate the insights necessary to drive their decisions.\nMonitor data and analytics product performance and identify opportunities for improvement.\nStay up-to-date with industry trends and emerging technologies related to data and analytics in the pharmaceutical industry.\nAct as a subject matter expert for data and analytics products and provide guidance to business stakeholders on how to effectively use these products.\nAccountable to Develop and maintain documentation, training materials, and user guides for data and analytics products.\nThe ideal candidate\nBachelors or Masters degree in computer science, information systems, or a related field.\n5+ years of experience in roles such as Senior Data & Analytics Specialist, Data Solutions Lead, Data Architect, or Data Consultant, with a focus on solution design and implementation. Alternatively, 3-4 years of experience in data streams (e.g., Data Science, Data Engineering, Data Governance) combined with a couple of years in Strategic Data Consultancy / Data Product Ownership. Experience in the pharmaceutical or healthcare industry is highly desirable.\nHigh Level understanding of data engineering, data science, Data governance and analytics concepts and technologies.\nExperience working with cross-functional teams, including data engineers, data scientists, and software developers.\nExcellent communication and interpersonal skills.\nStrong analytical and problem-solving skills.\nExperience with agile development methodologies.\nKnowledge of regulatory requirements related to data and analytics in the pharmaceutical industry.\nKnowledge of working with vendor and customer master data for different divisions - Pharmaceuticals, Diagnostic, & Diabetes care.\nUnderstanding of the transparency reporting landscape.\nHands-on experience of working on applications such as Jira, SQL, Postman, SAP GUI, Monday.com, Trello\nProficient in the knowledge of different CRM/Master Data Management systems such as SFDC, Reltio MDM\nUnderstanding data protection laws and consent processes applicable to healthcare professionals and organizations before transparency disclosure.\nWho we are\n.\nBasel is the headquarters of the Roche Group and one of its most important centres of pharmaceutical research. Over 10,700 employees from over 100 countries come together at our Basel/Kaiseraugst site, which is one of Roche`s largest sites. Read more.\nBesides extensive development and training opportunities, we offer flexible working options, 18 weeks of maternity leave and 10 weeks of gender independent partnership leave. Our employees also benefit from multiple services on site such as child-care facilities, medical services, restaurants and cafeterias, as well as various employee events.\nWe believe in the power of diversity and inclusion, and strive to identify and create opportunities that enable all people to bring their unique selves to Roche.\nRoche is an Equal Opportunity Employer.\nWho we are\nA healthier future drives us to innovate. Together, more than 100 000 employees across the globe are dedicated to advance science, ensuring everyone has access to healthcare today and for generations to come. Our efforts result in more than 26 million people treated with our medicines and over 30 billion tests conducted using our Diagnostics products. We empower each other to explore new possibilities, foster creativity, and keep our ambitions high, so we can deliver life-changing healthcare solutions that make a global impact.\n\nLet s build a healthier future, together.\nRoche is an Equal Opportunity Employer.\n""",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'SAP', 'Diagnostics', 'HP data protector', 'Analytical', 'Healthcare', 'JIRA', 'Analytics', 'SQL', 'CRM']",2025-06-12 13:49:21
Data & Analytics Specialist,Roche Diagnostics,5 - 10 years,Not Disclosed,['Pune'],"At Roche you can show up as yourself, embraced for the unique qualities you bring. Our culture encourages personal expression, open dialogue, and genuine connections, where you are valued, accepted and respected for who you are, allowing you to thrive both personally and professionally. This is how we aim to prevent, stop and cure diseases and ensure everyone has access to healthcare today and for generations to come. Join Roche, where every voice matters.\nThe Position The Position\nWe are looking for a Data & Analytics Specialist/ Business Analyst who will join us in the newly setup Integrated Informatics for a journey to drive transformation with data and foster automated and efficient decision making throughout the organisation\nThe Data and Analytics Specialist must be the big-picture thinker who understands the value of data to the organisation, has a strong focus on delivering high value, connecting the dots, investing in right initiatives with reusability at the heart of it.\nIn this position you will be acting as squad lead, have end to end ownership of Product delivery with setting up teams from multiple teams/areas with focus on Lifecycle management of the product\nResponsibilities\nYou will work on various aspects of Analytics Solution Development, Data Management, Governance and Information Architecture including but not limited to:\nCollaborate with business stakeholders to understand their data and analytics needs and develop a product roadmap that aligns with business goals.\nDefine and prioritise product requirements, user stories, and acceptance criteria for data and analytics products and ensure what was agreed gets delivered.\nWork with data engineers and data scientists to develop data pipelines, analytical models, and visualisations that meet business requirements.\nCollaborate with Infrastructure Teams and software developers to ensure that data and analytics products are integrated into existing systems and platforms in a sustainable way that still meets the needs of business to generate the insights necessary to drive their decisions.\nMonitor data and analytics product performance and identify opportunities for improvement.\nStay up-to-date with industry trends and emerging technologies related to data and analytics in the pharmaceutical industry.\nAct as a subject matter expert for data and analytics products and provide guidance to business stakeholders on how to effectively use these products.\nAccountable to Develop and maintain documentation, training materials, and user guides for data and analytics products.\nThe ideal candidate\nBachelors or Masters degree in computer science, information systems, or a related field.\n5+ years of experience in roles such as Senior Data & Analytics Specialist, Data Solutions Lead, Data Architect, or Data Consultant, with a focus on solution design and implementation. Alternatively, 3-4 years of experience in data streams (e.g., Data Science, Data Engineering, Data Governance) combined with a couple of years in Strategic Data Consultancy / Data Product Ownership. Experience in the pharmaceutical or healthcare industry is highly desirable.\nHigh Level understanding of data engineering, data science, Data governance and analytics concepts and technologies.\nExperience working with cross-functional teams, including data engineers, data scientists, and software developers.\nExcellent communication and interpersonal skills.\nStrong analytical and problem-solving skills.\nExperience with agile development methodologies.\nKnowledge of regulatory requirements related to data and analytics in the pharmaceutical industry.\nKnowledge of working with vendor and customer master data for different divisions - Pharmaceuticals, Diagnostic, & Diabetes care.\nUnderstanding of the transparency reporting landscape.\nHands-on experience of working on applications such as Jira, SQL, Postman, SAP GUI, Monday.com, Trello\nProficient in the knowledge of different CRM/Master Data Management systems such as SFDC, Reltio MDM\nUnderstanding data protection laws and consent processes applicable to healthcare professionals and organizations before transparency disclosure.\nWho we are\nAt Roche, more than 100,000 people across 100 countries are pushing back the frontiers of healthcare. Working together, we ve become one of the world s leading research-focused healthcare groups. Our success is built on innovation, curiosity and diversity.\nBasel is the headquarters of the Roche Group and one of its most important centres of pharmaceutical research. Over 10,700 employees from over 100 countries come together at our Basel/Kaiseraugst site, which is one of Roche`s largest sites. Read more.\nBesides extensive development and training opportunities, we offer flexible working options, 18 weeks of maternity leave and 10 weeks of gender independent partnership leave. Our employees also benefit from multiple services on site such as child-care facilities, medical services, restaurants and cafeterias, as well as various employee events.\nWe believe in the power of diversity and inclusion, and strive to identify and create opportunities that enable all people to bring their unique selves to Roche.\nRoche is an Equal Opportunity Employer.\nWho we are\n.\n\nLet s build a healthier future, together.\nRoche is an Equal Opportunity Employer.\n""",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'SAP', 'Diagnostics', 'HP data protector', 'Analytical', 'Healthcare', 'JIRA', 'Analytics', 'SQL', 'CRM']",2025-06-12 13:49:23
Data Analyst - L3,Wipro,3 - 5 years,Not Disclosed,['Hyderabad'],"Role Purpose\nThe purpose of this role is to interpret data and turn into information (reports, dashboards, interactive visualizations etc) which can offer ways to improve a business, thus affecting business decisions.\nDo\n1. Managing the technical scope of the project in line with the requirements at all stages\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\nb. Develop record management process and policies\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\nd. Providing sales data, proposals, data insights and account reviews to the client base\ne. Identify areas to increase efficiency and automation of processes\nf. Set up and maintain automated data processes\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\nh. Produce and track key performance indicators\n2. Analyze the data sets and provide adequate information\na. Liaise with internal and external clients to fully understand data content\nb. Design and carry out surveys and analyze survey data as per the customer requirement\nc. Analyze and interpret complex data sets relating to customers business and prepare reports for internal and external audiences using business analytics reporting tools\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\nf. Develop predictive models and share insights with the clients as per their requirement\n\nDeliver\n\nNoPerformance ParameterMeasure1.Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\nMandatory Skills: Business Analyst/ Data Analyst(Media). Experience: 3-5 Years.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'data validation', 'data mining', 'business analysis', 'data warehousing', 'business analytics', 'dbms', 'dashboards', 'sales', 'analytics reporting', 'reporting tools', 'data integration', 'digital transformation']",2025-06-12 13:49:26
Data & Gen AI Specialist,Altimetrik,1 - 4 years,Not Disclosed,['Bengaluru'],"Job Title: Data & GenAI AWS Specialist\nExperience: 1-4 Years\nLocation: Bangalore\nMandatory Qualification: B.E./ B.Tech/ M.Tech/ MS from IIT or IISc ONLY\nJob Overview:\nWe are seeking a seasoned Data & GenAI Specialist with deep expertise in AWS Managed Services (PaaS) to join our innovative team. The ideal candidate will have extensive experience in designing sophisticated, scalable architectures for data pipelines and Generative AI (GenAI) solutions leveraging cloud services.",,,,"['Generative Ai', 'Cloud', 'Data Science', 'Open Source', 'Data Pipeline', 'GCP', 'Azure Cloud', 'Snowflake', 'Machine Learning', 'AWS']",2025-06-12 13:49:28
Data Architect,Neo Aid,12 - 20 years,48-60 Lacs P.A.,['Bengaluru'],"Data Architect\nBangalore (Pune option).\nHybrid, 2-3 days WFO,\nUp to 60 LPA. Needs GCP, Data Engineering, Analytics, Visualization, Modeling. 1\n5-20 yrs exp, end-to-end data pipeline.\nNotice: 60 days.\nArchitecture - recent 1-2 years\n\n\nProvident fund",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Visualizing', 'Data Architecture', 'Data Modeling', 'Data Analytics', 'Gcp Cloud', 'ETL']",2025-06-12 13:49:30
Data Analyst - Senior,FedEx,4 - 7 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Role & responsibilities :\n\nAct as a technical expert on complex and specialist subject(s).\nSupport management with the analysis, interpretation and application of complex information, contributing to the achievement of divisional and corporate goals. Supports or leads projects by applying area of expertise.\nLead and implement advanced analytical processes through data/text mining, model development, and prediction to enable informed business decisions.\nApply sound analytical expertise to examine structured and unstructured data from multiple disparate sources to provide insights and recommend high-quality solutions to leadership across levels.\nPlan initiatives from concept to execution with minimal supervision and communicate results to a broad range of audiences. Develops a superior understanding of pricing and revenue management through internal and external sources to creatively solve business problems and lead the team from concept to execution of projects.\nTypically uses data, statistical and quantitative analysis, modeling, and fact-based management to drive decision-making. Provides regular expert consultative advice to senior leadership.\nEffectively shares best practices and fosters knowledge sharing across teams. Provides crossteam and cross-org consultation and supports communities of practice excellence.\n\n\n\nPreferred candidate profile\n\nRelevant experience in analytics/consulting/informatics and statistics\nKey Skills - Data and Business Analytics, Advanced Statistics and Predictive Modelling,\nStakeholder Management, Project Management\nExperience in pricing and revenue management yield management, customer segmentation analytics, revenue impact analytics, etc. is a plus\nExposure to predictive analytics, ML/ AI techniques is an added advantage\nTools - Oracle, SQL Server, Teradata, SAS, Python, Tableau/PowerBI/Spotfire\nGood to have cloud computing, big data, Azure",Industry Type: Courier / Logistics (Logistics Tech),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'Business Insights', 'Python', 'SQL', 'Power Bi', 'Business Acumen', 'Tableau']",2025-06-12 13:49:32
Data Science Manager,ZS,10 - 15 years,Not Disclosed,"['Pune', 'Bengaluru']","A key enabler of our services is leveraging data in delivering client solutions. The data available about customers is getting richer and the problems that our customers are trying to answer continue to evolve. In our endeavor to stay ahead in providing solutions to these evolving complex problems, ZS has set up an Advanced Data Science which has three major focus areas:\nResearch the evolving datasets and advanced analytical techniques to develop new offerings/solutions\nDeliver client impact by collaboratively implementing these solutions",,,,"['Team management', 'data science', 'Pharma', 'Analytical', 'Management consulting', 'Financial planning', 'Healthcare', 'Project planning', 'Predictive modeling', 'Financial services']",2025-06-12 13:49:35
ETL Developer - Data & Analytics,Canpack India,3 - 5 years,Not Disclosed,['Pune'],"Giorgi Global Holdings, Inc. ( GGH ) is a privately held, diversified consumer products/packaging company with approximately 11,000 employees and operations in 20 countries. GGH consists of four US based companies ( The Giorgi Companies ) and one global packaging company ( CANPACK ).\nGGH has embarked on a transformation journey to become a digital, technology enabled, customer-centric, data and insights-driven organization. This transformation is evolving our business, strategy, core operations and IT solutions.\nAs an ETL Developer, you will be an integral part of our Data and Analytics team, working closely with the ETL Architect and other developers to design, develop, and maintain efficient data integration and transformation solutions. We are looking for a highly skilled ETL Developer with a deep understanding of ETL processes and data warehousing. The ideal candidate is passionate about optimizing data extraction, transformation, and loading workflows, ensuring high performance, accuracy, and scalability to support business intelligence initiatives.\nWhat you will do:\n1. Design, develop, test and maintain ETL processes and data pipelines to support data integration and transformation needs.\n2. Continuously improve ETL performance and reliability through best practices and optimization techniques.\n3. Develop and implement data validation and quality checks to ensure the integrity and consistency of data.\n4. Collaborate with ETL Architect, Data Engineers, and Business Intelligence teams to understand business requirements and translate them into technical solutions.\n5. Monitor, troubleshoot, and resolve ETL job failures, performance bottlenecks, and data discrepancies.\n6. Proactively identify and resolve ETL-related issues, minimizing impact on business operations.\n7. Contribute to documentation, training, and knowledge sharing to enhance team capabilities.\n8. Communicate progress and challenges clearly to both technical and non-technical teams\nEssential Requirements:\nBachelor s or master s degree in information technology, Computer Science, or a related field.\n3-5 years of relevant experience.\nPower-BI, Tabular Editor/Dax Studio, ALM/Github/Azure Devops skills\nExposure to SAP Systems/Modules like SD, MD, etc. to understand functional data.,\nExposure to MS Fbric, MS Azure Synapse Analytics\nCompetencies needed:\n- Hands-on experience with ETL development and data integration for large-scale systems\n- Experience with platforms such as Synapse Analytics, Azure Data Factory, Fabric, Redshift or Databricks\n- A solid understanding of data warehousing and ETL processes\n- Advanced SQL and PL/SQL skills such as query optimization, complex joins, window functions\n- Expertise in Python (pySpark) programming with a focus on data manipulation and analysis\n- Experience with Azure DevOps and CI/CD process\n- Excellent problem-solving and analytical skills\n- Experience in creating post-implementation documentation\n- Strong team collaboration skills\n- Attention to detail and a commitment to quality\nStrong interpersonal skills including analytical thinking, creativity, organizational abilities, high commitment, initiative in task execution, and a fast-learning capability for understanding IT concepts\n\nIf you are a current CANPACK employee, please apply through your Workday account .\nCANPACK Group is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, colour, religion, age, sex, sexual orientation, gender identity, national origin, disability, or any other characteristic protected by law or not related to job requirements, unless such distinction is required by law.",Industry Type: Packaging & Containers,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'SAP', 'Analytical', 'Packaging', 'PLSQL', 'Business intelligence', 'Information technology', 'Analytics', 'Python', 'Data extraction']",2025-06-12 13:49:38
Senior Data Scientist,Epsilon,6 - 9 years,Not Disclosed,['Bengaluru'],"Responsibilities: -\nContribute and build an internal product library that is focused on solving business problems related to prediction & recommendation.\nResearch unfamiliar methodologies, techniques to fine tune existing models in the product suite and, recommend better solutions and/or technologies.\nImprove features of the product to include newer machine learning algorithms in the likes of product recommendation, real time predictions, fraud detection, offer personalization etc\nCollaborate with client teams to on-board data, build models and score predictions.\nParticipate in building automations and standalone applications around machine learning algorithms to enable a One Click solution to getting predictions and recommendations.\nAnalyze large datasets, perform data wrangling operations, apply statistical treatments to filter and fine tune input data, engineer new features and eventually aid the process of building machine learning models.\nRun test cases to tune existing models for performance, check criteria and define thresholds for success by scaling the input data to multifold.\nDemonstrate a basic understanding of different machine learning concepts such as Regression, Classification, Matrix Factorization, K-fold Validations and different algorithms such as Decision Trees, Random Forrest, K-means clustering.\nDemonstrate working knowledge and contribute to building models using deep learning techniques, ensuring robust, scalable and high-performance solutions\nMinimum Qualifications:\nEducation: Master's or PhD in a quantitative discipline (Statistics, Economics, Mathematics, Computer Science) is highly preferred.\nDeep Learning Mastery: Extensive experience with deep learning frameworks (TensorFlow, PyTorch, or Keras) and advanced deep learning projects across various domains, with a focus on multimodal data applications.\nGenerative AI Expertise: Proven experience with generative AI models and techniques, such as RAG, VAEs, Transformers, and applications at scale in content creation or data augmentation.\nProgramming and Big Data: Expert-level proficiency in Python and big data/cloud technologies (Databricks and Spark) with a minimum of 4-5 years of experience.\nRecommender Systems and Real-time Predictions: Expertise in developing sophisticated recommender systems, including the application of real-time prediction frameworks.\nMachine Learning Algorithms: In-depth experience with complex algorithms such as logistic regression, random forest, XGBoost, advanced neural networks, and ensemble methods.\nExperienced with machine learning algorithms such as logistic regression, random forest, XG boost, KNN, SVM, neural network, linear regression, lasso regression and k-means.\nDesirable Qualifications:\nGenerative AI Tools Knowledge: Proficiency with tools and platforms for generative AI (such as OpenAI, Hugging Face Transformers).\nDatabricks and Unity Catalog: Experience leveraging Databricks and Unity Catalog for robust data management, model deployment, and tracking.\nWorking experience in CI/CD tools such as GIT & BitBucket",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Data Engineering', 'Pyspark', 'Azure Aws', 'Generative AI', 'Big Data', 'AWS', 'Data Bricks', 'Deep Learning', 'Python', 'SQL']",2025-06-12 13:49:40
Senior Engineer II,AMERICAN EXPRESS,8 - 13 years,Not Disclosed,['Bengaluru'],"Join Team Amex and lets lead the way together.\nAmerican Express is looking for Senior Engineers to contribute to the company s focus on building products, like @Work, to support our large and global corporate clients. @Work helps our clients manage their Corporate Card and Corporate Purchasing Card programs more efficiently online. From performing everyday administrative tasks and account maintenance, to accessing reports and utilizing reconciliation solutions, @Work enables fast, efficient and effective program management resulting in time and cost savings for our clients.",,,,"['Computer science', 'Administration', 'Career development', 'Maven', 'Finance', 'Reconciliation', 'MySQL', 'Workflow', 'Monitoring', 'SQL']",2025-06-12 13:49:43
Enterprise Data Operations Manager,Pepsico,12 - 17 years,Not Disclosed,['Hyderabad'],"Overview\n\nDeputy Director - Data Engineering\n\nPepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT. The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCos global business scale to enable business insights, advanced analytics, and new product development. PepsiCos Data Management and Operations team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations, and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.\nIncrease awareness about available data and democratize access to it across the company.\nAs a data engineering lead, you will be the key technical expert overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be empowered to create & lead a strong team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premises data sources as well as cloud and remote systems.\nResponsibilities\n\nData engineering lead role for D&Ai data modernization (MDIP)\n\nIdeally Candidate must be flexible to work an alternative schedule either on tradition work week from Monday to Friday; or Tuesday to Saturday or Sunday to Thursday depending upon coverage requirements of the job. The candidate can work with immediate supervisor to change the work schedule on rotational basis depending on the product and project requirements.\nResponsibilities\nManage a team of data engineers and data analysts by delegating project responsibilities and managing their flow of work as well as empowering them to realize their full potential.\nDesign, structure and store data into unified data models and link them together to make the data reusable for downstream products.\nManage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.\nCreate reusable accelerators and solutions to migrate data from legacy data warehouse platforms such as Teradata to Azure Databricks and Azure SQL.\nEnable and accelerate standards-based development prioritizing reuse of code, adopt test-driven development, unit testing and test automation with end-to-end observability of data\nBuild and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality, performance and cost.\nCollaborate with internal clients (product teams, sector leads, data science teams) and external partners (SI partners/data providers) to drive solutioning and clarify solution requirements.\nEvolve the architectural capabilities and maturity of the data platform by engaging with enterprise architects to build and support the right domain architecture for each application following well-architected design standards.\nDefine and manage SLAs for data products and processes running in production.\nCreate documentation for learnings and knowledge transfer to internal associates.\nQualifications\n\n12+ years of engineering and data management experience\n\nQualifications\n12+ years of overall technology experience that includes at least 5+ years of hands-on software development, data engineering, and systems architecture.\n8+ years of experience with Data Lakehouse, Data Warehousing, and Data Analytics tools.\n6+ years of experience in SQL optimization and performance tuning on MS SQL Server, Azure SQL or any other popular RDBMS\n6+ years of experience in Python/Pyspark/Scala programming on big data platforms like Databricks\n4+ years in cloud data engineering experience in Azure or AWS.\nFluent with Azure cloud services. Azure Data Engineering certification is a plus.\nExperience with integration of multi cloud services with on-premises technologies.\nExperience with data modelling, data warehousing, and building high-volume ETL/ELT pipelines.\nExperience with data profiling and data quality tools like Great Expectations.\nExperience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.\nExperience with at least one business intelligence tool such as Power BI or Tableau\nExperience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.\nExperience with version control systems like ADO, Github and CI/CD tools for DevOps automation and deployments.\nExperience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.\nExperience with Statistical/ML techniques is a plus.\nExperience with building solutions in the retail or in the supply chain space is a plus.\nUnderstanding of metadata management, data lineage, and data glossaries is a plus.\nBA/BS in Computer Science, Math, Physics, or other technical fields.\nCandidate must be flexible to work an alternative work schedule either on tradition work week from Monday to Friday; or Tuesday to Saturday or Sunday to Thursday depending upon product and project coverage requirements of the job.\nCandidates are expected to be in the office at the assigned location at least 3 days a week and the days at work needs to be coordinated with immediate supervisor\nSkills, Abilities, Knowledge:\nExcellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.\nProven track record of leading, mentoring data teams.\nStrong change manager. Comfortable with change, especially that which arises through company growth.\nAbility to understand and translate business requirements into data and technical requirements.\nHigh degree of organization and ability to manage multiple, competing projects and priorities simultaneously.\nPositive and flexible attitude to enable adjusting to different needs in an ever-changing environment.\nStrong leadership, organizational and interpersonal skills; comfortable managing trade-offs.\nFoster a team culture of accountability, communication, and self-management.\nProactively drives impact and engagement while bringing others along.\nConsistently attain/exceed individual and team goals.\nAbility to lead others without direct authority in a matrixed environment.\nComfortable working in a hybrid environment with teams consisting of contractors as well as FTEs spread across multiple PepsiCo locations.\nDomain Knowledge in CPG industry with Supply chain/GTM background is preferred.",Industry Type: Beverage,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Pyspark', 'Azure', 'Power BI', 'Github', 'Azure Databricks', 'Tableau', 'ADO', 'Scala programming', 'SQL', 'Azure Data Factory', 'Azure Machine learning', 'Data Lakehouse', 'Azure Data Engineering', 'CI/CD', 'Data Warehousing', 'Data Analytics', 'AWS', 'Python']",2025-06-12 13:49:45
Associate Specialist Data Science,Merck Sharp & Dohme (MSD),2 - 7 years,Not Disclosed,['Pune'],"Primary Responsibilities\nSupport in establishing frameworks to standardize, productize and scale existing and new capabilities / analytical solutions\nImplement the vision, roadmap, and best practices for the Data Science Center of Excellence ( CoE ) to align with business goals\nSupport establishing governance frameworks to measure the value of products, standardize data science methodologies, coding practices, and project workflows\nWork with senior CoE members in development and maintenance of best practices for model and algorithm development and design, deployment, and monitoring across the enterprise functions\nCollaborate with product team on product development incorporating Agile framework and latest industry best practices and norms\nSupport in development of MLOps and ModelOps frameworks to streamline the development-to-deployment product pipeline\nDrive innovation by identifying, evaluating, and implementing cutting-edge data science methodologies based on latest published literature\n\nQualifications\nEducation & Work Experience Requiremen ts:\nMaster s degree (relevant field like Economics, Statistics, Mathematics, Operational Research) with 2+ years work experience.\nBachelor s degree (in Engineering or related field, such as Computer Science, Data Science, Statistics, Business, etc.) with at least 3 + years relevant experience\nPrior experience in research publications in reputed journal is a plus\nSkillset:\nCandidates must have -\nStrong programming skills in languages such as Python or R, and SQL with experience in data manipulation and analysis libraries (e.g., pandas, NumPy, scikit-learn, stats models)\nExperience with data science principles, machine learning (supervised and unsupervised) and GenAI algorithms, test-control analysis, propensity score matching etc.\nExposure to product roadmaps, Agile methodologies and backlog management, ensuring iterative and incremental product improvements\nStrong problem solving, business analysis and quantitative skills\nAbility to effectively communicate proposals to key stakeholders\nCandidates are desired but not mandatory to have -\nExperience and familiarity with underlying concepts such as Patient analytics, MMx etc.\nUnderstanding of Pharma commercial landscape will be a plus\nExperience working with healthcare, financial, or enterprise SaaS products\n  Search Firm Representatives Please Read Carefully\nEmployee Status:\nRegular\nRelocation:\nVISA Sponsorship:\nTravel Requirements:\nFlexible Work Arrangements:\nNot Applicable\nShift:\nValid Driving License:\nHazardous Material(s):\n\nRequired Skills:\nBusiness Intelligence (BI), Database Design, Data Engineering, Data Modeling, Data Science, Data Visualization, Machine Learning, Software Development, Stakeholder Relationship Management, Waterfall Model",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Relationship management', 'Business analysis', 'Coding', 'Pharma', 'Analytical', 'Healthcare', 'Business intelligence', 'Analytics', 'Monitoring', 'SQL']",2025-06-12 13:49:47
Data Scientist,"Sourced Group, an Amdocs Company",4 - 9 years,Not Disclosed,['Gurugram'],"0px> Who are we?\nIn one sentence\nThis is a hands-on position for a motivated and talented innovator. The Data Scientist performs data mining and develops algorithms that provide insight from data.\nWhat will your job look like?\nYou will be responsible for and perform end-top-end data-based research.\nYou will craft data mining solutions to be implemented and executed with alignment to the planned scope and design coverage and needs/uses, demonstrating knowledge and a broad understanding of E2E business processes and requirements.\nYou will define the data analytics research plan, scope and resources required to meet the objectives of his/her area of ownership.\nYou will identify and analyze new data analytic directions and their potential business impact to determine the accurate prioritization of data analytics activities based on business needs and analytics value.\nYou will identify data sources, supervises the data collection process and crafts the data structure in collaboration with data experts (BI or big-data) and subject matter and business experts. Ensures that data used in the data analysis activities are of the highest quality.\nYou will construct data models (algorithms and formulas) for required business needs and predictions.\nYou will present results, including the preparation of patents and white papers and facilitating presentations during conferences.\nAll you need is...\nPh.D. in Computer Science, Mathematics or Statistics\n4 years experience in tasks related to data analytics\nKnowledge of telecommunications and of the subject area being investigated - advantage\nKnowledge in the product (ACC or other) application knowledge and configuration knowledge\nKnowledge in BSS, billing, Telco and the business processes\nFamiliarity in the Telco Networking - mobile, landline, cable TV, Internet\nknowledge in Oracle SQL\nWhy you will love this job:\nYou will ensure timely resolution or critical issue within the agreed SLA. This includes creating a positive customer support experience and build strong relationships through problem understanding, presenting promptly on progress, and handling customers with a professional demeanour.\nYou will be able to demonstrates an understanding of key business drivers and ensures strategic directions are followed and the organization succeeds\nWe are a dynamic, multi-cultural organization that constantly innovates and empowers our employees to grow. Our people our passionate, daring, and phenomenal teammates that stand by each other with a dedication to creating a diverse, inclusive workplace!\nWe offer a wide range of stellar benefits including health, dental, vision, and life insurance as well as paid time off, sick time, and parental leave!\n",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Bss', 'Networking', 'Billing', 'Data collection', 'Customer handling', 'Customer support', 'Data mining', 'Amdocs']",2025-06-12 13:49:49
"Sr. Data Analyst – Tableau, SQL, Snowflake",Int9 Solutions,5 - 7 years,Not Disclosed,['Bengaluru'],"We are looking for a skilled Data Analyst with excellent communication skills and deep expertise in SQL, Tableau, and modern data warehousing technologies. This role involves designing data models, building insightful dashboards, ensuring data quality, and extracting meaningful insights from large datasets to support strategic business decisions.\n\nKey Responsibilities:\nWrite advanced SQL queries to retrieve and manipulate data from cloud data warehouses such as Snowflake, Redshift, or BigQuery.\nDesign and develop data models that support analytics and reporting needs.\nBuild dynamic, interactive dashboards and reports using tools like Tableau, Looker, or Domo.\nPerform advanced analytics techniques including cohort analysis, time series analysis, scenario analysis, and predictive analytics.\nValidate data accuracy and perform thorough data QA to ensure high-quality output.\nInvestigate and troubleshoot data issues; perform root cause analysis in collaboration with BI or data engineering teams.\nCommunicate analytical insights clearly and effectively to stakeholders.\n\nRequired Skills & Qualifications:\nExcellent communication skills are mandatory for this role.\n5+ years of experience in data analytics, BI analytics, or BI engineering roles.\nExpert-level skills in SQL, with experience writing complex queries and building views.\nProven experience using data visualization tools like Tableau, Looker, or Domo.\nStrong understanding of data modeling principles and best practices.\nHands-on experience working with cloud data warehouses such as Snowflake, Redshift, BigQuery, SQL Server, or Oracle.\nIntermediate-level proficiency with spreadsheet tools like Excel, Google Sheets, or Power BI, including functions, pivots, and lookups.\nBachelor's or advanced degree in a relevant field such as Data Science, Computer Science, Statistics, Mathematics, or Information Systems.\nAbility to collaborate with cross-functional teams, including BI engineers, to optimize reporting solutions.\nExperience in handling large-scale enterprise data environments.\nFamiliarity with data governance, data cataloging, and metadata management tools (a plus but not required).",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Tableau', 'Data Warehousing', 'Data Analytics', 'SQL', 'Scenario Analysis', 'Cohort Analysis', 'Data Modeling', 'Predictive Analysis', 'Redshift']",2025-06-12 13:49:51
"Senior Manager- Middle and Back Office Data Analyst- ISS,",Fidelity International,10 - 15 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Title: Middle and Back Office Data Analyst - ISS Data (Senior Manager)\nDepartment: Technology\nLocation: Bangalore & Gurgaon (hybrid / flexible working permitted)\nReports To: Middle and Back Office Data Product Owner\nLevel: Senior Manager\nWe re proud to have been helping our clients build better financial futures for over 50 years. How have we achieved this? By working together - and supporting each other - all over the world. So, join our [insert name of team/ business area] team and feel like you re part of something bigger.\nAbout your team\nThe Technology function provides IT services that are integral to running an efficient run-the business operating model and providing change-driven solutions to meet outcomes that deliver on our business strategy. These include the development and support of business applications that underpin our revenue, operational, compliance, finance, legal, marketing and customer service functions. The broader organisation incorporates Infrastructure services that the firm relies on to operate on a day-to-day basis including data centre, networks, proximity services, security, voice, incident management and remediation.\nThe ISS Technology group is responsible for providing Technology solutions to the Investment Solutions & Services (ISS) business (which covers Investment Management, Asset Management Operations & Distribution business units globally)\n\nThe ISS Technology team supports and enhances existing applications as well as designs, builds and procures new solutions to meet requirements and enable the evolving business strategy.\nAs part of this group, a dedicated ISS Data Programme team has been mobilised as a key foundational programme to support the execution of the overarching ISS strategy.\nAbout your role\nThe Middle and Back Office Data Analyst role is instrumental in the creation and execution of a future state design for Fund Servicing & Oversight data across Fidelity s key business areas. The successful candidate will have an in- depth knowledge of data domains that represent Middle and Back-office operations and technology.\nThe role will sit within the ISS Delivery Data Analysis chapter and fully aligned to deliver Fidelity s cross functional ISS Data Programme in Technology, and the candidate will leverage their extensive industry knowledge to build a future state platform in collaboration with Business Architecture, Data Architecture, and business stakeholders.\nThe role is to maintain strong relationships with the various business contacts to ensure a superior service to our clients.\nData Product - Requirements Definition and Delivery of Data Outcomes\nAnalysis of data product requirements to enable business outcomes, contributing to the data product roadmap\nCapture both functional and non-functional data requirements considering the data product and consumers perspectives.\nConduct workshops with both the business and tech stakeholders for requirements gathering, elicitation and walk throughs.\nResponsible for the definition of data requirements, epics and stories within the product backlog and providing analysis support throughout the SDLC.\nResponsible for supporting the UAT cycles, attaining business sign off on outcomes being delivered\nData Quality and Integrity:\nDefine data quality use cases for all the required data sets and contribute to the technical frameworks of data quality.\nAlign the functional solution with the best practice data architecture & engineering principles.\nCoordination and Communication:\nExcellent communication skills to influence technology and business stakeholders globally, attaining alignment and sign off on the requirements.\nCoordinate with internal and external stakeholders to communicate data product deliveries and the change impact to the operating model.\nAn advocate for the ISS Data Programme.\nCollaborate closely with Data Governance, Business Architecture, and Data owners etc.\nConduct workshops within the scrum teams and across business teams, effectively document the minutes and drive the actions.\nAbout you\nAt least 10 years of proven experience as a business/technical/data analyst within technology and/or business changes within the financial services /asset management industry.\nMinimum 5 years as a senior business/technical/data analyst adhering to agile methodology, delivering data solutions using industry leading data platforms such as Snowflake, State Street Alpha Data, Refinitiv Eikon, SimCorp Dimension, BlackRock Aladdin, FactSet etc.\nProven experience. of delivering data driven business outcomes using industry leading data platforms such as Snowflake.\nExcellent knowledge of data life cycle that drives Middle and Back Office capabilities such as trade execution, matching, confirmation, trade settlement, record keeping, accounting, fund & cash positions, custody, collaterals/margin movements, corporate actions , derivations and calculations such as holiday handling, portfolio turnover rates, funds of funds look through .\nIn Depth expertise in data and calculations across the investment industry covering the below.\nAsset-specific data: This includes data related to financial instruments reference data like asset specifications, maintenance records, usage history, and depreciation schedules.\nMarket data: This includes data like security prices, exchange rates, index constituents and licensing restrictions on them.\nABOR & IBOR data: This includes calculation engines covering input data sets, calculations and treatment of various instruments for ABOR and IBOR data leveraging platforms such as Simcorp, Neoxam, Invest1, Charles River, Aladdin etc. Knowledge of TPAs, how data can be structured in a unified way from heterogenous structures.\nShould possess Problem Solving, Attention to detail, Critical thinking.\nTechnical Skills: Excellent hands-on SQL, Advanced Excel, Python, ML (optional) and proven experience and knowledge of data solutions.\nKnowledge of data management, data governance, and data engineering practices\nHands on experience on data modelling techniques such as dimensional, data vault etc.\nWillingness to own and drive things, collaboration across business and tech stakeholders.\nFeel rewarded",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['IT services', 'Data analysis', 'Data management', 'Incident management', 'Scrum', 'Customer service', 'Asset management', 'SDLC', 'SQL', 'Python']",2025-06-12 13:49:54
Senior Backend Engineer - Bangalore - Hybrid (Product Based),Trigent Software Solutions,5 - 10 years,25-27.5 Lacs P.A.,['Bengaluru'],"Job Description:\nWe are looking for an experienced Backend Engineer to join our engineering team and contribute to building highly scalable, low-latency, and high-concurrency SaaS applications. The ideal candidate should have deep expertise in Java, cloud technologies (preferably AWS), and experience working with both RDBMS and NoSQL databases. You will be involved in the full software development lifecycle, from design to deployment, ensuring performance, security, and maintainability.\nKey Responsibilities:\nDesign, develop, and maintain high-performance, scalable, and secure backend services\nBuild and maintain RESTful APIs to support client-side applications and services\nWork on transactional and concurrent systems that serve large-scale SaaS platforms\nCollaborate with frontend engineers, architects, and DevOps to build robust cloud-based systems\nEnsure code quality and performance by writing unit/integration tests and performing code reviews\nTroubleshoot, debug, and optimize existing systems for reliability and efficiency\nFollow Agile development practices and participate in daily stand-ups and sprint planning\nMandatory Skills:\n58 years of backend development experience in building SaaS or transactional web applications\nStrong hands-on experience with Java and web application frameworks like Spring, Spring Boot\nExperience working on high-concurrency, low-latency, and high-availability systems\nSolid experience with at least one RDBMS (e.g., PostgreSQL, MySQL) and one NoSQL DB (e.g., MongoDB, Cassandra)\nExpertise in cloud platforms – preferably AWS (e.g., EC2, S3, RDS, Lambda)\nFamiliarity with application servers like Tomcat\nStrong knowledge of system design, data structures, and multithreading\nExperience with RESTful APIs and microservice architectures\nNice to Have:\nKnowledge of Big Data technologies (e.g., Kafka, Hadoop, Spark)\nFamiliarity with containerization tools like Docker and Kubernetes\nExperience with CI/CD tools and cloud deployment pipelines\nExposure to other cloud platforms like GCP or Azure",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['java', 'RDBMS', 'Saas Product Development', 'Nosql Databases', 'AWS', 'spring', 'tomcat']",2025-06-12 13:49:56
Senior Murex Front Office & Risk Support Engineer,Synechron,5 - 10 years,Not Disclosed,"['Pune', 'Bengaluru', 'Hinjewadi']","Job Summary\nSynechron is seeking an experienced Murex FO & Risk Support Specialist to join our dynamic team. This role is central to maintaining and supporting Murex platform functionalities related to front-office operations and risk management, with a focus on production support.\nThe individual will collaborate closely with business users and IT teams to resolve complex issues, optimize configurations, and ensure the stability of critical trading and risk systems. By providing expert-level support, this role contributes directly to the organizations ability to manage market and credit risks effectively, deliver timely business insights, and uphold operational resilience.\nSoftware Requirements\nRequired Software Proficiency:\nMurex platform (version 3.1 or later) extensive experience in FO & Risk modules from a production support perspective\nSQL and Database querying tools (Oracle, SQL Server) strong experience in data analysis and troubleshooting\nMarket data management tools and configurations within Murex\nIssue tracking and collaboration tools (e.g., JIRA, ServiceNow)\nPreferred Software Skills:\nFamiliarity with scripting languages (Python, Shell scripting) for automation\nVersion control systems (e.g., Git)\nCloud platforms (if applicable to client environment)\nOverall Responsibilities\nProvide second-line support for Murex front-office and risk modules, ensuring operational stability and performance\nAnalyze and troubleshoot issues related to P&L, market risk, credit risk, pricing, simulations, and market data, collaborating with business users to identify root causes\nManage Murex configurations, including GOM (Global Object Model) setups, static data configurations, and cross-asset class risk settings\nLiaise with business stakeholders and IT teams to resolve complex incidents, queries, and configuration challenges\nCollaborate on incident resolution, change management, and service improvement initiatives\nDocument technical procedures, resolutions, and configuration changes for knowledge sharing and audit compliance\nContinuously monitor platform health, performance, and data integrity, proposing proactive solutions for operational risks\nTechnical Skills (By Category)\nProgramming Languages:\nEssential: Murex editing languages, SQL\nPreferred: Python, Shell scripting for automation tasks\nDatabases / Data Management:\nEssential: Oracle, SQL Server experience with schema design, data queries, and performance tuning\nPreferred: Understand market data integration and static data management in Murex\nCloud Technologies:\nOptional/Preferred: Basic understanding of cloud integrations related to trading platforms, if applicable\nFrameworks and Libraries:\nNot applicable, focus on Murex platform and related tools\nDevelopment Tools and Methodologies:\nFamiliarity with ITSM processes, incident & problem management, Agile/Scrum methodologies\nSecurity Protocols:\nUnderstanding of access controls, data confidentiality, and system compliance relevant to financial platforms\nExperience Requirements\nMinimum of 5+ years experience supporting Murex FO & Risk modules in a production environment\nProven experience in analyzing issues related to P&L, market risk, credit risk, and pricing in a trading systems context\nStrong understanding of GOM configurations, static data setups, and cross-asset risk configurations\nExperience working directly with business lines, traders, risk managers, and IT teams\nIndustry background within banking, financial services, or capital markets preferred but not mandatory\nDay-to-Day Activities\nMonitor and support Murex FO & Risk modules, identifying and resolving operational issues\nPerform root-cause analysis on incidents related to market data, P&L, and risk calculations\nFine-tune GOM and static data configurations to optimize system performance and accuracy\nEngage in daily stand-ups, incident reviews, and change implementation meetings\nCollaborate with business users and technical teams to clarify issues and implement fixes\nConduct system audits and maintain detailed logs of support activities and configuration changes\nContribute to continuous improvement initiatives for system stability and efficiency\nQualifications\nBachelors degree or higher in Computer Science, Finance, Information Technology, or related field\nCertifications in Murex support, risk management, or related disciplines are a plus\nPrior experience with Murex platform support in financial markets, trading, or risk management environments\nKnowledge of market practices, instruments, and risk concepts across multiple asset classes\nWillingness to engage in ongoing professional development and staying current with Murex updates and industry trends\nProfessional Competencies\nStrong analytical and problem-solving skills, with a focus on root cause identification\nEffective communication skills to interact clearly with technical and business stakeholders\nAbility to work collaboratively within cross-functional teams and under pressure\nAdaptability to evolving systems, processes, and technology landscapes\nCustomer-centric approach, ensuring timely and quality support delivery\nDemonstrated organizational skills for managing multiple issues and priorities efficiently.",Industry Type: IT Services & Consulting,Department: Risk Management & Compliance,"Employment Type: Full Time, Permanent","['Murex support', 'risk management', 'Risk Support', 'credit risk', 'market risk', 'pricing']",2025-06-12 13:49:58
Senior Flexera Data Analyst,Luxoft,6 - 11 years,Not Disclosed,['Gurugram'],"Internal Data Structures & Modeling\nDesign, maintain, and optimize internal data models and structures within the Flexera environment.\nMap business asset data to Flexeras normalized software models with precision and accuracy.\nEnsure accurate data classification, enrichment, and normalization to support software lifecycle tracking.\nPartner with infrastructure, operations, and IT teams to ingest and reconcile data from various internal systems.\nReporting & Analytics\nDesign and maintain reports and dashboards in Flexera or via external BI tools such as Power BI or Tableau.\nProvide analytical insights on software usage, compliance, licensing, optimization, and risk exposure.\nAutomate recurring reporting processes and ensure timely delivery to business stakeholders.\nWork closely with business users to gather requirements and translate them into meaningful reports and visualizations.\nAutomated Data Feeds & API Integrations\nDevelop and support automated data feeds using Flexera REST/SOAP APIs.\nIntegrate Flexera with enterprise tools (e.g., CMDB, SCCM, ServiceNow, ERP) to ensure reliable and consistent data flow.\nMonitor, troubleshoot, and resolve issues related to data extracts and API communication.\nImplement robust logging, alerting, and exception handling for integration pipelines.\nSkills\nMust have\nMinimum 6+ years of working with Flexera or similar software.\nFlexera Expertise: Strong hands-on experience with Flexera One, FlexNet Manager Suite, or similar tools.\nTechnical Skills:\nProficient in REST/SOAP API development and integration.\nStrong SQL skills and familiarity with data transformation/normalization concepts.\nExperience using reporting tools like Power BI, Tableau, or Excel for data visualization.\nFamiliarity with enterprise systems such as SCCM, ServiceNow, ERP, CMDBs, etc.\nProcess & Problem Solving:\nStrong analytical and troubleshooting skills for data inconsistencies and API failures.\nUnderstanding of license models, software contracts, and compliance requirements.\nNice to have\nSoft Skills: Excellent communication skills to translate technical data into business insights.\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nData Engineer with Neo4j\nData Science\nIndia\nChennai\nData Engineer with Neo4j\nData Science\nIndia\nBengaluru\nData Scientist\nData Science\nIndia\nBengaluru\nGurugram, India\nReq. VR-114544\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114544\nApply for Senior Flexera Data Analyst in Gurugram\n*",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ERP', 'neo4j', 'Analytical', 'Data structures', 'data visualization', 'SCCM', 'Licensing', 'Analytics', 'Reporting tools', 'SQL']",2025-06-12 13:50:01
"AI/ML TESTING-AI, ML, DEEP LEARNING, DATA MINING",Zensar,2 - 7 years,Not Disclosed,['Pune'],"Zensar Technologies is looking for AI/ML TESTING-AI, ML, DEEP LEARNING, DATA MINING, ANALYTICS AI/ML TESTING-AI, ML, DEEP LEARNING, DATA MINING, ANALYTICS to join our dynamic team and embark on a rewarding career journey\n\nDevelops and executes test plans for AI and machine learning models\n\nValidates model accuracy, fairness, performance, and edge-case behavior\n\nImplements automation tools and creates synthetic test datasets\n\nEnsures compliance with model validation protocols and documentation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Design engineering', 'deep learning', 'Technology consulting', 'Focus', 'Agile', 'Conceptualization', 'Management', 'Data mining', 'Analytics', 'Testing']",2025-06-12 13:50:03
Business Data Analyst,CGI,5 - 8 years,Not Disclosed,['Hyderabad'],"Business Data Analyst - HealthCare\n\nJob Summary\nWe are seeking an experienced and results-driven Business Data Analyst with 5+ years of hands-on experience in data analytics, visualization, and business insight generation. This role is ideal for someone who thrives at the intersection of business and datatranslating complex data sets into compelling insights, dashboards, and strategies that support decision-making across the organization.\nYou will collaborate closely with stakeholders across departments to identify business needs, design and build analytical solutions, and tell compelling data stories using advanced visualization tools.\nKey Responsibilities\nData Analytics & Insights Analyze large and complex data sets to identify trends, anomalies, and opportunities that help drive business strategy and operational efficiency.\n• Dashboard Development & Data Visualization Design, develop, and maintain interactive dashboards and visual reports using tools like Power BI, Tableau, or Looker to enable data-driven decisions.\n• Business Stakeholder Engagement Collaborate with cross-functional teams to understand business goals, define metrics, and convert ambiguous requirements into concrete analytical deliverables.\n• KPI Definition & Performance Monitoring Define, track, and report key performance indicators (KPIs), ensuring alignment with business objectives and consistent measurement across teams.\n• Data Modeling & Reporting Automation Work with data engineering and BI teams to create scalable, reusable data models and automate recurring reports and analysis processes.\n• Storytelling with Data Communicate findings through clear narratives supported by data visualizations and actionable recommendations to both technical and non-technical audiences.\n• Data Quality & Governance Ensure accuracy, consistency, and integrity of data through validation, testing, and documentation practices.\nRequired Qualifications\nBachelor’s or Master’s degree in Business, Economics, Statistics, Computer Science, Information Systems, or a related field.\n• 5+ years of professional experience in a data analyst or business analyst role with a focus on data visualization and analytics.\n• Proficiency in data visualization tools: Power BI, Tableau, Looker (at least one).\n• Strong experience in SQL and working with relational databases to extract, manipulate, and analyze data.\n• Deep understanding of business processes, KPIs, and analytical methods.\n• Excellent problem-solving skills with attention to detail and accuracy.\n• Strong communication and stakeholder management skills with the ability to explain technical concepts in a clear and business-friendly manner.\n• Experience working in Agile or fast-paced environments.\nPreferred Qualifications\nExperience working with cloud data platforms (e.g., Snowflake, BigQuery, Redshift).\n• Exposure to Python or R for data manipulation and statistical analysis.\n• Knowledge of data warehousing, dimensional modeling, or ELT/ETL processes.\n• Domain experience in Healthcare is a plus.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Bigquery', 'Snowflake', 'Data Warehousing', 'Redshift', 'Python', 'ETL']",2025-06-12 13:50:05
Senior Data Manager/ Lead,Codeforce 360,6 - 8 years,Not Disclosed,['Hyderabad'],"Job Description:\nWe are looking for a highly experienced and dynamic Senior Data Manager / Lead to oversee a team of Data Engineers and Data Scientists. This role demands a strong background in data platforms such as Snowflake and proficiency in Python, combined with excellent people management and project leadership skills. While hands-on experience in the technologies is beneficial, the primary focus of this role is on team leadership, strategic planning, and project delivery .\n\nJob Title : Senior Data Manager / Lead\nLocation: Hyderabad (Work From Office)\nShift Timing: 10AM-7PM\nKey Responsibilities:\nLead, mentor, and manage a team of Data Engineers and Data Scientists.\nOversee the design and implementation of data pipelines and analytics solutions using Snowflake and Python.\nCollaborate with cross-functional teams (product, business, engineering) to align data solutions with business goals.\nEnsure timely delivery of projects, with high quality and performance.\nConduct performance reviews, training plans, and support career development for the team.\nSet priorities, allocate resources, and manage workloads within the data team.\nDrive adoption of best practices in data management, governance, and documentation.\nEvaluate new tools and technologies relevant to data engineering and data science.\n\nRequired Skills & Qualifications:\n6+ years of experience in data-related roles, with at least 23 years in a leadership or management position.\nStrong understanding of Snowflake architecture, performance tuning, data sharing, security, etc.\nSolid knowledge of Python for data engineering or data science tasks.\nExperience in leading data migration, ETL/ELT, and analytics projects.\nAbility to translate business requirements into technical solutions.\nExcellent leadership, communication, and stakeholder management skills.\nExposure to tools like Databricks, Dataiku, Airflow, or similar platforms is a plus.\nBachelors or Master’s degree in Computer Science, Engineering, Mathematics, or a related field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Data Bricks', 'Python', 'Airflow', 'Data Migration', 'Dataiku', 'Data Warehousing', 'ETL', 'ELT', 'SQL']",2025-06-12 13:50:07
Data Scientist,Big Oh Tech,4 - 6 years,Not Disclosed,['Noida'],"Key Responsibilities:\n\nDesign, build, and maintain robust and scalable data pipelines to support analytics and reporting needs.\nManage and optimize data lake architectures, with a focus on Apache Atlas for metadata management, data lineage, and governance.\nIntegrate and curate data from multiple structured and unstructured sources to enable advanced analytics.\nCollaborate with data scientists and business analysts to ensure availability of clean, well-structured data.\nImplement data quality, validation, and monitoring processes across data pipelines.\nDevelop and manage Power BI datasets and data models, supporting dashboard and report creation.\nSupport data cataloging and classification using Apache Atlas for enterprise-wide discoverability and compliance.\nEnsure adherence to data security, privacy, and compliance policies.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['advanced analytics', 'metadata', 'Compliance', 'Business Analyst', 'data security', 'power bi', 'Data quality', 'Management', 'Apache', 'Monitoring']",2025-06-12 13:50:09
Senior/Lead MLops Engineer,Tiger Analytics,7 - 10 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","JOB DESCRIPTION\n\nSenior MLE / Architect MLE (ML Ops) Chennai / Bangalore / Hyderabad (Hybrid)\n\nWho we are Tiger Analytics is a global leader in AI and analytics, helping Fortune 1000 companies solve their toughest challenges. We offer fullstack AI and analytics services & solutions to empower businesses to achieve real outcomes and value at scale. We are on a mission to push the boundaries of what AI and analytics can do to help enterprises navigate uncertainty and move forward decisively. Our purpose is to provide certainty to shape a better tomorrow. Our team of 4000+ technologists and consultants are based in the US, Canada, the UK, India, Singapore and Australia, working closely with clients across CPG, Retail, Insurance, BFS, Manufacturing, Life Sciences, and Healthcare. Many of our team leaders rank in Top 10 and 40 Under 40 lists, exemplifying our dedication to innovation and excellence. We are a Great Place to Work-Certified (2022-24), recognized by analyst firms such as Forrester, Gartner, HFS, Everest, ISG and others. We have been ranked among the Best and Fastest Growing analytics firms lists by Inc., Financial Times, Economic Times and Analytics India Magazine.",,,,"['MLops', 'Azure', 'Snowflake', 'Deployment', 'Ci/Cd', 'Machine Learning']",2025-06-12 13:50:11
Big Data Developer,Binary Infoways,6 - 10 years,12-20 Lacs P.A.,['Hyderabad'],"AWS (EMR, S3, Glue, Airflow, RDS, Dynamodb, similar)\nCICD (Jenkins or another)\nRelational Databases experience (any)\nNo SQL databases experience (any)\nMicroservices or Domain services or API gateways or similar\nContainers (Docker, K8s, similar)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'AWS', 'Python', 'Airflow', 'Java', 'Big Data', 'EMR', 'SQL', 'Jenkins', 'Glue', 'SCALA', 'Big Data Technologies', 'Spark']",2025-06-12 13:50:14
"Senior Staff Engineer, QA Automation",Nagarro,10 - 13 years,Not Disclosed,['India'],"We're Nagarro.\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal Experience 10+ years.\nStrong working experience in QA with a strong background in both manual and automation testing.\nHands-on experience with Selenium WebDriver, Appium, and Postman.\nSolid understanding of REST API testing and automation using tools like RestAssured.\nProficient in testing frameworks such as TestNG, JUnit, or Cypress.\nStrong experience in automation of mobile and web applications.\nFamiliarity with CI/CD tools like Jenkins, GitLab CI, or equivalent.\nWorking knowledge of bug tracking and test management tools (e.g., JIRA, TestRail).\nExperience with BDD frameworks like Cucumber.\nGood command of scripting or programming in Java, Python, or similar languages is a plus.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\nRESPONSIBILITIES:\nUnderstanding the clients business use cases and technical requirements and be able to convert them in to technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['API Testing', 'Appium', 'QA Automation', 'Selenium', 'Postman']",2025-06-12 13:50:16
"Senior Staff Engineer, Frontend React",Nagarro,10 - 13 years,Not Disclosed,['India'],"We're Nagarro.\n\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal Experience 10+ years.\nHands on working experience in front-end or full-stack development experience, with building production apps in React.js and Next.js.\nHands-on expertise writing unit/integration tests for React components (Jest, React Testing Library, etc.)\nSolid grasp of state-management patterns and libraries (Redux, React Context, Zustand, etc.).\nStrong understanding of RESTful APIs, asynchronous programming (Promises, async/await), and modern build tools (Webpack, Vite, or Turbopack).\nPractical experience with Git, pull-request workflows, and collaborative development tools (GitHub, GitLab, Bitbucket).\nAdvanced proficiency in JavaScript (ES6+) and TypeScript.\nProblem-solving mindset with the ability to tackle complex data engineering challenges. \nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\n\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them in to technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Javascript', 'React.Js', 'Nextjs', 'Typescript']",2025-06-12 13:50:18
Associate- Referral - Decision Science / Data Science,Axtria,3 - 5 years,Not Disclosed,['Gurugram'],"Position Summary \n\nThis Requisition is for the Employee Referral Campaign.\n\nWe are seeking high-energy, driven, and innovative Data Scientists to join our Data Science Practice to develop new, specialized capabilities for Axtria, and to accelerate the company’s growth by supporting our clients’ commercial & clinical strategies.\n\n Job Responsibilities \n\nBe an Individual Contributor tothe Data Science team and solve real-world problems using cutting-edge capabilities and emerging technologies.\n\nHelp clients translate the business use cases they are trying to crack into data science solutions. Provide genuine assistance to users by advising them on how to leverage Dataiku DSS to implement data science projects, from design to production.\n\nData Source Configuration, Maintenance, Document and maintain work-instructions.\n\nDeep working onmachine learning frameworks such as TensorFlow, Caffe, Keras, SparkML\n\nExpert knowledge in Statistical and Probabilistic methods such as SVM, Decision-Trees, Clustering\n\nExpert knowledge of python data-science and math packages such as NumPy , Pandas, Sklearn\n\nProficiency in object-oriented languages (Java and/or Kotlin),Python and common machine learning frameworks(TensorFlow, NLTK, Stanford NLP, Ling Pipe etc\n\n\n Education \n\nBachelor Equivalent - Engineering\nMaster's Equivalent - Engineering\n\n Work Experience \n\nData Scientist 3-5 years of relevant experience in advanced statistical and mathematical models and predictive modeling using Python. Experience in the data science space prior relevant experience in Artificial intelligence and machine Learning algorithms for developing scalable models supervised and unsupervised techniques likeNLP and deep Learning Algorithms. Ability to build scalable models using Python, R-Studio, R Shiny, PySpark, Keras, and TensorFlow. Experience in delivering data science projects leveraging cloud infrastructure. Familiarity with cloud technology such as AWS / Azure and knowledge of AWS tools such as S3, EMR, EC2, Redshift, and Glue; viz tools like Tableau and Power BI. Relevant experience in Feature Engineering, Feature Selection, and Model Validation on Big Data. Knowledge of self-service analytics platforms such as Dataiku/ KNIME/ Alteryx will be an added advantage.\n\nML Ops Engineering 3-5 years of experience with MLOps Frameworks like Kubeflow, MLFlow, Data Robot, Airflow, etc., experience with Docker and Kubernetes, OpenShift. Prior experience in end-to-end automated ecosystems including, but not limited to, building data pipelines, developing & deploying scalable models, orchestration, scheduling, automation, and ML operations. Ability to design and implement cloud solutions and ability to build MLOps pipelines on cloud solutions (AWS, MS Azure, or GCP). Programming languages like Python, Go, Ruby, or Bash, a good understanding of Linux, knowledge of frameworks such as Keras, PyTorch, TensorFlow, etc. Ability to understand tools used by data scientists and experience with software development and test automation. Good understanding of advanced AI/ML algorithms & their applications.\n\nGen AI :Minimum of 4-6 years develop, test, and deploy Python based applications on Azure/AWS platforms.Must have basic knowledge on concepts of Generative AI / LLMs / GPT.Deep understanding of architecture and work experience on Web Technologies.Python, SQL hands-on experience.Expertise in any popular python web frameworks e.g. flask, Django etc. Familiarity with frontend technologies like HTML, JavaScript, REACT.Be an Individual Contributor in the Analytics and Development team and solve real-world problems using cutting-edge capabilities and emerging technologies based on LLM/GenAI/GPT.Can interact with client on GenAI related capabilities and use cases.",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'gpm', 'machine learning', 'python data', 'statistics', 'kubernetes', 'microsoft azure', 'numpy', 'javascript', 'sql', 'docker', 'pandas', 'tensorflow', 'java', 'django', 'predictive modeling', 'python web framework', 'mathematical modeling', 'pytorch', 'keras', 'aws', 'flask', 'advanced statistical']",2025-06-12 13:50:20
Sr. Data Scientist-Stratup-Mid-Size companies Exp.@ Bangalore_Urgent,"A leader in this space, we deliver world...",8 - 13 years,Not Disclosed,['Bengaluru'],"Senior Data Scientist\n\nLocation: Onsite Bangalore\nExperience: 8+ years\n\nRole Overview\n\nWe are seeking a Senior Data Scientist with a strong foundation in machine learning, deep learning, and statistical modeling, with the ability to translate complex operational problems into scalable AI/ML solutions. In addition to core data science responsibilities, the role involves building production-ready backends in Python and contributing to end-to-end model lifecycle management. Exposure to computer vision is a plus, especially for industrial use cases like identification, intrusion detection, and anomaly detection.\n\nKey Responsibilities\n\nDevelop, validate, and deploy machine learning and deep learning models for forecasting, classification, anomaly detection, and operational optimization\nBuild backend APIs using Python (FastAPI, Flask) to serve ML/DL models in production environments\nApply advanced computer vision models (e.g., YOLO, Faster R-CNN) to object detection, intrusion detection, and visual monitoring tasks\nTranslate business problems into analytical frameworks and data science solutions\nWork with data engineering and DevOps teams to operationalize and monitor models at scale\nCollaborate with product, domain experts, and engineering teams to iterate on solution design\nContribute to technical documentation, model explainability, and reproducibility practices\n\n\nRequired Skills\n\nStrong proficiency in Python for data science and backend development\nExperience with ML/DL libraries such as scikit-learn, TensorFlow, or PyTorch\nSolid knowledge of time-series modeling, forecasting techniques, and anomaly detection\nExperience building and deploying APIs for model serving (FastAPI, Flask)\nFamiliarity with real-time data pipelines using Kafka, Spark, or similar tools\nStrong understanding of model validation, feature engineering, and performance tuning\nAbility to work with SQL and NoSQL databases, and large-scale datasets\nGood communication skills and stakeholder engagement experience\n\n\nGood to Have\n\nExperience with ML model deployment tools (MLflow, Docker, Airflow)\nUnderstanding of MLOps and continuous model delivery practices\nBackground in aviation, logistics, manufacturing, or other industrial domains\nFamiliarity with edge deployment and optimization of vision models\n\n\nQualifications\n\nMasters or PhD in Data Science, Computer Science, Applied Mathematics, or related field\n7+ years of experience in machine learning and data science, including end-to-end deployment of models in production",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scikit-learn', 'time-series modeling', 'ML/DL libraries', 'data science', 'Python', 'Airflow', 'Kafka', 'MLflow', 'logistics', 'anomaly detection', 'aviation', 'SQL', 'PyTorch', 'NoSQL', 'MLOps', 'forecasting techniques', 'Docker', 'manufacturing', 'FastAPI', 'Spark', 'TensorFlow', 'Flask']",2025-06-12 13:50:23
Data Science_ Lead,Rishabh Software,8 - 13 years,Not Disclosed,"['Ahmedabad', 'Bengaluru', 'Vadodara']","Job Description\n\nWith excellent analytical and problem-solving skills, you should understand business problems of the customers, translate them into scope of work and technical specifications for developing into Data Science projects. Efficiently utilize cutting edge technologies in AI, Generative AI areas and implement solutions for business problems. Good exposure technology platforms for Data Science, AI, Gen AI, cloud with implementation experience. Ability to provide end to end technical solutions leveraging latest AI, Gen AI tools, frameworks for the business problems. This Job requires the following:",,,,"['Data Science', 'gen ai', 'Computer Vision', 'Machine Learning', 'Deep Learning', 'Tensorflow', 'NLP', 'Artificial Intelligence', 'Dl', 'Python']",2025-06-12 13:50:25
Python Engineer - ML/Big Query - Hyd/Chennai/Bangalore,People staffing Solutions,5 - 10 years,12-20 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Key Responsibilities:\nDesign, develop, and maintain scalable and optimized ETL pipelines using Python and SQL.\nWork with Google BigQuery and other cloud-based platforms to build data warehousing solutions.\nDevelop and deploy ML models; collaborate with Data Scientists for productionizing models.\nWrite efficient and optimized SQL queries for large-scale data processing.\nBuild APIs using Flask/Django for machine learning and data applications.\nWork with both SQL and NoSQL databases including Elasticsearch.\nImplement data ingestion using batch and streaming technologies.\nEnsure data quality, integrity, and governance across the data lifecycle.\nAutomate and optimize CI/CD pipelines for data solutions.\nCollaborate with cross-functional teams to gather data requirements and deliver solutions.\nTroubleshoot and monitor data pipelines for seamless operations.\nRequired Skills & Qualifications:\nBachelor's or Master's degree in Computer Science, Engineering, or related field.\n5+ years of experience with Python in a data engineering and/or ML context.\nStrong hands-on experience with SQL, BigQuery, and cloud data platforms (preferably GCP).\nPractical knowledge of ML concepts and experience developing ML models.\nProficiency in frameworks such as Flask and Django.\nExperience with NoSQL databases and data streaming technologies.\nSolid understanding of data modeling, warehousing, and ETL frameworks.\nFamiliarity with CI/CD tools and automation best practices.\nExcellent communication, problem-solving, and collaboration skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Django', 'Machine Learning', 'Python', 'SQL', 'Pandas', 'Numpy', 'Ml', 'Flask']",2025-06-12 13:50:27
"Senior Staff Engineer, Mobile -Flutter",Nagarro,10 - 13 years,Not Disclosed,['India'],"We're Nagarro.\n\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal Experience 10+ years.\nStrong working experience in Dart and Flutter.\nSolid understanding of Mobile App Architecture (MVVM, BLoC, Provider, GetX).\nExperience with local databases like Sqflite or alternatives.\nHands-on experience in unit testing and test automation for Flutter apps.\nProven experience in building and deploying apps to the App Store and Google Play Store.\nFamiliarity with Git, GitHub/GitLab, CI/CD tools (e.g., Jenkins, Bitrise, GitHub Actions).\nDeep knowledge of mobile app lifecycle, design principles, and clean architecture patterns (MVVM, MVC, etc.)\nExpertise in mobile app performance optimization and security best practices.\nExperience in API integration, RESTful services, and handling JSON data.\nProficient in Version Control Systems like Git.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them in to technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GIT', 'Flutter', 'Dart', 'Swift', 'IOS', 'Android']",2025-06-12 13:50:29
MDM Data Analyst / Steward Lead,Gallagher Service Center (GSC),3 - 7 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\n\nThe MDM Analyst / Data Steward works closely with business stakeholders to understand and gather data requirements, develop data models and database designs, and define and implement data standards, policies, and procedures. This role also implements any rules inside of the MDM tool to improve the data, performs deduplication projects to develop golden records, and overall works towards improving the quality of data in the domain assigned.\n\nRequired skills :\nTechnical Skills: Proficiency in MDM tools and technologies such as Informatica MDM, CluedIn, or similar platforms is essential. Familiarity with data modeling, data integration, and data quality control techniques is also important. Experience with data governance platforms like Collibra and Alation can be beneficial1.\nAnalytical Skills: Strong analytical and problem-solving skills are crucial for interpreting and working with large volumes of data. The ability to translate complex business requirements into practical MDM solutions is also necessary.\nData Management: Experience in designing, implementing, and maintaining master data management systems and solutions. This includes conducting data cleansing, data auditing, and data validation activities.\nCommunication and Collaboration: Excellent communication and interpersonal skills to effectively collaborate with business stakeholders, IT teams, and other departments.\nData Governance: In-depth knowledge of data governance, data quality, and data integration principles. The ability to develop and implement data management processes and policies is essential.\nEducational Background: A Bachelor's or Master's degree in Computer Science, Information Systems, Data Science, or a related field is typically required1.\nCertifications: Certification in the MDM domain (e.g., Certified MDM Professional) can be a plus\n\nKey Skills:\nBecome the expert at the assigned domain of data\nUnderstand all source systems feeding into the MDM\nWrite documentation of stewardship for the domain\nDevelop rules and standards for the domain of data\nGenerate measures of improvement to demonstrate to the business the quality of the data\n\nWe are seeking candidates who can join immediately or within a maximum of 30 days' notice.\nMinimum of 3+ years of relevant experience is required.\nCandidates who are willing to relocate to Bangalore or are already based in Bangalore.\nCandidates should be flexible with working UK/US shifts.",Industry Type: Analytics / KPO / Research,Department: Other,"Employment Type: Full Time, Permanent","['Informatica Mdm', 'Data Modeling', 'Data Integration']",2025-06-12 13:50:31
Data Entry Job in Big Pharma Company at Borivali East in Mumbai,Big and Reputed Pharma Company of India ...,1 - 5 years,1.5-2.5 Lacs P.A.,['Mumbai (All Areas)'],"Only 1 Year+ experienced candidate in any field, who know basic Word, Excel and computer.\n\nThis is office job and you need to work on word, excel and email for the company\n\n2 Saturday and all Sunday are Holiday\n\nFor query call at 8000044060\n\nRequired Candidate profile\nOnly 1 Year+ experienced candidate in any field, who know basic Word, Excel and computer.\n\nThis is office job and you need to work on word, excel and email.\n\n2 Saturday and all Sunday are Holiday",Industry Type: Pharmaceutical & Life Sciences,Department: Administration & Facilities,"Employment Type: Full Time, Permanent","['Office Work', 'Back Office', 'Computer', 'Data Entry', 'operation', 'Backend', 'Typing', 'Excel', 'Word', 'Computer Operating', 'MS Office']",2025-06-12 13:50:34
HIH - Data Science Lead Analyst - Evernorth,ManipalCigna Health Insurance,5 - 8 years,Not Disclosed,['Hyderabad'],"Internal Title: Data Science Lead Analyst\nExternal Title: Data Science Lead Analyst\nRole Summary\nAs a member of the Data Science Center of Expertise (DSCOE), the DS Lead Analyst is responsible for leading and enabling Data Science within Cigna Group with demonstrable aptitude in Data Science (i) Technical Skills (ii) Leadership (iii) Scope & Impact (iv) Influence. Please see Qualifications section below for more details.\n\nThe role will support the development and maintenance of machine learning models, with a focus on ensuring that models meet Cigna s requirements for governance and legal compliance. The role will require collaboration with other data scientists and involve work across many lines of business.\nKey Responsibilities:\nAnalyze model performance of new models with specific regards to requirements for legal compliance and governance standards around accuracy and bias;\nPerform periodic analyses of performance of existing models to ensure continued compliance with internal and external standards for accuracy and bias;\nConduct research (i.e. literature review) to understand when bias may be biologically or medically justifiable, and to what degree, for example: finding evidence from literature that heart disease is more prevalent among older populations\nUsing machine learning development tools to mitigate model bias when this is determined to be necessary\nCollaborating with data scientists, business stakeholders, and governance/compliance teams to ensure models meet compliance and governance standards\nQualifications:\nBachelors or Masters/PhD (preferred) in statistics or computer science or equivalent field with 5-8 years of relevant experience\nStrong proficiency in ML, statistics, python or R, SQL, version control (e.g., Git), health care data (e.g., claims, EHR)\nAbility to promote best coding practices, championing a culture of documentation/logging\nThorough understanding of ML lifecycle, including necessary tradeoffs and associated risks\nLeadership in Data Science\nCan own a project end-to-end e.g., scoping, business value estimation, ideation, dev, prod, timeline\nCollaborates and guides junior team members in completion of projects and career development\nWorks cross functionally with technical (e.g., Data Science, Data Engineering) and business (e.g., clinical, marketing, pricing, business analysts) to implement solutions with measurable value\nScope and Impact\nIndependently delivers clear and well-developed presentations for both technical and business audiences\nCreates data science specific project goals associated with project deliverables\nArticulates timeline changes, rationale, and goals to meet deadlines moving forward\nValues diversity, growth mindset, and improving health outcomes of our customers\n\nLevel of Influence\nCommunicate with stakeholders to identify opportunities and possible solutions based on business need\nDraft project charter, timeline, and features/stories\nInfluence matrix-partner leadership\nAbout Evernorth Health Services",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Version control', 'Claims', 'data science', 'Legal compliance', 'Coding', 'Pharmacy', 'Machine learning', 'SQL', 'Python']",2025-06-12 13:50:36
"Sr. Data Analyst – Tableau, SQL, Snowflake",Int9 Solutions,5 - 10 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","We are looking for a skilled Data Analyst with excellent communication skills and deep expertise in SQL, Tableau, and modern data warehousing technologies. This role involves designing data models, building insightful dashboards, ensuring data quality, and extracting meaningful insights from large datasets to support strategic business decisions.\n\nKey Responsibilities:\nWrite advanced SQL queries to retrieve and manipulate data from cloud data warehouses such as Snowflake, Redshift, or BigQuery.\nDesign and develop data models that support analytics and reporting needs.\nBuild dynamic, interactive dashboards and reports using tools like Tableau, Looker, or Domo.\nPerform advanced analytics techniques including cohort analysis, time series analysis, scenario analysis, and predictive analytics.\nValidate data accuracy and perform thorough data QA to ensure high-quality output.\nInvestigate and troubleshoot data issues; perform root cause analysis in collaboration with BI or data engineering teams.\nCommunicate analytical insights clearly and effectively to stakeholders.\n\nRequired Skills & Qualifications:\nExcellent communication skills are mandatory for this role.\n5+ years of experience in data analytics, BI analytics, or BI engineering roles.\nExpert-level skills in SQL, with experience writing complex queries and building views.\nProven experience using data visualization tools like Tableau, Looker, or Domo.\nStrong understanding of data modeling principles and best practices.\nHands-on experience working with cloud data warehouses such as Snowflake, Redshift, BigQuery, SQL Server, or Oracle.\nIntermediate-level proficiency with spreadsheet tools like Excel, Google Sheets, or Power BI, including functions, pivots, and lookups.\nBachelor's or advanced degree in a relevant field such as Data Science, Computer Science, Statistics, Mathematics, or Information Systems.\nAbility to collaborate with cross-functional teams, including BI engineers, to optimize reporting solutions.\nExperience in handling large-scale enterprise data environments.\nFamiliarity with data governance, data cataloging, and metadata management tools (a plus but not required).\nLocation : - Mumbai, Delhi / NCR, Bengaluru , Kolkata, Chennai, Hyderabad, Ahmedabad, Pune, Remote",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Tableau', 'SQL', 'BI Tools', 'Scenario Analysis', 'Cohort Analysis', 'Data Warehousing', 'SQL Server', 'Data Modeling', 'Data Analytics', 'Predictive Analysis', 'Redshift']",2025-06-12 13:50:38
Senior Consultanr - AI Cloud Engineer,AstraZeneca India Pvt. Ltd,5 - 10 years,Not Disclosed,['Chennai'],"Job Title: Senior Consultant - AI Cloud Engineer Career Level: D2 Introduction to role:\nAre you ready to tackle some of the most exciting machine-learning challenges in drug discovery? We are seeking a Senior AI Platform Engineer to join our innovative AI platform team, IGNITE. With your expertise in AWS cloud environments, youll design and deploy large-scale production infrastructure that will redefine healthcare and improve the lives of millions worldwide. As part of a close-knit team of technical specialists, youll create tools that support major AI initiatives, from clinical trial data analysis to imaging and Omics. Your role will be pivotal in providing frameworks for data scientists to develop scalable machine learning models safely and robustly. Are you prepared to bridge the gap between science and engineering with your deep expertise?\nAccountabilities:\nDesign, implement, and manage cloud infrastructure on AWS using Infrastructure as Code (IaC) tools such as Terraform or AWS CloudFormation.\nMaintain and enhance CI/CD pipelines using tools like GitHub Actions, AWS CodePipeline, Jenkins, or ArgoCD.\nEnsure platform reliability, scalability, and high availability across development, staging, and production environments.\nAutomate operational tasks, environment provisioning, and deployments using scripting languages such as Python, Bash, or PowerShell.\nEnable and maintain Amazon SageMaker environments for scalable ML model training, hosting, and pipelines.\nIntegrate AWS Bedrock to provide foundation model access for generative AI applications, ensuring security and cost control.\nLead and publish curated infrastructure templates through AWS Service Catalogue to enable consistent and compliant provisioning.\nCollaborate with security and compliance teams to implement best practices around IAM, encryption, logging, monitoring, and cost optimization.\nImplement and manage observability tools like Amazon CloudWatch, Prometheus/Grafana, or ELK for monitoring and alerting.\nSupport container orchestration environments using EKS (Kubernetes), ECS, or Fargate.\nContribute to incident response, post-mortems, and continuous improvement of the platform s operational excellence.\nEssential Skills/Experience:\nBachelor s degree in Computer Science, Engineering, or related field (or equivalent experience).\n5+ years of hands-on experience with AWS cloud services.\nStrong experience with Terraform, AWS CDK, or CloudFormation.\nProficiency in Linux system administration and networking fundamentals.\nSolid understanding of IAM policies, VPC design, security groups, and encryption.\nExperience with Docker and container orchestration using Kubernetes (EKS preferred).\nHands-on experience with CI/CD tools and version control (Git).\nExperience with monitoring, logging, and alerting systems.\nStrong solving skills and ability to work independently or in a team.\nDesirable Skills/Experience:\nAWS Certification (e.g., AWS Certified DevOps Engineer, Solutions Architect - Associate/Professional).\nExperience with serverless technologies like AWS Lambda, Step Functions, and EventBridge.\nExperience supporting machine learning or big data workloads on AWS.\nExperience with SAFe agile principles and practices.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Version control', 'Networking', 'Machine learning', 'Agile', 'Healthcare', 'Monitoring', 'Python', 'Recruitment']",2025-06-12 13:50:41
Senior Flexera Data Analyst,Luxoft,6 - 11 years,Not Disclosed,['Bengaluru'],"Internal Data Structures & Modeling\nDesign, maintain, and optimize internal data models and structures within the Flexera environment.\nMap business asset data to Flexeras normalized software models with precision and accuracy.\nEnsure accurate data classification, enrichment, and normalization to support software lifecycle tracking.\nPartner with infrastructure, operations, and IT teams to ingest and reconcile data from various internal systems.\nReporting & Analytics\nDesign and maintain reports and dashboards in Flexera or via external BI tools such as Power BI or Tableau.\nProvide analytical insights on software usage, compliance, licensing, optimization, and risk exposure.\nAutomate recurring reporting processes and ensure timely delivery to business stakeholders.\nWork closely with business users to gather requirements and translate them into meaningful reports and visualizations.\nAutomated Data Feeds & API Integrations\nDevelop and support automated data feeds using Flexera REST/SOAP APIs.\nIntegrate Flexera with enterprise tools (e.g., CMDB, SCCM, ServiceNow, ERP) to ensure reliable and consistent data flow.\nMonitor, troubleshoot, and resolve issues related to data extracts and API communication.\nImplement robust logging, alerting, and exception handling for integration pipelines.\nSkills\nMust have\nMinimum 6+ years of working with Flexera or similar software.\nFlexera Expertise: Strong hands-on experience with Flexera One, FlexNet Manager Suite, or similar tools.\nTechnical Skills:\nProficient in REST/SOAP API development and integration.\nStrong SQL skills and familiarity with data transformation/normalization concepts.\nExperience using reporting tools like Power BI, Tableau, or Excel for data visualization.\nFamiliarity with enterprise systems such as SCCM, ServiceNow, ERP, CMDBs, etc.\nProcess & Problem Solving:\nStrong analytical and troubleshooting skills for data inconsistencies and API failures.\nUnderstanding of license models, software contracts, and compliance requirements.\nNice to have\nSoft Skills: Excellent communication skills to translate technical data into business insights.\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nData Engineer with Neo4j\nData Science\nIndia\nChennai\nData Engineer with Neo4j\nData Science\nIndia\nGurugram\nBusiness Analyst\nData Science\nPoland\nRemote Poland\nBengaluru, India\nReq. VR-114544\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114544\nApply for Senior Flexera Data Analyst in Bengaluru\n*",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ERP', 'neo4j', 'Analytical', 'Data structures', 'data visualization', 'SCCM', 'Licensing', 'Analytics', 'Reporting tools', 'SQL']",2025-06-12 13:50:43
Senior Flexera Data Analyst,Luxoft,6 - 11 years,Not Disclosed,['Chennai'],"Internal Data Structures & Modeling\nDesign, maintain, and optimize internal data models and structures within the Flexera environment.\nMap business asset data to Flexeras normalized software models with precision and accuracy.\nEnsure accurate data classification, enrichment, and normalization to support software lifecycle tracking.\nPartner with infrastructure, operations, and IT teams to ingest and reconcile data from various internal systems.\nReporting & Analytics\nDesign and maintain reports and dashboards in Flexera or via external BI tools such as Power BI or Tableau.\nProvide analytical insights on software usage, compliance, licensing, optimization, and risk exposure.\nAutomate recurring reporting processes and ensure timely delivery to business stakeholders.\nWork closely with business users to gather requirements and translate them into meaningful reports and visualizations.\nAutomated Data Feeds & API Integrations\nDevelop and support automated data feeds using Flexera REST/SOAP APIs.\nIntegrate Flexera with enterprise tools (e.g., CMDB, SCCM, ServiceNow, ERP) to ensure reliable and consistent data flow.\nMonitor, troubleshoot, and resolve issues related to data extracts and API communication.\nImplement robust logging, alerting, and exception handling for integration pipelines.\nSkills\nMust have\nMinimum 6+ years of working with Flexera or similar software.\nFlexera Expertise: Strong hands-on experience with Flexera One, FlexNet Manager Suite, or similar tools.\nTechnical Skills:\nProficient in REST/SOAP API development and integration.\nStrong SQL skills and familiarity with data transformation/normalization concepts.\nExperience using reporting tools like Power BI, Tableau, or Excel for data visualization.\nFamiliarity with enterprise systems such as SCCM, ServiceNow, ERP, CMDBs, etc.\nProcess & Problem Solving:\nStrong analytical and troubleshooting skills for data inconsistencies and API failures.\nUnderstanding of license models, software contracts, and compliance requirements.\nNice to have\nSoft Skills: Excellent communication skills to translate technical data into business insights.\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nData Engineer with Neo4j\nData Science\nIndia\nGurugram\nData Engineer with Neo4j\nData Science\nIndia\nBengaluru\nData Scientist\nData Science\nIndia\nBengaluru\nChennai, India\nReq. VR-114544\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114544\nApply for Senior Flexera Data Analyst in Chennai\n*",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ERP', 'neo4j', 'Analytical', 'Data structures', 'data visualization', 'SCCM', 'Licensing', 'Analytics', 'Reporting tools', 'SQL']",2025-06-12 13:50:45
Engineer II,AMERICAN EXPRESS,3 - 8 years,Not Disclosed,['Bengaluru'],"you're a talented, creative, and motivated engineer who loves developing powerful, stable, and intuitive apps and you're excited to work with a team of individuals with that same passion. you've accumulated years of experience, and you're excited about taking your mastery of Cloud, Big Data and Java to a new level. You enjoy challenging projects involving big data sets and are cool under pressure. you're no stranger to fast-paced environments and agile development methodologies in fact, you embrace them. With your strong analytical skills, your unwavering commitment to quality, your excellent technical skills, and your collaborative work ethic, you'll do great things here at American Express.",,,,"['GIT', 'RDBMS', 'Coding', 'Finance', 'MySQL', 'Agile', 'JIRA', 'SQL', 'Python']",2025-06-12 13:50:48
Data Scientist,Devon Software Services,3 - 7 years,12-19 Lacs P.A.,['Bengaluru'],"What you will do\nBuild end-to-end machine learning models to solve business problems in Marketing\nPerform feature engineering and support data engineering to build robust data pipelines on large marketing datasets from different sources\nCollaborate with ML Engineering to build ML Pipelines to Train, Test, Deploy, Serve and Monitor models, Tune Hyperparameters, detect model and data drift and resolve issues\nPresent machine learning models outcomes, and help interpret model predictions to various stakeholders using standard data visualization tools",,,,"['Tensorflow', 'Ai Algorithms', 'Ml Algorithms', 'Machine Learning', 'Python', 'Pytorch', 'Model Development']",2025-06-12 13:50:52
Software Engineer Gen AI,Wells Fargo,2 - 7 years,Not Disclosed,['Bengaluru'],"locationsBengaluru, India\nposted onPosted 4 Days Ago\njob requisition idR-462134\nAbout this role:\nWells Fargo is seeking a Software Engineer.\n\nIn this role, you will:\nParticipate in low to moderately complex initiatives and projects associated with the technology domain, including installation, upgrades, and deployment efforts\nIdentify opportunities for service quality and availability improvements within the technology domain environment\nDesign, code, test, debug, and document for low to moderately complex projects and programs associated with technology domain, including upgrades and deployments\nReview and analyze technical assignments or challenges that are related to low to medium risk deliverables and that require research, evaluation, and selection of alternative technology domains\nPresent recommendations for resolving issues or may escalate issues as needed to meet established service level agreements\nExercise some independent judgment while also developing understanding of given technology domain in reference to security and compliance requirements\nProvide information to technology colleagues, internal partners, and stakeholders\n\nRequired Qualifications:\n2+ years of software engineering experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education\n\nDesired Qualifications:\nWork as a Generative AI engineer developing enterprise-scale AI applications\nDesign, implement, and optimize LLM-based solutions using state-of-the-art frameworks\nLead Gen AI initiatives focused on developing intelligent agents and conversational systems\nDesign and build robust LLM interfaces and orchestration pipelines\nDevelop evaluation frameworks to measure and improve model performance\nImplement prompt engineering techniques to optimize model outputs\nIntegrate Gen AI capabilities with existing enterprise applications\nBuild and maintain frontend interfaces for AI applications\nStrong proficiency in Python/Java and LLM orchestration frameworks (LangChain, LangGraph)\nBasic Knowledge of model context protocols, RAG architectures, and embedding techniques\nExperience with model evaluation frameworks and metrics for LLM performance\nProficiency in frontend development with React.js for AI applications\nExperience with UI/UX design patterns specific to AI interfaces\nExperience with vector databases and efficient retrieval methods\nKnowledge of prompt engineering techniques and best practices\nExperience with containerization and microservices architecture\nStrong understanding of semantic search and document retrieval systems\nWorking knowledge of both structured and unstructured data processing\nExperience with version control using GitHub and CI/CD pipelines\nExperience working with globally distributed teams in Agile scrums\n\nJob Expectations:\nUnderstanding of enterprise use cases for Generative AI\nKnowledge of responsible AI practices and ethical considerations\nAbility to optimize AI solutions for performance and cost\nWell versed in MLOps concepts for LLM applications\nStaying current with rapidly evolving Gen AI technologies and best practices\nExperience implementing security best practices for AI applications",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Gen AI', 'Java', 'UI design', 'UX design', 'LLM orchestration', 'React.js', 'Python', 'MLOps concepts']",2025-06-12 13:50:54
Software Engineer Staff,Juniper Networks,10 - 15 years,Not Disclosed,['Bengaluru'],"The Infrastructure team under Juniper Apstra owns and evolves the backend infrastructure for Juniper Apstra.\nJoin a team where systems and data engineering converge\nyou'll work at the core of a high-performance, custom-built infrastructure platform while also helping shape the data pipelines that power our next-gen cloud analytics stack. This role offers rare dual exposure: from low-level optimization in performance-critical systems, to hands-on feature development in in-house time-series and graph databases . you'll contribute to building and refining connectivity for these databases to the cloud, driving real-world data flows across hybrid environments. If you're excited by deep infrastructure, data-intensive workloads, and full-stack thinking, this is where your impact multiplies.\n  What the Team Does\n  1. Core Infrastructure (60% of Role)\nMaintains and enhances the backend platform that powers Apstra s configuration, telemetry, and analytics pipeline.\nOwns:\nProduct packaging, upgrades, and deployment orchestration\nHomegrown file-based telemetry database (custom-built, optimized for performance)\nDistributed querying, data merging, system calls, IPC\nPython + C++ hybrid execution, including C-extensions for performance\n2. Data Engineering (40% of Role)\nEnables data synchronization from on-prem Apstra to on JCloud.\nConstructs data pipelines and ETL flows to prepare, transform, and deliver telemetry for downstream cloud analytics consumers.\nOwns cloud enablement and sync logic (eg examples datasets, format normalization, usage signals).\nWhy Candidates Should Join This Team\n  you'll Build Real Infrastructure Not Just Configure It\nThis isn t infra as code it s infrastructure as a product . you'll design, scale, and optimize distributed systems that support real-time, multi-version enterprise network workloads across campus, data center, and WAN fabrics. It s infrastructure that directly powers next-gen cloud analytics.\n  Hybrid Challenge: On-Prem Meets Cloud\nFew roles offer this blend: deep low-level systems work coupled with forward-looking data engineering . you'll help bridge Apstra s on-prem legacy with its evolving cloud-native future , working across environments to modernize and integrate.\n  Deep Ownership + Technical Core\nThis team doesn t just connect APIs. It owns the architecture behind Apstra s analytics and telemetry engines critical infrastructure that underpins Juniper s most advanced networking insights.\n  Custom Database Engineering + Analytics Edge\nyou'll work directly on our in-house telemetry database , focusing on:\nCompression, distributed query execution , and performance tuning\nBuilding feature enhancements for in-house time-series and graph databases\nEnabling cloud connectivity and insights generation for real-world data use cases\nCloud\nyou'll get early exposure to JCloud , Juniper s internal cloud platform. Think AWS-like building blocks serverless functions, ETL pipelines, distributed stores but tuned for networking workloads, security, and high observability.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'C++', 'Backend', 'Networking', 'WAN', 'Packaging', 'Distribution system', 'Analytics', 'Downstream', 'Python']",2025-06-12 13:50:56
Sr. Data Analyst,Icims,4 - 9 years,Not Disclosed,['Hyderabad'],"Overview\nThe Senior Data Analyst is responsible for serving as a subject matter expert who can lead efforts to analyze data with the goal of delivering insights that will influence our products and customers. This position will report into the Data Analytics Manager, and will work closely with members of our product and marketing teams, data engineers, and members of our Customer Success organization supporting client outreach efforts. The chief functions of this role will be finding and sharing data-driven insights to deliver value to less technical audiences, and instilling best practices for analytics in the rest of the team.",,,,"['server', 'data', 'vlookup', 'market data', 'data mapping', 'dashboards', 'research', 'sql', 'analytics', 'tables', 'prep', 'pivot', 'data visualization', 'communication skills', 'python', 'data analytics', 'data analysis', 'insights', 'pivot table', 'data engineering', 'graph', 'excel', 'data quality', 'tableau', 'data governance', 'root cause']",2025-06-12 13:51:00
Senior Associate Data Scientist,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will identify trends, root causes, and potential improvements in our products and processes, ensuring that patient voices are heard and addressed with utmost precision.\nAs the Sr Associate Data Scientist at Amgen, you will be responsible for developing and deploying basic machine learning, operational research, semantic analysis, and statistical methods to uncover structure in large data sets. This role involves creating analytics solutions to address customer needs and opportunities.\nCollect, clean, and manage large datasets related to product performance and patient complaints.\nEnsure data integrity, accuracy, and accessibility for further analysis.\nDevelop and maintain databases and data systems for storing patient complaints and product feedback.\nAnalyze data to identify patterns, trends, and correlations in patient complaints and product issues.\nUse advanced statistical methods and machine learning techniques to uncover insights and root causes.\nDevelop analytics or predictive models to foresee potential product issues and patient concerns to address customer needs and opportunities.\nPrepare comprehensive reports and visualizations to communicate findings to key collaborators.\nPresent insights and recommendations to cross-functional teams, including product development, quality assurance, and customer service.\nCollaborate with regulatory and compliance teams to ensure adherence to healthcare standards and regulations.\nFind opportunities for product enhancements and process improvements based on data analysis.\nWork with product complaint teams to implement changes and monitor their impact.\nStay abreast of industry trends, emerging technologies, and standard methodologies in data science and healthcare analytics.\nEvaluate data to support product complaints.\nWork alongside software developers and software engineers to translate algorithms into commercially viable products and services.\nWork in technical teams in development, deployment, and application of applied analytics, predictive analytics, and prescriptive analytics.\nPerform exploratory and targeted data analyses using descriptive statistics and other methods.\nWork with data engineers on data quality assessment, data cleansing and data analytics\nGenerate reports, annotated code, and other projects artifacts to document, archive, and communicate your work and outcomes.\n\nBasic Qualifications:\nMasters degree and 1 to 3 years of Data Science and with one or more analytic software tools or languages, and data visualization tools experience OR\nBachelors degree and 3 to 5 years of Data Science and with one or more analytic software tools or languages, and data visualization tools experience OR\nDiploma and 7 to 9 years of Data Science and with one or more analytic software tools or languages, and data visualization tools experience\nPreferred Qualifications:\nDemonstrated skill in the use of applied analytics, descriptive statistics, feature extraction and predictive analytics on industrial datasets.\nExperience in statistical techniques and hypothesis testing, experience with regression analysis, clustering and classification.\nExperience in analyzing time-series data for forecasting and trend analysis.\nExperience with Data Bricks platform for data analytics.\nExperience working with healthcare data, including patient complaints, product feedback, and regulatory requirements.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'data bricks', 'hypothesis testing', 'predictive analytics', 'data visualization', 'machine learning', 'statistics']",2025-06-12 13:51:03
Business Intel Engineer,Amazon,2 - 7 years,Not Disclosed,['Bengaluru'],"Amazon Business Payments and Lending organization is seeking a highly quantitative Business Intelligence Engineer to drive the development of analytics and insights. You will succeed in this role if you are an organized self-starter who can learn new technologies quickly and excel in a fast-paced environment. In this position, you will be a key contributor and sparring partner, developing analytical solutions that global executive management teams and business leaders will use to deep dive into the businesses and define strategies.\n\nOur team offers a unique opportunity to build a new set of analytical experiences from the ground up. You will be part of the team that is focused on payments around the world. The position is based out of India but will interact with global leaders and teams across Europe, Japan, and US. You should be highly analytical, resourceful, customer focused, team oriented, and have an ability to work independently under time constraints to meet deadlines. You will be comfortable thinking big and diving deep. A proven track record in taking on end-to-end ownership and successfully delivering results in a fast-paced, dynamic business environment is strongly preferred.\n\nNote: This role is part of the rekindle program. For more details on rekindle program, please visit\n\n\n\nOwn the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions. Ensure data accuracy by validating data for new and existing tools.\nRetrieve and analyze data using SQL, Excel, and other data management systems and develop reporting and data visualization solutions -using tools like AWS QuickSight and Looker.\nRecognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.\nModel data and metadata to support the reporting pipelines and automate the manual reporting solutions.\nUnderstand trends related to business metrics and recommend strategies to stakeholders to help drive business growth. Reporting of key insight trends, using statistical rigor (Hypothesis testing, measuring experiment success defining statistical significance, developing basic regression and forecasting models) to simplify and inform the larger team of noteworthy story lines.\n\nA day in the life\nAnalyze data and find insights to either drive strategic business decisions or to drive incremental signups or revenue.\nDefine and develop business critical metrics and reports across all international business levers, key performance indicators, and financials.\nOwn alignment and standardization of analytical initiatives across the global business teams\nDrive efforts across international business leaders, BI leaders and executive management across Europe, Asia and North America.\nOwn key executive reports and metrics that are consumed by our VPs and Directors\nProvide thought leadership in global business deep dives across a variety of key performance indicators 2+ years of analyzing and interpreting data with Redshift, Oracle, NoSQL etc. experience\nExperience with data visualization using Tableau, Quicksight, or similar tools\nExperience with one or more industry analytics visualization tools (e.g. Excel, Tableau, QuickSight, MicroStrategy, PowerBI) and statistical methods (e.g. t-test, Chi-squared)\nExperience with scripting language (e.g., Python, Java, or R) Masters degree, or Advanced technical degree\nKnowledge of data modeling and data pipeline design\nExperience with statistical analysis, co-relation analysis",,,,"['Microstrategy', 'metadata', 'Data management', 'Data modeling', 'Oracle', 'Business intelligence', 'Forecasting', 'Analytics', 'SQL', 'Python']",2025-06-12 13:51:06
Site Reliability Engineer,Apple,5 - 10 years,Not Disclosed,['Bengaluru'],"The people here at Apple don t just build products they craft the kind of wonder that has revolutionised entire industries\nIt s the diversity of those people and their ideas that encourages the innovation that runs through everything we do, from amazing technology to industry-leading environmental efforts\nImagine what you could do here\nJoin Apple, and help us leave the world better than we found it\nA job at Apple is unlike any other you ve had\nYou will be challenged\nYou will be inspired\nAnd you ll be proud! At Apple, phenomenal ideas have a way of becoming phenomenal products, services, and customer experiences very quickly\nBring passion and dedication to your job, and theres no telling what you could accomplish!The Apple Services Engineering team (ASE) is one of the most exciting examples of Apple s long-held passion for combining art and technology\nThese are the people who power the App Store, Apple TV, Apple Music, Apple Podcasts, and Apple Books\nAnd they do it at an extensive scale, meeting our high expectations with dedication to deliver a huge variety of entertainment in over 35 languages to more than 150 countries\nThese engineers build secure, end-to-end solutions\nThey develop the custom software used to process all the creative work, the tools that providers use to deliver that media, all the server-side systems, and the APIs for many Apple services\nThanks to Apple s unique integration of hardware, software, and services, engineers here partner to get behind a single unified vision\nThat vision always includes a deep commitment to strengthening Apple s privacy policy, one of our core values\nAlthough services are a bigger part of Apple s business than ever before, these teams remain small, and multi-functional, offering greater exposure to the array of opportunities here\nDescription\nThe Service Reliability Engineer (SRE) role in Apple Services Engineering requires a mix of strategic engineering and design along with hands-on, technical work\nThis SRE will configure, tune, and fix multi-tiered systems to achieve optimal application performance, stability and availability\nWe manage jobs as well as applications on bare-metal and cloud computing platforms to deliver data processing for many of Apple s global products\nOur teams work with exabytes of data, petabytes of memory, and tens of thousands of jobs to enable predicable and performant data analytics enabling features in Apple Music, TV+, Appstore and other world class products\nIf you love designing, running systems that will impact millions of users, then this is the place for you!The Main Responsibilities for this position include:- Support Java-based applications & Spark/Flink jobs on Baremetal, AWS & Kubernetes- Ability to understand the application requirements (Performance, Security, Scalability, etc\n) and assess the right services/topology on AWS, Baremetal & Kubernetes- Build automation to enable self-healing systems- Build tools to monitor high performance & alert the low-latency applications- Ability to troubleshoot application-specific, core network, system & performance issues\n- Involvement in challenging and fast paced projects supporting Apples business by delivering innovative solutions\n- Monitor production, staging, test and development environments for a myriad of applications in an agile and dynamic organisation\nBS degree in computer science or equivalent field with 5+ years or MS degree with 3+ years experience, or equivalent.\nAt least 5 years in a Site Reliability Engineering (SRE), DevOps role\n5+ years of running services in a large-scale *nix environment\nUnderstanding of SRE principles and goals along with prior on-call experience\nExtensive experience in managing applications on AWS & Kubernetes\nDeep understanding and experience in one or more of the following - Hadoop, Spark, Flink, Kubernetes, AWS\nPreferred Qualifications\nFast learner with excellent analytical problem solving and interpersonal skills\nExperience supporting Java applications\nExperience with Big Data Technologies\nExperience working with geographically distributed teams and implementing high level projects and migrations\nStrong communication skills and ability to deliver results on time with high quality",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Cloud computing', 'Interpersonal skills', 'spark', 'Analytical', 'Agile', 'Data processing', 'core network', 'big data', 'AWS']",2025-06-12 13:51:08
Etl Testing Engineer,Infosys,3 - 8 years,Not Disclosed,"['Kolkata', 'Hyderabad', 'Bengaluru']","Job description,\n\nHiring for ETL testing with experience range 3-10 years\n\nMandatory Skills: ETL Testing\n\nLocation - Bangalore/Hyderabad/Pune/kolkata/Chennai/Bhubaneswar\n\nEducation: BE/B.Tech/MCA/M.Tech/MSc./MS\n\nResponsibilities\nA day in the life of an Infoscion\nAs part of the Infosys consulting team, your primary role would be to actively aid the consulting team in different phases of the project including problem definition, effort estimation, diagnosis, solution generation and design and deployment\nYou will explore the alternatives to the recommended solutions based on research that includes literature surveys, information available in public domains, vendor evaluation information, etc. and build POCs\nYou will create requirement specifications from the business needs, define the to-be-processes and detailed functional designs based on requirements.\nYou will support configuring solution requirements on the products; understand if any issues, diagnose the root-cause of such issues, seek clarifications, and then identify and shortlist solution alternatives\nYou will also contribute to unit-level and organizational initiatives with an objective of providing high quality value adding solutions to customers. If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ETL Testing', 'ETL', 'DWH Testing']",2025-06-12 13:51:11
Software Engineer 4,Juniper Networks,7 - 13 years,Not Disclosed,['Bengaluru'],"you'll work at the core of a high-performance, custom-built infrastructure platform while also helping shape the data pipelines that power our next-gen cloud analytics stack. This role offers rare dual exposure: from low-level optimization in performance-critical systems, to hands-on feature development in in-house time-series and graph databases . you'll contribute to building and refining connectivity for these databases to the cloud, driving real-world data flows across hybrid environments. If you're excited by deep infrastructure, data-intensive workloads, and full-stack thinking, this is where your impact multiplies.\nWhat the Team Does\n1. Core Infrastructure (60% of Role)\nMaintains and enhances the backend platform that powers Apstra s configuration, telemetry, and analytics pipeline.\nOwns:\nProduct packaging, upgrades, and deployment orchestration\nHomegrown file-based telemetry database (custom-built, optimized for performance)\nDistributed querying, data merging, system calls, IPC\nPython + C++ hybrid execution, including C-extensions for performance\n2. Data Engineering (40% of Role)\nEnables data synchronization from on-prem Apstra to on JCloud.\nConstructs data pipelines and ETL flows to prepare, transform, and deliver telemetry for downstream cloud analytics consumers.\nOwns cloud enablement and sync logic (eg examples datasets, format normalization, usage signals).\nWhy Candidates Should Join This Team\nyou'll Build Real Infrastructure Not Just Configure It\nThis isn t infra as code it s infrastructure as a product . you'll design, scale, and optimize distributed systems that support real-time, multi-version enterprise network workloads across campus, data center, and WAN fabrics. It s infrastructure that directly powers next-gen cloud analytics.\nHybrid Challenge: On-Prem Meets Cloud\nFew roles offer this blend: deep low-level systems work coupled with forward-looking data engineering . you'll help bridge Apstra s on-prem legacy with its evolving cloud-native future , working across environments to modernize and integrate.\nDeep Ownership + Technical Core\nThis team doesn t just connect APIs. It owns the architecture behind Apstra s analytics and telemetry engines critical infrastructure that underpins Juniper s most advanced networking insights.\nCustom Database Engineering + Analytics Edge\nyou'll work directly on our in-house telemetry database , focusing on:\nCompression, distributed query execution , and performance tuning\nBuilding feature enhancements for in-house time-series and graph databases\nEnabling cloud connectivity and insights generation for real-world data use cases\nCloud\nyou'll get early exposure to JCloud , Juniper s internal cloud platform. Think AWS-like building blocks serverless functions, ETL pipelines, distributed stores but tuned for networking workloads, security, and high observability.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'C++', 'Backend', 'Networking', 'WAN', 'Packaging', 'Distribution system', 'Analytics', 'Downstream', 'Python']",2025-06-12 13:51:13
Software Development Engineer II,Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Want to join a team that protects and improves the buyer experience of millions Amazon customers and builds earths most customer-centric sellers daily using innovative technology including machine learning, data mining and big data analytics, cloud computing services, and highly available/scalable distributed systems that support hundreds of millions of transactions across the globe?\n\nWe have an exciting opportunity with the Regulatory Intelligence, Safety, and Compliance (RISC) engineering team, to architect and build next-generation engineering systems to quickly and accurately identify and mitigate product safety issues and potential risks to the customer experience.\n\nAs a Software Development Engineer, you will work with your team of highly skilled software, data, and ML engineers to invent, design, build and manage highly scalable distributed systems that provide availability, scalability and latency guarantees. You will work with your internal customers to balance customer requirements with team requirements and help your team and business evolve, by working with LLMs and large data sets. You will be using the latest AI, AWS and industry technologies to deliver a one-stop risk identification and remediation ecosystem for Amazon, keeping our customers safe and products compliant, building the software creating the world s most trustworthy data set on everything companies and customers need to know related to the safety and compliance of products and chains.\n\nEach and every person buying, selling, or handling Amazon products will be your customer.\n\nAs a member of this growing team, you ll be able to build the groundwork and influence its direction for the years to come. Our work cuts across various disciplines from delivering an awesome user experience via great UI/UX, to building massively scalable backend systems to support the most high-traffic pages on Amazon.com, to analytical and feedback systems which give us data-driven customer insights, to using machine learning and AI to influence recommendations and marketing. If you have a passion for consumer-facing applications, and are obsessed with customer experience, we want you!\n\nIf you d like to make a real-world difference by working hard, having fun, and making history, this is the team for you!\n\n\nIn this role you will:\nHelp define the system architecture, own and implement specific components, and help shape the overall experience\nCollaborate closely with product managers, UX designers, and other SDE team members to help define the scope of the product\nTake responsibility for technical problem solving, creatively meeting product objectives, and developing best practices\nDemonstrate cross-functional resource interaction to accomplish your goals\nWrite high-quality, efficient, testable code in Java and other object-oriented languages\nDesign Amazon-scale tools to facilitate internal business\nBuild highly available, secure, and low-latency systems\nMentor other developers\nFind out what it takes to engineer systems for ""Amazon Scale""\nDesign and build microservices\nOwn and operate the systems that you build based on real-time customer data and demanding service-level agreements\nContribute to planning, design, implementation, testing, operations, and process improvement\n\nA day in the life\nHigh-level designs, cross-team alignment, long-term architectural roadmap and technical strategy, understanding the business domain and proposing solutions to address customer and business problems, helping scope and analyze product requirements, mentorship, reviewing CRs, writing high-quality code to be an example for the team. 3+ years of non-internship professional software development experience\n2+ years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience\n3+ years of Video Games Industry (supporting title Development, Release, or Live Ops) experience\nExperience programming with at least one software programming language 3+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience\nBachelors degree in computer science or equivalent",,,,"['Computer science', 'System architecture', 'Cloud computing', 'Backend', 'Coding', 'Analytical', 'Machine learning', 'Data mining', 'Internship', 'Distribution system']",2025-06-12 13:51:16
Software Development Engineer II,Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Want to join a team that protects and improves the buyer experience of millions Amazon customers and builds earths most customer-centric sellers daily using innovative technology including machine learning, data mining and big data analytics, cloud computing services, and highly available/scalable distributed systems that support hundreds of millions of transactions across the globe?\n\nWe have an exciting opportunity with the Regulatory Intelligence, Safety, and Compliance (RISC) engineering team, to architect and build next-generation engineering systems to quickly and accurately identify and mitigate product safety issues and potential risks to the customer experience.\n\nAs a Software Development Engineer, you will work with your team of highly skilled software, data, and ML engineers to invent, design, build and manage highly scalable distributed systems that provide availability, scalability and latency guarantees. You will work with your internal customers to balance customer requirements with team requirements and help your team and business evolve, by working with LLMs and large data sets. You will be using the latest AI, AWS and industry technologies to deliver a one-stop risk identification and remediation ecosystem for Amazon, keeping our customers safe and products compliant, building the software creating the world s most trustworthy data set on everything companies and customers need to know related to the safety and compliance of products and chains.\n\nEach and every person buying, selling, or handling Amazon products will be your customer.\n\nAs a member of this growing team, you ll be able to build the groundwork and influence its direction for the years to come. Our work cuts across various disciplines from delivering an awesome user experience via great UI/UX, to building massively scalable backend systems to support the most high-traffic pages on Amazon.com, to analytical and feedback systems which give us data-driven customer insights, to using machine learning and AI to influence recommendations and marketing. If you have a passion for consumer-facing applications, and are obsessed with customer experience, we want you!\n\nIf you d like to make a real-world difference by working hard, having fun, and making history, this is the team for you!\n\n\nIn this role you will:\nHelp define the system architecture, own and implement specific components, and help shape the overall experience\nCollaborate closely with product managers, UX designers, and other SDE team members to help define the scope of the product\nTake responsibility for technical problem solving, creatively meeting product objectives, and developing best practices\nDemonstrate cross-functional resource interaction to accomplish your goals\nWrite high-quality, efficient, testable code in Java and other object-oriented languages\nDesign Amazon-scale tools to facilitate internal business\nBuild highly available, secure, and low-latency systems\nMentor other developers\nFind out what it takes to engineer systems for ""Amazon Scale""\nDesign and build microservices\nOwn and operate the systems that you build based on real-time customer data and demanding service-level agreements\nContribute to planning, design, implementation, testing, operations, and process improvement\n\nA day in the life\nHigh-level designs, cross-team alignment, long-term architectural roadmap and technical strategy, understanding the business domain and proposing solutions to address customer and business problems, helping scope and analyze product requirements, mentorship, reviewing CRs, writing high-quality code to be an example for the team. 3+ years of non-internship professional software development experience\n2+ years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience\n3+ years of Video Games Industry (supporting title Development, Release, or Live Ops) experience\nExperience programming with at least one software programming language 3+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience\nBachelors degree in computer science or equivalent",Industry Type: Internet,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'System architecture', 'Cloud computing', 'Backend', 'Coding', 'Analytical', 'Machine learning', 'Data mining', 'Internship', 'Distribution system']",2025-06-12 13:51:19
"Quality Assurance Engineer I, Ads QA",Amazon,2 - 7 years,Not Disclosed,['Bengaluru'],"Are you looking to join a team that is among the fastest growing organizations at Amazon? Does wearing multiple hats and working in a fast-paced, entrepreneurial environment sound like a good fit? Then consider joining Amazon Ads.\nAmazon Advertising operates at the intersection of e-commerce and advertising, offering a range of digital advertising solutions with the goal of helping customers discover and shop for anything they want to buy. The Amazon Advertising business is growing at a fast pace and this team s mission is to apply technology to accelerate that growth through best-in-class software engineering, data engineering, and business intelligence.\n\nWithin Amazons advertising ecosystem, the QA team serves as the cornerstone of quality assurance focusing on supporting testing for all initiatives that materially change the advertiser experience or involve significant re-architecture of the ad stack. This includes comprehensive testing of all Tier 1 launches, our most critical deployments that directly affect revenue and customer experience. We drive engineering excellence through automation, developing and maintaining tools that enhance developer productivity and solve testing challenges. We evangelize quality related best practices, building mechanisms to track and report them.\n\nWe are hiring experienced Quality Assurance Engineer (QAE) to drive quality excellence in our advertising systems. In this role, you will architect, design and build test suites and frameworks to push our advertising systems to their limits and beyond. You will work with program management, development teams and our QA organization to understand the customer requirements, scope out features and then work side-by-side with the team to ensure our high quality bar is met and raised. Youll be responsible for raising our quality standards through innovative automation solutions and technical leadership, while consistently exceeding delivery expectations.\n\n\nParticipate in the full development life cycle, working within broadly defined parameters, including test plan execution and software quality needs.\nWriting and executing test plans, designing and developing test tools, automation, debugging and reporting code bugs and pushing quality upstream.\nOwn the delivery of an entire software development test suites and frameworks.\nWork closely with the technical leaders to develop the best approach for testing our functionality at scale. You are capable of understanding the interaction between the components in a distributed system in order to ensure they are functioning properly.\nCreate and execute appropriate test strategies and processes that align with business objectives and project timelines. 2+ years of quality assurance engineering experience\nExperience in manual testing\nExperience in automation testing\nExperience designing and planning test conditions, test scripts, and test data sets to ensure appropriate and adequate coverage and control Experience in API & Mobile testing\nExperience with technologies (like Selenium, Junit, TestNG, and other open source tools)\nExperience with at least one modern language such as Java, Python, C++, or C# including object-oriented design",,,,"['C++', 'Manual testing', 'Test scripts', 'Debugging', 'Test planning', 'Selenium', 'software quality', 'Business intelligence', 'Open source', 'Python']",2025-06-12 13:51:21
Principal Architect (Data and Cloud),Neoware Technology Solutions,10 - 15 years,Not Disclosed,"['Chennai', 'Bengaluru']","Principal Architect (Data and Cloud) - Neoware Technology Solutions Private Limited Principal Architect (Data and Cloud)\nRequirements\nMore than 10 years of experience in Technical, Solutioning, and Analytical roles.\n5+ years of experience in building and managing Data Lakes, Data Warehouse, Data Integration, Data Migration and Business Intelligence/Artificial Intelligence solutions on Cloud (GCP/AWS/Azure).\nAbility to understand business requirements, translate them into functional and non-functional areas, define non-functional boundaries in terms of Availability, Scalability, Performance, Security, Resilience etc.\nExperience in architecting, designing, and implementing end to end data pipelines and data integration solutions for varied structured and unstructured data sources and targets.\nExperience of having worked in distributed computing and enterprise environments like Hadoop, GCP/AWS/Azure Cloud.\nWell versed with various Data Integration, and ETL technologies on Cloud like Spark, Pyspark/Scala, Dataflow, DataProc, EMR, etc. on various Cloud.\nExperience of having worked with traditional ETL tools like Informatica / DataStage / OWB / Talend , etc.\nDeep knowledge of one or more Cloud and On-Premise Databases like Cloud SQL, Cloud Spanner, Big Table, RDS, Aurora, DynamoDB, Oracle, Teradata, MySQL, DB2, SQL Server, etc.\nExposure to any of the No-SQL databases like Mongo dB, CouchDB, Cassandra, Graph dB, etc.\nExperience in architecting and designing scalable data warehouse solutions on cloud on Big Query or Redshift.\nExperience in having worked on one or more data integration, storage, and data pipeline tool sets like S3, Cloud Storage, Athena, Glue, Sqoop, Flume, Hive, Kafka, Pub-Sub, Kinesis, Dataflow, DataProc, Airflow, Composer, Spark SQL, Presto, EMRFS, etc.\nPreferred experience of having worked on Machine Learning Frameworks like TensorFlow, Pytorch, etc.\nGood understanding of Cloud solutions for Iaas, PaaS, SaaS, Containers and Microservices Architecture and Design.\nAbility to compare products and tools across technology stacks on Google, AWS, and Azure Cloud.\nGood understanding of BI Reporting and Dashboarding and one or more tool sets associated with it like Looker, Tableau, Power BI, SAP BO, Cognos, Superset, etc.\nUnderstanding of Security features and Policies in one or more Cloud environments like GCP/AWS/Azure.\nExperience of having worked in business transformation projects for movement of On-Premise data solutions to Clouds like GCP/AWS/Azure.\nBe a trusted technical advisor to customers and solutions for complex Cloud & Data related technical challenges.\nBe a thought leader in architecture design and development of cloud data analytics solutions.\nLiaison with internal and external stakeholders to design optimized data analytics solutions.\nPartner with SMEs and Solutions Architects from leading cloud providers to present solutions to customers.\nSupport Sales and GTM teams from a technical perspective in building proposals and SOWs.\nLead discovery and design workshops with potential customers across the globe.\nDesign and deliver thought leadership webinars and tech talks alongside customers and partners.\nResponsibilities\nLead multiple data engagements on GCP Cloud for data lakes, data engineering, data migration, data warehouse, and business intelligence.\nInterface with multiple stakeholders within IT and business to understand the data requirements.\nTake complete responsibility for the successful delivery of all allocated projects on the parameters of Schedule, Quality, and Customer Satisfaction.\nResponsible for design and development of distributed, high volume multi-thread batch, real-time, and event processing systems.\nImplement processes and systems to validate data, monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.\nWork with the Pre-Sales team on RFP, RFIs and help them by creating solutions for data.\nMentor young Talent within the Team, Define and track their growth parameters.\nContribute to building Assets and Accelerators.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Db2', 'Cognos', 'MySQL', 'Datastage', 'Presales', 'Informatica', 'Oracle', 'Teradata', 'Business intelligence', 'SQL']",2025-06-12 13:51:24
Data Solution Architect,Maveric,13 - 20 years,Not Disclosed,"['Chennai', 'Bengaluru']","Position Overview\nWe are looking for a highly experienced and versatile Solution Architect Data to lead the solution design and delivery of next-generation data solutions for our BFS clients. The ideal candidate will have a strong background in data architecture and engineering, deep domain expertise in financial services, and hands-on experience with cloud-native data platforms and modern data analytics tools. The role will require architecting solutions across Retail, Corporate, Wealth, and Capital Markets, as well as Payments, Lending, and Onboarding journeys. Possession of Data Analytics and Exposure to Data regulatory domain will be of distinct advantage. Hands on experience of AI & Gen AI enabling data related solution will be a distinct advantage for the position.",,,,"['Data Quality', 'Data Engineering', 'Data Governance', 'GenAI']",2025-06-12 13:51:26
"Associate Staff Engineer, Frontend React",Nagarro,5 - 7 years,Not Disclosed,['Bengaluru'],"We're Nagarro.\n\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal Experience 5+ years.\nHands on working experience in front-end or full-stack development experience, with building production apps in React.js and Next.js.\nHands-on expertise writing unit/integration tests for React components (Jest, React Testing Library, etc.)\nSolid grasp of state-management patterns and libraries (Redux, React Context, Zustand, etc.).\nStrong understanding of RESTful APIs, asynchronous programming (Promises, async/await), and modern build tools (Webpack, Vite, or Turbopack).\nPractical experience with Git, pull-request workflows, and collaborative development tools (GitHub, GitLab, Bitbucket).\nAdvanced proficiency in JavaScript (ES6+) and TypeScript.\nProblem-solving mindset with the ability to tackle complex data engineering challenges. \nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\n\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them in to technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Typescript', 'Javascript', 'React.Js']",2025-06-12 13:51:28
"Software Development Engineer II, RISC",Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Want to join a team that protects and improves the buyer experience of millions Amazon customers and builds earths most customer-centric sellers daily using innovative technology including machine learning, data mining and big data analytics, cloud computing services, and highly available/scalable distributed systems that support hundreds of millions of transactions across the globe?\n\nWe have an exciting opportunity with the Regulatory Intelligence, Safety, and Compliance (RISC) engineering team, to architect and build next-generation engineering systems to quickly and accurately identify and mitigate product safety issues and potential risks to the customer experience.\n\nAs a Software Development Engineer, you will work with your team of highly skilled software, data, and ML engineers to invent, design, build and manage highly scalable distributed systems that provide availability, scalability and latency guarantees. You will work with your internal customers to balance customer requirements with team requirements and help your team and business evolve, by working with LLMs and large data sets. You will be using the latest AI, AWS and industry technologies to deliver a one-stop risk identification and remediation ecosystem for Amazon, keeping our customers safe and products compliant, building the software creating the world s most trustworthy data set on everything companies and customers need to know related to the safety and compliance of products and chains.\n\nEach and every person buying, selling, or handling Amazon products will be your customer.\n\nAs a member of this growing team, you ll be able to build the groundwork and influence its direction for the years to come. Our work cuts across various disciplines from delivering an awesome user experience via great UI/UX, to building massively scalable backend systems to support the most high-traffic pages on Amazon.com, to analytical and feedback systems which give us data-driven customer insights, to using machine learning and AI to influence recommendations and marketing. If you have a passion for consumer-facing applications, and are obsessed with customer experience, we want you!\n\nIf you d like to make a real-world difference by working hard, having fun, and making history, this is the team for you!\n\n\nIn this role you will:\nHelp define the system architecture, own and implement specific components, and help shape the overall experience\nCollaborate closely with product managers, UX designers, and other SDE team members to help define the scope of the product\nTake responsibility for technical problem solving, creatively meeting product objectives, and developing best practices\nDemonstrate cross-functional resource interaction to accomplish your goals\nWrite high-quality, efficient, testable code in Java and other object-oriented languages\nDesign Amazon-scale tools to facilitate internal business\nBuild highly available, secure, and low-latency systems\nMentor other developers\nFind out what it takes to engineer systems for ""Amazon Scale""\nDesign and build microservices\nOwn and operate the systems that you build based on real-time customer data and demanding service-level agreements\nContribute to planning, design, implementation, testing, operations, and process improvement\n\nA day in the life\nHigh-level designs, cross-team alignment, long-term architectural roadmap and technical strategy, understanding the business domain and proposing solutions to address customer and business problems, helping scope and analyze product requirements, mentorship, reviewing CRs, writing high-quality code to be an example for the team. 3+ years of non-internship professional software development experience\n2+ years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience\n3+ years of Video Games Industry (supporting title Development, Release, or Live Ops) experience\nExperience programming with at least one software programming language 3+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience\nBachelors degree in computer science or equivalent",,,,"['Computer science', 'System architecture', 'Cloud computing', 'Backend', 'Coding', 'Analytical', 'Machine learning', 'Data mining', 'Internship', 'Distribution system']",2025-06-12 13:51:31
Senior Financial Data Analyst,Simcorp,4 - 5 years,Not Disclosed,['Noida'],"Financial Analyst WHAT MAKES US, US Join some of the most innovative thinkers in FinTech as we lead the evolution of financial technology. If you are an innovative, curious, collaborative person who embraces challenges and wants to grow, learn and pursue outcomes with our prestigious financial clients, say Hello to SimCorp! At its foundation, SimCorp is guided by our values caring, customer success-driven, collaborative, curious, and courageous. Our people-centered organization focuses on skills development, relationship building, and client success. We take pride in cultivating an environment where all team members can grow, feel heard, valued, and empowered. If you like what we re saying, keep reading!\nWHY THIS ROLE IS IMPORTANT TO US\nThe Financial Data Operator is responsible to perform the collection, composition, control and distribution of market and master data for financial instruments (Equities, Funds, Fixed Income, ABTS/MBS, OTS Derivatives, etc.) for various SimCorp clients and in accordance of the effective SLA agreements.\nFurthermore, this role is responsible for answering client questions and conduct all necessary data analyses of financial instruments data to resolve service delivery incidents to continue service delivery. The role is also responsible to adhere to all relevant operational risk as well as data governance and quality frameworks.\nEventually, this role also requires demonstrating very client-focused mindset, substantial know- how of financial instruments (such as Equities, Fixed Income, ABS/MBS, etc.) and provide coaching to other members.\nWHAT YOU WILL BE RESPONSIBLE FOR\nPerforms all daily service deliverables in terms of collecting, composing, controlling, and distributing financial instrument data according to effective client SLAs\nExecution of all quality checks part of the service scope and strict adherence to existing runbook(s) as well as data quality and governance frameworks and conduct first data analysis in case of unexpected data behavior\nResolve all data questions, service requests and requested audit support raised by clients in a timely and professional manner to ensure customer satisfaction and SLA compliance\nPerform all necessary tasks to comply existing operational risk frameworks (e.g., Sarbanes- Oxley Act (SOX), Risk and Control Engine (RACE) etc.)\nEfficiently support and contribute to continuous improvement of operational processes (with predominant focus on manual processes, high-risk areas), data quality checks and system functionality\nWork with local/regional clients to identify specific requirements, special data treatment or any other client demands which need to be delivered as part of the service scope\nExperience working cross-organizationally with both Business and Technology groups.\nPerform continuous know-how exchange between the different Data Operations teams in terms of processes, incidents, documentation, or other open topics to avoid know-how silos/gaps and assure service level consistency\nMonitor and report any kind of issues along the data supply chain including but not limited to interface issues, missing data files or interrupted business processes and trigger the necessary resolution processes to ensure service delivery continuation\nMaintain documentation in terms of business processes, functional descriptions, operational runbooks, or other manuals to ensure information transparency and enable know-how transfers\nWHAT WE VALUE\nFor the Financial Analyst position, we value\nMUST HAVE:\nExperience with data vendor feeds (Bloomberg, IDC, Reuters, etc.) and display products, 4- 5 years\nDeep knowledge of traditional and non-traditional financial instruments and markets including structured securities, Swaps, especially complex instruments like ABS/MBS, index linked bonds, and syndicated loans.\nBachelor s degree or equivalent in finance or engineering\nSolving master and reference data issues based on exception handling, 4-5 years\nExperience of data integration on any EDM platform, 4-5 years\nApplying operational data management and data governance, 2-3 years\nProcess design and engineering experience, 2-3 years\nExperience with service request systems or any other similar ticketing tool, like HPALM, Service Now Salesforce, etc., 4-5 years\nGOOD TO HAVE:\nAbility to troubleshoot technical glitches in existing data process and coordinate with Technology team to resolve.\nExperience in developing process automation, improvements, and streamlining using tools like KNIME, Alteryx, Excel VBA with scripting on programming language such as Python, PowerShell including intermediate knowledge of SQL",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Supply chain', 'Loans', 'Data analysis', 'Ticketing', 'Data management', 'Operational risk', 'Bloomberg', 'Fixed income', 'SQL', 'Auditing']",2025-06-12 13:51:33
Azure Data Bricks (4-15 Yrs) - Bangalore,Happiest Minds Technologies,4 - 9 years,Not Disclosed,['Bengaluru'],"Hi,\n\nGreetings from Happiest Minds Technologies\n\nCurrently we are hiring for below positions and looking for immediate joiners.\n1. Azure Databricks Bangalore 5 to 10 Yrs - Bangalore\nAs a Senior Azure Data Engineer, you will leverage Azure technologies to drive data transformation, analytics, and machine learning. You will design scalable Databricks data pipelines using PySpark, transforming raw data into actionable insights. Your role includes building, deploying, and maintaining machine learning models using MLlib or TensorFlow while optimizing cloud data integration from Azure Blob Storage, Data Lake, and SQL/NoSQL sources. You will execute large-scale data processing using Spark Pools, fine-tuning configurations for efficiency. The ideal candidate holds a Bachelors or Masters in Computer Science, Data Science, or a related field, with 7+ years in data engineering and 3+ years specializing in Azure Databricks, PySpark, and Spark Pools. Proficiency in Python PySpark, Pandas, NumPy, SciPy, Spark SQL, DataFrames, RDDs, Delta Lake, Databricks Notebooks, and MLflow is required, along with hands-on experience in Azure Data Lake, Blob Storage, and Synapse Analytics.",,,,"['Pyspark', 'Azure', 'Data Bricks', 'sql', 'ETL']",2025-06-12 13:51:35
App Dev & Support Engineer III,Conduent,6 - 11 years,Not Disclosed,['Bengaluru'],"Responsibilities\nDesign and develop highly scalable web-based applications based on business needs.\nDesign and customize software for client use with the aim of optimizing operational efficiency.\nA deep understanding of, and ability to use and explain all aspects of application integration in .NET and data integration with SQL Server and associated technologies and standards\nStrong background in building and operating SAAS platforms using the Microsoft technology stack with modern services-based architectures.\nAbility to recommend and configure Azure subscriptions and establish connectivity\nWork with IT teams to setup new application architecture requirements\nCoordinate releases with Quality Assurance Team and implement SDLC workflows and better source code integration.\nImplement build process and continuous build integration with Unit Testing framework.\nDevelop and maintain a thorough understanding of business needs from both technical and business perspectives\nAssist and mentor junior team members to enforce development guidelines.\nTake technical ownership of products and provide support with quick turnaround.\nEffectively prioritize and execute tasks in a high-pressure environment\n\n\nQualifications / Experience\nBachelor\\u2019s/master\\u2019s degree in computer science / computer engineering\nMinimum of 6+ years\\u2019 experience in building enterprise scale windows and web application using Microsoft .NET technologies.\n5+ years of experience in C#, ASP.NET MVC and.Net Core Web API\n1+ years of experience in Angular 2 or higher\nExperience in any of the following are also desirableBootstrap, Knockout, entity framework, nhibernate, Subversion, Linq, Asynchronous Module Definition (such as requirejs)\nIn depth knowledge on design patterns and unit testing frameworks.\nExperience with Agile application development.\nSQL Server development, performance tuning (SQL Server 2014/2016) and troubleshooting\nAbility to work with a sense of urgency and attention to detail\nExcellent oral and written communication skills.",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql server', 'application integration', 'design patterns', '.net', 'data integration', 'c#', 'unit testing framework', 'web application', 'performance tuning', 'unit testing', 'scale', 'knockoutjs', 'net mvc', 'angular', 'linq', 'saas applications', 'asp.net', 'web api', 'mvc', 'asp', 'microsoft net']",2025-06-12 13:51:38
Data Scientist,Ltimindtree,8 - 13 years,Not Disclosed,"['Chennai', 'Bengaluru', 'Mumbai (All Areas)']",We are looking for an experienced AI ML Developers experience in data science specializing in machine learning python statistical modelling and big data technologies pyspark sql.\n\nThe ideal candidate will have a strong background in developing and deploying machine learning models optimizing ML pipelines and handling largescale structured and unstructured data to drive business impact.\n\nDeep understanding of supervised and unsupervised learning including regression classification Multiclass classification clustering and NLP Proficiency in statistical analysis AB testing and causal inference techniques Experience with model deployment and MLOps in cloud environments AWS GCP \n\nKey Responsibilities\n\nDevelop and deploy machine learning models and predictive analytics solutions for business impact\nWork with largescale structured and unstructured data to extract insights and build scalable models\nDesign implement and optimize ML pipelines for realtime and batch processing\nCollaborate with engineering product and business stakeholders to translate business problems into data science solutions\nApply statistical modeling AB testing and causal inference techniques to evaluate business performance\nApply machine learning and statistical techniques for audience segmentation helping to identify patterns and optimise business strategies\nDrive research and innovation by staying updated with cuttingedge MLAI advancements and incorporating them into our solutions\nOptimize data science models for performance scalability and interpretability in production environments\nMentor junior data scientists and contribute to best practices in data science and engineering,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Aiml', 'Ab Testing']",2025-06-12 13:51:40
Ai Ml Engineer,Optum,5 - 10 years,Not Disclosed,['Noida'],"Optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health optimization on a global scale. Join us to start Caring. Connecting. Growing together.  \nAI Engineer is tasked with the design, development, and deployment of advanced generative AI models and systems. This position requires close collaboration with data scientists, product managers, and other stakeholders to integrate generative AI solutions into existing products and develop new innovative features. Proficiency in the Agentic AI framework is vital for coordinating multiple autonomous AI agents to accomplish complex tasks.\n\nPrimary Responsibilities:\nImplement Generative AI Models: Develop sophisticated generative AI algorithms and models to create new data samples, patterns, or content based on existing data or inputs\nData Processing: Collaborate with stakeholders to preprocess, analyze, and interpret extensive datasets\nModel Deployment: Deploy generative AI models into production environments, ensuring scalability and robustness\nOptimization: Conduct model testing, validation, and optimization to enhance performance\nIntegration: Work with cross-functional teams to seamlessly integrate generative AI solutions into products\nResearch: Stay current with the latest advancements in generative AI technologies and practices\nAgentic AI Framework: Utilize the Agentic AI framework to coordinate multiple AI agents for the completion of complex tasks\nMentorship: Provide mentorship to junior team members and offer technical guidance\nComply with the terms and conditions of the employment contract, company policies and procedures, and any and all directives (such as, but not limited to, transfer and/or re-assignment to different work locations, change in teams and/or work shifts, policies in regards to flexibility of work benefits and/or work environment, alternative work arrangements, and other decisions that may arise due to the changing business environment). The Company may adopt, vary or rescind these policies and directives in its absolute discretion and without any limitation (implied or otherwise) on its ability to do so\n\nRequired Qualifications:\nBachelor's or Master's degree in Computer Science, Engineering, or a related field\n5+ years of experience in software engineering with a focus on AI/ML\nExperience with data preprocessing and analysis\nKnowledge of the Agentic AI framework and its application in AI systems\nProficiency in machine learning frameworks such as TensorFlow and PyTorch\nSolid programming skills in Python, Java, or C++\nFamiliarity with cloud platforms (e.g., AWS, Google Cloud, Azure)\nProven excellent problem-solving abilities and algorithmic thinking\nProven solid communication and teamwork skills\n\nPreferred Qualifications:\nExperience with data processing\nKnowledge of version control systems like Git\nUnderstanding of Generative AI, associated technologies and frameworks like RAG, agents etc.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Agentic Ai', 'Gen AI', 'Cloud', 'RAG', 'LLM']",2025-06-12 13:51:43
"Business Intelligence Engineer, RBS ARTS",Amazon,5 - 10 years,Not Disclosed,['Chennai'],"An candidate will be a self-starter who is passionate about discovering and solving complicated problems, learning complex systems, working with numbers, and organizing and communicating data and reports. You will be detail-oriented and organized, capable of handling multiple projects at once, and capable of dealing with ambiguity and rapidly changing priorities. You will have expertise in process optimizations and systems thinking and will be required to engage directly with multiple internal teams to drive business projects/automation for the RBS team. Candidates must be successful both as individual contributors and in a team environment, and must be customer-centric. Our environment is fast-paced and requires someone who is flexible, detail-oriented, and comfortable working in a deadline-driven work environment. Responsibilities Include Works across team(s) and Ops organization at country, regional and/or cross regional level to drive improvements and enables to implement solutions for customer, cost savings in process workflow, systems configuration and performance metrics.\n\nBasic Qualifications\nBachelors degree in Computer Science, Information Technology, or a related field\nProficiency in automation using Python\nExcellent oral and written communication skills\nExperience with SQL, ETL processes, or data transformation\n\nPreferred Qualifications\nExperience with scripting and automation tools\nFamiliarity with Infrastructure as Code (IaC) tools such as AWS CDK\nKnowledge of AWS services such as SQS, SNS, CloudWatch and DynamoDB\nUnderstanding of DevOps practices, including CI/CD pipelines and monitoring solutions\nUnderstanding of cloud services, serverless architecture, and systems integration\n\n\nAs a Business Intelligence Engineer in the team, you will collaborate closely with business partners, architect, design, implement, and BI projects & Automations.\n\nResponsibilities:\n\nDesign, development and ongoing operations of scalable, performant data warehouse (Redshift) tables, data pipelines, reports and dashboards.\nDevelopment of moderately to highly complex data processing jobs using appropriate technologies (eg SQL, Python, Spark, AWS Lambda, etc)\nDevelopment of dashboards and reports.\nCollaborating with stakeholders to understand business domains, requirements, and expectations. Additionally, working with owners of data source systems to understand capabilities and limitations.\nDeliver minimally to moderately complex data analysis; collaborating as needed with Data Science as complexity increases.\nActively manage the timeline and deliverables of projects, anticipate risks and resolve issues.\nAdopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.\nInternal job description\n\nRetail Business Service, ARTS is a growing team that supports the Retail Efficiency and Paid Services business and tech teams. There is ample growth opportunity in this role for someone who exhibits Ownership and Insist on the Highest Standards, and has strong engineering and operational best practices experience.\n\nBasic qualifications:\n\n5+ years of relevant professional experience in business intelligence, analytics, statistics, data engineering, data science or related field.\nExperience with Data modeling, SQL, ETL, Data Warehousing and Data Lakes.\nStrong experience with engineering and operations best practices (version control, data quality/testing, monitoring, etc)\nExpert-level SQL.\nProficiency with one or more general purpose programming languages (eg Python, Java, Scala, etc)\nKnowledge of AWS products such as Redshift, Quicksight, and Lambda.\nExcellent verbal/written communication & data presentation skills, including ability to succinctly summarize key findings and effectively communicate with both business and technical teams.\n\nPreferred qualifications:\n\nExperience with data-specific programming languages/packages such as R or Python Pandas.\nExperience with AWS solutions such as EC2, DynamoDB, S3, and EMR.\nKnowledge of machine learning techniques and concepts. 3+ years of analyzing and interpreting data with Redshift, Oracle, NoSQL etc experience\nExperience with data visualization using Tableau, Quicksight, or similar tools\nExperience with data modeling, warehousing and building ETL pipelines\nExperience in Statistical Analysis packages such as R, SAS and Matlab\nExperience using SQL to pull data from a database or data warehouse and scripting experience (Python) to process data for modeling Experience with AWS solutions such as EC2, DynamoDB, S3, and Redshift\nExperience in data mining, ETL, etc and using databases in a business environment with large-scale, complex datasets",,,,"['SAS', 'Data modeling', 'Oracle', 'Business intelligence', 'MATLAB', 'Information technology', 'Analytics', 'SQL', 'Python']",2025-06-12 13:51:45
ETL Developer,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\n\nResponsibilities:\nDesign and implement the data modeling, data ingestion and data processing for various datasets\nDesign, develop and maintain ETL Framework for various new data source\nDevelop data ingestion using AWS Glue/ EMR, data pipeline using PySpark, Python and Databricks.\nBuild orchestration workflow using Airflow & databricks Job workflow\nDevelop and execute adhoc data ingestion to support business analytics.\nProactively interact with vendors for any questions and report the status accordingly\nExplore and evaluate the tools/service to support business requirement\nAbility to learn to create a data-driven culture and impactful data strategies.\nAptitude towards learning new technologies and solving complex problem.\nQualifications:\nMinimum of bachelors degree. Preferably in Computer Science, Information system, Information technology.\nMinimum 5 years of experience on cloud platforms such as AWS, Azure, GCP.\nMinimum 5 year of experience in Amazon Web Services like VPC, S3, EC2, Redshift, RDS, EMR, Athena, IAM, Glue, DMS, Data pipeline & API, Lambda, etc.\nMinimum of 5 years of experience in ETL and data engineering using Python, AWS Glue, AWS EMR /PySpark and Airflow for orchestration.\nMinimum 2 years of experience in Databricks including unity catalog, data engineering Job workflow orchestration and dashboard generation based on business requirements\nMinimum 5 years of experience in SQL, Python, and source control such as Bitbucket, CICD for code deployment.\nExperience in PostgreSQL, SQL Server, MySQL & Oracle databases.\nExperience in MPP such as AWS Redshift, AWS EMR, Databricks SQL warehouse & compute cluster.\nExperience in distributed programming with Python, Unix Scripting, MPP, RDBMS databases for data integration\nExperience building distributed high-performance systems using Spark/PySpark, AWS Glue and developing applications for loading/streaming data into Databricks SQL warehouse & Redshift.\nExperience in Agile methodology\nProven skills to write technical specifications for data extraction and good quality code.\nExperience with big data processing techniques using Sqoop, Spark, hive is additional plus\nExperience in data visualization tools including PowerBI, Tableau.\nNice to have experience in UI using Python Flask framework anglular\n\n\nMandatory Skills: Python for Insights. Experience: 5-8 Years.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ETL', 'data bricks', 'aws glue', 'amazon ec2', 'python', 'spark', 'glue', 'amazon redshift', 'cloud platforms', 'aws', 'data engineering', 'sql']",2025-06-12 13:51:48
Data Scientist,Ltimindtree,6 - 11 years,Not Disclosed,['Bengaluru'],"F2F Weekend Drive - Bangalore- 14th June - DS Gen AI\n\nJob description\n\nWe are having a F2F weekend drive for the requirement of a Data Scientist + Gen AI at our LTIM Bangalore Whitefield office.\nDate - 14th June 2025\nExperience - 6+ Years\nMandatory Skills - Data Science, Gen AI, Python, RAG and Azure/AWS, AI/ML, NLPt\n\nLocation - LTIMindtree Bangalore Whitefield Office\n\nSecondary - (Any) Machine Learning, Deep Learning, ChatGPT, Langchain, Prompt, vector stores, RAG, llama, Computer vision, Deep learning, Machine learning, OCR, Transformer, regression, forecasting, classification, hyper parameter tunning, MLOps, Inference, Model training, Model Deployment\nGeneric JD-\nMore than 6 years of experience in Data Engineering, Data Science and AI / ML domain\nExcellent understanding of machine learning techniques and algorithms, such as GPTs, CNN, RNN, k-NN, Naive Bayes, SVM, Decision Forests, etc.\nExperience using business intelligence tools (e.g. Tableau, PowerBI) and data frameworks (e.g. Hadoop)\nExperience in Cloud native skills.\nKnowledge of SQL and Python; familiarity with Scala, Java or C++ is an asset\nAnalytical mind and business acumen and Strong math skills (e.g. statistics, algebra)\nExperience with common data science toolkits, such as TensorFlow, KERAs, PyTorch, PANDAs, Microsoft CNTK, NumPy etc. Deep expertise in at least one of these is highly desirable.\nExperience with NLP, NLG and Large Language Models like BERT, LLaMa, LaMDA, GPT, BLOOM, PaLM, DALL-E, etc.\nGreat communication and presentation skills. Should have experience in working in a fast-paced team culture.\nExperience with AIML and Big Data technologies like AWS SageMaker, Azure Cognitive Services, Google Colab, Jupyter Notebook, Hadoop, PySpark, HIVE, AWS EMR etc.\nExperience with NoSQL databases, such as MongoDB, Cassandra, HBase, Vector databases\nGood understanding of applied statistics skills, such as distributions, statistical testing, regression, etc.\nShould be a data-oriented person with analytical mind and business acumen.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Artificial Intelligence', 'Machine Learning', 'Python']",2025-06-12 13:51:51
AI Engineer,HCLTech,10 - 14 years,Not Disclosed,['Noida'],"Seniority: Senior\nDescription & Requirements\nPosition Summary\nThe Senior AI Engineer with GenAI expertise is responsible for developing advanced technical solutions, integrating cutting-edge generative AI technologies. This role requires a deep understanding of modern technical and cloud-native practices, AI, DevOps, and machine learning technologies, particularly in generative models. You will support a wide range of customers through the Ideation to MVP journey, showcasing leadership and decision-making abilities while tackling complex challenges.",,,,"['AI engineering', 'VMware', 'Java', 'Azure', 'Data engineering', 'AI models', 'Node.js', 'NLP', 'Azure AKS', 'Machine Learning Operations', 'AWS', 'Kubernetes', 'Python']",2025-06-12 13:51:53
Data Analyst-Having Stratup-Mid-Size companies Exp.@ Bangalore_Urgent,"A leader in this space, we deliver world...",8 - 13 years,Not Disclosed,['Bengaluru'],"Data Analyst\n\nLocation: Bangalore\nExperience: 8 - 15 Yrs\nType: Full-time\n\nRole Overview\n\nWe are seeking a skilled Data Analyst to support our platform powering operational intelligence across airports and similar sectors. The ideal candidate will have experience working with time-series datasets and operational information to uncover trends, anomalies, and actionable insights. This role will work closely with data engineers, ML teams, and domain experts to turn raw data into meaningful intelligence for business and operations stakeholders.\n\nKey Responsibilities\n\nAnalyze time-series and sensor data from various sources\nDevelop and maintain dashboards, reports, and visualizations to communicate key metrics and trends.\nCorrelate data from multiple systems (vision, weather, flight schedules, etc) to provide holistic insights.\nCollaborate with AI/ML teams to support model validation and interpret AI-driven alerts (e.g., anomalies, intrusion detection).\nPrepare and clean datasets for analysis and modeling; ensure data quality and consistency.\nWork with stakeholders to understand reporting needs and deliver business-oriented outputs.\n\n\nQualifications & Required Skills\n\nBachelors or Masters degree in Data Science, Statistics, Computer Science, Engineering, or a related field.\n5+ years of experience in a data analyst role, ideally in a technical/industrial domain.\nStrong SQL skills and proficiency with BI/reporting tools (e.g., Power BI, Tableau, Grafana).\nHands-on experience analyzing structured and semi-structured data (JSON, CSV, time-series).\nProficiency in Python or R for data manipulation and exploratory analysis.\nUnderstanding of time-series databases or streaming data (e.g., InfluxDB, Kafka, Kinesis).\nSolid grasp of statistical analysis and anomaly detection methods.\nExperience working with data from industrial systems or large-scale physical infrastructure.\n\n\nGood-to-Have Skills\n\nDomain experience in airports, smart infrastructure, transportation, or logistics.\nFamiliarity with data platforms (Snowflake, BigQuery, Custom-built using open-source).\nExposure to tools like Airflow, Jupyter Notebooks and data quality frameworks.\nBasic understanding of AI/ML workflows and data preparation requirements.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Kafka', 'SQL', 'airports', 'InfluxDB', 'Airflow', 'structured Data', 'time-series', 'JSON', 'Tableau', 'Grafana', 'R', 'AI/ML', 'Kinesis', 'Snowflake', 'time-series databases', 'Data Preparation', 'Python', 'smart infrastructure', 'BigQuery', 'streaming data', 'Power BI', 'CSV', 'transportation', 'logistic', 'reporting tools']",2025-06-12 13:51:55
"Senior Engineer, Application Development",S&P Global Market Intelligence,5 - 8 years,Not Disclosed,['Hyderabad'],"Grade Level (for internal use):\n10\nMarket Intelligence\nThe Role: Senior Full Stack Developer\nGrade level :10\nThe Team: You will work with a team of intelligent, ambitious, and hard-working software professionals. The team is responsible for the architecture, design, development, quality, and maintenance of the next-generation financial data web platform. Other responsibilities include transforming product requirements into technical design and implementation. You will be expected to participate in the design review process, write high-quality code, and work with a dedicated team of QA Analysts, and Infrastructure Teams\nThe Impact: Market Intelligence is seeking a Software Developer to create software design, development, and maintenance for data processing applications. This person would be part of a development team that manages and supports the internal & external applications that is supporting the business portfolio. This role expects a candidate to handle any data processing, big data application development. We have teams made up of people that learn how to work effectively together while working with the larger group of developers on our\nplatform.\nWhats in it for you:\nOpportunity to contribute to the development of a world-class Platform Engineering team .\nEngage in a highly technical, hands-on role designed to elevate team capabilities and foster continuous skill enhancement.\nBe part of a fast-paced, agile environment that processes massive volumes of dataideal for advancing your software development and data engineering expertise while working with a modern tech stack.\nContribute to the development and support of Tier-1, business-critical applications that are central to operations.\nGain exposure to and work with cutting-edge technologies including AWS Cloud , EMR and Apache NiFi .\nGrow your career within a globally distributed team , with clear opportunities for advancement and skill development.\nResponsibilities:\nDesign and develop applications, components, and common services based on development models, languages and tools, including unit testing, performance testing and monitoring and implementation\nSupport business and technology teams as necessary during design, development and delivery to ensure scalable and robust solutions\nBuild data-intensive applications and services to support and enhance fundamental financials in appropriate technologies.( C#, .Net Core, Databricsk, Spark ,Python, Scala, NIFI , SQL)\nBuild data modeling, achieve performance tuning and apply data architecture concepts\nDevelop applications adhering to secure coding practices and industry-standard coding guidelines, ensuring compliance with security best practices (e.g., OWASP) and internal governance policies.\nImplement and maintain CI/CD pipelines to streamline build, test, and deployment processes; develop comprehensive unit test cases and ensure code quality\nProvide operations support to resolve issues proactively and with utmost urgency\nEffectively manage time and multiple tasks\nCommunicate effectively, especially written with the business and other technical groups\nWhat Were Looking For:\nBasic Qualifications:\nBachelorsMasters Degree in Computer Science, Information Systems or equivalent.\nMinimum 5 to 8 years of strong hand-development experience in C#, .Net Core, Cloud Native, MS SQL Server backend development. Proficiency with Object Oriented Programming.\nAdvance SQL programming skills\nPreferred experience or familiarity with tools and technologies such as Odata, Grafana, Kibana, Big Data platforms, Apache Kafka, GitHub, AWS EMR, Terraform, and emerging areas like AI/ML and GitHub Copilot.\nHighly recommended skillset in Databricks, SPARK, Scalatechnologies.\nUnderstanding of database performance tuning in large datasets\nAbility to manage multiple priorities efficiently and effectively within specific timeframes\nExcellent logical, analytical and communication skills are essential, with strong verbal and writing proficiencies\nKnowledge of Fundamentals, or financial industry highly preferred.\nExperience in conducting application design and code reviews\nProficiency with following technologies:\nObject-oriented programming\nPrograming Languages (C#, .Net Core)\nCloud Computing\nDatabase systems (SQL, MS SQL)\nNice to have: No-SQL (Databricks, Spark, Scala, python), Scripting (Bash, Scala, Perl, Powershell)\nPreferred Qualifications:\nHands-on experience with cloud computing platforms including AWS , Azure , or Google Cloud Platform (GCP) .\nProficient in working with Snowflake and Databricks for cloud-based data analytics and processing.\nBenefits:\nHealth & Wellness: Health care coverage designed for the mind and body.\nContinuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\nInvest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\nFamily Friendly Perks: Its not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\nBeyond the Basics: From retail discounts to referral incentive awardssmall perks can make a big difference.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GitHub Copilot', 'AI/ML', 'Kibana', 'python', 'GitHub', 'Scala', 'AWS EMR', 'Grafana', 'Odata', 'Big Data platforms', 'Terraform', 'Apache Kafka', 'Databricks', 'Spark']",2025-06-12 13:51:58
Senior Software Engineer - Python Developer,FactSet,5 - 10 years,Not Disclosed,['Hyderabad'],"FactSet creates flexible, open data and software solutions for over 200,000 investment professionals worldwide, providing instant access to financial data and analytics that investors use to make crucial decisions.\nAt FactSet, our values are the foundation of everything we do. They express how we act and operate , serve as a compass in our decision-making, and play a big role in how we treat each other, our clients, and our communities. We believe that the best ideas can come from anyone, anywhere, at any time, and that curiosity is the key to anticipating our clients needs and exceeding their expectations.",,,,"['Computer science', 'C++', 'Data analysis', 'GCP', 'Analytical', 'Machine learning', 'Technical leadership', 'Monitoring', 'SQL', 'Python']",2025-06-12 13:52:00
Senior Software Engineer,Dynamic Yield,5 - 8 years,Not Disclosed,['Pune'],"Our Purpose\nTitle and Summary\nSenior Software Engineer\nWho is Mastercard?\nMastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.\nOur decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.\n\nOur Team:\n\nThe AI & DPE team is responsible for product management and innovative product development of products and services to address the evolving risk and security needs of all of Mastercard s various customer segments. Platform Services team in AI & DPE focuses on defining the strategic direction for underlying platforms to enable the successful implementation of real-time, data-driven innovative products and services focused on network, security, fraud, digital identity and authentication. The team is responsible to look across all the products/services across AI & DPE and drive efficiency, re-usability and increase speed to market for our products and services.\n\nThe candidate for this position will focus on Data Unification across different data assets, enabling a single unified view of data from multiple sources and support the development of new innovative data driven cyber products, services and actionable insights.\n\nThe Role:\nWe are seeking a Senior Software Engineer who will:\n\nPerform data ingestion, aggregation, processing to drive and enable relevant insights from available data sets.\nPartner with various teams (i.e., Product Manager, Data Science, Platform Strategy, Technology) on data needs/requirements in order to deliver data solutions that generate business value.\nManipulate and analyze complex, high-volume, high-dimensionality data from varying sources using a variety of tools and data analysis techniques.\nIdentify innovative ideas and deliver proof of concepts, prototypes to deliver against the existing and future needs and propose new products, services and enhancements.\nIntegrate & Unify new data assets which increase the value proposition for our customers and enhance our existing solutions and services.\nAnalyse large volumes of transaction and product data to generate insights and actionable recommendations to drive business growth\nCollect and synthesize feedback from clients, development, product and sales teams for new solutions or product enhancements.\nApply knowledge of metrics, measurements, and benchmarking to complex and demanding solutions.\n\nAll about You\nMinimum 5-8 years of relevant experience.\nGood understanding of programming language preferably PySpark and Big Data technologies.\nExperience with Enterprise Business Intelligence Platform/Data platform.\nStrong SQL and higher-level programming languages with solid knowledge of data mining, machine learning algorithms and tools\nExperience with data integration tools - ETL/ELT tools (i.e. Apache NiFi, Azure Data Factory, Databricks)\nExposure to collecting and/or working with data including standardizing, summarizing, offering initial observations and highlighting inconsistencies.\nStrong understanding of the application of analytical methods and data visualization to support business decisions.\nAbility to understand complex operational systems and analytics/business intelligence tools for the delivery of information products and analytical offerings to a large, global user base.\nAble to work in a fast-paced, deadline-driven environment as part of a team and as an individual contributor\nAbility to easily move between business, analytical, and technical teams and articulate solution requirements for each group",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Data analysis', 'Information security', 'Analytical', 'Network security', 'Data mining', 'Business intelligence', 'Operations', 'Analytics', 'SQL']",2025-06-12 13:52:02
MDM Associate Data Steward,Amgen Inc,0 - 3 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\n\nRole Description\n\nWe are seeking an MDM Associate Data Steward who will be responsible for ensuring the accuracy, completeness, and reliability of master data across critical business domains such as Customer, Product, Affiliations, and Payer. This role involves actively managing and curating master data through robust data stewardship processes, comprehensive data cataloging, and data governance frameworks utilizing Informatica or Reltio MDM platforms. Additionally, the incumbent will perform advanced data analysis, data validation, and data transformation tasks through SQL queries and Python scripts to enable informed, data-driven business decisions. The role emphasizes cross-functional collaboration with various teams, including Data Engineering, Commercial, Medical, Compliance, and IT, to align data management activities with organizational goals and compliance standards.\n\nRoles & Responsibilities\nResponsible for master data stewardship, ensuring data accuracy and integrity across key master data domains (e.g., Customer, Product, Affiliations).\nConduct advanced data profiling, cataloging, and reconciliation activities using Informatica or Reltio MDM platforms.\nManage the reconciliation of potential matches, ensuring accurate resolution of data discrepancies and preventing duplicate data entries.\nEffectively manage Data Change Request (DCR) processes, including reviewing, approving, and documenting data updates in compliance with established procedures and SLAs.\nExecute and optimize SQL queries for validation and analysis of master data.\nPerform basic Python for data transformation, quality checks, and automation.\nCollaborate effectively with cross-functional teams including Data Engineering, Commercial, Medical, Compliance, and IT to fulfill data requirements.\nSupport user acceptance testing (UAT) and system integration tests for MDM related system updates.\nImplement data governance processes ensuring compliance with enterprise standards, policies, and frameworks.\nDocument and maintain accurate SOPs, Data Catalogs, Playbooks, and SLAs.\nIdentify and implement process improvements to enhance data stewardship and analytic capabilities.\nPerform regular audits and monitoring to maintain high data quality and integrity.\nBasic Qualifications and Experience\nMasters degree with 1 - 3 years of experience in Business, Engineering, IT or related fieldOR\nBachelors degree with 2 - 5 years of experience in Business, Engineering, IT or related fieldOR\nDiploma with 6 - 8 years of experience in Business, Engineering, IT or related field\nFunctional\n\nSkills:\nMust-Have Skills:\nDirect experience in data stewardship, data profiling, and master data management.\nHands-on experience with Informatica or Reltio MDM platforms.\nProficiency in SQL for data analysis and querying.\nKnowledge of data cataloging techniques and tools.\nBasic proficiency in Python scripting for data processing.\nGood-to-Have\n\nSkills:\nExperience with PySpark and Databricks for large-scale data processing.\nBackground in the pharmaceutical, healthcare, or life sciences industries.\nFamiliarity with AWS or other cloud-based data solutions.\nStrong project management and agile workflow familiarity (e.g., using Jira, Confluence).\nUnderstanding of regulatory compliance related to data protection (GDPR, CCPA).\nProfessional Certifications\nAny ETL certification ( e.g. Informatica)\nAny Data Analysis certification (SQL)\nAny cloud certification (AWS or AZURE)\nSoft\n\nSkills:\nStrong analytical abilities to assess and improve master data processes and solutions.\nExcellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders.\nEffective problem-solving skills to address data-related issues and implement scalable solutions.\nAbility to work effectively with global, virtual teams",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data management', 'python', 'project management', 'data analysis', 'data stewardship', 'agile database', 'data processing', 'sql', 'data profiling']",2025-06-12 13:52:05
Data & Analytics Subject Matter Expert,Trianz,10 - 15 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Role Overview\nWe are looking for a Data & Analytics Subject Matter Expert with deep expertise in Data Engineering, Business Intelligence (BI), and AWS cloud ecosystems . This role demands strategic thinking, hands-on execution, and collaboration across technical and business teams to deliver impactful data-driven solutions.\nKey Responsibilities\n1. Data Architecture & Engineering\nDesign and implement scalable, high-performance data solutions on AWS.\nBuild robust data pipelines, ETL/ELT workflows, and data lake architectures.\nEnforce data quality, security, and governance practices.\n2. Business Intelligence & Insights\nDevelop interactive dashboards and visualizations using Power BI, Tableau, or QuickSight.\nDefine data models and KPIs to support data-driven decision-making.\nCollaborate with business teams to extract insights that drive action.\n3. Cloud & Advanced Analytics\nDeploy data warehousing solutions using Redshift, Glue, S3, Athena, and other AWS services.\nOptimize storage and processing strategies for performance and cost-efficiency.\nExplore AI/ML integrations for predictive and advanced analytics (preferred).\n4. Collaboration & Best Practices\nPartner with cross-functional teams (engineering, data science, business) to align on data needs.\nChampion best practices in data governance, compliance, and architecture.\nTranslate business requirements into scalable technical solutions.\nRequired Qualifications\nEducation\nBachelor s or Master s in Computer Science, Information Technology, Data Science, or related discipline.\nExperience\n10+ years of experience in data engineering, BI, and analytics domains.\nProven experience with AWS data tools and modern data architectures.\nTechnical Skills\nStrong command of AWS services: Redshift, Glue, S3, Athena, Lambda, Kinesis.\nProficient in SQL, Python, or Scala.\nExperience building and maintaining ETL/ELT workflows and data models.\nExpertise in BI tools like Power BI, Tableau, QuickSight, or Looker.\nFamiliarity with AI/ML models and frameworks is a plus.\nCertifications\nPreferred: AWS Certified Data Analytics - Specialty.\nAdditional certifications in AWS, data engineering, or analytics are a plus.\nWhy Join Trianz\nJoin a high-growth, innovation-led firm delivering transformation at scale.\nCollaborate with global teams on cutting-edge cloud and analytics projects.\nEnjoy a competitive compensation structure and clear career progression pathways.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['power bi', 'Data analytics', 'Subject Matter Expert', 'Business intelligence', 'AWS', 'Information technology', 'Analytics', 'SQL', 'Data architecture', 'Python']",2025-06-12 13:52:07
Data Architecture,Top B2B MNC in Management Consulting Dom...,5 - 8 years,Not Disclosed,['Bengaluru'],"About the Company\nGreetings from Teamware Solutions a division of Quantum Leap Consulting Pvt. Ltd\n\nAbout the Role\nWe are hiring a Data Architecture\n\nLocation: Bangalore\nWork Model: Hybrid\nExperience: 5-9 Years\nNotice Period: Immediate to 15 Days\n\nJob Description:\nData Architecture, Data Governance, Data Modeling\n\nAdditional Information:\nMandatory Skills: Data Architecture, Data Governance, Data Modeling\nNice to have skills Certification in Data Engineering\nInterview Mode Virtual Interview\nminimum 5 yrs relevant experience and maximum 9 yrs for this requirement. Someone with more experience in building PySpark data streaming jobs on Azure Databricks\nwho have done real projects, have expertise, and hands-on experience also\nAlso, Data governance and data modeling experience with a minimum of 4 years is mandatory\nCommunication should be excellent\n\n\nPlease let me know if you are interested in this position and send me your resumes to netra.s@twsol.com",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Architecture', 'Data Modeling', 'Data Governance', 'Data Engineering']",2025-06-12 13:52:09
Senior Software Engineer,Mastercard,5 - 8 years,Not Disclosed,['Pune'],"Senior Software Engineer\n?\n\n\n\n\nThe AI & DPE team is responsible for product management and innovative product development of products and services to address the evolving risk and security needs of all of Mastercard s various customer segments. Platform Services team in AI & DPE focuses on defining the strategic direction for underlying platforms to enable the successful implementation of real-time, data-driven innovative products and services focused on network, security, fraud, digital identity and authentication. The team is responsible to look across all the products/services across AI & DPE and drive efficiency, re-usability and increase speed to market for our products and services.\n\nThe candidate for this position will focus on Data Unification across different data assets, enabling a single unified view of data from multiple sources and support the development of new innovative data driven cyber products, services and actionable insights.\n\nThe Role:\nWe are seeking a Senior Software Engineer who will:\n\nPerform data ingestion, aggregation, processing to drive and enable relevant insights from available data sets.\nPartner with various teams (i.e., Product Manager, Data Science, Platform Strategy, Technology) on data needs/requirements in order to deliver data solutions that generate business value.\nManipulate and analyze complex, high-volume, high-dimensionality data from varying sources using a variety of tools and data analysis techniques.\nIdentify innovative ideas and deliver proof of concepts, prototypes to deliver against the existing and future needs and propose new products, services and enhancements.\nIntegrate & Unify new data assets which increase the value proposition for our customers and enhance our existing solutions and services.\nAnalyse large volumes of transaction and product data to generate insights and actionable recommendations to drive business growth\nCollect and synthesize feedback from clients, development, product and sales teams for new solutions or product enhancements.\nApply knowledge of metrics, measurements, and benchmarking to complex and demanding solutions.\n\nAll about You\nMinimum 5-8 years of relevant experience.\nGood understanding of programming language preferably PySpark and Big Data technologies.\nExperience with Enterprise Business Intelligence Platform/Data platform.\nStrong SQL and higher-level programming languages with solid knowledge of data mining, machine learning algorithms and tools\nExperience with data integration tools - ETL/ELT tools (i.e. Apache NiFi, Azure Data Factory, Databricks)\nExposure to collecting and/or working with data including standardizing, summarizing, offering initial observations and highlighting inconsistencies.\nStrong understanding of the application of analytical methods and data visualization to support business decisions.\nAbility to understand complex operational systems and analytics/business intelligence tools for the delivery of information products and analytical offerings to a large, global user base.\nAble to work in a fast-paced, deadline-driven environment as part of a team and as an individual contributor\nAbility to easily move between business, analytical, and technical teams and articulate solution requirements for each group",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Data analysis', 'Information security', 'Analytical', 'Network security', 'Data mining', 'Business intelligence', 'Operations', 'Analytics', 'SQL']",2025-06-12 13:52:11
Data Architect - AWS,Happiest Minds Technologies,10 - 15 years,Not Disclosed,"['Noida', 'Pune', 'Bengaluru']","Roles and responsibilities\nWork closely with the Product Owners and stake holders to design the Technical Architecture for data platform to meet the requirements of the proposed solution.\nWork with the leadership to set the standards for software engineering practices within the machine learning engineering team and support across other disciplines\nPlay an active role in leading team meetings and workshops with clients.\nChoose and use the right analytical libraries, programming languages, and frameworks for each task.",,,,"['SQL', 'data architect', 'Python', 'Pyspark', 'Apache Airflow', 'GLUE', 'Kinesis', 'Amazon Redshift', 'Data Architecture Principles', 'Data Modeling', 'Data Warehousing', 'Athena', 'Lambda', 'AWS']",2025-06-12 13:52:13
Senior Software Quality Engineer,Mastercard,4 - 9 years,Not Disclosed,['Pune'],"Senior Software Quality Engineer\n?\n\nMastercard is a global technology company in the payments industry. We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.\n\n\n\nOverview:\n\nTransfer Solutions is responsible for driving Mastercard s expansion into new payment flows such as Disbursements & Remittances. The team is working on creating a market-leading money transfer proposition, Mastercard Move, to power the next generation of payments between people and businesses, whether money is moving domestically or across borders, by delivering the ability to pay and get paid with choice, transparency, and flexibility.\n\nThe Product & Engineering teams within Transfer Solutions are responsible for designing, developing, launching, and maintaining products and services designed to capture these flows from a wide range of customer segments. By addressing customer pain points for domestic and cross-border transfers, the goal is to scale Mastercard s Disbursements & Remittances business, trebling volume over the next 4 years.\n\nThe Role:\narticipate in requirements discussion, test planning, test data creation and execution of testing Plan in adherence with MasterCard standards, processes and best practices.\nWork with project teams to meet scheduled due dates, while identifying emerging issues and recommending solutions for problems and independently perform assigned tasks.\nDesign and develop test automation frameworks to validate system to system interfaces and complete software solutions (for Database/ETL, API and UI tests)\nInteract with business and development stakeholders to define test plans and schedules\nTranslate complex system requirements into test requirements and testing methods\nIdentify and implement complex automation efforts, including refactoring of automation code where needed\nDevelop test scripts and perform automated and manual exploratory testing to ensure software meets business and security requirements and established practices.\nDesign and develop test data management for defined test cases, recognize test environment preparation needs, and execute existing test plans and report results\nOwn responsibility for defect management and oversight and escalation of issues discovered during the testing phase\nDocument as per Software Development Best Practices and follow MasterCard Quality Assurance and Quality Control processes.\nDocument performance test strategies and test plans, and execute performance validation\nCollect quality metric data and communicate test status/risks to stakeholders\nAct as first-review for project-level reviews, walkthroughs and inspections\nProvide technical support and mentoring to junior team members\nPerform demos of new product functionality to stakeholders\nDevelop business and product knowledge over time.\nIdentify opportunities to improve effectiveness and time-to-market\nProvide training and guidance to team members on quality best practices and principles\nFacilitate knowledge sharing sessions to promote a culture of quality awareness\nBe a strong individual contributor to the implementation efforts of product solutions\n\nAll About You:\n\nBachelors degree in Information Technology, Computer Science or Management Information Systems or equivalent work experience\n8+ years of experience in the Software Engineering with a focus on Quality Engineering methodologies\nTechnical skills in Java, Selenium, Cucumber, Soap UI, Spring framework, REST, JSON, Eclipse, GIT, Jmeter/Blazemeter\nExcellent SQL skills to work on large and complex data sources and capability of comprehending and writing complex queries\nExperience testing APIs (REST and SOAP), web user interface, and/or reports\nExperience in implementing CI/CD build pipelines with tools like Git/Bit Bucket, Jenkins and Maven\nSuccessfully validated one or more application codebases via automation, for new feature functionality and regression testing\nExperience working in Agile teams and conversant with Agile/SAFe tenets and ceremonies. Strong analytical and problem-solving abilities, with quick adaptation to new technologies, methodologies, and systems\nExcellent English communication skills (both written and verbal) to effectively interact with multiple technical teams and other stakeholders\nHigh-energy, detail-oriented and proactive, with ability to function under pressure in an independent environment along with a high degree of initiative and self-motivation to drive results\nEager to experiment with new team processes and innovate on testing approach\nPrior experience with Data Analysis and Data Engineering is a plus\nStrong collaboration skills and ability to work effectively in a cross-functional, interdependent team environment",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Maven', 'Manager Quality Assurance', 'Eclipse', 'Information security', 'Agile', 'JSON', 'Selenium', 'Information technology', 'Technical support', 'SQL']",2025-06-12 13:52:16
HIH - Data Science Lead Analyst - Evernorth,Cigna Medical Group,5 - 8 years,Not Disclosed,['Hyderabad'],"Role Summary\nAs a member of the Data Science Center of Expertise (DSCOE), the DS Lead Analyst is responsible for leading and enabling Data Science within Cigna Group with demonstrable aptitude in Data Science (i) Technical Skills (ii) Leadership (iii) Scope & Impact (iv) Influence. Please see Qualifications section below for more details.\n\nThe role will support the development and maintenance of machine learning models, with a focus on ensuring that models meet Cigna requirements for governance and legal compliance. The role will require collaboration with other data scientists and involve work across many lines of business.\nKey Responsibilities:\nAnalyze model performance of new models with specific regards to requirements for legal compliance and governance standards around accuracy and bias;\nPerform periodic analyses of performance of existing models to ensure continued compliance with internal and external standards for accuracy and bias;\nConduct research (i.e. literature review) to understand when bias may be biologically or medically justifiable, and to what degree, for example: finding evidence from literature that heart disease is more prevalent among older populations\nUsing machine learning development tools to mitigate model bias when this is determined to be necessary\nCollaborating with data scientists, business stakeholders, and governance/compliance teams to ensure models meet compliance and governance standards\nQualifications:\nBachelors or Masters/PhD (preferred) in statistics or computer science or equivalent field with 5-8 years of relevant experience\nStrong proficiency in ML, statistics, python or R, SQL, version control (e.g., Git), health care data (e.g., claims, EHR)\nAbility to promote best coding practices, championing a culture of documentation/logging\nThorough understanding of ML lifecycle, including necessary tradeoffs and associated risks\nLeadership in Data Science\nCan own a project end-to-end e.g., scoping, business value estimation, ideation, dev, prod, timeline\nCollaborates and guides junior team members in completion of projects and career development\nWorks cross functionally with technical (e.g., Data Science, Data Engineering) and business (e.g., clinical, marketing, pricing, business analysts) to implement solutions with measurable value\nScope and Impact\nIndependently delivers clear and well-developed presentations for both technical and business audiences\nCreates data science specific project goals associated with project deliverables\nArticulates timeline changes, rationale, and goals to meet deadlines moving forward\nValues diversity, growth mindset, and improving health outcomes of our customers\n\nLevel of Influence\nCommunicate with stakeholders to identify opportunities and possible solutions based on business need\nDraft project charter, timeline, and features/stories\nInfluence matrix-partner leadership",Industry Type: Medical Services / Hospital,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Version control', 'GIT', 'Claims', 'data science', 'Legal compliance', 'Coding', 'Machine learning', 'SQL', 'Python']",2025-06-12 13:52:18
Data Quality Analyst,Yallas Technology Solutions Opc,5 - 10 years,Not Disclosed,[],"Title: Data Quality Analyst/Developer\nDuration: 6 months to 1 year contract\nLocation:  Remote\nNotice period - Immediate to 7 days\nUAN /EPFO Report Required\n\nWork Experience:\n5 + years of this experience - Experience doing Data Emendation\nDesign/Develop Rules, monitoring mechanisms, notification\nDesign/Develop UI, Workflows, security\nDesign/Develop analytics (overall DQ reporting, usage statistics, etc).\nDesign/Develop migration activities to migrate existing DQ assets between our existing DQ platform and new DQ platform.\nDesign integration with MDM & Catalog (as needed)\nMonitor system performance and suggest optimization strategies (as needed).\nWork with DT to maintain system - patches, backups, etc.\nWork with LYB's Data Stewards to support their governance activities.\nTesting\n\nThe DQ Analyst/Developer should have experience with IMDC (for the sake of our example) cloud DQ and observability, JSON (depending on tool) Deep SQL skills, Integration tools/methodologies - API as well as ETL, Data Analysis, Snowflake or Databricks knowledge (for lineage), Power BI (nice to have), SAP ECC knowledge (nice to have), experience with cloud platforms (Azure, AWS, Google).\nIf you are interested please share required details along with resume\nFull Name:\nCurrent or Previous organization:\nCurrent Location:\nTotal Experience:\nRelevant experience as Python Developer:\nhow many years of experience In Azure, AWS, Google\nHow many years of experience in UI, Workflows, security\nWorking as full time or contract:\nReason for job change:\nAny other offers inhand:\nCurrent CTC:\nexpected CTC:\nNotice Period:\nemail id:\ncontact Number :\nDomain name:\nare you ok to work Cotractual role?:\nshare your aadhar or pan card for the verification",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Data quality analyst', 'cloud data quality', 'Azure', 'data quality developer', 'JSON', 'google', 'Informatica', 'AWS']",2025-06-12 13:52:20
Master Data Management Architect,K-logix Partnering Solutions,8 - 13 years,Not Disclosed,[],"Bachelor'sMaster's\nOverview:\n\nWe are seeking a highly skilled and experienced Celonis MDM Data Architect to lead the design, implementation, and optimization of our Master Data Management (MDM) solutions in alignment with Celonis Process Mining and Execution Management System (EMS) capabilities.\nThe ideal candidate will play a key role in bridging data architecture and business process insights, ensuring data quality, consistency, and governance across the enterprise.\n\nKey Responsibilities:\nDesign and implement MDM architecture and data models aligned with enterprise standards and best practices.\n• Lead the integration of Celonis with MDM platforms to drive intelligent process automation, data governance, and operational efficiencies.\n• Collaborate with business stakeholders and data stewards to define MDM policies, rules, and processes.\n• Support data profiling, data cleansing, and data harmonization efforts to improve master data quality.\n• Work closely with Celonis analysts, data engineers, and process owners to deliver actionable insights based on MDM-aligned process data.\n• Develop and maintain scalable, secure, and high-performance data pipelines and integration architectures.\n• Translate business requirements into technical solutions, ensuring alignment with both MDM and Celonis data models.\n• Create and maintain data architecture documentation, data dictionaries, and metadata repositories.\n• Monitor and optimize the performance of MDM systems and Celonis EMS integrations.\nQualifications:\nBachelors or Masters degree in Computer Science, Information Systems, Data Engineering, or a related field.\n• 7+ years of experience in data architecture, MDM, or enterprise data management.\n• 2+ years of hands-on experience with Celonis and process mining tools.\n• Proficient in MDM platforms (e.g., Informatica MDM, SAP MDG, Oracle MDM, etc.).\n• Strong knowledge of data modeling, data governance, and metadata management.\n• Proficiency in SQL, data integration tools (e.g., ETL/ELT platforms), and APIs.\n• Deep understanding of business process management and data-driven transformation initiatives.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Celonis', 'MDM', 'Master Data Management', 'ETL', 'Elt']",2025-06-12 13:52:22
Clinical Data Programming Lead,ICON plc,5 - 10 years,Not Disclosed,['Chennai'],"Clinical Data Programming Lead (SAS + EDC) - Bangalore/Chennai (Hybrid)\nThe Clinical Data Programming Lead role is part of the Investigator Payments Group (IPG) and will be involved in the programming, delivery and oversight of data integration solutions between the Electronic Data Capture (EDC) system, the Clinical Trial Management System (CTMS) and our payment entitlement calculation system (APECS).\nThe Clinical Data Programming Lead provides support to and acts as a back-up for the IPG Manager. To effectively assist the IPG Manager in leading the activities for those under his/her jurisdiction in a manner that ensures all timeframes and targets are met.",,,,"['Assurance', 'Manager Quality Assurance', 'EDC', 'System integration', 'clinical development', 'Clinical research', 'Healthcare', 'SAS Programming', 'Monitoring', 'clinical data']",2025-06-12 13:52:24
Data Scientist,Ltimindtree,7 - 12 years,Not Disclosed,['Hyderabad'],Data Scientist\n\nJob Description\n\nResponsibilities\n\nWork with team members across multiple disciplines to understand the data behind product features user behaviors the security landscape and our goals\nAnalyze data from several large sources then automate solutions using scheduled processes models and alerts\nWork with partners to design and improve metrics that guide our decisions for the product\nDetect patterns associated with fraudulent accounts and anomalous behavior\nSolve scientific problems and create new methods independently\nTranslate requirements and security questions into data insights\nSet up alerting mechanisms so our leadership is always aware of the security posture\n\nQualifications\n\nPostgraduate degree with specialization in machine learning artificial intelligence statistics or related fields or 2 years of equivalent work experience in applied machine learning and analytics\nExperience with SQL Snowflake and NoSQL databases\nProficiency in Python programming\nFamiliarity with statistics modeling and data visualization\n\nExperience\n\nExperience building statistical and machine learning models applying techniques such as regression classification clustering and anomaly detection Time series and Classical ML modeling\nFamiliarity with Snowflake SQL\nFamiliarity with cloud platforms such as AWS\nSome experience to software development or data engineering\nAnalyze business problems or research questions identify relevant data points and extract meaningful insights,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Snowflake Sql', 'AWS']",2025-06-12 13:52:27
Technical Specialist - Data Scientist,Fidelity International,8 - 9 years,Not Disclosed,['Gurugram'],"Application Deadline: 21 June 2025\nTitle Senior Analyst- Data Scientist\nDepartment Data Value\nLocation Gurgaon\nReports To Suman Kaur\nLevel 3\nWe re proud to have been helping our clients build better financial futures for over 50 years. How have we achieved this? By working together - and supporting each other - all over the world. So, join our Data Value team and feel like you re part of something bigger.\nAbout your team\nData Value team drives the renewed focus of extracting value from Fidelity s data for business and client insights and working as one voice with the business, technology, and data teams. The team s vision is to create measurable business impact by leveraging technology and utilising the skills to generate valuable insights and streamline engagements. The Data Science function within Data Value supports Fidelity International s Sales, Marketing, Propositions, Risk, Finance, Customer Service and HR teams across the globe. The key objectives of the function are:\nTo develop deep customer insights for our businesses helping them segment and target customers more effectively\nTo develop a fact-based understanding of sales trends and identify actionable sales growth opportunities for each of our sales channels\nTo understand customer preferences in terms of products, service attributes and marketing activity to help refine each of these\nTo help develop new services lines e.g. develop customer analytics for key IFAs, DC Clients, Individual clients etc.\nTo develop market and competitive intelligence in our key markets to help shape our business planning in those markets\nThe function works directly with business heads and other senior stakeholder s stakeholders to identify areas of analytics, define problem statements and develop key insights.\nAbout your role\nYou will be expected to take a leading role in developing the Data Science and Advanced Analytics solutions for our business. This will involve:\nEngaging with the key stakeholders to understand Fidelity s sales, marketing, client services and propositions context\nImplement advanced analytics solutions on On-Premises/Cloud platforms, develop proof-of-concepts and engage with internal and external ecosystem to progress the proof of concepts to production.\nEngaging and collaborating with different other internal teams like Data engineering, DevOps, technology team etc for development of new tools, capabilities, and solutions.\nMaximize Adoption of Cloud Based advanced analytics solutions: Build out sandbox analytics environments using Snowflake, AWS, Adobe, Salesforce.\nAbout you\nKey Responsibilities\nDeveloping and Delivering Data Science solutions for business (40%)\nPartner with internal (FIL teams) & external ecosystem to design and deliver advanced analytics enabled Data Science solutions\nCreate advanced analytics solution on quantitative and text data using Artificial Intelligence, Machine Learning and NLP techniques.\nCreate compelling visualisations that enable the smooth consumption of predictions and insights for customer benefit\n. Stakeholder Management (30%)\nWorks with channel heads/stakeholders and other sponsors understand the business problem and translate it into appropriate analytics solution.\nEngages with key stakeholders for smooth execution, delivery, and implementation of solutions\nAdoption of Cloud enabled Data Science solutions: (20%)\nMaximize Adoption of Cloud Based advanced analytics solution\nBuild out sandbox analytics environments using Snowflake, AWS, Adobe, Salesforce\nDeploy solutions in productions while adhering to best practices involving Model Explainability, MLOps, Feature Stores, Model Management, Responsible AI etc\nCollaboration and Ownership (10%)\nSharing of knowledge, best practices with the team including coaching or training in some of deep learning/machine learning methodologies. Provides mentoring, coaching, and consulting advice and guidance to staff, e.g. analytic methodologies, data recommendations\nTakes complete independent ownership of the projects and the initiatives in the team with the minimal support\nExperience and Qualifications Required\nQualifications:\nEngineer from IIT/Master s in field related to Data Science/Economics/Mathematics (Tie1 Institutions like ISI, Delhi School of Economics)/M.B.A from tier 1 institutions\nMust have Skills & Experience Required:\nOverall, 8+ years of experience in Data Science and Analytics\n5+ years of hands-on experience in - Statistical Modelling /Machine Learning Techniques/Natural Language Processing/Deep Learning\n5+ years of experience in Python/Machine Learning/Deep Learning\nExcellent problem-solving skills\nShould be able to run analytics applications such as Python, SAS and interpret statistical results\nImplementation of models with clear measurable outcomes\nGood to have Skills & Experience Required:\nAbility to engage in discussion with senior stakeholders on defining business problems, designing analyses projects, and articulating analytical insights to stakeholders.\nExperience on SPARK/Hadoop/Big Data Platforms is a plus\nExperience with unstructured data and big data\nExperience with secondary data and knowledge of primary market research is a plus.\nAbility to independently own and manage the projects with minimal support.\nExcellent analytical skills and a strong sense for structure and logic\nAbility to develop, test and validate hypotheses.\nFeel rewarded",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SAS', 'Senior Analyst', 'Consulting', 'Machine learning', 'Business planning', 'Competitive intelligence', 'Customer service', 'Adobe', 'Stakeholder management', 'Salesforce']",2025-06-12 13:52:29
Full Stack Data Scientist,Vimo Getinsured,2 - 7 years,Not Disclosed,['Gurugram( Sector 61 Gurgaon )'],"About the Role\nAs a Data Science Engineer, you will need strong technical skills in data modeling, machine learning, data engineering, and software development. You will have the ability to conduct literature reviews and critically evaluate research papers to identify applicable techniques. Additionally, you should be able to design and implement efficient and scalable data processing pipelines, perform exploratory data analysis, and collaborate with other teams to integrate data science models into production systems. Passion for conversational AI and a desire to solve some of the most complex problems in the Natural Language Processing space are essential. You will work on highly scalable, stable, and automated deployments, aiming for high performance. Taking on the challenge of building and scaling a truly remarkable AI platform to impact the lives of millions of customers will be part of your responsibilities. Working in a challenging yet enjoyable environment, where learning new things is the norm, you should think of solutions beyond boundaries. You should also drive outcomes with full ownership, deeply believe in customer obsession, and thrive in a fast-paced environment of learning and innovation.\nYou will work in a challenging, consumer-facing problem space, where you can make an immediate impact. You will get to work with the latest technologies, learn to use new tools and get the opportunity to have your say in the final product. Youll work alongside a great team in an open, collaborative environment. We are part of Vimo, a well-funded, stable mid-size company with excellent salaries, medical/dental/vision coverage, and perks. Vimo is an Equal Opportunity Employer.",,,,"['python', 'Langchain', 'Neural Networks', 'LLM', 'Linux', 'Data Structures', 'Natural Language Processing', 'Jupyter Notebook', 'Machine Learning', 'Deep Learning', 'Numpy', 'Data Science', 'pandas', 'Nltk', 'Langgraph', 'Transformers', 'BERT', 'langsmith']",2025-06-12 13:52:32
Devops AWS DATA Engineeer|| Technical Analyst || 12Lakhs CTC,Robotics Technologies,7 - 9 years,11-12 Lacs P.A.,['Hyderabad( Banjara hills )'],"We are seeking a highly skilled Devops Engineer to join our dynamic development team. In this role, you will be responsible for designing, developing, and maintaining both frontend and backend components of our applications using Devops and associated technologies.\nYou will collaborate with cross-functional teams to deliver robust, scalable, and high-performing software solutions that meet our business needs. The ideal candidate will have a strong background in devops, experience with modern frontend frameworks, and a passion for full-stack development.\n\nRequirements:\nBachelor's degree in Computer Science Engineering, or a related field.\n7 to 9+ years of experience in full-stack development, with a strong focus on DevOps.\n\nDevOps with AWS Data Engineer - Roles & Responsibilities:\nUse AWS services like EC2, VPC, S3, IAM, RDS, and Route 53.\nAutomate infrastructure using Infrastructure as Code (IaC) tools like Terraform or AWS CloudFormation.\nBuild and maintain CI/CD pipelines using tools AWS CodePipeline, Jenkins,GitLab CI/CD.\nCross-Functional Collaboration\nAutomate build, test, and deployment processes for Java applications.\nUse Ansible, Chef, or AWS Systems Manager for managing configurations across environments.\nContainerize Java apps using Docker.\nDeploy and manage containers using Amazon ECS, EKS (Kubernetes), or Fargate.\nMonitoring & Logging using Amazon CloudWatch,Prometheus + Grafana,E\nStack (Elasticsearch, Logstash, Kibana),AWS X-Ray for distributed tracing manage access with IAM roles/policies.\nUse AWS Secrets Manager / Parameter Store for managing credentials.\nEnforce security best practices, encryption, and audits.\nAutomate backups for databases and services using AWS Backup, RDS Snapshots, and S3 lifecycle rules.\nImplement Disaster Recovery (DR) strategies.\nWork closely with development teams to integrate DevOps practices.\nDocument pipelines, architecture, and troubleshooting runbooks.\nMonitor and optimize AWS resource usage.\nUse AWS Cost Explorer, Budgets, and Savings Plans.\n\nMust-Have Skills:\nExperience working on Linux-based infrastructure.\nExcellent understanding of Ruby, Python, Perl, and Java.\nConfiguration and managing databases such as MySQL, Mongo.\nExcellent troubleshooting.\nSelecting and deploying appropriate CI/CD tools\nWorking knowledge of various tools, open-source technologies, and cloud services.\nAwareness of critical concepts in DevOps and Agile principles.\nManaging stakeholders and external interfaces.\nSetting up tools and required infrastructure.\nDefining and setting development, testing, release, update, and support processes for DevOps operation.\nHave the technical skills to review, verify, and validate the software code developed in the project.\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Iac', 'Devops', 'Jenkins', 'AWS', 'Kubernetes', 'RDS', 'Aws Cloudformation', 'Amazon Cloudwatch', 'Prometheus', 'Ci/Cd', 'Grafana', 'DR', 'Cloud Trail', 'Docker', 'IAM', 'Ansible / Chef', 'fargate', 'Gitlab', 'Monitoring', 'Python']",2025-06-12 13:52:34
"Director, Enterprise Data Architecture",Horizon Therapeutics,10 - 12 years,Not Disclosed,['Hyderabad'],"Career Category Engineering Job Description\nABOUT AMGEN\nAmgen harnesses the best of biology and technology to fight the world s toughest diseases, and make people s lives easier, fuller and longer. We discover, develop, manufacture and deliver innovative medicines to help millions of patients. Amgen helped establish the biotechnology industry more than 40 years ago and remains on the cutting-edge of innovation, using technology and human genetic data to push beyond what s known today.\nABOUT THE ROLE\nRole Description:\nThe Director for Data Architecture and Solutions will lead Amgen s enterprise data architecture and solutions strategy, overseeing the design, integration, and deployment of scalable, secure, and future-ready data systems. This leader will define the architectural vision and guide a high-performing team of architects and technical experts to implement data and analytics solutions that drive business value and innovation.\nThis role demands a strong blend of business acumen, deep technical expertise, and strategic thinking to align data capabilities with the companys mission and growth. The Director will also serve as a key liaison with executive leadership, influencing technology investment and enterprise data direction\n.\nRoles & Responsibilities:\nDevelop and own the enterprise data architecture and solutions roadmap, aligned with Amgen s business strategy and digital transformation goals.\nProvide executive leadership and oversight of data architecture initiatives across business domains (R&D, Commercial, Manufacturing, etc.).\nLead and grow a high-impact team of data and solution architects. Coach, mentor, and foster innovation and continuous improvement in the team.\nDesign and promote modern data architectures (data mesh, data fabric, lakehouse etc.) across hybrid cloud environments and enable for AI readiness.\nCollaborate with stakeholders to define solution blueprints, integrating business requirements with technical strategy to drive value.\nDrive enterprise-wide adoption of data modeling, metadata management, and data lineage standards.\nEnsure solutions meet enterprise-grade requirements for security, performance, scalability, compliance, and data governance.\nPartner closely with Data Engineering, Analytics, AI/ML, and IT Security teams to operationalize data solutions that enable advanced analytics and decision-making.\nChampion innovation and continuous evolution of Amgen s data and analytics landscape through new technologies and industry best practices.\nCommunicate architectural strategy and project outcomes to executive leadership and other non-technical stakeholders.\nFunctional Skills:\nMust-Have Skills:\n10+ years of experience in data architecture or solution architecture leadership roles, including experience at the enterprise level.\nProven experience leading architecture strategy and delivery in the life sciences or pharmaceutical industry.\nExpertise in cloud platforms (AWS, Azure, or GCP) and modern data technologies (data lakes, APIs, ETL/ELT frameworks).\nStrong understanding of data governance, compliance (e.g., HIPAA, GxP), and data privacy best practices.\nDemonstrated success managing cross-functional, global teams and large-scale data programs.\nExperience with enterprise architecture frameworks (TOGAF, Zachman, etc.).\nProven leadership skills with a track record of managing and mentoring high-performing data architecture teams.\nGood-to-Have Skills:\nMaster s or doctorate in Computer Science, Engineering, or related field.\nCertifications in cloud architecture (AWS, GCP, Azure).\nExperience integrating AI/ML solutions into enterprise Data Achitecture.\nFamiliarity with DevOps, CI/CD pipelines, and Infrastructure as Code (Terraform, CloudFormation).\nScaled Agile or similar methodology experience.\nLeadership and Communication Skills:\nStrategic thinker with the ability to influence at the executive level.\nStrong executive presence with excellent communication and storytelling skills.\nAbility to lead in a matrixed, global environment with multiple stakeholders.\nHighly collaborative, proactive, and business-oriented mindset.\nStrong organizational and prioritization skills to manage complex initiatives.\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nBasic Qualifications:\nDoctorate degree and 2 years of Information Systems experience, or\nMaster s degree and 6 years of Information Systems experience, or\nBachelor s degree and 8 years of Information Systems experience, or\nAssociates degree and 10 years of Information Systems experience, or\n4 years of managerial experience directly managing people and leadership experience leading teams, projects, or programs.\n.",Industry Type: Biotechnology,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Solution architecture', 'metadata', 'Data modeling', 'Enterprise architecture', 'TOGAF', 'HIPAA', 'Agile', 'Analytics', 'Data architecture']",2025-06-12 13:52:36
Devops AWS DATA Engineeer|| Technical Analyst || 12Lakhs CTC,Robotics Technologies,8 - 9 years,11-12 Lacs P.A.,['Hyderabad( Banjara hills )'],"We are seeking a highly skilled Devops Engineer to join our dynamic development team. In this role, you will be responsible for designing, developing, and maintaining both frontend and backend components of our applications using Devops and associated technologies.\nYou will collaborate with cross-functional teams to deliver robust, scalable, and high-performing software solutions that meet our business needs. The ideal candidate will have a strong background in devops, experience with modern frontend frameworks, and a passion for full-stack development.\n\nRequirements:\nBachelor's degree in Computer Science Engineering, or a related field.\n8 to 9+ years of experience in full-stack development, with a strong focus on DevOps.\n\nDevOps with AWS Data Engineer - Roles & Responsibilities:\nUse AWS services like EC2, VPC, S3, IAM, RDS, and Route 53.\nAutomate infrastructure using Infrastructure as Code (IaC) tools like Terraform or AWS CloudFormation.\nBuild and maintain CI/CD pipelines using tools AWS CodePipeline, Jenkins,GitLab CI/CD.\nCross-Functional Collaboration\nAutomate build, test, and deployment processes for Java applications.\nUse Ansible, Chef, or AWS Systems Manager for managing configurations across environments.\nContainerize Java apps using Docker.\nDeploy and manage containers using Amazon ECS, EKS (Kubernetes), or Fargate.\nMonitoring & Logging using Amazon CloudWatch,Prometheus + Grafana,E\nStack (Elasticsearch, Logstash, Kibana),AWS X-Ray for distributed tracing manage access with IAM roles/policies.\nUse AWS Secrets Manager / Parameter Store for managing credentials.\nEnforce security best practices, encryption, and audits.\nAutomate backups for databases and services using AWS Backup, RDS Snapshots, and S3 lifecycle rules.\nImplement Disaster Recovery (DR) strategies.\nWork closely with development teams to integrate DevOps practices.\nDocument pipelines, architecture, and troubleshooting runbooks.\nMonitor and optimize AWS resource usage.\nUse AWS Cost Explorer, Budgets, and Savings Plans.\n\nMust-Have Skills:\nExperience working on Linux-based infrastructure.\nExcellent understanding of Ruby, Python, Perl, and Java.\nConfiguration and managing databases such as MySQL, Mongo.\nExcellent troubleshooting.\nSelecting and deploying appropriate CI/CD tools\nWorking knowledge of various tools, open-source technologies, and cloud services.\nAwareness of critical concepts in DevOps and Agile principles.\nManaging stakeholders and external interfaces.\nSetting up tools and required infrastructure.\nDefining and setting development, testing, release, update, and support processes for DevOps operation.\nHave the technical skills to review, verify, and validate the software code developed in the project.\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Iac', 'Devops', 'Jenkins', 'AWS', 'Kubernetes', 'RDS', 'Aws Cloudformation', 'Amazon Cloudwatch', 'Prometheus', 'Ci/Cd', 'Grafana', 'DR', 'Cloud Trail', 'Docker', 'IAM', 'Ansible / Chef', 'fargate', 'Gitlab', 'Monitoring', 'Python']",2025-06-12 13:52:39
"Staff Engineer, Nodejs",Nagarro,7 - 10 years,Not Disclosed,['India'],"We're Nagarro.\n\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal Experience 7+ years.\nExcellent knowledge developing scalable and highly available Restful APIs using NodeJS technologies.\nThorough understanding of React.js and its core principles and experience with popular React.js workflows (such as Flux or Redux or Context API or Data Structures).\nFamiliarity with common programming tools such as RESTful APIs, TypeScript, version control software, and remote deployment tools, CI/CD tools.\nUnderstanding of linter libraries (TSLINT, Prettier etc) and Unit testing using Jest, Enzyme, Jasmine or equivalent framework.\nStrong proficiency in JavaScript, including DOM manipulation and the JavaScript object model. Proficient with the latest versions of ECMAScript (JavaScript or TypeScript).\nUnderstanding of containerization, experienced in Dockers, Kubernetes.\nExposed to API gateway integrations like 3Scale.\nUnderstanding of Single-Sign-on or token-based authentication (Rest, JWT, OAuth).\nPossess expert knowledge of task/message queues include but not limited to: AWS, Microsoft Azure, Pushpin. and Kafka.\nPractical experience with GraphQL is good to have.\nWriting tested, idiomatic, and documented JavaScript, HTML and CSS.\nExperiencing in Developing responsive web-based UI.\nHave experience on Styled Components, Tailwind CSS, Material UI and other CSS-in-JS techniques.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them into technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'Typescript', 'Node.Js', 'Docker', 'Microservices', 'Kubernetes']",2025-06-12 13:52:41
Azure Data Architect,Syren Technologies,10 - 18 years,Not Disclosed,[],"About Syren Cloud\n\nSyren Cloud Technologies is a cutting-edge company specializing in supply chain solutions and data engineering. Their intelligent insights, powered by technologies like AI and NLP, empower organizations with real-time visibility and proactive decision-making. From control towers to agile inventory management, Syren unlocks unparalleled success in supply chain management.\n\nRole Summary",,,,"['Pyspark', 'Azure', 'Architecture', 'Data Bricks']",2025-06-12 13:52:43
Data Architect,Calibo,12 - 16 years,Not Disclosed,[],"About the Role:\n\nWe are looking for a highly skilled Data Engineering Architect with strong Data Engineering pipeline implementation experience to serve as the lead Solution/Technical Architect and Subject Matter Expert for customer experience data solutions across multiple data sources. The ideal candidate will collaborate with the Enterprise Architect and the client IT team to establish and implement strategic initiatives.\n\nResponsibilities and Technical Skills:\n12+ years of relevant experience in designing and Architecting ETL, ELT, Reverse ETL, Data Management or Data Integration, Data Warehouse, Data Lake, and Data Migration.\nMust have expertise in building complex ETL pipelines and large Data Processing, Data Quality and Data security\nExperience in delivering quality work on time with multiple, competing priorities.\nExcellent troubleshooting and problem-solving skills must be able to consistently identify critical elements, variables and alternatives to develop solutions.\nExperience in identifying, analyzing and translating business requirements into conceptual, logical and physical data models in complex, multi-application environments.\nExperience with Agile and Scaled Agile Frameworks.\nExperience in identifying and documenting data integration issues, and challenges such as duplicate data, non-conformed data, and unclean data. Multiple platform development experience.\nStrong experience in performance tuning of ETL processes using Data Platforms\nMust have experience in handling Data formats like Delta Tables, Parquet files, Iceberg etc.\nExperience in Cloud technologies such as AWS/Azure or Google Cloud.\nApache Spark design and development experience using Scala, Java, Python or Data Frames with Resilient Distributed Datasets (RDDs).\nDevelopment experience in databases like Oracle, AWS Redshift, AWS RDS, Postgres Databricks and/or Snowflake.\nHands-on professional work experience with Python is highly desired.\nExperience in Hadoop ecosystem tools for real-time or batch data ingestion.\nStrong communication and teamwork skills to interface with development team members, business analysts, and project management. Excellent analytical skills.\nIdentification of data sources, internal and external, and defining a plan for data management as per business data strategy.\nCollaborating with cross-functional teams for the smooth functioning of the enterprise data system.\nManaging end-to-end data architecture, from selecting the platform, designing the technical architecture, and developing the application to finally testing and implementing the proposed solution.\nPlanning and execution of big data solutions using Databricks, Big Data, Hadoop, Big Query, Snowflake, MongoDB, DynamoDB, PostgreSQL and SQL Server\nHands-on experience in defining and implementing various Machine Learning models for different business needs.\nIntegrating technical functionality, ensuring data accessibility, accuracy, and security.\nProgramming / Scripting Languages like Python / Java / Go, Microservices\nMachine Learning / AI tools like Scikit-learn / TensorFlow / PyTorch",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cloud', 'ETL', 'AWS', 'Data Handling', 'Spark']",2025-06-12 13:52:45
Data Analytics Mgr,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\n\n\nIn this vital role you will report to the Organizational Planning Analytics & Insights Procurement & Sourcing Lead, you will support Amgens Tech & Workforce Strategy by applying business analytics and change leadership skills to drive insights that impact resource allocation and sourcing strategy.\n\nYour responsibilities include dashboard development, ad-hoc reporting, business partnering & engagement, and financial baselining. This role supports organizational change and enables the development of an integrated approach to global sourcing and financial planning.\n\nReporting to the Organizational Planning Analytics & Insights Procurement & Sourcing Lead, you will support Amgens Tech & Workforce Strategy by applying business analytics and change management skills to drive insights that impact resource allocation and sourcing strategy.\n\nYour responsibilities include dashboard development, ad-hoc reporting, business partnering & engagement, and financial baselining. This role supports change management and enables the development of an integrated approach to global sourcing and financial planning.\n\nRoles & Responsibilities:\nAddressing business challenges through process evaluation and insight generation.\nDevelop insights with a strong focus on Tableau and Power BI dashboard creation, as well as PowerPoint presentations.\nGuide data analysts and data engineers on standard methodologies for building data pipelines to support dashboards and other business objectives.\nConduct ad hoc analyses of FP&A and sourcing/procurement data.\nAddressing business challenges through process evaluation and insight generation.\nDevelop insights with a strong focus on Tableau and Power BI dashboard creation, as well as PowerPoint presentations.\nGuide data analysts and data engineers on standard methodologies for building data pipelines to support dashboards and other business objectives.\nConduct ad hoc analyses of FP&A and sourcing/procurement data.\nWhat we expect of you\n\nWe are all different, yet we all use our unique contributions to serve patients.\n\nBasic Qualifications:\nMasters degree and 4 to 6 years of applicable experience in business analysis (finance analysis, data analysis, sourcing analysis, or similar experience OR\nBachelors degree and 6 to 8 years of applicable experience in business analysis (finance analysis, data analysis, sourcing analysis, or similar experience OR\nDiploma and 10 to 12 years of applicable experience in business analysis (finance analysis, data analysis, sourcing analysis, or similar experience\nPreferred Qualifications:\nMasters degree in data science, business, statistics, data mining, applied mathematics, business analytics, engineering, computer science, or a related field\n4 years of relevant experience in data science, data analytics, consulting, and/or financial planning & analysis.\nA keen eye for design, with the ability to craft engaging PowerPoint decks and develop compelling Power BI and Tableau dashboards.\nProven expertise in statistical/mathematical modeling and working with structured/unstructured data.\nExperience with procurement, sourcing, and/or financial planning data.\nSkilled in automating data workflows using tools like Tableau, Python, R, Alteryx, and PowerApps.\nKnowledge of global finance systems, Procurement, and sourcing operations.\nExperience with data analysis, budgeting, forecasting, and strategic planning in the Bio-Pharmaceutical or biotech industry.\nGrowing in a start-up environment, building a data-driven transformation capability.\nUnderstanding of the Bio-Pharmaceutical and biotech industry trends and operations.\nProven ability to engage with cross-functional business leaders to align data strategies with corporate objectives, redefining complex data insights into actionable strategies.\nFlexible work models, including remote work arrangements, where possible\n\nAs we work to develop treatments that deal with others, we also work to care for your professional and personal growth and well-being. From our competitive benefits to our collaborative culture, well support your journey every step of the way.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'data analytics', 'data science', 'mathematical modeling', 'financial planning', 'financial planning and analysis', 'statistics']",2025-06-12 13:52:47
MDM Data Scientist,Amgen Inc,3 - 8 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nWe are seeking an accomplished and visionary Data Scientist/ GenAIdeveloper to join Amgens Enterprise Data Management team. As part of MDM team, you will be responsible for designing, developing, and deploying Generative AI and ML models to power data-driven decisions across business domains. This role is ideal for an AI practitioner who thrives in a collaborative environment and brings a strategic mindset to applying advanced AI techniques to solve real-world problems.To succeed in this role, the candidate must have strong AI/ML, Data Science, GenAI experience along with MDM knowledge, hence the candidates having only MDM experience are not eligible for this role. Candidate must have AI/ML, data science and GenAI experience on technologies like (PySpark/PyTorch, TensorFlow, LLM, Autogen, Hugging FaceVectorDB,Embeddings, RAGsetc), along with knowledge of MDM (Master Data Management)\nRoles & Responsibilities:\nDevelop enterprise-level GenAI applications using LLM frameworks such as Langchain, Autogen, and Hugging Face.\nDesign and develop intelligent pipelines using PySpark, TensorFlow, and PyTorch within Databricks and AWS environments.\nImplement embedding models andmanage VectorStores for retrieval-augmented generation (RAG) solutions.\nIntegrate and leverage MDM platforms like Informatica and Reltio to supply high-quality structured data to ML systems.\nUtilize SQL and Python for data engineering, data wrangling, and pipeline automation.\nBuild scalable APIs and services to serve GenAI models in production.\nLead cross-functional collaboration with data scientists, engineers, and product teams to scope, design, and deploy AI-powered systems.\nEnsure model governance, version control, and auditability aligned with regulatory and compliance expectations.\nBasic Qualifications and Experience:\nMasters degree with 4 - 6 years of experience in Business, Engineering, IT or related field OR\nBachelors degree with 6 - 9 years of experience in Business, Engineering, IT or related field OR\nDiploma with 10 - 12 years of experience in Business, Engineering, IT or related field\nFunctional Skills:\nMust-Have Skills:\n6+ years of experience working in AI/ML or Data Science roles, including designing and implementing GenAI solutions.\nExtensive hands-on experience with LLM frameworks and tools such as Langchain, Autogen, Hugging Face, OpenAI APIs, and embedding models.\nStrong programming background with Python, PySpark, and experience in building scalable solutions using TensorFlow, PyTorch, and SK-Learn.\nProven track record of building and deploying AI/ML applications in cloud environments such as AWS.\nExpertise in developing APIs, automation pipelines, and serving GenAI models using frameworks like Django, FastAPI, and DataBricks.\nSolid experience integrating and managing MDM tools (Informatica/Reltio) and applying data governance best practices.\nGuide the team on development activities and lead the solution discussions\nMust have core technical capabilities in GenAI, Data Science space\nGood-to-Have Skills:\nPrior experience in Data Modeling, ETL development, and data profiling to support AI/ML workflows.\nWorking knowledge of Life Sciences or Pharma industry standards and regulatory considerations.\nProficiency in tools like JIRA and Confluence for Agile delivery and project collaboration.\nFamiliarity with MongoDB, VectorStores, and modern architecture principles for scalable GenAI applications.\nProfessional Certifications:\nAny ETL certification (e.g. Informatica)\nAny Data Analysis certification (SQL)\nAny cloud certification (AWS or AZURE)\nData Science and ML Certification\nSoft Skills:\nStrong analytical abilities to assess and improve master data processes and solutions.\nExcellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders.\nEffective problem-solving skills to address data-related issues and implement scalable solutions.\nAbility to work effectively with global, virtual teams\nWe will ensure that individuals with disabilities are provided with reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['MDM', 'GenAI', 'Langchain', 'PySpark', 'VectorStores', 'Hugging Face', 'LLM', 'Data Science', 'DataBricks', 'SK-Learn', 'AI/ML', 'Autogen', 'PyTorch', 'Django', 'OpenAI APIs', 'FastAPI', 'MongoDB', 'Data Modeling', 'PySpark/PyTorch', 'TensorFlow', 'Python']",2025-06-12 13:52:49
Principal - Data Architect,Affine Analytics,8 - 12 years,Not Disclosed,['Chennai'],"We are seeking a highly skilled Data Architect to design and implement robust, scalable, and secure data solutions on AWS Cloud. The ideal candidate should have expertise in AWS services, data modeling, ETL processes, and big data technologies, with hands-on experience in Glue, DMS, Python, PySpark, and MPP databases like Snowflake, Redshift, or Databricks.\nKey Responsibilities:\nArchitect and implement data solutions leveraging AWS services such as EC2, S3, IAM, Glue (Mandatory), and DMS for efficient data processing and storage.",,,,"['Python', 'S3', 'AWS Glue', 'DMS', 'SQL Server', 'Redshift', 'Glue', 'IAM', 'EC2', 'Snowflake', 'Databricks', 'Oracle', 'Lambda']",2025-06-12 13:52:51
MDM Data Scientist,Amgen Inc,4 - 9 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nWe are seeking an accomplished and visionary Data Scientist/ GenAIdeveloper to join Amgens Enterprise Data Management team. As part of MDM team, you will be responsible for designing, developing, and deploying Generative AI and ML models to power data-driven decisions across business domains. This role is ideal for an AI practitioner who thrives in a collaborative environment and brings a strategic mindset to applying advanced AI techniques to solve real-world problems.\nTo succeed in this role, the candidate must have strong AI/ML, Data Science, GenAI experience along with MDM knowledge, hence the candidates having only MDM experience are not eligible for this role. Candidate must have AI/ML, data science and GenAI experience on technologies like (PySpark/PyTorch, TensorFlow, LLM, Autogen, Hugging FaceVectorDB,Embeddings, RAGsetc), along with knowledge of MDM (Master Data Management)\nRoles & Responsibilities:\nDevelop enterprise-level GenAI applications using LLM frameworks such as Langchain, Autogen, and Hugging Face.\nDesign and develop intelligent pipelines using PySpark, TensorFlow, and PyTorch within Databricks and AWS environments.\nImplement embedding models andmanage VectorStores for retrieval-augmented generation (RAG) solutions.\nIntegrate and leverage MDM platforms like Informatica and Reltio to supply high-quality structured data to ML systems.\nUtilize SQL and Python for data engineering, data wrangling, and pipeline automation.\nBuild scalable APIs and services to serve GenAI models in production.\nLead cross-functional collaboration with data scientists, engineers, and product teams to scope, design, and deploy AI-powered systems.\nEnsure model governance, version control, and auditability aligned with regulatory and compliance expectations.\nBasic Qualifications and Experience:\nMasters degree with 4 - 6 years of experience in Business, Engineering, IT or related field OR\nBachelors degree with 6 - 9 years of experience in Business, Engineering, IT or related field OR\nDiploma with 10 - 12 years of experience in Business, Engineering, IT or related field\nFunctional Skills:\nMust-Have Skills:\n6+ years of experience working in AI/ML or Data Science roles, including designing and implementing GenAI solutions.\nExtensive hands-on experience with LLM frameworks and tools such as Langchain, Autogen, Hugging Face, OpenAI APIs, and embedding models.\nStrong programming background with Python, PySpark, and experience in building scalable solutions using TensorFlow, PyTorch, and SK-Learn.\nProven track record of building and deploying AI/ML applications in cloud environments such as AWS.\nExpertise in developing APIs, automation pipelines, and serving GenAI models using frameworks like Django, FastAPI, and DataBricks.\nSolid experience integrating and managing MDM tools (Informatica/Reltio) and applying data governance best practices.\nGuide the team on development activities and lead the solution discussions\nMust have core technical capabilities in GenAI, Data Science space\nGood-to-Have Skills:\nPrior experience in Data Modeling, ETL development, and data profiling to support AI/ML workflows.\nWorking knowledge of Life Sciences or Pharma industry standards and regulatory considerations.\nProficiency in tools like JIRA and Confluence for Agile delivery and project collaboration.\nFamiliarity with MongoDB, VectorStores, and modern architecture principles for scalable GenAI applications.\nProfessional Certifications:\nAny ETL certification (e.g. Informatica)\nAny Data Analysis certification (SQL)\nAny cloud certification (AWS or AZURE)\nData Science and ML Certification\nSoft Skills:\nStrong analytical abilities to assess and improve master data processes and solutions.\nExcellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders.\nEffective problem-solving skills to address data-related issues and implement scalable solutions.\nAbility to work effectively with global, virtual teams.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'GenAI', 'Langchain', 'PySpark', 'Hugging Face', 'OpenAI API', 'Autogen', 'PyTorch', 'Django', 'MDM', 'FastAPI', 'Data Modeling', 'ETL', 'TensorFlow', 'Python']",2025-06-12 13:52:54
Job opening For Data Warehouse + ADF + ETL,bct,3 - 6 years,Not Disclosed,['Pune'],"Greetings of the Day !!!\n\nWe have job opening for Data Warehouse + ADF + ETL with one of our Client .If you are interested for this role , kindly share update resume along with below details in this email id : shaswati.m@bct-consulting.com\n\nJob Description:\nSenior Data Engineer\nAs a Senior Data Engineer, you will support the European World Area using the Windows & Azure suite of Analytics & Data platforms. The focus of the role is on the technical aspects and implementation of data gathering, integration and database design.\nWe look forward to seeing your application!\nIn This Role, Your Responsibilities Will Be:\nData Ingestion and Integration: Collaborate with Product Owners and analysts to understand data requirements & design, develop, and maintain data pipelines for ingesting, transforming, and integrating data from various sources into Azure Data Services.\nMigration of existing ETL packages: Migrate existing SSIS packages to Synapse pipelines\nData Modelling: Assist in designing and implementing data models, data warehouses, and databases in Azure Synapse Analytics, Azure Data Lake Storage, and other Azure services.\nData Transformation: Develop ETL (Extract, Transform, Load) processes using SQL Server Integration Services (SSIS), Azure Synapse Pipelines, or other relevant tools to prepare data for analysis and reporting.\nData Quality and Governance: Implement data quality checks and data governance practices to ensure the accuracy, consistency, and security of data assets.\nMonitoring and Optimization: Monitor and optimize data pipelines and workflows for performance, scalability, and cost efficiency.\nDocumentation: Maintain comprehensive documentation of processes, including data lineage, data dictionaries, and pipeline schedules.\nCollaboration: Work closely with cross-functional teams, including data analysts, data scientists, and business stakeholders, to understand their data needs and deliver solutions accordingly.\nAzure Services: Stay updated on Azure data services and best practices to recommend and implement improvements in our data architecture and processes\nFor This Role, You Will Need:\n3-5 years of experience in Data Warehousing with On-Premises or Cloud technologies\nStrong practical experience of Synapse pipelines / ADF.\nStrong practical experience of developing ETL packages using SSIS.\nStrong practical experience with T-SQL or any variant from other RDBMS.\nGraduate degree educated in computer science or a relevant subject.\nStrong analytical and problem-solving skills.\nStrong communication skills in dealing with internal customers from a range of functional areas.\nWillingness to work flexible working hours according to project requirements.\nTechnical documentation skills.\nFluent in English.\nPreferred Qualifications that Set You Apart:\nOracle PL/SQL.\nExperience in working on Azure Services like Azure Synapse Analytics, Azure Data Lake.\nWorking experience with Azure DevOps paired with knowledge of Agile and/or Scrum methods of delivery.\nLanguages: French, Italian, or Spanish would be an advantage.\nAgile certification.\nThanks,\nShaswati",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ADF', 'ETL', 'SSIS', 'Data ware house']",2025-06-12 13:52:56
"Advisor, Software Development Engineering",Fiserv,10 - 15 years,Not Disclosed,['Pune'],"Dashboard Development & Management : Design and maintain advanced Splunk dashboards to deliver comprehensive insights into system performance and File Transmission component health.\nPerformance Optimization : Improve dashboard efficiency when handling large datasets using techniques such as optimized queries, summary indexing, and data models.\nAdvanced Regex Utilization : Apply sophisticated regular expressions to create accurate search queries and extract meaningful data.\nCustom Alert Configuration : Implement highly customized alerting mechanisms to detect anomalies, manage alert actions, throttle conditions, and integrate with lookup tables and dynamic time-based arguments.\nFile Transmission Monitoring : Track and report on each stage of file transmission, continuously refining monitoring strategies for enhanced reliability and visibility.\nCross-Functional Collaboration : Work closely with various teams to integrate Splunk monitoring with broader IT systems and workflows.\nConduct discovery of file transmission workflows, including file life cycle, endpoint configurations, log analysis, SLA definitions, and exception scenarios.\nDevelop and deploy advanced Splunk queries to ensure end-to-end visibility into file transmission processes.\nConfigure and optimize alerting mechanisms for timely detection and resolution of issues.\nDesign and implement IT Service Intelligence (ITSI) strategies to enhance monitoring capabilities and deliver actionable insights.\nEstablish and manage monitoring frameworks based on the file life cycle to ensure traceability and accountability.\nCollaborate with IT and operations teams to integrate Splunk with other tools and resolve data ingestion issues.\nAnalyze monitoring data to identify trends, detect anomalies, and recommend improvements.\nServe as a Splunk subject matter expert, providing guidance, best practices, and training to team members.\nWhat You Will Need to Have\nEducation : bachelors and/or masters degree in Information Technology, Computer Science, or a related field.\nExperience : Minimum of 10 years in IT, with a focus on Splunk, SFTP tools, data integration, or technical support roles.\nSplunk Expertise : Proficiency in advanced SPL techniques including subsearches, joins, and statistical functions.\nRegex Proficiency : Strong command of regular expressions for search and data extraction.\nDatabase Skills : Experience with relational databases and writing complex SQL queries with advanced joins.\nFile Transmission Tools : Hands-on experience with platforms like Sterling File Gateway, IBM Sterling, or other MFT solutions.\nAnalytical Thinking : Proven problem-solving skills and the ability to troubleshoot technical issues effectively.\nCommunication : Strong verbal and written communication skills for collaboration with internal and external stakeholders.\nAttention to Detail : High level of accuracy to ensure data integrity and reliability.\nWhat Would Be Great to Have\nScripting & Automation : Proficiency in Python or similar scripting languages to automate monitoring tasks.\nTool Experience : Familiarity with tools such as Dynatrace, Sterling File Gateway, and other MFT solutions.\nLinux Proficiency : Strong working knowledge of Linux and command-line operations.\nSecure File Transfer Protocols : Hands-on experience with SFTP and tools like SFG, NDM, and MFT using SSH encryption.\nTask Scheduling Tools : Experience with job scheduling platforms such as AutoSys, Control-M, or cron.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Linux', 'Analytical', 'Scheduling', 'data integrity', 'Information technology', 'Technical support', 'Monitoring', 'Data extraction']",2025-06-12 13:52:58
Azure Data Factory,Swift Staffing,8 - 12 years,6.5-14 Lacs P.A.,"['Mumbai', 'Hyderabad', 'Pune']","Job Description:\n5+ years in data engineering with at least 2 years on Azure Synapse.\nStrong SQL, Spark, and Data Lake integration experience.\nFamiliarity with Azure Data Factory, Power BI, and DevOps pipelines.\nExperience in AMS or managed services environments is a plus.\nDetailed JD\nDesign, develop, and maintain data pipelines using Azure Synapse Analytics.\nCollaborate with customer to ensure SLA adherence and incident resolution.\nOptimize Synapse SQL pools for performance and cost.\nImplement data security, access control, and compliance measures.\nParticipate in calibration and transition phases with client stakeholders",Industry Type: Recruitment / Staffing,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Synapse', 'Data Engineering', 'Azure Databricks', 'SQL Azure', 'Power Bi', 'Devops']",2025-06-12 13:53:00
Data Architect Telecom Domain databrick BSS OSS,fast growing Data Driven IT solutions an...,10 - 20 years,45-55 Lacs P.A.,"['Noida', 'Hyderabad', 'Gurugram']","Data Architect Telecom Domain\nTo design comprehensive data architecture and technical solutions specifically for telecommunications industry challenges, leveraging TMforum frameworks and modern data platforms. To work closely with customers, and technology partners to deliver data solutions that address complex telecommunications business requirements including customer experience management, network optimization, revenue assurance, and digital transformation initiatives.\nResponsibilities:\nDesign and articulate enterprise-scale telecom data architectures incorporating TMforum standards and frameworks, including SID (Shared Information/Data Model), TAM (Telecom Application Map), and eTOM (enhanced Telecom Operations Map)\nDevelop comprehensive data models aligned with TMforum guidelines for telecommunications domains such as Customer, Product, Service, Resource, and Partner management\nCreate data architectures that support telecom-specific use cases including customer journey analytics, network performance optimization, fraud detection, and revenue assurance\nDesign solutions leveraging Microsoft Azure and Databricks for telecom data processing and analytics\nConduct technical discovery sessions with telecom clients to understand their OSS/BSS architecture, network analytics needs, customer experience requirements, and digital transformation objectives\nDesign and deliver proof of concepts (POCs) and technical demonstrations showcasing modern data platforms solving real-world telecommunications challenges\nCreate comprehensive architectural diagrams and implementation roadmaps for telecom data ecosystems spanning cloud, on-premises, and hybrid environments\nEvaluate and recommend appropriate big data technologies, cloud platforms, and processing frameworks based on telecom-specific requirements and regulatory compliance needs.\nDesign data governance frameworks compliant with telecom industry standards and regulatory requirements (GDPR, data localization, etc.)\nStay current with the latest advancements in data technologies including cloud services, data processing frameworks, and AI/ML capabilities\nContribute to the development of best practices, reference architectures, and reusable solution components for accelerating proposal development\nQualifications:\nBachelor's or Master's degree in Computer Science, Telecommunications Engineering, Data Science, or a related technical field\n10+ years of experience in data architecture, data engineering, or solution architecture roles with at least 5 years in telecommunications industry\nDeep knowledge of TMforum frameworks including SID (Shared Information/Data Model), eTOM, TAM, and their practical implementation in telecom data architectures\nDemonstrated ability to estimate project efforts, resource requirements, and implementation timelines for complex telecom data initiatives\nHands-on experience building data models and platforms aligned with TMforum standards and telecommunications business processes\nStrong understanding of telecom OSS/BSS systems, network management, customer experience management, and revenue management domains\nHands-on experience with data platforms including Databricks, and Microsoft Azure in telecommunications contexts\nExperience with modern data processing frameworks such as Apache Kafka, Spark and Airflow for real-time telecom data streaming\nProficiency in Azure cloud platform and its respective data services with an understanding of telecom-specific deployment requirements\nKnowledge of system monitoring and observability tools for telecommunications data infrastructure\nExperience implementing automated testing frameworks for telecom data platforms and pipelines\nFamiliarity with telecom data integration patterns, ETL/ELT processes, and data governance practices specific to telecommunications\nExperience designing and implementing data lakes, data warehouses, and machine learning pipelines for telecom use cases\nProficiency in programming languages commonly used in data processing (Python, Scala, SQL) with telecom domain applications\nUnderstanding of telecommunications regulatory requirements and data privacy compliance (GDPR, local data protection laws)\nExcellent communication and presentation skills with ability to explain complex technical concepts to telecom stakeholders\nStrong problem-solving skills and ability to think creatively to address telecommunications industry challenges\nGood to have TMforum certifications or telecommunications industry certifications\nRelevant data platform certifications such as Databricks, Azure Data Engineer are a plus\nWillingness to travel as required\nif you will all or most of the criteria contact bdm@intellisearchonline.net M 9341626895",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Telecom Bss', 'Data Architect', 'Telecom OSS', 'ETOM', 'Data Bricks']",2025-06-12 13:53:03
Snowflake Developer with Azure Data Factory,Net Connect,6 - 10 years,6-11 Lacs P.A.,['Hyderabad'],Greetings from NCG!\n\nWe have a opening for Snowflake Developer role in Hyderabad office!\n\nBelow JD for your reference\n\nJob Description:,,,,"['Azure Data Factory', 'Snowflake', 'SQL']",2025-06-12 13:53:05
Calibration Engineer,Motherson Technology Services Limited,5 - 10 years,Not Disclosed,['Pune'],"Role & responsibilities\n• Define and update the Application project plan for the assigned Projects.\n• Manage with pro-active approach the project plan changes during the development phase.\n• Adopt calibration methodologies, procedures and tools shared by Marelli HQ.\n• Promote effective solutions together with team Application and Functions Design Teams\n• Guarantee the compliance of calibration process workflow with the standards defined by Team.\n• Promote the use of statistical analysis and big data management, in cooperation with team, to validate strategies performance and diagnosis robustness.\n• Customer technical reference for all the issues related to calibration.\n• Support on Customer site for calibration activities development, when requested.\n• Confirm with the Car Maker for the process of calibration via label review.\nCoordinate the activities on the test development vehicles assigned to each project.\n• Take part to calibration design review - risk analyses with Team or with the Customers.\n• Analysis and resolution of vehicle fleets and vehicle market concerns\n\nPreferred candidate profile\nSkill\nGasoline/CNG/Flex fuel working experience.\nMPFI/GDI engines.\nBase calibration on test bed and vehicle.\nTransient calibration.\nDrivability and test trips experience.\nStart ability calibration.\nIdle calibration.\nEmission calibration.\nOBD-1 and OBD-2B calibration.",Industry Type: IT Services & Consulting,"Department: Production, Manufacturing & Engineering","Employment Type: Full Time, Permanent","['gasoline', 'calibration', 'Obd', 'INCA', 'Emission', 'Engine Calibration']",2025-06-12 13:53:07
Data Architect (Data Bricks),Diacto Technologies Pvt Ltd,5 - 9 years,Not Disclosed,['Pune( Baner )'],"Job Overview:\nDiacto is seeking an experienced and highly skilled Data Architect to lead the design and development of scalable and efficient data solutions. The ideal candidate will have strong expertise in Azure Databricks, Snowflake (with DBT, GitHub, Airflow), and Google BigQuery. This is a full-time, on-site role based out of our Baner, Pune office.\n\nQualifications:\nB.E./B.Tech in Computer Science, IT, or related discipline\nMCS/MCA or equivalent preferred\n\nKey Responsibilities:\nDesign, build, and optimize robust data architecture frameworks for large-scale enterprise solutions\nArchitect and manage cloud-based data platforms using Azure Databricks, Snowflake, and BigQuery\nDefine and implement best practices for data modeling, integration, governance, and security\nCollaborate with engineering and analytics teams to ensure data solutions meet business needs\nLead development using tools such as DBT, Airflow, and GitHub for orchestration and version control\nTroubleshoot data issues and ensure system performance, reliability, and scalability\nGuide and mentor junior data engineers and developers\n\nExperience and Skills Required:\n5 to12 years of experience in data architecture, engineering, or analytics roles\nHands-on expertise in Databricks, especially Azure Databricks\nProficient in Snowflake, with working knowledge of DBT, Airflow, and GitHub\nExperience with Google BigQuery and cloud-native data processing workflows\nStrong knowledge of modern data architecture, data lakes, warehousing, and ETL pipelines\nExcellent problem-solving, communication, and analytical skills\n\nNice to Have:\nCertifications in Azure, Snowflake, or GCP\nExperience with containerization (Docker/Kubernetes)\nExposure to real-time data streaming and event-driven architecture\n\nWhy Join Diacto Technologies?\nCollaborate with experienced data professionals and work on high-impact projects\nExposure to a variety of industries and enterprise data ecosystems\nCompetitive compensation, learning opportunities, and an innovation-driven culture\nWork from our collaborative office space in Baner, Pune\nHow to Apply:\nOption 1 (Preferred)\n\nCopy and paste the following link on your browser and submit your application for the automated interview process: -\n\nhttps://app.candidhr.ai/app/candidate/gAAAAABoRrTQoMsfqaoNwTxsE_qwWYcpcRyYJk7NzSUmO3LKb6rM-8FcU58CUPYQKc65n66feHor-TGdCEfyouj0NmKdgYcNbA==/\n\nOption 2\n\n1. Please visit our website's career section at https://www.diacto.com/careers/\n2. Scroll down to the ""Who are we looking for?"" section\n3. Find the listing for "" Data Architect (Data Bricks)"" and\n4. Proceed with the virtual interview by clicking on ""Apply Now.""",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Azure Databricks', 'Airflow', 'Etl Pipelines', 'Github', 'google BigQuery', 'DBT', 'Data Security', 'Data Modeling', 'Elt', 'Data Governance']",2025-06-12 13:53:09
Data Scientist For DMAI,Prodapt Solutions,2 - 5 years,Not Disclosed,['Chennai'],"Overview\n\nThe Senior Data Science Engineer will leverage advanced data science techniques to solve complex business problems, guide decision-making processes, and mentor junior team members. This role requires a combination of technical expertise in data analysis, machine learning, and project management skills.\n\nResponsibilities\n\n Data Analysis and Modeling Analyze large-scale telecom datasets to extract actionable insights and build predictive models for network optimization and customer retention.\n Conduct statistical analyses  to validate models and ensure their effectiveness.\n Machine Learning Development Design and implement machine learning algorithms for fraud detection, churn prediction, and network failure analysis.\n Telecom-Specific Analytics Apply domain knowledge to improve customer experience by analyzing usage patterns, optimizing services, and predicting customer lifetime value.\n ETL Processes Develop robust pipelines for extracting, transforming, and loading telecom data from diverse sources.\n Collaboration Work closely with data scientists, software engineers, and telecom experts to deploy solutions that enhance operational efficiency.\n Data Governance :  Ensure data integrity, privacy, security and compliance with industry standards\n\n\nAdvanced degree in Data Science, Statistics, Computer Science, or a related field.\nExtensive experience in data science roles with a strong focus on machine learning and statistical modeling.\nProficiency in programming languages such as Python or R and strong SQL skills.\nFamiliarity with big data technologies (e.g., Hadoop, Spark) is advantageous.\nExpertise in cloud platforms such as AWS or Azure.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['project management', 'data analysis', 'machine learning', 'sql', 'statistical modeling', 'algorithms', 'python', 'big data technologies', 'microsoft azure', 'cloud platforms', 'r', 'data science', 'spark', 'data governance', 'hadoop', 'aws', 'etl', 'machine learning algorithms', 'statistics']",2025-06-12 13:53:12
Sr. Database Engineer For US shift (Eastern Time),TEOCO,5 - 10 years,Not Disclosed,"['Kolkata', 'Bengaluru']","Position: Sr. Database Engineer US shift (Eastern Time)\nLocation: Kolkata or Bangalore\nFull time permanent position\n\nUS shift (Eastern Time) : 4.30PM to 12.30AM IST (complete work from home)\n\nExperience required: 6-8+ years\n\n\nMajor skills required: SQL, C# or Python, any ETL tool\n\n\nProduct Development:\n\nWork with the business analysts to understand the high level business need and requirements;\nWork with operation team to understand issues and provide step-by-step solutions;\nImplement business logic using SQL;\nMonitor the system for any issues and quickly respond to emergencies;\nDeep detailed understanding of internal ETL tool;\nPrepare product for the release and drive the process;\nProficiency in query optimization (understanding query plans, improving execution time etc.);\nWrite scripts using internal code language (C# based) in order to optimize the process;\nUnderstand the overall process and data flows;\nPerform detailed analysis of the code and do research to help analysts understand current situation and make a decision.\n\nExperience and required skills:\n\nStrong understanding of relational databases;\nAdvanced SQL knowledge is required;\nWorking experience with MPP (MPP Massively Parallel Processing) Databases, understanding database design (data distribution, partitioning etc.);\nMedium Linux knowledge is required;\nC# or Python mid-level;\nExperience with analytic reporting tools such as SAP Business Object is preferred;\nAbility to work in a multi-cultured team environment;\nStrong oral and written communication skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL', 'C#', 'python', 'ETL']",2025-06-12 13:53:14
Data Scientist,Celebal Technologies,3 - 6 years,Not Disclosed,"['Mumbai', 'Navi Mumbai']","About Us: Celebal Technologies is a leading Solution Service company that provide Services the field of Data Science, Big Data, Enterprise Cloud & Automation. We are at the forefront of leveraging cuttingedge technologies to drive innovation and enhance our business processes. As part of our commitment to staying ahead in the industry, we are seeking a talented and experienced Data & AI Engineer with strong Azure cloud competencies to join our dynamic team.\n\nJob Summary: We are looking for a highly skilled Data Scientist with deep expertise in time series forecasting, particularly in demand forecasting and customer lifecycle analytics (CLV). The ideal candidate will be proficient in Python or PySpark, have hands-on experience with tools like Prophet and ARIMA, and be comfortable working in Databricks environments. Familiarity with classic ML models and optimization techniques is a plus.\n\nKey Responsibilities\n• Develop, deploy, and maintain time series forecasting models (Prophet, ARIMA, etc.) for demand forecasting and customer behavior modeling.\n• Design and implement Customer Lifetime Value (CLV) models to drive customer retention and engagement strategies.\n• Process and analyze large datasets using PySpark or Python (Pandas).\n• Partner with cross-functional teams to identify business needs and translate them into data science solutions.\n• Leverage classic ML techniques (classification, regression) and boosting algorithms (e.g., XGBoost, LightGBM) to support broader analytics use cases.\n• Use Databricks for collaborative development, data pipelines, and model orchestration.\n• Apply optimization techniques where relevant to improve forecast accuracy and business decision-making.\n• Present actionable insights and communicate model results effectively to technical and non-technical stakeholders.\n\nRequired Qualifications\n• Strong experience in Time Series Forecasting, with hands-on knowledge of Prophet, ARIMA, or equivalent Mandatory.\n• Proven track record in Demand Forecasting Highly Preferred.\n• Experience in modeling Customer Lifecycle Value (CLV) or similar customer analytics use cases Highly Preferred.\n• Proficiency in Python (Pandas) or PySpark Mandatory.\n• Experience with Databricks Mandatory.\n• Solid foundation in statistics, predictive modeling, and machine learning",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning Operations', 'Demand Forecasting', 'Data Bricks', 'Pyspark', 'Large Language Model', 'Time Series', 'Spark', 'Machine Learning', 'Python']",2025-06-12 13:53:16
Senior Software Engineer-7916,WebMD,5 - 10 years,Not Disclosed,['Navi Mumbai'],"Position: Senior Software Engineer (Data Engineer)\nNo. of Positions: 1\nAbout WebMD:\nHeadquartered in El Segundo, Calif., Internet Brands is a fully integrated online media and software services\norganization focused on four high-value vertical categories: Health, Automotive, Legal, and Home/Travel. The\ncompanys award-winning consumer websites lead their categories and serve more than 250 million monthly\nvisitors, while a full range of web presence offerings has established deep, long-term relationships with SMB and\nenterprise clients. Internet Brands,powerful, proprietary operating platform provides the flexibility and\nscalability to fuel the companys continued growth. Internet Brands is a portfolio company of KKR and Temasek.\nWebMD Health Corp., an Internet Brands Company, is the leading provider of health information services, serving\npatients, physicians, health care professionals, employers, and health plans through our public and private online\nportals, mobile platforms, and health-focused publications. The WebMD Health Network includes WebMD\nHealth, Medscape, Jobson Healthcare Information, prIME Oncology, MediQuality, Frontline, QxMD, Vitals\nConsumer Services, MedicineNet, eMedicineHealth, RxList, OnHealth, Medscape Education, and other owned\nWebMD sites. WebMD, Medscape, CME Circle, Medpulse, eMedicine®, MedicineNet®, theheart.org®, and\nRxList® are among the trademarks of WebMD Health Corp. or its subsidiaries.\nFor Company details, visit our website: www.webmd.com / www.internetbrands.com\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex,\nsexual orientation, gender identity, national origin, disability, or veteran status\nEducation: B.E. Computer Science/IT degree (or any other engineering discipline)\nExperience: 5+ years\nWork timings: 2:00 PM to 11:00 PM IST.\nDescription:\nWe are seeking an experienced and passionate Senior Software Developer to join our team. In this role,\nyou will work closely with cross-functional teams, developers, stakeholders, and business units to\ngather and analyze business requirements, design, build and implement ETL Solutions, and maintain\nthe infrastructure. The ideal candidate will have a strong background in business analysis, SQL, Unix,\nPython, ETL Tools to ensure the successful execution of projects.\nResponsibilities:\nLead requirements gathering sessions with key stakeholders to understand business needs and\nobjectives.\nCollaborate with and across Agile teams to design, develop, test, implement and support ETL\nprocesses for data transformation and preparation.\nManage data pipelines for analytics and operational use.\nEnsure data quality, data accuracy and integrity across multiple sources and systems.\nPerform unit tests and conduct reviews with other team members to make sure your code is\nrigorously designed, elegantly coded, and effectively tuned for performance.\nShould be able to come up with multiple approaches to any ETL\nproblem statement/solution/technical challenge and take well informed decision to pick the\nbest solution.\nAutomate ETL Processes using Cron and/or using Job Scheduler tools like AirFlow.\nAdhere to company standards and Serve as a key contributor to the design and development of\nexception handling, code/data standardization procedures, resolution steps and Quality Assurance\ncontrols.\nMaintain a version repository and ensure version control.\nCreate visual aids such as diagrams, charts, and screenshots to enhance documentation.\nWork with Infrastructure/systems team and developers to ensure all modules are up-to- date and\nare compatible with the code.",,,,"['ETL', 'SQL', 'UNIX', 'Python']",2025-06-12 13:53:18
Data Scientist,Mindpro Technologies,4 - 9 years,5-12 Lacs P.A.,"['Karur', 'Dharwad']","Greetings From Mind Pro Technologies Pvt ltd (www.mindprotech.com)\n\nJob Title : Data Scientist\nWork Location : Karur (Tamil Nadu) or Dharwad (Karnataka )\nNp : 15days or Less\n\n\nJOB DESCRIPTION:\n Must have At least 4+ Years of experience in Python with Data Science.\n Must have worked on at least one Live project.\nExperience in relevant field such as Statistics, Computer Science or Applied Math or Operational Research.\nMust have Masters in (Maths/Statistics or Applied Mathematics/Machine Learning etc.)\nHistory of successfully performing customer implementations\nStrong customer facing skills, and previous consulting experience.\nExperience of handling high frequency streaming data for real time analysis and reporting.\nFamiliarity with - Natural Language Processing, Statistical Analysis (distribution analysis, correlation, variance, deep learning.\nExperience in tools like AWS, IBM Watson is a plus.\nExperience with open source technologies is a must.\nExcellent communication\nAbility to lead & build strong teams\nAbility to work in an ambiguous environment\n\nDesired Skills and Experience\nLanguages/Tools: Python/R.\nApproaches: Machine Learning\nConcepts: Supervised ANN, Bayesian, Gaussian, Vector Quantization, Logistic Model, Statistical, Predictive Modeling, Minimum Message Length, SVM, Random Forest, Ensembles, ANOVA, Decision Trees, Hidden Markov Models\nUnsupervised ANN, ARL, Clustering Hierarchical, Cluster Analysis\nReinforcement\nGen AI, LLM, LSTM, RNN, CNN, KNN\nBig Data (Good to have): Hadoop /Kafka / Storm / Spark streaming\nOS: Linux, Windows 32/64 bits.\n\nNote:  should know supervised and unsupervised learning,   semi-supervised learning, neural networks concepts, and how ML algorithms works with training and testing data. Experience on particular data set to train, test and roll-out for production use\n\nTool sets : Python, R, MATLAB or  any AI frame work, Neural network, Gen AI, LLM\nContact Details:\n\nRecruitment Team\nMindpro Technologies Pvt Ltd (www.mindprotech.com)\n+91-04324-240904 / +91-9600672304",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Gen AI', 'Statistical Modeling', 'LLM', 'Predictive Modeling', 'Artificial Intelligence', 'Natural Language Processing', 'Neural Networks', 'Machine Learning', 'Deep Learning', 'Python']",2025-06-12 13:53:21
"Senior Manager, Software Engineering",Diligent Corporation,10 - 15 years,Not Disclosed,['Bengaluru'],"Position Overview\nYou will work closely with technology and business teams to understand requirements, design robust architectures, and influence technology choices to deliver innovative solutions. In addition to leading SaaS software development, you will drive initiatives in Data Engineering, Data Warehousing, and Artificial Intelligence (AI). You will collaborate with principal engineers and leadership, and have opportunities to cross-collaborate with inter-disciplinary teams to solve unique challenges in the GRC & ESG domain.\nKey Responsibilities\nShape the product and technical vision for the team, collaborating with product, business, and engineering leadership across the company.\nManage and mentor a team of engineers developing highly scalable, performant, maintainable, and well-tested SaaS features.\nLead the design and implementation of modern data warehouse solutions, ensuring data quality, scalability, and security.\nOversee the integration of advanced AI and machine learning models into SaaS products to deliver intelligent features and insights.\nHire, mentor, and lead a world-class group of engineers with expertise in SaaS, data engineering, and AI.\nFoster a culture of innovation, experimentation, and continuous improvement.\nEvaluate engineering requirements and design proposals, especially in the context of data-driven and AI-powered applications.\nAssess and develop the technical performance of individual contributors within the team.\nStay current with the latest frameworks and technologies in SaaS, Data Engineering, and AI, influencing technology choices for the application stack.\nFacilitate daily stand-ups, risk identification and mitigation, dependency resolution, and follow-ups for gap closure.\nPartner with Product Management and Business teams to drive the agenda, set priorities, create project plans, and deliver outstanding products.\nRequired Experience/Skills\n10 to 15 years of relevant experience in developing enterprise SaaS applications using MERN/.NET, MySQL, MS SQL, and caching technologies.\n2+ years of experience leading engineering teams building scalable platforms and architectures, including data engineering and AI initiatives.\nProven experience designing, building, and maintaining data warehouse solutions (such as Snowflake, Redshift, or BigQuery) and data pipelines (ETL/ELT).\nHands-on experience with AI/ML frameworks (such as TensorFlow, PyTorch, or Scikit-learn) and integrating AI models into production SaaS environments.\nStrong background in data modeling, data governance, and data quality best practices.\nExperience with cloud platforms (AWS/Azure), CI/CD, DevOps, scripting, and SQL/NoSQL databases.\nDemonstrated success in migrating monolithic applications to microservices and on-premises solutions to cloud environments.\nPassion for building a data-driven culture, growing talent, and making a significant impact through technology.\nStrong communication skills for engaging with end users, technical, and business teams to gather requirements and describe product features and technical designs.\nAbility to seek clarity in ambiguous situations and drive projects to completion.\nExperience in Agile development and knowledge of Scrum and Kanban methodologies.\nSelf-motivated learner and builder with a strong customer focus and a commitment to delivering high-quality solutions.",Industry Type: Design,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'MS SQL', 'NoSQL', 'Data modeling', 'MySQL', 'Machine learning', 'Scrum', 'Data quality', 'SQL', 'Recruitment']",2025-06-12 13:53:23
"Senior Manager, Engineering",Reltio,10 - 15 years,Not Disclosed,['Bengaluru'],"Job Summary:\nAre you passionate about data management and unifying complex datasets? Do you have a track record of leading successful data unification projects? Reltio is seeking a dynamic and experienced Senior Manager to join our Data Unification Team in India. As a Senior Manager, you will play a pivotal role in driving the development and execution of data unification strategies and initiatives, ensuring high-quality and accurate data for our clients.\nJob Duties and Responsibilities:\nLeadership: Provide strong leadership and guidance to the Data Unification Team, driving a culture of excellence, innovation, and collaboration.\nData Unification Strategy: Develop and implement data unification strategies, frameworks, and best practices to deliver effective data management solutions.\nTeam Management: Lead, mentor, and inspire a team of data engineers and analysts, fostering their professional growth and ensuring the teams success in meeting project goals.\nData Governance: Define and enforce data governance policies, standards, and procedures to ensure data quality, integrity, and security across all data unification activities.\nProject Management: Oversee end-to-end project management, including scoping, planning, resource allocation, and execution of data unification projects, ensuring timely delivery within budget and scope.\nStakeholder Collaboration: Collaborate with cross-functional teams, including Product Management, Engineering, and Customer Success, to align data unification initiatives with overall business objectives and customer requirements.\nContinuous Improvement: Identify areas for process improvement, automation, and optimization, driving efficiency and scalability in data unification operations.\nIndustry Trends: Stay updated with industry trends, emerging technologies, and best practices in data management and unification, leveraging this knowledge to drive innovation and enhance our offerings.\nSkills You Must Have:\nMinimum of 9+ years of experience in data management, data unification, or related fields, with a focus on managing large-scale data projects.\nStrong leadership and managerial skills, with a proven track record of successfully leading and motivating high-performing teams.\nIn-depth knowledge of data unification methodologies, tools, and technologies, including Master Data Management (MDM) and data integration techniques.\nSolid understanding of data governance principles, data quality frameworks, and data security best practices.\nExcellent project management skills, with the ability to manage multiple projects simultaneously, prioritize tasks, and meet deadlines.\nStrong analytical and problem-solving abilities, with the capacity to analyze complex data sets, identify patterns, and propose innovative solutions.\nEffective communication and stakeholder management skills, with the ability to collaborate and influence cross-functional teams and senior leadership.\nBachelors or Masters degree in Computer Science, Information Systems, or a related field.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Computer science', 'Automation', 'Team management', 'data security', 'Analytical', 'Process improvement', 'Data quality', 'Stakeholder management', 'Analytics']",2025-06-12 13:53:26
Data Stage Developer,Anblicks Solutions,6 - 11 years,Not Disclosed,['Chennai'],"Job Summary:\nWe are looking for a seasoned ETL Engineer with hands-on experience in Talend or IBM DataStage, preferably both, to lead data integration efforts in the mortgage domain. The ideal candidate will play a key role in designing, developing, and managing scalable ETL solutions that support critical mortgage data processing and analytics workloads.\n\nKey Responsibilities:\nEnd-to-end ETL solution development using Talend or DataStage.Design and implement robust data pipelines for mortgage origination, servicing, and compliance data.Collaborate with business stakeholders and data analysts to gather requirements and deliver optimized solutions.Perform code reviews, mentor junior team members, and ensure adherence to data quality and performance standards.Manage job orchestration, scheduling, and error handling mechanisms.Document ETL workflows, data dictionaries, and system processes.Ensure data privacy and compliance requirements are embedded in all solutions.\n\nRequired Skills:\nStrong experience in ETL tools Talend (preferred) or IBM DataStage.Solid understanding of mortgage lifecycle and related data domains.Proficiency in SQL and experience with relational databases (e.g., Oracle, SQL Server, Snowflake).Familiarity with job scheduling tools, version control, and CI/CD pipelines.Excellent problem-solving, leadership, and communication skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Datastage', 'Ibm Datastage', 'Etl Datastage']",2025-06-12 13:53:28
Data Protection Officer (DPO) / GRC Officer,Fynd (Shopsense Retail Technologies Ltd.),5 - 10 years,Not Disclosed,['Mumbai'],"Fynd is India s largest omnichannel platform and multi-platform tech company with expertise in retail tech and products in AI, ML, big data ops, gaming+crypto, image editing and learning space. Founded in 2012 by 3 IIT Bombay alumni: Farooq Adam, Harsh Shah and Sreeraman MG. We are headquartered in Mumbai and have 1000+ brands under management, more than 10k stores and servicing 23k + pin codes.\n\nWe are seeking a highly skilled Data Protection Officer (DPO) / GRC Officer responsible for ensuring compliance with global security and data protection regulations. The ideal candidate will oversee governance, risk, and compliance (GRC) programs, implement security frameworks, and safeguard sensitive data across the organization.\n\nWhat will you do at Fynd ?\n\n1. Governance, Risk, and Compliance (GRC):\nDevelop, implement, and maintain GRC frameworks to align with regulatory and industry standards.\nEstablish risk assessment methodologies and ensure mitigation strategies are in place.\nConduct IT General Controls (ITGC) assessments to ensure effective security controls and processes.\nOversee third-party risk assessments, ensuring vendors comply with security policies.\n2. Data Protection & Privacy Compliance:\nImplement and oversee compliance with DPDP (Digital Personal Data Protection Act, India) and GDPR regulations.\nAct as the point of contact for data protection authorities and internal privacy matters.\nConduct Data Protection Impact Assessments (DPIAs) and privacy risk assessments.\nDevelop and enforce privacy policies, data retention, and protection measures.\n3. Information Security Compliance & Certifications:\nLead and maintain compliance with ISO 27001, ensuring policies and controls meet certification requirements.\nManage SOC 2 compliance efforts, including security, availability, processing integrity, confidentiality, and privacy principles.\nOversee PCI-DSS compliance for handling cardholder data securely.\nEnsure alignment with NIST security frameworks for risk management and cybersecurity resilience.\n4. Business Continuity & Incident Management:\nDevelop and maintain a Business Continuity Management (BCM) program, including disaster recovery plans.\nLead security incident response and investigations to mitigate data breaches and cybersecurity threats.\nConduct regular tabletop exercises and audits to test resilience and readiness.\n\nSome Specific Requirements\nBachelor s/Master s degree in Information Security, Cybersecurity, Compliance, or a related field.\nProfessional certifications such as CIPP/E, CIPM, CISSP, CISM, CISA, ISO 27001 Lead Auditor, or CRISC are highly preferred.\n5+ years of experience in Data Protection, Compliance, GRC, or Cybersecurity roles.\nStrong knowledge of regulatory frameworks (SOC2, ISO27001, GDPR, DPDP, PCI-DSS, NIST, ITGC, Third-Party Risk Management).\nExperience in implementing GRC tools and automating compliance processes.\nExcellent stakeholder management skills with the ability to work cross-functionally.\nStrong analytical, problem-solving, and decision-making skills.\n\nWhat do we offer?\n\nGrowth\nGrowth knows no bounds, as we foster an environment that encourages creativity, embraces challenges, and cultivates a culture of continuous expansion. We are looking at new product lines, international markets and brilliant people to grow even further. We teach, groom and nurture our people to become leaders. You get to grow with a company that is growing exponentially.\n\nFlex University\nWe help you upskill by organising in-house courses on important subjects\nLearning Wallet: You can also do an external course to upskill and grow, we reimburse it for you.\n\nCulture\nCommunity and Team building activities\nHost weekly, quarterly and annual events/parties.\n\nWellness\nMediclaim policy for you + parents + spouse + kids\nExperienced therapist for better mental health, improve productivity & work-life balance\n\nWe work from the office 5 days a week to promote collaboration and teamwork. Join us to make an impact in an engaging, in-person environment!",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Cism', 'security compliance', 'Cisa', 'Information security', 'SOC', 'Risk assessment', 'Disaster recovery', 'Flex', 'Incident management', 'Risk management']",2025-06-12 13:53:30
Senior Software Engineer,Joules to Watts,4 - 6 years,2-6 Lacs P.A.,['Bengaluru( Whitefield )'],"Only for Immediate Joiners\nCore Responsibility:\nThe project team will be spread between Paris and Bangalore. So, the candidate with an experience of 3-6 years is expected to work and coordinate on daily basis with the remote teams. Ability to learn new technology / framework / methodology.\n    Hands-on individual responsible for producing excellent quality of code, adhering to expected coding standards and industry best practices. \n    Must have strong knowledge and working experience on Big DATA ecosystem.\n    Must have strong experience in SPARK/SCALA, NIFI, KAFKA, HIVE, PIG.\n   Strong knowledge and experience working on HQL (hive Query Language)\n•    Must have strong strong expertise in Debugging and Fixing Production Issues on BIG DATA eco System.\n    Knowledge on code version management using Git & Jenkins, Nexus.\n•    High levels of ownership and commitment on deliverables. Strong and Adaptive Communication Skills; Should be comfortable interacting with Paris counterparts  to probe a technical problem or clarify requirement specifications. \n\nKEY SKILLS:\nSound knowledge on SPARK/SCALA, NIFI, KAFKA - Must Have\nSound Knowledge on HQL\nKnowledge on Kibana, Elastic Search Log stash Good to know\nBasic Awareness of CD/CI concepts & Technologies\nBig Data Ecosystem Good to know",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Hive', 'Apache Pig', 'Hadoop', 'SCALA', 'Kafka', 'Spark', 'Apache Nifi']",2025-06-12 13:53:32
Senior Lead business execution consultant,Wells Fargo,7 - 12 years,Not Disclosed,['Bengaluru'],"About this role:\nWells Fargo is seeking a Senior Lead business execution consultant\n\nIn this role, you will:\nAct as a Business Execution advisor to leadership to drive performance and initiatives, and develop and implement information delivery or presentations to key stakeholders and senior management",,,,"['Business execution', 'Business Implementation', 'Data Engineering', 'NLP', 'generative AI', 'Data Mining', 'machine learning', 'Strategic Planning', 'agentic AI']",2025-06-12 13:53:35
Sr. Associate Full Stack Software Engineer,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\n\n\nIn this vital role you will be responsible for designing, developing, and maintaining software applications and solutions that meet business needs and ensuring the availability and performance of critical systems and applications. This role involves working closely with product managers, designers, data engineers, and other engineers to create high-quality, scalable software solutions and automating operations, monitoring system health, and responding to incidents to minimize downtime.\n\nYou will play a key role in a regulatory submission content automation initiative which will modernize and digitize the regulatory submission process, positioning Amgen as a leader in regulatory innovation. The initiative demonstrates innovative technologies, including Generative AI, Structured Content Management, and integrated data to automate the creation, and management of regulatory content.\n\n\n\nRoles & Responsibilities:\nPossesses strong rapid prototyping skills and can quickly translate concepts into working code\nContribute to both front-end and back-end development using cloud technology\nDevelop innovative solution using generative AI technologies\nEnsure code quality and consistency to standard methodologies\nCreate and maintain documentation on software architecture, design, deployment, disaster recovery, and operations\nIdentify and resolve technical challenges effectively\nStay updated with the latest trends and advancements\nWork closely with product team, business team, and other collaborators\nDesign, develop, and implement applications and modules, including custom reports, interfaces, and enhancements\nAnalyze and understand the functional and technical requirements of applications, solutions and systems and translate them into software architecture and design specifications\nDevelop and implement unit tests, integration tests, and other testing strategies to ensure the quality of the software\nIdentify and resolve software bugs and performance issues\nWork closely with multi-functional teams, including product management, design, and QA, to deliver high-quality software on time\nCustomize modules to meet specific business requirements\nWork on integrating with other systems and platforms to ensure seamless data flow and functionality\nProvide ongoing support and maintenance for applications, ensuring that they operate smoothly and efficiently\n\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients.\nMasters degree and 1 to 3 years of experience in Computer Science, IT or related field OR\nBachelors degree and 3 to 5 years of experience in Computer Science, IT or related field OR\nDiploma and 7 to 9 years of experience in Computer Science, IT or related field\nPreferred Qualifications:\n\n\n\nFunctional\n\nSkills:\nMust-Have Skills:\nProficiency in Python/PySpark development, Fast API, PostgreSQL, Databricks, DevOps Tools, CI/CD, Data Ingestion.\nCandidates should be able to write clean, efficient, and maintainable code.\nKnowledge of HTML, CSS, and JavaScript, along with popular front-end frameworks like React or Angular, is required to build interactive and responsive web applications\nIn-depth knowledge of data engineering concepts, ETL processes, and data architecture principles. Solid understanding of cloud computing principles, particularly within the AWS ecosystem\nSolid understanding of software development methodologies, including Agile and Scrum\nExperience with version control systems like Git\nHands on experience with various cloud services, understand pros and cons of various cloud service in well architected cloud design principles\nStrong problem solving, analytical skills; Ability to learn quickly; Good communication and interpersonal skills\nExperienced with API integration, serverless, microservices architecture.\nExperience in SQL/NOSQL database, vector database for large language models\n\n\n\nGood-to-Have\n\nSkills:\nSolid understanding of cloud platforms (e.g., AWS, GCP, Azure) and containerization technologies (e.g., Docker, Kubernetes)\nExperience with monitoring and logging tools (e.g., Prometheus, Grafana, Splunk)\nExperience with data processing tools like Hadoop, Spark, or similar\n\n\n\nSoft\n\nSkills:\nExcellent analytical and solving skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'PySpark development', 'Data Ingestion', 'PostgreSQL', 'Fast API', 'CI/CD', 'DevOps Tools', 'Databricks']",2025-06-12 13:53:37
Senior Software Engineer Python,Growexx,5 - 10 years,Not Disclosed,['Ahmedabad'],"built on top of the Revelation Open Insights analytics platform. This role is ideal for an engineer who thrives in complex, data-intensive environments and has a passion for modernizing lega operational stability. Key Responsibilities\nLead the maintenance, enhancement, and refactoring of the Python-based Legacy Registry system.\nCollaborate with data engineers, DevOps, and platform architects to ensure seamless integration with Revelation Open Insights.\nAnalyze and optimize legacy code for performance, scalability, and maintainability.\nDesign and implement automated testing strategies and CI/CD pipelines for legacy services.\nTroubleshoot and resolve production issues, ensuring high system availability and data integrity.\nDocument system architecture, workflows, and technical decisions to support long-term maintainability.\nParticipate in roadmap planning and contribute to modernization strategies.\nKey Skills\nDeep understanding of legacy system architecture, data modelling, and refactoring techniques.\nExperience working with SQL databases (e.g., PostgreSQL, MySQL) and data integration pipelines.\nFamiliarity with containerization (Docker), orchestration (Kubernetes), and CI/CD tools (e.g., GitHub Actions, Jenkins).\nStrong debugging, profiling, and performance tuning skills.\nExperience with enterprise data platforms or analytics systems.\nFamiliarity with the Revelation Open Insights platform or similar data intelligence tools.\nExposure to data governance, metadata management, or registry systems.\nEducation and Experience\nB.Tech or B.E. or MCA or BCA\n5+ years of professional experience in Python development, with a strong focus on backend systems\nAnalytical and Personal Skills Must have good logical reasoning and analytical skills\nAbility to break big goals to small incremental actions\nExcellent Communication and collaboration skills Demonstrate Ownership and Accountability of their work\nGreat attention to details\nDemonstrate ownership of tasks Positive and Cheerful outlook in life Work with the problem solver engineers team (Doc / PDF Only, Max file size 2 MB) By using this form you agree with the storage and handling of your data by this website. *\nYou cannot copy content of this page\nReconciliation Automation Data Sheet\nThis field is for validation purposes and should be left unchanged.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'System architecture', 'Automation', 'metadata', 'Postgresql', 'MySQL', 'Debugging', 'Analytics', 'SQL', 'Python']",2025-06-12 13:53:39
Specialist Data Scientist,Atlasrtx,3 - 7 years,Not Disclosed,['Pune'],"So, what s the role all about\n\nNICE provides state-of-the-art enterprise level AI and analytics for all forms of business communications between speech and digital. We are a world class research team developing new algorithms and approaches to help companies with solving critical issues such as identifying their best performing agents, preventing fraud, categorizing customer issues, and determining overall customer satisfaction. If you have interacted with a major contact center in the last decade, it is very likely we have processed your call.\n\nThe research group partners with all areas of NICE s business to scale out the delivery of new technology and AI models to customers around the world that are tailored to their company, industry, and language needs.\n\n\nHow will you make an impact\n\nConduct cutting-edge research and develop advanced NLP algorithms and models.\n\nBuild and fine-tune deep learning and machine learning models, with a focus on large language models.\n\nWork closely with internal stakeholders to define model requirements and ensure alignment with business objectives.\n\nDevelop AI predictive models and perform data and model accuracy analyses.\n\nProduce and present findings, technical concepts, and model recommendations to both technical and non-technical stakeholders.\n\nDevelop and maintain scripts/tools to automate both new model production and updates to existing model packages.\n\nStay abreast of the latest advancements in data science research and contribute to the development of our knowledge base.\n\nCollaborate with developers to design automation and tool improvements for model building.\n\nMaintain documentation of processes and projects across all supported languages and environments.\n\n\nHave you got what it takes\n\nMasters degree in the field of Computer Science, Technology, Engineering, Math, or equivalent practical experience\n\nMinimum of 8 years of data science work experience, including implementing machine learning and NLP models using real-life data.\n\nExperience with Retrieval-Augmented Generation (RAG) pipelines or LLMOps.\n\nAdvanced knowledge of statistics and machine learning algorithms.\n\nProficiency in Python programming and familiarity with R.\n\nExperience with deep learning models and libraries such as PyTorch, TensorFlow, and JAX.\n\nFamiliarity with relational databases and query languages (e. g. , MSSQL) and basic SQL knowledge.\n\nHands-on experience with transformer models (BERT, FlanT5, Llama, etc. ) and GenAI frameworks (HuggingFace, LangChain, Ollama, etc. ).\n\nExperience deploying NLP models in production environments, ensuring scalability and performance using AWS/GCP/Azure\n\nStrong verbal and written communication skills, including effective presentation abilities.\n\nAbility to work independently and as part of a team, demonstrating analytical thinking and problem-solving skills.\n\n\n\nYou will have an advantage if you also have:\n\nExpertise with Big Data technologies (e. g. , PySpark).\n\nBackground in knowledge graphs, graph databases, or GraphRAG architectures.\n\nUnderstanding of multimodal models (text, audio, vision).\n\nExperience in Customer Experience domains.\n\nExperience with package development and technical writing.\n\nFamiliarity with tools like Jira, Confluence, and source control packages and methodology.\n\nKnowledge and interest in foreign languages and linguistics.\n\nExperience working on international, globe-spanning teams and with AWS.\n\nPast participation in a formal research setting.\n\nExperience as part of a software organization.\n\n\n\nWhat s in it for you\n\n\n\nEnjoy NICE-FLEX!\n\n\n\nRequisition ID : 7481\nReporting into : Tech Manager\nRole Type : Individual Contributor\n\nAbout NICE",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Technical writing', 'GCP', 'Analytical', 'Machine learning', 'Flex', 'Analytics', 'SQL', 'Python']",2025-06-12 13:53:41
Lead ML Ops Engineer with GCP,TVS Next,8 - 10 years,Not Disclosed,['Bengaluru'],"What you’ll be doing:\nAssist in developing machine learning models based on project requirements\nWork with datasets by preprocessing, selecting appropriate data representations, and ensuring data quality.\nPerforming statistical analysis and fine-tuning using test results.\nSupport training and retraining of ML systems as needed.\nHelp build data pipelines for collecting and processing data efficiently.",,,,"['hive', 'kubernetes', 'data pipeline', 'sql', 'docker', 'tensorflow', 'java', 'product management', 'gcp', 'spark', 'pytorch', 'bigquery', 'hadoop', 'big data', 'programming', 'hbase', 'ml', 'cloud sql', 'python', 'airflow', 'cloud spanner', 'cloud pubsub', 'machine learning', 'data engineering', 'ops', 'mapreduce', 'kafka', 'cloud storage', 'hdfs', 'bigtable', 'aws']",2025-06-12 13:53:43
Quality Assurance - ETL QA Engineer - Lead,Kearney-Cervello,6 - 10 years,Not Disclosed,['Bengaluru'],"About the Role:\nAs a Lead ETL QA Engineer, you will drive the QA strategy and execution for a major data pipeline modernization initiative on Azure and Snowflake. This role requires a deep understanding of data quality frameworks, test planning, and stakeholder engagement. The candidate should possess leadership capabilities and be hands-on with SQL and automation.\n\nThe job responsibilities are as follows:",,,,"['Azure Data Factory', 'Quality Assurance', 'ETL Testing', 'SQL', 'Database Testing', 'Sql Database Testing']",2025-06-12 13:53:46
Senior JavaScript Software Engineer,Ciklum,6 - 10 years,Not Disclosed,"['Pune', 'Chennai']","Ciklum is looking for a Senior JavaScript Software Engineer to join our team full-time in India.\nWe are a custom product engineering company that supports both multinational organizations and scaling startups to solve their most complex business challenges. With a global team of over 4,000 highly skilled developers, consultants, analysts and product owners, we engineer technology that redefines industries and shapes the way people live.\n\nAbout the role:\nAs a Senior JavaScript Software Engineer, become a part of a cross-functional development team engineering experiences of tomorrow.\nClient for this project is a leading global provider of audit and assurance, consulting, financial advisory, risk advisory, tax, and related services. They are launching a digital transformation project to evaluate existing technology across the tax lifecycle and determine the best future state for that technology. This will include decomposing existing assets to determine functionality, assessment of those functionalities to determine the appropriate end state and building of new technologies to replace those functionalities.\n\nResponsibilities:\nParticipate in requirements analysis\nCollaborate with US and Vendors teams to produce software design and architecture\nWrite clean, scalable code using Angular with Typescript, HTML, CSS, and NET programming languages\nParticipate in pull request code review process\nTest and deploy applications and systems\nRevise, update, refactor and debug code\nDevelop, support and maintain applications and technology solutions\nEnsure that all development efforts meet or exceed client expectations. Applications should meet requirements of scope, functionality, and time and adhere to all defined and agreed upon standards\nBecome familiar with all development tools, testing tools, methodologies and processes\nBecome familiar with the project management methodology and processes\nEncourage collaborative efforts and camaraderie with on-shore and off-shore team members\nDemonstrate a strong working understanding of the best industry standards in software development and version controlling\nEnsure the quality and low bug rates of code released into production\nWork on agile projects, participate in daily SCRUM calls and provide task updates\nDuring design and key development phases, we might need to work a staggered shift as applicable to ensure appropriate overlap with US teams and project deliveries\n\nRequirements:\n  We know that sometimes, you cant tick every box. We would still love to hear from you if you think you will be a good fit\n6+ years of strong hands-on experience with JavaScript (ES6/ES2015+), HTML5, CSS3\n2+ years with hands-on experience with Typescript\n2+ years of hands-on experience with Angular 11+ component architecture, applying design patterns\nExperience with Angular 11+ and migrating to newer versions\nExperience with Angular State management or NgXs\nExperience with RxJS operators\nHands on experience with Kendo UI or Angular material or SpreadJS libraries\nExperience with Nx – Nrwl/Nx library for monorepos\nSkill for writing reusable components, Angular services, directives and pipes\nHands-on experience on C#, SQL Server, OOPS Concepts, Micro Services Architecture\nAt least two-year hands-on experience on .NET Core, ASP.NET Core Web API, SQL, NoSQL, Entity Framework 6 or above, Azure, Database performance tuning, Applying Design Patterns, Agile\n.Net back-end development with data engineering expertise. Experience with MS Fabric as a data platform/ Snowflake or similar tools would be a plus, but not a must need\nSkill for writing reusable libraries\nComfortable with Git & Git hooks using PowerShell, Terminal or a variation thereof\nFamiliarity with agile development methodologies\nExcellent Communication skills both oral & written\nExcellent troubleshooting and communication skills, ability to communicate clearly with US counterparts\n\nDesirable:\nExposure to micro-frontend architecture\nKnowledge on Yarn, Webpack, Mongo DB, NPM, Azure Devops Build/Release configuration\nSignalR, ASP.NET Core and WebSockets\nThis is an experienced level position, and we will train the qualified candidate in the required applications\nWillingness to work extra hours to meet deliverables\nExposure to Application Insights & Adobe Analytics\nUnderstanding of cloud infrastructure design and implementation\nExperience in CI/CD configuration\nGood knowledge of data analysis in enterprises\nExperience with Databricks, Snowflake\nExposure to Docker and its configurations, Experience with Kubernetes\n\nWhat's in it for you?\nCare: your mental and physical health is our priority. We ensure comprehensive company-paid medical insurance, as well as financial and legal consultation\nTailored education path: boost your skills and knowledge with our regular internal events (meetups, conferences, workshops), Udemy licence, language courses and company-paid certifications\nGrowth environment: share your experience and level up your expertise with a community of skilled professionals, locally and globally\nFlexibility: hybrid work mode at Chennai or Pune \nOpportunities: we value our specialists and always find the best options for them. Our Resourcing Team helps change a project if needed to help you grow, excel professionally and fulfil your potential\nGlobal impact: work on large-scale projects that redefine industries with international and fast-growing clients\nWelcoming environment: feel empowered with a friendly team, open-door policy, informal atmosphere within the company and regular team-building events\n\nAbout us:\nAt Ciklum, we are always exploring innovations, empowering each other to achieve more, and engineering solutions that matter. With us, you’ll work with cutting-edge technologies, contribute to impactful projects, and be part of a One Team culture that values collaboration and progress.\nIndia is a strategic innovation hub for Ciklum, with growing teams in Chennai and Pune leading advancements in EdgeTech, AR/VR, IoT, and beyond. Join us to collaborate on game-changing solutions and take your career to the next level.\nWant to learn more about us? Follow us on Instagram, Facebook, LinkedIn\n\nExplore, empower, engineer with Ciklum!\nExperiences of tomorrow. Engineered together\nInterested already?\nWe would love to get to know you! Submit your application. Can’t wait to see you at Ciklum.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Typescript', 'Angular', 'CSS', '.Net', 'HTML']",2025-06-12 13:53:48
Sr Software Engineer: Integration Engineer,HMH,5 - 7 years,Not Disclosed,['Pune'],"The Data Integration Engineer will play a key role in designing, building, and maintaining data integrations between core business systems such as Salesforce and SAP and our enterprise data warehouse on Snowflake. This position is ideal for an early-career professional (1 to 4 years of experience) eager to contribute to transformative data integration initiatives and learn in a collaborative, fast-paced environment.\n\nDuties & Responsibilities:\nCollaborate with cross-functional teams to understand business requirements and translate them into data integration solutions.\nDevelop and maintain ETL/ELT pipelines using modern tools like Informatica IDMC to connect source systems to Snowflake.\nEnsure data accuracy, consistency, and security in all integration workflows.\nMonitor, troubleshoot, and optimize data integration processes to meet performance and scalability goals.\nSupport ongoing integration projects, including Salesforce and SAP data pipelines, while adhering to best practices in data governance.\nDocument integration designs, workflows, and operational processes for effective knowledge sharing.\nAssist in implementing and improving data quality controls at the start of processes to ensure reliable outcomes.\nStay informed about the latest developments in integration technologies and contribute to team learning and improvement.",,,,"['GCP', 'Azure', 'IDMC', 'XML', 'CSV', 'JSON', 'SQL Server', 'AWS', 'data integration', 'data engineering']",2025-06-12 13:53:51
Data Model Architect,Prodapt Solutions,6 - 10 years,Not Disclosed,['Chennai'],"Overview\n\nProdapt is looking for a Data Model Architect. The candidate should be good with design and data architecture in Telecom domain.    \n\n\nResponsibilities\n\n Deliverables  \nDesign & Document – Data Model for CMDB, P-S-R Catalogue (Product, Service and Resource management layer)\nDesign & Document Build Interface Speciation for Data Integration.\n\n Activities  \n\n Data Architecture and Modeling  \nDesign and maintain conceptual, logical, and physical data models\nEnsure scalability and adaptability of data models for future organizational needs.\n\n Data Model   P-S-R catalogs   in the existing Catalogs,SOM,COM systems\n\n CMDB Design and Management  \nArchitect and optimize the CMDB to accurately reflect infrastructure components, telecom assets, and their relationships.\nDefine data governance standards and enforce data consistency across the CMDB.\n\n Design    data integrations   between across systems (e.g., OSS/BSS, network monitoring tools, billing systems).\n\n\nGood Communication skills.\n\nBachelors Degree.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data warehousing', 'data architecture', 'sql', 'data governance', 'data integration', 'python', 'oracle', 'performance tuning', 'power bi', 'microsoft azure', 'erwin', 'business intelligence', 'sql server', 'plsql', 'data quality', 'tableau', 'data modeling', 'dwbi', 'etl', 'ssis', 'aws', 'informatica', 'unix']",2025-06-12 13:53:53
Senior High Performance Computing Engineer,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will.\nRole Description:\nThe role is responsible for the design, integration, and management of high performance computing (HPC) systems that encompass both hardware and software components into the organizations network infrastructure. This individual will be responsible for all activities related to handling and supporting the Business and platforms including system administration, as well as incorporating new technologies under the challenge of a sophisticated and constantly evolving technology landscape. This role involves ensuring that all parts of a system work together seamlessly to meet the organizations requirements.\nRoles & Responsibilities:\nImplement, and manage cloud-based infrastructure that supports HPC environments that support data science (e.g. AI/ML workflows, Image Analysis).\nCollaborate with data scientists and ML engineers to deploy scalable machine learning models into production.\nEnsure the security, scalability, and reliability of HPC systems in the cloud.\nOptimize cloud resources for cost-effective and efficient use.\nKeep abreast of the latest in cloud services and industry standard processes.\nProvide technical leadership and guidance in cloud and HPC systems management.\nDevelop and maintain CI/CD pipelines for deploying resources to multi-cloud environments.\nMonitor and fix cluster operations/applications and cloud environments.\nDocument system design and operational procedures.\nBasic Qualifications:\nMasters degree with a 4 - 6 years of experience in Computer Science, IT or related field with hands-on HPC administration OR\nBachelors degree with 6 - 8 years of experience in Computer Science, IT or related field with hands-on HPC administration OR\nDiploma with 10-12 years of experience in Computer Science, IT or related field with hands-on HPC administration\nDemonstrable experience in cloud computing (preferably AWS) and cloud architecture.\nExperience with containerization technologies (Singularity, Docker) and cloud-based HPC solutions.\nExperience with infrastructure-as-code (IaC) tools such as Terraform, CloudFormation, Packer, Ansible and Git.\nExpert with scripting (Python or Bash) and Linux/Unix system administration (preferably Red Hat or Ubuntu).\nProficiency with job scheduling and resource management tools (SLURM, PBS, LSF, etc.).\nKnowledge of storage architectures and distributed file systems (Lustre, GPFS, Ceph).\nUnderstanding of networking architecture and security best practices.\nPreferred Qualifications:\nExperience supporting research in healthcare life sciences.\nExperience with Kubernetes (EKS) and service mesh architectures.\nKnowledge of AWS Lambda and event-driven architectures.\nExposure to multi-cloud environments (Azure, GCP).\nFamiliarity with machine learning frameworks (TensorFlow, PyTorch) and data pipelines.\nCertifications in cloud architecture (AWS Certified Solutions Architect, Google Cloud Professional Cloud Architect, etc.).\nExperience in an Agile development environment.\nPrior work with distributed computing and big data technologies (Hadoop, Spark).\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nRed Hat Certified Engineer (RHCE) or Linux Professional Institute Certification (LPIC)\nAWS Certified Solutions Architect Associate or Professional\nSoft Skills:\nStrong analytical and problem-solving skills.\nAbility to work effectively with global, virtual teams\nEffective communication and collaboration with cross-functional teams.\nAbility to work in a fast-paced, cloud-first environment.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cloud computing', 'resource management', 'Ubuntu', 'Unix system administration', 'linux', 'unix production support', 'Python']",2025-06-12 13:53:56
Machine Learning Engineer,Avani Infosoft,1 - 2 years,2.4-6.6 Lacs P.A.,['Bengaluru( Kamakshipalya )'],"Responsibilities:\n* Develop machine learning models using TensorFlow, NumPy & OpenCV.\n* Implement computer vision solutions with CNNs & object detection techniques.\n\n\nProvident fund",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Handling', 'Object Detection', 'Opencv', 'Deployment', 'Model Development', 'Tensorflow', 'Cnn', 'Computer Vision', 'Machine Learning', 'Numpy', 'Deep Learning']",2025-06-12 13:53:58
Engineer,Reltio,2 - 7 years,Not Disclosed,['Bengaluru'],"Job Title: Engineer\nLocation: Bengaluru, India - (Hybrid)\nAt Reltio , we believe data should fuel business success. Reltio s AI-powered data unification and management capabilities encompassing entity resolution, multi-domain master data management (MDM), and data products transform siloed data from disparate sources into unified, trusted, and interoperable data. Reltio Data Cloud delivers interoperable data where and when its needed, empowering data and analytics leaders with unparalleled business responsiveness. Leading enterprise brands across multiple industries around the globe rely on our award-winning data unification and cloud-native MDM capabilities to improve efficiency, manage risk and drive growth.\nAt Reltio, our values guide everything we do. With an unyielding commitment to prioritizing our Customer First , we strive to ensure their success. We embrace our differences and are Better Together as One Reltio. We are always looking to Simplify and Share our knowledge when we collaborate to remove obstacles for each other. We hold ourselves accountable for our actions and outcomes and strive for excellence. We Own It . Every day, we innovate and evolve, so that today is Always Better Than Yesterday . If you share and embody these values, we invite you to join our team at Reltio and contribute to our mission of excellence.\nReltio has earned numerous awards and top rankings for our technology, our culture and our people. Reltio was founded on a distributed workforce and offers flexible work arrangements to help our people manage their personal and professional lives. If you re ready to work on unrivaled technology where your desire to be part of a collaborative team is met with a laser-focused mission to enable digital transformation with connected data, let s talk!\nJob Summary:\nCore Platform development is spread across multiple cross-functional teams, each building large and complex components of the MDM platform. Engineer plays senior role in a team is technically responsible for particular feature delivery which consists of:\nPresenting a solution to Architects\nTechnical drive feature delivery, make decisions, development leadership - distribute tasks across one or two regular engineers\nWork with QA to review testing approaches, provide all details needed for testing, and review test plans\nBe responsible for all internal feature documentation\nJob Duties and Responsibilities:\nWork closely with Tech leads and architects, Kanban master, QA and PM to deliver high-quality work\nComplex features development\nProvide good code coverage in unit and integration testing\nBe ready for every day technical decisions\nProfessional communication with other cross-functional team representatives\nBe ready for sustenance in areas under engineers responsibility\nSkills You Must Have:\n2+ years of experience in enterprise application design, development\n2+ Experience in backend applications development\nProven ability to deliver solutions, offer implementation support, and engage with customers.\n1+ years of experience in building scalable distributed data systems using Java and Cloud Technology.\nGood technical background with previous development experience for enterprise customers\nStrong Java knowledge (Java 8 and above), Java Runtime basics\nProvide Technical mentorship and leadership to junior team members.\nHands-on technically, comfortable reviewing and writing code.\nSolid foundation in computer science, with strong competencies in algorithms, data structures, software design, and building large, distributed systems\nExperience in performance optimisations, code profiling, and Java application runtime analysis\nSkills That Are Nice to Have:\nExperience in customer support in the SaaS area\nExperience in building scalable distributed data systems using Cloud Technology\nTechnical background with previous development experience for enterprise customers\nExperience with Big Data Technologies\nExperience with No-SQL databases\nExperience with cloud-managed services - AWS, GBT, Azure\nMS or equivalent experience\nWhy Join Reltio?*\nHealth & Wellness:\nComprehensive Group medical insurance including your parents with additional top-up options.\nAccidental Insurance\nLife insurance\nFree online unlimited doctor consultations\nAn Employee Assistance Program (EAP)\nWork-Life Balance:\n36 annual leaves, which includes Sick leaves - 18, Earned Leaves - 18\n26 weeks of maternity leave, 15 days of paternity leave\nVery unique to Reltio - 01 week of additional off as recharge week every year globally\nSupport for home office setup:\nHome office setup allowance.\nStay Connected, Work Flexibly: Mobile & Internet Reimbursement\nNo need to pack a lunch we ve got you covered with a free meal.And many more ..",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Software design', 'Backend', 'Managed services', 'Integration testing', 'Data structures', 'Customer support', 'Distribution system', 'Analytics', 'SQL']",2025-06-12 13:54:00
Sales Engineer - CLOUD Products ( Cochin Location ),Redington Group,0 - 2 years,2.75-6 Lacs P.A.,['Kochi'],Role & responsibilities\n\nTO Handle field sales ( CLOUD PRODUCTS )\n\n1. Primarily a hunter and hustler personality with 0-5 years of experience in SME & Enterprise Segment. Strong enterprise sales background in solutions / SaaS space ideally with knowledge of BI / analytics / Big data / AI/Azure /AWS.,,,,"['Cloud Technologies', 'Sales Engineering', 'Cloud Applications', 'Technical Sales']",2025-06-12 13:54:03
Associate Software Engineer – Biological Studies ( In Vivo ),Amgen Inc,0 - 3 years,Not Disclosed,['Hyderabad'],"In this vital role you will join a multi-functional team of scientists and software professionals that enables technology and data capabilities to evaluate drug candidates and assess their abilities to affect the biology of drug targets.\nThis team implements scientific software platforms that enable the capture, analysis, storage, and report of in vitro assays and in vivo / pre-clinical studies as well as those that manage compound inventories / biological sample banks. The ideal candidate possesses experience in the pharmaceutical or biotech industry, strong technical skills, and full stack software engineering experience (spanning SQL, back-end, front-end web technologies, automated testing).\nRoles & Responsibilities:\nDesign, develop, and implement applications and modules, including custom reports, interfaces, and enhancements\nAnalyze and understand the functional and technical requirements of applications, solutions and systems and translate them into software architecture and design specifications\nDevelop and execute unit tests, integration tests, and other testing strategies to ensure the quality of the software\nIdentify and resolve software bugs and performance issues\nWork closely with cross-functional teams, including product management, design, and QA, to deliver high-quality software on time\nMaintain documentation of software designs, code, and development processes\nCustomize modules to meet specific business requirements\nWork on integrating with other systems and platforms to ensure seamless data flow and functionality\nProvide ongoing support and maintenance for applications, ensuring that they operate smoothly and efficiently\nContribute to both front-end and back-end development using cloud technology\nDevelop innovative solution using generative AI technologies\nIdentify and resolve technical challenges effectively\nWork closely with product team, business team including scientists, and other stakeholders\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The [vital attribute] professional we seek is a [type of person] with these qualifications.\nBasic Qualifications:\nBachelors degree and 0 to 3 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nDiploma and 4 to 7 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field\nPreferred Qualifications:\nExperience in implementing and supporting biopharma scientific software platforms\nFunctional Skills:\nMust-Have Skills:\nProficient in a General Purpose High Level Language (e.g. Python, Java, C#.NET)\nProficient in a Javascript UI Framework (e.g. React, ExtJs)\nProficient in a SQL (e.g. Oracle, PostGres, Databricks)\nExperience with event-based architecture (e.g. Mulesoft, AWS EventBridge, AWS Kinesis, Kafka)\nGood-to-Have Skills:\nStrong understanding of software development methodologies, mainly Agile and Scrum\nHands-on experience with Full Stack software development\nStrong understanding of cloud platforms (e.g AWS) and containerization technologies (e.g., Docker, Kubernetes)\nWorking experience with DevOps practices and CI/CD pipelines\nExperience with big data technologies (e.g., Spark, Databricks)\nExperience with API integration, serverless, microservices architecture (e.g. Mulesoft, AWS Kafka)\nExperience with monitoring and logging tools (e.g., Prometheus, Grafana, Splunk)\nExperience of infrastructure as code (IaC) tools (Terraform, CloudFormation)\nExperience with version control systems like Git\nExperience with automated testing tools and frameworks\nExperience with Benchling\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nAWS Certified Cloud Practitioner preferred\nSoft Skills:\nExcellent problem solving, analytical, and troubleshooting skills\nStrong communication and interpersonal skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to learn quickly & work independently\nTeam-oriented, with a focus on achieving team goals\nAbility to manage multiple priorities successfully\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'C#', 'Java', 'JavaScript', 'Kafka', '.NET', 'React', 'ExtJs', 'SQL', 'Mulesoft', 'AWS EventBridge', 'AWS Kinesis']",2025-06-12 13:54:06
Database Engineer-Architect 7+ Years C2H,Greetings from BCforward INDIA TECHNOLOG...,7 - 12 years,15-30 Lacs P.A.,"['Bangalore Rural', 'Bengaluru']","Greetings from BCforward INDIA TECHNOLOGIES PRIVATE LIMITED.\n\nContract To Hire(C2H) Role\nLocation: Bangalore\nPayroll: BCforward\nWork Mode: Hybrid\n\nJD\n\nPreferred Skills: 5+ years relevant experience into Database Engineer-Architect(Postgres / Postgresql; Oracle; Linux)\n\nDescription:\nSkills: Postgres / Postgresql; Oracle; Linux\n\nDesirable attributes\nSkill at the Unix command line\nThe ability to write code or script, e.g. for test harnesses or other database-related tools and utilities\nExperience as a database administrator and/or a Unix system administrator\nKnowledge of virtualization and cloud infrastructures, and of implementations such as VMWare, OpenShift, Kubernetes and Docker\nKnowledge of AWS, Google Cloud, Azure or other public cloud offerings\nKnowledge of modern storage and compute technologies, including hyper-converged infrastructures\nBachelors degree preferred.\n\nPlease share your Updated Resume, PAN card soft copy, Passport size Photo & UAN History.\n\nInterested applicants can share updated resume to g.sreekanth@bcforward.com\n\nNote: Looking for Immediate to 15-Days joiners at most.\n\nAll the best",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database Engineer', 'Postgresql', 'Oracle', 'database Architect', 'Linux', 'Unix command']",2025-06-12 13:54:08
Onix is Hiring Hadoop GCP Engineers!!!,Datametica,4 - 8 years,Not Disclosed,"['Pune', 'Bengaluru']","We are looking for skilled Hadoop and Google Cloud Platform (GCP) Engineers to join our dynamic team. If you have hands-on experience with Big Data technologies and cloud ecosystems, we want to hear from you!\nKey Skills:\nHadoop Ecosystem (HDFS, MapReduce, YARN, Hive, Spark)\nGoogle Cloud Platform (BigQuery, DataProc, Cloud Composer)\nData Ingestion & ETL pipelines\nStrong programming skills (Java, Python, Scala)\nExperience with real-time data processing (Kafka, Spark Streaming)\nWhy Join Us?\nWork on cutting-edge Big Data projects\nCollaborate with a passionate and innovative team\nOpportunities for growth and learning\nInterested candidates, please share your updated resume or connect with us directly!",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['gcp', 'Big Data', 'pyspark', 'Hive', 'Sqoop', 'mapreduce', 'Bigquery', 'Hadoop', 'Spark', 'YARN', 'pig', 'bigq']",2025-06-12 13:54:10
Security Engineer II,Tekion Corp,1 - 5 years,Not Disclosed,['Bengaluru'],"About Tekion:\nPositively disrupting an industry that has not seen any innovation in over 50 years, Tekion has challenged the paradigm with the first and fastest cloud-native automotive platform that includes the revolutionary Automotive Retail Cloud (ARC) for retailers, Automotive Enterprise Cloud (AEC) for manufacturers and other large automotive enterprises and Automotive Partner Cloud (APC) for technology and industry partners. Tekion connects the entire spectrum of the automotive retail ecosystem through one seamless platform. The transformative platform uses cutting-edge technology, big data, machine learning, and AI to seamlessly bring together OEMs, retailers/dealers and consumers. With its highly configurable integration and greater customer engagement capabilities, Tekion is enabling the best automotive retail experiences ever. Tekion employs close to 3,000 people across North America, Asia and Europe.",,,,"['Patch management', 'Computer science', 'Automation', 'Coding', 'SOC', 'Machine learning', 'Vulnerability', 'Information technology', 'Automotive', 'Python']",2025-06-12 13:54:13
Senior Copy Writer- WFH,Aegis Softtech,6 - 7 years,6.5-9 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Immediate Openings for Senior Copy Writer (Permanent WFH)\n\nCopywriter - Job Description\n\nAbout Aegis :\nAegis Softtech is a global technology services firm delivering customized software solutions in AI, ML, Cloud, Data Engineering, CRM Consulting, and more. We work with tech leaders, enterprises, and fast-scaling startups to help them solve business problems with scalable, future-ready software.\n\nWe are building a lean, quality-first content team.\nIf you're a copywriter who knows how to write for humans, doesn't hide behind jargon, and loves shaping complex tech stories into simple, compelling narratives, lets work together.\n\nWhat Youll Do:\n1.Craft compelling, conversion-driven content across formats: website pages, landing pages, email copy, social posts, and ad creatives\n2. Own copy development for key service areas like AI/ML, Data & Cloud, and CRM solutions (Microsoft Dynamics, Salesforce, etc.)\n3. Translate technical inputs into benefit-focused, client-first messaging that aligns with our authoritative yet approachable voice\n4.Collaborate with the Content Lead, designers, and developers to align messaging across touchpoints.\n5. Edit and refine content for clarity, brevity, tone, and SEO, without diluting meaning.\n6.Bring consistency to brand tone across different regions and verticals.\n\nWhat Were Looking For\n> 5–8 years of proven experience as a copywriter, ideally in the B2B tech or SaaS industry.\n> Comfort working across multiple formats and content lengths—from short CTAs to full-service pages.\n> A storytelling mindset with a keen understanding of buyer psychology and content structure.\n> Strong grasp of SEO principles and how to write for both humans and search engines.\n> Ability to simplify complex tech ideas without dumbing them down.\n> Self-driven, collaborative, and comfortable with remote async work.\n\nWhy Join Us\n> Flexible remote work with a global team of tech thinkers and builders\n> A chance to influence messaging at a strategic level, not just execute briefs\n> Open and transparent communication culture\n> Opportunity to work closely with a Lead who values content quality as much as delivery speed.\n\nTo Apply:\nSend your resume, a short note about why this role speaks to you, and 2–3 samples that show your ability to:\nTranslate tech to value.\nBuild momentum with words.\nWrite with clarity and character.\nEmail your application to hr@aegissofttech.com with the subject line: Copywriter Application – [Your Name].",Industry Type: IT Services & Consulting,"Department: Content, Editorial & Journalism","Employment Type: Full Time, Permanent","['SEO Writing', 'technical content', 'copy writing', 'Content Writing', 'Content Strategy']",2025-06-12 13:54:15
Sr Associate Software Engineer,Horizon Therapeutics,1 - 5 years,Not Disclosed,['Hyderabad'],"Career Category Information Systems Job Description\nRole Description:\nThe role is responsible for designing, developing, and maintaining software solutions for Research scientists . Additionally, it involves automating operations, monitoring system health, and responding to incidents to minimize downtime. Y ou will join a multi-functional team of scientists and software professionals that enables technology and data capabilities to evaluate drug candidates and assess their abilities to affect the biology of drug targets. This team implements scientific software platforms that enable the capture, analysis, storage, and report ing for our Large Molecule Discovery Research team (Design, Make, Test and Analyze processes) . The team also interfaces heavily with teams supporting our in vi tro assay management systems and our compound inventory platforms . The ideal candidate possesses experience in the pharmaceutical or biotech industry, strong technical skills, and full stack software engineering experience (spanning SQL, back-end, front-end web technologies, automated testing ).\nRoles Responsibilities:\nWork closely with product team, business team including scientists, and other stakeholders\nAnalyze and understand the functional and technical requirements of applications, solutions and systems and translate them into software architecture and design specifications\nDesign, develop, and implement applications and modules, including custom reports, interfaces, and enhancements\nDevelop and execute unit tests, integration tests, and other testing strategies to ensure the quality of the software\nConduct code reviews to ensure code quality and adherence to best practices\nCreate and maintain documentation on software architecture, design, deployment, disaster recovery, and operations\nProvide ongoing support and maintenance for applications, ensuring that they operate smoothly and efficiently\nStay updated with the latest technology and security trends and advancements\nBasic Qualifications and Experience:\nMaster s degree with 1 - 3 years of experience in Computer Science, IT , Computational Chemistry, Computational Biology/ Bioinformatics or related field OR\nBachelor s degree with 4 - 6 years of experience in Computer Science, IT , Computational Chemistry, Computational Biology/ Bioinformatics or related field OR\nDiploma with 7 - 9 years of experience in Computer Science, IT , Computational Chemistry, Computational Biology/ Bioinformatics or related field\nPreferred Qualifications and Experience:\n1 + years of experience in implementing and supporting biopharma scientific software platforms\nFunctional Skills:\nMust-Have Skills :\nProficient in Java or Python\nProficient in at least one JavaScript UI Framework ( e. g. ExtJS , React, or Angular)\nProficient in SQL ( e. g. Oracle, PostgreSQL , Databricks)\nGood-to-Have Skills:\nExperience with event-based architecture and serverless AWS services such as EventBridge , SQS, Lambda or ECS.\nExperience with Benchling\nHands - on experience with Full Stack software development\nStrong understanding of software development methodologies, mainly Agile and Scrum\nWorking experience with DevOps practices and CI/CD pipelines\nExperience of infrastructure as code ( IaC ) tools (Terraform, CloudFormation)\nExperience with monitoring and logging tools (e. g. , Prometheus, Grafana, Splunk)\nExperience with automated testing tools and frameworks\nExperience with big data technologies (e. g. , Spark, Databricks, Kafka)\nExperience with leveraging the use of AI-assistants ( e. g. GitHub Copilot) to accelerate software development and improve code quality\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nAWS Certified Cloud Practitioner preferred\nSoft Skills:\nExcellent problem solving, analytical , and troubleshooting skills\nStrong communication and interpersonal skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to learn quickly work independently\nTeam-oriented, with a focus on achieving team goals\nAbility to manage multiple priorities successfully\nStrong presentation and public speaking skills\nEQUAL OPPORTUNITY STATEMENT\nAmgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.\nWe will ensure that individuals with disabilities are provided with reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request an accommodation .\n.",Industry Type: Biotechnology,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Computational chemistry', 'Front end', 'Pharma', 'Javascript', 'Oracle', 'Monitoring', 'SQL', 'Python', 'Computational biology']",2025-06-12 13:54:17
Staff Engineer,Reltio,10 - 15 years,Not Disclosed,['Bengaluru'],"Job Title: Staff Engineer\nLocation: Bengaluru, India - (Hybrid)\nAt Reltio , we believe data should fuel business success. Reltio s AI-powered data unification and management capabilities encompassing entity resolution, multi-domain master data management (MDM), and data products transform siloed data from disparate sources into unified, trusted, and interoperable data. Reltio Data Cloud delivers interoperable data where and when its needed, empowering data and analytics leaders with unparalleled business responsiveness. Leading enterprise brands across multiple industries around the globe rely on our award-winning data unification and cloud-native MDM capabilities to improve efficiency, manage risk and drive growth.\nAt Reltio, our values guide everything we do. With an unyielding commitment to prioritizing our Customer First , we strive to ensure their success. We embrace our differences and are Better Together as One Reltio. We are always looking to Simplify and Share our knowledge when we collaborate to remove obstacles for each other. We hold ourselves accountable for our actions and outcomes and strive for excellence. We Own It . Every day, we innovate and evolve, so that today is Always Better Than Yesterday . If you share and embody these values, we invite you to join our team at Reltio and contribute to our mission of excellence.\nReltio has earned numerous awards and top rankings for our technology, our culture and our people. Reltio was founded on a distributed workforce and offers flexible work arrangements to help our people manage their personal and professional lives. If you re ready to work on unrivaled technology where your desire to be part of a collaborative team is met with a laser-focused mission to enable digital transformation with connected data, let s talk!\nJob Summary:\nCore Platform development is spread across multiple cross-functional teams, each building a large and complex component of the MDM platform. As a Senior Engineer in the team, you will be responsible for successful delivery of features in our areas of ownership, covering the following key aspects:\nWork with PMs to build a solid product roadmap with clear, well-defined goals and KPIs\nDrive the design and development of features, provide thought and execution leadership\nWork across functions with QA, DevOps & Doc teams to ensure a high-quality delivery\nPresent design for new initiatives in the global forum of architects\nJob Duties and Responsibilities:\nDeliver features with high quality, and be customer-focused.\nChampion engineering excellence, raise the bar on code coverage, coding guidelines, and review processes.\nProvide technical leadership and mentor junior team members.\nDeal with ambiguity in complex problems, show bias for action.\nWork closely with architects to actively shape the architecture of your work area and the broader product.\nWork closely with PM, QA & Doc teams to ensure a thorough testing strategy, robust deployment and lucid documentation of features.\nEffective communication with cross-functional teams and global stakeholders.\nSkills You Must Have:\nAn engineering degree in the computer science field\nSolid foundation in computer science, with strong competencies in algorithms, data structures, software design, and building large, distributed systems\n8+ years of experience in the design and development of products for enterprise customers.\n4+ years of experience in building large-scale distributed data systems in Java and cloud technologies.\nProven track record of building functionalities from conception to delivery, engaging with customers and partners to drive successful adoption and customer satisfaction.\nExperience in leading projects with a crew of engineers, providing technical mentorship.\nExperience building services on one of the 3 major cloud service providers: AWS, Azure or GCP, hands-on experience in managed cloud databases\nSkills That Are Nice to Have:\nExperience working with NoSQL databases like Cassandra, and queue services like SQS\nExperience in cloud security principles\nExperience with Kubernetes\nExperience with big data technologies\nExperience in driving customer focus for SaaS products is a big plus\nWhy Join Reltio?*\nHealth & Wellness:\nComprehensive Group medical insurance, including your parents, with additional top-up options.\nAccidental Insurance\nLife insurance\nFree online unlimited doctor consultations\nAn Employee Assistance Program (EAP)\nWork-Life Balance:\n36 annual leaves, which include Sick leaves - 18, Earned Leaves - 18\n26 weeks of maternity leave, 15 days of paternity leave\nVery unique to Reltio - 01 week of additional off as recharge week every year globally\nSupport for home office setup:\nHome office setup allowance.\nStay Connected, Work Flexibly: Mobile & Internet Reimbursement\nNo need to pack a lunch we ve got you covered with a free meal. And many more ..",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Software design', 'NoSQL', 'Coding', 'cassandra', 'GCP', 'Data structures', 'Medical insurance', 'Distribution system', 'Analytics']",2025-06-12 13:54:20
Senior Applied AI Scientist,ZS,4 - 9 years,Not Disclosed,['Bengaluru'],"Write complex SQL queries for data extraction, perform exploratory data analysis (EDA) to uncover insights.\nStrong proficiency in Python and Py Spark for scalable data processing and analytics.\nCreate, transform, and optimize features to enhance model performance.\nTrain, evaluate, and maintain machine learning models in production.\nWrite efficient, maintainable, and version-controlled code that handles large datasets.\nRegularly update internal teams and clients on project progress, results, and insights.\nConduct hypothesis testing and experiment analysis to drive data-driven decisions using AB testing.",,,,"['Data analysis', 'data security', 'Financial planning', 'Management consulting', 'Machine learning', 'Data processing', 'Analytics', 'Data extraction', 'Python']",2025-06-12 13:54:22
Senior Manager - Internal Audit,Flipkart,7 - 10 years,Not Disclosed,['Bengaluru'],Experience in internal audits with Big4 or Large organizations risk and control functions Big data Analytics skills Excellent communications and presentation skills Analytical bent of mind Experience in an e-commerce or retail industry (desirable) Ability to manage multiple audit projects,Industry Type: Courier / Logistics,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['Internal audit', 'internal controls', 'COSO', 'Big data Analytics', 'SOX', 'SQL']",2025-06-12 13:54:24
"GenAI Field Architect, Customer Value Engineering",Ema Unlimited,8 - 13 years,Not Disclosed,['Bengaluru'],"Who we are\nEma is building the world s largest agentic AI platform to revolutionize enterprise productivity. Our proprietary technology enables companies to delegate repetitive tasks to Ema, the Universal AI employee, unlocking 10x gains in workforce efficiency. Founded by ex-Google, Coinbase, Flipkart, and Okta executives, we bring deep product and scaling expertise to this mission. Our team includes top engineers from Google, Microsoft Research, Facebook, Square, and Coinbase, and alumni of Stanford, MIT, UC Berkeley, CMU, and IITs.\nWe ve raised over $60M from top-tier investors including Accel, Naspers/Prosus, Section32, SCB10X, Sozo Ventures, Hitachi Ventures, and prominent angels like Sheryl Sandberg, Jerry Yang, and Dustin Moskovitz. Ema is headquartered in Silicon Valley with a second hub in Bangalore, India. This is a hybrid role requiring in-office work three days per week.\nRole Overview\nEma is on a mission to redefine the future of work by building a Universal AI Employee that empowers every worker to delegate repetitive, high-value tasks to intelligent, agentic AI systems. As the Architect on our Customer Value Engineering (CVE) team, you ll play a pivotal role in translating this vision into real-world impact. From discovery workshops to production deployments, you will lead the design of scalable, secure, and observable AI-powered workflows turning customer ambitions into enterprise-grade solutions that deliver measurable value.\nRoles and Responsibilities:\nWorkflow Discovery & Solution Design: Lead discovery workshops to understand human workflows, pain points, data flows, and integration needs. Translate business objectives into AI architecture blueprints covering integration, data, security, and success metrics. Author and maintain AI-Employee design documents to guide implementation from blueprint to SLOs.\nData Integration & Action Building: Use declarative connectors and APIs to ingest, normalize, and secure data from enterprise systems (CRM, ERP, ATS, etc.). Build and maintain reusable action blocks using REST/SOAP/RPA and integrate them into agent-based workflows.\nAgentic Reasoning & Prompt Engineering: Design and compose reasoning agents using modern frameworks (e.g., LangChain, Semantic Kernel). Tune prompts, set up memory management, and define mesh topologies to ensure robust agentic reasoning. Own prompt evaluation, experiment design (A/A, A/B), and rollback strategies.\nHuman-AI Interaction: Define and implement user interaction flows by integrating with Slack, Microsoft Teams, widgets, or customer-facing apps. Ensure seamless handoff between automated and human-in-the-loop experiences.\nMonitoring, Metrics & Observability: Define success metrics, KPIs, and SLOs; wire them into dashboards, alerts, and observability systems. Steward observability packs, including Terraform/Helm configurations and alerting strategies.\nSecurity, Identity & Permissions: Enforce zero-trust identity patterns including SSO (Okta/AD) and RBAC. Own auditing, access control, and compliance posture across deployments.\nCollaboration & Continuous Improvement: Lead technical, architectural, and security reviews with internal teams and customer stakeholders. Partner with CVEs and system integrators to unblock issues and ensure deployment success. Monitor production system health and performance, lead continuous-improvement sprints, and codify learnings into reusable playbooks and runbooks. Maintain registries of versioned prompts, connectors, and action templates to support scale and reuse. Channel field insights to Product and ML teams to refine roadmap and platform capabilities.\nQualifications\n8+ years in software/solutions architecture, including 3+ years with LLM or event-driven systems\nExpert in Python, REST/JSON APIs, and cloud infrastructure (GCP, AWS, or Azure)\nProven record deploying AI systems in enterprise environments\nFamiliarity with agent frameworks like LangChain, Semantic Kernel, or similar\nExperience integrating SaaS platforms like CRM, ATS, ERP\nStrong understanding of RBAC, SSO (Okta/AD), and security standards\nOutstanding communication and executive presence in high-stakes environments\nPreferred Qualifications\nExperience with SQL optimization, graph DBs, or real-time processing\nHands-on knowledge of Terraform, Helm, and infrastructure-as-code\nPrior work in FedRAMP, HIPAA, or similar compliance environments\nOpen-source contributions in AI infrastructure or observability domains\nSoft Skills:\nOwnership & Accountability : You drive end-to-end outcomes, not just deliverables\nCuriosity & Learning : You seek emerging tools and best practices in GenAI\nCollaboration : You thrive at the intersection of product, engineering, and customer success\nEmpathy & Clarity : You simplify the complex and build trust through communication",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ERP', 'Product engineering', 'Workflow', 'JSON', 'microsoft', 'Open source', 'Monitoring', 'SQL', 'CRM', 'Python']",2025-06-12 13:54:27
Infrastructure Automation Engineer – ServiceNow,Amgen Inc,0 - 2 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will be responsible for developing innovative self-service solutions for our global workforce and further enhancing our self-service automation built on the ServiceNow platform.\nAs part of a scaled Agile product delivery team, the Developer works closely with product feature owners, project stakeholders, operational support teams, peer developers and testers to develop solutions to enhance self-service capabilities and solve business problems by identifying requirements, conducting feasibility analysis, proof of concepts and design sessions.\nThe Developer serves as a subject matter expert on the design, integration and operability of solutions to support innovation initiatives with business partners and shared services technology teams.\nThis role sits within the Digital, Technology and Innovation Infrastructure Automation product team and is tasked with delivering solutions that will integrate and facilitate the automation of various processes and enterprise systems. Please note, this is an onsite role based in Hyderabad.\nKey Responsibilities:\nDeliver outstanding self-service and automation experiences for our global workforce\nCreate ServiceNow catalog items, workflows, and cross-platform API integrations\nCreate and configure Business Rules, UI Policies, UI Actions, Client Scripts, REST APIs and ACLs including advanced scripting logic.\nCreate and configure Notifications, UI pages, UI Macros, Script Includes, Formatters, etc.\nCreate and maintain data integrations between ServiceNow and other systems\nDevelop system integrations and process automation\nParticipate in design review, client requirements sessions and development teams to deliver features and capabilities supporting automation initiatives\nCollaborate with product owners, stakeholders, testers and other developers to understand, estimate, prioritize and implement solutions\nDesign, code, debug, document, deploy and maintain solutions in a highly efficient and effective manner\nParticipate in problem analysis, code review, and system design\nRemain current on new technology and apply innovation to improve functionality\nCollaborate closely with stakeholders and team members to configure, improve and maintain current applications\nWork directly with users to resolve support issues within product team responsibilities\nMonitor health, performance and usage of developed solutions\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients.\nBasic Qualifications:\nMasters degree and 0 to 2 years of experience in computer science, IT, or related field OR\nBachelors degree and 2 to 5 years of experience in computer science, IT, or related field OR\nDiploma and 4 to 7 years of experience in computer science, IT, or related field\nRequired Skills & Qualifications:\n3+ years of deep hands-on experience with ServiceNow administration and development in two or more products: ITSM, ITBM, ITOM, GRC, HRSD, or Security Operations\nServiceNow development using JavaScript, AngularJS, AJAX, HTML, CSS, and Bootstrap;\nStrong understanding of user-centered design and building scalable, high-performing web and mobile interfaces on the ServiceNow platform\nExperience creating and managing Scoped Applications\nWorkflow automation and integration development using REST, SOAP, or MID servers\nScripting skills in Python, Bash, or other programming languages\nWorking in an Agile (SAFe, Scrum, and Kanban) environment\nPreferred Qualifications:\nGood-to-Have Skills:\nExperience with other configuration management tools (e.g., Puppet, Chef).\nExperience with Linux administration, scripting (Python, Bash), and CI/CD tools (GitHub Actions, CodePipeline, etc.)\nExperience with Terraform & CloudFormation for AWS infrastructure automation\nKnowledge of AWS Lambda and event-driven architectures.\nExposure to multi-cloud environments (Azure, GCP)\nExperience operating within a validated systems environment (FDA, European Agency for the Evaluation of Medicinal Products, Ministry of Health, etc.)\nProfessional Certifications (preferred):\nService Now Certified System Administrator\nService Now Certified Application Developer\nService Now Certified Technical Architect\nSoft Skills:\nStrong analytical and problem-solving skills.\nAbility to work effectively with global, virtual teams\nEffective communication and collaboration with cross-functional teams.\nAbility to work in a fast-paced environment.\nShift Information: This position is required to be onsite and participate in 24/5 and weekend on call in rotation fashion and may require you to work a later shift. Candidates must be willing and able to work off hours, as required based on business requirements.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ServiceNow', 'AngularJS', 'HTML', 'SAFe', 'ITBM', 'JavaScript', 'code review', 'Scrum', 'Python', 'CSS', 'Azure', 'UI pages', 'ITSM', 'system design', 'ITOM', 'Bash', 'SOAP', 'REST', 'UI Macros', 'problem analysis', 'GCP', 'GRC', 'HRSD', 'Script Includes', 'AJAX']",2025-06-12 13:54:29
Claims Business Process Analyst Senior,Optum,1 - 4 years,Not Disclosed,['Bengaluru'],"Optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health optimization on a global scale. Join us to start Caring. Connecting. Growing together.\n\nPrimary Responsibilities:\nReporting Development and Data Integration\nAssist with data projects related to integration with our core claims adjudication engines, eligibility, and other database items as necessary\nSupport the data leads by producing ad hoc reports as needed based on requirements from the business\nReport on key milestones to our project leads\nEnsuring all reporting aligns with brand standards\nEnsuring PADU guidelines for tools, connections, and data security\nBuild a network with internal partners to assist with validating data quality\nAnalytical Skills Utilization\nApplying analytical skills and developing business knowledge to support operations\nIdentify automation opportunities through the trends and day to day tasks to help create efficiencies within the team\nPerform root cause analysis via the 5 why root causing to identify process gaps and initiate process improvement efforts\nAssist with user testing for reports, business insights dashboards, and assist with automation validation review\nComply with the terms and conditions of the employment contract, company policies and procedures, and any and all directives (such as, but not limited to, transfer and/or re-assignment to different work locations, change in teams and/or work shifts, policies in regards to flexibility of work benefits and/or work environment, alternative work arrangements, and other decisions that may arise due to the changing business environment). The Company may adopt, vary or rescind these policies and directives in its absolute discretion and without any limitation (implied or otherwise) on its ability to do so\n\nRequired Qualifications:\nDegree or equivalent data science, analysis, mathematics experience\nExperience supporting operational teams' performance with reports and analytics\nExperience using Word (creating templates/documents), PowerPoint (creation and presentation), Teams, and SharePoint (document access/storage, sharing, List development and management)\nBasic understanding of reporting using Business Insights tools including Tableau and PowerBI\nExpertise in Excel (data entry, sorting/filtering) and VBA\nProven solid communication skills including oral, written, and organizational skills\nProven ability to manage emotions effectively in high-pressure situations, maintaining composure, and fosters a positive work environment conducive to collaboration and productivity\nPreferred Qualifications:\nExperience leveraging and creating automation such as macros, PowerAutomate, Alteryx/ETL Applications\nExperience working with cloud-based servers, knowledge of database structure, stored procedures\nExperience performing root cause analysis and demonstrated problem solving skills\nKnowledge of R/Python, SQL, DAX or other coding languages\nKnowledge of multiple lines of business, benefit structures and claims processing systems",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['power bi', 'sql', 'alteryx', 'tableau', 'vba', 'python', 'macros', 'dbms', 'sharepoint', 'process improvement', 'business process analysis', 'root cause analysis', 'dashboards', 'stored procedures', 'data quality', 'r', 'data science', 'business insights', 'dax', 'etl', 'data integration']",2025-06-12 13:54:31
Senior Software Engineer,eG Innovations,5 - 8 years,Not Disclosed,['Chennai'],Responsibilities:\n\nDesign and manage application module for data Integration / ETL collection/processing/storage/retrieval) using latest technologies\nShould be completely aware of coding standards and should be able to design & develop high performance & scalable application\nAbility to prototype solutions quickly and analyse and compare multiple solutions and products based on requirements.,,,,"['Core Java', 'Spring Boot', 'JMS', 'Open AI', 'Kafka', 'J2Ee', 'ETL', 'Data Analytics', 'SOAP', 'Microservices', 'Data Integration']",2025-06-12 13:54:33
ETL Test Engineer - Automation Testing,Stanra Tech Solutions,6 - 7 years,Not Disclosed,['Thiruvananthapuram'],"ETL Test Automation Engineer Trivandrum (WFO)-Immediate Joiner ONLY\n\nRole : ETL Test Automation Engineer to design, implement, and maintain test automation frameworks for real-time streaming ETL pipelines in a cloud based environment.\n\nRequired Experience : 6-7 Year Mandatory in ETL Automation Testing\n\nWorking Hours : Normal working hours (Monday to Friday, )\n\nAdditional Information :\n\nStart Date : Immediate-Immediate Joiners Only Mandatory\n\nBackground Verification : Mandatory through a third-party verification process.\n\nRequired Experience : 6-7 Yr Mandatory in ETL Automation Testing\n\nPlease Note : We are looking for Immediate Joiner Only\n\nSkills :\nStrong knowledge of ETL Testing with a focus on streaming data pipelines (Apache Flink, Kafka).\nExperience with database testing and validation (PostgreSQL, NoSQL).\nProficiency in Java for writing test automation scripts.\nHands-on experience with automation scripting for ETL workflows, data validation, and transformation checks.\nExperience in performance testing and optimizing ETL jobs to handle large-scale data streams.\nProficient in writing and executing complex database queries for data validation and reconciliation.\nExperience with CI/CD tools (Jenkins, GitHub Actions, Azure DevOps) for integrating test automation.\nFamiliarity with cloud-based ETL testing in Azure.\nAbility to troubleshoot data flow and transformation issues in real-time.\nNice to Have:\nExperience with Karate DSL, NiFi and CLI-based testing tools.\nExperience in developing self-healing mechanisms for data integrity issues.\nHands-on experience with automated data drift detection.\nKnowledge of data observability tools.\nFamiliarity with containerization (Docker, Kubernetes) and Infrastructure as Code (Terraform, Ansible) for ETL deployment.\nExperience with testing message queues (Kafka, Pulsar, RabbitMQ, etc.).\nDomain knowledge of banking and financial institutions and/or large enterprise IT environment will be considered a strong asset .",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ETL Testing', 'Datawarehouse Testing', 'Performance Testing', 'Integration Testing', 'Automation Testing', 'Test Strategy']",2025-06-12 13:54:35
Consultant - Software Engineer (with C#),Affine Analytics,5 - 8 years,Not Disclosed,['Bengaluru'],"We are looking for a highly skilled API & Pixel Tracking Integration Engineer to lead the development and deployment of server-side tracking and attribution solutions across multiple platforms. The ideal candidate brings deep expertise in CAPI integrations (Meta, Google, and other platforms), secure data handling using cryptographic techniques, and experience working within privacy-first environments like Azure Clean Rooms.\nThis role requires strong hands-on experience in C# development, Azure cloud services, OCI (Oracle Cloud Infrastructure), and marketing technology stacks including Adobe Tag Management and Pixel Management. You will work closely with engineering, analytics, and marketing teams to deliver scalable, compliant, and secure data tracking solutions that drive business insights and performance.",,,,"['C#', 'Azure Data Factory', 'Adobe Tag Management', 'ADF', 'ADLS', 'Azure Functions', 'cryptography', 'Key Vault', 'Cosmos DB']",2025-06-12 13:54:38
Senior QA Engineer - Digital & IT,Skillsoft Software Services,2 - 8 years,Not Disclosed,['Hyderabad'],"We are seeking a passionate technology enthusiast with a strong background in ensuring the delivery of high-quality software applications. If you thrive in a dynamic, fast-paced environment where accountability and creativity are valued, then this opportunity might be the right fit for you. We are looking for an experienced QA Engineer to join our dynamic team.\n\nAs a Senior QA Engineer, you will be an integral part of our agile scrum development process, leading automation efforts within the sprint and ensuring the robustness of our diverse Software-as-a-Service (SaaS) and custom applications. If ensuring software quality is your forte, we are eager to connect with you.",,,,"['automation framework', 'workday', 'business requirements', 'test cases', 'api automation', 'salesforce', 'scripting', 'dynamics', 'java', 'git', 'automation', 'selenium', 'api', 'communication skills', 'python', 'development', 'bdd', 'sap', 'software testing', 'javascript', 'tdd', 'web technologies', 'salesforce testing', 'scrum', 'gui', 'agile']",2025-06-12 13:54:40
Senior ODI Developer (OCI PaaS/IaaS Expertise),Oracle,5 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru', 'Mumbai (All Areas)']","Role Overview:\nWe are seeking a highly skilled Senior ODI Developer with strong hands-on experience in SQL, PL/SQL, and Oracle Data Integrator (ODI) projects, particularly on OCI (Oracle Cloud Infrastructure) PaaS or IaaS platforms. The ideal candidate will design, implement, and optimize ETL processes, leveraging cloud-based solutions to meet evolving business needs. Prior experience in banking or insurance projects is a significant advantage.\nKey Responsibilities:\nDesign, develop, and deploy ETL processes using Oracle Data Integrator (ODI) on OCI PaaS/IaaS.\nConfigure and manage ODI instances on OCI, ensuring optimal performance and scalability.\nDevelop and optimize complex SQL and PL/SQL scripts for data extraction, transformation, and loading.\nImplement data integration solutions, connecting diverse data sources like cloud databases, on-premise systems, APIs, and flat files.\nMonitor and troubleshoot ODI jobs running on OCI to ensure seamless data flow and resolve any issues promptly.\nCollaborate with data architects and business analysts to understand integration requirements and deliver robust solutions.\nConduct performance tuning of ETL processes, SQL queries, and PL/SQL procedures.\nPrepare and maintain detailed technical documentation for developed solutions.\nAdhere to data security and compliance standards, particularly in cloud-based environments.\nProvide guidance and best practices for ODI and OCI-based data integration projects.\nSkills and Qualifications:\nMandatory Skills:\nStrong hands-on experience with Oracle Data Integrator (ODI) development and administration.\nProficiency in SQL and PL/SQL for complex data manipulation and query optimization.\nExperience deploying and managing ODI solutions on OCI PaaS/IaaS environments.\nDeep understanding of ETL processes, data warehousing concepts, and cloud data integration.\nPreferred Experience:\nHands-on experience in banking or insurance domain projects, with knowledge of domain-specific data structures.\nFamiliarity with OCI services like Autonomous Database, Object Storage, Compute, and Networking.\nExperience in integrating on-premise and cloud-based data sources.\nOther Skills:\nStrong problem-solving and debugging skills.\nExcellent communication and teamwork abilities.\nKnowledge of Agile methodologies and cloud-based DevOps practices.\nEducation and Experience:\nBachelors degree in computer science, Information Technology, or a related field.\n5 to 8 years of experience in ODI development, with at least 2 years of experience in OCI-based projects.\nDomain experience in banking or insurance is an added advantage.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Oracle Data Integrator', 'OCI', 'Data Integrator', 'Data Warehousing', 'ETL', 'SQL']",2025-06-12 13:54:42
"Quality Engineer, QA",XL India Business Services Pvt. Ltd,2 - 6 years,Not Disclosed,['Gurugram'],"Quality Engineer Bangalore/ Gurgaon, India AXA XL offers risk transfer and risk management solutions to clients globally\n\nWe offer worldwide capacity, flexible underwriting solutions, a wide variety of client-focused loss prevention services and a team-based account management approach\n\nAXA XL recognizes data and information as critical business assets, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained advantage\n\nOur Chief Data Office is focused on driving innovation through optimizing how we leverage data to drive strategy and create a new business model - disrupting the insurance market\n\nAs we develop an enterprise-wide data and digital strategy that moves us toward greater focus on the use of data and data-driven insights, we are seeking an Engineer for the Quality Engineering team\n\nThe Engineer sits next to our Business Partners and tests our AXIOM platform according to our stakeholders needs\n\nWhat you ll be DOING What will your essential responsibilities include? Possess excellent domain knowledge of Data warehousing technologies, SQL, Data Models to develop test strategies, approaches from Quality Engineering perspective\n\nIn close coordination with Project teams help lead all efforts from Quality Engineering perspective\n\nWork with data engineers or data scientists to collect and prepare the necessary test data sets\n\nEnsure the data adequately represents real-world scenarios and covers a diverse range of inputs\n\nExcellent domain knowledge of Data warehousing technologies, SQL, Data Models to build out test strategies and lead projects from Quality Engineering perspective\n\nWith an Automation-first mindset, work towards testing of user interfaces such as Business Intelligence solutions and validation of functionalities while constantly looking out for efficiency gains and process improvements\n\nTriage and Prioritization of stories and epics with all stakeholders to ensure optimal deliveries\n\nEngage with various stakeholders like Business Partners, Product Owners, Development and Infrastructure teams to ensure alignments with overall roadmap\n\nTrack current progress of testing activities, finding and tracking test metrics, estimating and communicating improvement actions based on the test metrics results and the experience\n\nAutomation for processes such as Data Loads, user interfaces such as Business Intelligence solutions and other validations of business KPIs\n\nAdopt and implement best practices towards Documentation of test plan, cases, results in JIRA\n\nTriage and Prioritization of defects with all stakeholders\n\nLeadership accountability for ensuring that every release to customers is fit for purpose, performant\n\nKnowledge on Scaled Agile, Scrum or Kanban methodology\n\nYou will report to Lead UAT\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: A minimum of a bachelor s or masters degree (preferred) in a relevant discipline\n\nRelevant years of excellent testing background, including knowledge/experience in automation\n\nInsurance experience in data, underwriting, claims or operations, including influencing, collaborating, and leading efforts in complex, disparate, and interrelated teams\n\nExcellent Experience with SQL Server, Azure Databricks Notebook, PowerBI, ADLS, CosmosDB, SQL DW Analytics\n\nShould have a robust background in Software development with experience in ingesting, transforming, and storing data from large datasets using Pyspark in Azure Databricks with robust knowledge of distributed computing concepts\n\nHands-on experience in designing and developing ETL Pipelines in Pyspark in Azure Databricks with robust python scripting\n\nDesired Skills and Abilities: Having experience doing UAT/System Integration testing in the insurance industry\n\nExcellent technical testing experience such as API testing, UI automation is a plus\n\nKnowledge/Experience of Testing in cloud-based systems in different data staging layers",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['System integration testing', 'Test planning', 'Account management', 'Business strategy', 'Business intelligence', 'Risk management', 'JIRA', 'Analytics', 'SQL', 'Python']",2025-06-12 13:54:45
"Engineer, ETL",XL India Business Services Pvt. Ltd,2 - 5 years,Not Disclosed,['Gurugram'],"Engineer, ETL Gurgaon/Bangalore, India AXA XL recognizes data and information as critical business assets, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained industrious advantage\n\nOur Chief Data Office also known as our Innovation, Data Intelligence & Analytics team (IDA) is focused on driving innovation through optimizing how we leverage data to drive strategy and create a new business model - disrupting the insurance market\n\nAs we develop an enterprise-wide data and digital strategy that moves us toward greater focus on the use of data and data-driven insights, we are seeking an Data Engineer\n\nThe role will support the team s efforts towards creating, enhancing, and stabilizing the Enterprise data lake through the development of the data pipelines\n\nThis role requires a person who is a team player and can work well with team members from other disciplines to deliver data in an efficient and strategic manner\n\nWhat you ll be DOING What will your essential responsibilities include? Act as a data engineering expert and partner to Global Technology and data consumers in controlling complexity and cost of the data platform, whilst enabling performance, governance, and maintainability of the estate\n\nUnderstand current and future data consumption patterns, architecture (granular level), partner with Architects to ensure optimal design of data layers\n\nApply best practices in Data architecture\n\nFor example, balance between materialization and virtualization, optimal level of de-normalization, caching and partitioning strategies, choice of storage and querying technology, performance tuning\n\nLeading and hands-on execution of research into new technologies\n\nFormulating frameworks for assessment of new technology vs business benefit, implications for data consumers\n\nAct as a best practice expert, blueprint creator of ways of working such as testing, logging, CI/CD, observability, release, enabling rapid growth in data inventory and utilization of Data Science Platform\n\nDesign prototypes and work in a fast-paced iterative solution delivery model\n\nDesign, Develop and maintain ETL pipelines using Pyspark in Azure Databricks using delta tables\n\nUse Harness for deployment pipeline\n\nMonitor Performance of ETL Jobs, resolve any issue that arose and improve the performance metrics as needed\n\nDiagnose system performance issue related to data processing and implement solution to address them\n\nCollaborate with other teams to ensure successful integration of data pipelines into larger system architecture requirement\n\nMaintain integrity and quality across all pipelines and environments\n\nUnderstand and follow secure coding practice to make sure code is not vulnerable\n\nYou will report to Technical Lead\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Effective Communication skills\n\nBachelor s degree in computer science, Mathematics, Statistics, Finance, related technical field, or equivalent work experience\n\nRelevant years of extensive work experience in various data engineering & modeling techniques (relational, data warehouse, semi-structured, etc), application development, advanced data querying skills\n\nRelevant years of programming experience using Databricks\n\nRelevant years of experience using Microsoft Azure suite of products (ADF, synapse and ADLS)\n\nSolid knowledge on network and firewall concepts\n\nSolid experience writing, optimizing and analyzing SQL\n\nRelevant years of experience with Python\n\nAbility to break complex data requirements and architect solutions into achievable targets\n\nRobust familiarity with Software Development Life Cycle (SDLC) processes and workflow, especially Agile\n\nExperience using Harness\n\nTechnical lead responsible for both individual and team deliveries\n\nDesired Skills and Abilities: Worked in big data migration projects\n\nWorked on performance tuning both at database and big data platforms\n\nAbility to interpret complex data requirements and architect solutions\n\nDistinctive problem-solving and analytical skills combined with robust business acumen\n\nExcellent basics on parquet files and delta files\n\nEffective Knowledge of Azure cloud computing platform\n\nFamiliarity with Reporting software - Power BI is a plus\n\nFamiliarity with DBT is a plus\n\nPassion for data and experience working within a data-driven organization\n\nYou care about what you do, and what we do",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'System architecture', 'Coding', 'Agile', 'Workflow', 'Application development', 'SDLC', 'SQL', 'Python', 'Firewall']",2025-06-12 13:54:47
Artificial Intelligence Engineer,Infosys,5 - 10 years,5-10 Lacs P.A.,['Pune'],"Our client INFOSYS is looking for Artificial Intelligence Engineer position with 5+ years of experience in Pune location. CONTRACT TO HIRE AND WORK FROM OFFICE\n\nJob Description :\nMandatory Skills : Python + LLMs + AI + Azure Certified.\nole Definition:\nThis is a specialized role for an AI Software Engineers design, build, and deploy scalable AI models and systems. They work with machine learning frameworks, cloud platforms, and data engineering tools to create and optimize AI solutions.\nSkills:\nProficient:\nLanguages/Framework: Fast API, Azure UI Search API (React)\nCloud: Azure Cloud Basics (Azure DevOps)\nGitlab: Gitlab Pipeline\nAnsible and REX: Rex Deployment\nData Science: Prompt Engineering + Modern Testing\nData pipeline development\nUnderstanding of AI/ML algorithms and their applications\nMLOps frameworks\nKnowledge of cloud platforms (Azure ML especially)\nModel deployment process\no             Data pipeline monitoring\n              Expert: (in addition to proficient skills)\no             Languages/Framework: Azure Open AI\no             Data Science: Open AI GPT Family of models 4o/4/3, Embeddings + Vector Search\no             Databases and ETL: Azure Storage Account, Postgresql, Cosmos\no             Experience with ML frameworks (TensorFlow, PyTorch, Scikit-learn)\no             Knowledge of cloud platforms (AWS SageMaker, Google AI Platform)\no             Expertise in data preprocessing, feature engineering, and model evaluation\no             Understanding of software engineering principles (version control, CI/CD, containerization)\no             Familiarity with distributed computing and big data tools (Spark, Hadoop)\no             Ability to optimize models for performance and scalability\no             Experience with Azure AI Search",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Temporary/Contractual","['llm', 'Python', 'Artificial Intelligence']",2025-06-12 13:54:49
"Associate Engineer, Release Management",XL India Business Services Pvt. Ltd,1 - 5 years,Not Disclosed,['Gurugram'],"Associate Engineer, Release Management Gurgaon / Bangalore, India AXA XL offers risk transfer and risk management solutions to clients globally\n\nWe offer worldwide capacity, flexible underwriting solutions, a wide variety of client-focused loss prevention services and a team-based account management approach\n\nAXA XL recognizes data and information as critical business assets, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained unique features\n\nOur Chief Data Office also known as our Enterprise Business Data Solutions team (EDS) is focused on driving innovation through optimizing how we leverage data to drive strategy and create a new business model - disrupting the insurance market\n\nThis role is part of the Data Build Management, which is responsible for supporting various teams to plan, build and support through the product lifecycle to ensure, performance stability and reliability of data solutions by providing world-class support\n\nWhat you ll be DOING What will your essential responsibilities include? Provide top-class Build, Release Management, and DevSecOps functionalities and support for various data solutions owned and managed by the IDA organization\n\nAnalyze and mitigate risks (technical or otherwise) associated with data solution build and release delivery timelines\n\nProvide support for data pipelines and DataOps in cloud and big data environments\n\nEnforce governance and reporting, and ensure adherence to policies, standards, and best practices for Build Pipelines and Release Management\n\nPlan the release of project deliverables and manage the release lifecycle\n\nCommunicate project-related tasks such as plans, timelines, and requirements between different teams\n\nCoordinate the release schedule and resources required based on third-party applications, defect backlogs, planned releases, and infrastructure updates\n\nIdentify risks that could delay the release and manage them to ensure that the scheduled scope and quality of the release is not affected\n\nTrack progress and identify issues, if any, while continuously working to improve the release process\n\nEnsure that the release is planned according to requirements and budget\n\nSchedule release readiness reviews before deployment and milestone reviews after each release\n\nCreate implementation and deployment plans in accordance with the release schedule\n\nPlan and provide weekly updates on release activities\n\nLead Go-Live activities to ensure successful software deployment\n\nCollaborate with relevant development teams responsible for building the automation tools used to develop and deploy the software\n\nAttend CAB meetings to discuss release schedules with the team and identify any roadblocks\n\nMaintain documentation related to build and release procedures, various notification lists, and dependencies\n\nDemonstrate proactive communication with business users, development teams, technology, production support, delivery teams, and senior management\n\nEstablish a feedback loop by collaborating with internal and external teams\n\nPartner with the Product and Production Support teams as a Build/Release/Technical Subject Matter Expert (SME) for new development or migration of re-architected product functionalities to the new cloud platform\n\nOversee the development and maintenance of Build and Release Management processes and their documentation\n\nBuild and maintain various critical monitoring processes, alerts, and overall health reports (performance and functional) for production and pre-production environments to be used by the Production Support Teams\n\nEnsure timely and accurate completion of emergency release pipelines/processes in a manner that is auditable, testable, and maintainable\n\nEnsure that all builds are consistent with solution design, security recommendations, and business specifications\n\nAbility to work in a ""Follow the Sun"" support model, providing coverage across Digital Data Dev division responsibilities\n\nAchieve and maintain the highest levels of business customer confidence and net promoter score (NPS)\n\nYou will report to the Scientist, Digital Data Development\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: A minimum of an Undergraduate University Degree in Computer Science or related fields with years of experience\n\nExperience in systems integration, and developer support tools Azure DevOps, Docker, Kubernetes, Harness, CICD, release management, and configuration management, Python\n\nUnderstanding of DataOps, ITIL and AGILE methodologies\n\nExperience in building reports & dashboards using Excel, and Power BI\n\nRelevant years of experience in supporting areas of Data Management\n\nOwn availability, scalability, and efficiency for day-to-day operations\n\nPossess a robust understanding of Azure fundamentals (Microsoft AZ-900)\n\nExperience using tools like Git, JIRA, etc Knowledge of artifact repositories such as JFrog and X-Ray\n\nDesired Skills and Abilities: Ability to troubleshoot issues and able to handle different types of user inquiries\n\nExercises judgment for decision-making on complex issues that impact the delivery\n\nMakes recommendations to management on new processes, tools and techniques, or the development of new products and services\n\nPossesses skills, awareness and consistent with project management, analytical and application skills\n\nConsistently determines and executes basic research and/or referrals resiliently\n\nDemonstrates level of experience/ability to influence, and understand business problems in technical terminology\n\nEffective analytical and problem-solving skills\n\nExtremely effective interpersonal skills and relationship building and able to liaise with staff at all levels in the organization\n\nExcellent writing skills, with the ability to create clear requirements, specifications and documentation for data systems",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Production support', 'Data management', 'Project management', 'Configuration management', 'Agile', 'microsoft', 'Risk management', 'Release management', 'Monitoring', 'Python']",2025-06-12 13:54:52
Jr.AI Engineer,Tekone It Services,1 - 3 years,1.5-6.5 Lacs P.A.,['Hyderabad'],"Position Overview\nWe are hiring five AI Engineers with 12 years of experience to join our dynamic team in Hyderabad. The ideal candidates will have a solid foundation in Large Language Models (LLMs), LangChain, and Generative AI (GenAI) frameworks. This is a great opportunity to work on innovative AI solutions, contributing to projects that integrate LLMs, prompt engineering, RAG pipelines, and cloud-based deployments.\nKey Responsibilities\nContribute to the design and development of AI-powered applications utilizing LLMs (GPT-3.5, GPT-4, Gemini).\nAssist in building LangChain-based pipelines and workflows, including LangSmith and LangGraph.\nSupport the implementation of Retrieval-Augmented Generation (RAG) frameworks using vector databases such as ChromaDB.\nApply prompt engineering techniques to optimize model responses and improve contextual accuracy.\nDevelop RESTful APIs using Flask or FastAPI to enable model consumption in production environments.\nWrite and manage data workflows using SQL, PySpark, and Spark SQL.\nDeploy and monitor models on Azure Machine Learning or AWS Bedrock platforms.\nCollaborate with cross-functional teams, including data scientists, engineers, and business stakeholders.\nRequired Skills\nProficiency in Python, SQL, PySpark, and Spark SQL\nHands-on experience with LLMs: GPT-3.5, GPT-4, Gemini\nKnowledge of LangChain, LangSmith, LangGraph\nFamiliarity with Vector Databases (e.g., ChromaDB) and embeddings\nExperience with prompt engineering and RAG-based architectures\nExposure to cloud platforms such as Azure ML or AWS Bedrock\nStrong understanding of REST APIs and version control systems (Git/GitHub)\nPreferred Qualifications\nBachelor's degree in Computer Science, Artificial Intelligence, Data Science, or a related field\nInternship or academic project experience in NLP, LLMs, or GenAI technologies\nFamiliarity with MLOps tools and practices (e.g., CI/CD, Airflow)\nStrong problem-solving abilities, attention to detail, and a collaborative mindset",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'Prompt Engineering', 'Artificial Intelligence', 'llm']",2025-06-12 13:54:54
"Engineer, Release Management",XL India Business Services Pvt. Ltd,2 - 6 years,Not Disclosed,['Gurugram'],"Engineer, Release Management Gurgaon / Bangalore, India AXA XL offers risk transfer and risk management solutions to clients globally\n\nWe offer worldwide capacity, flexible underwriting solutions, a wide variety of client-focused loss prevention services and a team-based account management approach\n\nAXA XL recognizes data and information as critical business assets, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained unique features\n\nOur Chief Data Office also known as our Enterprise Business Data Solutions team (EDS) is focused on driving innovation through optimizing how we leverage data to drive strategy and create a new business model - disrupting the insurance market\n\nThis role is part of the Data Build Management, which is responsible for supporting various teams to plan, build and support through the product lifecycle to ensure, performance stability and reliability of data solutions by providing world-class support\n\nWhat you ll be DOING What will your essential responsibilities include? Provide top-class Build, Release Management, and DevSecOps functionalities and support for various data solutions owned and managed by the IDA organization\n\nAnalyze and mitigate risks (technical or otherwise) associated with data solution build and release delivery timelines\n\nProvide support for data pipelines and DataOps in cloud and big data environments\n\nEnforce governance and reporting, and ensure adherence to policies, standards, and best practices for Build Pipelines and Release Management\n\nPlan the release of project deliverables and manage the release lifecycle\n\nCommunicate project-related tasks such as plans, timelines, and requirements\n\nbetween different teams\n\nCoordinate the release schedule and resources required based on third-party applications, defect backlogs, planned releases, and infrastructure updates\n\nIdentify risks that could delay the release and manage them to ensure that the scheduled scope and quality of the release is not affected\n\nTrack progress and identify issues, if any, while continuously working to improve the release process\n\nEnsure that the release is planned according to requirements and budget\n\nSchedule release readiness reviews before deployment and milestone reviews after each release\n\nCreate implementation and deployment plans in accordance with the release schedule\n\nPlan and provide weekly updates on release activities\n\nLead Go-Live activities to ensure successful software deployment\n\nCollaborate with relevant development teams responsible for building the automation tools used to develop and deploy the software\n\nAttend CAB meetings to discuss release schedules with the team and identify any roadblocks\n\nMaintain documentation related to build and release procedures, various notification lists, and dependencies\n\nDemonstrate proactive communication with business users, development teams, technology, production support, delivery teams, and senior management\n\nEstablish a feedback loop by collaborating with internal and external teams\n\nPartner with the Product and Production Support teams as a Build/Release/Technical Subject Matter Expert (SME) for new development or migration of re-architected product functionalities to the new cloud platform\n\nOversee the development and maintenance of Build and Release Management processes and their documentation\n\nBuild and maintain various critical monitoring processes, alerts, and overall health reports (performance and functional) for production and pre-production environments to be used by the Production Support Teams\n\nEnsure timely and accurate completion of emergency release pipelines/processes in a manner that is auditable, testable, and maintainable\n\nEnsure that all builds are consistent with solution design, security recommendations, and business specifications\n\nAbility to work in a ""Follow the Sun"" support model, providing coverage across Digital Data Dev division responsibilities\n\nAchieve and maintain the highest levels of business customer confidence and net promoter score (NPS)\n\nYou will report to the Scientist, Digital Data Development\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: A minimum of an Undergraduate University Degree in Computer Science or related fields with years of experience\n\nExperience in systems integration, and developer support tools Azure DevOps, Docker, Kubernetes, Harness, CICD, release management, and configuration management, Python\n\nUnderstanding of DataOps, ITIL and AGILE methodologies\n\nExperience in building reports & dashboards using Excel, and Power BI\n\nRelevant years of experience in supporting areas of Data Management\n\nOwn availability, scalability, and efficiency for day-to-day operations\n\nPossess a robust understanding of Azure fundamentals (Microsoft AZ-900)\n\nExperience using tools like Git, JIRA, etc Knowledge of artifact repositories such as JFrog and X-Ray\n\nDesired Skills and Abilities: Ability to troubleshoot issues and able to handle different types of user inquiries\n\nExercises judgment for decision-making on complex issues that impact the delivery\n\nMakes recommendations to management on new processes, tools and techniques, or the development of new products and services\n\nPossesses skills, awareness and consistent with project management, analytical and application skills\n\nConsistently determines and executes basic research and/or referrals resiliently\n\nDemonstrates level of experience/ability to influence, and understand business problems in technical terminology\n\nEffective analytical and problem-solving skills\n\nExtremely effective interpersonal skills and relationship building and able to liaise with staff at all levels in the organization\n\nExcellent writing skills, with the ability to create clear requirements, specifications and documentation for data systems",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Production support', 'Data management', 'Project management', 'Configuration management', 'Agile', 'microsoft', 'Risk management', 'Release management', 'Monitoring', 'Python']",2025-06-12 13:54:56
Machine Learning Engineer,Draft N Craft Legal Outsourcing,2 - 3 years,10-15 Lacs P.A.,['New Delhi'],"Role Overview:\n\nAs an ML Engineer you will embark on a wonderful journey of developing and implementing various Machine Learning models that will ultimately act as an enabler in the companys growth.\nSince Draft Craft is a legal services-oriented company in which we serve clients from across the border, your work will be majorly aimed towards creating ML workflows that will serve the legal industry and help improve efficiency of the in-house teams. Draft n Craft offers you the perfect opportunity to grow and hone your ML/Data Engineering skills while contributing towards building a worthwhile product.\n\nKey Responsibilities:\nDeveloping data ingestion & data preprocessing pipelines for transforming data presented for legal requirements to extract key and actionable insights. \nData cleaning for supplying accurate, consistent & relevant content to ML models.\nExploring and experimenting with different ML models and architectures that can be used with data from the legal industry in a safe and compliant manner.\nDeveloping and deploying ML models that can function in production environments for the use-cases required by the company.\nAnalyzing key metrics for model performance and devising methods to improve efficiencies of the models.\nDocument the steps involved in data preprocessing, model development, and optimizations undertaken.\nExplore techniques for feature extraction, transformation, and selection to improve model performance.\nUnderstanding software development related terminologies to collaborate with the existing team of software engineers within the company.\nDevise methods of integration of the ML models within the company’s already developed software solutions and employee workflows.\n\nRequired Qualifications:\nDegree holder from Computer Science Engineering, Data Science or related fields.\nMinimum experience of 2 years working as an ML engineer or Data Scientist in a professional capacity.\nStrong programming skills including python and familiarity with related libraries Tensorflow, PyTorch, Pandas etc.\nDatabase Querying in terms of SQL/NoSQL.\nWorking with data extracting and ETL pipelines for pre-processing of data/documents.\nRelevant experience working in NLP, Neural Networks, and Gen AI technologies.\nFamiliarity with working or applying transfer-learning on LLM models like Llama, etc.\nSome experience in software development is preferred.\nKnowledge of deploying ML models and data pipelines to cloud services like AWS/Azure.",Industry Type: Legal,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Generative Ai', 'Retrieval Augmented Generation', 'Python', 'NoSQL', 'Large Language Model', 'Data Extraction', 'Machine Learning', 'SQL']",2025-06-12 13:54:58
Machine Learning Engineer,Tek Ninjas,8 - 13 years,Not Disclosed,['Pune'],"Skills:\n     Proficient:\nLanguages/Framework: Fast API, Azure UI Search API (React)\nCloud: Azure Cloud Basics (Azure DevOps)\nGitlab: Gitlab Pipeline\nAnsible and REX: Rex Deployment\nData Science: Prompt Engineering + Modern Testing\nData pipeline development\nUnderstanding of AI/ML algorithms and their applications\nMLOps frameworks\nKnowledge of cloud platforms (Azure ML especially)\nModel deployment process\nData pipeline monitoring\nLanguages/Framework: Azure Open AI\nData Science: Open AI GPT Family of models 4o/4/3, Embeddings + Vector Search\nDatabases and ETL: Azure Storage Account, Postgresql, Cosmos\nExperience with ML frameworks (TensorFlow, PyTorch, Scikit-learn)\nKnowledge of cloud platforms (AWS SageMaker, Google AI Platform)\nExpertise in data preprocessing, feature engineering, and model evaluation\nUnderstanding of software engineering principles (version control, CI/CD, containerization)\nFamiliarity with distributed computing and big data tools (Spark, Hadoop)\nAbility to optimize models for performance and scalability\nExperience with Azure AI Search\nDesired skills*\nAzure DevOps; MLOps frameworks; Postgresql; Cosmos",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['MLOPS', 'Aiml', 'Machine Learning', 'Azure Devops']",2025-06-12 13:55:01
Ai Ml Engineer,Tek Ninjas,8 - 13 years,Not Disclosed,['Pune'],"Skills:\nProficient:\nLanguages/Framework: Fast API, Azure UI Search API (React)\nCloud: Azure Cloud Basics (Azure DevOps)\nGitlab: Gitlab Pipeline\nAnsible and REX: Rex Deployment\nData Science: Prompt Engineering + Modern Testing\nData pipeline development\nUnderstanding of AI/ML algorithms and their applications\nMLOps frameworks\nKnowledge of cloud platforms (Azure ML especially)\nModel deployment process\nData pipeline monitoring\nLanguages/Framework: Azure Open AI\nData Science: Open AI GPT Family of models 4o/4/3, Embeddings + Vector Search\nDatabases and ETL: Azure Storage Account, PostgreSQL, Cosmos\nExperience with ML frameworks (TensorFlow, PyTorch, Scikit-learn)\nKnowledge of cloud platforms (AWS SageMaker, Google AI Platform)\nExpertise in data preprocessing, feature engineering, and model evaluation\nUnderstanding of software engineering principles (version control, CI/CD, containerization)\nFamiliarity with distributed computing and big data tools (Spark, Hadoop)\nAbility to optimize models for performance and scalability\nExperience with Azure AI Search\nDesired skills*\nAzure DevOps; MLOps frameworks; Postgresql; Cosmos",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['M lops', 'Machine Learning', 'Azure Devops', 'Azure Machine Learning']",2025-06-12 13:55:03
Software Engineer-8910,WebMD,3 - 8 years,Not Disclosed,['Navi Mumbai'],"Position: Software Engineer\npositions: 1\nNo.of\nAbout Company:\nHeadquartered in El Segundo, Calif., Internet Brands is a fully integrated online media and\nsoftware services organization focused on four high-value vertical categories: Health,\nAutomotive, Legal, and Home/Travel. The company's award-winning consumer websites\nlead their categories and serve more than 250 million monthly visitors, while a full range of\nweb presence offerings has established deep, long-term relationships with SMB and\nenterprise clients. Internet Brands' powerful, proprietary operating platform provides the\nflexibility and scalability to fuel the company's continued growth. Internet Brands is a portfolio\ncompany of KKR and Temasek.\nWebMD Health Corp., an Internet Brands Company, is the leading provider of health\ninformation services, serving patients, physicians, health care professionals, employers, and\nhealth plans through our public and private online portals, mobile platforms, and health-\nfocused publications. The WebMD Health Network includes WebMD Health, Medscape,\nJobson Healthcare Information, prIME Oncology, MediQuality, Frontline, QxMD, Vitals\nConsumer Services, MedicineNet, eMedicineHealth, RxList, OnHealth, Medscape\nEducation, and other owned WebMD sites. WebMD, Medscape, CME Circle,\nMedpulse®, eMedicine®, MedicineNet®, theheart.org®, and RxList® are among the\ntrademarks of WebMD Health Corp. or its subsidiaries.\nAll qualified applicants will receive consideration for employment without regard to race,\ncolor, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran\nstatus.\nFor Company details, visit our website: www.webmd.com and www.internetbrands.com\nAll qualified applicants will receive consideration for employment without regard to\nrace, color, religion, sex, sexual orientation, gender identity, national origin, disability,\nor veteran status.\nFor Company details, visit our website: www.webmd.com\nhttps://www.webmd.com/corporate/physicians-interactive",,,,"['ETL', 'SQL', 'Data Warehousing']",2025-06-12 13:55:05
BI Engineer,Amgen Inc,2 - 5 years,Not Disclosed,['Hyderabad'],"Role Description:\nLets do this. We are seeking an experienced Senior BI Engineer to lead the design, development, and optimization of scalable business intelligence (BI) solutions that empower data-driven decision-making across the organization. The ideal candidate is highly skilled in data modeling, dashboard development, ETL design, and cloud-based BI platforms, with a passion for turning complex data into clear, actionable insights. As a senior member of the BI team, you will work closely with data engineers, analysts, business stakeholders, and product teams to deliver robust, user-friendly analytics solutions that support strategic and operational goals.\nRoles & Responsibilities:\nDesign, develop, and maintain enterprise-grade BI dashboards and reports using tools like Power BI, Tableau, or Looker.\nBuild and optimize semantic models, tabular data structures, and reusable datasets for self-service BI users.\nPartner with business stakeholders to translate requirements into technical solutions, delivering accurate, relevant, and timely insights.\nWork closely with data engineering teams to integrate BI solutions into data lake, warehouse, or lakehouse architectures (e.g., Snowflake, Redshift, Databricks, BigQuery).\nImplement best practices for BI development, including version control, performance optimization, and data governance.\nEnsure BI solutions are secure, scalable, and aligned with enterprise data governance standards.\nMentor junior BI developers and analysts, setting standards for dashboard usability, data visualization, and design consistency.\nCollaborate with cross-functional teams to promote self-service BI adoption and data literacy throughout the organization.\nMonitor BI performance, usage, and adoption, providing continuous improvements and training to enhance impact.\nMust-Have Skills:\n5 - 8 years of experience in BI development and data visualization, with deep expertise in tools such as Power BI, Tableau, or Looker.\nStrong knowledge of SQL, data modeling techniques, and BI architecture best practices.\nExperience working with data warehouses and cloud data platforms\nProficiency in building dashboards, KPIs, and executive-level reporting that align with business priorities.\nSolid understanding of ETL/ELT processes, data pipelines, and integration with BI tools.\nStrong collaboration skills with the ability to work effectively across engineering, product, finance, and business teams.\nExcellent communication skills, with a proven ability to translate technical concepts into business value.\nGood-to-Have Skills:\nExperience in cloud platforms (AWS, Azure, or GCP) and modern data stack environments.\nFamiliarity with data governance, data cataloging, metadata management, and access control.\nExposure to Agile methodologies, CI/CD for BI, and DevOps practices.\nBI or data certifications (e.g., Microsoft Certified: Power BI Data Analyst, Tableau Certified Professional).\nEducation and Professional Certifications\nMasters degree and 3 to 4 + years of Computer Science, IT or related field experience\nOR\nBachelors degree and 5 to 8 + years of Computer Science, IT or related field experience\nPowerBI / Tableau certifications preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['BI development', 'Azure', 'Power BI', 'BI tools', 'Agile methodologies', 'data modeling techniques', 'Tableau', 'SQL', 'BI architecture', 'GCP', 'CI/CD', 'cloud data platforms', 'data visualization', 'data warehouses', 'AWS']",2025-06-12 13:55:07
Engineering Manager,Amgen Inc,10 - 14 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are seeking a seasoned Engineering Manager (Data Engineering) to lead the end-to-end management of enterprise data assets and operational data workflows. This role is critical in ensuring the availability, quality, consistency, and timeliness of data across platforms and functions, supporting analytics, reporting, compliance, and digital transformation initiatives. You will be responsible for the day-to-day data operations, manage a team of data professionals, and drive process excellence in data intake, transformation, validation, and delivery. You will work closely with cross-functional teams including data engineering, analytics, IT, governance, and business stakeholders to align operational data capabilities with enterprise needs.\nRoles & Responsibilities:\nLead and manage the enterprise data operations team, responsible for data ingestion, processing, validation, quality control, and publishing to various downstream systems.\nDefine and implement standard operating procedures for data lifecycle management, ensuring accuracy, completeness, and integrity of critical data assets.\nOversee and continuously improve daily operational workflows, including scheduling, monitoring, and troubleshooting data jobs across cloud and on-premise environments.\nEstablish and track key data operations metrics (SLAs, throughput, latency, data quality, incident resolution) and drive continuous improvements.\nPartner with data engineering and platform teams to optimize pipelines, support new data integrations, and ensure scalability and resilience of operational data flows.\nCollaborate with data governance, compliance, and security teams to maintain regulatory compliance, data privacy, and access controls.\nServe as the primary escalation point for data incidents and outages, ensuring rapid response and root cause analysis.\nBuild strong relationships with business and analytics teams to understand data consumption patterns, prioritize operational needs, and align with business objectives.\nDrive adoption of best practices for documentation, metadata, lineage, and change management across data operations processes.\nMentor and develop a high-performing team of data operations analysts and leads.\nFunctional Skills:\nMust-Have Skills:\nExperience managing a team of data engineers in biotech/pharma domain companies.\nExperience in designing and maintaining data pipelines and analytics solutions that extract, transform, and load data from multiple source systems.\nDemonstrated hands-on experience with cloud platforms (AWS) and the ability to architect cost-effective and scalable data solutions.\nExperience managing data workflows in cloud environments such as AWS, Azure, or GCP.\nStrong problem-solving skills with the ability to analyze complex data flow issues and implement sustainable solutions.\nWorking knowledge of SQL, Python, or scripting languages for process monitoring and automation.\nExperience collaborating with data engineering, analytics, IT operations, and business teams in a matrixed organization.\nFamiliarity with data governance, metadata management, access control, and regulatory requirements (e.g., GDPR, HIPAA, SOX).\nExcellent leadership, communication, and stakeholder engagement skills.\nWell versed with full stack development & DataOps automation, logging frameworks, and pipeline orchestration tools.\nStrong analytical and problem-solving skills to address complex data challenges.\nEffective communication and interpersonal skills to collaborate with cross-functional teams.\nGood-to-Have Skills:\nData Engineering Management experience in Biotech/Life Sciences/Pharma\nExperience using graph databases such as Stardog or Marklogic or Neo4J or Allegrograph, etc.\nEducation and Professional Certifications\n9 to 12 years of experience in Computer Science, IT or related field\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data operations', 'fullstack development', 'GCP', 'stakeholder engagement', 'troubleshooting', 'cloud platforms', 'AWS']",2025-06-12 13:55:09
Machine Learning Engineer (Full Stack),Hubnex,5 - 10 years,Not Disclosed,['Gurugram'],"Machine Learning Engineer (Full Stack)\nLocation: Gurugram, India (On-site/Hybrid)\nType: Full-Time | 5+ Years Experience | AI & Product Engineering\nHubnex Labs is seeking a highly skilled Machine Learning Engineer with strong capabilities in Full Stack Development to lead the development and deployment of production-grade AI systems. This role requires expertise in building end-to-end ML pipelines from data preprocessing to deployment while also contributing to the full stack of software platforms that power our solutions.\nKey Responsibilities Machine Learning & Data Science\nUnderstand business goals and translate them into ML-based solutions\nDevelop, analyze, and compare machine learning algorithms for various problem statements\nBuild robust validation strategies and design appropriate preprocessing and feature engineering pipelines\nPerform data exploration, visualization , and quality verification , including data cleaning and augmentation\nTrain models, tune hyperparameters , and interpret model performance\nAnalyze errors and design strategies to improve model robustness\nDiscover and utilize public datasets for model training and benchmarking\nDeploy models into production environments with real-world performance and latency considerations\nSoftware & System Development\nDesign and develop end-to-end production systems , including backend APIs and frontend interfaces\nMaintain full stack web applications , ensuring seamless ML model integration\nEnsure efficient use of hardware resources for training and inference\nCollaborate cross-functionally with engineering, product, and design teams\nTechnical Skills Required\n5+ years of hands-on experience in machine learning and full stack development\nProficiency in Python and ML libraries like scikit-learn , pandas , NumPy , etc.\nDeep learning experience using TensorFlow , Keras , or equivalent frameworks\nProficiency with OpenCV and image/video processing techniques\nExperience with data visualization tools and big data handling\nStrong understanding of data pipelines , feature engineering , and augmentation techniques\nProficiency in Full Stack Web Development (e.g., React, Node.js, Express, MongoDB or similar)\nExperience deploying models using REST APIs, Flask/FastAPI, Docker, etc.\nFamiliarity with Linux environments and GPU-accelerated compute systems\nUnderstanding of hardware requirements and optimization for real-time ML performance\nWhy Join Hubnex Labs?\nWork on impactful AI products deployed in real-world use cases\nBe a part of a fast-growing tech consulting and product innovation company\nCollaborate with a diverse team of engineers, data scientists, and innovators\nFlexible and collaborative culture based in Gurugram , with hybrid work options\nIdeal Candidate\nPassionate about building smart systems that go live in production\nCan operate independently and take full ownership of ML products\nBlends deep technical skill with product thinking and business awareness",Industry Type: Internet,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Product engineering', 'Backend', 'Linux', 'Product innovation', 'Consulting', 'Machine learning', 'Web development', 'MongoDB', 'Business awareness', 'Python']",2025-06-12 13:55:12
Etl Engineer,Global Energy,4 - 9 years,Not Disclosed,[],"Role & responsibilities\n\nWe are looking for 5 years of experience in ETL\n\nWork mode :Remote\n\nMandatory skills : CData, ETl\n\nNeed a dedicated ETL engineer to manage all Extract, Transform, Load (ETL) processes.\nStrong expertise in ETL workflows is required.\nMust have hands-on experience with C-Data, as it is the primary tool used by the client.\nPreferably someone with advanced or in-depth experience in C-Data, not just basic knowledge.\nSome exposure or understanding of Salesforce is expected, since it is the client's main system.\nFamiliarity with complex healthcare data is preferred, as the data can be intricate and challenging to work with.\nData handling with C-Data drivers & knowledge should be there",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Cdata sync', 'ETL', 'Salesforce Integration', 'Etl Process', 'Informatica']",2025-06-12 13:55:13
Lead Power BI Developer,Conduent,8 - 13 years,Not Disclosed,['Bengaluru'],"Job Overview: \nWe are looking for a BI & Visualization Developer who will be part of our Analytics Practice and will be expected to actively work in a multi-disciplinary fast paced environment. This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project; its primary responsibility is to support the design, development and maintainance of business intelligence and analytics solutions.\n\n\n Responsibilities: \nDevelop reports, dashboards, and advanced visualizations. Works closely with the product managers, business analysts, clients etc. to understand the needs / requirements and develop visualizations needed.\nProvide support to new of existing applications while recommending best practices and leading projects to implement new functionality.\nLearn and develop new visualization techniques as required to keep up with the contemporary visualization design and presentation.\nReviews the solution requirements and architecture to ensure selection of appropriate technology, efficient use of resources and integration of multiple systems and technology.\nCollaborate in design reviews and code reviews to ensure standards are met. Recommend new standards for visualizations.\nBuild and reuse template/components/web services across multiple dashboards\nSupport presentations to Customers and Partners\nAdvising on new technology trends and possible adoption to maintain competitive advantage\nMentoring Associates\n\n\n Experience Needed: \n8+ years of related experience is required.\nA Bachelor degree or Masters degree in Computer Science or related technical discipline is required\nHighly skilled in data visualization tools like PowerBI, Tableau, Qlikview etc.\nVery Good Understanding of PowerBI Tabular Model/Azure Analysis Services using large datasets.\nStrong SQL coding experience with performance optimization experience for data queries.\nUnderstands different data models like normalized, de-normalied, stars, and snowflake models.\nWorked in big data environments, cloud data stores, different RDBMS and OLAP solutions.\nExperience in design, development, and deployment of BI systems.\nCandidates with ETL experience preferred.\nIs familiar with the principles and practices involved in development and maintenance of software solutions and architectures and in service delivery.\nHas strong technical background and remains evergreen with technology and industry developments.\nAdditional\nDemonstrated ability to have successfully completed multiple, complex technical projects\nPrior experience with application delivery using an Onshore/Offshore model\nExperience with business processes across multiple Master data domains in a services based company\nDemonstrates a rational and organized approach to the tasks undertaken and an awareness of the need to achieve quality.\nDemonstrates high standards of professional behavior in dealings with clients, colleagues and staff.\nStrong written communication skills. Is effective and persuasive in both written and oral communication.\nExperience with gathering end user requirements and writing technical documentation\nTime management and multitasking skills to effectively meet deadlines under time-to-market pressure\nMay require occasional travel",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql coding', 'azure analysis services', 'sql', 'tableau', 'tabular model', 'snowflake', 'rdbms', 'web services', 'bi', 'power bi', 'business analysis', 'business intelligence', 'dashboards', 'qlikview', 'report development', 'azure analysis', 'olap', 'code review', 'data visualization', 'etl', 'big data']",2025-06-12 13:55:16
Starburst Engineer,Luxoft,1 - 13 years,Not Disclosed,['Pune'],"Design, develop, and maintain scalable data solutions using Starburst.\nCollaborate with cross-functional teams to integrate Starburst with existing data sources and tools.\nOptimize query performance and ensure data security and compliance.\nImplement monitoring and alerting systems for data platform health.\nStay updated with the latest developments in data engineering and analytics.\nSkills\nMust have\nBachelors degree or Masters in a related technical field; or equivalent related professional experience.\nPrior experience as a Software Engineer applying new engineering principles to improve existing systems including leading complex, well defined projects.\nStrong knowledge of Big-Data Languages including:\nSQL\nHive\nSpark/Pyspark\nPresto\nPython\nStrong knowledge of Big-Data Platforms, such as:o The Apache Hadoop ecosystemo AWS EMRo Qubole or Trino/Starburst\nGood knowledge and experience in cloud platforms such as AWS, GCP, or Azure.\nContinuous learner with the ability to apply previous experience and knowledge to quickly master new technologies.\nDemonstrates the ability to select among technology available to implement and solve for need.\nAble to understand and design moderately complex systems.\nUnderstanding of testing and monitoring tools.\nAbility to test, debug, fix issues within established SLAs.\nExperience with data visualization tools (e.g., Tableau, Power BI).\nUnderstanding of data governance and compliance standards.\nNice to have\nData Architecture & Engineering: Design and implement efficient and scalable data warehousing solutions using Azure Databricks and Microsoft Fabric.\nBusiness Intelligence & Data Visualization: Create insightful Power BI dashboards to help drive business decisions.\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Azure AI & ML Engineer\nData Science\nUnited States of America\nAlpharetta\nSenior Azure AI & ML Engineer\nData Science\nUnited States of America\nRemote United States\nData Engineer with Neo4j\nData Science\nIndia\nChennai\nPune, India\nReq. VR-114886\nData Science\nBCM Industry\n05/06/2025\nReq. VR-114886\nApply for Starburst Engineer in Pune\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data security', 'data governance', 'Engineering Design', 'Apache', 'Business intelligence', 'microsoft', 'Analytics', 'SQL', 'Python', 'Data architecture']",2025-06-12 13:55:18
Lead Pyspark Developer,Synechron,5 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","job requisition idJR1027452\n\n\n\nOverall Responsibilities:\nData Pipeline Development:Design, develop, and maintain highly scalable and optimized ETL pipelines using PySpark on the Cloudera Data Platform, ensuring data integrity and accuracy.\nData Ingestion:Implement and manage data ingestion processes from a variety of sources (e.g., relational databases, APIs, file systems) to the data lake or data warehouse on CDP.\nData Transformation and Processing:Use PySpark to process, cleanse, and transform large datasets into meaningful formats that support analytical needs and business requirements.\nPerformance Optimization:Conduct performance tuning of PySpark code and Cloudera components, optimizing resource utilization and reducing runtime of ETL processes.\nData Quality and Validation:Implement data quality checks, monitoring, and validation routines to ensure data accuracy and reliability throughout the pipeline.\nAutomation and Orchestration:Automate data workflows using tools like Apache Oozie, Airflow, or similar orchestration tools within the Cloudera ecosystem.\nMonitoring and Maintenance:Monitor pipeline performance, troubleshoot issues, and perform routine maintenance on the Cloudera Data Platform and associated data processes.\nCollaboration:Work closely with other data engineers, analysts, product managers, and other stakeholders to understand data requirements and support various data-driven initiatives.\nDocumentation:Maintain thorough documentation of data engineering processes, code, and pipeline configurations.\n\n\n\nSoftware :\nAdvanced proficiency in PySpark, including working with RDDs, DataFrames, and optimization techniques.\nStrong experience with Cloudera Data Platform (CDP) components, including Cloudera Manager, Hive, Impala, HDFS, and HBase.\nKnowledge of data warehousing concepts, ETL best practices, and experience with SQL-based tools (e.g., Hive, Impala).\nFamiliarity with Hadoop, Kafka, and other distributed computing tools.\nExperience with Apache Oozie, Airflow, or similar orchestration frameworks.\nStrong scripting skills in Linux.\n\n\n\nCategory-wise Technical\n\nSkills:\nPySpark:Advanced proficiency in PySpark, including working with RDDs, DataFrames, and optimization techniques.\nCloudera Data Platform:Strong experience with Cloudera Data Platform (CDP) components, including Cloudera Manager, Hive, Impala, HDFS, and HBase.\nData Warehousing:Knowledge of data warehousing concepts, ETL best practices, and experience with SQL-based tools (e.g., Hive, Impala).\nBig Data Technologies:Familiarity with Hadoop, Kafka, and other distributed computing tools.\nOrchestration and Scheduling:Experience with Apache Oozie, Airflow, or similar orchestration frameworks.\nScripting and Automation:Strong scripting skills in Linux.\n\n\n\nExperience:\n5-12 years of experience as a Data Engineer, with a strong focus on PySpark and the Cloudera Data Platform.\nProven track record of implementing data engineering best practices.\nExperience in data ingestion, transformation, and optimization on the Cloudera Data Platform.\n\n\n\nDay-to-Day Activities:\nDesign, develop, and maintain ETL pipelines using PySpark on CDP.\nImplement and manage data ingestion processes from various sources.\nProcess, cleanse, and transform large datasets using PySpark.\nConduct performance tuning and optimization of ETL processes.\nImplement data quality checks and validation routines.\nAutomate data workflows using orchestration tools.\nMonitor pipeline performance and troubleshoot issues.\nCollaborate with team members to understand data requirements.\nMaintain documentation of data engineering processes and configurations.\n\n\n\nQualifications:\nBachelors or Masters degree in Computer Science, Data Engineering, Information Systems, or a related field.\nRelevant certifications in PySpark and Cloudera technologies are a plus.\n\n\n\nSoft\n\nSkills:\nStrong analytical and problem-solving skills.\nExcellent verbal and written communication abilities.\nAbility to work independently and collaboratively in a team environment.\nAttention to detail and commitment to data quality.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cloudera', 'hive', 'pyspark', 'linux', 'hadoop', 'scala', 'amazon redshift', 'data warehousing', 'emr', 'sql', 'docker', 'apache', 'java', 'spark', 'gcp', 'etl', 'big data', 'hbase', 'data lake', 'python', 'oozie', 'airflow', 'microsoft azure', 'impala', 'data engineering', 'nosql', 'amazon ec2', 'mapreduce', 'kafka', 'sqoop', 'aws']",2025-06-12 13:55:21
Specialist Software Engineer - Large Molecule Discovery,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will.\nRole Description:\nThe role is responsible for designing, developing, and maintaining software solutions for Research scientists. Additionally, it involves automating operations, monitoring system health, and responding to incidents to minimize downtime.\nYou will join a multi-functional team of scientists and software professionals that enables technology and data capabilities to evaluate drug candidates and assess their abilities to affect the biology of drug targets. This team implements scientific software platforms that enable the capture, analysis, storage, and reporting for our Large Molecule Discovery Research team (Design, Make, Test and Analyze processes).\nThe team also interfaces heavily with teams supporting our in vitro assay management systems and our compound inventory platforms. The ideal candidate possesses experience in the pharmaceutical or biotech industry, strong technical skills, and full stack software engineering experience (spanning SQL, back-end, front-end web technologies, automated testing).\nRoles & Responsibilities:\nTake ownership of complex software projects from conception to deployment\nWork closely with product team, business team including scientists, and other collaborators\nAnalyze and understand the functional and technical requirements of applications, solutions and systems and translate them into software architecture and design specifications\nDesign, develop, and implement applications and modules, including custom reports, interfaces, and enhancements\nDevelop and execute unit tests, integration tests, and other testing strategies to ensure the quality of the software\nConduct code reviews to ensure code quality and alignment to standard methodologies\nCreate and maintain documentation on software architecture, design, deployment, disaster recovery, and operations\nProvide ongoing support and maintenance for applications, ensuring that they operate smoothly and efficiently\nStay updated with the latest technology and security trends and advancements\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The professional we seek is a [type of person] with these qualifications.\nBasic Qualifications:\nDoctorate Degree OR Masters degree with 4 - 6 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nBachelors degree with 6 - 8 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/ Bioinformatics or related field OR\nDiploma with 10 - 12 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/ Bioinformatics or related field\nPreferred Qualifications and Experience:\n3+ years of experience in implementing and supporting biopharma scientific software platforms\nSome experience with ML or generative AI technologies\nProficient in Java or Python\nProficient in at least one JavaScript UI Framework (e.g. ExtJS, React, or Angular)\nProficient in SQL (e.g. Oracle, PostgreSQL, Databricks)\nExperience with event-based architecture and serverless AWS services such as EventBridge, SQS, Lambda or ECS.\nPreferred Qualifications:\nExperience with Benchling\nHands-on experience with Full Stack software development\nStrong understanding of software development methodologies, mainly Agile and Scrum\nWorking experience with DevOps practices and CI/CD pipelines\nExperience of infrastructure as code (IaC) tools (Terraform, CloudFormation)\nExperience with monitoring and logging tools (e.g., Prometheus, Grafana, Splunk)\nExperience with automated testing tools and frameworks\nExperience with big data technologies (e.g., Spark, Databricks, Kafka)\nExperience with leveraging the use of AI-assistants (e.g. GitHub Copilot) to accelerate software development and improve code quality\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nAWS Certified Cloud Practitioner preferred\nSoft Skills:\nExcellent problem solving, analytical, and troubleshooting skills\nStrong communication and interpersonal skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to learn quickly & work independently\nTeam-oriented, with a focus on achieving team goals\nAbility to manage multiple priorities successfully\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Large Molecule Discovery', 'Java', 'DevOps', 'PostgreSQL', 'JavaScript', 'Databricks', 'Oracle', 'React', 'AWS', 'ExtJS', 'Angular', 'Python']",2025-06-12 13:55:23
Informatica IDMC Engineer,Qentelli,5 - 10 years,15-25 Lacs P.A.,['Hyderabad'],"Job Summary:\nWe are seeking a highly skilled Informatica IDMC & Data Governance Specialist to join our dynamic team. This role requires hands-on expertise in various IDMC modules, including Snowflake, Cloud Data Quality (CDQ), Cloud Application Integration (CAI), Cloud Data Governance & Catalog (CDGC), and Cloud Data Marketplace (CDMP). The ideal candidate will have a strong background in data governance and data quality, with a proven ability to create and configure data profiles and quality rules. If you have a passion for cloud technologies and data management, this is an exciting opportunity to contribute to the ongoing success of our data initiatives.\n\nKey Responsibilities:\nIDMC Modules Management: Manage and support the following IDMC modules:\nSnowflake (including RBAC)\nCloud Data Quality (CDQ)\nCloud Application Integration (CAI)\nCloud Data Governance & Catalog (CDGC)\nCloud Data Marketplace (CDMP)\nData Profiling & Quality: Create and configure Data Profiles with appropriate quality rules to ensure data meets required standards.\nData Governance & Compliance: Collaborate with data governance teams to ensure compliance with data governance policies and standards.\nPlatform Optimization & Troubleshooting: Continuously monitor the performance of IDMC platforms, resolving issues, optimizing configurations, and supporting ongoing integrations and data workflows.\nCollaboration & Support: Work cross-functionally with infrastructure, application, and data teams to ensure efficient use of the IDMC platform and ensure business data needs are met.\n\nMandatory Skills:\nExtensive work experience is required for below areas to configure everything from scratch for any complex requirement\nCDI (Cloud Data Integration)\nCDQ (Cloud Data Quality)\nCAI (Cloud Application Integration)\nEDC (On-Prem Informatica Enterprise Data Catalog)\n\nNice-to-Have Skills:\nKnowledge of Data Integration (IPC) and Git.\nPrior experience in Data Governance initiatives, including the use of governance frameworks and best practices.\n\nQualifications:\nStrong analytical and problem-solving abilities.\nExcellent communication skills, both written and verbal, to explain technical concepts to nontechnical stakeholders.\nAbility to prioritize tasks, manage multiple projects, and work in a fast-paced environment.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Cloud Data Governance & Catalog', 'Cloud Application Integration', 'Cloud Data Quality', 'Cloud Data Marketplace', 'Snowflake']",2025-06-12 13:55:26
EPM Principal/Sr Principal Consultant FCCS/ARCS/TRCS/PBCS/EPBCS/PCMCS,Oracle,10 - 20 years,Not Disclosed,"['Pune', 'Bengaluru', 'Mumbai (All Areas)']","Hiring for Oracle Experts at different level from 5-20 years.\n\nJob locations - Bangalore, Mumbai, Pune, Hyderabad, Chennai, Kolkata, Noida, Gurgaon, Gandhinagar\n\nEPM Products - ARCS, TRCS, FCCS, EPBCS, PBCS, EPRCS, PCMCS\n\nPlanning - PBCS/ EPBCS\nExperience in Implementation of Hyperion with strong Application Development process experience on Hyperion EPM Product Suite.\nExperience in Requirement Gathering & Solution Design\nSound knowledge on Hyperion Planning/PBCS/EPBCS\nSound functional knowledge (Understand of planning modelling like P&L, BS, Workforce, Capex planning etc.. and inter dependencies)\nSound Knowledge on Business Rules/Forms / Task Lists / Reports.\nHands on Experience on Planning Modules is must.\nGood communication Skills\n\nFCCS\n\nFunction as applications design architect/Lead for Oracle FCCS\nApplication Design point of contact for FCCS Analyst Teams\nProvide Solutions to existing Architecture Design on the current system\nCollaborate effectively with other groups\nAdditional Requirements:\nEPM Experience 5+ Years\nExperience in Implementation of EPM cloud with strong Application Development process, experience on FCCS/HFM and good knowledge on consolidation process.\nExperience in Requirement Gathering & Solution Design\nDesired functional knowledge (Understand of Income statement, Balance Sheet, different methods of consolidation and their calculations and disclosure in financial statements)\nSound functional knowledge Finance/accounting/ General Ledger/Sub Ledgers\nSound Knowledge on Financial Reports and SmartView Reports\n\nARCS\nExperience implementing ARCS from design, configuration, data integration, and testing\nSound knowledge on ARM/ARCS including Reconciliation Compliance & Transaction Matching\nFunctional knowledge of Finance/accounting and account reconciliation is a must\nKnowledge and experience working with a consolidation tool and general ledger is a plus\nProvide Solutions to existing Architecture Design on current system\nCollaborate effectively with other groups\nAdditional Requirements:\nEPM Experience 5+ Years\nExperience in Implementation of Hyperion with strong Application Development process experience on Hyperion EPM Product Suite.\nExperience in Requirement Gathering & Solution Design\nSound functional knowledge Finance/accounting/ General Ledger/Sub Ledgers\nSound Knowledge on standard and custom reports\n\nTRCS\nFunction as applications design architect/Lead for Tax Reporting Cloud application development\nApplication Design point of contact for Tax Reporting Teams\nProvide Solutions to existing Architecture Design on current system\nCollaborate effectively with other groups\n\nEPM Experience 5+ Years\nExperience in Implementation of Hyperion with strong Application Development process experience on Hyperion EPM Product Suite.\nExperience in Requirement Gathering & Solution Design\nSound knowledge on Tax reporting and compliance processes, tax accounting and direct tax functions, CBCR and Deferred Tax Calculations\nSound knowledge on Hyperion Consolidation\nDesired functional knowledge (Understand of Income statement, Balance Sheet, different methods of consolidation and their calculations and disclosure in financial statements)\nSound Knowledge on Business Rules/Forms / Task Lists / Reports.\nGood communication Skills\n\nPCMCS\nFunction as applications design architect/Lead for Profitability and Cost Management\nApplication Design point of contact Profitability and Cost Management\nProvide Solutions to existing Architecture Design on current system\nAble to understand functional requirement of client and build the solution accordingly.\nCollaborate effectively with other groups\nAdditional Requirements:\nEPM Experience 7+ Years\nShould have completed at least 3 implementations on PCMCS.\nIn depth understanding of Oracle Hyperion Essbase (ASO and BSO)\nIn depth knowledge in Integration (Data Management)\nAbility to design and develop complex Reports using Web Reporting Studio.\nExperience in Implementation of Hyperion with strong Application Development process experience on Hyperion EPM Product Suite.\nExperience in Requirement Gathering & Solution Design\nAble to leverage the modern best practices for design-development and automation process wherever required.\nSound functional knowledge\nKnowledge on Microsoft office tools including Excel, Word and Power point, leverage Smartview to build report and ad hoc analysis.\nPCMCS Certification is a value add",Industry Type: Software Product,Department: Consulting,"Employment Type: Full Time, Permanent","['EPM', 'Cloud EPM', 'solution design', 'consulting', 'implementation', 'EPBCS', 'EPRCS', 'PBCS', 'TRCS', 'PCMCS', 'FCCS', 'ARCS']",2025-06-12 13:55:28
AI/ML,Larsen & Toubro (L&T),2 - 4 years,Not Disclosed,"['Chennai', 'Bengaluru']","Experience Required\n\n2 to 4 years of experience in AI/ML model development, deployment, and optimization. Hands-on experience in building machine learning pipelines and working with large datasets\n\nDomain Experience (Functional)\nExperience in domains such as natural language processing (NLP), computer vision, predictive analytics, or recommendation systems. Exposure to industry-specific AI applications (e.g., healthcare, finance, retail, manufacturing) is a plus.\n\nQualification\nBachelors or Masters degree in Computer Science, Artificial Intelligence, Data Science, Mathematics, or a related field\n\nRoles & Responsibilities\nDesign, develop, and deploy machine learning and deep learning models.\nCollaborate with data engineers and domain experts to collect, clean, and preprocess data.\nConduct experiments, evaluate model performance, and iterate for improvement.\nIntegrate AI models into production systems and monitor their performance.\nStay updated with the latest research and advancements in AI/ML.\nDocument model development processes and contribute to knowledge sharing.\n\nTechnical Skills\n\nProficient in Python and core ML libraries: TensorFlow, PyTorch, Scikit-learn.\nStrong with Pandas, NumPy for data handling.\nSolid grasp of ML algorithms, statistics, and model evaluation.\nFamiliar with cloud platforms (AWS/Azure/GCP).\nExperience with Git and basic CI/CD for model deployment",Industry Type: Engineering & Construction,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Npl', 'Aiml', 'Tensorflow', 'Ci/Cd', 'Machine Learning', 'Deep Learning', 'Scikit-Learn', 'Numpy', 'Pytorch', 'GCP', 'Pandas', 'Microsoft Azure', 'AWS', 'Python']",2025-06-12 13:55:30
Senior Analytics Consultant,Wells Fargo,4 - 9 years,Not Disclosed,['Hyderabad'],"About this role:\nWells Fargo is seeking a Senior Analytics Consultant with a proven track record of success preferably in the banking industry.\n\nIn this role, you will:\nConsult, review and research moderately complex business, operational, and technical challenges that require an in-depth evaluation of variable data factors",,,,"['data manipulation', 'Data Engineering', 'data analysis', 'data management', 'SQL']",2025-06-12 13:55:32
Software Engineer,Growexx,2 - 7 years,Not Disclosed,['Ahmedabad'],"join our growing engineering team. You will play a key role in designing, developing, and maintaining scalable software solutions that power our analytics platform. This is an exciting opportunity to work on impactful projects in a collaborative, fast-paced environment. Key Responsibilities\nDesign, develop, test, and deploy high-quality software solutions\nCollaborate with product managers, designers, and data scientists to deliver new features and enhancements\nWrite clean, maintainable, and efficient code following best practices\nParticipate in code reviews and contribute to the continuous improvement of engineering processes\nTroubleshoot and resolve technical issues across the stack\nStay current with emerging technologies and propose innovative solutions\nKey Skills\nProficiency in one or more programming languages (e.g., Python, JavaScript, TypeScript, Go, Java)\nExperience with modern web frameworks (e.g., React, Angular, Vue)\nFamiliarity with RESTful APIs, microservices, and cloud platforms (e.g., AWS, Azure, GCP)\nStrong problem-solving skills and attention to detail\nExperience with data visualization libraries (e.g., D3.js, Plotly)\nKnowledge of data pipelines, ETL processes, or big data technologie\nFamiliarity with containerization (Docker, Kubernetes)\nExposure to machine learning or AI-driven applications\nEducation and Experience\nBachelor s degree in Computer Science, Engineering, or related field\n2+ years of professional software development experience Analytical and Personal skills Must have good logical reasoning and analytical skills\nAbility to break big goals to small incremental actions\nExcellent Communication skills in English both written and verbal\nDemonstrate Ownership and Accountability of their work\nGreat attention to details\nDemonstrate ownership of tasks Positive and Cheerful outlook in life Work with the problem solver engineers team (Doc / PDF Only, Max file size 2 MB) By using this form you agree with the storage and handling of your data by this website. *\nYou cannot copy content of this page\nReconciliation Automation Data Sheet\nThis field is for validation purposes and should be left unchanged.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'PDF', 'Analytical', 'Reconciliation', 'Machine learning', 'Javascript', 'Continuous improvement', 'Analytics', 'Python']",2025-06-12 13:55:35
Asset & Wealth Management - AM FI Macro Strats - Associate,Goldman Sachs,2 - 7 years,Not Disclosed,['Bengaluru'],"Who We Are\nAt Goldman Sachs, we connect people, capital and ideas to help solve problems for our clients. We are a leading global financial services firm providing investment banking, securities and investment management services to a substantial and diversified client base that includes corporations, financial institutions, governments and individuals.\nAt Goldman Sachs, our Engineers don t just make things - we make things possible. We change the world by connecting people and capital with ideas and solve the most challenging and pressing engineering problems for our clients. Our engineering teams build scalable software and systems, architect low latency infrastructure solutions, proactively guard against cyber threats, and leverage machine learning alongside financial engineering to continuously turn data into action.\nEngineering, which is comprised of our Technology Division and global strategist groups, is at the critical center of our business. Our dynamic environment requires innovative strategic thinking. Want to push the limit of digital possibilities? Start here.\nGoldman Sachs Asset & Wealth Management\nAs one of the worlds leading asset managers, our mission is to help our clients achieve their investment goals. To best serve our clients diverse and evolving needs, we have built our business to be global, broad and deep across asset classes, geographies and solutions.\nGoldman Sachs Asset & Wealth Management is one of the worlds leading asset management institutions. AWM delivers innovative investment solutions managing close to Two Trillion US Dollars on a global, multi-product platform. In addition to traditional products (e.g. Equities, Fixed Income) our product offering also includes Hedge Funds, Private Equity, Fund of Funds, Quantitative Strategies, Fundamental Equity and a Multi-Asset Pension Solutions Business. Software is engineered in a fast-paced, dynamic environment, adapting to market and customer needs to deliver robust solutions in an ever-changing business environment. AM Data Engineering builds on top of cutting edge in-house and cloud platforms complimented with a strong focus on leveraging open source solutions.\nBusiness Overview\nThe External Investing Group ( XIG ) provides investors with investment and advisory solutions across leading private equity funds, hedge fund managers, real estate managers, public equity strategies, and fixed income strategies. XIG manages globally diversified programs, targeted sector-specific strategies, customized portfolios, and a range of advisory services. Our investors access opportunities through new fund commitments, fund-of-fund investments, strategic partnerships, secondary-market investments, co-investments, and seed-capital investments. With over 350 professionals across 11 offices around the world, XIG provides manager diligence, portfolio construction, risk management, and liquidity solutions to investors, drawing on Goldman Sachs market insights and risk management expertise. We extend these global capabilities to the world s leading sovereign wealth funds, pension plans, governments, financial institutions, endowments, foundations, and family offices, for which we invest or advise on over $300 billion of alternative investments, public equity strategies, and fixed income strategies.\nWhat We Do\nWithin Asset Management, Strategists (also known as Strats ) play important roles in research, valuation, portfolio construction, and risk management analytics. A Strategist will apply quantitative and analytical methods to come up with solutions that are accurate, robust, and scalable. Strats are innovators and problem-solvers, building novel and creative solutions for manager selection, portfolio construction, and risk management. You will develop advanced computational models, architectures, and applications to meet the challenges of a rapidly growing and evolving business.\nStrats collaborate across the business to develop solutions. These daily interactions with other team members across geographies demand an ability to communicate clearly about complex financial, business, and mathematical concepts. We look for creative collaborators who evolve, adapt to change, and thrive in a fast-paced global environment.\nBasic Qualifications\nOutstanding background in a quantitative discipline, with excellent analytical, quantitative, and problem-solving skills, and demonstrated abilities in research and data visualization\nProgramming expertise in a scripting language (e.g. Python, R, Matlab)\nStrong general and technical communication skills, with an ability to effectively articulate complex financial and mathematical concepts\nCreativity and problem-solving skills\nAbility to work independently and in a team environment\n2+ years of applicable experience\nGoldman Sachs Engineering Culture",Industry Type: Banking,"Department: BFSI, Investments & Trading","Employment Type: Full Time, Permanent","['Wealth management', 'Analytical', 'Fixed income', 'Investment banking', 'Asset management', 'Investment management', 'Risk management', 'Private equity', 'Analytics', 'Financial services']",2025-06-12 13:55:37
Sr. Technology Auditor,AMERICAN EXPRESS,2 - 4 years,13-18 Lacs P.A.,"['Gurugram', 'Delhi / NCR']","Role & responsibilities\n•       Translate business risks, controls and supporting data into analytic requirements and partners with colleagues to build effective analytics and insights\n•       Responsible for multiple simultaneous audit projects of all sizes and complexity across multiple business areas within and outside of local region, in unfamiliar areas, and for different audit leaders\n•       Link analytics and insights to ongoing strategic initiatives\n•       Apply proven/ advanced data algorithms, advanced analytic and modeling techniques to draw insights essential to driving improvement initiatives",,,,"['Natural Language Processing', 'Tableau', 'Machine Learning', 'SQL', 'Python']",2025-06-12 13:55:40
Full Stack AI Engineer (Lead),Inclusive Business Solutions,3 - 8 years,20-35 Lacs P.A.,[],"AI specialists for Full Stack, Computer Vision, and Speech Processing roles. Responsibilities include real-time data integration, emotion recognition, and speech analysis. Must have expertise in AI frameworks, ML models, and optimization techniques.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Speech Recognition', 'Natural Language Processing', 'Computer Vision', 'Machine Learning', 'Python', 'Tensorflow', 'Large Language Model', 'Artificial Intelligence', 'AWS', 'Deep Learning']",2025-06-12 13:55:42
Customer Support Engineer/ OPS Engineer,Pratiti Technologies,1 - 4 years,Not Disclosed,['Pune'],"[{""Posting_Title"":""Customer Support Engineer/ OPS Engineer"" , ""Is_Locked"":false , ""City"":""Pune City"",""Industry"":""Software Product"",""Job_Description"":""\nCustomer Support Engineer/ OPS Engineer\n\nRole description for Customer Support Engineer/ OPS Engineer\nWe are looking for a dynamic executive to join our customer support operations team. The\nexecutive will be responsible for technical customer support and manage relationships with\ncustomers. This position is ideal for young solar photovoltaic enthusiast in renewable energy\nsector, who is looking for demonstrating technical understanding in challenges in solar PV\npower plant O&M and asset management and providing best customer support service.\n\nThe executive would be closely working directly with customer support manager and\noperations head on daily basis. The executive will be hands on working on Apollo\\u2122, our\npatented performance intelligence and health analytics solution for solar PV power plants.\nResponsibilities\n1. Configuration of new plants on to Apollo\n2. Checking all engineering files of the power plant received from the customer\n3. Setting up plant topology and configuration in Apollo\n4. Setting up data pipeline connection between Apollo and customer site\n5. Activating all product features in apollo relayed to central monitoring, analytics and\nCMMS\n6. Validating plant onboarding checks and preparing handover files to the customer\n7. Creating UAT report for the handover plants\nMust-Haves\n\\u2219 Customer Support and Technical Support skills\n\\u2219 Troubleshooting and Field Service abilities\n\\u2219 Data Integration Protocol\n\\u2219 Experience in renewable energy or related industry is a plus\n\\u2219 Knowledge of solar and wind power plant operations\n\\u2219 Bachelor\\u2019s degree in Engineering or renewable field\n\\u2219 Pune based candidates only\n\nQualification\nBachelor in Computer Science/Engineering\n\na\n\n\n\n\n\nCustomer Support Engineer/ OPS Engineer\n\n\n\n\n"",""Job_Type"":""Full time"" , ""Job_Opening_Name"":""Customer Support Engineer/ OPS Engineer"" , ""State"":""Maharashtra"" , ""Country"":""India"" , ""Zip_Code"":""411001"" , ""id"":""481259000051634803"" , ""Publish"":true , ""Date_Opened"":""2025-04-17"" , ""Keep_on_Career_Site"":false}]",Industry Type: IT Services & Consulting,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['Computer science', 'Plant operations', 'Renewable energy', 'Senior Executive', 'Customer support', 'Asset management', 'Troubleshooting', 'Technical support', 'Monitoring', 'Analytics']",2025-06-12 13:55:45
"Business Research Analyst - II, RBS ACCX Program",Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Amazon.com strives to be Earths most customer-centric company where people can find and discover virtually anything they want to buy online. By giving customers more of what they want low prices, vast selection, and convenience Amazon.com continues to grow and evolve as a world-class e-commerce platform. Amazons evolution from Web site to e-commerce partner to development platform is driven by the spirit of innovation that is part of the companys DNA. The worlds brightest technology minds come to Amazon.com to research and develop technology that improves the lives of shoppers and sellers around the world.\n\nOverview of the role\nThe Business Research Analyst will be responsible for Data and Machine learning part of continuous improvement projects across the Discoverability space. This will require collaboration with local and global teams. The Research Analyst should be a self-starter who is passionate about discovering and solving complicated problems, learning complex systems, working with numbers, and organizing and communicating data and reports. The Research Analyst will perform Big data analysis to identify patterns, train model to generate product to product relationship and product to brand & model relationship. The Research Analyst is also expected to continuously improve the ML/LLM solutions in terms of precision & recall, efficiency and scalability. The Research Analyst should be able to write clear and detailed functional specifications based on business requirements.\n\n\nScoping, driving and delivering complex projects across multiple teams.\nPerforms root cause analysis by understanding the data need, get data / pull the data and analyze it to form the hypothesis and validate it using data.\nBuild programs to create a culture of continuous improvement within the business unit, and foster a customer-centric focus on the quality, productivity, and scalability of our services.\nFind the scalable solution for business problem by executing pilots and build Deterministic and ML/LLM models.\nManages meetings, business and technical discussions regarding their part of the projects.\nMakes recommendations and decisions that impact development schedules and the success for a product or project.\nDrives team(s)/partners to meet program and/or product goals.\nCoordinates design effort between internal team and External team to develop optimal solutions.\nPerforms supporting research, conduct analysis of the bigger part of the projects and effectively interpret reports to identify opportunities, optimize processes, and implement changes.\nAbility to convince and interact with stakeholders at all level either to gather data and information or to execute and implement according to the plan.\nAbility to deal with ambiguity and problem solver\nCommunicate ideas effectively and with influence (both verbally and in writing), within and outside the team.\n\nKey Performance Areas:\nSolve large and complex business problems by aligning multiple teams together.\nData analytics and Data Sciences\nMachine learning\nProject/Program Management\nAutomation initiative conceptualization and implementation\nBig Data analytics\nProduct development Scoping and Testing\nDefect Elimination\nAgile Continuous Improvement\n\nAbout the team\nThe RBS group in Chennai/Bangalore is an integral part of Amazon online product lifecycle and buying operations. The team is designed to ensure Amazon remains competitive in the online retail space with the best price, wide selection and good product information. The team s primary role is to create and enhance retail selection on the worldwide Amazon online catalog. The tasks handled by this group have a direct impact on customer buying decisions and online user experience. 3+ years of analyzing and interpreting data with Redshift, Oracle, NoSQL etc. experience\nExperience with data visualization using Tableau, Quicksight, or similar tools\nExperience with data modeling, warehousing and building ETL pipelines\nExperience writing complex SQL queries\nExperience in Statistical Analysis packages such as R, SAS and Matlab\nExperience using SQL to pull data from a database or data warehouse and scripting experience (Python) to process data for modeling Experience with AWS solutions such as EC2, DynamoDB, S3, and Redshift\nExperience in data mining, ETL, etc. and using databases in a business environment with large-scale, complex datasets",,,,"['Automation', 'Data analysis', 'SAS', 'Data modeling', 'Machine learning', 'Agile', 'Oracle', 'Data mining', 'MATLAB', 'Python']",2025-06-12 13:55:48
"Sr Business Analyst (Process Modeling, and Stakeholder Collaboration)",Synechron,7 - 12 years,Not Disclosed,"['Pune', 'Hinjewadi']","Job Summary\nSynechron is seeking a highly experienced and detail-oriented Senior Business Analyst to join our dynamic team. In this role, you will serve as a key contributor to our business analysis function, translating complex business needs into effective solutions that support organizational goals. Your expertise will enable our teams to deliver value-driven projects efficiently and effectively, ensuring alignment with strategic objectives and stakeholder expectations.\nSoftware Requirements\nRequired Skills:\nBusiness analysis tools (e.g., Microsoft Visio, (version 2016 or later))\nData analysis and visualization software (e.g., Microsoft Excel - advanced proficiency, tools like Tableau or Power BI)\nRequirement management tools (e.g., Jira, Confluence - recent versions)\nWorkflow and process modeling software (e.g., BPMN tools)\nPreferred Skills:\nBasic understanding of enterprise-level ERP/CRM systems (e.g., SAP, Salesforce)\nKnowledge of project management tools (e.g., Microsoft Project, MS Teams)\nOverall Responsibilities\nGather, analyze, and document business requirements by engaging with stakeholders, ensuring clarity, completeness, and alignment with organizational objectives.\nDevelop detailed functional specifications, use cases, process flows, and user stories to guide development teams and project execution.\nFacilitate communication between business units and technical teams to ensure a mutual understanding of project scope and deliverables.\nSupport project planning, monitoring progress, and ensuring deliverables meet quality standards and deadlines.\nContribute to process improvement initiatives by analyzing current workflows and recommending efficiencies.\nAssist in testing and validating solutions to verify they meet business needs and specifications.\nProvide ongoing support during implementation, including stakeholder training and documentation.\nStrategic Objectives:\nDeliver comprehensive requirements that enable timely and successful project deliveries.\nEnhance stakeholder engagement and satisfaction through clear communication and tailored solutions.\nPromote continuous improvement by identifying opportunities to optimize business processes.\nPerformance Outcomes & Expectations:\nAccurate and comprehensive requirement documentation.\nSuccessful facilitation of collaborative sessions and stakeholder buy-in.\nOn-time delivery of specifications and supporting documentation.\nPositive feedback from stakeholders regarding clarity and usability of deliverables.\nTechnical Skills (By Category)\nProgramming Languages:\nRequired: Basic understanding of scripting or programming concepts (e.g., SQL, Python) is preferred but not mandatory.\nPreferred: None specifically required.\nDatabases/Data Management:\nRequired: Experience with relational databases (e.g., SQL Server, Oracle) and data querying techniques.\nPreferred: Experience with big data tools or NoSQL databases.\nCloud Technologies:\nRequired: Familiarity with cloud platforms (e.g., AWS, Azure) focusing on cloud-based data storage and services.\nPreferred: Certification or practical experience in cloud services.\nFrameworks and Libraries:\nRequired: Understanding of business process frameworks (e.g., BPMN, UML modeling).\nPreferred: Knowledge of agile frameworks like Scrum or Kanban.\nDevelopment Tools & Methodologies:\nRequired: Experience with Agile, Scrum, or Waterfall project methodologies.\nPreferred: Exposure to DevOps practices.\nSecurity Protocols:\nOptional: Basic understanding of data security, compliance, and privacy protocols relevant to business analysis.\nExperience Requirements\nMinimum of 7+ years in business analysis roles within financial services or related industries.\nProven track record of managing complex projects from requirements gathering through implementation.\nExtensive experience in stakeholder engagement, documentation, and process modeling.\nExperience working in diverse regulatory environments and compliance standards is advantageous.\nCandidates with alternative pathways demonstrating equivalent skillssuch as extensive cross-functional project leadershipare encouraged to apply.\nDay-to-Day Activities\nConduct interviews and workshops with stakeholders to elicit detailed business requirements.\nAnalyze existing business processes and document workflows to identify improvement opportunities.\nPrepare functional specifications, use cases, user stories, and process diagrams for project teams.\nCollaborate closely with developers, testers, and project managers in an Agile or traditional setting.\nParticipate in sprint planning, review sessions, and status meetings.\nSupport user acceptance testing (UAT) and assist with issue resolution.\nMaintain clear and organized documentation of requirements, decisions, and project artifacts.\nProvide ongoing communication and updates to stakeholders on project progress.\nDecision-Making Authority & Responsibilities:\nValidate solution approaches against requirements.\nRecommend process improvements and inform implementation strategies.\nEscalate issues related to scope or requirements misalignment to project leadership.\nQualifications\nBachelors degree in Business Administration, Information Systems, Computer Science, or related field.\nRelevant certifications (preferred but not mandatory): CBAP, CCBA, PMI-PBA, or equivalents.\nParticipation in ongoing professional development, such as courses in business analysis, project management, or domain-specific training.\nDemonstrated commitment to continuous learning and adapting industry best practices.\nProfessional Competencies\nStrong analytical and critical thinking skills, with an ability to interpret complex data and business scenarios.\nEffective collaboration and stakeholder management skills across varying levels of the organization.\nExcellent written and verbal communication abilities, ensuring clarity and mutual understanding.\nResilience and adaptability in fast-paced environments, with a proactive approach to problem-solving.\nInnovative mindset, open to leveraging new tools and methods to enhance processes.\nSkilled in prioritizing tasks, managing time efficiently, and meeting deadlines.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Python', 'Azure', 'Kanban', 'NoSQL', 'Scrum', 'SQL Server', 'Oracle', 'AWS']",2025-06-12 13:55:50
Azure Devops Engineer,Celebal Technologies,4 - 9 years,10-20 Lacs P.A.,['Jaipur( Malviya Nagar )'],"Job Role Azure DevOps Engineer\n\nJob Location – Jaipur\nExperience Required-4+ Years\n\nAbout Us -Celebal Technologies is a premier software services company in the field of Data Science, Big Data and Enterprise Cloud. Celebal Technologies helps you to discover the competitive advantage by employing intelligent data solutions using cutting-edge technology solutions that can bring massive value to your organization. The core offerings are around ""Data to Intelligence"", wherein we leverage data to extract intelligence and patterns thereby facilitating smarter and quicker decision making for clients. With Celebal Technologies, who understands the core value of modern analytics over the enterprise, we help the business in improving business intelligence and more data-driven in architecting solutions.",,,,"['Azure Kubernetes', 'Azure Cloud', 'Azure Devops', 'Azure Pipelines', 'Ci Cd Pipeline', 'Container Orchestration', 'Aks', 'Arm Templates', 'Azure Monitoring', 'Terraform', 'Containerization', 'Docker', 'Kubernetes']",2025-06-12 13:55:52
Sr. Developer,Cognizant,7 - 10 years,Not Disclosed,['Chennai'],"Data Engineer Skills and Qualifications\nSQL - Mandatory\nStrong knowledge of AWS services (e.g., S3, Glue, Redshift, Lambda). - Mandatory\nExperience working with DBT – Nice to have\nProficiency in PySpark or Python for big data processing. - Mandatory\nExperience with orchestration tools like Apache Airflow and AWS CodePipeline. - Mandatory\nFamiliarity with CI/CD tools and DevOps practices.",,,,"['continuous integration', 'kubernetes', 'orchestration', 'aws iam', 'modeling', 'amazon redshift', 'data warehousing', 'pyspark', 'ci/cd', 'aws codedeploy', 'tools', 'sql', 'docker', 'apache', 'java', 'data modeling', 'devops', 'linux', 'etl', 'big data', 'cd', 'python', 'airflow', 'data processing', 'javascript', 'lambda expressions', 'aws', 'etl process']",2025-06-12 13:55:55
Sr. Computer Scientist-I,Adobe,8 - 13 years,Not Disclosed,['Noida'],"Our engineering team develops the Adobe Experience Platform, offering innovative data management and analytics.\nDeveloping a reliable, resilient system at large scale is crucial. We use Big Data and open-source tech for Adobes services.\nOur support for large enterprise products spans across geographies, requiring us to manage disparate data sources and ingestion mechanisms. The data must be easily accessible at very low latency to support various scenarios and use cases. We seek candidates with deep expertise in building low latency services at high scales who can lead us in accomplishing our vision.\n  What you will need to succeed\n8+ years in design and development of data-driven large distributed systems\n3+ years as an architect building large-scale data-intensive distributed systems and services\nRelevant experience building application layers on top of Apache Spark\nStrong experience with Hive SQL and Presto DB\nExperience leading architecture designs to approval while collaborating with multiple collaborators, dependencies, and internal/external customer requirements\nIn-depth work experience with open-source technologies like Apache Kafka, Apache Spark, Kubernetes, etc\nExperience with big data technologies on public clouds such as Azure, AWS, or Google Cloud Platform\nExperience with in-memory distributed caches like Redis, Memcached, etc\nStrong coding (design patterns) and design proficiencies setting examples for others; contributions to open source are highly desirable\nProficiency in data structures and algorithms\nCost consciousness around computation and memory requirements\nStrong verbal and written communication skills\nBTech/MTech/MS in Computer Science\nWhat you'll do\nLead the technical design and implementation strategy for major systems and components of the Adobe Experience Platform\nEvaluate and drive the architecture and technology choices for major systems/components\nDesign, build, and deploy products with outstanding quality\nInnovate the current system to improve robustness, ease, and convenience\nArticulate design and code choices to cross-functional teams\nMentor and guide a high-performing team\nReview and provide feedback on features, technology, architecture, design, time & budget estimates, and test strategies\nEngage in creative problem-solving\nDevelop and evolve engineering standard methodologies to improve the team s efficiency\nPartner with other teams across Adobe to achieve common goals",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data management', 'Coding', 'Data structures', 'Open source', 'Adobe', 'big data', 'Distribution system', 'Analytics', 'SQL']",2025-06-12 13:55:57
Senior Mis Executive,Thyrocare,2 - 6 years,2.5-5 Lacs P.A.,['Mumbai (All Areas)'],"Roles and Responsibilities\nAnalyse, extract, and review relevant data from various client and proprietary systems to create spreadsheet reports for use of management, efficiency, quality, and productivity analysis, employee stack-ranking, cost analysis, billing/invoicing\nAnalyse data and publish daily, weekly and monthly reports as per the pre-defined timelines\nBuild and manage the tools to collect raw data that is not available from current software and systems.\nCreate multi-level reports from the same data to serve multiple stakeholders with minimum manual re-work\nSuch other activities as may be assigned by your manager\nParticipate in cross-functional meetings to resolve recurring customer issues.\nAnalyze current business processes and make recommendations for improvements\nMaintain thorough understanding of data and information resources\nMaintain a status on all projects and proactively communicate with management\n\nDesired Candidate Profile\nCandidates with 3+ Years of experience in creating and maintaining MIS reports for Banking, Financial Services, BPO/KPO/LPO, or Back-office Operations.\nAdvanced Excel Knowledge including but not limited to: Creating Impressive Dashboards, working with large excel data running into lakhs of rows and several hundred columns, use of excel tools and formulas: pivot, xlookup, vlookup, index, sumifs, countifs, maxifs, sumproduct, offset, string and date related formulas, multiple layer nested if loops, rank, use of array formulas such as unique, filter, sort, sortby.\nAnd Knowledge of Big Data analysis tools such as SQL, Python etc\nKnowledge of Power Suite: Power Query, Power Automate and Power BI would be an added advantage\nProficiency with macros and VBA coding would be an added advantage\nHigh attention to detail\nMust have an analytical bent of mind\nShould be able to build tools with high scalability and agility\nQualifications/ Requirements:\nUG- Any Graduate\nHigh producer with attention to quality\nStrong PC skills, with demonstrated proficiency with Microsoft Office\nWilling to work in shifts as per business requirements\nWillingness to learn and invest time and effort for career development.",Industry Type: Pharmaceutical & Life Sciences,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['Purchase', 'MIS', 'Advanced Excel', 'MIS Preparation', 'MIS Operations', 'Supply Chain Management', 'MIS Reporting', 'Management Information System', 'Inventory', 'Excel Report Preparation']",2025-06-12 13:55:59
Pyspark Developer,Synechron,5 - 10 years,Not Disclosed,['Bengaluru'],"Responsibilities:\nAbility to design and build Python-based code generation framework and runtime engine by reading Business Rules repository in order to.\n\nRequirements:\nMinimum 5 years of experience in build & deployment of Bigdata applications using SparkSQL, SparkStreaming in Python;",,,,"['Pyspark', 'Airflow', 'Hive', 'python', 'Sqoop', 'Hadoop', 'Big data', 'MongoDB', 'SQL', 'HBase']",2025-06-12 13:56:02
"Assoc Dir , Project Manager - Reltio MDM",Iqvia Biotech,10 - 15 years,Not Disclosed,"['Kochi', 'Bengaluru']","12 years of experience: Significant experience in project management, with a focus on MDM and ETL projects in life science. PMP certified preferred.\nProject Planning and Execution: Define project scope, objectives, timelines, and resources. Develop project plans, schedules, and budgets, and manage their execution. Should have a background in data integration projects and hands-on experience in managing risks and interdependencies with upstream and downstream applications.\nMDM and ETL Expertise : Should have lead MDM and ETL projects, ensuring data quality, consistency, and accuracy.\nStakeholder Management : Communicate effectively with stakeholders, manage expectations, and ensure their satisfaction.\nProject Management Methodologies : Familiarity with project management methodologies (e.g., Agile, Waterfall). Experience with JIRA or other tools.\nStrong Leadership and Communication Skills : Ability to lead and motivate teams, communicate effectively with stakeholders, and manage expectations. Communicates progress and escalates key decisions, issues, risks, and opportunities as required to achieve project objectives and deliverables.\nLocation : Kochi primary, Bangalore- Secondary\n. We create intelligent connections to accelerate the development and commercialization of innovative medical treatments to help improve patient outcomes and population health worldwide . Learn more at https://jobs.iqvia.com",Industry Type: Medical Devices & Equipment,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['PMP', 'Project management', 'Agile', 'Healthcare', 'Clinical research', 'Project planning', 'Data quality', 'Life sciences', 'JIRA', 'Downstream']",2025-06-12 13:56:04
Senior MSBI Developer (SQL & SSRS/SSIS Expert),Synechron,8 - 10 years,Not Disclosed,"['Pune', 'Hinjewadi']","job requisition idJR1027513\n\nJob Summary\nSynechron is seeking an experienced and detail-oriented Senior MSBI Developerexpertise in MSBI (Microsoft Business Intelligence) to join our data and analytics team. In this role, you will contribute to designing, developing, and maintaining robust reporting and data integration solutions that support our business objectives. Your expertise will help deliver actionable insights, improve decision-making processes, and enhance overall data management efficiency within the organization.\n\nSoftware\n\nRequired\n\nSkills:\nMSBI Suite (including SSIS, SSRS, SSAS)\nSQL Server (including SQL Server Management Studio and Query Performance Tuning)\nVersionsRecent versions of SQL Server (2016 or later preferred)\nProven experience in creating complex reports, data transformation, and integration workflows\nPreferred\n\nSkills:\nPower BI or other visualization tools\nExperience with cloud-based data solutions (e.g., Azure SQL, Synapse Analytics)\nOverall Responsibilities\nDevelop, implement, and maintain MSBI solutions such as SSIS packages, SSRS reports, and data models to meet business requirements\nCollaborate with business stakeholders and data teams to gather reporting needs and translate them into scalable solutions\nOptimize and troubleshoot existing reports and data pipelines to improve performance and reliability\nEnsure data accuracy, security, and compliance within reporting processes\nDocument solution architectures, workflows, and processes for ongoing support and knowledge sharing\nParticipate in team initiatives to enhance data governance and best practices\nContribute to strategic planning for data platform evolution and modernization\nTechnical Skills (By Category)\n\nProgramming Languages:\nRequiredSQL (Advanced proficiency in query writing, stored procedures, and performance tuning)\nPreferredT-SQL scripting for data transformations and automation\nDatabases / Data Management:\nRequiredDeep knowledge of relational database concepts with extensive experience in SQL Server databases\nPreferredFamiliarity with data warehouse concepts, OLAP cubes, and data mart design\nCloud Technologies:\nDesiredBasic understanding of cloud-based data platforms like Azure Data Factory, Azure Synapse\nFrameworks and Libraries:\nNot directly applicable, focus on MSBI tools\nDevelopment Tools and Methodologies:\nExperience working within Agile development environments\nData pipeline development and testing best practices\nSecurity Protocols:\nImplement data security measures, role-based access controls, and ensure compliance with data privacy policies\nExperience\n8 to 10 years of professional experience in software development with substantial hands-on MSBI expertise\nDemonstrated experience in designing and deploying enterprise-level BI solutions\nDomain experience in finance, healthcare, retail, or similar industries is preferred\nAlternative candidacyExtensive prior experience with BI tools and proven success in similar roles may be considered in lieu of exact industry background\nDay-to-Day Activities\nDesign and develop SSIS data integration workflows to automate data loading processes\nCreate and optimize SSRS reports and dashboards for various organizational units\nEngage in troubleshooting and resolving technical issues in existing BI solutions\nCollaborate with data architects, developers, and business analysts to align data solutions with business needs\nConduct code reviews, testing, and validation of reports and data pipelines\nParticipate in scrum meetings, planning sessions, and stakeholder discussions\nEnsure documentation of solutions, processes, and workflows for ease of maintenance and scalability\nQualifications\nBachelors degree or equivalent in Computer Science, Information Technology, or related field\nRelevant certifications in Microsoft BI or SQL Server (e.g., Microsoft Certified Data Engineer Associate) preferred\nOngoing engagement in professional development related to BI, data management, and analytics tools\nProfessional Competencies\nAnalytical mindset with strong problem-solving abilities in data solution development\nCapable of working collaboratively across diverse teams and communicating technical concepts effectively\nStakeholder management skills to interpret and prioritize reporting needs\nAdaptability to evolving technologies and continuous learning mindset\nFocus on delivering high-quality, sustainable data solutions with attention to detail\nEffective time management, prioritizing tasks to meet project deadlines",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['performance tuning', 'stored procedures', 'sql scripting', 'query writing', 'msbi', 'sql server database', 'software development', 'ssas', 'power bi', 'microsoft azure', 'data mart design', 'olap cubes', 'sql server', 'sql azure', 'ssrs', 'data warehousing concepts', 'data transformation', 'ssis']",2025-06-12 13:56:06
Dot Net Architect,Conduent,10 - 15 years,Not Disclosed,['Bengaluru'],"Responsibilities \nDesign and develop highly scalable web based applications based on business needs\nDesign and customize software for client use with the aim of optimizing operational efficiency\nA deep understanding of, and ability to use and explain all aspects of application integration in .NET and data integration with SQL Server and associated technologies and standards\nDesign and provide architecture solutions based on the business needs\nStrong background in building and operating SAAS platforms using the Microsoft technology stack with modern services based architectures\nAbility to recommend and configure Azure subscriptions and establish connectivity\nWork with IT teams to setup new application architecture requirements\nCoordinate releases with Quality Assurance Team and implement SDLC work flows and better source code integration\nImplement build process and continuous build integration with Unit Testing framework\nDevelop and maintain a thorough understanding of business needs from both technical and business perspectives\nAssist and mentor junior team members to enforce development guidelines\nTake technical ownership of products and provide support with quick turnaround\nEffectively prioritize and execute tasks in a high-pressure environment\n\n\n Qualifications / Experience \nBachelor\\u2019s/Master\\u2019s degree in Computer Science / Computer Engineering\nMinimum of 10+ years\\u2019 experience in building enterprise scale windows and web application using Microsoft .NET technologies\n5+ years of experience in C#, ASP.NET MVC and Microsoft Web API\n1+ years of experience in Angular 2 or higher\nExperience in solution architecture in .Net technologies\nExperience in any of the following are also desirableBootstrap, Knockout, entity framework nhibernate, Subversion, Linq, Asynchronous Module Definition (such as requirejs)\nIn depth knowledge on design patterns and unit testing frameworks\nExperience with Agile application development\nSQL server performance tuning (SQL Server 2014/2016) and troubleshooting\nAbility to work with a sense of urgency and attention to detail\nExcellent oral and written communication skills",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql server', 'application integration', 'design patterns', '.net', 'data integration', 'c#', 'unit testing framework', 'web application', 'unit testing', 'scale', 'knockoutjs', 'net mvc', 'angular', 'asp.net core mvc', 'linq', 'saas applications', 'web api', 'agile', 'sdlc', 'asp', 'microsoft net']",2025-06-12 13:56:08
Deputy General Manager- Electrical,Tata Consultant Engineering Limited,16 - 25 years,Not Disclosed,['Bengaluru'],"About Us\nTata Consulting Engineers Limited (TCE) is the largest Indian private sector engineering and project consultancy and an emerging global leader in integrated engineering solutions. With more than 60 years of engineering excellence, TCE has a presence in over 64 countries and over 12000 completed projects, the company operates in 3 core Industry segments -Infrastructure (Water, Environment, Urban Development, Buildings, Manufacturing Facilities, Ports and Harbours, Transportation), Power (Thermal, Hydro, Nuclear, Renewable, Transmission and Distribution) and Resources - Hydrocarbons and Chemicals (Oil, Gas and Refineries, Chemicals, Petrochemicals, Fertilizers, Speciality Chemicals, Pulp and Paper, Cement, Food, Pharmaceuticals and Beverages, Tyre, Glass) as well as Mining and Metallurgy (Mining, Geology, Beneficiation, Steel, Non-ferrous). TCE serves domestic as well as international markets and is known for several first-of-its-kind projects offering Engineering Studies, Design Engineering Services, Project Management Consultancy Services, OPEX and IIOT across all three verticals. A part of Tata Group - India’s most respected group, TCE is a 100 percent subsidiary of Tata Sons Limited Design your Future with us At TCE, you will experience a supportive environment that empowers you to excel, whether you are based in our offices or at a client site. We embrace diversity, equity, and inclusion, fostering a workplace where every individual can thrive by contributing their unique skills and perspectives to deliver exceptional results for our clients. Our comprehensive compensation and benefits packages are designed to meet the diverse needs of our employees and their families, complemented by a robust global well-being program. As a leading global infrastructure firm, we are committed to your growth and success, offering access to cutting-edge technology and impactful projects that offer flexibility and significant professional opportunities. Join us and become part of a global company that values your potential and supports your career development.\nPurpose & Scope of Position\nEngineering Manager (EM) is responsible for engineering and design matters on the project and assists the project manager. An EM will fill 3 main roles during the execution of a project. These are Manager, Technical Leader and Communicator. The EM on a project is responsible for scope definition and manages the planning, resourcing and scheduling all the engineering related activities. In addition, the EM provides technical leadership, integrates the disciplines and resolves issues. The EM is responsible for delivery of all the technical deliverables to achieve the project objectives within the specified time frame and budget.\nExperience\n• Typically 15 years & above of experience in a multi-disciplinary environment on major projects \nDesign and procurement activities related to electrical systems / equipment.\nSizing calculations for all major electrical equipment or systems.\nPreparation of RFQs for major electrical packages\nVendor print review for all electrical equipment or systems along with cable schedules & interconnection wiring diagrams\nExposure on Layouts such as General Arrangement drawings, Cable & conduit drawings, Earthing & Lightning Protection drawings, Lighting drawings.\nExposure on Control and protection, review of schematics for EHV, MV & LV systems.\nExposure to Load flow, Short circuit, Largest Motor starting studies using ETAP.\nEstimation of BOQs for cabling, earthing, lightning & lighting systems.\nExposure to relay setting and co-ordination\nInspection services for major electrical equipment (Optional)\nFamiliarity in ETAP, DIALux, AutoGrid Pro, etc.\nFamiliarity in 3D Engineering tools Aveva E3D, Hexagon Smart 3D, Navisworks\nQualification\n• Postgraduate or graduate in an engineering discipline • Registration as a professional engineer with the governing authority (preferable)\nKey Responsibilities\n1.Ensure the scope of work is developed and effective change management system is in place 2. Provide input into the monthly progress report with respect to design progress and issues of concern and recommended changes if required to achieve overall objectives 3. Contribute to the development of the project execution plan together with the project leadership team and allocate roles and responsibilities 4. Make Contact with client management at key milestones / tollgates throughout project to ensure that engineering deliverables are meeting client requirements. 5. Direct and review the engineering activities to ensure that the work quality is satisfactory and appropriate technical personnel development programs are conducted 6. Involve specialist expertise from within TCE or externally as necessary. work with Technology Organization and DHs for mandatory reviews 7. Raise safety awareness and ensure the design for safety principles are applied to the projects 8. Ensure Engineering risk assessments are carried out and all identified issues are addressed 9. Arrange and facilitate design reviews and participate as required in engineering and management reviews 10. Co-ordinate quality audit verifications to ensure compliance with all relevant engineering standards and internal procedures for all design activities. Instigate corrective actions as required. 11. Cooperate with the TCE engineering practice of other BU’s as directed on matters involving sharing of available expertise. 12. Responsible to ensure timely availability of inter-disciplinary data, integration of the same and resolution of any issues and challenges 13. Obtain regular feedback from customer and take appropriate action 14. Document value additions and best practices and ensure communication of the same to other project teams and leadership 15. Recommend rewards and recognition for exemplary performance from project resources\nCompetencies\nManages Conflict\nSelf-Development\nDrives Results\nEnsures Accountability\nOptimizes Work processes\nPlans and Aligns\nDecision Quality\nSituational Adaptability\nTech Savvy\nInterpersonal Savvy",Industry Type: IT Services & Consulting,"Department: Production, Manufacturing & Engineering","Employment Type: Full Time, Permanent","['dialux', 'lightning protection', 'design engineering', 'change management', 'smart', 'wiring', 'cable schedule', 'refinery', '3d', 'general', 'oil', 'drawing', 'gas', 'cabling', 'cables', 'equipment', 'engineering', 'lighting', 'schematic', 'estimation', 'electricals', 'etap', 'site', 'e3d', 'earthing', 'electrical equipment', 'petrochemical']",2025-06-12 13:56:11
CAD Engineering Manager (Engineer-to-Order),Pratiti Technologies,4 - 7 years,Not Disclosed,['Pune'],"[{""Posting_Title"":""CAD Engineering Manager (Engineer-to-Order)"" , ""Is_Locked"":false , ""City"":""Pune City"",""Industry"":""Software Product"",""Job_Description"":""\nJob Summary:\nWe are seeking a highly motivated and experienced CAD Engineering Manager to lead our team in developing and implementing KBEsolutions for our ETO products. The ideal candidate will possess a strongunderstanding of electrical and mechanical design principles, experience withplatforms like Rulestream and AutoCAD, and a proven track record of drivingprocess improvements through KBE. This role will be responsible for managing ateam of engineers, developing KBE strategies, and ensuring the successful implementationof KBE tools to streamline our design and manufacturing processes.\nResponsibilities:\nKBE Strategy & Development:\nDevelop and implement a comprehensive KBE strategy to automate and optimize design and engineering processes for ETO products.\nIdentify and evaluate opportunities for KBE implementation across various product lines and design workflows.\nDefine and maintain KBE standards, guidelines, and best practices.\nTeam Leadership & Management:\nLead and mentor a team of KBE engineers, providing technical guidance and support.\nManage project timelines, resources, and budgets to ensure successful KBE implementation.\nFoster a collaborative and innovative team environment.\nRulestream & AutoCAD Expertise:\nLead the development and maintenance of KBE applications using Rulestream.\nIntegrate Rulestream with AutoCAD and other design tools to automate design tasks.\nTroubleshoot and resolve technical issues related to Rulestream and AutoCAD.\nEnsure proper data flow between CAD and KBE systems.\nETO Process Optimization:\nAnalyze existing ETO design and manufacturing processes to identify areas for improvement through KBE.\nDevelop and implement KBE solutions to reduce design cycle time, improve design accuracy, and enhance product quality.\nWork closely with cross-functional teams (e.g., sales, manufacturing, quality) to ensure seamless KBE integration.\nDocumentation & Training:\nDevelop and maintain comprehensive documentation for KBE applications and processes.\nProvide training and support to engineers and other stakeholders on KBE tools and methodologies.\nMaintain well documented libraries of rules and configurations.\nContinuous Improvement:\nStay up-to-date on the latest KBE technologies and industry trends.\nDrive continuous improvement initiatives to enhance KBE capabilities and efficiency.\nQualifications:\nBachelors or Masters degree in Mechanical Engineering, Electrical Engineering, or a related field.\nMinimum 1 of [Number] years of experience in engineering, with a focus on ETO industries.\nProven experience in developing and implementing KBE solutions using Rulestream.\nStrong proficiency in AutoCAD for electrical and mechanical design.\nExcellent understanding of electrical and mechanical design principles and practices.\nExperience with database management and data integration.\nStrong project management and leadership skills.\nExcellent communication and interpersonal skills.\nExperience with other CAD/CAM/CAE software is a plus.\nUnderstanding of configuration management.\nExperience in creating and maintaining complex rule sets.\nPreferred Qualifications:\nExperience with other KBE platforms.\nKnowledge of manufacturing processes and materials.\nExperience with PLM/PDM systems.\n\n\n"",""Job_Type"":""Full time"",""Job_Opening_Name"":""CAD Engineering Manager (Engineer-to-Order)"" , ""State"":""Maharashtra"" , ""Country"":""India"" , ""Zip_Code"":""411045"" , ""id"":""481259000051041047"" , ""Publish"":true , ""Date_Opened"":""2025-04-17"" , ""Keep_on_Career_Site"":false}]",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['PLM', 'Product quality', 'Process optimization', 'AutoCAD', 'Project management', 'Configuration management', 'CAD', 'Mechanical design', 'Continuous improvement', 'KBE']",2025-06-12 13:56:14
MDM Technical Lead,Iqvia Biotech,5 - 7 years,Not Disclosed,"['Kochi', 'Bengaluru']","Overview\nAs an MDM Technical Delivery Manager, you will be responsible for leading and overseeing the end-to-end delivery of Master Data Management (MDM) solutions. You will collaborate with cross-functional teams to drive technical implementation, ensure data governance, and align with business objectives. Your expertise in MDM platforms, integration strategies, and project execution will be key to delivering high-quality solutions\nKey Responsibilities\nOversee a team of experienced professionals, fostering collaboration and high performance.\nGuide and mentor team members, supporting their job performance and career growth.\nLead the technical delivery of MDM implementations, ensuring successful project execution.\nDefine MDM architecture, strategy, and integration frameworks with enterprise systems.\nCollaborate with business stakeholders to understand data requirements and align solutions.\nOversee data governance, quality, and compliance with regulatory standards.\nManage MDM development teams, ensuring adherence to best practices and standards.\nOptimize data models, workflows, and processes for efficient MDM operations.\nDrive continuous improvements in MDM technologies, methodologies, and performance.\nCommunicate project updates, risks, and resolutions to leadership and stakeholders.\nRequired Qualifications\n\nBachelor s degree in Computer Engineering, Computer Science, or a related field.\n5-7+ years of experience in software development and data Management.\n5+ years of expertise in MDM implementation, with hands-on experience in Reltio, DataBricks, Azure, Oracle, and Snowflake.\nStrong background in integration design and development.\nStrong expertise in data integration design, ETL processes, and API development.\nAt least 2+ years in an MDM Technical Lead and Delivery role.\nProven track record in leading MDM projects and cross-functional teams.\nSolid understanding of diverse data sets, sources, and country-specific data models.\nExperience in life sciences MDM implementations.\nExperience in life sciences, healthcare, or pharmaceutical industries is a plus.\nExcellent communication, leadership, and problem-solving skills.\n. We create intelligent connections to accelerate the development and commercialization of innovative medical treatments to help improve patient outcomes and population health worldwide . Learn more at https://jobs.iqvia.com",Industry Type: Medical Devices & Equipment,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data management', 'Pharma', 'data governance', 'Healthcare', 'Clinical research', 'Technical Lead', 'Life sciences', 'Oracle', 'Project execution']",2025-06-12 13:56:16
Python Fullstack Developer,CGI,2 - 5 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\nExcellent Knowledge & Understanding of Python, Pandas and Oracle, SQL.\nGood Knowledge & Understanding of data modelling.\nFlair for learning new tools & technology.\n2-5 years of experience in Banking IT, with a good understanding of the Corporate and Institutional\nBanking activity. Knowledge of Capital Markets an asset.\nGood knowledge and understanding of Windows Batch or PowerShell scripting.\nGood knowledge in systems, application frameworks, database optimization, and experience being\nresponsible for the success of software development projects.\nProven record interpreting and fulfilling requirements by developing high performing, scalable and\nmaintainable solutions with multiple technologies.\nHands-on experience with SDLC methodologies and best practices including Waterfall Process, Agile\nmethodologies, deployment automation, code reviews, and test-driven development.\nStrong coordination and organizational skills\nExcellent communication skills and multi-tasking capabilities.\nBeing aware of new technologies and frameworks.\nExperience and knowledge of Python language features like basic data types, functions with keywords\nargs, default args, variable length args.\nExperience in using Python database API with any relational database like Oracle/SQL Server using pure\nPython or native database driver.\nExperience in using built in collection types [for example: sequence types, dict, sets].\nExperience in using text manipulation facilities like regex, string methods from the standard library.\nExperience in using file and directory manipulation modules like paths, globs from the standard library.\nKnowledge of widely used libraries in data science like NumPy, Pandas is a must.\nVisualization is a good to have but not on the mandatory path.\nKnowledge and Experience in using collections from the collections module will be good to have though\nnot mandatory.\nFamiliarity with date manipulation using modules like datetime.\n•\nKnowledge of Generators will be good to have though not mandatory\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Django', 'Pandas', 'Oracle', 'Python', 'SQL']",2025-06-12 13:56:18
Etl Tester- Bangalore(Pan India Infosys),Infosys,3 - 8 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Job description\nHiring for ETL Testing with experience range 3 to 10 years\nMandatory Skills: ETL/DWH/Big data testing\nEducation: BE/B.Tech/MCA/M.Tech/MSc./MS\nResponsibilities\nA day in the life of an Infoscion\nAs part of the Infosys consulting team, your primary role would be to actively aid the consulting team in different phases of the project including problem definition, effort estimation, diagnosis, solution generation and design and deployment\nYou will explore the alternatives to the recommended solutions based on research that includes literature surveys, information available in public domains, vendor evaluation information, etc. and build POCs\nYou will create requirement specifications from the business needs, define the to-be-processes and detailed functional designs based on requirements.\nYou will support configuring solution requirements on the products; understand if any issues, diagnose the root-cause of such issues, seek clarifications, and then identify and shortlist solution alternatives\nYou will also contribute to unit-level and organizational initiatives with an objective of providing high quality value adding solutions to customers. If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ETL Testing', 'Big Data Testing', 'DWH Testing']",2025-06-12 13:56:20
Quantitative Analytics Manager,Wells Fargo,4 - 8 years,Not Disclosed,['Bengaluru'],"In this role, you will:\nManage a team responsible for the creation and implementation of low to moderate complex financial areas\nMitigate operational risk and compute capital requirements\nDetermine scope and prioritization of work in consultation with experienced management\nParticipate in the development of strategy, policies, procedures, and organizational controls with model users, developers, validators, and technology",,,,"['Quantitative Analytics', 'strategy Planning', 'marketing', 'Git', 'GitHub', 'talent development', 'credit risk analysis']",2025-06-12 13:56:22
Lead Analytics Consultant,Wells Fargo,5 - 10 years,Not Disclosed,['Hyderabad'],"locationsHyderabad, India\nposted onPosted Yesterday\njob requisition idR-446112\nAbout this role:\nWells Fargo is seeking a Lead Analytics Consultant People Analytics. As a consultant, you will work as analytics professional in HR People Analytics and Business Insights delivery team and will be responsible for effective delivery of projects as per the business priority. The incumbent is expected to be an expert into executive summary, people strategy, HR consulting, HR advisory, advanced analytics & data science and value addition to the projects.",,,,"['Data Analytics', 'Data science tools', 'product lifecycle', 'SAS programming', 'ETL development', 'Alteryx', 'Data Management', 'Tableau Prep', 'SQL']",2025-06-12 13:56:24
Etl Tester - Pan India,Infosys,3 - 8 years,Not Disclosed,"['Kolkata', 'Chennai', 'Bengaluru']","Job description,\n\nHiring for ETL testing with experience range 3-10 years\n\nMandatory Skills: ETL Testing\n\nLocation - Bangalore/Hyderabad/Pune/kolkata/Chennai/Bhubaneswar\n\nEducation: BE/B.Tech/MCA/M.Tech/MSc./MS\n\nResponsibilities\nA day in the life of an Infoscion\nAs part of the Infosys consulting team, your primary role would be to actively aid the consulting team in different phases of the project including problem definition, effort estimation, diagnosis, solution generation and design and deployment\nYou will explore the alternatives to the recommended solutions based on research that includes literature surveys, information available in public domains, vendor evaluation information, etc. and build POCs\nYou will create requirement specifications from the business needs, define the to-be-processes and detailed functional designs based on requirements.\nYou will support configuring solution requirements on the products; understand if any issues, diagnose the root-cause of such issues, seek clarifications, and then identify and shortlist solution alternatives\nYou will also contribute to unit-level and organizational initiatives with an objective of providing high quality value adding solutions to customers. If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ETL Testing', 'ETL', 'Etl Testers']",2025-06-12 13:56:26
Power Bi Engineer,Scalable Systems,4 - 6 years,10-15 Lacs P.A.,['Kochi'],Power Bi Engineer\nLocation : Kochi\nFulltime,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power Bi', 'Data Visualization', 'Power System', 'Dashboard Development']",2025-06-12 13:56:28
Technical Illustrator(Automotive),Cyient,4 - 9 years,Not Disclosed,"['Pune', 'Bengaluru']","Create and revise Parts Lists in the Parts Catalogs for automotive, agriculture and construction equipment.\nCreating exploded view artworks associated with parts catalogs as per given standards.\nProduce complete, clear and accurate parts catalogs content as per customer standards and guidelines.\nProcessing of information and data (engineering documents, changes, etc.) from\nEngineering, Manufacturing, Parts Marketing, Customer Service, Parts Warehouse and Suppliers.",,,,"['Iso Draw', 'Technical Illustration']",2025-06-12 13:56:30
Python/Pyspark developer,Zensar,4 - 5 years,Not Disclosed,"['Pune', 'Bengaluru']","Job Description:\nWe are seeking a highly skilled and motivated Python/PySpark Developer to join our growing team. In this role, you will be responsible for designing, developing, and maintaining high-performance data processing pipelines using Python and the PySpark framework. You will work closely with data engineers, data scientists, and other stakeholders to deliver impactful data-driven solutions.\nResponsibilities:\n- Design, develop, and implement scalable and efficient data pipelines using PySpark.\n- Write clean, well-documented, and maintainable Python code.\n- Optimize data processing performance and resource utilization.\n- Implement ETL (Extract, Transform, Load) processes to migrate and transform data across various systems.\n- Collaborate with data scientists and analysts to understand data requirements and translate them into technical solutions.\n- Troubleshoot and debug data processing issues.\n- Stay up-to-date with the latest advancements in big data technologies and best practices.\nQualifications:\n- Bachelor's degree in Computer Science, Engineering, or a related field.\n- 3+ years of experience in Python development.\n- 2+ years of experience with PySpark and Spark ecosystem.\n- Strong understanding of data structures, algorithms, and object-oriented programming.\n- Experience with SQL and relational databases.\n- Familiarity with cloud platforms such as AWS, Azure, or GCP (preferred).\n- Excellent problem-solving and analytical skills.\n- Strong communication and teamwork skills.\nBonus Points:\n- Experience with data visualization tools (e.g., Tableau, Power BI).\n- Knowledge of machine learning and data science concepts.\n- Experience with containerization technologies (e.g., Docker, Kubernetes).\n- Contributions to open-source projects.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Cloud Technologies', 'SQL', 'Python']",2025-06-12 13:56:32
Sr SQL Developer,HTC Global Services,8 - 13 years,Not Disclosed,['Hyderabad'],"We are looking for a skilled SSIS/T-SQL Developer with 8 years of good expertise. The incumbent will be responsible for developing, deploying and maintaining SSIS packages, as well as writing and optimizing T-SQL queries, stored procedures and functions. The role involves working closely with stakeholders to understand data requirements and deliver high-quality data solutions.\nRequirements:\nStrong understanding of relational database concepts and data modelling.\nExperience with SQL query optimization and performance tuning.",,,,"['T-SQL', 'Performance tuning', 'SQL queries', 'Data migration', 'query optimization', 'Data modeling', 'Debugging', 'Stored procedures', 'SSIS', 'Data warehousing']",2025-06-12 13:56:35
Learning Consultant India Learning Consultant,Zensar,5 - 10 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Consultants are responsible for advising, guiding, scoping, developing strategic learning plans that align to channel partner business teams and their outcomes. Although this is a learning consulting role, having a strong ability to get buy-ins from senior business stakeholders and being able to showcase L&D is crucial to success. They establish and embed processes for learner journeys into partner organizations. Motivated, analytical, and strategic thinkers who also have excellent stakeholder management skills and the ability to develop and execute plans will succeed in this consultative role.\n  High-level skills and experience\nStrong consultative mindset and consulting skill set with a strong point-of-view on solutions.\nStrategic, big picture thinking and connecting the dots.\nAbility to understand the why along with the what and how of a partner need (ie, we'll-rounded learning plans that drive business outcomes rather than just chasing headcount/numbers)\nStrong commercial awareness/business acumen of the client s and partner s business.\nAbility to align learning with client and partner ROI and business drivers.\nThought leadership with new ideas, strategies, framework, processes.\nExecutive presence with senior stakeholders\nData analytics (ability to dig into trends and forecast) and data story telling.\nTechnical prowess to pick up data/AI/cloud/technical knowledge.\nExcellent oral/written communication and presentation skills for business reviews with senior stakeholders.\nHighly collaborative across various business functions/senior stakeholder management.\nProactive and independent, ability to excel in a fast-paced environment while working from vision through execution.\nExpected tasks and responsibilities.\nExecute frameworks to assess capability and capacity skill gaps.\nConduct partner needs analysis (discovery and scoping at business, technical and learning levels)\nEnabling consulting conversations with business leaders in partner teams to understand their business drivers.\nPropose strategic learning plans to all internal and external stakeholders.\nPrescribe relevant learning based on scope and workload (such as certifications, solutions, products, etc)\nBuild custom partner learning plans, where required, based on the results of scoping/needs analysis and business priority.\nMonitor learner progress and detect any issues.\nCreate and present Quarterly/Monthly Business Reviews with senior L&D/Business/Technology stakeholders\nMonitor and analyze learning stats for data-driven decisions.\nExplore business problems and create different solution models.\nMake recommendations for improvement and present to partners and internal stakeholders.\nDevelop and implement new procedures and/or training to support proposed changes.\nQualifications\n5+ years relevant experience being an L&D consultant or in any business facing consultant role driving outcomes.\nbachelors degree in Engineering and Computer Science, or a related field (required)\nMasters candidates (preferred)\nCertifications in Data Engineering, AI/ML, Cloud platforms (preferred).\nChannel partner/pre-sales/data & AI background encouraged.\nAbout the role\nChannel consultants are part of the Channel and Partner Success Consulting team and are passionate about helping our client strengthen their channel partner readiness program through technical and pre-sales capability and capacity assessment. They work closely with our client s channel partners to build partner teams and skills that deliver to business objectives and outcomes. The client is a global technology company.",Industry Type: IT Services & Consulting,Department: Strategic & Top Management,"Employment Type: Full Time, Permanent","['Training', 'Analytical', 'Consulting', 'Cloud', 'Manager Technology', 'Presales', 'Data analytics', 'Stakeholder management', 'Monitoring']",2025-06-12 13:56:37
AVP - Finance Analyst,Wells Fargo,2 - 7 years,Not Disclosed,['Bengaluru'],"About this role:\nWells Fargo is seeking a Finance Analyst\n\nIn this role, you will:\nParticipate in functions related to financial research and reporting\nForecast analysis of key metrics, as well as other financial consulting related to business performance, operating and strategic reviews",,,,"['financial research', 'Data analysis', 'Project management', 'documentation', 'Gap analysis', 'financial consulting', 'SQL']",2025-06-12 13:56:39
Lead Business Analyst,Indegene,3 - 8 years,Not Disclosed,['Bengaluru'],"Reporting and Optimization in Adobe Analytics, Google Analytics\nCreate documents like Business Req. Doc, Tech Spec Doc etc\nDo the measurement planning for Digital Analytics and Implementation projects\nDesign a solution and digital strategy\nExperience in Data integration and BigQuery integration\nCreate data visualization dashboards specially on Workspace, Data Studio, MS Excel and Adobe Report builder\nDevelop the strategy of enterprise level solutions as we'll as architecting extensible and maintainable solutions utilizing the Adobe and Google analytics platforms\nUnderstand and use multitude of tag managers and writing JavaScript code to realize client driven business requirements\nExcellent understanding of digital analytics specially Clickstream Data\nAgile method understanding\n\nManagement Skills:\nExcellent written and oral communication skills\nExcellent listener\nStaying abreast of new technologies and issues in the software-as-a-service industry, including current technologies, platforms, standards and methodologies\n\nYour impact:\nAbout you: (Desired profile)\nMust have: (Requirements)\nAnalytics Platforms - Google Analytics, Adobe Analytics/Omniture SiteCatalyst\nBig Query\nNice to have: (Additional desired qualities)\nTag Managers - Adobe Launch/DTM, Tealium IQ, Google Tag Manager, Piwik Pro, Signal/Bright Tag\nOptimization Platform - Adobe Target, Google Optimize, Optimizely1+ years in a client facing role for solutioning and / or evangelizing technology approaches.\nProgramming Languages - JavaScript, jQuery\nMarkup Languages - HTML, CSS",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['jQuery', 'Google Analytics', 'Service industry', 'Javascript', 'Agile', 'Healthcare', 'HTML', 'Omniture', 'data visualization', 'Adobe']",2025-06-12 13:56:41
"Walk-in For Testing Positions at Chennai, Pune, Bangalore Location",Hexaware Technologies,5 - 10 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']",Exciting Career Opportunity at Hexaware Technologies!\n\nJoin our dynamic team and advance your career with us! We are looking for committed professionals to fill the following positions:\n\n- API Automation Testing (Rest Assured) (6 to 10 Years)\n- Python Automation (6 to 10 Years)\n- ETL Testing (Bigdata/Cloud) (6 to 12 Years),,,,"['Api Automation', 'Cloud Testing', 'Python Testing', 'Automation Testing', 'ETL Testing', 'Big Data Testing', 'Rest Assured', 'Selenium Testing', 'Selenium With Java']",2025-06-12 13:56:44
Oracle CDC Specialist Oracle CDC Specialist,Zensar,5 - 10 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","IT Specialist - Oracle CDC, Kafka Connectors & Docker\nJob Overview\nWe are seeking a skilled IT Specialist with expertise in Oracle Change Data Capture (CDC), Kafka topics, event streams, and running Kafka connectors on Docker containers. The ideal candidate will design, implement, and maintain robust data integration solutions to support our real-time data processing needs.\nKey Responsibilities\nConfigure and manage Oracle CDC to capture and process real-time data changes.\nDesign and maintain Kafka topics and event streams for efficient data flow.\nDeploy and operate Kafka connectors within Docker containers for seamless integration.\nMonitor and optimize performance of data pipelines and streaming processes.\nCollaborate with cross-functional teams to ensure data integrity and system scalability.\nTroubleshoot and resolve issues related to data streaming and containerized environments.\nRequired Skills and Qualifications\nBachelor s degree in Computer Science, IT, or related field (or equivalent experience).\n5+ years of experience with Oracle CDC for real-time data capture.\nStrong knowledge of Oracle databases, specifically CDC capabilities such as LogMiner & XStreams API\nStrong knowledge of Apache Kafka, including topic management and event streaming.\nProficiency in deploying and managing Kafka connectors in Docker containers.\nAble to deploy java monitoring through JMX for Kafka connectors.\nFamiliarity with container orchestration tools (e.g., Kubernetes) is a plus.\nExcellent problem-solving skills and ability to work in a fast-paced environment.\nPreferred Qualifications\nExperience with cloud platforms (e.g., AWS).\nKnowledge of additional streaming technologies or data integration tools.\nStrong scripting skills (e.g., Python, Bash) for automation.\nIT Specialist - Oracle CDC, Kafka Connectors & Docker\nJob Overview\nWe are seeking a skilled IT Specialist with expertise in Oracle Change Data Capture (CDC), Kafka topics, event streams, and running Kafka connectors on Docker containers. The ideal candidate will design, implement, and maintain robust data integration solutions to support our real-time data processing needs.\nKey Responsibilities\nConfigure and manage Oracle CDC to capture and process real-time data changes.\nDesign and maintain Kafka topics and event streams for efficient data flow.\nDeploy and operate Kafka connectors within Docker containers for seamless integration.\nMonitor and optimize performance of data pipelines and streaming processes.\nCollaborate with cross-functional teams to ensure data integrity and system scalability.\nTroubleshoot and resolve issues related to data streaming and containerized environments.\nRequired Skills and Qualifications\nBachelor s degree in Computer Science, IT, or related field (or equivalent experience).\n5+ years of experience with Oracle CDC for real-time data capture.\nStrong knowledge of Oracle databases, specifically CDC capabilities such as LogMiner & XStreams API\nStrong knowledge of Apache Kafka, including topic management and event streaming.\nProficiency in deploying and managing Kafka connectors in Docker containers.\nAble to deploy java monitoring through JMX for Kafka connectors.\nFamiliarity with container orchestration tools (e.g., Kubernetes) is a plus.\nExcellent problem-solving skills and ability to work in a fast-paced environment.\nPreferred Qualifications\nExperience with cloud platforms (e.g., AWS).\nKnowledge of additional streaming technologies or data integration tools.\nStrong scripting skills (e.g., Python, Bash) for automation.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'orchestration', 'JMX', 'Data processing', 'data integrity', 'Oracle', 'Apache', 'Python', 'Scripting']",2025-06-12 13:56:46
Manager - BIM,Axtria,10 - 15 years,Not Disclosed,['Bengaluru'],"Position Summary \n\nLooking for a Salesforce Data Cloud Engineer to design, implement, and manage data integrations and solutions using Salesforce Data Cloud (formerly Salesforce CDP). This role is essential for building a unified, 360-degree view of the customer by integrating and harmonizing data across platforms.\n\n Job Responsibilities \n\nConsolidate the Customer data to create a Unified Customer profile\nDesign and implement data ingestion pipelines into Salesforce Data Cloud from internal and third-party systems .\nWork with stakeholders to define Customer 360 data model requirements, identity resolution rules, and calculated insights.\nConfigure and manage the Data Cloud environment, including data streams, data bundles, and harmonization.\nImplement identity resolution, micro segmentation, and activation strategies.\nCollaborate with Salesforce Marketing Cloud, to enable real-time personalization and journey orchestration.\nEnsure data governance, and platform security.\nMonitor data quality, ingestion jobs, and overall platform performance.\n\n\n Education \n\nBE/B.Tech\nMaster of Computer Application\n\n Work Experience \nOverall experience of minimum 10 years in Data Management and Data Engineering role, with a minimum experience of 3 years as Salesforce Data Cloud Data Engineer\nHands-on experience with Salesforce Data Cloud (CDP), including data ingestion, harmonization, and segmentation.\nProficient in working with large datasets, data modeling, and ETL/ELT processes.\nUnderstanding of Salesforce core clouds (Sales, Service, Marketing) and how they integrate with Data Cloud.\nExperience with Salesforce tools such as Marketing Cloud.\nStrong knowledge of SQL, JSON, Apache Iceberg and data transformation logic.\nFamiliarity with identity resolution and customer 360 data unification concepts.\nSalesforce certifications (e.g., Salesforce Data Cloud Accredited Professional, Salesforce Administrator, Platform App Builder).\nExperience with CDP platforms other than Salesforce (e.g., Segment, Adobe Experience Platform (Good to have)).\nExperience with cloud data storage and processing tools (Azure, Snowflake, etc.).\n\n\n Behavioural Competencies \n\nTeamwork & Leadership\nMotivation to Learn and Grow\nOwnership\nCultural Fit\nTalent Management\n\n Technical Competencies \n\nProblem Solving\nAzure Data Factory\nAzure DevOps\nAzure SQL",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'apache', 'data modeling', 'data transformation', 'etl', 'snowflake', 'navisworks', 'data management', 'bim', 'revit architecture', 'microsoft azure', 'revit mep', 'azure data factory', 'autocad', 'azure devops', 'salesforce', 'talent management', 'sql azure', 'revit', 'json', 'salesforce core']",2025-06-12 13:56:49
Test Engineer - C# Selenium,Encora,5 - 8 years,Not Disclosed,['Chennai'],Greetings from Encora Innovation Labs Pvt Ltd!\n\nWe have received your profile for Test Engineer role. We would appreciate if you take out a few minutes of your time and share the below information to proceed further interview process.\n\nImportant Note : Please do not reply if you have a 60/90-days notice period. Kindly excuse us for candidates with a longer notice period .,,,,"['C#', 'Automation Testing', 'Appium', 'API', 'Agile', 'Selenium', 'Manual Testing']",2025-06-12 13:56:51
Sr. ETL QA /ETL QA Lead,Visionet Systems,6 - 11 years,Not Disclosed,['Bengaluru'],"Position Summary:\nWe are seeking a highly skilled ETL QA Engineer with at least 6 years of experience in ETL/data pipeline testing on the AWS cloud stack , specifically with Redshift, AWS Glue, S3 , and related data integration tools. The ideal candidate should be proficient in SQL , capable of reviewing and validating stored procedures , and should have the ability to automate ETL test cases using Python or suitable automation frameworks . Strong communication skills are essential, and web application testing exposure is a plus.\nTechnical Skills Required:",,,,"['Data validation', 'orchestration', 'Web testing', 'Test scenarios', 'Selenium', 'Stored procedures', 'Test cases', 'Reporting tools', 'SQL', 'Python']",2025-06-12 13:56:54
YASH Technologies is Hiring - Sr. Consultant - SAP Build Apps,Yash Technologies,1 - 3 years,Not Disclosed,"['Hyderabad', 'Pune']","Develop applications using SAP Build Apps , SAP Build Process Automation, and SAP Build Work Zone.\nIntegrate SAP Build solutions with core SAP systems (SAP S/4HANA, SAP ECC, SAP BTP)\nConfigure and extend SAP Fiori applications using SAP Build tools.\nImplement process automation workflows using SAP Build Process Automation.\nWork with APIs (OData, REST) to enable data integration with SAP and non-SAP systems.\nEnsure application performance, quality, and responsiveness.\nTroubleshoot and resolve technical issues related to SAP Build apps and integrations.",,,,"['SAP ECC', 'SAP Build Apps', 'SAP systems', 'SAP BTP', 'SAP S/4HANA']",2025-06-12 13:56:56
Senior Software Developer - Java & Angular,S&P Global Market Intelligence,7 - 12 years,Not Disclosed,"['Mumbai', 'Maharastra']","Grade Level (for internal use) : - 10\nThe Team\nYou will be an expert contributor and part of the Rating Organizations Data Services Product Engineering Team\nThis team, who has a broad and expert knowledge on Ratings organizations critical data domains, technology stacks and architectural patterns, fosters knowledge sharing and collaboration that results in a unified strategy\nAll Data Services team members provide leadership, innovation, timely delivery, and the ability to articulate business value\nBe a part of a unique opportunity to build and evolve S&P Ratings next gen analytics platform\nResponsibilities:\nDesign and implement innovative software solutions to enhance S&P Ratings' cloud-based data platforms.\nMentor a team of engineers fostering a culture of trust, continuous growth, and collaborative problem-solving.\nCollaborate with business partners to understand requirements, ensuring technical solutions align with business goals.\nManage and improve existing software solutions, ensuring high performance and scalability.\nParticipate actively in all Agile scrum ceremonies, contributing to the continuous improvement of team processes.\nProduce comprehensive technical design documents and conduct technical walkthroughs.\nExperience & Qualifications:\nBachelors degree in computer science, Information Systems, Engineering, equivalent or more is required\nProficient with software development lifecycle (SDLC) methodologies like Agile, Test-driven development\n7+ years of development experience in enterprise products, modern web development technologies Java/J2EE, UI frameworks like Angular, React, SQL, Oracle, NoSQL Databases like MongoDB\nExperience designing transactional/data warehouse/data lake and data integrations with Big data eco system leveraging AWS cloud technologies\nExp. with Delta Lake systems like Databricks using AWS cloud technologies and PySpark is a plus\nThorough understanding of distributed computing\nPassionate, smart, and articulate developer\nQuality first mindset with a strong background and experience with developing products for a global audience at scale\nExcellent analytical thinking, interpersonal, oral and written communication skills with strong ability to influence both IT and business partners\nSuperior knowledge of system architecture, object-oriented design, and design patterns.\nGood work ethic, self-starter, and results-oriented\nExcellent communication skills are essential, with strong verbal and writing proficiencies\nAdditional Preferred Qualifications:\nExperience working AWS\nExperience with SAFe Agile Framework\nBachelor's/PG degree in Computer Science, Information Systems or equivalent.\nHands-on experience contributing to application architecture & designs, proven software/enterprise integration design principles\nAbility to prioritize and manage work to critical project timelines in a fast-paced environment\nExcellent Analytical and communication skills are essential, with strong verbal and writing proficiencies\nAbility to train and mentor\nBenefits:\nHealth & Wellness: Health care coverage designed for the mind and body.\nContinuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\nInvest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\nFamily Friendly Perks: Its not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\nBeyond the Basics: From retail discounts to referral incentive awardssmall perks can make a big difference.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'PySpark', 'Delta Lake systems', 'React', 'Angular', 'UI frameworks', 'SQL', 'NoSQL Databases', 'MongoDB', 'Databricks', 'Oracle', 'AWS', 'AWS cloud']",2025-06-12 13:56:58
EPM Principal/Sr Principal Consultant FCCS/ARCS/TRCS/PBCS/EPBCS/PCMCS,Oracle,10 - 20 years,Not Disclosed,"['Noida', 'Gurugram', 'Chennai']","Hiring for Oracle Experts at different level from 5-20 years.\n\nJob locations - Bangalore, Mumbai, Pune, Hyderabad, Chennai, Kolkata, Noida, Gurgaon, Gandhinagar\n\nEPM Products - ARCS, TRCS, FCCS, EPBCS, PBCS, EPRCS, PCMCS\n\nPlanning - PBCS/ EPBCS\nExperience in Implementation of Hyperion with strong Application Development process experience on Hyperion EPM Product Suite.\nExperience in Requirement Gathering & Solution Design\nSound knowledge on Hyperion Planning/PBCS/EPBCS\nSound functional knowledge (Understand of planning modelling like P&L, BS, Workforce, Capex planning etc.. and inter dependencies)\nSound Knowledge on Business Rules/Forms / Task Lists / Reports.\nHands on Experience on Planning Modules is must.\nGood communication Skills\n\nFCCS\n\nFunction as applications design architect/Lead for Oracle FCCS\nApplication Design point of contact for FCCS Analyst Teams\nProvide Solutions to existing Architecture Design on the current system\nCollaborate effectively with other groups\nEPM Experience 5+ Years\nExperience in Implementation of EPM cloud with strong Application Development process, experience on FCCS/HFM and good knowledge on consolidation process.\nExperience in Requirement Gathering & Solution Design\nDesired functional knowledge (Understand of Income statement, Balance Sheet, different methods of consolidation and their calculations and disclosure in financial statements)\nSound functional knowledge Finance/accounting/ General Ledger/Sub Ledgers\nSound Knowledge on Financial Reports and SmartView Reports\n\nARCS\nExperience implementing ARCS from design, configuration, data integration, and testing\nSound knowledge on ARM/ARCS including Reconciliation Compliance & Transaction Matching\nFunctional knowledge of Finance/accounting and account reconciliation is a must\nKnowledge and experience working with a consolidation tool and general ledger is a plus\nProvide Solutions to existing Architecture Design on current system\nCollaborate effectively with other groups\nEPM Experience 5+ Years\nExperience in Implementation of Hyperion with strong Application Development process experience on Hyperion EPM Product Suite.\nExperience in Requirement Gathering & Solution Design\nSound functional knowledge Finance/accounting/ General Ledger/Sub Ledgers\nSound Knowledge on standard and custom reports\n\nTRCS\nFunction as applications design architect/Lead for Tax Reporting Cloud application development\nApplication Design point of contact for Tax Reporting Teams\nProvide Solutions to existing Architecture Design on current system\nCollaborate effectively with other groups\nEPM Experience 5+ Years\nExperience in Implementation of Hyperion with strong Application Development process experience on Hyperion EPM Product Suite.\nExperience in Requirement Gathering & Solution Design\nSound knowledge on Tax reporting and compliance processes, tax accounting and direct tax functions, CBCR and Deferred Tax Calculations\nSound knowledge on Hyperion Consolidation\nDesired functional knowledge (Understand of Income statement, Balance Sheet, different methods of consolidation and their calculations and disclosure in financial statements)\nSound Knowledge on Business Rules/Forms / Task Lists / Reports.\nGood communication Skills\n\nPCMCS\nFunction as applications design architect/Lead for Profitability and Cost Management\nApplication Design point of contact Profitability and Cost Management\nProvide Solutions to existing Architecture Design on current system\nAble to understand functional requirement of client and build the solution accordingly.\nCollaborate effectively with other groups\nEPM Experience 5+ Years\nShould have completed at least 3 implementations on PCMCS.\nIn depth understanding of Oracle Hyperion Essbase (ASO and BSO)\nIn depth knowledge in Integration (Data Management)\nAbility to design and develop complex Reports using Web Reporting Studio.\nExperience in Implementation of Hyperion with strong Application Development process experience on Hyperion EPM Product Suite.\nExperience in Requirement Gathering & Solution Design\nAble to leverage the modern best practices for design-development and automation process wherever required.\nSound functional knowledge\nKnowledge on Microsoft office tools including Excel, Word and Power point, leverage Smartview to build report and ad hoc analysis.\nPCMCS Certification is a value add",Industry Type: Software Product,Department: Consulting,"Employment Type: Full Time, Permanent","['EPM', 'Cloud EPM', 'solution design', 'consulting', 'implementation', 'EPBCS', 'EPRCS', 'PBCS', 'TRCS', 'PCMCS', 'FCCS', 'ARCS']",2025-06-12 13:57:01
Technical Architect,PwC India,10 - 15 years,Not Disclosed,"['Mumbai', 'Navi Mumbai', 'Gurugram']","Role Description\nWe are looking for a suitable candidate for the opening of Data/Technical Architect role for Data Management, preferably for one who has worked in Insurance or Banking and Financial Services domain and holds relevant experience of 10+ years. The candidate should be willing to take up the role of Senior Manager/Associate Director in an organization based on overall experience.\nLocation : Mumbai and Gurugram\nRelevant experience : 10+ years",,,,"['Data Architecture', 'Technical Architecture', 'Java', 'Bigquery', 'SCALA', 'Data Lake', 'Data Warehousing', 'Data Modeling', 'Data Bricks', 'Python']",2025-06-12 13:57:03
Artificial Intelligence Architect,Emerson,10 - 20 years,Not Disclosed,['Pune'],"Role & responsibilities\nDesign robust and scalable AI/ML architectures that support the development and deployment of machine learning models and AI solutions.\nDevelop and guide the implementation of end-to-end AI/ML solutions, including model development, data processing, and system integration.\nEvaluate and recommend the latest AI/ML technologies, frameworks, and tools to enhance system capabilities and performance.\nCollaborate with software engineers and other development teams to integrate AI/ML solutions into existing systems and applications. Ensure seamless operation and performance.\nWork with cross-functional teams, including developers, data scientists, machine learning engineers, and business stakeholders, to understand requirements and design solutions that align with business objectives.\n\nPreferred candidate profile\nBachelors degree in computer science, Data Science, Statistics, or a related field or a master's degree or higher is preferred.\nMore than 3 years of experience in designing and implementing AI/ML architectures, with a proven track record of successful projects.\nExtensive experience with machine learning frameworks (e.g., Go, TensorFlow, PyTorch), programming languages C#, .Net, NodeJS and data processing tools.\nStrong understanding of system architecture principles, including distributed systems, microservices, and cloud computing.\nExperience with Microsoft Azure cloud services and their AI/ML offerings\nExperience with event-handling systems such as Kafka\nExperience with big data technologies and data engineering practices.\nExcellent verbal and written communication skills, with the ability to convey complex technical concepts to non-technical stakeholders.",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Aiml', 'Ml', 'Python', 'Tensorflow', 'Pytorch', 'Architecture', 'Artificial Intelligence', '.Net', 'Machine Learning', 'Scikit-Learn']",2025-06-12 13:57:05
"Sr. Associate, Application Developer",XL India Business Services Pvt. Ltd,2 - 5 years,Not Disclosed,"['Hyderabad', 'Ahmedabad', 'Bengaluru']","Senior Associate Application Developer Bangalore, Karnataka, India We are seeking a skilled Senior Associate Application Developer with excellent expertise in C#, Salesforce, and DevOps practices\n\nThe ideal candidate will play a key role in developing, implementing, and maintaining software applications that align with our business goals\n\nYou will work collaboratively with cross-functional teams to deliver high-quality solutions that enhance our operational efficiency\n\nWhat you ll be DOING What will your essential responsibilities include? Design, develop, and maintain applications using C# and Salesforce technologies\n\nCollaborate with business analysts and stakeholders to gather requirements and translate them into technical specifications\n\nImplement DevOps practices to automate the deployment and monitoring of applications, ensuring a seamless integration process\n\nTroubleshoot and resolve application issues, providing support to end-users as needed\n\nCollaborate with the product owner and other squad members to understand user requirements and translate them into functional and technical specifications Carry out code reviews and provide feedback to other developers to ensure that the code meets functional and non-functional requirements Participate in agile ceremonies such as sprint planning, daily stand-ups, sprint reviews, etc to ensure smooth delivery Developing and maintaining custom data integration solutions with various sources and formats, including structured and unstructured data, to ensure data quality and consistency\n\nManaging and monitoring data pipelines and ensuring their availability, reliability, and scalability\n\nTroubleshooting issues related to data ingestion and transformation\n\nYou will report to the Release Train Engineer\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Developing and maintaining custom data integration solutions with various sources and formats, including structured and unstructured data, to ensure data quality and consistency\n\nEnsures that the solution and codebase is maintainable, scalable and adheres to best practices of software development\n\nDelivering high-quality, scalable, and reliable data ingestion pipelines\n\nSupports other members of the squad in resolving technical questions related to best practice, feasibility etc Desired Skills and Abilities: Insurance background\n\nProficiency in programming language such as C#\n\nExperience with data integration tools\n\nExcellent understanding of database management systems, data warehousing, and data modeling\n\nExperience with cloud platforms such as Azure\n\nFamiliarity with big data technologies such as Hadoop, Spark, and Kafka\n\nKnowledge of data privacy and security regulations such as GDPR\n\nExcellent problem-solving and troubleshooting skills\n\nEffective communication and collaboration skills\n\nWho WE are AXA XL, the P&C and specialty risk division of AXA, is known for solving complex risks\n\nFor mid-sized companies, multinationals and even some inspirational individuals we don t just provide re/insurance, we reinvent it\n\nHow? By combining a comprehensive and efficient capital platform, data-driven insights, leading technology, and the best talent in an agile and inclusive workspace, empowered to deliver top client service across all our lines of business property, casualty, professional, financial lines and specialty\n\nWith an innovative and flexible approach to risk solutions, we partner with those who move the world forward\n\nLearn more at axaxl\n\ncom What we OFFER Inclusion AXA XL is committed to equal employment opportunity and will consider applicants regardless of gender, sexual orientation, age, ethnicity and origins, marital status, religion, disability, or any other protected characteristic\n\nAt AXA XL, we know that an inclusive culture and a diverse workforce enable business growth and are critical to our success\n\nThat s why we have made a strategic commitment to attract, develop, advance and retain the most diverse workforce possible, and create an inclusive culture where everyone can bring their full selves to work and can reach their highest potential\n\nIt s about helping one another and our business to move forward and succeed\n\nFive Business Resource Groups focused on gender, LGBTQ+, ethnicity and origins, disability and inclusion with 20 Chapters around the globe Robust support for Flexible Working Arrangements Enhanced family friendly leave benefits Named to the Diversity Best Practices Index Signatory to the UK Women in Finance Charter Learn more at axaxl\n\ncom / about-us / inclusion-and-diversity\n\nAXA XL is an Equal Opportunity Employer\n\nTotal Rewards AXA XL s Reward program is designed to take care of what matters most to you, covering the full picture of your health, wellbeing, lifestyle and financial security\n\nIt provides competitive compensation and personalized, inclusive benefits that evolve as you do\n\nWe re committed to rewarding your contribution for the long term, so you can be your best self today and look forward to the future with confidence\n\nSustainability At AXA XL, Sustainability is integral to our business strategy\n\nIn an ever-changing world, AXA XL protects what matters most for our clients and communities\n\nWe know that sustainability is at the root of a more resilient future\n\nOur 2023-26 Sustainability strategy, called Roots of resilience , focuses on protecting natural ecosystems, addressing climate change, and embedding sustainable practices across our operations\n\nOur Pillars: Valuing nature: How we impact nature affects how nature impacts us\n\nResilient ecosystems - the foundation of a sustainable planet and society - are essential to our future\n\nWe re committed to protecting and restoring nature - from mangrove forests to the bees in our backyard - by increasing biodiversity awareness and inspiring clients and colleagues to put nature at the heart of their plans\n\nAddressing climate change: The effects of a changing climate are far reaching and significant\n\nUnpredictable weather, increasing temperatures, and rising sea levels cause both social inequalities and environmental disruption\n\nWere building a net zero strategy, developing insurance products and services, and mobilizing to advance thought leadership and investment in societal-led solutions\n\nIntegrating ESG: All companies have a role to play in building a more resilient future\n\nIncorporating ESG considerations into our internal processes and practices builds resilience from the roots of our business\n\nWe re training our colleagues, engaging our external partners, and evolving our sustainability governance and reporting\n\nAXA Hearts in Action: We have established volunteering and charitable giving programs to help colleagues support causes that matter most to them, known as AXA XL s Hearts in Action programs\n\nThese include our Matching Gifts program, Volunteering Leave, and our annual volunteering day - the Global Day of Giving\n\nFor more information, please see axaxl\n\ncom/sustainability",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data modeling', 'Database management', 'Agile', 'Data quality', 'data privacy', 'Business strategy', 'Application software', 'Operations', 'Monitoring', 'Salesforce']",2025-06-12 13:57:07
Solution Architect,Ericsson,9 - 10 years,Not Disclosed,['Gurugram'],"About this opportunity:\n\nAt Ericsson, we are offering a fantastic opportunity for a passionate and motivated Solution Architect to join our dynamic and diverse team. In this role, you will contribute to the design, construction, and management of Ericsson-based solutions. Familiarity with big data technologies, agile methodology and practices constitutes an integral part of the role.\nWhat you will do:\nManaging the overall operations of multiple solutions deployed within the customer environment.\nCustomer engagement is essential to secure agreements on the proposed solutions.\nPrepare technical presentations, proposals, and conduct walkthroughs with customers.\nLead the technical risk analysis and assist the Program Manager/Program Director in the overall risk analysis process.\nManage internal and external stakeholders to identify and bridge gaps.\nIdentify New Business Opportunities.\nLeading the delivery team by assigning tasks and reviewing progress.\nLead User Acceptance Testing (UAT) for the Customer.\nManaging the L1, L2, L3, and CNS (Support) teams, as well as the customers Operations and Maintenance (O&M) team.\nIdentify scope creep and change requests during the delivery phase.\nSupport Pre-Sales Activities\nPrepare Effort Estimation\nLead Customer Presentations and Demonstrations\nInterface with third-party providers (3PP) and original equipment manufacturers (OEMs) to evaluate and integrate their solutions into Ericssons offerings.\nAct as a Solution Lifecycle Manager for the proposed or implemented solution.\nProactively develop competence in new solution areas within the domain and technologies.\nMentor solution integrators, developers, and system architects, providing a transparent and open environment for growth and development.",,,,"['Maven', 'Performance management', 'XML', 'Configuration management', 'MySQL', 'SNMP', 'Presales', 'JSON', 'JIRA', 'Python']",2025-06-12 13:57:09
Associate - Digital Product Management,AMERICAN EXPRESS,5 - 10 years,Not Disclosed,['Gurugram'],"In this role, the person will report to the Product Manager Travel & Lifestyle Services, this role is an exciting opportunity for a PO/Analyst, the person will be working on data related products and to maintain quality of data for TLS in the Big Data Platform Cornerstone.\n  Minimum Qualifications\n5+ years experience in travel domain or minimum background in financial domain\nAt least 5 years of experience in technology product management or data-related products.\nAt least 5 years of experience in Software Architecture and Software Development.\n3 years experience with SQL\nExperience with agile methodologies, ie, rally, agile.\nAn ability to solve complex problems and a highly analytical approach.\nDemonstrate the ability to learn and be curious to understand and master the travel domain. You are excited and passionate for the travel domain.\nSelf-starter with the ability to think creatively and strategically\nStrong communication and stakeholder management skills\nExcellent communication skills with the ability to engage, influence, and inspire partners to drive collaboration and alignment.\nDemonstrate the ability to maintain a positive attitude and sense of humor in the face of chaos and challenges\nHas a successful record of leading and coordinating business, delivery, and technology teams to define, prioritize, and deliver on a product roadmap\nStrong product management skills that will take full ownership from analysis through implementation.\nHigh degree of organization, individual initiative, and personal accountability.\nPlatform Knowledge\nExperience working w/ Hadoop and Big Data Platform Cornerstone, Google Cloud Platform (GCP)\nProficient in Microsoft Suit, Power BI, Tableau, and SQL\nEducation\nBachelors in related fields (Computer Science, Information Technology, Engineer, Electronics)\nPreferred Qualifications\nMasters in related in fields (Computer Science, Information Technology, Engineer, Electronics)\nWe back you with benefits that support your holistic we'll-being so you can be and deliver your best. This means caring for you and your loved ones physical, financial, and mental health, as we'll as providing the flexibility you need to thrive personally and professionally:\nCompetitive base salaries\nBonus incentives\nSupport for financial-we'll-being and retirement\nComprehensive medical, dental, vision, life insurance, and disability benefits (depending on location)\nFlexible working model with hybrid, onsite or virtual arrangements depending on role and business need\nGenerous paid parental leave policies (depending on your location)\nFree access to global on-site we'llness centers staffed with nurses and doctors (depending on location)\nFree and confidential counseling support through our Healthy Minds program\nCareer development and training opportunities",Industry Type: Financial Services,Department: Product Management,"Employment Type: Full Time, Permanent","['Product management', 'Career development', 'Finance', 'Analytical', 'Agile', 'microsoft', 'Stakeholder management', 'Information technology', 'SQL']",2025-06-12 13:57:12
Manager - Software Development,Amway,8 - 12 years,Not Disclosed,['Hyderabad'],"Primary Responsibilities\nCloud Expertise: Familiarity or hands-on experience with AWS and Google Cloud Platform (GCP) technologies to support data transformation, data structures, metadata management, dependency tracking, and workload orchestration.\nCollaboration & Independence: Self-motivated and capable of supporting the data needs of multiple teams, systems, and products within Amways data ecosystem.\nBig Data & Distributed Systems: Strong understanding of distributed systems for large-scale data processing and analytics, with a proven track record of manipulating, processing, and deriving insights from large, complex, and disconnected datasets.",,,,"['Data Transformation', 'GCP', 'Cloud', 'AWS']",2025-06-12 13:57:14
Consultant,Amdocs,4 - 9 years,Not Disclosed,['Pune'],"Amdocs helps those who build the future to make it amazing. With our market-leading portfolio of software products and services, we unlock our customers innovative potential, empowering them to provide next-generation communication and media experiences for both the individual end user and enterprise customers. Our employees around the globe are here to accelerate service providers migration to the cloud, enable them to differentiate in the 5G era, and digitalize and automate their operations. Listed on the NASDAQ Global Select Market, Amdocs had revenue of $5.00 billion in fiscal 2024. For more information, visit www.amdocs.com\n\n\nIn one sentence\n\nWe are seeking a Data Engineer with advanced expertise in Databricks SQL, PySpark, Spark SQL, and workflow orchestration using Airflow. The successful candidate will lead critical projects, including migrating SQL Server Stored Procedures to Databricks Notebooks, designing incremental data pipelines, and orchestrating workflows in Azure Databricks\n\n\nWhat will your job look like\n\nMigrate SQL Server Stored Procedures to Databricks Notebooks, leveraging PySpark and Spark SQL for complex transformations.\nDesign, build, and maintain incremental data load pipelines to handle dynamic updates from various sources, ensuring scalability and efficiency.\nDevelop robust data ingestion pipelines to load data into the Databricks Bronze layer from relational databases, APIs, and file systems.\nImplement incremental data transformation workflows to update silver and gold layer datasets in near real-time, adhering to Delta Lake best practices.\nIntegrate Airflow with Databricks to orchestrate end-to-end workflows, including dependency management, error handling, and scheduling.\nUnderstand business and technical requirements, translating them into scalable Databricks solutions.\nOptimize Spark jobs and queries for performance, scalability, and cost-efficiency in a distributed environment.\nImplement robust data quality checks, monitoring solutions, and governance frameworks within Databricks.\nCollaborate with team members on Databricks best practices, reusable solutions, and incremental loading strategies\n\n\nAll you need is...\n\nBachelor s degree in computer science, Information Systems, or a related discipline.\n4+ years of hands-on experience with Databricks, including expertise in Databricks SQL, PySpark, and Spark SQL.\nProven experience in incremental data loading techniques into Databricks, leveraging Delta Lake's features (e.g., time travel, MERGE INTO).\nStrong understanding of data warehousing concepts, including data partitioning, and indexing for efficient querying.\nProficiency in T-SQL and experience in migrating SQL Server Stored Procedures to Databricks.\nSolid knowledge of Azure Cloud Services, particularly Azure Databricks and Azure Data Lake Storage.\nExpertise in Airflow integration for workflow orchestration, including designing and managing DAGs.\nFamiliarity with version control systems (e.g., Git) and CI/CD pipelines for data engineering workflows.\nExcellent analytical and problem-solving skills with a focus on detail-oriented development.\n  Preferred Qualifications  \nAdvanced knowledge of Delta Lake optimizations, such as compaction, Z-ordering, and vacuuming.\nExperience with real-time streaming data pipelines using tools like Kafka or Azure Event Hubs.\nFamiliarity with advanced Airflow features, such as SLA monitoring and external task dependencies.\nCertifications such as Databricks Certified Associate Developer for Apache Spark or equivalent.\nExperience in Agile development methodologie\n\n\nWhy you will love this job:\nYou will be able to use your specific insights to lead business change on a large scale and drive transformation within our organization.\nYou will be a key member of a global, dynamic and highly collaborative team with various possibilities for personal and professional development.\nYou will have the opportunity to work in multinational environment for the global market leader in its field!\nWe offer a wide range of stellar benefits including health, dental, vision, and life insurance as well as paid time off, sick time, and parental leave!",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure databricks', 'airflow', 'pyspark', 'sql', 'spark', 'azure cloud services', 'continuous integration', 'azure data lake', 'workflow orchestration', 'ci/cd', 'warehouse', 't-sql', 'sql server', 'stored procedures', 'data bricks', 'git', 'kafka', 'data warehousing concepts', 'agile']",2025-06-12 13:57:17
Advisor Application Support,Fiserv,10 - 15 years,Not Disclosed,['Pune'],"Design and Development Automation Script : Design and maintain advanced Python scripts to deliver comprehensive insights into File Transmission component and its various Life Cycle.\nPerformance Optimization : Improve efficiency when handling large datasets using techniques such as optimized large data manipulation, and RDBMS data models.\nAdvanced Regex Utilization : Apply sophisticated regular expressions to create accurate field extraction and mapping to the large dataset.\nFile Transmission Monitoring Automation : Track and report on each stage of file transmission, continuously refining monitoring strategies for enhanced reliability and visibility.\nCross-Functional Collaboration : Work closely with various teams to integrate Python script with broader IT systems and workflows.\nDevelop and maintain automation scripts using Python for testing, data validation, and system operations.\nDesign and implement automation frameworks.\nAutomate File Transmission applications using Python and Selenium.\nMaintain automated workflows and troubleshooting issues in context of File Transmissions.\nWrite reusable, scalable, and maintainable code with proper documentation.\nWhat You Will Need to Have\nEducation : bachelors and/or masters degree in Information Technology, Computer Science, or a related field.\nExperience : Minimum of 10 years in IT, with a focus on Python, SFTP tools, data integration, or technical support roles.\nProficiency in Python programming.\nExperience with Selenium for automation.\nFamiliarity with test automation frameworks like PyTest or Robot Framework.\nUnderstanding of REST APIs and tools like Postman or Python requests.\nBasic knowledge of Linux/Unix environments and shell scripting.\nDatabase Skills : Experience with relational databases and writing complex SQL queries with advanced joins.\nFile Transmission Tools : Hands-on experience with platforms like Sterling File Gateway, IBM Sterling, or other MFT solutions.\nAnalytical Thinking : Proven problem-solving skills and the ability to troubleshoot technical issues effectively.\nCommunication : Strong verbal and written communication skills for collaboration with internal and external stakeholders.\nWhat Would Be Great to Have (Optional)\nTool Experience : Familiarity with tools such as Splunk, Dynatrace, Sterling File Gateway, File Transfer tool.\nLinux : Working knowledge of Linux and command-line operations.\nSecure File Transfer Protocols : Hands-on experience with SFTP and tools like SFG, NDM, and MFT using SSH encryption.\nTask Scheduling Tools : Experience with job scheduling platforms such as AutoSys, Control-M, or cron",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Application support', 'Linux', 'RDBMS', 'Shell scripting', 'Selenium', 'Troubleshooting', 'Information technology', 'Technical support', 'Python']",2025-06-12 13:57:19
Power BI Developer,Kellogg Brown & Root (KBR),5 - 10 years,Not Disclosed,['Chennai'],"Title:\nPower BI Developer\nCollaborate with all levels of finance organization on reporting requirements for both internal and external customers.\nWork independently and in partnership with business owners to provide innovative interactive reporting solutions to address a wide range of business needs using Power BI, Power Query, VBA, Cognos and other reporting tools.\nTransform financial data into visualization charts using Power BI and other reporting tools.\nLeverage multiple databases to merge and compile information to calculate relevant financial and business performance metrics.\nMaximize automation of routine tasks and processes using advanced toolsets (Artificial Intelligence or AI , Optical Character Recognition or OCR , Robotic Process Automation or RPA or Bots ).\nAutomate translation and migration of data between different systems (Costpoint, Cobra, EPM, EDW, OnBase).\nEnsure data quality by identifying and correcting errors, inconsistencies, and missing data to improve accuracy.\nCreate documentation and work instructions for applications and processes, ensure compliance with KBR IT standards and controls.\nBasic Qualifications:\nBachelor s Degree or equivalent in Finance, Accounting, Business Information Technology, Business Analytics, Information Systems or a related field.\nProficiency in Power BI, Data Modeling, SQL, VBA, Power Query.\nExpert understanding of Power BI functionality (reporting, publishing, security, mobile app).\nFoundational understanding of financial reporting metrics (Revenue, Cost of Goods Sold, Indirect Rate Application, EBIT, Cashflow, DSO, DPO)\nWorking knowledge of project management core concepts (contract types, cost sets, schedule, budgets).\nExperience with data analysis techniques, data integration, data modeling and data visualization.\nFamiliarity with basic software testing and implementation concepts and methods.\nPreferred Qualifications:\nWorking knowledge of Costpoint, Cobra, Hyperion (EPM, FCCS), OnBase, EDW, MSD.\nCapability with alternate programming and reporting tools (DAX, Python or R, Appian, Cognos).\nProject management Professional (PMP) or EVMS certification.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['PMP', 'Data analysis', 'Publishing', 'Data modeling', 'Cognos', 'Hyperion', 'Data quality', 'Information technology', 'SQL', 'Python']",2025-06-12 13:57:21
EPBCS Principal Consultant//EPBCS Senior Consultant,Leading MNC,4 - 9 years,25-40 Lacs P.A.,[],"Leading MNC in Bangalore & Kochi\nHiring for EPBCS Principal Consultant/EPBCS Senior Consultant\nImmediate Joiners///Lesser notice period\nExperience 4-10 Years\n. The ideal candidate will have a strong background in Oracle Cloud EPM, particularly PBCS/EPBCS, and experience across multiple full-cycle implementation projects. You should be proficient in configuring out-of-the- box modules, writing Groovy scripts, and integrating data using tools like FDMEE, Data Management, or ODI. A solid understanding of financial and functional processes, coupled with excellent communication and documentation skills, is essential. Experience in public sector implementations and automation using EPM Automate is highly desirable.\n\nKey Responsibilities\nLead 1- 4 full-cycle Oracle Cloud EPM (PBCS/EPBCS) implementations.\nGather business requirements and create functional/technical specifications.\nConfigure and customize EPBCS modules (Financials, Workforce, Capex, Projects, Strategic Modeling).\nDevelop Groovy scripts, business rules, and automation using EPM Automate.\nManage data integrations using FDMEE, Data Management, or ODI.\nConduct system testing, troubleshoot issues, and support users post-go-live.\nBuild reports using Excel, SmartView, and Essbase tools.\nWork independently and document solutions clearly.\nCollaborate with clients and cross-functional teams to ensure project success.\nExperience in public sector implementations is a plus.\n\nRequired Skills & Qualifications\nRelevant experience ranging from 4 to 10 years\nExperience should comprise of exposure to at least 2-4 full project life cycles (Development projects)\nSolid experience with Oracle Cloud EPM (preferably PBCS/EPBCS)\nExtensive experience in analyzing requirements, writing functional specifications, conducting tests, troubleshooting issues and interfacing with business users\nExperience in Groovy scripting.\nExperience in out of the box modules Financials, Workforce, Capex, Projects and Strategic Modelling\nImplementing EPBCS at Public Sectors will be an added advantage.\nSpecific knowledge of integration tools such as; FDMEE, Data Management or ODI (Oracle Data Integrator)\nExperience with automation scripts such as EPM Automate\nExtensive knowledge of Excel, Essbase Spreadsheet Add-in, and SmartView\nExcellent written and communication skills\nKnowledge of financial process and functional processes\nAbility to work independently with minimum guidance\nStrong technical documentation skills\nExperience in a project team environment using structured methodologies for gathering business requirements, business processes, data conversions and system interfaces\n\nInterested Candidates can mail their cv at simmi@hiresquad.in or call at 8467054123",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['EPM', 'Epbcs', 'Smartview', 'Fdme', 'Oracle Epm', 'Essbase', 'Fccs']",2025-06-12 13:57:24
Senior DevSecOps (Bangalore) - 8+ Years - Hybrid,Databuzz Ltd,8 - 10 years,5-15 Lacs P.A.,['Bengaluru'],"Databuzz is Hiring for Senior DevSecOps Engineer Dynamics 365 (Bangalore) - 8+ Years - Hybrid\n\nPlease mail your profile to jagadish.raju@databuzzltd.com with the below details, If you are Interested.\n\nAbout DatabuzzLTD: Databuzz is One stop shop for data analytics specialized in Data Science, Big Data, Data Engineering, AI & ML, Cloud Infrastructure and Devops. We are an MNC based in both UK and INDIA. We are a ISO 27001 & GDPR complaint company.\n\nCTC -\nECTC -\nNotice Period/LWD - (Candidate serving notice period will be preferred)\nDOB-\n\nPosition: Senior DevSecOps Engineer Dynamics 365 (Bangalore) - 8+ Years - Hybrid\n\nMandatory Skills:\n\nShould have experience in Azure DevOps\nShould have experience Dynamics - 365\nStrong experience with Python and PowerShell for scripting and automation tasks\nExperienced in working with Kubernetes, Terraform,\nGood to have Service Now, YAML\n\nRegards,\nJagadish Raju - Talent Acquisition Specialist\njagadish.raju@databuzzltd.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Dynamics 365', 'Powershell', 'Azure Devops', 'Python', 'Terraform', 'Docker', 'Servicenow', 'Yaml', 'Kubernetes']",2025-06-12 13:57:26
"Startup Account Manager, North",Amazon,10 - 15 years,Not Disclosed,['Gurugram'],"Sales, Marketing and Global Services (SMGS)\n\nAWS Sales, Marketing, and Global Services (SMGS) is responsible for driving revenue, adoption, and growth from the largest and fastest growing smalland mid-market accounts to enterprise-level customers including public sector.\n\nAmazon Web Services (AWS) offers a set of cloud services that enable all companies, from startups to enterprises, to run virtually everything in the cloud, including mobile applications, big data analytics, AI/ML platforms, and microservices/serverless infrastructures. AWS India Pvt. Ltd. , the reseller for cloud services in India, is looking for a Senior Startup Account Manager to help drive the growth of high-potential startups in India.\nYou need to possess passion about Startups, be a self-starter with a strong entrepreneurial spirit who is prepared to work in a fast-paced, often ambiguous environment, execute against ambitious goals, and consistently embrace the Amazon Culture. Your responsibilities will include driving growth and user adoption, migrations and ensuring startups select AWS as their preferred cloud provider in India. You will work closely with counterparts in business development, marketing, solution architecture and partner teams to lead execution of BD plays.\n\nThe candidate should have technical background that enables him/her to drive engagement at the CXO level as well as with software developers and IT architects. The candidate should be an exceptional analytical thinker who thrives in fast-paced dynamic environments and has excellent communication and presentation skills. The candidate should be visioning and executing via collaboration with an extended team to address all startup s needs.\n\n\nEnsure customer success with early and growth stage startups in India\nDrive growth and market share in a defined territory\nAccelerate customer adoption through well-developed BD engagements\nDevelop and execute against a comprehensive account/territory plan.\nCreate & articulate compelling value propositions around AWS services.\nAccelerate customer adoption by engaging Founders, CXO, Board of Directors and VC influencers\nWork with AWS partners to manage joint selling opportunities\nAssist customers in identifying use cases for priority adoption of AWS as well as best practices implementations\nDevelop long-term strategic relationships with key accounts.\n\nA day in the life\nMeet startup CXOs and help them Build on AWS\nLeverage AWS startup programs to support early stage startups to bring idea to market\nTrack investments, technology trends; build coverage plans and oversee execution\nCollaborate with cross functional teams such as Sales, VC BD, Solutions Architect, Partners, Marketing\nEnsure high standards and maintain sales pipeline hygiene\n\nAbout the team\nThe AWS Startups team partners with startups around the world to build, launch, grow, and help scale their business. We don t just support startups with cloud infrastructure, but also partner with our startup customers throughout their journey by providing resources to tackle challenges from early stage fundraising to building technical teams and developing startup culture.\n\nAbout AWS\n\nDiverse Experiences\nAWS values diverse experiences. Even if you do not meet all of the preferred qualifications and skills listed in the job description, we encourage candidates to apply. If your career is just starting, hasn t followed a traditional path, or includes alternative experiences, don t let it stop you from applying.\n\nWhy AWS?\nAmazon Web Services (AWS) is the world s most comprehensive and broadly adopted cloud platform. We pioneered cloud computing and never stopped innovating that s why customers from the most successful startups to Global 500 companies trust our robust suite of products and services to power their businesses.\n\nInclusive Team Culture\nHere at AWS, it s in our nature to learn and be curious. Our employee-led affinity groups foster a culture of inclusion that empower us to be proud of our differences. Ongoing events and learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences, inspire us to never stop embracing our uniqueness.\n\nMentorship & Career Growth\nWe re continuously raising our performance bar as we strive to become Earth s Best Employer. That s why you ll find endless knowledge-sharing, mentorship and other career-advancing resources here to help you develop into a better-rounded professional.\n\nWork/Life Balance\nWe value work-life harmony. Achieving success at work should never come at the expense of sacrifices at home, which is why we strive for flexibility as part of our working culture. When we feel supported in the workplace and at home, there s nothing we can t achieve in the cloud. 10+ years of technology experience with a focus on field BD (quota-carrying) Experience in working with Startups in identifying, developing, negotiating, and closing large-scale technology deals.\nExperience in positioning and selling technology to new customers and in new market segments. Experience in proactively growing customer relationships within an account while expanding their understanding of the customer s business.\nExcellent verbal and written communications skills Functioned in an environment where they managed an account list in technology which included large growth in net new opportunities.\nProven track record of consistent territory growth and quota attainment. BA/BS/B.Tech degree required. Masters or MBA is a plus.\nUnderstanding of AWS and/or technology as a service (Iaas,SaaS,PaaS) is preferred.",,,,"['Solution architecture', 'Territory growth', 'Cloud computing', 'big data analytics', 'Cloud Services', 'Analytical', 'PAAS', 'cxo', 'Mobile applications', 'Solution Architect']",2025-06-12 13:57:29
Sr ETL/SSIS developer,Sagility India,6 - 11 years,Not Disclosed,['Bengaluru'],"Job Summary\nWe are seeking a highly skilled and self-driven SSIS with strong communication and client-facing skills to join our healthcare analytics team. This role requires a combination of deep technical expertise in SSIS and data integration along with the ability to consult and collaborate directly with clients to understand and address their data needs.\nThe ideal candidate will be experienced in building and maintaining scalable data pipelines, working with diverse healthcare data sources, and ensuring data quality and availability for downstream analytics. You will play a key role in delivering clean, trusted, and timely data for insights and reporting.\nKey Responsibilities\nDesign, develop, and maintain robust and scalable SSIS to support healthcare analytics and reporting platforms.\nEngage directly with clients to gather requirements, provide consultation, and translate business needs into technical solutions.\nIntegrate and normalize data from diverse healthcare data sources, including claims, EMR, lab, pharmacy, and eligibility systems.\nEnsure data accuracy, completeness, and consistency throughout ingestion and transformation processes.\nOptimize and tune data workflows for performance and scalability in a cloud or on-premise data platform.\nTroubleshoot and resolve data issues in a timely and proactive manner to support high data availability.\nCollaborate with analysts, data scientists, and business stakeholders to ensure data pipelines meet analytical needs.\nCreate and maintain comprehensive technical documentation for data pipelines, data dictionaries, and workflows.\nStay informed on healthcare compliance requirements (e.g., HIPAA), and ensure data handling practices follow regulatory standards.\nRequired Skills and Qualifications\n6+ years of experience in SSIS development and data engineering\nProven ability to interact directly with clients and translate business problems into data solutions\nStrong experience with SQL, SSIS, or PySpark for data processing\nDeep understanding of data warehousing concepts and dimensional modeling\nExperience working with healthcare datasets (e.g., claims, eligibility, clinical data)\nFamiliarity with cloud platforms (Azure, AWS, or GCP) and data lakes\nStrong troubleshooting, problem-solving, and performance tuning skills\nExcellent verbal and written communication skills\nBachelor's or Masters degree in Computer Science, Engineering, Information Systems, or a related field\nPreferred Qualifications\nProficiency in building data pipelines using tools such as Azure Data Factory, Informatica, Databricks, or equivalent\nExperience with FHIR, HL7, or other healthcare data standards\nFamiliarity with HIPAA and healthcare compliance requirements\nKnowledge of reporting tools like Power BI or Tableau\nExposure to CI/CD and data pipeline automation\nWhy Join Us?\nWork on high-impact healthcare projects with meaningful outcomes\nEngage directly with clients and make a tangible difference in their data strategy\nCollaborative team culture and continuous learning opportunities\nFlexible work arrangements and competitive compensation\n\nLocation - Bangalore\nShit Timing - 2 Pm to 11 PM\nWork - Hybrid\n\nRegards,\nnaveen.vediyappan@sagility.com",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SSIS', 'SQL', 'ETL']",2025-06-12 13:57:31
SR. Databricks Developer,Labcorp,7 - 12 years,Not Disclosed,['Bengaluru'],"Labcorp is hiring a Senior Data engineer.  This person will be an integrated member of Labcorp Data and Analytics team and work within the IT team.   Play a crucial role in designing, developing and maintaining data solutions using Databricks, Fabric, Spark, PySpark and Python.  Responsible to review business requests and translate them into technical solution and technical specification.  In addition, work with team members to mentor fellow developers to grow their knowledge and expertise.  Work in a fast paced and high-volume processing environment, where quality and attention to detail are vital.\n\nRESPONSIBILITIES:\nDesign and implement end-to-end data engineering solutions by leveraging the full suite of Databricks, Fabric tools, including data ingestion, transformation, and modeling.\nDesign, develop and maintain end-to-end data pipelines by using spark, ensuring scalability, reliability, and cost optimized solutions.\nConduct performance tuning and troubleshooting to identify and resolve any issues.\nImplement data governance and security best practices, including role-based access control, encryption, and auditing.\nWork in fast-paced environment and perform effectively in an agile development environment.\n\nREQUIREMENTS:\n8+ years of experience in designing and implementing data solutions with at least 4+ years of experience in data engineering.\nExtensive experience with Databricks, Fabric, including a deep understanding of its architecture, data modeling, and real-time analytics.\nMinimum 6+ years of experience in Spark, PySpark and Python.\nMust have strong experience in SQL, Spark SQL, data modeling & RDBMS concepts.\nStrong knowledge of Data Fabric services, particularly Data engineering, Data warehouse, Data factory, and Real- time intelligence.\nStrong problem-solving skills, with ability to perform multi-tasking.\nFamiliarity with security best practices in cloud environments, Active Directory, encryption, and data privacy compliance.\nCommunicate effectively in both oral and written.\nExperience in AGILE development, SCRUM and Application Lifecycle Management (ALM).\nPreference given to current or former Labcorp employees.\n\nEDUCATION:\nBachelors in engineering, MCA.",Industry Type: Medical Services / Hospital (Diagnostics),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Bricks', 'Python', 'Parquet', 'UDP', 'Shell Scripting', 'Microsoft SQL Server', 'DW BI project', 'Kafka', 'Mapreduce', 'EMR', 'Redshift', 'Hive', 'MySQL', 'Spark', 'Aws Databricks', 'Oracle', 'Redshift spectrum', 'Fabric', 'Lambda', 'Athena']",2025-06-12 13:57:34
"Spark, Java, Kafka- Hyderabad",Cognizant,12 - 15 years,Not Disclosed,['Hyderabad'],"Skill: Java, Spark, Kafka\nExperience: 10 to 16 years\nLocation: Hyderabad\n As Data Engineer, you will :\n       Support in designing and rolling out the data architecture and infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources\n       Identify data source, design  and implement data schema/models and integrate data that meet the requirements of the business stakeholders",,,,"['hive', 'cloudera', 'modeling', 'scala', 'data warehousing', 'apache pig', 'data pipeline', 'data architecture', 'scalability', 'sql', 'java', 'data modeling', 'spark', 'mysql', 'hadoop', 'etl', 'big data', 'hbase', 'python', 'oozie', 'data processing', 'airflow', 'elt', 'data engineering', 'nosql', 'mapreduce', 'kafka', 'feasibility analysis', 'hdfs', 'sqoop', 'aws']",2025-06-12 13:57:36
Senior Etl Informatica Developer,VBeyond,6 - 8 years,19-25 Lacs P.A.,"['Noida', 'Chennai', 'Bengaluru']","We are seeking a highly skilled and experienced Senior ETL & Reporting QA Analyst to join our dynamic team. The ideal candidate will bring strong expertise in ETL and Report Testing, with a solid command of SQL, and hands-on experience in Informatica, as well as BI Reporting tools. A strong understanding of the Insurance domain is crucial to this role. This position will be instrumental in ensuring the accuracy, reliability, and performance of our data pipelines and reporting solutions.\n\nKey Responsibilities:\nDesign, develop, and execute detailed test plans and test cases for ETL processes, data migration, and data warehousing solutions.\nPerform data validation and data reconciliation using complex SQL queries across various source and target systems.\nValidate Informatica ETL workflows and mappings to ensure accurate data transformation and loading.\nConduct end-to-end report testing and dashboard validations using Cognos (preferred), or comparable BI tools such as Tableau or Power BI.\nCollaborate with cross-functional teams including Business Analysts, Developers, and Data Engineers to understand business requirements and transform them into comprehensive test strategies.\nIdentify, log, and track defects to closure using test management tools and actively participate in defect triage meetings.\nMaintain and enhance test automation scripts and frameworks where applicable.\nEnsure data integrity, consistency, and compliance across reporting environments, particularly in the insurance domain context.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Informatica', 'ETL', 'Power Bi', 'Insurance Domain', 'Tableau', 'SQL']",2025-06-12 13:57:38
"Salesforce Developer (Apex, Lightning Web Components & Salesforce)",Synechron,5 - 10 years,Not Disclosed,['Pune'],"job requisition idJR1023217\n\nJob Summary\nSynechron is seeking a skilled and dedicated\n\nSalesforce Developer to join our dynamic team. In this role, you will be responsible for designing, developing, and implementing robust Salesforce solutions that enhance business processes and improve user experience. You will work closely with business analysts, product managers, and technical teams to deliver high-quality, scalable applications and configurations within the Salesforce platform. Your contribution will directly support our organization's digital transformation initiatives and ensure our Salesforce ecosystem aligns with evolving industry standards and client needs.\n\nSoftware\n\nRequired\n\nSkills:\nExtensive hands-on experience with Salesforce development including Apex, Lightning Web Components (LWC), Visualforce, and Salesforce configuration tools.\nProficiency with Salesforce SFDX, Visual Studio Code, and Git-based version control systems (e.g., Git, Bitbucket).\nKnowledge of Salesforce's declarative tools such as Flows, Process Builder, Approval Processes, and Email Templates.\nStrong understanding of Salesforce security model (Profiles, Permission Sets, Sharing Settings, Sharing Rules).\nExperience with Salesforce Mobile and Experience Cloud setup and configurations.\nGood understanding of Salesforce Governor Limits and best practices for optimization.\nExperience with SOQL, SOSL, DML operations, and data modeling.\nKnowledge of Salesforce integration techniques, including REST/SOAP APIs, and middleware tools like MuleSoft or AutoRabbit (preferred).\nFamiliar with development tools such as Salesforce DX, VS Code, and CI/CD pipelines using Jenkins or similar tool\nPreferred\n\nSkills:\nSalesforce Admin and Developer certifications.\nExperience with Einstein Analytics, Mulesoft, or AutoRabbit.\nKnowledge of industry-specific compliance or security standards, especially within Financial Services.\nOverall Responsibilities\nDevelop, customize, and maintain Salesforce applications, including Apex classes, triggers, Lightning Web Components, and Visualforce pages.\nDesign scalable and reusable Lightning components and configurations that meet business requirements.\nCollaborate with functional stakeholders to translate business needs into technical solutions.\nEnsure the integrity, security, and performance tuning of Salesforce applications.\nMaintain continuous alignment with Salesforce best practices, including governance, security policies, and performance optimization.\nCreate technical documentation, including design specifications, test cases, and deployment instructions.\nConduct code reviews, unit testing, and participate in release management.\nSupport data integrations with external systems using APIs or middleware.\nContribute to SCRUM/Agile development cycles, providing timely updates and collaborating effectively with team members.\nTechnical Skills (By Category)\nProgramming Languages:\nEssentialApex, JavaScript\nPreferredJava, R, or additional languages relevant to integration and automation\nDatabases & Data Management:\nStrong knowledge of Salesforce objects, data modeling, SOQL, SOSL, and DML operations\nCloud Technologies:\nSalesforce Cloud (Experience Cloud, Mobile)\nOptionalMuleSoft, AutoRabbit for integrations\nFrameworks and Libraries:\nLightning Web Components (LWC), Aura Components\nDevelopment Tools & Methodologies:\nSalesforce DX, Visual Studio Code, Git/Bitbucket, Agile/Scrum methodologies\nSecurity Protocols:\nSalesforce security model, sharing settings, and best practices for data protection\nExperience\nMinimum of 5+ years of extensive Salesforce development experience including large-scale customizations and integrations.\nProven track record delivering end-to-end Salesforce solutions in complex enterprise environments.\nExperience working within Agile teams with continuous integration/deployment.\nIndustry experience in Financial Services is preferred, especially in Investment Banking, Asset Management, or Banking.\nDemonstrated ability to work collaboratively with cross-functional teams and stakeholders.\nDay-to-Day Activities\nDevelop and test Apex classes, triggers, Lightning Web Components, and other custom Salesforce components.\nCollaborate with business analysts and product owners to refine requirements.\nConduct code reviews, unit testing, and assist in user acceptance testing.\nManage Salesforce releases, deployments, and configurations.\nInvestigate and debug system issues, and optimize platform performance.\nParticipate in sprint planning, stand-ups, and retrospectives.\nStay current on Salesforce platform updates, new features, and best practices.\nSupport ongoing enhancements and user support initiatives.\nQualifications\nBachelor's degree in Computer Science, Information Technology, or a related field.\nSalesforce certifications such as Salesforce Certified Platform Developer I & II and Salesforce Certified Service Cloud Consultant are strongly preferred.\nStrong understanding of Salesforce architecture, frameworks, and best practices.\nProfessional Competencies\nExcellent problem-solving and analytical skills.\nEffective communication skills to interface with technical and non-technical stakeholders.\nAbility to work independently and as part of a team in a fast-paced environment.\nStrong organizational skills with an ability to prioritize tasks effectively.\nSelf-motivated learner committed to staying current with Salesforce updates and industry trends.\nAdaptability and eagerness to contribute to innovative solutions.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sosl', 'soql', 'salesforce', 'sales force development', 'salesforce integration', 'visualforce', 'salesforce dx', 'continuous integration', 'rest', 'dml', 'salesforce lightning', 'ci/cd', 'salesforce security model', 'javascript', 'apex', 'visual studio code', 'git', 'data modeling', 'lwc', 'agile', 'soap']",2025-06-12 13:57:41
Cyber Security Manager // 6-10 years // Mumbai,2coms,7 - 12 years,Not Disclosed,['Mumbai'],"SUMMARY\nOur client is IT MNC part of one of the major insurance groups based out of Germany and Europe. The Group is represented in around 30 countries worldwide, with Over 40,000 people worldwide, focusing mainly on Europe and Asia. Our client offers a comprehensive range of insurances, pensions, investments and services by focusing on all cutting edge technologies majorly on Could, Digital, Robotics Automation, IoT, Voice Recognition, Big Data science, advanced mobile solutions and much more to accommodate the customers future needs around the globe thru supporting millions of internal and external customers with state of-the-art IT solutions to everyday problems & dedicated to bringing digital innovations to every aspect of the landscape of insurance.\n \nJob Location: Hiranandani Gardens, Powai, Mumbai\n \nMode: Work from Office\n\n\nRequirements\nRoles & Responsibilities:\nDefine project scope,   objectives, and deliverables in collaboration with stakeholders.\nDevelop comprehensive project plans, including timelines, budgets, and resource allocation.\nManage and coordinate project teams, including security engineers, analysts, and other technical resources.\nTrack project progress, identify and manage risks and issues, and implement effective mitigation strategies.\nEnsure adherence to project management methodologies and best practices.\nStay up-to-date with the latest cyber security trends   and technologies.\nSkill & Competencies:\nStrong track record of delivering IT projects in a large, complex environment.  (7 years), especially experience in the implementation of financial and regulatory requirements in the CFO context in Group-wide systems and their integration\nProven 5+ years experience as a PM \nBachelor's degree in Computer Science, Information Technology, or a related field.\nProven experience  (typically 5+ years) managing IT projects, with a significant focus on cyber security initiatives.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['risk management', 'it security', 'cyber security', 'information technology', 'iso', 'owasp', 'soc', 'ceh', 'information security', 'vulnerability management', 'siem', 'vulnerability assessment', 'cissp', 'nessus', 'cyber', 'security', 'it projects', 'security engineering', 'application security', 'vapt', 'penetration testing']",2025-06-12 13:57:44
"Manager, WFM, Account Health Support (AHS)",Amazon,3 - 8 years,Not Disclosed,['Hyderabad'],"Amazon.com strives to be Earths most customer-centric company where people can find and discover virtually anything they want to buy online. By giving customers more of what they want low prices, vast selection, and convenience Amazon.com continues to grow and evolve as a world-class e-commerce platform. The Account Health Support (AHS) Specialist within the AHS team acts as the primary interface between Amazon and our Selling partners. We obsess over providing world class support to Sellers selling on the Amazon platform. We strive to predict Sellers needs before they recognize they may have a problem, create innovative self-help tools, and provide solutions to help our partners better serve their customers.\n\n\nCalculate demand volume forecast at interval level, day level, week level and at month level, along with knowledge of statistical indicators to check their accuracy.\nCapacity planning at weekly/ monthly level, so that the required headcount for hiring could be shared with senior leadership.\nCreate schedules on excel and on a scheduling tool (preferably Aspect) based week level, day level and interval level volume pattern.\nDiving deep into data/processes to identify problems and solutions and presenting them to leadership.\nKeeping regular communication with site operations, senior leadership, technology teams and other stakeholders to manage critical parameters, employee experience, contingency etc.\nPublishing reports of critical WFM and other important parameters to drive efficiency in them and to keep all relevant stakeholders regularly informed.\nCreating employees rotational plan and conducting shift bid process to help shift rollover for frontline staff.\nOptimizing break, meeting and other non-productive activities, managing interval level service level.\nManaging real time analysts and schedulers.\n\nA day in the life\nThe ideal candidate is passionate about leveraging data and tools to deliver actionable insights that drive improvements in planning accuracy, and has a strong delivery record and experienced in driving execution in a cross-functional environment, backed by analysis and data. They thrive in a fast-paced environment, relishes working with large transactional volumes and big data and enjoys the challenge of highly complex, and sometimes ambiguous, business context. You will work cross-functionally to ensure that decisions are made and actioned, which will ensure our operations have the volume to run as efficiently as possible.\n\nAbout the team\nThe Account Health Support Workforce Management Team has a mission of fulfilling the Service Level agreements continuously in partnership with Operations, throughout all verticals/marketplaces along with optimum utilization of the available resources and meeting the goal thresholds for all the capacity level attributes (Shrinkage, TPH etc.). To attain to the program objectives, AHS Workforce team sets appropriate goals for Operations (Shrinkage), drives effective queue management, time to time checks to ensure capacity on each Vertical is sufficient to handle projected volume and take necessary actions to meet the requirement if otherwise, scheduling heads appropriately to match the incoming patterns, queueing tasks manually to fill for the deficit in projected volume and to support any new launches, effective management of non-production time to reduce idle hours and sharing reports on different performance metrics to drive the results.\n3+ years of team management experience\n3+ years of program or project management experience\n3+ years of working cross functionally with tech and non-tech teams experience\n3+ years of delivering cross functional projects experience\nExperience defining program requirements and using data and metrics to determine improvements Experience implementing repeatable processes and driving automation or standardization\nExperience in data mining, data management, reporting, and SQL queries",Industry Type: Internet,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Automation', 'Service level', 'Team management', 'Publishing', 'Workforce management', 'Data management', 'Project management', 'Scheduling', 'Data mining', 'Capacity planning']",2025-06-12 13:57:46
Application Developer - Dot Net IFRS 17 // Mumbai // 4-8 Yrs,2coms,4 - 9 years,Not Disclosed,['Mumbai'],"SUMMARY\nAbout the client:\n\nOur client is an IT Technology & Services Management MNC , supporting millions of internal and external customers with state of-the-art IT solutions to everyday problems & dedicated to bringing digital innovations to every aspect of the landscape of insurance. Our Client is a part of one of the major insurance groups based out of Germany and Europe. The Group is represented in around 26 countries worldwide, with Over 37,000 people worldwide, focusing mainly on Europe and Asia & offers a comprehensive range of insurances, pensions, investments and services by focusing on all cutting edge technologies majorly on Could, Digital, Robotics Automation, IoT, Voice Recognition, Big Data science, advanced mobile solutions and much more to accommodate the customers future needs around the globe.\n\n\n\n\n\nRequirements\nCompentences for .NET\nNET Framework (incl. .net core), SQL Server, Entity Framework\nPrioritize business impact and urgency\nAbility to learn new technology and methodology quickly.\nKnowledge User Task API, GUI (Graphical User Interfaces) Design Compentences for NTS (New Tech Stack)\nNTS (New Tech Stack), Container runtime environment with Docker containers and Kubernetes, Cloud with AWS (Amazon Web Services), CI/CD - Continuous Integration / Continuous Deployment with Jenkins, Knowledge Source Management Github and Nexus\nOpenshift, Kerberos Authentication, Competences for Cluster Workflow\nDesign + implementation of process model, Design + implementation of input interfaces in REST format\nDevelopment of flow services below process model\n\nEducational Qualifications:\nBachelor’s or Master’s  degree in Computer Science /Engineering/Information Technology\nCandidate with non-computer science degree must have minimum 1 year of relevant experience\nMBA in IT / Insurance/Finance   can also apply for Requirements Engineer and Test Engineer role.\n\n\n\nBenefits",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'kubernetes', 'nexus', 'web services', 'openshift', 'ci/cd', 'kerberos', 'ccm', 'business objects administration', 'docker', 'profibus', 'jenkins', 'cd', 'rest', 'e-discovery', 'github', 'entity framework', 'intools', 'sql server', 'application development', 'net application', '.net core', '.net', 'abb dcs', 'aws']",2025-06-12 13:57:48
IT Project Associate,ZS,1 - 3 years,Not Disclosed,['Pune'],"Executes the end-to-end management of application development projects: including resource management, change management, vendor coordination, communications, training requirements, and budget (if applicable).\nEstimate the resources and participants needed to achieve project goals.\nReviews and recommends changes, reductions or additions to the overall project.\nActs as the liaison between IT, vendors, and end-users.\nMaintains the efficiency of the project coordination process such as planning, scheduling, and budget and risk assessment.",,,,"['Change management', 'Team management', 'Project management', 'Risk assessment', 'Consulting', 'Financial planning', 'Management consulting', 'Scheduling', 'Application development', 'Resource management']",2025-06-12 13:57:51
AI Associate Consultant - Platform Delivery,ZS,3 - 5 years,Not Disclosed,['Gurugram'],"Customer Success Associate Consultant design, develop and execute high-impact analytics solutions for large, complex, structured, and unstructured data sets (including big data) to drive impact on client business (topline). This person will lead the engagement for AI based SaaS product deployment to clients across industries. Leverage their strong Data Science, analytics and engineering skills to build Advanced analytics processes, build scalable and operational process pipelines and find data-driven insights that help our clients solve their most important business problems and bring optimizations. Associate Consultants also engage with Project Leadership team and clients to help them understand the insights, summaries, implications and make plans to act on them.",,,,"['Hospitality', 'Team management', 'Analytical', 'Management consulting', 'Financial planning', 'Healthcare', 'Predictive modeling', 'Stakeholder management', 'Analytics', 'SQL']",2025-06-12 13:57:53
Business Analyst,CGI,6 - 11 years,Not Disclosed,['Hyderabad'],"Business Data Analyst - HealthCare\nPosition Description\nJob Summary\nWe are seeking an experienced and results-driven Business Data Analyst with 5+ years of hands-on experience in data analytics, visualization, and business insight generation. This role is ideal for someone who thrives at the intersection of business and datatranslating complex data sets into compelling insights, dashboards, and strategies that support decision-making across the organization.\nYou will collaborate closely with stakeholders across departments to identify business needs, design and build analytical solutions, and tell compelling data stories using advanced visualization tools.\nKey Responsibilities\nData Analytics & Insights Analyze large and complex data sets to identify trends, anomalies, and opportunities that help drive business strategy and operational efficiency.\n• Dashboard Development & Data Visualization Design, develop, and maintain interactive dashboards and visual reports using tools like Power BI, Tableau, or Looker to enable data-driven decisions.\n• Business Stakeholder Engagement Collaborate with cross-functional teams to understand business goals, define metrics, and convert ambiguous requirements into concrete analytical deliverables.\n• KPI Definition & Performance Monitoring Define, track, and report key performance indicators (KPIs), ensuring alignment with business objectives and consistent measurement across teams.\n• Data Modeling & Reporting Automation Work with data engineering and BI teams to create scalable, reusable data models and automate recurring reports and analysis processes.\n• Storytelling with Data Communicate findings through clear narratives supported by data visualizations and actionable recommendations to both technical and non-technical audiences.\n• Data Quality & Governance Ensure accuracy, consistency, and integrity of data through validation, testing, and documentation practices.\nRequired Qualifications\nBachelor’s or Master’s degree in Business, Economics, Statistics, Computer Science, Information Systems, or a related field.\n• 5+ years of professional experience in a data analyst or business analyst role with a focus on data visualization and analytics.\n• Proficiency in data visualization tools: Power BI, Tableau, Looker (at least one).\n• Strong experience in SQL and working with relational databases to extract, manipulate, and analyze data.\n• Deep understanding of business processes, KPIs, and analytical methods.\n• Excellent problem-solving skills with attention to detail and accuracy.\n• Strong communication and stakeholder management skills with the ability to explain technical concepts in a clear and business-friendly manner.\n• Experience working in Agile or fast-paced environments.\nPreferred Qualifications\nExperience working with cloud data platforms (e.g., Snowflake, BigQuery, Redshift).\n• Exposure to Python or R for data manipulation and statistical analysis.\n• Knowledge of data warehousing, dimensional modeling, or ELT/ETL processes.\n• Domain experience in Healthcare is a plus.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Healthcare Domain', 'Bigquery', 'Redshift Aws', 'Snowflake', 'Data Analytics', 'Data Visualization', 'Python']",2025-06-12 13:57:55
Power platform Developer,ITC Infotech,3 - 8 years,9-19 Lacs P.A.,['Bengaluru'],"Key Responsibilities:\nDesign, develop, test, and deploy custom business applications using Microsoft Power Platform (Power Apps, Power Automate, Power BI, Power Virtual Agents).\nCollaborate with business users and IT teams to gather and analyze requirements.\nCreate automated workflows and process automation using Power Automate to improve operational efficiency.\nDevelop interactive dashboards and reports using Power BI to provide actionable insights.",,,,"['Canvas', 'Power Platform', 'Powerapps', 'Microsoft Power Automate', 'Power Automate']",2025-06-12 13:57:57
Senior Power BI Developer,Luxoft,6 - 11 years,Not Disclosed,['Bengaluru'],"Power BI Dashboard Development (UI Dashboards)\nDesign, develop, and maintain visually compelling, interactive Power BI dashboards aligned with business needs.\nCollaborate with business stakeholders to gather requirements, develop mockups, and refine dashboard UX.\nImplement advanced Power BI features like bookmarks, drill-throughs, dynamic tooltips, and DAX calculations.\nConduct regular UX/UI audits and performance tuning on reports.\nData Modeling in SQL Server & Dataverse\nBuild and manage scalable, efficient data models in Power BI, Dataverse, and SQL Server.\nApply best practices in dimensional modeling (star/snowflake schema) to support analytical use cases.\nEnsure data consistency, accuracy, and alignment across multiple sources and business areas.\nPerform optimization of models and queries for performance and load times.\nPower BI Dataflows & ETL Pipelines\nDevelop and maintain reusable Power BI Dataflows for centralized data transformations.\nCreate ETL processes using Power Query, integrating data from diverse sources including SQL Server, Excel, APIs, and Dataverse.\nAutomate data refresh schedules and monitor dependencies across datasets and reports.\nEnsure efficient data pipeline architecture for reuse, scalability, and maintenance.\nSkills\nMust have\nExperience: 6+ years in Business Intelligence or Data Analytics with a strong focus on Power BI and SQL Server.\nTechnical Skills:\nExpert-level Power BI development, including DAX, custom visuals, and report optimization.\nStrong knowledge of SQL (T-SQL) and relational database design.\nExperience with Dataverse and Power Platform integration.\nProficiency in Power Query, Dataflows, and ETL development.\nModeling: Proven experience in dimensional modeling, star/snowflake schema, and performance tuning.\nData Integration: Skilled in connecting and transforming data from various sources, including APIs, Excel, and cloud data services.\nCollaboration: Ability to work with stakeholders to define KPIs, business logic, and dashboard UX.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'data services', 'Data modeling', 'Database design', 'Analytical', 'Schema', 'power bi', 'SSIS', 'Business intelligence', 'microsoft']",2025-06-12 13:58:00
Director - BIM,Axtria,5 - 10 years,Not Disclosed,['Noida'],"Position Summary \nThis position is part of the technical leadership in data warehousing and Business Intelligence areas. Someone who can work on multiple project streams and clients for better business decision making especially in the area of Lifesciences/ Pharmaceutical domain.\n\n Job Responsibilities \n\no Technology Leadership – Lead guide the team independently or with little support to design, implement deliver complex cloud data management and BI project assignments. o Technical portfolio – Expertise in a range of BI and data hosting technologies like the AWS stack (Redshift, EC2), Snowflake, Spark, Full Stack, Qlik, Tableau, Microstrategy o Project Management – Get accurate briefs from the Client and translate into tasks for team members with priorities and timeline plans. Must maintain high standards of quality and thoroughness. Should be able to monitor accuracy and quality of others' work. Ability to think in advance about potential risks and mitigation plans. o Logical Thinking – Able to think analytically, use a systematic and logical approach to analyze data, problems, and situations. Must be able to guide team members in analysis. o Handle Client Relationship, P&L – Manage client relationship and client expectations independently. Should be able to deliver results back to the Client independently. Should have excellent communication skills.\n\n Education \n\nBE/B.Tech\nMaster of Computer Application\n\n Work Experience \n\nMinimum of 5 years of relevant experience in Pharma domain. TechnicalShould have 15 years of hands on experience in the following tools\nMust have working knowledge of toolsAtleast 2 of the following – Qlikview, QlikSense, Tableau, Microstrategy, Spotfire Aware of techniques such asUI design, Report modeling, performance tuning and regression testing Basic expertise with MS excel Advanced expertise with SQL FunctionalShould have experience in following concepts and technologies\nSpecifics\nPharma data sources like IMS, Veeva, Symphony, Cegedim etc.\nBusiness processes like alignment, market definition, segmentation, sales crediting, activity metrics calculation 0-2 years of relevant experience in a large/midsize IT services/Consulting/Analytics Company1-3 years of relevant experience in a large/midsize IT services/Consulting/Analytics Company3-5 years of relevant experience in a large/midsize IT services/Consulting/Analytics Company3-5 years of relevant experience in a large/midsize IT services/Consulting/Analytics Company\n\n Behavioural Competencies \n\nProject Management\nCommunication\nAttention to P&L Impact\nTeamwork & Leadership\nMotivation to Learn and Grow\nLifescience Knowledge\nOwnership\nCultural Fit\nScale of resources managed\nScale of revenues managed / delivered\nProblem solving\nTalent Management\nCapability Building / Thought Leadership\n\n Technical Competencies \n\nAWS KnowHow\nFormal Industry Certification AWS Certified Cloud Practitioner\nSnowflake\nData Engineering\nData Governance\nData Modelling\nData Operations (Service Management)\nData Warehousing & Data Lake\nDatabricks\nDataiku\nFormal Industry Certification Informatica_Cloud Data Warehouse & Data Lake Modernization\nMaster Data Management\nPatient Data Analytics Know How\nPharma Commercial Data - US\nPharma Commercial Data - EU",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['spotfire', 'sql', 'qlikview', 'microstrategy', 'tableau', 'snowflake', 'pharmaceutical', 'project management', 'performance tuning', 'regression testing', 'hosting', 'amazon redshift', 'bi', 'bim', 'sales', 'veeva', 'advanced ms excel', 'talent management', 'amazon ec2', 'spark', 'full stack', 'aws']",2025-06-12 13:58:02
Business Technology Solutions Associate Consultant,ZS,4 - 6 years,Not Disclosed,"['Pune', 'Gurugram']","Undertake primary ownership in driving self and team effort across all phases of a project lifecycle;\nTranslate business requirements into technical terms and drive team effort to design, build and manage technology solutions that solve business problems;\nApply appropriate development methodologies (eg: agile, waterfall) and best practices (eg: mid-development client reviews, embedded QA procedures, unit testing) to ensure successful and timely completion;\nPartner with Project lead/ Program lead in delivering projects and assist in project management responsibility like - project plans, people management, staffing and risk mitigation;",,,,"['Compliance', 'Data management', 'Staffing', 'MIS', 'Project management', 'Consulting', 'Financial planning', 'Data processing', 'Scheduling', 'SQL']",2025-06-12 13:58:05
Associate - Business Information Management,Axtria,5 - 10 years,Not Disclosed,['Gurugram'],"Position Summary \n\nThis is the Requisition for Employee Referrals Campaign and JD is Generic.\n\nWe are looking for Associates with 5+ years of experience in delivering solutions around Data Engineering, Big data analytics and data lakes, MDM, BI, and data visualization. Experienced to Integrate and standardize structured and unstructured data to enable faster insights using cloud technology. Enabling data-driven insights across the enterprise.\n\n Job Responsibilities \n\n\nHe/she should be able to design implement and deliver complex Data Warehousing/Data Lake, Cloud Data Management, and Data Integration project assignments.\n\nTechnical Design and Development – Expertise in any of the following skills.\n\nAny ETL tools (Informatica, Talend, Matillion, Data Stage), andhosting technologies like the AWS stack (Redshift, EC2) is mandatory.\n\nAny BI toolsamong Tablau, Qlik & Power BI and MSTR.\n\nInformatica MDM, Customer Data Management.\n\nExpert knowledge of SQL with the capability to performance tune complex SQL queries in tradition and distributed RDDMS systems is must.\n\nExperience across Python, PySpark and Unix/Linux Shell Scripting.\n\nProject Managementis\n\nmust to have. Should be able create simple to complex project plans in Microsoft Project Plan and think in advance about potential risks and mitigation plans as per project plan.\n\nTask Management – Should be able to onboard team on the project plan and delegate tasks to accomplish milestones as per plan. Should be comfortable in discussing and prioritizing work items with team members in an onshore-offshore model.\n\nHandle Client Relationship – Manage client communication and client expectations independently or with support of reporting manager. Should be able to deliver results back to the Client as per plan. Should have excellent communication skills.\n\n\n Education \n\nBachelor of Technology\nMaster's Equivalent - Engineering\n\n Work Experience \n\nOverall, 5- 7years of relevant experience inData Warehousing, Data management projects with some experience in the Pharma domain.\n\nWe are hiring for following roles across Data management tech stacks -\n\nETL toolsamong Informatica, IICS/Snowflake,Python& Matillion and other Cloud ETL.\n\nBI toolsamong Power BI and Tableau.\n\nMDM - Informatica/ Raltio, Customer Data Management.\n\nAzure cloud Developer using Data Factory and Databricks\n\nData Modeler-Modelling of data - understanding source data, creating data models for landing, integration.\n\nPython/PySpark -Spark/ PySpark Design, Development, and Deployment",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['aws stack', 'sql', 'etl tool', 'data visualization', 'sql queries', 'data management', 'amazon redshift', 'bi', 'data warehousing', 'pyspark', 'spark', 'etl', 'data lake', 'snowflake', 'python', 'big data analytics', 'datastage', 'talend', 'power bi', 'data engineering', 'tableau', 'mdm', 'aws', 'informatica', 'unix']",2025-06-12 13:58:07
OAC ODI Architect (Senior Oracle Analytics Consultant),Mastek,10 - 15 years,15-30 Lacs P.A.,"['Ahmedabad', 'Chennai']","We are looking for OAC ODI Architect to be based in Ahemdabad or Chennai\nMinimum Architect exp 4 Yrs\nOracle Analytics Consultant (OAC, ODI, FDI) Tech Architect\nLocation: [Specify Location or Remote] Chennai Ahmedabad\nExpected DOJ June\nEmployment Type: Full-time\nExperience Level: 10 - 15+ Years\nJob Summary:\nWe are seeking an experienced and results-driven Senior Oracle Analytics Consultant with over 10 years of hands-on experience in Oracle Analytics Cloud (OAC), Oracle Data Integrator (ODI), and Fusion Data Intelligence (FDI). The ideal candidate will have a deep understanding of enterprise data architecture, data integration best practices, and cloud-based analytics solutions. This role involves working closely with cross-functional teams to design, implement, and support advanced analytics and data integration solutions that drive business value.\nKey Responsibilities:\nLead the design, development, and deployment of analytics solutions using Oracle Analytics Cloud (OAC).\nArchitect and implement data integration pipelines using Oracle Data Integrator (ODI) for on-prem and cloud data sources.\nCollaborate with business and IT stakeholders to design and deploy Fusion Data Intelligence (FDI) based dashboards and KPIs.\nOptimize performance of OAC dashboards and reports, including data modeling and visualization best practices.\nDevelop and manage data models, RPDs, and semantic layers within OAC.\nBuild and maintain ETL mappings, packages, and workflows in ODI.\nIntegrate Oracle Fusion Applications with OAC and FDI for near-real-time reporting.\nDrive data governance and quality initiatives across analytics platforms.\nTroubleshoot technical issues and provide solutions in a timely manner.\nMentor junior developers and provide technical leadership on complex projects.\nQualifications:\nBachelors or Masters degree in Computer Science, Information Systems, or related field.\n10+ years of relevant experience with strong focus on:\nOracle Analytics Cloud (OAC) - Must\nOracle Data Integrator (ODI) - Must\nFusion Data Intelligence (FDI) Good to Have\nExpertise in Oracle Fusion ERP/HCM data models and subject areas.\nExperience integrating multiple data sources, including on-premise and cloud systems.\nStrong understanding of SQL, PL/SQL, and performance tuning.\nFamiliarity with data lake architecture, data warehousing, and ELT/ETL design patterns.\nProven experience working in Agile/DevOps environments.\nExcellent communication, analytical thinking, and problem-solving skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Oac', 'ODI', 'Odi Architecture', 'FDI']",2025-06-12 13:58:09
Snowflake Architect,Allegis Global Solutions (AGS),9 - 14 years,Not Disclosed,[],"Snowflake Architect\nJob Location: Hyderabad / Bangalore / Chennai / Kolkata / Noida/ Gurgaon / Pune / Indore / Mumbai\n\nJob Details\n\nTechnical Expertise:\nStrong proficiency in Snowflake architecture, including data sharing, partitioning, clustering, and materialized views.\nAdvanced experience with DBT for data transformations and workflow management.\nExpertise in Azure services, including Azure Data Factory, Azure Data Lake, Azure Synapse, and Azure Functions.\nData Engineering:\nProficiency in SQL, Python, or other relevant programming languages.\nStrong understanding of data modeling concepts, including star schema and normalization.\nHands-on experience with ETL/ELT pipelines and data integration tools.\nSoft Skills:\nExcellent problem-solving and analytical skills.\nStrong communication and stakeholder management abilities.\nAbility to work in agile teams and handle multiple priorities.\nPreferred Qualifications:\nCertifications in Snowflake, DBT, or Azure Data Engineering.\nFamiliar with data visualization tools like Power BI or Tableau.\nKnowledge of CI/CD pipelines and DevOps practices for data workflows.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'Snowflake', 'Data Build Tool', 'SQL']",2025-06-12 13:58:12
Manager - BIM,Axtria,8 - 13 years,Not Disclosed,['Pune'],"We are looking for a highly skilled and experienced Data Engineering Manager to lead our data engineering team. The ideal candidate will possess a strong technical background, strong project management abilities, and excellent client handling/stakeholder management skills. This role requires a strategic thinker who can drive the design, development and implementation of data solutions that meet our clients needs while ensuring the highest standards of quality and efficiency.\nJob Responsibilities\nTechnology Leadership Lead guide the team independently or with little support to design, implement deliver complex cloud-based data engineering / data warehousing project assignments\nSolution Architecture & Review Expertise in conceptualizing solution architecture and low-level design in a range of data engineering (Matillion, Informatica, Talend, Python, dbt, Airflow, Apache Spark, Databricks, Redshift) and cloud hosting (AWS, Azure) technologies\nManaging projects in fast paced agile ecosystem and ensuring quality deliverables within stringent timelines\nResponsible for Risk Management, maintaining the Risk documentation and mitigations plan.\nDrive continuous improvement in a Lean/Agile environment, implementing DevOps delivery approaches encompassing CI/CD, build automation and deployments.\nCommunication & Logical Thinking Demonstrates strong analytical skills, employing a systematic and logical approach to data analysis, problem-solving, and situational assessment. Capable of effectively presenting and defending team viewpoints, while securing buy-in from both technical and client stakeholders.\nHandle Client Relationship Manage client relationship and client expectations independently. Should be able to deliver results back to the Client independently. Should have excellent communication skills.\nEducation\nBE/B.Tech\nMaster of Computer Application\nWork Experience\nShould have expertise and 8+ years of working experience in at least two ETL tools among Matillion, dbt, pyspark, Informatica, and Talend\nShould have expertise and working experience in at least two databases among Databricks, Redshift, Snowflake, SQL Server, Oracle\nShould have strong Data Warehousing, Data Integration and Data Modeling fundamentals like Star Schema, Snowflake Schema, Dimension Tables and Fact Tables.\nStrong experience on SQL building blocks. Creating complex SQL queries and Procedures.\nExperience in AWS or Azure cloud and its service offerings\nAware of techniques such as: Data Modelling, Performance tuning and regression testing\nWillingness to learn and take ownership of tasks.\nExcellent written/verbal communication and problem-solving skills and\nUnderstanding and working experience on Pharma commercial data sets like IQVIA, Veeva, Symphony, Liquid Hub, Cegedim etc. would be an advantage\nHands-on in scrum methodology (Sprint planning, execution and retrospection)\nBehavioural Competencies\nTeamwork & Leadership\nMotivation to Learn and Grow\nOwnership\nCultural Fit\nTalent Management\nTechnical Competencies\nProblem Solving\nLifescience Knowledge\nCommunication\nAgile\nPySpark\nData Modelling\nDesigning technical architecture\nAWS Data Pipeline",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['BIM', 'Azure', 'Snowflake', 'Databricks', 'SQL Server', 'Oracle', 'AWS', 'Redshift']",2025-06-12 13:58:14
Manager - BIM,Axtria,5 - 10 years,Not Disclosed,['Gurugram'],"Position Summary \n\nTo be a technology expert architecting solutions and mentoring people in BI / Reporting processes with prior expertise in the Pharma domain.\n\n Job Responsibilities \n\no Technology Leadership – Lead guide the team independently or with little support to design, implement deliver complex reporting and BI project assignments. o Technical portfolio – Expertise in a range of BI and hosting technologies like the AWS stack (Redshift, EC2), Qlikview, QlikSense, Tableau, Microstrategy, Spotfire o Project Management – Get accurate briefs from the Client and translate into tasks for team members with priorities and timeline plans. Must maintain high standards of quality and thoroughness. Should be able to monitor accuracy and quality of others' work. Ability to think in advance about potential risks and mitigation plans. o Logical Thinking – Able to think analytically, use a systematic and logical approach to analyze data, problems, and situations. Must be able to guide team members in analysis. o Handle Client Relationship – Manage client relationship and client expectations independently. Should be able to deliver results back to the Client independently. Should have excellent communication skills.\n\n Education \n\nBE/B.Tech\nMaster of Computer Application\n\n Work Experience \n\n- Minimum of 5 years of relevant experience in Pharma domain.\n- Technical:\nShould have 10+ years of hands on experience in the following tools:\nMust have working knowledge of toolsAtleast 2 of the following – Qlikview, QlikSense, Tableau, Microstrategy, Spotfire/ (Informatica, SSIS, Talend & metallion)/ Big Data technologies - Hadoop ecosystem.\nAware of techniques such asUI design, Report modeling, performance tuning and regression testing\nBasic expertise with MS excel\nAdvanced expertise with SQL\n- Functional:\nShould have experience in following concepts and technologies:\nSpecifics:\nPharma data sources like IMS, Veeva, Symphony, Cegedim etc.\nBusiness processes like alignment, market definition, segmentation, sales crediting, activity metrics calculation\nCalculation of all sales, activity and managed care KPIs\n\n Behavioural Competencies \n\nTeamwork & Leadership\nMotivation to Learn and Grow\nOwnership\nCultural Fit\nTalent Management\n\n Technical Competencies \n\nProblem Solving\nLifescience Knowledge\nCommunication\nProject Management\nAttention to P&L Impact\nCapability Building / Thought Leadership\nScale of revenues managed / delivered",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['bi', 'sql', 'spotfire', 'qlikview', 'tableau', 'hive', 'pharmaceutical', 'regression testing', 'scala', 'amazon redshift', 'bim', 'big data technologies', 'spark', 'hadoop', 'big data', 'project management', 'performance tuning', 'oozie', 'talend', 'microstrategy', 'amazon ec2', 'sqoop', 'ssis', 'aws', 'informatica']",2025-06-12 13:58:17
Sap Cloud Platform Integration Consultant(lookingfor immediate joiner),Xforia Technologies,3 - 8 years,Not Disclosed,"['Pune', 'Bengaluru']","Role & responsibilities:\nProfessional Experience:\n\n10+ years of extensive experience in SAP integration technologies, with a strong focus on SAP Integration Suite.\nProven experience in architecting and implementing large-scale integration solutions.\nDeep understanding of various integration methodologies, tools, and platforms (e.g., SAP PI/PO, Ariba Managed Gateway for Spend and Network, SAP Cloud Integration for Data Services, Dell Boomi).\nProven track record in leading and mentoring technical teams.\n\nTechnical Skills:\n\nExpertise in SAP Integration Suite, including designing and implementing complex integration scenarios.\nStrong programming skills in Java, Groovy, or other relevant scripting languages.\nProficient in SOAP, REST, OData, and other web service protocols.\nExtensive experience with SAP APIs, BAPIs, IDocs, RFCs, and ALE.\nKnowledge of security and compliance standards, especially as they relate to data integration.\n\nSoft Skills:\nExceptional problem-solving and analytical skills.\nExcellent leadership and project management abilities.sibilities",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['client handling', 'end to end implementation', 'CPI']",2025-06-12 13:58:19
Global Talent Acquisition Specialist II Recruiter,Innodata India,5 - 7 years,Not Disclosed,['Noida'],"Role Overview:\nWe are looking for a dynamic and experienced Recruiter / Talent Acquisition Specialist with strong hands-on experience in global hiring across APAC, EMEA (Europe, Middle East, Africa), and the UK. This role demands someone who can handle language-based hiring as well as large-scale hiring drives in AI/data service functions like content moderation, annotation, and related roles.\nKey Responsibilities:\nManage end-to-end recruitment for global roles across APAC, EMEA, and UK regions.\nUtilize LinkedIn Recruiter and other global sourcing tools effectively.\nExperience in language-based hiring: Mandarin, French, Spanish, and other multilingual roles.\nHandle bulk hiring and specialized hiring for roles like Content Moderators, Annotators, Language Experts etc.\nPartner with business stakeholders to understand hiring needs and deliver within tight timelines.\nBuild a strong pipeline of candidates and maintain proactive candidate engagement.\nEnsure a seamless candidate experience and adherence to hiring SLAs.\nRequired Skills & Experience:\n57 years of proven experience in global recruitment.\nExcellent sourcing skills using LinkedIn Recruiter, job boards, and headhunting.\nPrior experience in language/content/annotation-based hiring is a must.\nExperience hiring for AI/Data Engineering/Content Services domains is a strong plus.\nStrong communication and stakeholder management skills.\nAbility to thrive in a fast-paced, target-driven environment.\nImmediate joiners preferred.",Industry Type: IT Services & Consulting,Department: Human Resources,"Employment Type: Full Time, Permanent","['International Hiring', 'EMEA', 'Global Talent Acquisition', 'Apac Recruitment']",2025-06-12 13:58:22
Manager - BIM,Axtria,4 - 8 years,Not Disclosed,['Noida'],"Position Summary \n\nTo be a technology expert architecting solutions and mentoring people in BI / Reporting processes with prior expertise in the Pharma domain.\n\n Job Responsibilities \nIndependently, he/she should be able to drive and deliver complex reporting and BI project assignments in PowerBI on AWS/Azure Cloud. Should be able to design and deliver across Power BI services, Power Query, DAX, and data modelling concepts. Should be able to write complex SQLs focusing on Data Aggregation and analytic calculations used in the reporting KPIs.\nBe able to analyse the data and understand the requirements directly from customer or from project teams across pharma commercial data sets\nShould be able to drive the team on the day-to-day tasks in alignment with the project plan and collaborate with team to accomplish milestones as per plan. Should be comfortable in discussing and prioritizing work items in an onshore-offshore model.\nAble to think analytically, use a systematic and logical approach to analyse data, problems, and situations.\nManage client communication and client expectations independently. Should be able to deliver results back to the Client as per plan. Should have excellent communication skills .\n\n\n Education \n\nBE/B.Tech\nMaster of Computer Application\n\n Work Experience \nShould have 4-8 years of working on experience in developing Power BI reports. Must have proficiency in Power BI services, Power Query, DAX, and data modelling concepts.\nShould have experience in design techniques such as UI designing and creating mock-ups/intuitive visualizations seamless user experience.\nShould have expertise in writing complex SQLs focusing on Data Aggregation and analytic calculations used for deriving the reporting KPIs.\nStrong understanding of data integration, ETL processes, data warehousing , preferably on AWS Redshift and/or Snowflake.\nExcellent problem-solving skills with the ability to troubleshoot and resolve technical issues.\nStrong communication and interpersonal skills, with the ability to collaborate effectively with cross-functional teams.\nGood to have experience in the Pharma Commercial data sets and related KPIs for Sale Performance, Managed Market, Customer 360, Patient Journey etc.\nGood to have experience and additional know-how on other reporting tools.\n\n\n Behavioural Competencies \n\nTeamwork & Leadership\nMotivation to Learn and Grow\nOwnership\nCultural Fit\nTalent Management\n\n Technical Competencies \n\nProblem Solving\nLifescience Knowledge\nCommunication\nCapability Building / Thought Leadership\nPower BI\nSQL\nBusiness Intelligence(BI)\nSnowflake",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data warehousing', 'dax', 'power bi', 'power query', 'etl process', 'amazon redshift', 'bim', 'sql', 'sqls', 'java', 'data modeling', 'ssrs', 'asp.net', 'etl', 'c#', 'snowflake', 'power bi reports', 'ssas', 'user interface designing', 'vb', 'azure cloud', 'sql server', 'data crunching', 'aws', 'ssis', 'data integration']",2025-06-12 13:58:24
Senior ServiceNow Developer,Luxoft,6 - 11 years,Not Disclosed,['Bengaluru'],"Internal Data Structures & Configuration\nDesign, build, and maintain data models, tables, and relationships within the ServiceNow platform.\nExtend and customize out-of-the-box modules (e.g., CMDB, Incident, Change, Request, etc.) to meet business requirements.\nEnsure data integrity, normalization, and performance optimization across the ServiceNow environment.\nCollaborate with stakeholders to translate business requirements into scalable ServiceNow configurations or custom applications.\nReporting & Dashboards\nDevelop real-time dashboards and reports using ServiceNow Reporting Tools and Performance Analytics.\nDeliver insights into key ITSM metrics such as SLAs, incident trends, and operational KPIs.\nAutomate the generation and distribution of recurring reports to stakeholders.\nWork with business and technical teams to define and implement reporting frameworks tailored to their needs.\nAutomated Feeds & API Integration\nDevelop and manage robust data integrations using ServiceNow REST/SOAP APIs.\nBuild and maintain data pipelines to and from external systems (e.g., CMDB, HRIS, ERP, Flexera, etc.).\nImplement secure, scalable automation for data exchange with appropriate error handling, logging, and monitoring.\nTroubleshoot and resolve integration-related issues to ensure smooth system interoperability.\nSkills\nMust have\nMinimum 6+ years of hands-on experience with ServiceNow, including ITSM, CMDB, and integrations.\nTechnical Expertise:\nAdvanced knowledge of ServiceNow architecture, configuration, and scripting (JavaScript, Glide).\nStrong experience with REST/SOAP APIs for ServiceNow integrations.\nSolid understanding of relational databases, data normalization, and model optimization.\nFamiliarity with common enterprise systems such as ERP, HRIS, Flexera, and CMDB tools.\nReporting Skills:\nProficiency in ServiceNow Performance Analytics, standard reporting, and dashboard design.\nExperience defining KPIs and building automated reporting solutions.\nSoft Skills:\nStrong communication and collaboration skills.\nProven ability to translate business requirements into scalable ServiceNow solutions.\nAnalytical and detail-oriented mindset with a problem-solving approach.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ERP', 'Automation', 'SAP', 'HRIS', 'Analytical', 'Javascript', 'Data structures', 'Risk management', 'Analytics', 'Monitoring']",2025-06-12 13:58:26
Hadoop with Scala Developer,Allime Tech Solutions,8 - 12 years,Not Disclosed,['Bengaluru'],"Allime Tech Solutions is looking for Hadoop with Scala Developer to join our dynamic team and embark on a rewarding career journey.\nA Developer is responsible for designing, developing, and maintaining software applications and systems\nThey collaborate with a team of software developers, designers, and stakeholders to create software solutions that meet the needs of the business\nKey responsibilities:Design, code, test, and debug software applications and systemsCollaborate with cross-functional teams to identify and resolve software issuesWrite clean, efficient, and well-documented codeStay current with emerging technologies and industry trendsParticipate in code reviews to ensure code quality and adherence to coding standardsParticipate in the full software development life cycle, from requirement gathering to deploymentProvide technical support and troubleshooting for production issues\nRequirements:Strong programming skills in one or more programming languages, such as Python, Java, C++, or JavaScriptExperience with software development tools, such as version control systems (e\ng\nGit), integrated development environments (IDEs), and debugging toolsFamiliarity with software design patterns and best practicesGood communication and collaboration skills",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'c++', 'python', 'software testing', 'scala', 'software design', 'version control', 'hadoop development', 'apache pig', 'sql', 'coding', 'git', 'java', 'spark', 'design patterns', 'software solutions', 'debugging', 'troubleshooting', 'code review', 'hadoop', 'sqoop', 'programming', 'communication skills']",2025-06-12 13:58:29
o9 SSIS integration Consultant,Thoucentric,3 - 8 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","At Thoucentric, we work on various problem statements.\nThe most popular ones are -\nBuilding capabilities that address a market need, basis our ongoing research efforts\nSolving a specific use case for a current or potential client based on challenges on-ground\nDeveloping new systems that help be a better employer and a better partner to clients",,,,"['O9', 'SSIS', 'SQL']",2025-06-12 13:58:31
.Net Fullstack Developer,S&P Global Market Intelligence,3 - 8 years,Not Disclosed,['Hyderabad'],"The Team:\nOur team is responsible for the design, architecture, and development of our client facing applications using MI Platform and Office Add-Ins that are regularly updated as new technologies emerge. You will have the opportunity every day to work with people from a wide variety of backgrounds and will be able to develop a close team dynamic with coworkers from around the globe.\nThe Impact:\nAs a member of the Team, you will be responsible for analysis, design, architecture, development, and support several applications.The ideal candidate should have expertise with cutting edge technologies and a desire to drive change through all alignment across the enterprise. The role requires the candidate to be a hands-on problem solver and developer helping to extend and manage the applications. The work you do will be used every single day, its the essential code youll write that provides the data and analytics required for crucial, daily decisions in the capital and commodities markets.\nWhats in it for you:\nBuild a career with a global company\nExposure to work on Latest cutting-edge Technologies and Grow and improve your skills by working on enterprise level products.\nOpportunity to grow personally and professionally.\nWork on code that fuels the global financial markets.\nResponsibilities:\nDemonstrate a strong sense of ownership and responsibility with release goals. This includes understanding requirements, technical specifications, design, architecture, implementation, unit testing, builds/deployments, and code management.\nEnsure compliance through the adoption of enterprise standards and promotion of best practice guiding principles aligned with organization standards.\nHands-on position requiring strong analytical, architecture, development and debugging skills that includes both development and operations.\nAttaint in-depth Functional knowledge of the domain that we are working on.\nUnderstand Incident Management, Problem Management and able to do root cause analysis.\nEnsure data governance principles adopted, data quality checks and data lineage implemented in each hop of the Data.\nProvide technical guidance and mentoring to the team and help them adopt change as new processes are introduced\nChampion best practices and serve as a subject matter authority\nDevelop solutions to develop/support key business needs\nEngineer components and common services based on standard development models, languages and tools\nProduce system design documents and lead technical walkthroughs\nRegularly evaluate cloud applications, hardware, and software.\nProduce high quality code\nCollaborate effectively with technical and non-technical partners\nAs a team-member should continuously improve the architecture\nWhat We Are Looking For:\nBasic Qualifications\nBachelor'smasters degree in computer science, Information Systems or equivalent.\n3-10 years of experience in application development using Microsoft Technologies.\nKnowledge of object-oriented design, .NET framework and design patterns.\nCommand of essential technologies: HTML, Single Page Application (SPA) frameworks, JavaScript Frameworks (jQuery, KnockoutJS, TypeScript, Durandal, React JS, Require), C#, .NET Framework, CSS, LESS, SQL Server, Web API, Web services\nGood to have: ASP.Net\nExperience with developing solutions involving relational database technologies on SQL Server platform, stored procedure programming experience using Transact SQL.\nExperience deploying data engineering solutions in public clouds like AWS, GCP, or Azure, leveraging cloud power to its fullest.\nProficient with software development lifecycle (SDLC) methodologies like Agile, Test- driven development.\nExperience working with any relational Databases preferably SQL Server.\nExperience in continuous delivery through CI/CD pipelines, containers, and orchestration technologies.\nExperience working with cross functional teams, with strong interpersonal and written communication skills.\nCandidate must have the desire and ability to quickly understand and work within new technologies.\nGood communication and collaboration skills and Ability to work as team player, train and mentor when needed.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['.Net', 'C#', 'CSS', 'Web API', 'KnockoutJS', 'HTML', 'SQL Server', '.NET Framework', 'React JS', 'Single Page Application', 'TypeScript', 'software development lifecycle', 'jQuery', 'Web services', 'ASP.Net', 'Durandal', 'LESS']",2025-06-12 13:58:33
"Senior Backend Developer (Java, Spring Boot)",Apex One,6 - 11 years,Not Disclosed,['Bengaluru'],"Senior Backend Developer (Java, Spring Boot, object databases, Elasticsearch)\nRequirements:\n6+ years of experience\nProficiency in Java, Spring Boot, object databases, ElasticSearch/Solr,\nPractice in using AWS cloud, Docker & Kubernetes, REST APIs\nExperience in building scalable and high-performance systems\nStrong communication skills in English (B2+)\nNice-to-have: Knowledge of Python, ETL experience, and big data solutions\nResponsibilities:\nMaintenance of a large modern search platform\nHandling production issues and incidents\nOptimizing and maintaining existing code for performance and availability\nEnsuring high performance and availability of the system\nEngage in the Release Process\nTeam Information:\nWork within a SAFe, scrum / kanban methodology, and agile approach\nCollaborative and friendly atmosphere\nUtilization of microservice architecture and extensive CI/CD automation\nTools used: git, IntelliJ, Jira, Confluence, i3 by Tieto as search backend",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Backend Development', 'Java', 'Kanban', 'Elasticsearch', 'object databases', 'CI/CD', 'scrum', 'Spring Boot']",2025-06-12 13:58:36
"Senior Specialist, BA/DA",XL India Business Services Pvt. Ltd,2 - 7 years,Not Disclosed,['Gurugram'],"Senior Specialist, BA/DA Gurgaon/Bangalore, India AXA XL recognizes data and information as critical business assets, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained dynamic advantage\n\nOur Innovation, Data & Analytics function is focused on driving innovation through optimizing how we leverage data to drive strategy and differentiate ourselves from the competition\n\nAs we develop an enterprise-wide data and digital strategy that moves us toward greater focus on the use of data and data-driven insights, we are seeking a Senior Specialist for our Data Sourcing & Solutions team\n\nThe role sits across the Innovation, Data & Analytics Department to ensure customer requirements are properly captured and transformed into actionable data specifications\n\nSuccess in the role will require focus on proactive management of the sourcing and management of data from source through usage\n\nWhat you ll be DOING What will your essential responsibilities include? Identify, evaluate, and acquire various data sources that align with the customer needs\n\nThis may involve collaborating with business stakeholders, third party vendors and source system teams\n\nDesign and implement data integration strategies to combine diverse datasets from internal and external sources\n\nThis would include accountable for documenting data requirements to the ETL processes, APIs, and data pipelines\n\nDevelop data solutions to address specific business challenges\n\nThis might involve creating custom data models that provide actionable insights which integrate with the existing data assets\n\nOversee the organization and management of data within databases, ensuring data security, integrity, and accessibility\n\nAble to work in Agile framework by defining and prioritizing the product backlog, collaborating with agile teams to deliver the business goals and customer needs\n\nWork closely with cross-functional teams, including Data Engineers, Data Science, Data Management, Data Governance, Data Quality, BI Solutions and Stakeholders\n\nImplement measures to maintain data accuracy, consistency, and completeness\n\nPerform data validation and cleansing as needed\n\nAdhere to data governance standards, ensuring compliance with regulations and internal policies related to data usage and privacy\n\nProficiency in various data technologies such as SQL, Azure cloud technologies, Databricks to analyze and produce data insights\n\nStay updated with emerging technologies in data management\n\nDeveloping expertise in the Insurance domain to better understand the context of the data\n\nIdentify data related issues, troubleshoot problems, and recommend solutions to enhance data sourcing and integration processes\n\nProvide guidance and mentorship to junior analysts and team members, fostering a culture of continuous learning and improvement\n\nTranslate complex technical concepts into understandable insights for non-technical stakeholders to drive data-informed decision-making\n\nExplore innovative approaches to data acquisition, integration, and solution development that can lead to improved efficiency and effectiveness\n\nIt is pivotal in ensuring that an organization s data ecosystem is robust, well-integrated, and capable of providing accurate and actionable insights to become a data driven organization\n\nInstill a customer-first attitude, prioritizing service for our business stakeholders above all else\n\nYou will report to Lead Specialist, Data Sourcing & Solutions\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Extensive experience in a data role (business analyst, data analyst, analytics) preferably in the Insurance industry and within a data division\n\nExcellent presentation, communication (oral & written), and relationship building skills, across all levels of management and customer interaction\n\nExcellent SQL knowledge, exposure to Azure cloud technologies (including Databricks) and technical ability to query AXA XL data sources to understand our data\n\nDeep insurance experience in data, underwriting, claims and/or operations, including influencing, collaborating, and leading efforts in complex, disparate and inter-related teams with competing priorities\n\nPassion for data and experience working within a data driven organization\n\nIntegrate internal data with external industry data to deliver holistic solutions\n\nWork with unstructured data to unlock information needed by the business to create unique products for the insurance industry\n\nPossesses excellent exploratory analysis skills and high intellectual curiosity\n\nDisplays exceptional organizational skills and is detail oriented\n\nEffective conceptual thinker who connects dots, critical thinking, and analytical skills\n\nAbility to take ownership, work under pressure, and meet deadlines\n\nAbility to work with team members across the globe and across departments\n\nDesired Skills and Abilities: Builds trust and rapport within and across groups\n\nApplies in-depth knowledge of business and specialized areas to solve business problems and understand integration challenges and long-term impact creatively and strategically\n\nAbility to manage data needs of an individual project(s) while being able to understand the broader enterprise data perspective\n\nExpected to recommend innovation and improvement to policies, procedures, deploying resources and performing core activities",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data validation', 'Claims', 'Data management', 'data security', 'Underwriting', 'Agile', 'data governance', 'Data quality', 'Business strategy', 'Customer interaction']",2025-06-12 13:58:38
"Digital Analytics Manager, BI Solution Design & Transformation",XL India Business Services Pvt. Ltd,6 - 11 years,Not Disclosed,['Gurugram'],"Senior Specialist - BI answer Design & Transformation Bengaluru (Bangalore) India or Gurgaon India AXA XL recognizes digital, data and information assets are critical for the business, both in terms of managing risk and enabling new business opportunities\n\nData and Insights should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and achieve sustained dynamic advantage\n\nOur Innovation, Data & Analytics (IDA) function is focused on driving innovation by optimizing how we leverage digital, data and AI to drive strategy and differentiate ourselves from the competition\n\nAs we develop an enterprise-wide data and digital strategy that moves us toward greater focus on the use of data and strengthen our capabilities, we are seeking a Senior Specialist to join our BI & Reporting function, as part of our BI Solution Design & Transformation team\n\nIn this role, you will be a key technical BI & Reporting lead for all our global Power BI (inc\n\nPower Apps, Power Automate etc) solutions, and will lead efforts to enhance, transform and streamline the BI platform landscape\n\nYou will be an ambassador of new and existing BI solutions across all regions and must be comfortable proactively engaging with business users\n\nThis will enable the organization to gain necessary, timely insights to drive business decisions, build our dynamic advantage and help differentiate in the market through easy-to-use, scalable, cost effective, secure, and high-performance BI platforms\n\nIn this role, you will report to the Division Lead, BI Solution Design & Transformation, based in London\n\nYou will work within a global team in your day-to-day work with teams and business users around the world\n\nWhat you ll be doing What will your essential responsibilities include? Be a technical lead and go to expert in the team on the development of new BI Reporting solutions globally, and make sure new products are fit for customer purpose, built to best practice standards, and efficiently utilize company resources and data assets\n\nBe able to work directly with customers in the business, to understand requirements, design new solutions and troubleshoot issues, as well as prioritize work based upon a deep customer understanding\n\nBe able to advise the wider team and bring industry best practice techniques to make sure our BI platforms are well-governed, maintained, remain secure and optimized for our customers\n\nThis includes AI in BI and related design, testing, end to end roll out and ongoing continuous improvement\n\nEnable business to self-serve and meet BI demands through provision and ongoing management of appropriate tools, coupled with leading relevant training for the business\n\nProactively partner with key areas within Global Technology, Transformation & Change Delivery, Group and other teams for Projects and for Business-as-Usual deliverables\n\nInstill a customer-first culture, prioritizing service for our business stakeholders above all else\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Very high technical proficiency in use, development and solution design in Microsoft Power BI (inc\n\nPower Apps, Power Automate etc), with experience of usage in large, global, and complicated organization\n\nBe an expert on data modelling best practices, data engineering and data management (inc\n\nSQL)\n\nAble to articulate these to both technical and non-technical users\n\nAbility to communicate effectively directly with users, peers, senior management and teams across the globe, manage stakeholders effectively and navigate a matrixed virtual global organization\n\nBrings in a collaborative spirit, can-do attitude and a Customer First mindset to everything they put their mind to\n\nPassion for digital, data and AI and experience working within a digital and data driven organization\n\nA minimum of a bachelor s or masters degree in a relevant discipline\n\nApplies in-depth knowledge of business and specialized areas to solve problems and understand integration challenges and long-term impact creatively and strategically\n\nIs a self-starter who can operate independently and lead others in the team in strategic thinking and solution design\n\nDesired Skills and Abilities: Use and development skills in AI in BI; Power BI Copilot, connections with LLMs/models\n\nDatabricks and Databricks Genie ideal\n\nOther programing language proficiency, e g R, Python PowerShell etc Ability to operate and thrive in an agile team working environment (inc\n\nuse of Jira and other workflow tools)\n\nInsurance experience with both financial and non-financial metric understanding\n\nWho WE are AXA XL, the P&C and specialty risk division of AXA, is known for solving complex risks\n\nFor mid-sized companies, multinationals and even some inspirational individuals we don t just provide re/insurance, we reinvent it\n\nHow? By combining a comprehensive and efficient capital platform, data-driven insights, leading technology, and the best talent in an agile and inclusive workspace, empowered to deliver top client service across all our lines of business property, casualty, professional, financial lines and specialty\n\nWith an innovative and flexible approach to risk solutions, we partner with those who move the world forward\n\nLearn more at axaxl\n\ncom What we OFFER Inclusion AXA XL is committed to equal employment opportunity and will consider applicants regardless of gender, sexual orientation, age, ethnicity and origins, marital status, religion, disability, or any other protected characteristic\n\nAt AXA XL, we know that an inclusive culture and a diverse workforce enable business growth and are critical to our success\n\nThat s why we have made a strategic commitment to attract, develop, advance and retain the most diverse workforce possible, and create an inclusive culture where everyone can bring their full selves to work and can reach their highest potential\n\nIt s about helping one another and our business to move forward and succeed\n\nFive Business Resource Groups focused on gender, LGBTQ+, ethnicity and origins, disability and inclusion with 20 Chapters around the globe Robust support for Flexible Working Arrangements Enhanced family friendly leave benefits Named to the Diversity Best Practices Index Signatory to the UK Women in Finance Charter Learn more at axaxl\n\ncom / about-us / inclusion-and-diversity\n\nAXA XL is an Equal Opportunity Employer\n\nTotal Rewards AXA XL s Reward program is designed to take care of what matters most to you, covering the full picture of your health, wellbeing, lifestyle and financial security\n\nIt provides dynamic compensation and personalized, inclusive benefits that evolve as you do\n\nWe re committed to rewarding your contribution for the long term, so you can be your best self today and look forward to the future with confidence\n\nSustainability At AXA XL, Sustainability is integral to our business strategy\n\nIn an ever-changing world, AXA XL protects what matters most for our clients and communities\n\nWe know that sustainability is at the root of a more resilient future\n\nOur 2023-26 Sustainability strategy, called Roots of resilience, focuses on protecting natural ecosystems, addressing climate change, and embedding sustainable practices across our operations\n\nOur Pillars: Valuing nature: How we impact nature affects how nature impacts us\n\nResilient ecosystems - the foundation of a sustainable planet and society - are essential to our future\n\nWe re committed to protecting and restoring nature - from mangrove forests to the bees in our backyard - by increasing biodiversity awareness and inspiring clients and colleagues to put nature at the heart of their plans\n\nAddressing climate change: The effects of a changing climate are far reaching and significant\n\nUnpredictable weather, increasing temperatures, and rising sea levels cause both social inequalities and environmental disruption\n\nWere building a net zero strategy, developing insurance products and services, and mobilizing to advance thought leadership and investment in societal-led solutions\n\nIntegrating ESG: All companies have a role to play in building a more resilient future\n\nIncorporating ESG considerations into our internal processes and practices builds resilience from the roots of our business\n\nWe re training our colleagues, engaging our external partners, and evolving our sustainability governance and reporting\n\nAXA Hearts in Action: We have established volunteering and charitable giving programs to help colleagues support causes that matter most to them, known as AXA XL s Hearts in Action programs\n\nThese include our Matching Gifts program, Volunteering Leave, and our annual volunteering day - the Global Day of Giving\n\nFor more information, please see Sustainability at AXA XL\n\nThe U\n\nS\n\npay range for this position is USD 106,500 - 186,500\n\nActual pay will be determined based upon the individual s skills, experience, and location\n\nWe strive for market alignment and internal equity with our colleagues pay\n\nAt AXA XL, we know how important physical, mental, and financial health are to our employees, which is why we are proud to offer benefits such as a dynamic retirement savings plan, health and wellness programs, and many other benefits\n\nWe also believe in fostering our colleagues development and offer a wide range of learning opportunities for colleagues to hone their professional skills and to position themselves for the next step of their careers\n\nFor more details about AXA XL s benefits offerings, please visit US Benefits at a Glance 2025",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data management', 'Powershell', 'Agile', 'Workflow', 'Business strategy', 'microsoft', 'JIRA', 'Continuous improvement', 'SQL', 'Python']",2025-06-12 13:58:40
Senior Power BI Developer,Luxoft,6 - 11 years,Not Disclosed,['Gurugram'],"Power BI Dashboard Development (UI Dashboards)\nDesign, develop, and maintain visually compelling, interactive Power BI dashboards aligned with business needs.\nCollaborate with business stakeholders to gather requirements, develop mockups, and refine dashboard UX.\nImplement advanced Power BI features like bookmarks, drill-throughs, dynamic tooltips, and DAX calculations.\nConduct regular UX/UI audits and performance tuning on reports.\nData Modeling in SQL Server & Dataverse\nBuild and manage scalable, efficient data models in Power BI, Dataverse, and SQL Server.\nApply best practices in dimensional modeling (star/snowflake schema) to support analytical use cases.\nEnsure data consistency, accuracy, and alignment across multiple sources and business areas.\nPerform optimization of models and queries for performance and load times.\nPower BI Dataflows & ETL Pipelines\nDevelop and maintain reusable Power BI Dataflows for centralized data transformations.\nCreate ETL processes using Power Query, integrating data from diverse sources including SQL Server, Excel, APIs, and Dataverse.\nAutomate data refresh schedules and monitor dependencies across datasets and reports.\nEnsure efficient data pipeline architecture for reuse, scalability, and maintenance.\nSkills\nMust have\nExperience: 6+ years in Business Intelligence or Data Analytics with a strong focus on Power BI and SQL Server.\nTechnical Skills:\nExpert-level Power BI development, including DAX, custom visuals, and report optimization.\nStrong knowledge of SQL (T-SQL) and relational database design.\nExperience with Dataverse and Power Platform integration.\nProficiency in Power Query, Dataflows, and ETL development.\nModeling: Proven experience in dimensional modeling, star/snowflake schema, and performance tuning.\nData Integration: Skilled in connecting and transforming data from various sources, including APIs, Excel, and cloud data services.\nCollaboration: Ability to work with stakeholders to define KPIs, business logic, and dashboard UX.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'data services', 'Excel', 'Data modeling', 'Database design', 'Analytical', 'Schema', 'power bi', 'Business intelligence', 'microsoft']",2025-06-12 13:58:42
Senior Research Analyst,Demandbase,8 - 10 years,Not Disclosed,['Hyderabad'],"Introduction to Demandbase:\nDemandbase is the Smarter GTM company for B2B brands. We help marketing and sales teams overcome the disruptive data and technology fragmentation that inhibits insight and forces them to spam their prospects. We do this by injecting Account Intelligence into every step of the buyer journey, wherever our clients interact with customers, and by helping them orchestrate every action across systems and channels - through advertising, account-based experience, and sales motions. The result? You spot opportunities earlier, engage with them more intelligently, and close deals faster.\nAs a company, we re as committed to growing careers as we are to building world-class technology. We invest heavily in people, our culture, and the community around us. We have offices in the San Francisco Bay Area, New York, Seattle, and teams in the UK and India . We have also been continuously recognized as one of the best places to work in the San Francisco Bay Area.\nWere committed to attracting, developing, retaining, and promoting a diverse workforce. By ensuring that every Demandbase employee is able to bring a diversity of talents to work, were increasingly capable of living out our mission to transform how B2B goes to market. We encourage people from historically underrepresented backgrounds and all walks of life to apply. Come grow with us at Demandbase!\nAbout the Role:\nWe are seeking a highly motivated and detail-oriented Senior Research Analyst to join our dynamic team. This role is crucial for driving informed business decisions through data gathering, analysis, and insightful reporting. The ideal candidate will possess a strong understanding of business research methodologies, data analysis techniques, and a passion for data accuracy and problem-solving.\nKey Responsibilities:\nLead Comprehensive Data Research and Analysis: Source, collect, research, and analyze data from a variety of business information sources and specialized databases to generate actionable insights.\nDrive Data-Driven Decision-Making: Conduct in-depth strategic analysis to identify trends, anomalies, and root causes, translating complex findings into clear, impactful recommendations for product and business growth.\nEnsure Data Quality and Integrity: Apply strong problem-solving skills to resolve data queries, perform rigorous quality checks, and proactively identify and address data coverage gaps.\nProvide Training and Knowledge Transfer: Mentor and train new team members on industry best practices and advanced data analysis techniques.\nLeverage Domain and Product Expertise: Work closely with data engineers, product teams, and business stakeholders to define and deliver technical roadmaps, ensuring sound solutions and maximizing customer value.\nRequired Skills & Experience:\nBachelor s or Master s degree in Business or Commerce\n8-10 years of relevant work experience\nExpertise in sourcing and extracting data from diverse business information sources\nAdvanced proficiency in Microsoft Excel (e.g., pivot tables, VLOOKUP, complex formulas, data validation, charting) for data manipulation, analysis, and reporting\nSkill in translating complex data into visually compelling narratives for various audiences\nAbility to design and create clear, insightful, and actionable dashboards and reports\nExcellent communication and interpersonal skills\nSelf-organized and self-driven, with strong personal integrity\nStrong understanding and application of data quality principles and best practices\nAbility to perform root cause analysis on large datasets and identify underlying business drivers\nProven ability to train and mentor new team members, sharing best practices and advanced techniques and strong knowledge transfer skills.\nA strong passion for data, continuous learning, and staying updated with industry best practices and emerging analytical techniques.\nStrong organizational and time management skills\nAbility to work independently, manage multiple priorities, and meet deadlines in a fast-paced environment.\nOur Commitment to Diversity, Equity, and Inclusion at Demandbase\nAt Demandbase, we believe in creating a workplace culture that values and celebrates diversity in all its forms. We recognize that everyone brings unique experiences, perspectives, and identities to the table, and we are committed to building a community where everyone feels valued, respected, and supported. Discrimination of any kind is not tolerated, and we strive to ensure that every individual has an equal opportunity to succeed and grow, regardless of their gender identity, sexual orientation, disability, race, ethnicity, background, marital status, genetic information, education level, veteran status, national origin, or any other protected status. We do not automatically disqualify applicants with criminal records and will consider each applicant on a case-by-case basis.\nWe recognize that not all candidates will have every skill or qualification listed in this job description. If you feel you have the level of experience to be successful in the role, we encourage you to apply!\nWe acknowledge that true diversity and inclusion require ongoing effort, and we are committed to doing the work required to make our workplace a safe and equitable space for all. Join us in building a community where we can learn from each other, celebrate our differences, and work together.\nPersonal information that you submit will be used by Demandbase for recruiting and other business purposes. Our Privacy Policy explains how we collect and use personal information.\nPersonal information that you submit will be used by Demandbase for recruiting and other business purposes. Our Privacy Policy explains how we collect and use personal information.",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SAN', 'Data analysis', 'Data validation', 'Excel', 'Data research', 'Business research', 'VLOOKUP', 'Analytical', 'Data quality', 'Senior Research Analyst']",2025-06-12 13:58:45
AI Delivery Lead,Tata Technologies,10 - 15 years,Not Disclosed,['Pune'],"10+ years of experience in machine learning, AI, or data science, with at least 3 years in a leadership role.Proven track record of delivering ML/AI projects at scale in an enterprise environment.Deep functional expertise in AI/ML, coupled with a solid understan ding of data science solution developmentExperience in managing teams and stakeholder expectations.Strong communication skills and demonstrated experience in stakeholder managementTechnical SkillsStrong expertise in ML frameworks and tools (e.g., TensorFlow, P yTorch, Scikit-learn).Good experience in Gen AI (various LLMs and its framework)Proficiency in programming languages like Python, R, or Java.Familiarity with cloud platforms (AWS, Azure, GCP) for ML workflows.Solid understanding of MLOps principles, including CI/CD pipelines, model deployment, and monitoring.Knowledge of big data technologies (e.g., Spark, Hadoop) and data engineering bes t practices.Preferred Certifications AWS/Azure/GCP Certified ML",Industry Type: Building Material (Cement),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['CI/CD pipelines', 'Java', 'Azure', 'R', 'GCP', 'Hadoop', 'Spark', 'AWS', 'Python']",2025-06-12 13:58:47
Senior Power BI Developer,Luxoft,6 - 11 years,Not Disclosed,['Chennai'],"Power BI Dashboard Development (UI Dashboards)\nDesign, develop, and maintain visually compelling, interactive Power BI dashboards aligned with business needs.\nCollaborate with business stakeholders to gather requirements, develop mockups, and refine dashboard UX.\nImplement advanced Power BI features like bookmarks, drill-throughs, dynamic tooltips, and DAX calculations.\nConduct regular UX/UI audits and performance tuning on reports.\nData Modeling in SQL Server & Dataverse\nBuild and manage scalable, efficient data models in Power BI, Dataverse, and SQL Server.\nApply best practices in dimensional modeling (star/snowflake schema) to support analytical use cases.\nEnsure data consistency, accuracy, and alignment across multiple sources and business areas.\nPerform optimization of models and queries for performance and load times.\nPower BI Dataflows & ETL Pipelines\nDevelop and maintain reusable Power BI Dataflows for centralized data transformations.\nCreate ETL processes using Power Query, integrating data from diverse sources including SQL Server, Excel, APIs, and Dataverse.\nAutomate data refresh schedules and monitor dependencies across datasets and reports.\nEnsure efficient data pipeline architecture for reuse, scalability, and maintenance.\nSkills\nMust have\nExperience: 6+ years in Business Intelligence or Data Analytics with a strong focus on Power BI and SQL Server.\nTechnical Skills:\nExpert-level Power BI development, including DAX, custom visuals, and report optimization.\nStrong knowledge of SQL (T-SQL) and relational database design.\nExperience with Dataverse and Power Platform integration.\nProficiency in Power Query, Dataflows, and ETL development.\nModeling: Proven experience in dimensional modeling, star/snowflake schema, and performance tuning.\nData Integration: Skilled in connecting and transforming data from various sources, including APIs, Excel, and cloud data services.\nCollaboration: Ability to work with stakeholders to define KPIs, business logic, and dashboard UX.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'data services', 'Data modeling', 'Database design', 'Analytical', 'Schema', 'power bi', 'SSIS', 'Business intelligence', 'microsoft']",2025-06-12 13:58:49
Product manager (Solar),Renew,4 - 7 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Responsibilities\nUnderstand Business problems and identify constraints\nDesign digital and advance analytics solutions to Business problems\nImplement the solution with an understanding of end-to-end architecture\nIdentify opportunities for implementation of new use cases\nKeep updates of any policy changes in power markets\nEnsure ReD targets are met and delivered on time\nEnsure documentation of Use Cases\nOur Ideal Candidate\nEducation - Engineering (Electrical/Electronics/IT) + MBA\nExperience Range - 4 to 7 years\nExperience in Renewable energy/ Storage/Hydro/RTC power/Power Trading\nGood program management, Project planning & coordination skills\nGood Experience of working in cross functional teams\nGood IT skills - understanding different solutions and matching solutions to problems\nAnalytical approach to solving problems with focus on solution delivery\nCapable of extrapolating current situation to future scenarios\nFunctional/ Domain expertise\nKnowledge of Power markets is a must\nShould have experience in evaluating or implementing any of the new technologies (BESS/ Hybrids/ EV, Charging Infra/ Pumped Storage/ Hydrogen/Market procurement of RE - GTAM)\nParticipated in some Digital transformation/enablement exercise in organisation\nBasic understanding of work of Data Scientists & Data engineering roles is a plus\nExperience in agile working methodology would be a plus\nExperience with tools like PowerBI, Tableau, JIRA would be a plus\nCommunication skills\nAbility to communicate with cross functional roles is a must\nExcellent written and verbal communication skills\nGood presentation skills\nTeamwork\nAbility to work as a self-motivated team player\nHas worked in large teams with an agile setup in the past\nHandle multiple projects across intra and inter-department teams",Industry Type: Power,Department: Product Management,"Employment Type: Full Time, Permanent","['Procurement', 'RTC', 'Renewable energy', 'Analytical', 'Agile', 'Project planning', 'JIRA', 'digital transformation', 'Analytics', 'Hydro']",2025-06-12 13:58:52
Technical lead - Salesforce,XL India Business Services Pvt. Ltd,4 - 9 years,Not Disclosed,"['Hyderabad', 'Ahmedabad', 'Bengaluru']","Technical Architect-AVP Bangalore, Karnataka, India We are looking for an experienced Technical Architect to lead the design and implementation of complex software solutions\n\nThe ideal candidate will possess a deep understanding of software architecture principles, technology stacks, and development methodologies\n\nYou will collaborate with cross-functional teams to ensure that our technology solutions are scalable, secure, and aligned with business objectives\n\nWhat you ll be DOING What will your essential responsibilities include? Responsible for the design and technical delivery of the Salesforce Underwriter Journey\n\nCollaborate with business analysts and stakeholders to gather requirements and translate them into technical specifications\n\nProducing quality, secure, scalable, high-performing, and resilient designs for new or improved services\n\nLead the systems analysts, developers, and testers in sympathetic change to the applications\n\nResponsible for partnering with engineers, DevOps engineers, Administrators, and other roles responsible for implementing solutions on Azure to ensure sound infrastructure solution options are leveraged Accountable for leading technical project delivery within the applications landscape\n\nResponsible for handling multiple tech initiatives in Agile delivery models\n\nActively lead the development teams to help assist PI planning and prioritization\n\nFor internal assets, support Product Owners to develop and maintain the Product Roadmap\n\nDefine and maintain development standards such as system and data design, coding, etc Maintain a capacity plan with historical performance metrics, a future forecast, and a capacity model to ensure services and infrastructure deliver performance and growth targets in a cost effective and proactive manner\n\nManage architecture exceptions for the application, including identifying, documenting, taking through exception approval process, and remediation where and when possible\n\nMonitor application services to ensure performance consistently meets non-functional requirements (response time, security, etc)\n\nLeads the DevOps team and developers in targeted use of DevOps for their application platform assets\n\nYou will report to the Release Train Engineer\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Developing and maintaining custom data integration solutions with various sources and formats, including structured and unstructured data, to ensure data quality and consistency\n\nEnsures that the solution and codebase is maintainable, scalable and adheres to best practices of software development\n\nDelivering high-quality, scalable, and reliable data ingestion pipelines\n\nSupports other members of the squad in resolving technical questions related to best practice, feasibility etc Desired Skills and Abilities: Insurance background\n\nProficiency in programming language such as C#\n\nExperience with data integration tools\n\nExcellent analytical skills to evaluate complex problems and devise efficient solutions\n\nExperience with cloud platforms such as Azure, including continuous integration and continuous deployment (CI/CD)\n\nFamiliarity with big data technologies such as Hadoop, Spark, and Kafka\n\nKnowledge of data privacy and security regulations such as GDPR\n\nEffective communication and collaboration skills\n\nWho WE are AXA XL, the P&C and specialty risk division of AXA, is known for solving complex risks\n\nFor mid-sized companies, multinationals and even some inspirational individuals we don t just provide re/insurance, we reinvent it\n\nHow? By combining a comprehensive and efficient capital platform, data-driven insights, leading technology, and the best talent in an agile and inclusive workspace, empowered to deliver top client service across all our lines of business property, casualty, professional, financial lines and specialty\n\nWith an innovative and flexible approach to risk solutions, we partner with those who move the world forward\n\nLearn more at axaxl\n\ncom What we OFFER Inclusion AXA XL is committed to equal employment opportunity and will consider applicants regardless of gender, sexual orientation, age, ethnicity and origins, marital status, religion, disability, or any other protected characteristic\n\nAt AXA XL, we know that an inclusive culture and a diverse workforce enable business growth and are critical to our success\n\nThat s why we have made a strategic commitment to attract, develop, advance and retain the most diverse workforce possible, and create an inclusive culture where everyone can bring their full selves to work and can reach their highest potential\n\nIt s about helping one another and our business to move forward and succeed\n\nFive Business Resource Groups focused on gender, LGBTQ+, ethnicity and origins, disability and inclusion with 20 Chapters around the globe Robust support for Flexible Working Arrangements Enhanced family friendly leave benefits Named to the Diversity Best Practices Index Signatory to the UK Women in Finance Charter Learn more at axaxl\n\ncom / about-us / inclusion-and-diversity\n\nAXA XL is an Equal Opportunity Employer\n\nTotal Rewards AXA XL s Reward program is designed to take care of what matters most to you, covering the full picture of your health, wellbeing, lifestyle and financial security\n\nIt provides competitive compensation and personalized, inclusive benefits that evolve as you do\n\nWe re committed to rewarding your contribution for the long term, so you can be your best self today and look forward to the future with confidence\n\nSustainability At AXA XL, Sustainability is integral to our business strategy\n\nIn an ever-changing world, AXA XL protects what matters most for our clients and communities\n\nWe know that sustainability is at the root of a more resilient future\n\nOur 2023-26 Sustainability strategy, called Roots of resilience , focuses on protecting natural ecosystems, addressing climate change, and embedding sustainable practices across our operations\n\nOur Pillars: Valuing nature: How we impact nature affects how nature impacts us\n\nResilient ecosystems - the foundation of a sustainable planet and society - are essential to our future\n\nWe re committed to protecting and restoring nature - from mangrove forests to the bees in our backyard - by increasing biodiversity awareness and inspiring clients and colleagues to put nature at the heart of their plans\n\nAddressing climate change: The effects of a changing climate are far reaching and significant\n\nUnpredictable weather, increasing temperatures, and rising sea levels cause both social inequalities and environmental disruption\n\nWere building a net zero strategy, developing insurance products and services, and mobilizing to advance thought leadership and investment in societal-led solutions\n\nIntegrating ESG: All companies have a role to play in building a more resilient future\n\nIncorporating ESG considerations into our internal processes and practices builds resilience from the roots of our business\n\nWe re training our colleagues, engaging our external partners, and evolving our sustainability governance and reporting\n\nAXA Hearts in Action: We have established volunteering and charitable giving programs to help colleagues support causes that matter most to them, known as AXA XL s Hearts in Action programs\n\nThese include our Matching Gifts program, Volunteering Leave, and our annual volunteering day - the Global Day of Giving\n\nFor more information, please see axaxl\n\ncom/sustainability",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['remediation', 'continuous integration', 'software architecture', 'Coding', 'Agile', 'Data quality', 'data privacy', 'Business strategy', 'Project delivery', 'Salesforce']",2025-06-12 13:58:54
Senior Manager Information Systems,Amgen Inc,8 - 13 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will develop an insight driven sensing capability with a focus on revolutionizing decision making. In this role you will lead the technical delivery for this capability as part of a team data engineers and software engineers. The team will rely on your leadership to own and refine the vision, feature prioritization, partner alignment, and experience leading solution delivery while building this ground-breaking new capability for Amgen. You will drive the software engineering side of the product release and will deliver for the outcomes.\nRoles & Responsibilities:\nLead delivery of overall product and product features from concept to end of life management of the product team comprising of technical engineers, product owners and data scientists to ensure that business, quality, and functional goals are met with each product release\nDrives excellence and quality for the respective product releases, collaborating with Partner teams.\nImpacts quality, efficiency and effectiveness of own team. Has significant input into priorities.\nIncorporate and prioritize feature requests into product roadmap; Able to translate roadmap into execution\nDesign and implement usability, quality, and delivery of a product or feature\nPlan releases and upgrades with no impacts to business\nHands on expertise in driving quality and best in class Agile engineering practices\nEncourage and motivate the product team to deliver innovative and exciting solutions with an appropriate sense of urgency\nManages progress of work and addresses production issues during sprints\nCommunication with partners to make sure goals are clear and the vision is aligned with business objectives\nDirect management and staff development of team members\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients.\nBasic Qualifications:\nMasters degree and 8 to 10 years of Information Systems experience OR\nBachelors degree and 10 to 14 years ofInformation Systems experience OR\nDiploma and 14 to 18 years of Information Systems experience\nThorough understanding of modern web application development and delivery, Gen AI applications development, Data integration and enterprise data fabric concepts, methodologies, and technologies e.g. AWS technologies, Databricks\nDemonstrated experience in building strong teams with consistent practices.\nDemonstrated experience in navigating matrix organization and leading change.\nPrior experience writing business case documents and securing funding for product team delivery; Financial/Spend management for small to medium product teams is a plus.\nIn-depth knowledge of Agile process and principles.\nDefine success metrics for developer productivity metrics; on a monthly/quarterly basis analyze how the product team is performing against established KPIs.\nFunctional Skills:\nLeadership:\nInfluences through Collaboration: Builds direct and behind-the-scenes support for ideas by working collaboratively with others.\nStrategic Thinking: Anticipates downstream consequences and tailors influencing strategies to achieve positive outcomes.\nTransparent Decision-Making: Clearly articulates the rationale behind decisions and their potential implications, continuously reflecting on successes and failures to enhance performance and decision-making.\nAdaptive Leadership: Recognizes the need for change and actively participates in technical strategy planning.\nPreferred Qualifications:\nStrong influencing skills, influence stakeholders and be able to balance priorities.\nPrior experience in vendor management.\nPrior hands-on experience leading full stack development using infrastructure cloud services (AWS preferred) and cloud-native tools and design patterns (Containers, Serverless, Docker, etc.)\nExperience with developing solutions on AWS technologies such as S3, EMR, Spark,\nAthena, Redshift and others\nFamiliarity with cloud security (AWS /Azure/ GCP)\nConceptual understanding of DevOps tools (Ansible/ Chef / Puppet / Docker /Jenkins)\nProfessional Certifications\nAWS Certified Solutions Architect (preferred)\nCertified DevOps Engineer (preferred)\nCertified Agile Leader or similar (preferred)\nSoft Skills:\nStrong desire for continuous learning to pick new tools/technologies.\nHigh attention to detail is essential with critical thinking ability.\nShould be an active contributor on technological communities/forums\nProactively engages with cross-functional teams to resolve issues and design solutions using critical thinking and analysis skills and best practices.\nInfluences and energizes others toward the common vision and goal. Maintains excitement for a process and drives to new directions of meeting the goal even when odds and setbacks render one path impassable\nEstablished habit of proactive thinking and behavior and the desire and ability to self-start/learn and apply new technologies\nExcellent organizational and time-management skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.\nShift Information:\nThis position requires you to work a later shift and may be assigned a second or third shift schedule. Candidates must be willing and able to work during evening or night shifts, as required based on business requirements.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Information Systems', 'Azure', 'DevOps', 'Gen AI application development', 'GCP', 'Data integration', 'AWS', 'web application development']",2025-06-12 13:58:56
Test Lead,Infosys,5 - 10 years,10-20 Lacs P.A.,[],"Role & responsibilities\n\nThe Test Lead oversees the testing strategy and execution for the Microsoft Fabric migration and Power BI reporting solutions. This offshore role ensures quality, reliability, and client satisfaction through rigorous validation.\nThe successful candidate will have a strong testing background and coordination skills.\nResponsibilities\nDevelop and execute the testing strategy for Microsoft Fabric and Power BI deliverables.\nValidate data migration, pipeline functionality, and report accuracy against requirements.\nCoordinate with the Offshore Project Manager to align testing with development milestones.\nCollaborate with onsite technical leads to validate results and resolve defects. • Oversee offshore testers, ensuring comprehensive coverage and quality standards.\nProactively identify risks and articulate solutions to minimize delivery issues.\nSkills\nBachelors degree in IT, computer science, or a related field.\n5+ years of experience in test leadership for data platforms and BI solutions.\nKnowledge of Microsoft Fabric, Power BI, and data migration testing.\nProficiency with testing tools (e.g., Azure DevOps, Selenium) and SQL.\nStrong communication and stakeholder management skills.\nDetail-oriented with a focus on quality and continuous improvement\n1. JD for Data Modeler\nThe Data Modeler designs and implements data models for Microsoft Fabric and Power BI, supporting the migration from Oracle/Informatica. This offshore role ensures optimized data structures for performance and reporting needs. The successful candidate will bring expertise in data modeling and a collaborative approach.\nResponsibilities\nDevelop conceptual, logical, and physical data models for Microsoft Fabric and Power BI solutions.\nImplement data models for relational, dimensional, and data lake environments on target platforms.\nCollaborate with the Offshore Data Engineer and Onsite Data Modernization Architect to ensure model alignment.\nDefine and govern data modeling standards, tools, and best practices.\nOptimize data structures for query performance and scalability.\nProvide updates on modeling progress and dependencies to the Offshore Project Manager.\nSkills\nBachelor’s or master’s degree in computer science, data science, or a related field.\n5+ years of data modeling experience with relational and NoSQL platforms.\nProficiency with modeling tools (e.g., Erwin, ER/Studio) and SQL.\nExperience with Microsoft Fabric, data lakes, and BI data structures.\nStrong analytical and communication skills for team collaboration.\nAttention to detail with a focus on performance and consistency.\nmanagement, communication, and presentation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Test lead', 'Migration', 'power bi', 'microsoft fabric']",2025-06-12 13:58:59
Senior Project Manager,PM,5 - 7 years,10-15 Lacs P.A.,['Hyderabad'],"Role description\n\nWere looking for a driven, organized team member to support the Digital & Analytics team with support of Talent transformation projects from both a systems and process perspective. The role will primarily provide PMO support. The individual will need to demonstrate strong project management skills, be very collaborative and detail oriented to coordinate meetings, track and update project plans, risks, and decision logs. This individual would also need to create project materials, support design sessions, user acceptance testing, and escalate project or system issues as needed.\n\nWork youll do As the Talent PMO Support you will:\n\nSupport the Digital & Analytics Manager with support of Talent transformation projects.\nTrack and drive closure of action items and open decisions.\nSchedule follow up calls, take notes, and distribute action items from discussions.\nCoordinate with Talent process owners and subject matter advisors to manage change requests and risks, actions, and decisions.\nCoordinate across Talent, Technology, and Consulting teams to track and escalate issues as appropriate.\nUpdate the Talent project plan items, resource tracker and the risks, actions and decisions log as needed.\nLeverage shared project team site and OneNote notebook to ensure structure and access to communications, materials, and documents for all project team members.\nSupport testing, cut-over, training, and service rehearsal testing processes as needed.\nCollaborate with the Consulting, Technology, and Talent team members to ensure project deliverables move forward.\nQualifications:\n\nBachelors degree and 5-7 years of relevant work experience\nBackground and experience with project management support to implement Talent processes, from ideation through deployment phases.\nStrong written/verbal executive communication and presentation skills; strong listening, facilitation and influencing skills with audiences of all management and leadership levels.\nWorks well in a dynamic, complex, client, and team focused environment with minimal oversight and an agile mindset\nExcited by prospect of working in a developing, ambiguous, and challenging situation.\nProficient Microsoft Office skills (e.g., PowerPoint, Excel, OneNote, Word, Teams)",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Temporary/Contractual","['Digital Project Management', 'PMO', 'Project Management', 'data engineer', 'Scrum', 'Microsoft']",2025-06-12 13:59:01
Senior Business Analyst,Skillsoft Software Services,2 - 7 years,Not Disclosed,['Hyderabad'],"India-based candidates only. We’re primarily a NYC-based team, but have a growing international team. \n \nROLE OVERVIEW:  \nAs a Senior Data Analyst, you will be pivotal in driving strategic decision-making for the Codecademy consumer and enterprise business lines. Reporting to the senior manager, Strategy and business Operations, this role will work cross-functionally to tackle the business’ highest priorities. You will utilize your technical expertise in data analytics, financial modeling, and executive communication to create actionable business strategies that drive growth.",,,,"['snowflake', 'python', 'data analytics', 'data analysis', 'modeling', 'analytical', 'verbal communication', 'business analysis', 'sql', 'analytics', 'marketing analytics', 'data integration tools', 'looker', 'writing', 'financial modelling', 'data visualization', 'business operations', 'reporting', 'communication skills']",2025-06-12 13:59:04
ETL QA Lead,Wissen Technology,8 - 13 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Job Description for QA Engineer:\n7+ years of experience in ETL Testing, Snowflake, DWH Concepts.\nStrong SQL knowledge & debugging skills are a must.\nExperience on Azure and Snowflake Testing is plus\nExperience with Qlik Replicate and Compose tools (Change Data Capture) tools is considered a plus\nStrong Data warehousing Concepts, ETL tools like Talend Cloud Data Integration, Pentaho/Kettle tool\nExperience in JIRA, Xray defect management toolis good to have.\nExposure to the financial domain knowledge is considered a plus\nTesting the data-readiness (data quality) address code or data issues\nDemonstrated ability to rationalize problems and use judgment and innovation to define clear and concise solutions\nDemonstrate strong collaborative experience across regions (APAC, EMEA and NA) to effectively and efficiently identify root cause of code/data issues and come up with a permanent solution\nPrior experience with State Street and Charles River Development (CRD) considered a plus\nExperience in tools such as PowerPoint, Excel, SQL\nExposure to Third party data providers such as Bloomberg, Reuters, MSCI and other Rating agencies is a plus",Industry Type: IT Services & Consulting,"Department: BFSI, Investments & Trading","Employment Type: Full Time, Permanent","['Data Warehouse Testing', 'ETL Testing', 'SQL', 'Snowflake']",2025-06-12 13:59:06
Senior ServiceNow Developer,Luxoft,6 - 11 years,Not Disclosed,['Gurugram'],"Internal Data Structures & Configuration\nDesign, build, and maintain data models, tables, and relationships within the ServiceNow platform.\nExtend and customize out-of-the-box modules (e.g., CMDB, Incident, Change, Request, etc.) to meet business requirements.\nEnsure data integrity, normalization, and performance optimization across the ServiceNow environment.\nCollaborate with stakeholders to translate business requirements into scalable ServiceNow configurations or custom applications.\nReporting & Dashboards\nDevelop real-time dashboards and reports using ServiceNow Reporting Tools and Performance Analytics.\nDeliver insights into key ITSM metrics such as SLAs, incident trends, and operational KPIs.\nAutomate the generation and distribution of recurring reports to stakeholders.\nWork with business and technical teams to define and implement reporting frameworks tailored to their needs.\nAutomated Feeds & API Integration\nDevelop and manage robust data integrations using ServiceNow REST/SOAP APIs.\nBuild and maintain data pipelines to and from external systems (e.g., CMDB, HRIS, ERP, Flexera, etc.).\nImplement secure, scalable automation for data exchange with appropriate error handling, logging, and monitoring.\nTroubleshoot and resolve integration-related issues to ensure smooth system interoperability.\nSkills\nMust have\nMinimum 6+ years of hands-on experience with ServiceNow, including ITSM, CMDB, and integrations.\nTechnical Expertise:\nAdvanced knowledge of ServiceNow architecture, configuration, and scripting (JavaScript, Glide).\nStrong experience with REST/SOAP APIs for ServiceNow integrations.\nSolid understanding of relational databases, data normalization, and model optimization.\nFamiliarity with common enterprise systems such as ERP, HRIS, Flexera, and CMDB tools.\nReporting Skills:\nProficiency in ServiceNow Performance Analytics, standard reporting, and dashboard design.\nExperience defining KPIs and building automated reporting solutions.\nSoft Skills:\nStrong communication and collaboration skills.\nProven ability to translate business requirements into scalable ServiceNow solutions.\nAnalytical and detail-oriented mindset with a problem-solving approach.\n",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ERP', 'Automation', 'SAP', 'HRIS', 'Analytical', 'Javascript', 'Data structures', 'Risk management', 'Analytics', 'Monitoring']",2025-06-12 13:59:08
Senior ServiceNow Developer,Luxoft,6 - 11 years,Not Disclosed,['Chennai'],"Internal Data Structures & Configuration\nDesign, build, and maintain data models, tables, and relationships within the ServiceNow platform.\nExtend and customize out-of-the-box modules (e.g., CMDB, Incident, Change, Request, etc.) to meet business requirements.\nEnsure data integrity, normalization, and performance optimization across the ServiceNow environment.\nCollaborate with stakeholders to translate business requirements into scalable ServiceNow configurations or custom applications.\nReporting & Dashboards\nDevelop real-time dashboards and reports using ServiceNow Reporting Tools and Performance Analytics.\nDeliver insights into key ITSM metrics such as SLAs, incident trends, and operational KPIs.\nAutomate the generation and distribution of recurring reports to stakeholders.\nWork with business and technical teams to define and implement reporting frameworks tailored to their needs.\nAutomated Feeds & API Integration\nDevelop and manage robust data integrations using ServiceNow REST/SOAP APIs.\nBuild and maintain data pipelines to and from external systems (e.g., CMDB, HRIS, ERP, Flexera, etc.).\nImplement secure, scalable automation for data exchange with appropriate error handling, logging, and monitoring.\nTroubleshoot and resolve integration-related issues to ensure smooth system interoperability.\nSkills\nMust have\nMinimum 6+ years of hands-on experience with ServiceNow, including ITSM, CMDB, and integrations.\nTechnical Expertise:\nAdvanced knowledge of ServiceNow architecture, configuration, and scripting (JavaScript, Glide).\nStrong experience with REST/SOAP APIs for ServiceNow integrations.\nSolid understanding of relational databases, data normalization, and model optimization.\nFamiliarity with common enterprise systems such as ERP, HRIS, Flexera, and CMDB tools.\nReporting Skills:\nProficiency in ServiceNow Performance Analytics, standard reporting, and dashboard design.\nExperience defining KPIs and building automated reporting solutions.\nSoft Skills:\nStrong communication and collaboration skills.\nProven ability to translate business requirements into scalable ServiceNow solutions.\nAnalytical and detail-oriented mindset with a problem-solving approach.\nNice to have\nN/A.\n",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ERP', 'Automation', 'SAP', 'HRIS', 'Analytical', 'Javascript', 'Data structures', 'Risk management', 'Analytics', 'Monitoring']",2025-06-12 13:59:11
AWS Cloud Tech Lead,ITC Infotech,5 - 9 years,Not Disclosed,['Pune'],"You will participate in the design, development, and deployment of scalable and robust applications using AWS Cloudfront, S3 buckets, Node.js, TypeScript, React.js, Next.js, Stencil.js, and Aurora Postgres SQL.\nImplement microservices that integrate with Databricks as a Data Lake and consume data products.\nKey Responsibilities\nCollaborate closely with other technical leads to align on standards, interfaces, and dependencies",,,,"['System architecture', 'Backend', 'Front end', 'Coding', 'Postgresql', 'Agile', 'Scrum', 'JIRA', 'AWS', 'microservices']",2025-06-12 13:59:13
Backend Developer - Python,Derisk360,6 - 11 years,Not Disclosed,"['Chennai', 'Gurugram', 'Bengaluru']","Join our engineering team as a Senior Backend Engineer and lead the development of cloud-native, scalable microservices RESTful APIs using modern Python frameworks . Youll work with CI/CD tools to build robust backend systems powering next-gen platforms. If you have hands-on experience with , and are skilled in distributed systems relational/NoSQL databases , we want to hear from you. Key Responsibilities: Microservices Development Design, build, and optimize microservices architecture using patterns like Service Discovery Circuit Breaker API Gateway Saga orchestration REST API Engineering Develop high-performance frameworks like Django REST Framework Cloud-Native Backend Systems Build and deploy containerized applications . Familiarity with Kubernetes (K8s) for orchestration is a plus. CI/CD Automation Create and maintain DevOps pipelines GitLab CI/CD GitHub Actions for automated testing and deployment. Source Code Management Collaborate through Git-based version control , ensuring code quality via pull requests peer reviews on platforms like Event-Driven Architecture Implement and manage data streaming messaging pipelines Apache Kafka Amazon Kinesis , or equivalent. Database Engineering Work with , and optionally solutions such as Cloud Infrastructure Architect and manage AWS backend services Big Data Integration (Desirable) for distributed data processing and scalable ETL workflows in data engineering Polyglot Collaboration Integrate with backend services or data processors developed in , or other enterprise technologies. Required Skills & Qualifications: Bachelors or Masters in Computer Science Software Engineering , or a related technical field. 6+ years in backend development Proven expertise in API development cloud-native applications Proficiency in database schema design , and query optimization. Strong grasp of DevOps best practices Git workflows code quality standards Experience with streaming platforms message queues event-driven design Nice to Have: Exposure to big data tools (e.g., Familiarity with Agile/Scrum methodologies cross-functional teams Competitive salary and performance-based bonuses Opportunity to build next-gen backend platforms for global-scale applications. Work with a team that values",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Architect', 'Backend', 'Automation', 'MySQL', 'Cloud', 'Management', 'big data', 'Apache', 'SQL', 'Python']",2025-06-12 13:59:16
IICS/IDMC Developer,Qualitest,0 - 5 years,Not Disclosed,['Bengaluru'],"Design, develop, and maintain data integration workflows using Informatica IICS (Cloud Data Integration and Application Integration).\nDevelop and optimize ETL solutions using Informatica PowerCenter.\nWork on Snowflake to support data warehousing solutions, including data ingestion, transformation, and performance tuning.\nWrite efficient and optimized SQL and PL/SQL queries for data extraction, transformation, and validation.\nDevelop and support Unix Shell Scripts for automation and job scheduling.\nCollaborate with business and technical stakeholders to understand requirements and deliver scalable solutions.\nParticipate in design reviews, code reviews, and performance tuning exercises.\nContribute to cloud migration and modernization initiatives, particularly on Azure.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Performance tuning', 'Automation', 'Assurance', 'Managed services', 'Functional testing', 'PLSQL', 'Healthcare', 'Scheduling', 'Informatica']",2025-06-12 13:59:18
Sr. Databricks Developer,Newscape Consulting,7 - 9 years,Not Disclosed,['Pune( Baner )'],"We are looking for a highly skilled Senior Databricks Developer to join our data engineering team. You will be responsible for building scalable and efficient data pipelines using Databricks, Apache Spark, Delta Lake, and cloud-native services (Azure/AWS/GCP). You will work closely with data architects, data scientists, and business stakeholders to deliver high-performance, production-grade solutions.\nKey Responsibilities :\n- Design, build, and maintain scalable and efficient data pipelines on Databricks using PySpark, Spark SQL, and optionally Scala.\n- Work with Databricks components including Workspace, Jobs, DLT (Delta Live Tables), Repos, and Unity Catalog.\n- Implement and optimize Delta Lake solutions aligned with Lakehouse and Medallion architecture best practices.\n- Collaborate with data architects, engineers, and business teams to understand requirements and deliver production-grade solutions.\n- Integrate CI/CD pipelines using tools such as Azure DevOps, GitHub Actions, or similar for Databricks deployments.\n- Ensure data quality, consistency, governance, and security by using tools like Unity Catalog or Azure Purview.\n- Use orchestration tools such as Apache Airflow, Azure Data Factory, or Databricks Workflows to schedule and monitor pipelines.\n- Apply strong SQL skills and data warehousing concepts in data modeling and transformation logic.\n- Communicate effectively with technical and non-technical stakeholders to translate business requirements into technical solutions.\nRequired Skills and Qualifications :\n- Hands-on experience in data engineering, with specifically in Databricks.\n- Deep expertise in Databricks Workspace, Jobs, DLT, Repos, and Unity Catalog.\n- Strong programming skills in PySpark, Spark SQL; Scala experience is a plus.\n- Proficient in working with one or more cloud platforms : Azure, AWS, or GCP.\n- Experience with Delta Lake, Lakehouse architecture, and medallion architecture patterns.\n- Proficient in building CI/CD pipelines for Databricks using DevOps tools.\n- Familiarity with orchestration and ETL/ELT tools such as Airflow, ADF, or Databricks Workflows.\n- Strong understanding of data governance, metadata management, and lineage tracking.\n- Excellent analytical, communication, and stakeholder management skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Pyspark', 'Azure Databricks', 'ETL', 'Delta Lake', 'Azure Data Lake', 'Apache', 'Data Bricks']",2025-06-12 13:59:20
MS CRM Lead,Ducen,10 - 15 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Job title: Microsoft Dynamics CRM Lead Developer\nProgram description:\nDynamics CRM Lead Developer to become part of our DEV team creating and supporting application in several different environments. Must have a strong understanding of data analytics relating to MS SQL Server. The ideal candidate will have a deep understanding of MS Dynamics, SQL databases, and business processes to support the agency goals and departments use of Microsoft Dynamics for customer service and business case management.\nThe ideal candidate will bring several qualities to the team, including:\nThe ability to translate user requirements into application, workflow, and database configurations, within the context of a configurable CRM application.\nStrong skills in Microsoft Excel, data management and data integration skills with an ability to organize, analyze and correlate data information.\nSolid interpersonal skills with the ability to communicate with technical and non-technical stakeholders.\nThe ability to organize, analyze and correlate data and information.\nThe ability to work and collaborate with others in a fast-paced environment with organization, attention to detail, responsiveness, and accountability.\nQualifications:\n10+ years of IT experience including 6+ years in Microsoft Dynamics CRM development and Lead role\nExperience and in-depth hands-on knowledge of the Microsoft Dynamics 365 CRM platform, the entity model, security model, and Web services\nStrong functional knowledge of Microsoft Dynamics CRM Customer Service, Sales, Marketing, and modules\nSkills on CRM processes in Insurance industry preferably P&C (Property and Casualty)\nExperience with ASP.NET, MVC 4.0, C#, Java script, jQuery, Bootstrap, ADO.NET\nExperience with DevOps, Agile-Scrum\nStrong proficiency in Microsoft Dynamics 365 platform and its various modules\nExperience/proficiency with Power Platform (Power Apps, Power Automate and Power BI)\nUnderstanding of web services, REST APIs, and integration patterns (pub/sub patterns preferred)\nFamiliarity with Azure cloud services and DevOps practices\nEducation Required\nBachelor s degree in computer science, or related field\nNice to Have\nProperty and Casualty Insurance industry experience\nBuilding and deploying Dynamics 365 packages in development, testing, and production environments.\nCandidate Privacy Policy\nOrion Systems Integrators, LLC and its subsidiaries and its affiliates (collectively, Orion, we or us ) are committed to protecting your privacy. This (orioninc.com) ( Notice ) explains:\nWhat information we collect during our application and recruitment process and why we collect it;\nHow we handle that information; and\nHow to access and update that information.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'MS SQL', 'jQuery', 'Data management', 'Workflow', 'Life sciences', 'Customer service', 'Automotive', 'Financial services', 'CRM']",2025-06-12 13:59:22
Etl Developer,Tata Communications,2 - 4 years,Not Disclosed,['Chennai'],"This is an operational role responsible for providing data analysis & management support. The incumbent may seek appropriate level of guidance and advice to ensure delivery of quality outcomes.\nResponsibilities\nGathering and preparing relevant data to use in analytics applications.\nAcquiring data from primary or secondary data sources and support in maintaining databases\nIdentify, analyze, and interpret trends or patterns in data sets.\nFilter and clean data by reviewing computer reports, printouts, and performance indicators to locate and correct code problems.\nDevelop and Support ETL Jobs, Schedule batch jobs via CRON, Database modeling for RDBMS\nGather data requirements, follow Scrum methodology, ownership from development to deployment",,,,"['Etl Development', 'Informatica', 'Informatica Powercenter', 'ETL']",2025-06-12 13:59:25
F2F Weekend Drive - Bangalore- 14th June - DS Gen AI,Ltimindtree,6 - 11 years,Not Disclosed,['Bengaluru'],"Job description\nWe are having a F2F weekend drive for the requirement of a Data Scientist + Gen AI at our LTIM Bangalore Whitefield office.\nDate - 14th June 2025\nExperience - 6+ Years\nMandatory Skills - Data Science, Gen AI, Python, RAG and Azure/AWS, AI/ML, NLP\nLocation - LTIMindtree Bangalore Whitefield Office\nSecondary - (Any) Machine Learning, Deep Learning, ChatGPT, Langchain, Prompt, vector stores, RAG, llama, Computer vision, Deep learning, Machine learning, OCR, Transformer, regression, forecasting, classification, hyper parameter tunning, MLOps, Inference, Model training, Model Deployment\n\nGeneric JD-\nMore than 6 years of experience in Data Engineering, Data Science and AI / ML domain\nExcellent understanding of machine learning techniques and algorithms, such as GPTs, CNN, RNN, k-NN, Naive Bayes, SVM, Decision Forests, etc.\nExperience using business intelligence tools (e.g. Tableau, PowerBI) and data frameworks (e.g. Hadoop)\nExperience in Cloud native skills.\nKnowledge of SQL and Python; familiarity with Scala, Java or C++ is an asset\nAnalytical mind and business acumen and Strong math skills (e.g. statistics, algebra)\nExperience with common data science toolkits, such as TensorFlow, KERAs, PyTorch, PANDAs, Microsoft CNTK, NumPy etc. Deep expertise in at least one of these is highly desirable.\nExperience with NLP, NLG and Large Language Models like BERT, LLaMa, LaMDA, GPT, BLOOM, PaLM, DALL-E, etc.\nGreat communication and presentation skills. Should have experience in working in a fast-paced team culture.\nExperience with AIML and Big Data technologies like AWS SageMaker, Azure Cognitive Services, Google Colab, Jupyter Notebook, Hadoop, PySpark, HIVE, AWS EMR etc.\nExperience with NoSQL databases, such as MongoDB, Cassandra, HBase, Vector databases\nGood understanding of applied statistics skills, such as distributions, statistical testing, regression, etc.\nShould be a data-oriented person with analytical mind and business acumen.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Generative AI', 'Machine Learning', 'Deep Learning', 'Python', 'Azure']",2025-06-12 13:59:27
Power Bi & SQL Developer,Sonata Software,3 - 6 years,Not Disclosed,['Chennai'],"Role & responsibilities\n\nWe are seeking a talented and detail-oriented Power BI Developer with strong skills in SQL and experience working with Azure Databricks. The ideal candidate will be responsible for transforming raw data into meaningful insights using business intelligence tools and data engineering practices. This role involves building dashboards, writing optimized queries, and working with large-scale data platforms to support business decision-making.\nKey Responsibilities:",,,,"['Power BI', 'SQL', 'Data Bricks']",2025-06-12 13:59:30
Azure and GenAi,Sightspectrum,5 - 10 years,20-30 Lacs P.A.,"['Hyderabad', 'Ahmedabad', 'Bengaluru']",Role & responsibilities\n\nGenAI\nAzure\nAI ML,Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GenAi', 'Azure', 'AIML']",2025-06-12 13:59:32
Principal Architect-Platform & Applications @ Bangalore_Urgent,"A leader in this space, we deliver world...",13 - 20 years,Not Disclosed,['Bengaluru'],"Principal Architect - Platform & Application Architect\n\nExperience\n15+ years in software/data platform architecture\n5+ years in architectural leadership roles\nArchitecture & Data Platform Expertise\n\nEducation\nBachelors/Master’s in CS, Engineering, or related field\n\n\nTitle: Principal Architect\n\nLocation: Onsite Bangalore\n\nExperience: 15+ years in software & data platform architecture and technology strategy\n\nRole Overview\n\nWe are seeking a Platform & Application Architect to lead the design and implementation of a next-generation, multi-domain data platform and its ecosystem of applications. In this strategic and hands-on role, you will define the overall architecture, select and evolve the technology stack, and establish best practices for governance, scalability, and performance. Your responsibilities will span across the full data lifecycle—ingestion, processing, storage, and analytics—while ensuring the platform is adaptable to diverse and evolving customer needs. This role requires close collaboration with product and business teams to translate strategy into actionable, high-impact platform & products.\n\nKey Responsibilities\n\n1. Architecture & Strategy\nDesign the end-to-end architecture for a On-prem / hybrid data platform (data lake/lakehouse, data warehouse, streaming, and analytics components).\nDefine and document data blueprints, data domain models, and architectural standards.\nLead build vs. buy evaluations for platform components and recommend best-fit tools and technologies.\n2. Data Ingestion & Processing\nArchitect batch and real-time ingestion pipelines using tools like Kafka, Apache NiFi, Flink, or Airbyte.\nOversee scalable ETL/ELT processes and orchestrators (Airflow, dbt, Dagster).\nSupport diverse data sources: IoT, operational databases, APIs, flat files, unstructured data.\n3. Storage & Modeling\nDefine strategies for data storage and partitioning (data lakes, warehouses, Delta Lake, Iceberg, or Hudi).\nDevelop efficient data strategies for both OLAP and OLTP workloads.\nGuide schema evolution, data versioning, and performance tuning.\n4. Governance, Security, and Compliance\nEstablish data governance, cataloging, and lineage tracking frameworks.\nImplement access controls, encryption, and audit trails to ensure compliance with DPDPA, GDPR, HIPAA, etc.\nPromote standardization and best practices across business units.\n5. Platform Engineering & DevOps\nCollaborate with infrastructure and DevOps teams to define CI/CD, monitoring, and DataOps pipelines.\nEnsure observability, reliability, and cost efficiency of the platform.\nDefine SLAs, capacity planning, and disaster recovery plans.\n6. Collaboration & Mentorship\nWork closely with data engineers, scientists, analysts, and product owners to align platform capabilities with business goals.\nMentor teams on architecture principles, technology choices, and operational excellence.\nSkills & Qualifications\n\nBachelor’s or Master’s degree in Computer Science, Engineering, or a related field.\n12+ years of experience in software engineering, including 5+ years in architectural leadership roles.\nProven expertise in designing and scaling distributed systems, microservices, APIs, and event-driven architectures using Java, Python, or Node.js.\nStrong hands-on experience with building scalable data platforms on premise/Hybrid/cloud environments.\nDeep knowledge of modern data lake and warehouse technologies (e.g., Snowflake, BigQuery, Redshift) and table formats like Delta Lake or Iceberg.\nFamiliarity with data mesh, data fabric, and lakehouse paradigms.\nStrong understanding of system reliability, observability, DevSecOps practices, and platform engineering principles.\nDemonstrated success in leading large-scale architectural initiatives across enterprise-grade or consumer-facing platforms.\nExcellent communication, documentation, and presentation skills, with the ability to simplify complex concepts and influence at executive levels.\nCertifications such as TOGAF or AWS Solutions Architect (Professional) and experience in regulated domains (e.g., finance, healthcare, aviation) are desirable.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Architecture', 'Principal Architect', 'on-prem data platforms data lakes', 'Data Platform', 'Designed hybrid', 'Airflow', 'Airbyte', 'architectural leadership', 'Kafka', 'lakehouses', 'Flink', 'or Node.js', 'microservices', 'Hudi', 'streaming', 'DWH', 'dbt', 'Dagster', 'Delta Lake', 'Iceberg', 'distributed systems', 'NiFi', 'Python']",2025-06-12 13:59:34
"Technical Architect, AVP",XL India Business Services Pvt. Ltd,13 - 20 years,Not Disclosed,"['Hyderabad', 'Ahmedabad', 'Bengaluru']","Technical Architect-AVP Bangalore, Karnataka, India We are looking for an experienced Technical Architect to lead the design and implementation of complex software solutions\n\nThe ideal candidate will possess a deep understanding of software architecture principles, technology stacks, and development methodologies\n\nYou will collaborate with cross-functional teams to ensure that our technology solutions are scalable, secure, and aligned with business objectives\n\nWhat you ll be DOING What will your essential responsibilities include? Responsible for the design and technical delivery of the Salesforce Underwriter Journey\n\nCollaborate with business analysts and stakeholders to gather requirements and translate them into technical specifications\n\nProducing quality, secure, scalable, high-performing, and resilient designs for new or improved services\n\nLead the systems analysts, developers, and testers in sympathetic change to the applications\n\nResponsible for partnering with engineers, DevOps engineers, Administrators, and other roles responsible for implementing solutions on Azure to ensure sound infrastructure solution options are leveraged Accountable for leading technical project delivery within the applications landscape\n\nResponsible for handling multiple tech initiatives in Agile delivery models\n\nActively lead the development teams to help assist PI planning and prioritization\n\nFor internal assets, support Product Owners to develop and maintain the Product Roadmap\n\nDefine and maintain development standards such as system and data design, coding, etc Maintain a capacity plan with historical performance metrics, a future forecast, and a capacity model to ensure services and infrastructure deliver performance and growth targets in a cost effective and proactive manner\n\nManage architecture exceptions for the application, including identifying, documenting, taking through exception approval process, and remediation where and when possible\n\nMonitor application services to ensure performance consistently meets non-functional requirements (response time, security, etc)\n\nLeads the DevOps team and developers in targeted use of DevOps for their application platform assets\n\nYou will report to the Release Train Engineer\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Developing and maintaining custom data integration solutions with various sources and formats, including structured and unstructured data, to ensure data quality and consistency\n\nEnsures that the solution and codebase is maintainable, scalable and adheres to best practices of software development\n\nDelivering high-quality, scalable, and reliable data ingestion pipelines\n\nSupports other members of the squad in resolving technical questions related to best practice, feasibility etc Desired Skills and Abilities: Insurance background\n\nProficiency in programming language such as C#\n\nExperience with data integration tools\n\nExcellent analytical skills to evaluate complex problems and devise efficient solutions\n\nExperience with cloud platforms such as Azure, including continuous integration and continuous deployment (CI/CD)\n\nFamiliarity with big data technologies such as Hadoop, Spark, and Kafka\n\nKnowledge of data privacy and security regulations such as GDPR\n\nEffective communication and collaboration skills\n\nWho WE are AXA XL, the P&C and specialty risk division of AXA, is known for solving complex risks\n\nFor mid-sized companies, multinationals and even some inspirational individuals we don t just provide re/insurance, we reinvent it\n\nHow? By combining a comprehensive and efficient capital platform, data-driven insights, leading technology, and the best talent in an agile and inclusive workspace, empowered to deliver top client service across all our lines of business property, casualty, professional, financial lines and specialty\n\nWith an innovative and flexible approach to risk solutions, we partner with those who move the world forward\n\nLearn more at axaxl\n\ncom What we OFFER Inclusion AXA XL is committed to equal employment opportunity and will consider applicants regardless of gender, sexual orientation, age, ethnicity and origins, marital status, religion, disability, or any other protected characteristic\n\nAt AXA XL, we know that an inclusive culture and a diverse workforce enable business growth and are critical to our success\n\nThat s why we have made a strategic commitment to attract, develop, advance and retain the most diverse workforce possible, and create an inclusive culture where everyone can bring their full selves to work and can reach their highest potential\n\nIt s about helping one another and our business to move forward and succeed\n\nFive Business Resource Groups focused on gender, LGBTQ+, ethnicity and origins, disability and inclusion with 20 Chapters around the globe Robust support for Flexible Working Arrangements Enhanced family friendly leave benefits Named to the Diversity Best Practices Index Signatory to the UK Women in Finance Charter Learn more at axaxl\n\ncom / about-us / inclusion-and-diversity\n\nAXA XL is an Equal Opportunity Employer\n\nTotal Rewards AXA XL s Reward program is designed to take care of what matters most to you, covering the full picture of your health, wellbeing, lifestyle and financial security\n\nIt provides competitive compensation and personalized, inclusive benefits that evolve as you do\n\nWe re committed to rewarding your contribution for the long term, so you can be your best self today and look forward to the future with confidence\n\nSustainability At AXA XL, Sustainability is integral to our business strategy\n\nIn an ever-changing world, AXA XL protects what matters most for our clients and communities\n\nWe know that sustainability is at the root of a more resilient future\n\nOur 2023-26 Sustainability strategy, called Roots of resilience , focuses on protecting natural ecosystems, addressing climate change, and embedding sustainable practices across our operations\n\nOur Pillars: Valuing nature: How we impact nature affects how nature impacts us\n\nResilient ecosystems - the foundation of a sustainable planet and society - are essential to our future\n\nWe re committed to protecting and restoring nature - from mangrove forests to the bees in our backyard - by increasing biodiversity awareness and inspiring clients and colleagues to put nature at the heart of their plans\n\nAddressing climate change: The effects of a changing climate are far reaching and significant\n\nUnpredictable weather, increasing temperatures, and rising sea levels cause both social inequalities and environmental disruption\n\nWere building a net zero strategy, developing insurance products and services, and mobilizing to advance thought leadership and investment in societal-led solutions\n\nIntegrating ESG: All companies have a role to play in building a more resilient future\n\nIncorporating ESG considerations into our internal processes and practices builds resilience from the roots of our business\n\nWe re training our colleagues, engaging our external partners, and evolving our sustainability governance and reporting\n\nAXA Hearts in Action: We have established volunteering and charitable giving programs to help colleagues support causes that matter most to them, known as AXA XL s Hearts in Action programs\n\nThese include our Matching Gifts program, Volunteering Leave, and our annual volunteering day - the Global Day of Giving\n\nFor more information, please see axaxl\n\ncom/sustainability",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['remediation', 'continuous integration', 'software architecture', 'Coding', 'Agile', 'Data quality', 'data privacy', 'Business strategy', 'Project delivery', 'Salesforce']",2025-06-12 14:00:10
Loyalty Program - Delhi,Orient Electric,8 - 13 years,Not Disclosed,"['New Delhi', 'Delhi / NCR']","Role & responsibilities\n\nOrient Electric runs several loyalty programs across key influencers to improve recommendation of Orient products across the value chain. This role is a cross functional lead role across the several business units and various departments including sales, marketing, category and technology teams to ensure smooth functioning of the loyalty programs of the company.\nIt also requires first principle disruptive thinking to create unique value propositions for these influencers and an agile mindset for execution with speed with multiple stakeholders.\nIdeal Experience\n\nWe are looking for someone with experience in product marketing, sales excellence, customer value propositions/loyalty programs. Should have an entrepreneurial mindset with ability to create solutions from scratch with an ability to analyse data continuously to improve outcomes. Prior experience in categories where influencers play a strong role in decision making (Home improvement, FMEG, Paint etc) and B2B experience will be valuable. Understanding of digital media landscapes, marketing automation systems and data integrations is needed with excellent project management skills.\n\nSet up and scale various loyalty programs for Electricians, Retailers, Shop Boys, Architects and Interior Designers to grow market share across categories that OEL operates in including\nDefining the overall structure of the program across BUs – Rewards Framework and value proposition for each influencer segment\nBuild and Maintain technology interface and processes to run the program smoothly\nContinue to deeply understand the motivations of influencers for brand recommendation and build learnings into the program. Leverage audience insights and segmentation to improve outcomes\nDefine, monitor and publish key success metrics for onboarding, earning and redemption of rewards and quantify correlation of the same to business growth\nBuild dashboards with key success metrics and set goals across business units for the program’s success\nLiaison with key members of sales, marketing, service to identify areas of opportunity and growth\nDrive user app penetration and engagement metrics and deliver on goals of acquisition, usage and redemption. Reduce end user churn through proactive actions and ensure active engagement goals are met\nExecute multichannel campaigns and programs to improve engagement and retention on the platform. Execute multiple channels of marketing including email, push notifications, product messaging ads etc to improve engagement goals\nCollaborate with business and product teams to facilitate new product launches and other adoption initiatives\nResponsible for financial hygiene and compliance and audits for the program",Industry Type: Consumer Electronics & Appliances,Department: Other,"Employment Type: Full Time, Permanent","['Loyalty', 'Loyalty Program Management', 'Loyalty Programs']",2025-06-12 14:00:12
Position For Business Intelligence Developer with ( DBT & Snowflake ),Synergy Technologies,5 - 9 years,Not Disclosed,[],"Hi ,  \nSynergy Technologies is a leader in technology services and consulting. We enable clients across the world to create and execute strategies .We help our clients find the right problems to solve, and to solve these effectively. We bring our expertise and innovation to every project we undertake\n\nPosition: Business Intelligence Developer\nDuration :  Contract to Full Time",,,,"['BI tools', 'DBT', 'Snowflake', 'DAX calculations', 'Business Intelligence Developer', 'AWS']",2025-06-12 14:00:14
Principle Architect - Backend,Circles.Life,5 - 8 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Circles.Life is looking for Principle Architect - Backend to join our dynamic team and embark on a rewarding career journey\nLead and manage a team of architects, providing technical guidance and mentoring\nDevelop and oversee the execution of the architectural design for new projects or systems\nCollaborate with stakeholders to determine project requirements and ensure the design meets those requirements\nProvide technical expertise on system architecture, design patterns, and development methodologies\nIdentify and address technical issues throughout the development process\nEnsure that the final product meets quality standards, including performance, security, and scalability\nProvide leadership in the selection of technology platforms, development tools, and frameworks\nManage and prioritize project timelines and budgets, ensuring that projects are delivered on time and within budget\nCommunicate with stakeholders and team members to ensure that all project goals are met\nExcellent problem-solving skills and ability to identify and address technical issues\nStrong communication and interpersonal skills",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['architectural design', 'project management', 'enterprise architecture', 'microsoft azure', 'mentoring', 'machine learning', 'scalability', 'microservices', 'docker', 'system architecture', 'design patterns', 'design', 'aws', 'cloud computing', 'big data', 'communication skills', 'architecture']",2025-06-12 14:00:16
Architect,Trianz,12 - 17 years,Not Disclosed,['Bengaluru'],"Role Snowflake Architect\nJob Summary -\nAs a Data Architect, you are core to the D&AI (Data & AI) Practices success. Data is foundational to everything we do, and you are accountable for defining and delivering best-in-class Snowflake data management solutions across all major cloud platforms. This is a senior role with high visibility and reporting to the D&AI Practice Tower Lead.\nJob Responsibilities\nArchitectural Design: Architect secure, scalable, highly performant data engineering and management solutions, including data warehouses, data lake, ELT / ETL and real-time data engineering / pipeline solutions. Support Principal Data Architect in defining and maintaining Practice reference data engineering and data management architectures.\nSnowflake Implementation: Design and manage scalable end-to-end data solutions leveraging native Snowflake workloads including : Data Engineering; Data Lake; Data Warehouse; Applications; Unistore; AI/ML; Governed Collaboration, Marketplace, Streamlit.\nHyperscaler Design: Competently leverage data-related cloud platform (AWS or Azure) capabilities to architect and develop end-to-end data engineering and data management solutions.\nClient Engagement: Regular collaboration and partnership with clients to understand their challenges and needs then translate requirements into data solutions that drive customer value. Support proposal development.\nData Modeling: Create and maintain conceptual, logical, and physical data models that support both transactional and analytical needs. Ensure data models are optimized for performance and scalability.\nCreativity: Be an out-of-the-box thinker and passionate about applying your skills to new and existing solutions alike while always demonstrating a customer-first mentality.\nMandatory Skills\n12+ years hands-on data solution architecture and implementation experience on modern cloud platforms (AWS preferred) including microservice and event-driven architectures.\nSnowflake SnowPro Advanced Architect certification. An architectural certification on either AWS, Azure or GCP.\nHands-on experience with Snowflake capabilities including Snowpipe, Snowpark, Cortex, Polaris Catalog, native applications, Notebooks, Horizon, Marketplace, Streamlit.\nPractical experience with end-to-end data engineering and data management supporting functions including data modeling (conceptual, logical & physical), BI & analytics, data governance, data quality, data security / privacy / compliance, IAM, performance optimization. Advanced SQL and data profiling.\nPython, Scala or Java. Strong communication skills with the ability to convey technical concepts to non-technical users.\nStrong, self-management skills demonstrating ability to multitask and self-manage goals and activities.\nAdditional / Nice-to-have Qualifications-\nSnowflake SnowPro Advanced Data Engineer certification Snowflake SnowPro Advanced Data Scientist certification Snowflake SnowPro Advanced Administrator certification Snowflake SnowPro Advanced Data Analyst certification\nRequired Education Master or Bachelor (CS, IT, Applied Mathematics or demonstrated experience)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Solution architecture', 'Architect', 'Data management', 'Data modeling', 'data security', 'Analytical', 'Data quality', 'Analytics', 'SQL', 'Python']",2025-06-12 14:00:19
Software Developer - Python,Squarepoint Technologies India,6 - 10 years,Not Disclosed,['Bengaluru'],"Team: Development - Alpha Data\n\nPosition Overview:\nWe are seeking an experienced Python developer to join our Alpha Data team, responsible for delivering a vast quantity of data served to users worldwide. You will be a cornerstone of a growing Data team, becoming a technical subject matter expert and developing strong working relationships with quant researchers, traders, and fellow colleagues across our Technology organisation.\nAlpha Data teams are able to deploy valuable data to the rest of the Squarepoint business at speed. Ingestion pipelines and data transformation jobs are resilient and highly maintainable, while the data models are carefully designed in close collaboration with our researchers for efficient query construction and alpha generation.\nWe achieve an economy of scale through building new frameworks, libraries, and services used to increase the team's quality of life, throughput, and code quality. Teamwork and collaboration are encouraged, excellence is rewarded and diversity of thought and creative solutions are valued. Our emphasis is on a culture of learning, development, and growth.\nTake part ownership of our ever-growing estate of data pipelines,\nPropose and contribute to new abstractions and improvements - make a real positive impact across our team globally,\nDesign, implement, test, optimize and troubleshoot our data pipelines, frameworks, and services,\nCollaborate with researchers to onboard new datasets,\nRegularly take the lead on production support operations - during normal working hours only.\n\nRequired Qualifications:\n5+ years of experience coding to a high standard in Python, React, Javascript\nBachelor's degree in a STEM subject,\nExperience with and knowledge of SQL, and one or more common RDBMS systems (we mostly use Postgres),\nPractical knowledge of commonly used protocols and tools used to transfer data (e.g. FTP, SFTP, HTTP APIs, AWS S3),\nExcellent communication skills.\n\nNice to haves\nExperience with big data frameworks, databases, distributed systems, or Cloud development.\nExperience with any of these: C++, kdb+/q, Rust.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'Python Framework', 'Full Stack', 'Backend Development', 'SQL', 'Backend', 'Software Development', 'Full Software Development Life Cycle', 'RDBMS', 'Fullstack Development', 'Javascript', 'Python Data', 'Python Development', 'React.Js']",2025-06-12 14:00:21
Oracle GRC(PAN INDIA) -8+ Yrs -Hybrid- Immediate joiners,Databuzzltd,8 - 12 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']","Databuzz is Hiring for Oracle GRC(PAN INDIA) -8+ Yrs -Hybrid- Immediate joiners\n\nPlease mail your profile to alekya.chebrolu@databuzzltd.com with the below details, If\nyou are Interested.\n\nAbout DatabuzzLTD:\n\nDatabuzz is One stop shop for data analytics specialized in Data Science, Big Data, Data Engineering, AI & ML, Cloud Infrastructure and Devops. We are an MNC based in both UK and INDIA. We are a ISO 27001 & GDPR complaint company.\n\nCTC -\nECTC -\nNotice Period/LWD - (Candidate serving notice period will be preferred)\n\nPosition: Oracle GRC(PAN INDIA) -8+ Yrs -Hybrid\nExp -8+ yrs\n\nMandatory Skills:\nShould have Oracle GRC, E Business Suite Governance, Risks and Compliance\nDevelop and Implement GRC Programs and Policies Create and enforce governance risk management and compliance programs to ensure the organization adheres to regulatory requirements and internal policies\nShould have Knowledge of software engineering methodologies reporting tools modeling and testing\nShould have Lean Six Sigma and Business Process Modelling Understanding of Lean Six Sigma and Business Process Modelling and Notation\n\n\nRegards,\nAlekya\nTalent Acquisition Specialist\nalekya.chebrolu@databuzzltd.com",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['Oracle Grc', 'Risk Management', 'E Business Suite Governance']",2025-06-12 14:00:24
Azure Databricks (Power Bi and AAS expert),Apex One,4 - 8 years,Not Disclosed,['Bengaluru'],"Power Bi and AAS expert (Strong SC or Specialist Senior)\nShould have hands-on experience of Data Modelling in Azure SQL Data Warehouse and Azure Analysis Service\nShould be able to write and test Dex queries.\nShould be able generate Paginated Reports in Power BI\nShould have minimum 3 Years working experience in delivering projects in Power Bi\nMust Have:-\n3 to 8 years of experience working on design, develop, and deploy ETL processes on Databricks to support data integration and transformation.\nOptimize and tune Databricks jobs for performance and scalability.\nExperience with Scala and/or Python programming languages.\nProficiency in SQL for querying and managing data.\nExpertise in ETL (Extract, Transform, Load) processes.\nKnowledge of data modeling and data warehousing concepts.\nImplement best practices for data pipelines, including monitoring, logging, and error handling.\nExcellent problem-solving skills and attention to detail.\nExcellent written and verbal communication skills\nStrong analytical and problem-solving abilities.\nExperience in version control systems (e.g., Git) to manage and track changes to the codebase.\nDocument technical designs, processes, and procedures related to Databricks development.\nStay current with Databricks platform updates and recommend improvements to existing process.\nGood to Have:-\nAgile delivery experience.\nExperience with cloud services, particularly Azure (Azure Databricks), AWS (AWS Glue, EMR), or Google Cloud Platform (GCP).\nKnowledge of Agile and Scrum Software Development Methodologies.\nUnderstanding of data lake architectures.\nFamiliarity with tools like Apache NiFi, Talend, or Informatica.\nSkills in designing and implementing data models.",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Data Modelling', 'data lake architectures', 'Power Bi', 'Azure SQL Data Warehouse', 'Scrum Software Development Methodologies', 'Google Cloud Platform', 'technical designs', 'data warehousing', 'Apache NiFi', 'data modeling', 'ETL', 'Talend']",2025-06-12 14:00:26
.NET developer(Azure)-7+yrs-Pan India-Hybrid,Databuzz ltd,7 - 12 years,Not Disclosed,"['Pune', 'Bengaluru', 'Mumbai (All Areas)']","Databuzz is Hiring for .NET developer(Azure)-7+yrs-Pan India-Hybrid\n\nPlease mail your profile to haritha.jaddu@databuzzltd.com with the below details, If\nyou are Interested.\n\nAbout DatabuzzLTD:\nDatabuzz is One stop shop for data analytics specialized in Data Science, Big Data, Data Engineering, AI & ML, Cloud Infrastructure and Devops. We are an MNC based in both UK and INDIA. We are a ISO 27001 & GDPR complaint company.\n\nCTC -\nECTC -\nNotice Period/LWD - (Candidate serving notice period will be preferred)\n\nPosition: .NET developer(Azure)\nLocation: Pan India\nExp -7+ yrs\n\nMandatory skills :\nCandidate should have strong .NET development experience\nShould have Cloud services for application deployment and integration, including components such as databases, message queuing, secret management storage & retrieval (any cloud provider is acceptable; AWS experience is a plus).\nShould have Message Queuing services like Kafka, RabbitMQ etc\nShould have TDD, Unit & Integration testing experience\nExperience on SQL and NOSQL datastores\nSOLID principles and hands on experience applying them\nCI/CD processes (No experience required but at least understanding on delivering code from development to PROD is required)\n\n\nRegards,\nHaritha\nTalent Acquisition specialist\nharitha.jaddu@databuzzltd.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql/nosql', 'TDD', '.Net', 'Messaging Queue', 'azure', 'Unit Integration Testing', 'Microservices']",2025-06-12 14:00:29
Oracle Fusion SCM with Procurement(PAN INDIA) -8+ Yrs -Hybrid,Databuzzltd,8 - 12 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']","Databuzz is Hiring for Oracle Fusion SCM with Procurement(PAN INDIA) -8+ Yrs -Hybrid- Immediate joiners\n\nPlease mail your profile to alekya.chebrolu@databuzzltd.com with the below details, If\nyou are Interested.\n\nAbout DatabuzzLTD:\n\nDatabuzz is One stop shop for data analytics specialized in Data Science, Big Data, Data Engineering, AI & ML, Cloud Infrastructure and Devops. We are an MNC based in both UK and INDIA. We are a ISO 27001 & GDPR complaint company.\n\nCTC -\nECTC -\nNotice Period/LWD - (Candidate serving notice period will be preferred)\n\nPosition: Oracle Fusion SCM with Procurement(PAN INDIA) -8+ Yrs -Hybrid(PAN INDIA)\nExp -8+ yrs\n\nMandatory Skills:\nShould have Fusion SCM, Procurement\nShould have experience with 7 years on Fusion Finance implementation and support\nImplementation and Configuration Lead the implementation of Oracle Fusion Financials modules including General Ledger Accounts Payable Account Receivable Cash Management and Fixed Assets\nTesting and Quality Assurance Develop and execute test plans to ensure the successful implementation of Oracle Fusion Financials\nHands-on experience in configuring and customizing Oracle Fusion applications\n\n\nRegards,\nAlekya\nTalent Acquisition Specialist\nalekya.chebrolu@databuzzltd.com",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['Oracle Fusion SCM', 'Oracle Fusion Financials']",2025-06-12 14:00:31
MDM Developer,NetApp,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Summary\nParticipates in reviewing, analyzing, and modifying client/server applications and systems.\nJob Requirements\nDevelop and maintain integrations between CDM and other systems, such as CRM, order management, and other boundary systems.\nCustomize and extend MDM functionality using Oracle tools, such as Oracle Integration Cloud, Oracle Application Composer, Oracle Visual Builder.",,,,"['data management', 'web services', 'unit testing', 'groovy scripting', 'data migration', 'tools', 'oracle fusion', 'master data management', 'sql', 'plsql', 'cloud', 'operations', 'java', 'spark', 'oracle erp', 'visual', 'etl', 'pubsub', 'rest', 'python', 'oracle', 'software testing', 'application', 'mdm', 'integration', 'oracle sql', 'data integration']",2025-06-12 14:00:34
Java or Spark developer,IPS GROUP,6 - 10 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']","1 Role- Java Spark Developer\n2 Technical Skill Set- Spark / Java Big Data\n3 Experience - 6 to 10 yrs\n4 Location- Bengaluru, Mumbai, Hyderabad\n\n*Must-Have*\nSpark programming\nJava / J2EE. Oracle Database, Microservices, Springboot\nAWS\n*Good-to-Have*\n1 Experience in writing Spark programming for Bigdata Hadoop.\n2 Good and hands-on experienced in Java, , Microservices, Springboot ,AWS and Spark programming.\n3 Ability to understand and do shell scripting in Unix.\n4 Having Java/J2EE experience is a plus along with working in Agile environment.\n5",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Spark Programming', 'Spark', 'Pyspark', 'Oracle Database', 'Unix Shell Scripting', 'J2Ee', 'Spring Boot', 'pyspark developer', 'Microservices', 'java developer', 'Big Data Hadoop', 'AWS']",2025-06-12 14:00:36
SAP PP Consultant - S/4 HANA Module,Zettamine Labs,6 - 11 years,Not Disclosed,"['Mumbai', 'Pune', 'Bengaluru']","Location : Pune, Mumbai, Bangalore\n\nNotice Period : Immediate\n\nJob Overview :\n\nWe are looking for an experienced SAP PP (Production Planning) Consultant to join our team. The ideal candidate will have strong expertise in SAP PP module, including configuration, implementation, and integration with other SAP modules.\n\nKey Responsibilities :\n\n- Implement and configure SAP PP to optimize production planning and control.\n\n- Collaborate with stakeholders to gather business requirements and translate them into system solutions.\n\n- Perform end-to-end process mapping for demand planning, capacity planning, and shop floor control.\n\n- Ensure seamless integration with SAP MM, SD, and QM modules.\n\n- Troubleshoot and resolve technical issues in SAP PP environments.\n\n- Provide end-user training and documentation support.\n\n- Work closely with SAP ERP teams to enable master data integration and production planning\nrequirements.\n\n- Support SAP S/4HANA migration and implementation projects.\n\n- Optimize Bill of Materials (BOM), Routing, and Work Centers for efficient production\nprocesses.\n\n- Lead data migration from legacy systems to SAP PP.\n\nRequired Skills and Experience :\n\n- 6-12 years of experience in SAP PP implementation.\n\n- Strong knowledge of production planning, demand management, and shop floor control.\n\n- Hands-on experience in SAP PP configuration, development (ABAP), and integration.\n\n- Expertise in SAP S/4HANA integration for production planning.\n\n- Experience in capacity planning, MRP, and scheduling.\n\n- Strong understanding of warehouse business processes related to production planning.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['S/4 HANA Module', 'SAP QM', 'S/4 HANA', 'SAP SD', 'SAP PP', 'SAP ABAP', 'SAP MM', 'SAP Integration', 'SAP Implementation', 'SAP WM']",2025-06-12 14:00:39
SAP IS Retail Consultant,Bytespoke Com,7 - 10 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Contract\nJob Summary :\nWe are seeking an experienced SAP Consultant - DTC (Direct-to-Consumer) with strong expertise in SAP Customer Activity Repository (CAR) and Retail (S/4HANA Fashion) solutions. The consultant will play a critical role in integrating core merchandising (SAP S/4 Fashion) with multi-channel POS systems, covering areas such as POS data capture via CAR, retail price and markdown management, and sales audit processes.\nThis role involves close collaboration with global retail business stakeholders and offers exposure to the latest SAP cloud technologies , providing a unique opportunity to enhance your experience in a mature SAP ecosystem.\nExperience & Skills Required :\n7-10 years of SAP consulting experience with SAP CAR , Retail , AFS , or S/4HANA Fashion\nProven experience in POS data integration , retail pricing , and sales audit processes\nFull lifecycle implementation experience in ERP transformation programs (minimum 1-2 end-to-end cycles)\nIn-depth knowledge of POS data processing (sales, inventory, receipts, tenders, financial transactions) within SAP ERP\nFamiliarity with S/4HANA Fiori apps , Launchpad, Personas, and retail-specific transactions\nAbility to work independently and collaboratively across global teams\nExcellent problem-solving, communication, and stakeholder management skills\nNice to Have :\nExperience in Agile project environments\nKnowledge of integration components across functional SAP modules",Industry Type: Management Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Retail', 'SAP ERP', 'SAP', 'Sales audit', 'Sales', 'SAP IS-Retail', 'Agile', 'Data processing', 'Merchandising', 'Stakeholder management']",2025-06-12 14:00:42
Artificial Intelligence Architect,Ltimindtree,12 - 16 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']",We are looking for an experienced AI ML Developers experience in data science specializing in machine learning python statistical modelling and big data technologies pyspark sql.\n\nThe ideal candidate will have a strong background in developing and deploying machine learning models optimizing ML pipelines and handling largescale structured and unstructured data to drive business impact.\n\nDeep understanding of supervised and unsupervised learning including regression classification Multiclass classification clustering and NLP Proficiency in statistical analysis AB testing and causal inference techniques Experience with model deployment and MLOps in cloud environments AWS GCP \n\nKey Responsibilities\n\nDevelop and deploy machine learning models and predictive analytics solutions for business impact\nWork with largescale structured and unstructured data to extract insights and build scalable models\nDesign implement and optimize ML pipelines for realtime and batch processing\nCollaborate with engineering product and business stakeholders to translate business problems into data science solutions\nApply statistical modeling AB testing and causal inference techniques to evaluate business performance\nApply machine learning and statistical techniques for audience segmentation helping to identify patterns and optimise business strategies\nDrive research and innovation by staying updated with cuttingedge MLAI advancements and incorporating them into our solutions\nOptimize data science models for performance scalability and interpretability in production environments\nMentor junior data scientists and contribute to best practices in data science and engineering,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Architect', 'MLOps', 'Machine Learning', 'Ai Solutions', 'Aiml', 'Ml']",2025-06-12 14:00:44
Data Engineer,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Neo4j, Stardog\n\n\n\n\nGood to have skills :JavaMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand their data needs and provide effective solutions, ensuring that the data infrastructure is robust and scalable to meet the demands of the organization.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Mentor junior team members to enhance their skills and knowledge in data engineering.- Continuously evaluate and improve data processes to enhance efficiency and effectiveness.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Neo4j.- Good To Have\n\n\n\n\nSkills:\nExperience with Java.- Strong understanding of data modeling and graph database concepts.- Experience with data integration tools and ETL processes.- Familiarity with data quality frameworks and best practices.- Proficient in programming languages such as Python or Scala for data manipulation.\nAdditional Information:- The candidate should have minimum 5 years of experience in Neo4j.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['scala', 'java', 'data modeling', 'python', 'neo4j', 'hive', 'pyspark', 'data warehousing', 'sql', 'spark', 'hadoop', 'data visualization', 'etl', 'big data', 'data manipulation', 'airflow', 'machine learning', 'data engineering', 'data quality', 'tableau', 'mapreduce', 'kafka', 'sqoop', 'aws', 'etl process']",2025-06-13 05:09:18
Data Engineer,HARMAN,5 - 10 years,Not Disclosed,['Bengaluru'],"-Strong analytical thinking and problem-solving skills, with the ability to translate complex data into actionable insights\n-Excellent communication skills, with the ability to effectively convey complex findings to both technical and non-technical stakeholders.\nCandidate to work form SRIB Bangalore with 3 days working from office is mandatory\n  What You Will Do",,,,"['Digital media', 'CTV', 'Analytical', 'Machine learning', 'Agile', 'Data processing', 'Automotive', 'Python']",2025-06-13 05:09:19
Big Data Developer/Data Engineer,Grid Dynamics,5 - 10 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\nExperience: 5 - 8 years\nEmployment Type: Full-Time\n\nJob Summary:\nWe are looking for a highly skilled Scala and Spark Developer to join our data engineering team. The ideal candidate will have strong experience in building scalable data processing solutions using Apache Spark and writing robust, high-performance applications in Scala. You will work closely with data scientists, data analysts, and product teams to design, develop, and optimize large-scale data pipelines and ETL workflows.\n\nKey Responsibilities:\nDevelop and maintain scalable data processing pipelines using Apache Spark and Scala.\nWork on batch and real-time data processing using Spark (RDD/DataFrame/Dataset).\nWrite efficient and maintainable code following best practices and coding standards.\nCollaborate with cross-functional teams to understand data requirements and implement solutions.\nOptimize performance of Spark jobs and troubleshoot data-related issues.\nIntegrate data from multiple sources and ensure data quality and consistency.\nParticipate in design reviews, code reviews, and provide technical leadership when needed.\nContribute to data modeling, schema design, and architecture discussions.\nRequired Skills:\nStrong programming skills in Scala.\nExpertise in Apache Spark (Core, SQL, Streaming).\nHands-on experience with distributed computing and large-scale data processing.\nExperience with data formats like Parquet, Avro, ORC, and JSON.\nGood understanding of functional programming concepts.\nFamiliarity with data ingestion tools (Kafka, Flume, Sqoop, etc.).\nExperience working with Hadoop ecosystem (HDFS, Hive, YARN, etc.) is a plus.\nStrong SQL skills and experience working with relational and NoSQL databases.\nExperience with version control tools like Git.\nPreferred Qualifications:\nBachelor's or Masters degree in Computer Science, Engineering, or related field.\nExperience with cloud platforms like AWS, Azure, or GCP (especially EMR, Databricks, etc.).\nKnowledge of containerization (Docker, Kubernetes) is a plus.\nFamiliarity with CI/CD tools and DevOps practices.ndidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Scala', 'Pyspark', 'Spark']",2025-06-13 05:09:21
Hadoop Data Engineer,Info Origin,0 - 2 years,Not Disclosed,['Gurugram'],"Job Overview:\nWe are looking for experienced Data Engineers proficient in Hadoop, Hive, Python, SQL, and Pyspark/Spark to join our dynamic team. Candidates will be responsible for designing, developing, and maintaining scalable big data solutions.\nKey Responsibilities:\nDevelop and optimize data pipelines for large-scale data processing.\nWork with structured and unstructured datasets to derive actionable insights.\nCollaborate with cross-functional teams to enhance data-driven decision-making.\nEnsure the performance, scalability, and reliability of data architectures.\nImplement best practices for data security and governance.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'Scalability', 'data security', 'spark', 'Hadoop', 'Data processing', 'big data', 'SQL', 'Python']",2025-06-13 05:09:23
Data Engineer,Capgemini,6 - 9 years,Not Disclosed,['Gurugram'],"\nesign, implement, and maintain data pipelines for data ingestion, processing, and transformation in Azure.\nWork together with data scientists and analysts to understand the needs for data and create effective data workflows.\nCreate and maintain data storage solutions including Azure SQL Database, Azure Data Lake, and Azure Blob Storage.\nUtilizing Azure Data Factory or comparable technologies, create and maintain ETL (Extract, Transform, Load) operations.\nImplementing data validation and cleansing procedures will ensure the quality, integrity, and dependability of the data.\nImprove the scalability, efficiency, and cost-effectiveness of data pipelines.\nMonitoring and resolving data pipeline problems will guarantee consistency and availability of the data.\nWorks in the area of Software Engineering, which encompasses the development, maintenance and optimization of software solutions/applications.1. Applies scientific methods to analyse and solve software engineering problems.2. He/she is responsible for the development and application of software engineering practice and knowledge, in research, design, development and maintenance.3. His/her work requires the exercise of original thought and judgement and the ability to supervise the technical and administrative work of other software engineers.4. The software engineer builds skills and expertise of his/her software engineering discipline to reach standard software engineer skills expectations for the applicable role, as defined in Professional Communities.5. The software engineer collaborates and acts as team player with other software engineers and stakeholders.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'azure data factory', 'sql', 'azure blob storage', 'sql azure', 'hive', 'azure databricks', 'python', 'data validation', 'pyspark', 'data warehousing', 'power bi', 'data engineering', 'spark', 'data ingestion', 'software engineering', 'hadoop', 'etl', 'big data', 'aws', 'sql database']",2025-06-13 05:09:24
"Data Engineer, Devices",Amazon,5 - 10 years,Not Disclosed,['Noida'],"Are you a highly skilled data engineer and project leaderDo you think big, enjoy complexity and building solutions that scaleAre you curious to know what you could achieve in a company that pushes the boundaries of modern technologyIf you answered yes and you have a background in FinTech you ll love this role and Amazon s data obsessed culture.\n\nAmazon Devices and Services Fintech is the global team that designs and builds the financial planning and analysis tools for wide variety of Amazon s new and established organizations. From Kindle to Ring and even new and exciting companies like Kuiper (our new interstellar satellite play) this team enjoys a wide variety of complex and interesting problem spaces. They are almost like FinTech consultants embedded in Amazon.\n\nThis team are looking for a Data Engineer to build and enhance the businesses finance systems with TM1 at its core. You will manage all aspects from requirements gathering, technical design, development, deployment, and integration to solve budgeting, planning, performance management and reporting challenges\n\n\nDesign and implement next generation financial solutions assisted by almost unlimited access to AWS resources including EC2, RDS, Redshift, Stepfunctions, EMR, Lambda and 3rd party software TM1.\nBuild and deliver high quality data pipelines capable of scaling from running for a single month of data during month end close to 150 and more months when doing restatements.\nContinually improve ongoing reporting and analysis processes and infrastructure, automating or simplifying self-service capabilities for customers.\nDive deep to resolve problems at their root, looking for failure patterns and suggesting and implementing fixes or enhancements.\nPrepare runbooks, methods of procedures, tutorials, training videos on best practices for global delivery.\nSolve unique challenges presented by the massive data volume and diverse data sets working for one of the largest companies in the wo 5+ years data engineering experience.\nExtensive experience writing SQL queries and stored procedures.\nExperience with big data tools and distributed computing.\nFinance experience, exhibiting knowledge of financial reporting, budgeting and forecasting functions and processes.\nBachelors degree. Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)\nExperience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions.\nExperience with programming languages such as python, java shell scripts.\nExperience with IBM Planning Analytics/TM1 both scripting processes and writing rules.\nExperience with design delivery of formal training curriculum and programs.\nProject management, scoping, reporting, and scheduling experience.",,,,"['Performance management', 'Financial reporting', 'Project management', 'Financial planning', 'Scheduling', 'Stored procedures', 'Budgeting', 'Forecasting', 'Analytics', 'Python']",2025-06-13 05:09:26
Data Engineer,AMERICAN EXPRESS,3 - 8 years,Not Disclosed,['Chennai'],"You Lead the Way. We've Got Your Back.\n\nWith the right backing, people and businesses have the power to progress in incredible ways. When you join Team Amex, you become part of a global and diverse community of colleagues with an unwavering commitment to back our customers, communities and each other. Here, youll learn and grow as we help you create a career journey thats unique and meaningful to you with benefits, programs, and flexibility that support you personally and professionally.\nAt American Express, you’ll be recognized for your contributions, leadership, and impact—every colleague has the opportunity to share in the company’s success. Together, we’ll win as a team, striving to uphold our company values and powerful backing promise to provide the world’s best customer experience every day. And we’ll do it with the utmost integrity, and in an environment where everyone is seen, heard and feels like they belong. As part of our diverse tech team, you can architect, code and ship software that makes us an essential part of our customers’ digital lives. Here, you can work alongside talented engineers in an open, supportive, inclusive environment where your voice is valued, and you make your own decisions on what tech to use to solve challenging problems. Amex offers a range of opportunities to work with the latest technologies and encourages you to back the broader engineering community through open source. And because we understand the importance of keeping your skills fresh and relevant, we give you dedicated time to invest in your professional development. Find your place in technology on #TeamAmex.",,,,"['Data Engineering', 'GCP', 'Airflow', 'Pyspark', 'Bigquery', 'Hadoop', 'Big Data', 'SQL', 'Python', 'Backend Development']",2025-06-13 05:09:28
Data Engineer,Bajaj Financial Securities,2 - 5 years,Not Disclosed,['Pune'],"We're Hiring: Data Engineer | 25 Years Experience | AWS + Real-time Focus\nJoin our fast-moving team as a Data Engineer where you'll build scalable, real-time data pipelines, own cloud infrastructure, and collaborate across teams to drive data-first decisions.\nIf you're strong in Python, experienced with streaming platforms like Kafka/Kinesis, and have shipped cloud-native data pipelines (preferably AWS) — we want to hear from you.\nMust-Haves:\n2–5 years of experience in Data Engineering\nPython (Pandas, PySpark, async), SQL, ETL/ELT\nStreaming experience (Kafka/Kinesis)\nAWS cloud stack (Glue, Lambda, S3, Athena)\nExperience in APIs, data warehousing, and data modelling\nBonus if you know: Docker, Kubernetes, Airflow/dbt, or have a background in MLOps.Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Aws Lambda', 'Docker', 'Cloud Platform', 'Data Warehousing', 'Python', 'Pyspark', 'Api Gateway', 'Kinesis', 'Kafka', 'ETL', 'SQL', 'Kubernetes']",2025-06-13 05:09:30
"Senior Staff Engineer, Big Data Engineer",Nagarro,9 - 13 years,Not Disclosed,['India'],"We're Nagarro.\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal experience 9+ years.\nExcellent knowledge and experience in Big data engineer.\nStrong working experience with architecture and development in Apache Spark, Spark, Python, Azure Databricks, Data Pipelines, Azure Devops, Kafka, SQL Server/NoSQL.\nStrong expertise in Python, Django Rest Framework, Databricks and PostgreSQL.\nHands on experience in building data pipelines and building data frameworks for unit testing, data lineage tracking, and automation.\nFamiliarity with streaming technologies (e.g., Kafka, Kinesis, Flink).\nExperience with building and maintaining a cloud system.\nFamiliarity with data modeling, data warehousing, and building distributed systems.\nExpertise in Spanner for high-availability, scalable database solutions.\nKnowledge of data governance and security practices in cloud-based environments.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\n\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them in to technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken\nCarrying out POCs to make sure that suggested design/technologies meet the requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Spark', 'Data Bricks', 'Python', 'Pyspark', 'Django Framework', 'Azure Databricks', 'SQL Server']",2025-06-13 05:09:31
Senior Big Data Engineer,Qualcomm,2 - 7 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Software Engineer, you will design, develop, create, modify, and validate embedded and cloud edge software, applications, and/or specialized utility programs that launch cutting-edge, world class products that meet and exceed customer needs. Qualcomm Software Engineers collaborate with systems, hardware, architecture, test engineers, and other teams to design system-level software solutions and obtain information on performance requirements and interfaces.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\n\nGeneral Summary\n\nPreferred Qualifications\n\n3+ years of experience as a Data Engineer or in a similar role\n\nExperience with\n\ndata modeling, data warehousing, and building ETL pipelines\n\nSolid working experience with\n\nPython, AWS analytical technologies and related resources (Glue, Athena, QuickSight, SageMaker, etc.,)\n\nExperience with\n\nBig Data tools, platforms and architecture with solid working experience with SQL\n\nExperience working in a very large data warehousing environment,\n\nDistributed System.\n\nSolid understanding on various data exchange formats and complexities\n\nIndustry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets\n\nStrong data visualization skills\n\nBasic understanding of Machine Learning; Prior experience in ML Engineering a plus\n\nAbility to manage on-premises data and make it inter-operate with AWS based pipelines\n\nAbility to interface with Wireless Systems/SW engineers and understand the Wireless ML domain; Prior experience in Wireless (5G) domain a plus\n\n\nEducation\n\nBachelor's degree in computer science, engineering, mathematics, or a related technical discipline\n\nPreferred QualificationsMasters in CS/ECE with a Data Science / ML Specialization\n\n\nMinimum Qualifications:\n\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\n\nOR\n\nMaster's degree in Engineering, Information Systems, Computer Science, or related field\n\nOR\n\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n3+ years of experience with Programming Language such as C, C++, Java, Python, etc.\n\n\nDevelops, creates, and modifies general computer applications software or specialized utility programs. Analyzes user needs and develops software solutions. Designs software or customizes software for client use with the aim of optimizing operational efficiency. May analyze and design databases within an application area, working individually or coordinating database development as part of a team. Modifies existing software to correct errors, allow it to adapt to new hardware, or to improve its performance. Analyzes user needs and software requirements to determine feasibility of design within time and cost constraints. Confers with systems analysts, engineers, programmers and others to design system and to obtain information on project limitations and capabilities, performance requirements and interfaces. Stores, retrieves, and manipulates data for analysis of system capabilities and requirements. Designs, develops, and modifies software systems, using scientific analysis and mathematical models to predict and measure outcome and consequences of design.\n\nPrincipal Duties and Responsibilities:\n\nCompletes assigned coding tasks to specifications on time without significant errors or bugs.\n\nAdapts to changes and setbacks in order to manage pressure and meet deadlines.\n\nCollaborates with others inside project team to accomplish project objectives.\n\nCommunicates with project lead to provide status and information about impending obstacles.\n\nQuickly resolves complex software issues and bugs.\n\nGathers, integrates, and interprets information specific to a module or sub-block of code from a variety of sources in order to troubleshoot issues and find solutions.\n\nSeeks others' opinions and shares own opinions with others about ways in which a problem can be addressed differently.\n\nParticipates in technical conversations with tech leads/managers.\n\nAnticipates and communicates issues with project team to maintain open communication.\n\nMakes decisions based on incomplete or changing specifications and obtains adequate resources needed to complete assigned tasks.\n\nPrioritizes project deadlines and deliverables with minimal supervision.\n\nResolves straightforward technical issues and escalates more complex technical issues to an appropriate party (e.g., project lead, colleagues).\n\nWrites readable code for large features or significant bug fixes to support collaboration with other engineers.\n\nDetermines which work tasks are most important for self and junior engineers, stays focused, and deals with setbacks in a timely manner.\n\nUnit tests own code to verify the stability and functionality of a feature.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'sql', 'software engineering', 'data visualization', 'aws', 'quicksight', 'c', 'software development', 'glue', 'aws sagemaker', 'data warehousing', 'machine learning', 'business intelligence', 'data engineering', 'java', 'data science', 'data modeling', 'athena', 'wireless', 'big data', 'etl', 'ml']",2025-06-13 05:09:33
Data Engineer,Accenture,15 - 20 years,Not Disclosed,['Pune'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Neo4j, Stardog\n\n\n\n\nGood to have skills :JavaMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand their data needs and provide effective solutions, ensuring that the data infrastructure is robust and scalable to meet the demands of the organization.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Mentor junior team members to enhance their skills and knowledge in data engineering.- Continuously evaluate and improve data processes to enhance efficiency and effectiveness.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Neo4j.- Good To Have\n\n\n\n\nSkills:\nExperience with Java.- Strong understanding of data modeling and graph database concepts.- Experience with data integration tools and ETL processes.- Familiarity with data quality frameworks and best practices.- Proficient in programming languages such as Python or Scala for data manipulation.\nAdditional Information:- The candidate should have minimum 5 years of experience in Neo4j.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['scala', 'java', 'data modeling', 'python', 'neo4j', 'hive', 'pyspark', 'data warehousing', 'sql', 'spark', 'hadoop', 'data visualization', 'etl', 'big data', 'data manipulation', 'airflow', 'machine learning', 'data engineering', 'data quality', 'tableau', 'mapreduce', 'kafka', 'sqoop', 'aws', 'etl process']",2025-06-13 05:09:35
Data Engineer,Accenture,5 - 10 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems. Your day will involve working on data solutions and ensuring data integrity and quality.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Develop and maintain data pipelines for efficient data processing.- Implement ETL processes to migrate and deploy data across systems.- Ensure data quality and integrity throughout the data solutions.- Collaborate with cross-functional teams to optimize data processes.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of data engineering principles.- Experience with cloud-based data solutions like AWS or Azure.- Knowledge of SQL and NoSQL databases.- Hands-on experience with data modeling and schema design.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'data engineering', 'sql', 'etl', 'aws', 'hive', 'python', 'data processing', 'microsoft azure', 'pyspark', 'data warehousing', 'data integrity', 'knowledge of sql', 'nosql', 'database design', 'data quality', 'data modeling', 'spark', 'hadoop', 'big data', 'etl process', 'nosql databases']",2025-06-13 05:09:37
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Informatica MDM\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to effectively migrate and deploy data across various systems, contributing to the overall efficiency and reliability of data management within the organization.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with cross-functional teams to gather requirements and translate them into technical specifications.- Monitor and optimize data pipelines for performance and reliability.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Informatica MDM.- Good To Have\n\n\n\n\nSkills:\nExperience with data warehousing concepts and practices.- Strong understanding of data modeling techniques and best practices.- Familiarity with SQL and database management systems.- Experience in implementing data governance and data quality frameworks.\nAdditional Information:- The candidate should have minimum 3 years of experience in Informatica MDM.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data warehousing', 'sql', 'data modeling', 'etl', 'informatica mdm', 'hive', 'python', 'data management', 'data engineering', 'data quality', 'tableau', 'spark', 'mdm', 'data governance', 'data warehousing concepts', 'technical specifications', 'hadoop', 'big data', 'informatica', 'etl process']",2025-06-13 05:09:39
Data Engineer,Accenture,12 - 15 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Data Engineering\n\n\n\n\nGood to have skills :Java Enterprise EditionMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand their data needs and provide effective solutions, ensuring that the data infrastructure is robust and scalable to meet the demands of the organization.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Mentor junior team members to enhance their skills and knowledge in data engineering.- Continuously evaluate and improve data processes to enhance efficiency and effectiveness.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Engineering.- Strong understanding of data modeling and database design principles.- Experience with ETL tools and frameworks.- Familiarity with cloud platforms such as AWS, Azure, or Google Cloud.- Knowledge of data warehousing concepts and technologies.\nAdditional Information:- The candidate should have minimum 12 years of experience in Data Engineering.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data engineering', 'database design', 'data modeling', 'design principles', 'aws', 'hive', 'python', 'scala', 'microsoft azure', 'data warehousing', 'java collections', 'sql', 'data quality', 'java', 'gcp', 'etl tool', 'spark', 'data warehousing concepts', 'hadoop', 'etl', 'big data', 'etl process']",2025-06-13 05:09:41
Data Engineer,Accenture,2 - 7 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Google Cloud Data Services\n\n\n\n\nGood to have skills :GCP Dataflow, Data EngineeringMinimum\n\n\n\n2 year(s) of experience is required\n\n\n\n\nEducational Qualification :standard 15 years\n\n\nSummary:As a Data Engineer, you will be responsible for designing, developing, and maintaining data solutions for data generation, collection, and processing. Your role involves creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across systems. You will play a crucial part in the data management process.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work-related problems.- Develop and maintain data solutions for data generation, collection, and processing.- Create data pipelines to streamline data flow.- Ensure data quality and integrity throughout the data lifecycle.- Implement ETL processes for data migration and deployment.- Collaborate with cross-functional teams to optimize data processes.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Google Cloud Data Services.- Good To Have\n\n\n\n\nSkills:\nExperience with Data Engineering and GCP Dataflow.- Strong understanding of cloud-based data services.- Experience in designing and implementing data pipelines.- Knowledge of ETL processes and data migration techniques.\nAdditional Information:- The candidate should have a minimum of 2 years of experience in Google Cloud Data Services.- This position is based at our Bengaluru office.- A standard 15 years of education is required.\n\nQualification\n\nstandard 15 years",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['gcp', 'etl', 'data services', 'google', 'data engineering', 'hive', 'data management', 'data warehousing', 'data migration', 'business intelligence', 'sql', 'plsql', 'data modeling', 'spark', 'hadoop', 'big data', 'python', 'sql server', 'data quality', 'tableau', 'aws', 'ssis', 'data flow', 'informatica', 'etl process']",2025-06-13 05:09:43
Data Engineer,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Data Modeling Techniques and Methodologies\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems. Your day will involve working on data architecture and collaborating with cross-functional teams to optimize data processes.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Lead data modeling initiatives to design and implement data structures.- Optimize data storage and retrieval processes.- Develop and maintain data pipelines for efficient data flow.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Modeling Techniques and Methodologies.- Strong understanding of database management systems.- Experience with data warehousing and ETL processes.- Knowledge of data governance and compliance.- Hands-on experience with data visualization tools.\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Data Modeling Techniques and Methodologies.- This position is based at our Bengaluru office.PFB Education details- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['database management system', 'data warehousing', 'data modeling', 'data visualization', 'etl', 'hive', 'python', 'data architecture', 'data engineering', 'sql', 'database management', 'data quality', 'tableau', 'spark', 'data governance', 'data structures', 'hadoop', 'big data', 'data flow', 'etl process']",2025-06-13 05:09:45
Data Engineer,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Data Architecture Principles\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. You will create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems. Your day will involve working on various data-related tasks and collaborating with teams to optimize data processes.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Develop innovative data solutions to meet business requirements.- Optimize data pipelines for efficiency and scalability.- Implement data governance policies to ensure data quality and security.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Architecture Principles.- Strong understanding of data modeling and database design.- Experience with ETL tools and processes.- Knowledge of cloud platforms and big data technologies.- Good To Have\n\n\n\n\nSkills:\nData management and governance expertise.\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Data Architecture Principles.- This position is based at our Bengaluru office.Education information - - A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data management', 'data architecture', 'database design', 'data architecture principles', 'data modeling', 'hive', 'python', 'big data technologies', 'cloud platforms', 'data engineering', 'sql', 'data quality', 'etl tool', 'spark', 'data governance', 'hadoop', 'etl', 'big data', 'etl process']",2025-06-13 05:09:46
Data Engineer,Accenture,15 - 20 years,Not Disclosed,['Bhubaneswar'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :PySpark, Microsoft Azure Databricks, Microsoft Azure Analytics Services, Microsoft Azure Data Services\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Mentor junior team members to enhance their skills and knowledge.- Continuously evaluate and improve data processes to enhance efficiency.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in PySpark, Microsoft Azure Databricks, Microsoft Azure Data Services, Microsoft Azure Analytics Services.- Strong experience in designing and implementing data pipelines.- Proficient in data modeling and database design.- Familiarity with data warehousing concepts and technologies.- Experience with data quality and data governance practices.\nAdditional Information:- The candidate should have minimum 5 years of experience in PySpark.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure databricks', 'data services', 'pyspark', 'microsoft azure', 'data modeling', 'hive', 'python', 'analytics services', 'azure analytics', 'data warehousing', 'data engineering', 'sql', 'database design', 'data quality', 'spark', 'data governance', 'data warehousing concepts', 'etl', 'etl process']",2025-06-13 05:09:48
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Hyderabad'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Informatica Data Quality\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to effectively migrate and deploy data across various systems. You will collaborate with team members to enhance data workflows and contribute to the overall efficiency of data management practices.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist in the design and implementation of data architecture to support data initiatives.- Monitor and optimize data pipelines for performance and reliability.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Informatica Data Quality.- Strong understanding of data integration techniques and ETL processes.- Experience with data profiling and data cleansing methodologies.- Familiarity with database management systems and SQL.- Knowledge of data governance and data quality best practices.\nAdditional Information:- The candidate should have minimum 3 years of experience in Informatica Data Quality.- This position is based at our Hyderabad office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['informatica data quality', 'sql', 'etl', 'data integration', 'etl process', 'hive', 'python', 'data management', 'data architecture', 'data engineering', 'data cleansing', 'database management', 'profiling', 'spark', 'data governance', 'hadoop', 'big data', 'informatica', 'data profiling']",2025-06-13 05:09:50
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Navi Mumbai'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:09:52
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Chennai'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:09:54
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Pune'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to effectively migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and contribute to the overall data strategy of the organization, ensuring that data solutions are efficient, scalable, and aligned with business objectives.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with stakeholders to gather and analyze data requirements.- Design and implement robust data pipelines to support data processing and analytics.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of data modeling and database design principles.- Experience with ETL tools and data integration techniques.- Familiarity with cloud platforms and services related to data storage and processing.- Knowledge of programming languages such as Python or Scala for data manipulation.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data analytics', 'database design', 'data modeling', 'design principles', 'hive', 'scala', 'data manipulation', 'data processing', 'pyspark', 'data warehousing', 'data engineering', 'sql', 'data quality', 'tableau', 'etl tool', 'spark', 'hadoop', 'etl', 'big data', 'data integration', 'etl process']",2025-06-13 05:09:56
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Indore'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:09:58
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Bhubaneswar'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:10:00
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Bhubaneswar'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:10:02
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Pune'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:10:03
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Chennai'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:10:05
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Hyderabad'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:10:07
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Navi Mumbai'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:10:09
Data Engineer,Capgemini,6 - 9 years,Not Disclosed,['Hyderabad'],"\nDesign, implement, and maintain data pipelines for data ingestion, processing, and transformation in Azure.\nWork together with data scientists and analysts to understand the needs for data and create effective data workflows.\nCreate and maintain data storage solutions including Azure SQL Database, Azure Data Lake, and Azure Blob Storage.\nUtilizing Azure Data Factory or comparable technologies, create and maintain ETL (Extract, Transform, Load) operations.\nImplementing data validation and cleansing procedures will ensure the quality, integrity, and dependability of the data.\nImprove the scalability, efficiency, and cost-effectiveness of data pipelines.\nMonitoring and resolving data pipeline problems will guarantee consistency and availability of the data.\nWorks in the area of Software Engineering, which encompasses the development, maintenance and optimization of software solutions/applications.1. Applies scientific methods to analyse and solve software engineering problems.2. He/she is responsible for the development and application of software engineering practice and knowledge, in research, design, development and maintenance.3. His/her work requires the exercise of original thought and judgement and the ability to supervise the technical and administrative work of other software engineers.4. The software engineer builds skills and expertise of his/her software engineering discipline to reach standard software engineer skills expectations for the applicable role, as defined in Professional Communities.5. The software engineer collaborates and acts as team player with other software engineers and stakeholders.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'azure data factory', 'sql', 'azure blob storage', 'sql azure', 'hive', 'azure databricks', 'python', 'data validation', 'pyspark', 'data warehousing', 'power bi', 'data engineering', 'spark', 'data ingestion', 'software engineering', 'hadoop', 'etl', 'big data', 'aws', 'sql database']",2025-06-13 05:10:11
Data Engineer,Accenture,15 - 20 years,Not Disclosed,['Hyderabad'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Microsoft Azure Data Services\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n2 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to effectively migrate and deploy data across various systems, contributing to the overall efficiency and reliability of data management within the organization.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with cross-functional teams to gather requirements and deliver data solutions that meet business needs.- Monitor and optimize data pipelines for performance and reliability.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Microsoft Azure Data Services.- Good To Have\n\n\n\n\nSkills:\nExperience with Azure Data Factory, Azure SQL Database, and Azure Synapse Analytics.- Strong understanding of data modeling and database design principles.- Experience with data integration and ETL tools.- Familiarity with data governance and data quality best practices.\nAdditional Information:- The candidate should have minimum 2 years of experience in Microsoft Azure Data Services.- This position is based at our Hyderabad office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data services', 'microsoft azure', 'database design', 'data modeling', 'design principles', 'python', 'data management', 'azure synapse', 'azure data factory', 'data engineering', 'sql', 'data quality', 'sql azure', 'etl tool', 'data governance', 'etl', 'data integration', 'etl process', 'sql database']",2025-06-13 05:10:13
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Indore'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Google BigQuery\n\n\n\n\nGood to have skills :Microsoft SQL Server, Google Cloud Data ServicesMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will be responsible for designing, developing, and maintaining data solutions for data generation, collection, and processing. You will create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Develop and maintain data pipelines.- Ensure data quality throughout the data lifecycle.- Implement ETL processes for data migration and deployment.- Collaborate with cross-functional teams to understand data requirements.- Optimize data storage and retrieval processes.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Google BigQuery.- Strong understanding of data engineering principles.- Experience with cloud-based data services.- Knowledge of SQL and database management systems.- Hands-on experience with data modeling and schema design.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Google BigQuery.- This position is based at our Mumbai office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data engineering', 'sql', 'data modeling', 'bigquery', 'etl', 'schema', 'hive', 'data services', 'python', 'amazon redshift', 'data warehousing', 'google', 'data migration', 'knowledge of sql', 'sql server', 'database design', 'data quality', 'tableau', 'spark', 'etl tool', 'hadoop', 'big data', 'aws', 'etl process']",2025-06-13 05:10:15
Data Engineer,Accenture,2 - 3 years,Not Disclosed,['Kochi'],"Job Title - Data Engineer Sr.Analyst ACS SONG\n\n\n\nManagement Level:Level 10 Sr. Analyst\n\n\n\nLocation:Kochi, Coimbatore, Trivandrum\n\n\n\nMust have skills:Python/Scala, Pyspark/Pytorch\n\n\n\n\nGood to have skills:Redshift\n\n\n\n\n\n\n\nJob\n\n\nSummary\n\nYoull capture user requirements and translate them into business and digitally enabled solutions across a range of industries. Your responsibilities will include:\n\n\n\nRoles and Responsibilities\n\nDesigning, developing, optimizing, and maintaining data pipelines that adhere to ETL principles and business goals\n\nSolving complex data problems to deliver insights that helps our business to achieve their goals.\n\nSource data (structured unstructured) from various touchpoints, format and organize them into an analyzable format.\n\nCreating data products for analytics team members to improve productivity\n\nCalling of AI services like vision, translation etc. to generate an outcome that can be used in further steps along the pipeline.\n\nFostering a culture of sharing, re-use, design and operational efficiency of data and analytical solutions\n\nPreparing data to create a unified database and build tracking solutions ensuring data quality\n\nCreate Production grade analytical assets deployed using the guiding principles of CI/CD.\n\n\n\n\nProfessional and Technical Skills\n\nExpert in Python, Scala, Pyspark, Pytorch, Javascript (any 2 at least)\n\nExtensive experience in data analysis (Big data- Apache Spark environments), data libraries (e.g. Pandas, SciPy, Tensorflow, Keras etc.), and SQL. 2-3 years of hands-on experience working on these technologies.\n\nExperience in one of the many BI tools such as Tableau, Power BI, Looker.\n\nGood working knowledge of key concepts in data analytics, such as dimensional modeling, ETL, reporting/dashboarding, data governance, dealing with structured and unstructured data, and corresponding infrastructure needs.\n\nWorked extensively in Microsoft Azure (ADF, Function Apps, ADLS, Azure SQL), AWS (Lambda,Glue,S3), Databricks analytical platforms/tools, Snowflake Cloud Datawarehouse.\n\n\n\n\nAdditional Information\n\nExperience working in cloud Data warehouses like Redshift or Synapse\n\nCertification in any one of the following or equivalent\n\nAWS- AWS certified data Analytics- Speciality\n\nAzure- Microsoft certified Azure Data Scientist Associate\n\nSnowflake- Snowpro core- Data Engineer\n\nDatabricks Data Engineering\n\n\nAbout Our Company | Accenture (do not remove the hyperlink)\n\n\n\nQualification\n\n\n\nExperience:3.5 -5 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scala', 'pyspark', 'pytorch', 'python', 'microsoft azure', 'glue', 'amazon redshift', 'sql', 'tensorflow', 'sql azure', 'spark', 'keras', 'big data', 'etl', 'snowflake', 'scipy', 'data analysis', 'azure data lake', 'power bi', 'data engineering', 'javascript', 'data bricks', 'pandas', 'tableau', 'lambda expressions', 'aws']",2025-06-13 05:10:17
Data Engineer,Accenture,5 - 10 years,Not Disclosed,['Chennai'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems. Your day will involve working on data architecture and engineering tasks to support business operations and decision-making.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Develop and maintain data pipelines for efficient data processing.- Implement ETL processes to ensure seamless data migration and deployment.- Collaborate with cross-functional teams to design and optimize data solutions.- Conduct data quality assessments and implement improvements for data integrity.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of data architecture principles.- Experience in designing and implementing data solutions.- Proficient in SQL and other data querying languages.- Knowledge of cloud platforms such as AWS or Azure.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Chennai office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'sql', 'data architecture principles', 'etl', 'aws', 'hive', 'python', 'data processing', 'airflow', 'microsoft azure', 'pyspark', 'data warehousing', 'data integrity', 'data migration', 'data engineering', 'data quality', 'spark', 'hadoop', 'business operations', 'big data', 'etl process']",2025-06-13 05:10:19
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Coimbatore'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Talend ETL\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems. Be involved in the end-to-end data management process.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work-related problems.- Develop and maintain data pipelines for efficient data processing.- Ensure data quality and integrity throughout the data lifecycle.- Implement ETL processes to extract, transform, and load data.- Collaborate with cross-functional teams to optimize data solutions.- Conduct data analysis to identify trends and insights.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Talend ETL.- Strong understanding of data integration and ETL processes.- Experience with data modeling and database design.- Knowledge of SQL and database querying languages.- Hands-on experience with data warehousing concepts.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Talend ETL.- This position is based at our Hyderabad office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['sql', 'talend etl', 'etl', 'data integration', 'etl process', 'hive', 'python', 'data analysis', 'data management', 'talend', 'data processing', 'data warehousing', 'knowledge of sql', 'data engineering', 'database design', 'data quality', 'data modeling', 'spark', 'data warehousing concepts', 'hadoop']",2025-06-13 05:10:21
Data Engineer,AMERICAN EXPRESS,2 - 4 years,13-17 Lacs P.A.,"['Gurugram', 'Delhi / NCR']","Role & responsibilities\nUnderstanding business use cases and be able to convert to technical design\nPart of a cross-disciplinary team, working closely with other data engineers, software engineers, data scientists, data managers and business partners.\nYou will be designing scalable, testable and maintainable data pipelines\nIdentify areas for data governance improvements and help to resolve data quality problems through the appropriate choice of error detection and correction, process control and improvement, or process design changes",,,,"['Spark', 'SQL', 'Python', 'Hadoop', 'Big Data']",2025-06-13 05:10:22
Data Engineer,Grid Dynamics,4 - 9 years,Not Disclosed,['Bengaluru'],"Required Qualifications:\n4+ years of professional experience in data engineering and data analysis roles.\nStrong proficiency in SQL and experience with database management systems such as MySQL, PostgreSQL, Oracle, and MongoDB.\nHands-on experience with big data tools like Hadoop and Apache Spark.\nProficient in Python programming.\nExperience with data visualization tools such as Tableau, Power BI, and Jupyter Notebooks.\nProven ability to design, build, and maintain scalable ETL pipelines using tools like Apache Airflow, DBT, Composer (GCP), Control-M, Cron, and Luigi.\nFamiliarity with data engineering tools including Hive, Kafka, Informatica, Talend, SSIS, and Dataflow.\nExperience working with cloud data warehouses and services (Snowflake, Redshift, BigQuery, AWS Glue, GCP Dataflow, Azure Data Factory).\nUnderstanding of data modeling concepts and data lake/data warehouse architectures.\nExperience supporting CI/CD practices with Git, Docker, Terraform, and DevOps workflows.\nKnowledge of both relational and NoSQL databases, including PostgreSQL, BigQuery, MongoDB, and DynamoDB.\nExposure to Agile and DevOps methodologies.\nExperience with at least one cloud platform:\nGoogle Cloud Platform (BigQuery, Dataflow, Composer, Cloud Storage, Pub/Sub)\nAmazon Web Services (S3, Glue, Redshift, Lambda, Athena)\nMicrosoft Azure (Data Factory, Synapse Analytics, Blob Storage)\nEssential functions\nKey Responsibilities:\nDesign, develop, and maintain robust, scalable ETL pipelines using Apache Airflow, DBT, Composer (GCP), Control-M, Cron, Luigi, and similar tools.\nBuild and optimize data architectures including data lakes and data warehouses.\nIntegrate data from multiple sources ensuring data quality and consistency.\nCollaborate with data scientists, analysts, and stakeholders to translate business requirements into technical solutions.\nAnalyze complex datasets to identify trends, generate actionable insights, and support decision-making.\nDevelop and maintain dashboards and reports using Tableau, Power BI, and Jupyter Notebooks for visualization and pipeline validation.\nManage and optimize relational and NoSQL databases such as MySQL, PostgreSQL, Oracle, MongoDB, and DynamoDB.\nWork with big data tools and frameworks including Hadoop, Spark, Hive, Kafka, Informatica, Talend, SSIS, and Dataflow.\nUtilize cloud data services and warehouses like AWS Glue, GCP Dataflow, Azure Data Factory, Snowflake, Redshift, and BigQuery.\nSupport CI/CD pipelines and DevOps workflows using Git, Docker, Terraform, and related tools.\nEnsure data governance, security, and compliance standards are met.\nParticipate in Agile and DevOps processes to enhance data engineering workflows.\nQualifications\nData Engineer with experience in MySQL or SQL or PL/SQL and any cloud experience like GCP or AWS or Azure\nWould be a plus\nPreferred Skills:\nStrong problem-solving and communication skills.\nAbility to work independently and collaboratively in a team environment.\nExperience with service development, REST APIs, and automation testing is a plus.\nFamiliarity with version control systems and workflow automation.\nWe offer\nOpportunity to work on bleeding-edge projects\nWork with a highly motivated and dedicated team\nCompetitive salary\nFlexible schedule\nBenefits package - medical insurance, sports\nCorporate social events\nProfessional development opportunities\nWell-equipped office",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'Automation', 'Data modeling', 'MySQL', 'Workflow', 'Informatica', 'Oracle', 'Apache', 'SSIS', 'Analytics']",2025-06-13 05:10:24
Data Engineer,Infiniti Research,3 - 7 years,22.5-25 Lacs P.A.,['Bengaluru'],"Role & responsibilities\n3-6 years of experience in Data Engineering Pipeline Ownership and Quality Assurance, with hands-on expertise in building, testing, and maintaining data pipelines.\nProficiency with Azure Data Factory (ADF), Azure Databricks (ADB), and PySpark for data pipeline orchestration and processing large-scale datasets.\nStrong experience in writing SQL queries and performing data validation, data profiling, and schema checks.\nExperience with big data validation, including schema enforcement, data integrity checks, and automated anomaly detection.\nAbility to design, develop, and implement automated test cases to monitor and improve data pipeline efficiency.\nDeep understanding of Medallion Architecture (Raw, Bronze, Silver, Gold) for structured data flow management.\nHands-on experience with Apache Airflow for scheduling, monitoring, and managing workflows.\nStrong knowledge of Python for developing data quality scripts, test automation, and ETL validations.\nFamiliarity with CI/CD pipelines for deploying and automating data engineering workflows.\nSolid data governance and data security practices within the Azure ecosystem.\n\nAdditional Requirements:\nOwnership of data pipelines ensuring end-to-end execution, monitoring, and troubleshooting failures proactively.\nStrong stakeholder management skills, including follow-ups with business teams across multiple regions to gather requirements, address issues, and optimize processes.\nTime flexibility to align with global teams for efficient communication and collaboration.\nExcellent problem-solving skills with the ability to simulate and test edge cases in data processing environments.\nStrong communication skills to document and articulate pipeline issues, troubleshooting steps, and solutions effectively.\nExperience with Unity Catalog or willingness to learn.\n\nPreferred candidate profile\nImmediate Joiner's",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ADF', 'pyspark', 'Unity Catalog', 'ADB', 'SQL', 'Medallion Architecture']",2025-06-13 05:10:26
Data Engineer,Grid Dynamics,4 - 9 years,Not Disclosed,['Bengaluru'],"Qualifications we are looking for\nMaster/Bachelor degree in Computer Science, Electrical Engineering, Information Systems or other technical discipline; advanced degree preferred.\nMinimum of 7+ years of software development experience (with a concentration in data centric initiatives), with demonstrated expertise in leveraging standard development best practice methodologies.\nMinimum 4+ years of experience in Hadoop using Core Java Programming, Spark, Scala, Hive and Go lang\nExpertise in Object Oriented Programming Language Java\nExperience using CI/CD Process, version control and bug tracking tools.\nExperience in handling very large data volume in Real Time and batch mode.\nExperience with automation of job execution and validation\nStrong knowledge of Database concepts\nStrong team player.\nStrong communication skills with proven ability to present complex ideas and document in a clear and concise way.\nQuick learner; self-starter, detailed and in-depth.",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['Data Engineering', 'SCALA', 'Bigdata Technologies', 'Spark']",2025-06-13 05:10:27
Data Engineer,Dun & Bradstreet,5 - 9 years,Not Disclosed,['Hyderabad'],"Key Responsibilities:\n1. Design, build, and deploy new data pipelines within our Big Data Eco-Systems using Streamsets/Talend/Informatica BDM etc. Document new/existing pipelines, Datasets.\n2. Design ETL/ELT data pipelines using StreamSets, Informatica or any other ETL processing engine. Familiarity with Data Pipelines, Data Lakes and modern Data Warehousing practices (virtual data warehouse, push down analytics etc.)\n3. Expert level programming skills on Python\n4. Expert level programming skills on Spark\n5. Cloud Based Infrastructure: GCP\n6. Experience with one of the ETL Informatica, StreamSets in creation of complex parallel loads, Cluster Batch Execution and dependency creation using Jobs/Topologies/Workflows etc.,\n7. Experience in SQL and conversion of SQL stored procedures into Informatica/StreamSets, Strong exposure working with web service origins/targets/processors/executors, XML/JSON Sources and Restful APIs.\n8. Strong exposure working with relation databases DB2, Oracle & SQL Server including complex SQL constructs and DDL generation.\n9. Exposure to Apache Airflow for scheduling jobs\n10. Strong knowledge of Big data Architecture (HDFS), Cluster installation, configuration, monitoring, cluster security, cluster resources management, maintenance, and performance tuning\n11. Create POCs to enable new workloads and technical capabilities on the Platform.\n12. Work with the platform and infrastructure engineers to implement these capabilities in production.\n13. Manage workloads and enable workload optimization including managing resource allocation and scheduling across multiple tenants to fulfill SLAs.\n14. Participate in planning activities, Data Science and perform activities to increase platform skills\n\nKey Requirements:\n1. Minimum 6 years of experience in ETL/ELT Technologies, preferably StreamSets/Informatica/Talend etc.,\n2. Minimum of 6 years hands-on experience with Big Data technologies e.g. Hadoop, Spark, Hive.\n3. Minimum 3+ years of experience on Spark\n4. Minimum 3 years of experience in Cloud environments, preferably GCP\n5. Minimum of 2 years working in a Big Data service delivery (or equivalent) roles focusing on the following disciplines:\n6. Any experience with NoSQL and Graph databases\n7. Informatica or StreamSets Data integration (ETL/ELT)\n8. Exposure to role and attribute based access controls\n9. Hands on experience with managing solutions deployed in the Cloud, preferably on GCP\n10. Experience working in a Global company, working in a DevOps model is a plus",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Airflow', 'GCP', 'Data engineer', 'Spark', 'ETL']",2025-06-13 05:10:29
Data Engineer || Paisabazaar || Gurgaon,Paisabazaar,3 - 5 years,Not Disclosed,['Gurugram'],"Qualifications for Data Engineer :\n3+ Years of experience in building and optimizing big data solutions required to fulfill business and technology requirements.\n4+ years of technical expertise in areas of design and implementation using big data technology Hadoop, Hive, Spark, Python/Java.\nStrong analytic skills to understand and create solutions for business use cases.\nEnsure best practices to implement data governance principles, data quality checks on each data layer.",,,,"['Pyspark', 'Data Engineering', 'Hadoop', 'Hive', 'Java', 'Scala Programming', 'Big Data', 'SQL', 'Azure Cloud', 'GCP', 'Spark', 'AWS', 'Python']",2025-06-13 05:10:31
DataBricks - Data Engineering Professional,Wipro,3 - 5 years,Not Disclosed,['Pune'],"Role Purpose\nThe purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\n\nDo\n1. Instrumental in understanding the requirements and design of the product/ software\n\n\n\n\n\n\n\n\n\n\nMandatory Skills: DataBricks - Data Engineering. Experience: 3-5 Years.",,,,"['Data Engineering', 'data bricks', 'software development life cycle', 'continuous integration', 'software development', 'mis', 'software management', 'root cause analysis']",2025-06-13 05:10:33
Data Engineer,ZS,1 - 6 years,Not Disclosed,['Pune'],"Create and maintain optimal data pipeline architecture.\nIdentify, design, and implement internal process improvements, automating manual processes, optimizing data delivery, re-designing infrastructure for scalability.\nDesign, develop and deploy high volume ETL pipelines to manage complex and near-real time data collection.\nDevelop and optimize SQL queries and stored procedures to meet business requirements.\nDesign, implement, and maintain REST APIs for data interaction between systems.\nEnsure performance, security, and availability of databases.",,,,"['Root cause analysis', 'SQL database', 'Management consulting', 'Financial planning', 'Data collection', 'Manager Technology', 'Engineering Manager', 'Information technology', 'Analytics']",2025-06-13 05:10:35
Data Engineer,Kinara Capital,0 - 1 years,3-5 Lacs P.A.,['Bengaluru'],"1\nAbout Company\nKinara Capital is a FinTech NBFC dedicated to driving Financial Inclusion\nin the MSME sector. Our mission is to transform lives, livelihoods, and\nlocal economies by providing fast and flexible loans without property\ncollateral to small business entrepreneurs. Led by a women-majority\nmanagement team, Kinara Capital values diversity and inclusion and\nfosters a collaborative working environment.\nKinara Capital is the only company from India recognized globally by the\nWorld Bank/IFC with a gold award in 2019 as Bank of the Year-Asia’ for\nour innovative work in SME financing. Kinara Capital is an RBI-registered\nSystemically Important NBFC.\nHeadquartered in Bangalore, we have 110 branches across Karnataka,\nGujarat, Maharashtra, Andhra Pradesh, Telangana, Tamil Nadu, and UT\nPuducherry with more than 1000 employees. https://kinaracapital.com/\nTitle:\nData Engineer\nTeam:\nData Warehouse Team\nPurpose of Job:\nThis is a hands-on coding and designing position and we are looking for\nan exceptionally talented data engineer who has exposure in\nimplementing AWS services to build data pipelines, api integration and\ndesigning data warehouse.\nJob Responsibilities:\nExcellent coding skills in Python, PySpark, SQL.\nHave extensive experience in Spark ecosystem and has\nworked on both real time and batch processing\nHave experience in AWS Glue, EMR, DMS, Lambda, S3,\nDynamoDB, Step functions, Airflow, RDS, Aurora etc.\nExperience with modern Database systems such as\nRedshift, Presto, Hive etc.\nWorked on building data lakes in the past on S3 or\nApache Hudi\nSolid understanding of Data Warehousing Concepts\nGood to have experience on tools such as Kafka or Kinesis\nGood to have AWS Developer Associate or Solutions\nArchitect Associate Certification\nQualifications:\nAt least a bachelor’s degree in Science, Engineering, Applied\nMathematics.\nOther Requirements:\nLearning Attitude and good communication skills\nReport to:\nLead Data Engineer\nPlace of work:\nHead office, Bangalore.\nJob Type:\nFull Time\nNo. of Posts:\n2",Industry Type: FinTech / Payments,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Amazon Redshift', 'Data Warehousing', 'Spark', 'Python', 'SQL', 'Snowflake', 'ETL']",2025-06-13 05:10:37
DE&A - Core - Big Data Engineering - ETL Orchestration DE&A - Core,Zensar,5 - 9 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Design of complex ETL interfaces with agnostic tool set for various source/target types (SAP BODS preferred)\nPerformance Tuning & Troubleshooting skills across all technologies used\nStrong in DB and SQLs with decent data modeling skills\nAbility to lead developers and QA and adhere to committed timelines\nAgile experience is preferred\nAbility to work with upstream, downstream, and reporting system stakeholders and convert business requirements into technical requirements (JIRA stories)\n10 years of experience in:\n- ETL tools\n- Repository maintenance\n- Migration of jobs/workflows/dataflows\n- Job monitoring, break fix, user maintenance\n- CDC (Change Data Capture)\n- Oracle Golden Gate\n- Working with different sources/targets (Oracle, SAP, Files as source; SQL Server, file uploads as targets)\nRole Description :\n- Responsible for planning, developing, implementing, and managing data warehouse project deliverables\n- Design documents and high-level data mapping for ETL specifications\n- Create transformations in ETL jobs to achieve data cleansing and standardizations during initial data load\n- Provide support for integration testing, defect fixing, and deployment\nProficient knowledge in:\n- Transformations like Query Push down, SQL Table Comparison, Pivot, Look up, etc.\n- Databases like Oracle, SQL Queries, SQL Server\n- Dimensional modeling, data mining, and data warehouse concepts\nExcellent analytical skills\nKnowledge to transfer data from Oracle, SQL Server tables, and Web Services either incrementally (Delta Load) or full load to the data warehouse on a periodic basis\nTroubleshooting existing ETL jobs and improving performance of existing jobs\nCreating and loading data into aggregate tables using transformations\nAbility to perform tasks individually and independently\nDesign of complex ETL interfaces with agnostic tool set for various source/target types (SAP BODS preferred)\nPerformance Tuning & Troubleshooting skills across all technologies used\nStrong in DB and SQLs with decent data modeling skills\nAbility to lead developers and QA and adhere to committed timelines\nAgile experience is preferred\nAbility to work with upstream, downstream, and reporting system stakeholders and convert business requirements into technical requirements (JIRA stories)\n10 years of experience in:\n- ETL tools\n- Repository maintenance\n- Migration of jobs/workflows/dataflows\n- Job monitoring, break fix, user maintenance\n- CDC (Change Data Capture)\n- Oracle Golden Gate\n- Working with different sources/targets (Oracle, SAP, Files as source; SQL Server, file uploads as targets)\nRole Description :\n- Responsible for planning, developing, implementing, and managing data warehouse project deliverables\n- Design documents and high-level data mapping for ETL specifications\n- Create transformations in ETL jobs to achieve data cleansing and standardizations during initial data load\n- Provide support for integration testing, defect fixing, and deployment\nProficient knowledge in:\n- Transformations like Query Push down, SQL Table Comparison, Pivot, Look up, etc.\n- Databases like Oracle, SQL Queries, SQL Server\n- Dimensional modeling, data mining, and data warehouse concepts\nExcellent analytical skills\nKnowledge to transfer data from Oracle, SQL Server tables, and Web Services either incrementally (Delta Load) or full load to the data warehouse on a periodic basis\nTroubleshooting existing ETL jobs and improving performance of existing jobs\nCreating and loading data into aggregate tables using transformations\nAbility to perform tasks individually and independently",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'SAP', 'Data modeling', 'Integration testing', 'Agile', 'data mapping', 'Troubleshooting', 'Data mining', 'JIRA', 'Monitoring']",2025-06-13 05:10:38
Senior Cloud Data Engineer,PwC India,10 - 15 years,12-22 Lacs P.A.,['Bengaluru'],"Role & responsibilities\n\nExperienced Senior Data Engineer utilizing Big Data & Google Cloud technologies to develop large scale, on-cloud data processing pipelines and data warehouses with Overall 12 to 15 years of experience\nHave 3 to 4 years of experience of leading Data Engineer teams developing enterprise grade data processing pipelines on multi Clouds like GCP and AWS",,,,"['Pyspark', 'Python', 'SQL', 'Datafactory', 'GCP', 'Microsoft Azure', 'ETL', 'AWS', 'Data Bricks']",2025-06-13 05:10:40
"Senior Data Engineer ( T-SQL & SSIS,Data Warehousing & ETL Specialist)",Synechron,5 - 10 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Job Summary\nSynechron is seeking a highly skilled Senior Data Engineer specializing in T-SQL and SSIS to lead and advance our data integration and warehousing initiatives. In this role, you will design, develop, and optimize complex ETL processes and database solutions to support enterprise data needs. Your expertise will enable efficient data flow, ensure data integrity, and facilitate actionable insights, contributing to our organizations commitment to data-driven decision-making and operational excellence.\nSoftware Requirements\nOverall Responsibilities\nTechnical Skills (By Category)\nExperience Requirements\nDay-to-Day Activities\nQualifications\nProfessional Competencies",,,,"['Data Engineering', 'T-SQL', 'Azure Data Factory', 'query optimization', 'performance tuning', 'database security', 'AWS Glue', 'Data Warehousing', 'SSIS', 'ETL']",2025-06-13 05:10:42
Sr. Data Engineer (Analyst- BI/Visualization),Visa,5 - 10 years,Not Disclosed,['Bengaluru'],"We are looking for a seasoned individual contributor who is comfortable engaging with business owners, data enthusiasts, and technology teams. Successful candidates have strong experience in developing complex data solutions for business intelligence applications. This role will be hands-on and focused on building data pipelines, business intelligence solutions at scale and with a focus on sustained operational excellence.\nHelp improve Visa s decision-making by making data more accessible and relevant to key partners\nDevelop deep partnerships with engineering, finance, and product teams to deliver on major cross-functional solution development\nWork with a team of talented data and analytics engineers with the ability to not only keep up with, but also pioneer, in this space.\nCreate extract, transform, load (ETL) processes for easy ingestion and use\nDevelop visualizations to make your sophisticated analyses accessible to a broad audience\nFind opportunities to create, automate and scale repeatable financial and statistical analysis\nProvide technical leadership in a team that generates business insights based on big data, identify impactful recommendations, and communicate the findings to clients\nBrainstorm innovative ways to use our unique data to answer business problems\nCollaborate with and influence leadership so that your teams work directly impacts company strategy and direction.\nCommunicate effectively to all levels of the organization, including executives.\n\n\nBasic Qualifications\n5 or more years of work experience with a Bachelors Degree or an Advanced Degree (e.g. Masters, MBA, JD, MD, or PhD)\nPreferred Qualifications\n5 or more years of work experience with a Bachelor s Degree or more than 2 years of work experience with an Advanced Degree (e.g. Masters, MBA, JD, MD)\n4+ years experience in data-based decision-making or quantitative analysis\nMaster s degree in Statistics, Operations Research, Applied Mathematics, Economics, Data Science, Business Analytics, Computer Science, or a related technical field\nExposure to Financial Services/ Payments data analytics and ETL development.\nExperience and expertise with data visualization using Tableau, Power BI or similar tools\nExperience in implementing ETL pipelines in Spark, Python, HIVE or SAS that process transaction and account level data and standardization of data pipelines.\nAdvanced experience in writing and optimizing efficient SQL queries with Python, Spark, Hive, Scala handling Large Data Sets in Big-Data Environments.\nExperience with Unix/Shell and exposure to Scheduling tools like Oozie and Airflow.\nStrong written, verbal, and interpersonal skills needed to effectively communicate technical insights and recommendations with business customers and leadership team.\nGood business acumen to orient data analysis to business needs of clients.\nAbility to translate data and technical concepts into requirements documents, business cases and user stories.\nAbility to learn new tools and paradigms as data science continues to evolve at Visa and elsewhere.\nDemonstrated intellectual and analytical rigor, team oriented, energetic, collaborative, diplomatic, and flexible style.\nPrevious experience with product valuations, financial engineering, customer lifetime values or net present value methodologies",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Unix', 'Computer science', 'Data analysis', 'SAS', 'Business analytics', 'Analytical', 'Scheduling', 'Business intelligence', 'Financial services', 'Python']",2025-06-13 05:10:44
IN Senior Associate Azure Data Engineer Data & Analaytics,PwC Service Delivery Center,2 - 5 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nData, Analytics & AI\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\n& Summary A career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.\nResponsibilities\nDesign, develop, and optimize data pipelines and ETL processes using PySpark or Scala to extract, transform, and load large volumes of structured and unstructured data from diverse sources.\nImplement data ingestion, processing, and storage solutions on Azure cloud platform, leveraging services such as Azure Databricks, Azure Data Lake Storage, and Azure Synapse Analytics.\nDevelop and maintain data models, schemas, and metadata to support efficient data access, query performance, and analytics requirements.\nMonitor pipeline performance, troubleshoot issues, and optimize data processing workflows for scalability, reliability, and costeffectiveness.\nImplement data security and compliance measures to protect sensitive information and ensure regulatory compliance.\nRequirement\nProven experience as a Data Engineer, with expertise in building and optimizing data pipelines using PySpark, Scala, and Apache Spark.\nHandson experience with cloud platforms, particularly Azure, and proficiency in Azure services such as Azure Databricks, Azure Data Lake Storage, Azure Synapse Analytics, and Azure SQL Database.\nStrong programming skills in Python and Scala, with experience in software development, version control, and CI/CD practices.\nFamiliarity with data warehousing concepts, dimensional modeling, and relational databases (e.g., SQL Server, PostgreSQL, MySQL).\nExperience with big data technologies and frameworks (e.g., Hadoop, Hive, HBase) is a plus.\nMandatory skill sets\nSpark, Pyspark, Azure\nPreferred skill sets\nSpark, Pyspark, Azure\nYears of experience required\n4 8\nEducation qualification\nB.Tech / M.Tech / MBA / MCA\nEducation\nDegrees/Field of Study required Bachelor of Technology, Master of Business Administration\nDegrees/Field of Study preferred\nRequired Skills\nMicrosoft Azure\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data management', 'Data modeling', 'Postgresql', 'MySQL', 'Database administration', 'Agile', 'Apache', 'Business intelligence', 'SQL', 'Python']",2025-06-13 05:10:46
Senior Data Engineer,Robert Bosch Engineering and Business Solutions Private Limited,4 - 8 years,Not Disclosed,['Bengaluru'],"As a Data engineer in our team, you work with large scale manufacturing data coming from our globally distributed plants. You will focus on building efficient, scalable data-driven applications.\nThe data sets produced by these applications - whether data streams or data at rest - need to be highly available, reliable, consistent and quality-assured so that they can serve as input to wide range of other use cases and downstream applications.\nWe run these applications on Azure databricks, you will be building applications, you will also contribute to scaling the platform including topics such as automation and observability.\nFinally, you are expected to interact with customers and other technical teams e. g. for requirements clarification definition of data models.\nPrimary responsibilities:\nBe a key contributor to the Bosch hybrid cloud data platform (on-prem cloud)\nDesigning building data pipelines on a global scale, ranging from small to huge datasets\nDesign applications and data models based on deep business understanding and customer requirements\nDirectly work with architects and technical leadership to design implement applications and / or architectural components\nArchitectural proposal and estimation for the application, technical leadership to the team\nCoordination/Collaboration with central teams for tasks and standards\nDevelop data integration workflow in Azure\nDeveloping streaming application using scala.\nIntegrating the end-to-end Azure Databricks pipeline to take data from source systems to target system ensuring the quality and consistency of data.\nDefining data quality and validation checks.\nConfiguring data processing and transformation.\nWriting unit test cases for data pipelines.\nDefining and implementing data quality and validation check.\nTuning pipeline configurations for optimal performance.\nParticipate in Peer review and PR review for the code written by team members",Industry Type: Automobile,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Architecture', 'Technical leadership', 'Data processing', 'Workflow', 'Data quality', 'Test cases', 'Unit testing', 'Downstream']",2025-06-13 05:10:47
Senior Data Engineering Analyst,Optum,4 - 7 years,Not Disclosed,['Bengaluru'],"Job Description\n\nExperience 4 to 7 years.\nExperience in any ETL tools [e.g. DataStage] with implementation experience in large Data Warehouse\nProficiency in programming languages such as Python etc.\nExperience with data warehousing solutions (e.g., Snowflake, Redshift) and big data technologies (e.g., Hadoop, Spark).\nStrong knowledge of SQL and database management systems.\nFamiliarity with cloud platforms (e.g., AWS, Azure, GCP) and data pipeline orchestration tools (e.g. Airflow).\nProven ability to lead and develop high-performing teams, with excellent communication and interpersonal skills.\nStrong analytical and problem-solving abilities, with a focus on delivering actionable insights.\nResponsibilities\nDesign, develop, and maintain advanced data pipelines and ETL processes using niche technologies.\nCollaborate with cross-functional teams to understand complex data requirements and deliver tailored solutions.\nEnsure data quality and integrity by implementing robust data validation and monitoring processes.\nOptimize data systems for performance, scalability, and reliability.\nDevelop comprehensive documentation for data engineering processes and systems.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ETL', 'SQL', 'Python', 'Azure', 'Datastage', 'Snowflake', 'Ab Initio', 'Informatica', 'Teradata', 'AWS']",2025-06-13 05:10:49
IN Manager Sr Cloud Data Engineer,PwC Service Delivery Center,3 - 4 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\nAt PwC , you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\nAt PwC , we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm s growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n& Summary We are looking for a seasoned Sr Cloud Data Engineer\nResponsibilities\nHave 3 to 4 years of experience of leading Data Engineer teams developing enterprise grade data processing pipelines on multi Clouds like GCP and AWS Has led at least one project of medium to high complexity of migrating ETL pipelines and Data warehouses to cloud. 3 to 5 years of latest experience should be with premium consulting companies Indepth handson expertise with Google and AWS Cloud Platform services especially BigQuery, Dataform, Dataplex , Redshift. Exceptional communication skills to converse equally well with Data Engineers, Technology and Business leadership. Ability to leverage knowledge on GCP to other cloud environments.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience required\n10 15 Years\nEducation qualification\nBE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Bachelor of Engineering, Master of Business Administration, Bachelor of Technology\nDegrees/Field of Study preferred\nRequired Skills\nAWS Devops, Microsoft Azure, Python (Programming Language)\nDevOps\nNo",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Business administration', 'SAP', 'Consulting', 'Cloud', 'Manager Technology', 'Data processing', 'microsoft azure', 'Corporate advisory', 'AWS', 'Analytics']",2025-06-13 05:10:51
IN Manager Sr Cloud Data Engineer,PwC Service Delivery Center,3 - 4 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\n& Summary We are looking for a seasoned Sr Cloud Data Engineer\nResponsibilities\nHave 3 to 4 years of experience of leading Data Engineer teams developing enterprise grade data processing pipelines on multi Clouds like GCP and AWS Has led at least one project of medium to high complexity of migrating ETL pipelines and Data warehouses to cloud. 3 to 5 years of latest experience should be with premium consulting companies Indepth handson expertise with Google and AWS Cloud Platform services especially BigQuery, Dataform, Dataplex , Redshift. Exceptional communication skills to converse equally well with Data Engineers, Technology and Business leadership. Ability to leverage knowledge on GCP to other cloud environments.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience required\n10 15 Years\nEducation qualification\nBE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Master of Business Administration, Bachelor of Engineering, Bachelor of Technology\nDegrees/Field of Study preferred\nRequired Skills\nAWS Devops, Microsoft Azure, Python (Programming Language)\nDevOps\nNo",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Business administration', 'SAP', 'Consulting', 'Cloud', 'Manager Technology', 'Data processing', 'microsoft azure', 'Corporate advisory', 'AWS', 'Analytics']",2025-06-13 05:10:53
IN Senior Associate Cloud Data Engineer,PwC Service Delivery Center,5 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\nResponsibilities\nStrong handson experience with multi cloud (AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Proficient in PySpark and SQL for building scalable data processing pipelines Knowledge of utilizing serverless technologies like AWS Lambda, and Google Cloud Functions Experience in orchestration frameworks like Apache Airflow, Kubernetes, and Jenkins to manage and orchestrate data pipelines Experience in developing and optimizing ETL/ELT pipelines and working on cloud data warehouse migration projects. Exposure to clientfacing roles, with strong problemsolving and communication skills. Prior experience in consulting or working in a consulting environment is preferred.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience required\n5 8 Years\nEducation qualification\nBE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Master of Business Administration, Bachelor of Technology, Bachelor of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nAWS Devops, Microsoft Azure, Python (Programming Language)\nDevOps\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Business administration', 'SAP', 'Consulting', 'Manager Technology', 'Data processing', 'microsoft azure', 'Corporate advisory', 'AWS', 'Analytics', 'SQL']",2025-06-13 05:10:55
IN Senior Associate Cloud Data Engineer,PwC Service Delivery Center,5 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\nResponsibilities\nStrong handson experience with multi cloud (AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Proficient in PySpark and SQL for building scalable data processing pipelines Knowledge of utilizing serverless technologies like AWS Lambda, and Google Cloud Functions Experience in orchestration frameworks like Apache Airflow, Kubernetes, and Jenkins to manage and orchestrate data pipelines Experience in developing and optimizing ETL/ELT pipelines and working on cloud data warehouse migration projects. Exposure to clientfacing roles, with strong problemsolving and communication skills. Prior experience in consulting or working in a consulting environment is preferred.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience required\n5 8 Years\nEducation qualification\nBE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Bachelor of Engineering, Bachelor of Technology, Master of Business Administration\nDegrees/Field of Study preferred\nRequired Skills\nAWS Devops\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'Data modeling', 'Analytical', 'Consulting', 'Database administration', 'Data processing', 'Corporate advisory', 'DBMS', 'Database management system', 'SQL']",2025-06-13 05:10:57
IN Senior Associate Cloud Data Engineer,PwC Service Delivery Center,5 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\n& Summary We are looking for a seasoned Cloud Data Engineer\nResponsibilities\nStrong handson experience with multi cloud (AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Proficient in PySpark and SQL for building scalable data processing pipelines Knowledge of utilizing serverless technologies like AWS Lambda, and Google Cloud Functions Experience in orchestration frameworks like Apache Airflow, Kubernetes, and Jenkins to manage and orchestrate data pipelines Experience in developing and optimizing ETL/ELT pipelines and working on cloud data warehouse migration projects. Exposure to clientfacing roles, with strong problemsolving and communication skills. Prior experience in consulting or working in a consulting environment is preferred.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience required\n5 8 Years\nEducation qualification\nBE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Master of Business Administration, Master of Engineering, Bachelor of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nCloud Data Catalog\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'GCP', 'Data modeling', 'Analytical', 'Consulting', 'Agile', 'DBMS', 'Apache', 'SQL', 'Python']",2025-06-13 05:10:58
IN Senior Associate Cloud Data Engineer,PwC Service Delivery Center,5 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\nResponsibilities\nStrong handson experience with multi cloud (AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Proficient in PySpark and SQL for building scalable data processing pipelines Knowledge of utilizing serverless technologies like AWS Lambda, and Google Cloud Functions Experience in orchestration frameworks like Apache Airflow, Kubernetes, and Jenkins to manage and orchestrate data pipelines Experience in developing and optimizing ETL/ELT pipelines and working on cloud data warehouse migration projects. Exposure to clientfacing roles, with strong problemsolving and communication skills. Prior experience in consulting or working in a consulting environment is preferred.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience required\n5 8 Years\nEducation qualification\nBE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Bachelor of Engineering, Master of Business Administration, Bachelor of Technology\nDegrees/Field of Study preferred\nRequired Skills\nAWS Devops\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'Data modeling', 'Analytical', 'Consulting', 'Database administration', 'Data processing', 'Corporate advisory', 'DBMS', 'Database management system', 'SQL']",2025-06-13 05:11:00
IN Senior Associate Cloud Data Engineer-- Data and Analytics,PwC Service Delivery Center,4 - 7 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nData, Analytics & AI\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\n& Summary\ns\nJob Title Cloud Data Engineer (AWS/Azure/Databricks/GCP)\nExperience 47 years in Data Engineering\nWe are seeking skilled and dynamic Cloud Data Engineers specializing in AWS, Azure, Databricks, and GCP. The ideal candidate will have a strong background in data engineering, with a focus on data ingestion, transformation, and warehousing. They should also possess excellent knowledge of PySpark or Spark, and a proven ability to optimize performance in Spark job executions.\nKey Responsibilities\nDesign, build, and maintain scalable data pipelines for a variety of cloud platforms including AWS, Azure, Databricks, and GCP.\nImplement data ingestion and transformation processes to facilitate efficient data warehousing.\nUtilize cloud services to enhance data processing capabilities\nAWS Glue, Athena, Lambda, Redshift, Step Functions, DynamoDB, SNS.\nAzure Data Factory, Synapse Analytics, Functions, Cosmos DB, Event Grid, Logic Apps, Service Bus.\nGCP Dataflow, BigQuery, DataProc, Cloud Functions, Bigtable, Pub/Sub, Data Fusion.\nOptimize Spark job performance to ensure high efficiency and reliability.\nStay proactive in learning and implementing new technologies to improve data processing frameworks.\nCollaborate with crossfunctional teams to deliver robust data solutions.\nWork on Spark Streaming for realtime data processing as necessary.\nQualifications\n47 years of experience in data engineering with a strong focus on cloud environments.\nProficiency in PySpark or Spark is mandatory.\nProven experience with data ingestion, transformation, and data warehousing.\nIndepth knowledge and handson experience with cloud services(AWS/Azure/GCP)\nDemonstrated ability in performance optimization of Spark jobs.\nStrong problemsolving skills and the ability to work independently as well as in a team.\nCloud Certification (AWS, Azure, or GCP) is a plus.\nFamiliarity with Spark Streaming is a bonus.\nMandatory skill sets\nPython, Pyspark, SQL with (AWS or Azure or GCP)\nPreferred skill sets\nPython, Pyspark, SQL with (AWS or Azure or GCP)\nYears of experience required\n710\nEducation qualification\nBE/BTECH, ME/MTECH, MBA, MCA\nEducation\nDegrees/Field of Study required Bachelor of Engineering, Master of Business Administration, Bachelor of Technology, Master of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nPySpark, Python (Programming Language), Structured Query Language (SQL)\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 28 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GCP', 'Data modeling', 'Analytical', 'Agile', 'Data processing', 'DBMS', 'Apache', 'AWS', 'SQL', 'Python']",2025-06-13 05:11:02
Data Engineer,Orangemint Technologies,3 - 5 years,20-25 Lacs P.A.,['Bengaluru'],"Company name: PulseData labs Pvt Ltd (captive Unit for URUS, USA)\nAbout URUS We are the URUS family (US), a global leader in products and services for Agritech.\nSENIOR DATA ENGINEER This role is responsible for the design, development, and maintenance of data integration and reporting solutions. The ideal candidate will possess expertise in Databricks and strong skills in SQL Server, SSIS and SSRS, and experience with other modern data engineering tools such as Azure Data Factory. This position requires a proactive and results-oriented individual with a passion for data and a strong understanding of data warehousing principles.\nResponsibilities Data Integration\nDesign, develop, and maintain robust and efficient ETL pipelines and processes on Databricks.\nTroubleshoot and resolve Databricks pipeline errors and performance issues.\nMaintain legacy SSIS packages for ETL processes.\nTroubleshoot and resolve SSIS package errors and performance issues.\nOptimize data flow performance and minimize data latency.\nImplement data quality checks and validations within ETL processes.\nDatabricks Development\nDevelop and maintain Databricks pipelines and datasets using Python, Spark and SQL.\nMigrate legacy SSIS packages to Databricks pipelines.\nOptimize Databricks jobs for performance and cost-effectiveness.\nIntegrate Databricks with other data sources and systems.\nParticipate in the design and implementation of data lake architectures.\nData Warehousing\nParticipate in the design and implementation of data warehousing solutions.\nSupport data quality initiatives and implement data cleansing procedures.\nReporting and Analytics\nCollaborate with business users to understand data requirements for department driven reporting needs.\nMaintain existing library of complex SSRS reports, dashboards, and visualizations.\nTroubleshoot and resolve SSRS report issues, including performance bottlenecks and data inconsistencies.\nCollaboration and Communication\nComfortable in entrepreneurial, self-starting, and fast-paced environment, working both independently and with our highly skilled teams.\nCollaborate effectively with business users, data analysts, and other IT teams.\nCommunicate technical information clearly and concisely, both verbally and in writing.\nDocument all development work and procedures thoroughly.\nContinuous Growth\nKeep abreast of the latest advancements in data integration, reporting, and data engineering technologies.\nContinuously improve skills and knowledge through training and self-learning.\nThis job description reflects managements assignment of essential functions; it does not prescribe or restrict the tasks that may be assigned.\nRequirements\nBachelor's degree in computer science, Information Systems, or a related field.\n2+ years of experience in data integration and reporting.\nExtensive experience with Databricks, including Python, Spark, and Delta Lake.\nStrong proficiency in SQL Server, including T-SQL, stored procedures, and functions.\nExperience with SSIS (SQL Server Integration Services) development and maintenance.\nExperience with SSRS (SQL Server Reporting Services) report design and development.\nExperience with data warehousing concepts and best practices.\nExperience with Microsoft Azure cloud platform and Microsoft Fabric desirable.\nStrong analytical and problem-solving skills.\nExcellent communication and interpersonal skills.\nAbility to work independently and as part of a team.\nExperience with Agile methodologies.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Etl Development', 'Azure Databricks', 'Databricks Engineer', 'Spark', 'SQL Server', 'Data Warehousing', 'Pythonspark']",2025-06-13 05:11:04
Data Engineer,Axis Finance (AFL),7 - 11 years,Not Disclosed,"['Mumbai', 'Mumbai (All Areas)']","Key Responsibilities:\nShould have experience in below\nDesign, develop, and implement a Data Lake House architecture on AWS, ensuring scalability, flexibility, and performance.\nBuild ETL/ELT pipelines for ingesting, transforming, and processing structured and unstructured data.\nCollaborate with cross-functional teams to gather data requirements and deliver data solutions aligned with business needs.\nDevelop and manage data models, schemas, and data lakes for analytics, reporting, and BI purposes.\nImplement data governance practices, ensuring data quality, security, and compliance.\nPerform data integration between on-premise and cloud systems using AWS services.\nMonitor and troubleshoot data pipelines and infrastructure for reliability and scalability.\nSkills and Qualifications:\n7 + years of experience in data engineering, with a focus on cloud data platforms.\nStrong experience with AWS services: S3, Glue, Redshift, Athena, Lambda, IAM, RDS, and EC2.\nHands-on experience in building data lakes, data warehouses, and lake house architectures.\nShould have experience in ETL/ELT pipelines using tools like AWS Glue, Apache Spark, or similar.\nExpertise in SQL and Python or Java for data processing and transformations.\nFamiliarity with data modeling and schema design in cloud environments.\nUnderstanding of data security and governance practices, including IAM policies and data encryption.\nExperience with big data technologies (e.g., Hadoop, Spark) and data streaming services (e.g., Kinesis, Kafka).\nHave lending domain knowledge will be added advantage\nPreferred Skills:\nExperience with Databricks or similar platforms for data engineering.\nFamiliarity with DevOps practices for deploying data solutions on AWS (CI/CD pipelines).\nKnowledge of API integration and cloud data migration strategies.",Industry Type: NBFC,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['pipeline tools', 'lending domain', 'AWS', 'data models', 'spark', 'devops', 'databricks', 'date engineering platforms', 'hadoop', 'data lake house', 'API integration']",2025-06-13 05:11:06
Data Engineer,Supersourcing,4 - 8 years,Not Disclosed,"['Noida', 'Pune', 'Bengaluru']","Job Description-\nWe are hiring a Data Engineer with strong expertise in JAVA, Apache Spark and AWS Cloud. You will design and develop high-performance, scalable applications and data pipelines for cloud-based environments.\n\nKey Skills:\n3+ years in Java (Java 8+), Spring Boot, and REST APIs\n3+ years in Apache Spark (Core, SQL, Streaming)\nStrong hands-on with AWS services: S3, EC2, Lambda, Glue, EMR\nExperience with microservices, CI/CD, and Git\nGood understanding of distributed systems and performance tuning\n\nNice to Have:\nExperience with Kafka, Airflow, Docker, or Kubernetes\nAWS Certification (Developer/Architect)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Spark', 'Pyspark', 'SQL']",2025-06-13 05:11:07
Data Engineer,Aqilea Softech,5 - 9 years,13-20 Lacs P.A.,"['Bangalore Rural', 'Bengaluru']","Job Title: Data Engineer\nCompany : Aqilea India(Client : H&M India)\nEmployment Type: Full Time\nLocation: Bangalore(Hybrid)\nExperience: 4.5 to 9 years\nClient : H&M India\n\nAt H&M, we welcome you to be yourself and feel like you truly belong. Help us reimagine the future of an entire industry by making everyone look, feel, and do good. We take pride in our history of making fashion accessible to everyone and led by our values we strive to build a more welcoming, inclusive, and sustainable industry. We are privileged to have more than 120,000 colleagues, in over 75 countries across the world. Thats 120 000 individuals with unique experiences, skills, and passions. At H&M, we believe everyone can make an impact, we believe in giving people responsibility and a strong sense of ownership. Our business is your business, and when you grow, we grow.\nWebsite : https://career.hm.com/\n\nWe are seeking a skilled and forward-thinking Data Engineer to join our Emerging Tech team. This role is designed for someone passionate about working with cutting-edge technologies such as AI, machine learning, IoT, and big data to turn complex data sets into actionable insights.\nAs the Data Engineer in Emerging Tech, you will be responsible for designing, implementing, and optimizing data architectures and processes that support the integration of next-generation technologies. Your role will involve working with large-scale datasets, building predictive models, and utilizing emerging tools to enable data-driven decision-making across the business. You ll collaborate with technical and business teams to uncover insights, streamline data pipelines, and ensure the best use of advanced analytics technologies.\n\nKey Responsibilities:\nDesign and build scalable data architectures and pipelines that support machine learning, analytics, and IoT initiatives.\nDevelop and optimize data models and algorithms to process and analyse large-scale, complex data sets.\nImplement data governance, security, and compliance measures to ensure high-quality\nCollaborate with cross-functional teams (engineering, product, and business) to translate business requirements into data-driven solutions.\nEvaluate, integrate, and optimize new data technologies to enhance analytics capabilities and drive business outcomes.\nApply statistical methods, machine learning models, and data visualization techniques to deliver actionable insights.\nEstablish best practices for data management, including data quality, consistency, and scalability.\nConduct analysis to identify trends, patterns, and correlations within data to support strategic business initiatives.\nStay updated on the latest trends and innovations in data technologies and emerging data management practices.\n\nSkills Required :\nBachelors or masters degree in data science, Computer Science, Engineering, Statistics, or a related field.\n4.5-9 years of experience in data engineering, data science, or a similar analytical role, with a focus on emerging technologies.\nProficiency with big data frameworks (e.g., Hadoop, Spark, Kafka) and experience with modern cloud platforms (AWS, Azure, or GCP).\nSolid skills in Python, SQL, and optionally R, along with experience using machine learning libraries such as Scikit-learn, TensorFlow, or PyTorch.\nExperience with data visualization tools (e.g., Tableau or Power BI or D3.js) to communicate insights effectively.\nFamiliarity with IoT and edge computing data architectures is a plus.\nUnderstanding of data governance, compliance, and privacy standards.\nAbility to work with both structured and unstructured data.\nExcellent problem-solving, communication, and collaboration skills, with the ability to work in a fast-paced, cross-functional team environment.\nA passion for emerging technologies and a continuous desire to learn and innovate.\nInterested Candidates can share your Resumes to mail id karthik.prakadish@aqilea.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Powerbi', 'Hadoop', 'Kafka', 'Tableau', 'Azure', 'GCP', 'Data Engineer', 'Spark', 'AWS', 'Python', 'SQL']",2025-06-13 05:11:09
Data Engineer,LTIMindtree,5 - 8 years,Not Disclosed,"['Noida', 'Chennai', 'Bengaluru']","1.Big Data Engineer:\n\nCompany: LTIMINDTREE\nMandotory skills - Python,Pyspark & AWS\nLocation : Noida & Pan India\nExperience : 5-8Years\nSalary: 19LPA\n\nShare your cv at Muktai.S@alphacom.in",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'AWS', 'Python']",2025-06-13 05:11:11
nior Data Engineer,M360 Research,5 - 8 years,Not Disclosed,['Bengaluru'],"M3 Global Research, an M3 company, is seeking a Senior Data Engineer to join our data engineering team. This role will focus on building and maintaining robust data pipelines, working closely with stakeholders to ensure data solutions align with business objectives, and utilizing tools like Power BI for data visualization and reporting. The ideal candidate has strong analytical skills, a passion for data-driven decision-making, and excellent communication abilities to work effectively with stakeholders across the organization.\nEssential Duties and Responsibilities:\nDesign, develop, and maintain high-quality, secure data pipelines and processes to manage and transform data efficiently.\nLead the architecture and implementation of data models, schemas, and integrations that support business intelligence and reporting needs.\nCollaborate with cross-functional teams to understand data requirements and deliver optimal data solutions that align with business goals.\nMaintain and enhance data infrastructure, including data warehouses, lakes, and integration tools.\nProvide guidance on best practices for data management, security, and compliance.\nSupport Power BI and other visualization tools, ensuring consistent and reliable access to data insights.\nOversee the delivery of data initiatives, ensuring they meet project milestones, KPIs, and deadlines.\nEssential Job Functions:\nMaintain regular and punctual attendance.\nWork cooperatively and communicate effectively with team members and stakeholders.\nComply with all company policies and procedures.\nSupervisory Responsibility:\nYes\nOutcomes:\nDeliver high-quality, reliable data solutions.\nProvide stakeholders with clear and actionable insights through Power BI reports.\nEnsure data pipelines and ETL processes are optimized and running efficiently.\nFoster strong relationships with stakeholders to ensure their data needs are met.\nCompetencies:\nAttention to detail.\nAnalytical thinking and problem-solving skills.\nStrong communication and interpersonal skills to engage effectively with stakeholders.\nAbility to work in a fast-paced, agile environment.\nKnowledge and Skills:\nProficiency with data engineering tools and technologies (eg, SQL, Python, ETL tools).\nStrong experience with Power BI for data visualization and reporting.\nFamiliarity with cloud-based data platforms (eg, AWS, Azure, Google Cloud).\nExperience with data modeling, data warehousing, and designing scalable data architectures.\nStrong knowledge of database systems (eg, SQL Server, Oracle, PostgreSQL).\nExperience working in an Agile development environment.\nExcellent communication skills to work effectively with both technical and non-technical stakeholders.\nAbility to multi-task and manage multiple projects simultaneously.\nProblem-solving mindset with a desire to continuously improve data processes.\nMinimum Experience:\n5+ years of experience in data engineering or related fields.\n2+ years of experience with Power BI or similar data visualization tools.\nEducation and Training Required:\nbachelors degree in computer science, Data Science, or a related field, or equivalent experience",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data management', 'ISO', 'Analytical', 'Healthcare', 'Market research', 'Oracle', 'Business intelligence', 'Japanese', 'Recruitment', 'SQL']",2025-06-13 05:11:13
Data Engineer- MS Fabric,InfoCepts,5 - 10 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Position: Data Engineer - MS Fabric\nPurpose of the Position: As an MS Fabric Data engineer you will be responsible for designing, implementing, and managing scalable data pipelines. Strong experience in implementation and management of lake House using MS Fabric Azure Tech stack (ADLS Gen2, ADF, Azure SQL) .\nProficiency in data integration techniques, ETL processes and data pipeline architectures. Well versed in Data Quality rules, principles and implementation.\nLocation: Bangalore/ Pune/ Nagpur/ Chennai\nType of Employment: FTE",,,,"['Performance tuning', 'Data modeling', 'Coding', 'XML', 'Scheduling', 'Data quality', 'JSON', 'Business intelligence', 'Data architecture', 'Python']",2025-06-13 05:11:14
Data Engineer,Clifyx Technology,5 - 10 years,Not Disclosed,['Bengaluru'],"2\nData Engineer\nAzure Synapse/ADF , Workiva\nTo manage and maintain the associated Connector, Chains, Tables and Queries, making updates, as needed, as new metrics or requirements are identified\nDevelop functional and technical requirements for any changes impacting wData (Workiva Data)\nConfigure and unit test any changes impacting wData (connector, chains, tables, queries\nPromote wData changes",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'python', 'oracle', 'azure data lake', 'informatica powercenter', 'azure synapse', 'microsoft azure', 'data warehousing', 'power bi', 'azure data factory', 'data engineering', 'sql server', 'sql', 'plsql', 'sql azure', 'spark', 'oracle adf', 'hadoop', 'etl', 'ssis', 'big data', 'informatica', 'unix']",2025-06-13 05:11:16
Databricks Data Engineer,Mumba Technologies,6 - 10 years,22.5-25 Lacs P.A.,['Bengaluru'],"Mandatory Skills & Experience:\n6 to 8 years of experience in data engineering, with strong experience in Oracle\nDWH/ODS environments.\nMinimum 3+ years hands-on experience in Databricks (including PySpark, SQL,\nDelta Lake, Workflows).\nStrong understanding of Lakehouse architecture, cloud data platforms, and big\ndata processing.\n\nProven experience in migrating data warehouse and ETL workloads from Oracle to\ncloud platforms.\nExperience with PL/SQL, query tuning, and reverse engineering legacy systems.\nExposure to Pentaho and/or TIBCO Data Virtualization/Integration tools.\nExperience with CI/CD pipelines, version control (e.g., Git), and automated\ntesting.\nFamiliarity with data governance, security policies, and compliance in cloud\nenvironments.\nStrong communication and documentation skills.\n\nPreferred Skills (Advantage):\nExperience in cloud migration projects (AWS/Azure).\nKnowledge of Delta Lake, Unity Catalog, and Databricks workflows.\nExposure to Kafka for real-time data streaming.\nExperience with ETL tools like Pentaho or Tibco will be an added advantage.\nAWS/Azure/Databricks certifications\nTools & Technologies:\nDatabricks, Oracle, Hadoop (HDFS, Hive, Sqoop), AWS (S3, EMR, Glue, Lamda, RDS)\nPySpark, SQL, Python, Kafka\nCI/CD (Jenkins, GitHub Actions), Orchestration (Airflow, Control-M)\nJIRA, Confluence, Git (GitHub/Bitbucket)\nCloud Certifications (Preferred):\nDatabricks Certified Data Engineer\nAWS Certified Solutions Architect/Developer",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Delta Lake', 'Databricks', 'Oracle DWH', 'Data warehouse', 'SQL']",2025-06-13 05:11:18
Data Engineer - Databricks,KPI Partners,3 - 6 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","About KPI Partners.\nKPI Partners is a leading provider of data analytics solutions, dedicated to helping organizations transform data into actionable insights. Our innovative approach combines advanced technology with expert consulting, allowing businesses to leverage their data for improved performance and decision-making.\n\nJob Description.\nWe are seeking a skilled and motivated Data Engineer with experience in Databricks to join our dynamic team. The ideal candidate will be responsible for designing, building, and maintaining scalable data pipelines and data processing solutions that support our analytics initiatives. You will collaborate closely with data scientists, analysts, and other engineers to ensure the consistent flow of high-quality data across our platforms.",,,,"['python', 'data analytics', 'analytical', 'scala', 'pyspark', 'microsoft azure', 'data warehousing', 'data pipeline', 'data architecture', 'data engineering', 'sql', 'data bricks', 'cloud', 'analytics', 'data quality', 'data modeling', 'gcp', 'teamwork', 'integration', 'aws', 'etl', 'programming', 'communication skills', 'etl scripts']",2025-06-13 05:11:20
Data Engineer - Databricks,Inorg,2 - 5 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']",InOrg Global is looking for Data Engineer - Databricks to join our dynamic team and embark on a rewarding career journey.\n\nLiaising with coworkers and clients to elucidate the requirements for each task.\nConceptualizing and generating infrastructure that allows big data to be accessed and analyzed.\nReformulating existing frameworks to optimize their functioning.\nTesting such structures to ensure that they are fit for use.\nPreparing raw data for manipulation by data scientists.\nDetecting and correcting errors in your work.\nEnsuring that your work remains backed up and readily accessible to relevant coworkers.\nRemaining up - to - date with industry standards and technological advancements that will improve the quality of your outputs.,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent",['Data Engineer - Databricks'],2025-06-13 05:11:21
Consultant - Data Engineer (with Fabric),Affine Analytics,5 - 8 years,Not Disclosed,['Bengaluru'],"We are looking for a highly skilled API & Pixel Tracking Integration Engineer to lead the development and deployment of server-side tracking and attribution solutions across multiple platforms. The ideal candidate brings deep expertise in CAPI integrations (Meta, Google, and other platforms), secure data handling using cryptographic techniques, and experience working within privacy-first environments like Azure Clean Rooms.\nThis role requires strong hands-on experience in C# development, Azure cloud services, OCI (Oracle Cloud Infrastructure), and marketing technology stacks including Adobe Tag Management and Pixel Management. You will work closely with engineering, analytics, and marketing teams to deliver scalable, compliant, and secure data tracking solutions that drive business insights and performance.",,,,"['Adobe Tag Management', 'Data Engineering', 'Azure Data Factory', 'Azure security', 'ADF', 'ADLS', 'Azure Functions', 'Meta CAPI', 'Google Enhanced Conversions', 'Key Vault', 'Cosmos DB']",2025-06-13 05:11:23
Consultant - Data Engineer,Affine Analytics,5 - 8 years,Not Disclosed,['Bengaluru'],"We are looking for a highly skilled API & Pixel Tracking Integration Engineer to lead the development and deployment of server-side tracking and attribution solutions across multiple platforms. The ideal candidate brings deep expertise in CAPI integrations (Meta, Google, and other platforms), secure data handling using cryptographic techniques, and experience working within privacy-first environments like Azure Clean Rooms.\nThis role requires strong hands-on experience in Azure cloud services, OCI (Oracle Cloud Infrastructure), and marketing technology stacks including Adobe Tag Management and Pixel Management. You will work closely with engineering, analytics, and marketing teams to deliver scalable, compliant, and secure data tracking solutions that drive business insights and performance.",,,,"['Python', 'Azure Cloud technologies', 'Azure Data Factory', 'Adobe Tag Management', 'Azure security', 'ADF', 'ADLS', 'Azure Functions', 'Key Vault']",2025-06-13 05:11:25
Data Engineer,Talent Aspire,2 - 7 years,Not Disclosed,"['Chandigarh', 'Bengaluru']","As the Data Engineer, you will play a pivotal role in shaping our data infrastructure and\nexecuting against our strategy. You will ideate alongside engineering, data and our clients to\ndeploy data products with an innovative and meaningful impact to clients. You will design, build, and maintain scalable data pipelines and workflows on AWS. Additionally, your expertise in AI and machine learning will enhance our ability to deliver smarter, more predictive solutions.\n\nKey Responsibilities\nCollaborate with other engineers, customers to brainstorm and develop impactful data\nproducts tailored to our clients.\nLeverage AI and machine learning techniques to integrate intelligent features into our\nofferings.\nDevelop, and optimize end-to-end data pipelines on AWS\nFollow best practices in software architecture and development.\nImplement effective cost management and performance optimization strategies.\nDevelop and maintain systems using Python, SQL, PySpark, and Django for front-end\ndevelopment.\nWork directly with clients and end-users and address their data needs\nUtilize databases and tools including and not limited to, Postgres, Redshift, Airflow, and\nMongoDB to support our data ecosystem.\nLeverage AI frameworks and libraries to integrate advanced analytics into our solutions.\nQualifications\n\nExperience:\nMinimum of 3 years of experience in data engineering, software development, or\nrelated roles.\nProven track record in designing and deploying AWS cloud infrastructure\nsolutions\nAt least 2 years in data analysis and mining techniques to aid in descriptive and\ndiagnostic insights\nExtensive hands-on experience with Postgres, Redshift, Airflow, MongoDB, and\nreal-time data workflows.\n\nTechnical Skills:\nExpertise in Python, SQL, and PySpark\nStrong background in software architecture and scalable development practices.\nTableau, Metabase or similar viz tools experience\nWorking knowledge of AI frameworks and libraries is a plus.\nLeadership & Communication:\nDemonstrates ownership and accountability for delivery with a strong\ncommitment to quality.\nExcellent communication skills with a history of effective client and end-user\nengagement.\nStartup & Fintech Mindset:\nAdaptability and agility to thrive in a fast-paced, early-stage startup environment.\nPassion for fintech innovation and a strong desire to make a meaningful impact\non the future of finance.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'performance optimization strategies', 'PySpark', 'Django', 'cost management', 'AWS', 'AI frameworks', 'Python', 'SQL']",2025-06-13 05:11:26
Data Engineer-Pyspark,A leading technology services and consul...,5 - 10 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","Our client is Global IT Service & Consulting Organization\nExp-5+ yrs\n\nSkil Apache Spark\n\nLocation- Bangalore, Hyderabad, Pune, Chennai, Coimbatore, Gr. Noida\n\n\nExcellent Knowledge on Spark; The professional must have a thorough understanding Spark framework, Performance Tuning etc\nExcellent Knowledge and hands-on experience of at least 4+ years in Scala or PySpark\nExcellent Knowledge of the Hadoop eco System- Knowledge of Hive mandatory\nStrong Unix and Shell Scripting Skills\nExcellent Inter-personal skills and for experienced candidates Excellent leadership skills\nMandatory for anyone to have Good knowledge of any of the CSPs like Azure,AWS or GCP; Certifications on Azure will be additional Pl",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Cloud', 'Microsoft Azure', 'Python', 'GCP', 'data engineer', 'Hadoop', 'AWS']",2025-06-13 05:11:28
Data Engineer,Clifyx Technology,5 - 8 years,Not Disclosed,['Bengaluru'],"BE/B Tech/MCA/MSc Comp science. -Only\nDetailed job description - Skill Set:\n7+ years of experience with data analytics, data modeling, and database design.\n5+ years of experience with Vertica.\n2+ years of coding and scripting (Python/Java/Scala) and design experience.\n2+ years of experience with Airflow.\nExperience with ELT methodologies and tools.\nExperience with GitHub.\nExpertise in tuning and troubleshooting SQL.\nStrong data integrity, analytical and multitasking skills.\nExcellent communication, problem solving, organizational and analytical skills.\nMandatory Skills\nSQL, Python and Vertica",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Analytical skills', 'vertica', 'Coding', 'Data modeling', 'Database design', 'Billing', 'data integrity', 'Troubleshooting', 'SQL', 'Python']",2025-06-13 05:11:29
Microsoft Fabrics Data Engineer,Swits Digital,5 - 10 years,Not Disclosed,['Bengaluru'],"Job TItle: Microsoft Fabric Data Engineer\nLocation: Bangalore\nJob Type: Conract (24 Months)\nJob Description:\nWe are seeking a highly skilled and experienced Microsoft Fabric Data Engineer/Architect to design, develop, and maintain robust, scalable, and secure data solutions within the Microsoft Fabric ecosystem. This role will leverage the full suite of Microsoft Azure data services, including Azure Data Bricks, Azure Data Factory, and Azure Data Lake, to build end-to-end data pipelines, data warehouses, and data lakehouses that enable advanced analytics and business intelligence.\nRequired Skills & Qualifications:\nBachelors degree in Computer Science, Engineering, or a related field.\n5+ years of experience in data architecture and engineering, with a strong focus on Microsoft Azure data platforms.\nProven hands-on expertise with Microsoft Fabric and its components, including:\nOneLake\nData Factory (Pipelines, Dataflows Gen2)\nSynapse Analytics (Data Warehousing, SQL analytics endpoint)\nLakehouses and Warehouses\nNotebooks (PySpark)\nExtensive experience with Azure Data Bricks, including Spark development (PySpark, Scala, SQL).\nStrong proficiency in Azure Data Factory for building and orchestrating ETL/ELT pipelines.\nDeep understanding and experience with Azure Data Lake Storage Gen2.\nProficiency in SQL (T-SQL, Spark SQL), Python, and/or other relevant scripting languages.\nSolid understanding of data warehousing concepts, dimensional modeling, and data lakehouse architectures.\nExperience with data governance principles and tools (e.g., Microsoft Purview).\nFamiliarity with CI/CD practices, version control (Git), and DevOps for data pipelines.\nExcellent problem-solving, analytical, and communication skills.\nAbility to work independently and collaboratively in a fast-paced, agile environment.\nPreferred Qualifications:\nMicrosoft certifications in Azure Data Engineering (e.g., DP-203, DP-600: Microsoft Fabric Analytics Engineer Associate).\nExperience with Power BI for data visualization and reporting.\nFamiliarity with real-time analytics and streaming data processing.\nExposure to machine learning workflows and integrating ML models with data solutions",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['GIT', 'Analytical', 'microsoft azure', 'data visualization', 'microsoft', 'Business intelligence', 'Data warehousing', 'Analytics', 'Data architecture', 'Python']",2025-06-13 05:11:31
Data Engineer with Neo4j,Luxoft,3 - 5 years,Not Disclosed,['Gurugram'],"Graph Data Modeling & Implementation.\nDesign and implement complex graph data models using Cypher and Neo4j best practices.\nLeverage APOC procedures, custom plugins, and advanced graph algorithms to solve domain-specific problems.\nOversee integration of Neo4j with other enterprise systems, microservices, and data platforms.\nDevelop and maintain APIs and services in Java, Python, or JavaScript to interact with the graph database.\nMentor junior developers and review code to maintain high-quality standards.\nEstablish guidelines for performance tuning, scalability, security, and disaster recovery in Neo4j environments.\nWork with data scientists, analysts, and business stakeholders to translate complex requirements into graph-based solutions.\nSkills\nMust have\n12+ years in software/data engineering, with at least 3-5 years hands-on experience with Neo4j.\nLead the technical strategy, architecture, and delivery of Neo4j-based solutions.\nDesign, model, and implement complex graph data structures using Cypher and Neo4j best practices.\nGuide the integration of Neo4j with other data platforms and microservices.\nCollaborate with cross-functional teams to understand business needs and translate them into graph-based models.\nMentor junior developers and ensure code quality through reviews and best practices.\nDefine and enforce performance tuning, security standards, and disaster recovery strategies for Neo4j.\nStay up-to-date with emerging technologies in the graph database and data engineering space.\nStrong proficiency in Cypher query language, graph modeling, and data visualization tools (e.g., Bloom, Neo4j Browser).\nSolid background in Java, Python, or JavaScript and experience integrating Neo4j with these languages.\nExperience with APOC procedures, Neo4j plugins, and query optimization.\nFamiliarity with cloud platforms (AWS) and containerization tools (Docker, Kubernetes).\nProven experience leading engineering teams or projects.\nExcellent problem-solving and communication skills.\nNice to have\nN/A\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Flexera Data Analyst\nData Science\nIndia\nBengaluru\nSenior Flexera Data Analyst\nData Science\nIndia\nChennai\nData Scientist\nData Science\nIndia\nBengaluru\nGurugram, India\nReq. VR-114556\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114556\nApply for Data Engineer with Neo4j in Gurugram\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'query optimization', 'neo4j', 'data science', 'Data modeling', 'Disaster recovery', 'Javascript', 'Data structures', 'data visualization', 'Python']",2025-06-13 05:11:33
Data Engineer with Neo4j,Luxoft,3 - 5 years,Not Disclosed,['Bengaluru'],"Graph Data Modeling & Implementation.\nDesign and implement complex graph data models using Cypher and Neo4j best practices.\nLeverage APOC procedures, custom plugins, and advanced graph algorithms to solve domain-specific problems.\nOversee integration of Neo4j with other enterprise systems, microservices, and data platforms.\nDevelop and maintain APIs and services in Java, Python, or JavaScript to interact with the graph database.\nMentor junior developers and review code to maintain high-quality standards.\nEstablish guidelines for performance tuning, scalability, security, and disaster recovery in Neo4j environments.\nWork with data scientists, analysts, and business stakeholders to translate complex requirements into graph-based solutions.\nSkills\nMust have\n12+ years in software/data engineering, with at least 3-5 years hands-on experience with Neo4j.\nLead the technical strategy, architecture, and delivery of Neo4j-based solutions.\nDesign, model, and implement complex graph data structures using Cypher and Neo4j best practices.\nGuide the integration of Neo4j with other data platforms and microservices.\nCollaborate with cross-functional teams to understand business needs and translate them into graph-based models.\nMentor junior developers and ensure code quality through reviews and best practices.\nDefine and enforce performance tuning, security standards, and disaster recovery strategies for Neo4j.\nStay up-to-date with emerging technologies in the graph database and data engineering space.\nStrong proficiency in Cypher query language, graph modeling, and data visualization tools (e.g., Bloom, Neo4j Browser).\nSolid background in Java, Python, or JavaScript and experience integrating Neo4j with these languages.\nExperience with APOC procedures, Neo4j plugins, and query optimization.\nFamiliarity with cloud platforms (AWS) and containerization tools (Docker, Kubernetes).\nProven experience leading engineering teams or projects.\nExcellent problem-solving and communication skills.\nNice to have\nN/A\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Flexera Data Analyst\nData Science\nIndia\nGurugram\nSenior Flexera Data Analyst\nData Science\nIndia\nChennai\nBusiness Analyst\nData Science\nPoland\nRemote Poland\nBengaluru, India\nReq. VR-114556\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114556\nApply for Data Engineer with Neo4j in Bengaluru\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'query optimization', 'neo4j', 'data science', 'Data modeling', 'Disaster recovery', 'Javascript', 'Data structures', 'data visualization', 'Python']",2025-06-13 05:11:35
Data Engineer with Neo4j,Luxoft,3 - 5 years,Not Disclosed,['Chennai'],"Graph Data Modeling & Implementation.\nDesign and implement complex graph data models using Cypher and Neo4j best practices.\nLeverage APOC procedures, custom plugins, and advanced graph algorithms to solve domain-specific problems.\nOversee integration of Neo4j with other enterprise systems, microservices, and data platforms.\nDevelop and maintain APIs and services in Java, Python, or JavaScript to interact with the graph database.\nMentor junior developers and review code to maintain high-quality standards.\nEstablish guidelines for performance tuning, scalability, security, and disaster recovery in Neo4j environments.\nWork with data scientists, analysts, and business stakeholders to translate complex requirements into graph-based solutions.\nSkills\nMust have\n12+ years in software/data engineering, with at least 3-5 years hands-on experience with Neo4j.\nLead the technical strategy, architecture, and delivery of Neo4j-based solutions.\nDesign, model, and implement complex graph data structures using Cypher and Neo4j best practices.\nGuide the integration of Neo4j with other data platforms and microservices.\nCollaborate with cross-functional teams to understand business needs and translate them into graph-based models.\nMentor junior developers and ensure code quality through reviews and best practices.\nDefine and enforce performance tuning, security standards, and disaster recovery strategies for Neo4j.\nStay up-to-date with emerging technologies in the graph database and data engineering space.\nStrong proficiency in Cypher query language, graph modeling, and data visualization tools (e.g., Bloom, Neo4j Browser).\nSolid background in Java, Python, or JavaScript and experience integrating Neo4j with these languages.\nExperience with APOC procedures, Neo4j plugins, and query optimization.\nFamiliarity with cloud platforms (AWS) and containerization tools (Docker, Kubernetes).\nProven experience leading engineering teams or projects.\nExcellent problem-solving and communication skills.\nNice to have\nN/A\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Flexera Data Analyst\nData Science\nIndia\nGurugram\nSenior Flexera Data Analyst\nData Science\nIndia\nBengaluru\nData Scientist\nData Science\nIndia\nBengaluru\nChennai, India\nReq. VR-114556\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114556\nApply for Data Engineer with Neo4j in Chennai\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'query optimization', 'neo4j', 'data science', 'Data modeling', 'Disaster recovery', 'Javascript', 'Data structures', 'data visualization', 'Python']",2025-06-13 05:11:36
Data Engineer,Luxoft,5 - 8 years,Not Disclosed,['Pune'],"Help Group Enterprise Architecture team to develop our suite of EA tools and workbenches\nWork in the development team to support the development of portfolio health insights\nBuild data applications from cloud infrastructure to visualization layer\nProduce clear and commented code\nProduce clear and comprehensive documentation\nPlay an active role with technology support teams and ensure deliverables are completed or escalated on time\nProvide support on any related presentations, communications, and trainings\nBe a team player, working across the organization with skills to indirectly manage and influence\nBe a self-starter willing to inform and educate others\nSkills\nMust have\nB.Sc./M.Sc. degree in computing or similar\n5-8+ years experience as a Data Engineer, ideally in a large corporate environment\nIn-depth knowledge of SQL and data modelling/data processing\nStrong experience working with Microsoft Azure\nExperience with visualisation tools like PowerBI (or Tableau, QlikView or similar)\nExperience working with Git, JIRA, GitLab\nStrong flair for data analytics\nStrong flair for IT architecture and IT architecture metrics\nExcellent stakeholder interaction and communication skills\nUnderstanding of performance implications when making design decisions to deliver performant and maintainable software.\nExcellent end-to-end SDLC process understanding.\nProven track record of delivering complex data apps on tight timelines\nFluent in English both written and spoken.\nPassionate about development with focus on data and cloud\nAnalytical and logical, with strong problem solving skills\nA team player, comfortable with taking the lead on complex tasks\nAn excellent communicator who is adept in, handling ambiguity and communicating with both technical and non-technical audiences\nComfortable with working in cross-functional global teams to effect change\nPassionate about learning and developing your hard and soft professional skills\nNice to have\nExperience working in the financial industry\nExperience in complex metrics design and reporting\nExperience in using artificial intelligence for data analytics\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Power BI Developer\nBI Engineering\nIndia\nBengaluru\nSenior Power BI Developer\nBI Engineering\nIndia\nChennai\nSenior Power BI Developer\nBI Engineering\nIndia\nGurugram\nPune, India\nReq. VR-114797\nBI Engineering\nBCM Industry\n02/06/2025\nReq. VR-114797\nApply for Data Engineer in Pune\n*",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['GIT', 'Enterprise architecture', 'Analytical', 'Artificial Intelligence', 'Data processing', 'Data analytics', 'QlikView', 'JIRA', 'SDLC', 'SQL']",2025-06-13 05:11:38
Data Engineer,Amakshar Technology,4 - 7 years,Not Disclosed,"['Mumbai', 'Pune', 'Chennai', 'Bengaluru']","Job Category: IT\nJob Type: Full Time\nJob Location: Bangalore Chennai Mumbai Pune\nLocation- Mumbai, Pune, Bangalore, Chennai\nExperience- 5+\nData Engineer: Expertise in Python Language is MUST. SQL (should be able to write complex SQL Queries) is MUST Data Lake Development experience. Orchestration (Apache Airflow is preferred). Spark and Hive: Optimization of Spark/PySpark and Hive apps is MUST Trino/(AWS Athena) (Good to have) Snowflake (good to have). Data Quality (good to have). File Storage (S3 is good to have)\nKind Note: Please apply or share your resume only if it matches the above criteria",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'SQL queries', 'orchestration', 'spark', 'Data quality', 'Apache', 'AWS', 'Python']",2025-06-13 05:11:40
Data Engineer,Lenskart,1 - 4 years,Not Disclosed,['Bengaluru'],"Key Responsibilities\nBuild and maintain scalable ETL/ELT data pipelines using Python and cloud-native tools.\nDesign and optimize data models and queries on Google BigQuery for analytical workloads.\nDevelop, schedule, and monitor workflows using orchestration tools like Apache Airflow or Cloud Composer.\nIngest and integrate data from multiple structured and semi-structured sources, including MySQL, MongoDB, APIs, and cloud storage.",,,,"['GCP', 'Bigquery', 'MySQL', 'MongoDB', 'Python']",2025-06-13 05:11:41
Data Engineer (5-10 Years) | @ Banking | Bangalore & Mumbai,Net Connect,5 - 10 years,Not Disclosed,"['Bengaluru', 'Mumbai (All Areas)']","Job Summary\nWe are seeking a skilled Data Engineer to design, develop, and optimize scalable data pipelines and infrastructure. The ideal candidate will have expertise in relational databases, data modeling, cloud migration, and automation, working closely with cross-functional teams to drive data-driven decision-making.\n\nKey Responsibilities",,,,"['Data Modeling', 'ETL', 'Scripting', 'SQL', 'Azure Data Factory', 'Informatica']",2025-06-13 05:11:43
Data Scientist (Data & AI Engineer),Visa,2 - 7 years,Not Disclosed,['Bengaluru'],"The Client Services BI & Analytics team strives to create an open, trusting data culture where the cost of curiosity - the number of steps, amount of time, and complexity of effort needed to use operational data to derive insights - is as low as possible. We govern Client Services operational data and metrics, create easily usable dashboards and data sources, and analyze data to share insights.\nWe are a part of the Client Services Global Business Operations function and work with all levels of stakeholders, from executive leaders sharing insights with the C-Suite to customer-facing colleagues who rely on our assets to incorporate data into their daily responsibilities.\nThis specialist role makes data available from new sources, builds robust data models, creates and optimizes data enrichment pipelines, and provides engineering support to specific projects. You will partner with our Data Visualizers and Solution Designers to ensure that data needed by the business is available and accurate and to develop certified data sets. This technical lead and architect role is a force multiplier to our Visualizers, Analysts, and other data users across Client Services.\nResponsibilities\nDesign, develop, and maintain scalable data pipelines and systems.\nMonitor and troubleshoot data pipeline issues to ensure seamless data flow.\nEstablish data processes and automation based on business and technology requirements, leveraging Visa s supported data platforms and tools\nDeliver small to large data engineering and Machine learning projects either individually or as part of a project team\nSetup ML Ops pipelines to Productionalize ML models and setting up Gen AI pipelines\nCollaborate with cross-functional teams to understand data requirements and ensure data quality, with a focus on implementing data validation and data quality checks at various stages of the pipeline\nProvide expertise in data warehousing, ETL, and data modeling to support data-driven decision making, with a strong understanding of best practices in data pipeline design and performance optimization\nExtract and manipulate large datasets using standard tools such as Hadoop (Hive), Spark, Python (pandas, NumPy), Presto, and SQL\nDevelop data solutions using Agile principles\nProvide ongoing production support\nCommunicate complex concepts in a clear and effective manner\nStay up to date with the latest data engineering trends and technologies to ensure the companys data infrastructure is always state-of-the-art, with an understanding of best practices in cloud-based data engineering\nThis is a remote position. A remote position does not require job duties be performed within proximity of a Visa office location. Remote positions may be required to be present at a Visa office with scheduled notice.\n\n\nBasic Qualifications\n-2 or more years of work experience with a Bachelor s Degree or an Advanced Degree (e.g. Masters, MBA, JD, MD, or PhD)\n\nPreferred Qualifications\n-3 or more years of work experience with a Bachelor s Degree or more than 2 years of work experience with an Advanced Degree (e.g. Masters, MBA, JD, MD)\n-3+ years of work experience with a bachelor s degree in the STEM field.\n-Strong experience with SQL, Python, Hadoop, Spark, Hive, Airflow and MPP data bases\n-5+ years of analytics experience with a focus on data Engineering and AI\n-Experience with both traditional data warehousing tools and techniques (such as SSIS, ODI, and on-prem SQL Server, Oracle) as well as modern technologies (such as Hadoop, Denodo, Spark, Airflow, and Python), and a solid understanding of best practices in data engineering\n-Advanced knowledge of SQL (e.g., understands subqueries, self-joining tables, stored procedures, can read an execution plan, SQL tuning, etc.)\n-Solid understanding of best practices in data warehousing, ETL, data modeling, and data architecture.\n-Experience with NoSQL databases (e.g., MongoDB, Cassandra)\n-Experience with cloud-based data warehousing and data pipeline management (AWS, GCP, Azure)\n-Experience in Python, Spark, and exposure to scheduling tools like Tuber/Airflow is preferred.\n-Able to create data dictionaries, setup and monitor data validation alerts, and execute periodic jobs to maintain data pipelines for completed projects\n-Experience with visualization software (e.g., Tableau, QlikView, PowerBI) is a plus.\n-A team player and collaborator, able to work well with a diverse group of individuals in a matrixed environment",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'Production support', 'Data modeling', 'Agile', 'Stored procedures', 'QlikView', 'Oracle', 'SSIS', 'Analytics', 'Python']",2025-06-13 05:11:44
Big Data Engineer - Hadoop,Info Origin Technologies Pvt Ltd,3 - 7 years,Not Disclosed,"['Hyderabad', 'Gurugram']","Role: Hadoop Data Engineer\nLocation: Gurgaon / Hyderabad\nWork Mode: Hybrid\nEmployment Type: Full-Time\nInterview Mode: First Video then In Person\nJob Description\nJob Overview:\nWe are looking for experienced Data Engineers proficient in Hadoop, Hive, Python, SQL, and Pyspark/Spark to join our dynamic team. Candidates will be responsible for designing, developing, and maintaining scalable big data solutions.\nKey Responsibilities:\nDevelop and optimize data pipelines for large-scale data processing.\nWork with structured and unstructured datasets to derive actionable insights.\nCollaborate with cross-functional teams to enhance data-driven decision-making.\nEnsure the performance, scalability, and reliability of data architectures.\nImplement best practices for data security and governance.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hive', 'Hadoop', 'Pyspark', 'Big Data', 'Python', 'SQL']",2025-06-13 05:11:46
Senior Data Engineer- (Big data and Data Pipelines),Findem,5 - 9 years,Not Disclosed,"['New Delhi', 'Bengaluru']","What is Findem:\n\nFindem is the only talent data platform that combines 3D data with AI. It automates and consolidates top-of-funnel activities across your entire talent ecosystem, bringing together sourcing, CRM, and analytics into one place. Only 3D data connects people and company data over time - making an individual s entire career instantly accessible in a single click, removing the guesswork, and unlocking insights about the market and your competition no one else can. Powered by 3D data, Findem s automated workflows across the talent lifecycle are the ultimate competitive advantage. Enabling talent teams to deliver continuous pipelines of top, diverse candidates while creating better talent experiences, Findem transforms the way companies plan, hire, and manage talent. Learn more at www.findem.ai\n\nExperience - 5 - 9 years\n\nWe are looking for an experienced Big Data Engineer, who will be responsible for building, deploying and managing various data pipelines, data lake and Big data processing solutions using Big data and ETL technologies.\n\nLocation- Delhi, India\nHybrid- 3 days onsite\nResponsibilities\nBuild data pipelines, Big data processing solutions and data lake infrastructure using various Big data and ETL technologies\nAssemble and process large, complex data sets that meet functional non-functional business requirements ETL from a wide variety of sources like MongoDB, S3, Server-to-Server, Kafka etc., and processing using SQL and big data technologies\nBuild analytical tools to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics Build interactive and ad-hoc query self-serve tools for analytics use cases\nBuild data models and data schema for performance, scalability and functional requirement perspective Build processes supporting data transformation, metadata, dependency and workflow management\nResearch, experiment and prototype new tools/technologies and make them successful\nSkill Requirements\nMust have-Strong in Python/Scala\nMust have experience in Big data technologies like Spark, Hadoop, Athena / Presto, Redshift, Kafka etc\nExperience in various file formats like parquet, JSON, Avro, orc etc\nExperience in workflow management tools like airflow Experience with batch processing, streaming and message queues\nAny of visualization tools like Redash, Tableau, Kibana etc\nExperience in working with structured and unstructured data sets\nStrong problem solving skills\nGood to have\nExposure to NoSQL like MongoDB\nExposure to Cloud platforms like AWS, GCP, etc\nExposure to Microservices architecture\nExposure to Machine learning techniques\nThe role is full-time and comes with full benefits. We are globally headquartered in the San Francisco Bay Area with our India headquarters in Bengaluru.\n\nEqual Opportunity",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAN', 'metadata', 'Prototype', 'Machine learning', 'Schema', 'JSON', 'Analytics', 'SQL', 'CRM', 'Python']",2025-06-13 05:11:48
Data Engineer (C2H),First Mile Consulting,4 - 8 years,Not Disclosed,"['Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Very strong in python, pyspark and SQL. Good experience in any cloud . They use AWS but any cloud experience is ok. They will train on other things but if candidates have experience with ETL (like AWS Airflow), datalakes like Snowflake",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Pyspark', 'Cloud', 'Data engineer', 'Python', 'Sql', 'Airflow']",2025-06-13 05:11:50
Data Engineer,Prohr Strategies,9 - 11 years,Not Disclosed,['Bengaluru'],"Hands-on Data Engineer with strong Databricks expertise in Git/DevOps integration, Unity Catalog governance, and performance tuning of data transformation workloads. Skilled in optimizing pipelines and ensuring secure, efficient data operations.",Industry Type: Emerging Technologies (AI/ML),Department: Data Science & Analytics,"Employment Type: Full Time, Temporary/Contractual","['Data Transformation', 'GIT', 'Azure Databricks', 'Databricks', 'Devops', 'Data Engineering', 'Governance', 'Catalog', 'Code Versioning Tools']",2025-06-13 05:11:51
Data Engineer - Ranchi,Teqfocus Consulting,0 - 2 years,Not Disclosed,['Ranchi'],"Job Title: Data Engineer\nExperience: 1+ Years (Freshers with relevant training & certification may apply)\nLocation: Ranchi (Work from Office)\n\nJob Summary:\nWe are looking for a Data Engineer with at least 1 year of hands-on experience in data engineering practices. The ideal candidate will work closely with our data and analytics teams to build robust and scalable data pipelines. Experience with Snowflake is a plus.\n\nKey Responsibilities:\nDesign, build, and maintain data pipelines using modern data engineering tools.\nTransform and clean data from multiple sources for reporting and analytics.\nOptimize data pipelines for performance and scalability.\nCollaborate with cross-functional teams including BI, analytics, and application developers.\nMonitor, troubleshoot, and maintain data workflows.\n\nRequired Skills:\nStrong understanding of data warehousing concepts.\nProficiency in SQL and Python.\nKnowledge of ETL tools and processes.\nFamiliarity with cloud platforms such as AWS, Snowflake, Databricks, Azure or GCP.\nExposure to data visualization tools is a plus.\n\nGood to have any of below certification:\nSnowflake SnowPro Core Certification\nSnowflake Advanced: Data Engineer Certification\nGoogle Cloud Professional Data Engineer\nMicrosoft Certified: Azure Data Engineer Associate\nAWS Certified Data Analytics Specialty\n\nQualifications:\nBachelor's degree in Computer Science, Information Technology, or related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Snowflake', 'AWS', 'Data Bricks', 'GCP', 'Microsoft Azure', 'Python']",2025-06-13 05:11:53
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Indore'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-13 05:11:55
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Pune'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-13 05:11:57
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,"['Mumbai', 'Any Location']","We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-13 05:11:59
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Nagpur'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python Programming', 'Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-13 05:12:00
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Ahmedabad'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python Programming', 'Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-13 05:12:02
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Delhi / NCR'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python Programming', 'Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-13 05:12:04
Data Engineer _Technology Lead,Broadridge,6 - 10 years,Not Disclosed,['Bengaluru'],"Key Responsibilities:\nAnalyzes and solve problems using technical experience, judgment and precedents\nProvides informal guidance to new team members\nExplains complex information to others in straightforward situations\n1. Data Engineering and Modelling:\nDesign & Develop Scalable Data Pipelines: Leverage AWS technologies to design, develop, and manage end-to-end data pipelines with services like .",,,,"['Star Schema', 'Snowflake', 'AWS', 'Apache Airflow']",2025-06-13 05:12:05
Lead Data Engineer,Acuity Knowledge Partners,8 - 13 years,Not Disclosed,"['Pune', 'Gurugram', 'Bengaluru']","Preferred candidate profile\n\n9+ years of overall experience in software development with a focus on data projects using Python, PySpark, and associated frameworks.\nProven experience as a Data Engineer with experience in Azure cloud.\nExperience implementing solutions using Azure cloud services, Azure Data Factory, Azure Lake Gen 2, Azure Databases, Azure Data Fabric, API Gateway management, Azure Functions.",,,,"['Pyspark', 'Azure Cloud', 'Python', 'SQL']",2025-06-13 05:12:07
Data Engineer Sr. Analyst,Accenture,5 - 7 years,Not Disclosed,['Kochi'],"Job Title - + +\n\n\n\nManagement Level:\n\n\n\nLocation:Kochi, Coimbatore, Trivandrum\n\n\n\nMust have skills:Databricks including Spark-based ETL, Delta Lake\n\n\n\n\nGood to have skills:Pyspark\n\n\n\nJob\n\n\nSummary\n\nWe are seeking a highly skilled and experienced Senior Data Engineer to join our growing Data and Analytics team. The ideal candidate will have deep expertise in Databricks and cloud data warehousing, with a proven track record of designing and building scalable data pipelines, optimizing data architectures, and enabling robust analytics capabilities. This role involves working collaboratively with cross-functional teams to ensure the organization leverages data as a strategic asset. Your responsibilities will include:\n\n\n\nRoles and Responsibilities\nDesign, build, and maintain scalable data pipelines and ETL processes using Databricks and other modern tools.\nArchitect, implement, and manage cloud-based data warehousing solutions on Databricks (Lakehouse Architecture)\nDevelop and maintain optimized data lake architectures to support advanced analytics and machine learning use cases.\nCollaborate with stakeholders to gather requirements, design solutions, and ensure high-quality data delivery.\nOptimize data pipelines for performance and cost efficiency.\nImplement and enforce best practices for data governance, access control, security, and compliance in the cloud.\nMonitor and troubleshoot data pipelines to ensure reliability and accuracy.\nLead and mentor junior engineers, fostering a culture of continuous learning and innovation.\nExcellent communication skills\nAbility to work independently and along with client based out of western Europe.\n\n\n\nProfessional and Technical Skills\n3.5-5 years of experience in Data Engineering roles with a focus on cloud platforms.\nProficiency in Databricks, including Spark-based ETL, Delta Lake, and SQL.\nStrong experience with one or more cloud platforms (AWS preferred).\nHandson Experience with Delta lake, Unity Catalog, and Lakehouse architecture concepts.\nStrong programming skills in Python and SQL; experience with Pyspark a plus.\nSolid understanding of data modeling concepts and practices (e.g., star schema, dimensional modeling).\nKnowledge of CI/CD practices and version control systems (e.g., Git).\nFamiliarity with data governance and security practices, including GDPR and CCPA compliance.\n\n\n\n\nAdditional Information\nExperience with Airflow or similar workflow orchestration tools.\nExposure to machine learning workflows and MLOps.\nCertification in Databricks, AWS\nFamiliarity with data visualization tools such as Power BI\n\n(do not remove the hyperlink)Qualification\n\n\n\nExperience:3.5 -5 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data warehousing', 'sql', 'data modeling', 'python', 'data bricks', 'hive', 'kubernetes', 'catalog', 'pyspark', 'data architecture', 'docker', 'ansible', 'git', 'java', 'spark', 'devops', 'hadoop', 'etl', 'big data', 'data lake', 'airflow', 'power bi', 'cloud platforms', 'machine learning', 'data engineering', 'aws']",2025-06-13 05:12:09
"Senior Manager, Senior Data Engineer",Merck Sharp & Dohme (MSD),6 - 11 years,Not Disclosed,['Hyderabad'],"Senior Manager, Data Engineer\nThe Opportunity\nBased in Hyderabad, join a global healthcare biopharma company and be part of a 130- year legacy of success backed by ethical integrity, forward momentum, and an inspiring mission to achieve new milestones in global healthcare.\nBe part of an organisation driven by digital technology and data-backed approaches that support a diversified portfolio of prescription medicines, vaccines, and animal health products.\nDrive innovation and execution excellence. Be a part of a team with passion for using data, analytics, and insights to drive decision-making, and which creates custom software, allowing us to tackle some of the worlds greatest health threats.\nOur Technology Centers focus on creating a space where teams can come together to deliver business solutions that save and improve lives. An integral part of our company s IT operating model, Tech Centers are globally distributed locations where each IT division has employees to enable our digital transformation journey and drive business outcomes. These locations, in addition to the other sites, are essential to supporting our business and strategy.\nA focused group of leaders in each Tech Center helps to ensure we can manage and improve each location, from investing in growth, success, and well-being of our people, to making sure colleagues from each IT division feel a sense of belonging to managing critical emergencies. And together, we must leverage the strength of our team to collaborate globally to optimize connections and share best practices across the Tech Centers.\nRole Overview\nResponsibilities\nDesigns, builds, and maintains data pipeline architecture - ingest, process, and publish data for consumption.\nBatch processes collected data, formats data in an optimized way to bring it analyze-ready\nEnsures best practices sharing and across the organization\nEnables delivery of data-analytics projects\nDevelops deep knowledge of the companys supported technology; understands the whole complexity/dependencies between multiple teams, platforms (people, technologies)\nCommunicates intensively with other platform/competencies to comprehend new trends and methodologies being implemented/considered within the company ecosystem\nUnderstands the customer and stakeholders business needs/priorities and helps building solutions that support our business goals\nEstablishes and manages the close relationship with customers/stakeholders\nHas overview of the date engineering market development to be able to come up/explore new ways of delivering pipelines to increase their value/contribution\nBuilds community of practice leveraging experience from delivering complex analytics projects\nIs accountable for ensuring that the team delivers solutions with high quality standards, timeliness, compliance and excellent user experience\nContributes to innovative experiments, specifically to idea generation, idea incubation and/or experimentation, identifying tangible and measurable criteria\nQualifications:\nBachelor s degree in Computer Science, Data Science, Information Technology, Engineering or a related field.\n3+ plus years of experience as a Data Engineer or in a similar role, with a strong portfolio of data projects.\n3+ plus years experience SQL skills, with the ability to write and optimize queries for large datasets.\n1+ plus years experience and proficiency in Python for data manipulation, automation, and pipeline development.\nExperience with Databricks including creating notebooks and utilizing Spark for big data processing.\nStrong experience with data warehousing solution (such as Snowflake), including schema design and performance optimization.\nExperience with data governance and quality management tools, particularly Collibra DQ.\nStrong analytical and problem-solving skills, with an attention to detail.\nSAP Basis experience working on SAP S/4HANA deployments on Cloud platforms (example: AWS, GCP or Azure).\nOur technology teams operate as business partners, proposing ideas and innovative solutions that enable new organizational capabilities. We collaborate internationally to deliver services and solutions that help everyone be more productive and enable innovation.\nWho we are:\nWhat we look for:\n#HYDIT\nCurrent Employees apply HERE\nCurrent Contingent Workers apply HERE\nSearch Firm Representatives Please Read Carefully\nEmployee Status:\nRegular\nRelocation:\nVISA Sponsorship:\nTravel Requirements:\nFlexible Work Arrangements:\nHybrid\nShift:\nValid Driving License:\nHazardous Material(s):\n\nRequired Skills:\nBusiness, Business, Business Intelligence (BI), Business Management, Contractor Management, Cost Reduction, Database Administration, Database Optimization, Data Engineering, Data Flows, Data Infrastructure, Data Management, Data Modeling, Data Optimization, Data Quality, Data Visualization, Design Applications, ETL Tools, Information Management, Management Process, Operating Cost Reduction, Senior Program Management, Social Collaboration, Software Development, Software Development Life Cycle (SDLC) {+ 1 more}\n\nPreferred Skills:\nJob Posting End Date:\n08/13/2025\n*A job posting is effective until 11:59:59PM on the day BEFORE the listed job posting end date. Please ensure you apply to a job posting no later than the day BEFORE the job posting end date.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'Data management', 'Data modeling', 'Analytical', 'Healthcare', 'Data processing', 'Business intelligence', 'Information technology', 'Analytics', 'SQL']",2025-06-13 05:12:10
Senior Data Engineer,Randstad Global,5 - 8 years,Not Disclosed,['Hyderabad'],"Key Responsibilities\n\nDesign, develop, and maintain data pipelines and ETL processes focused on data transformation using SQL, Python, and DBT.\nDevelop complex SQL queries for data extraction, transformation, and loading (ETL), with focus on aggregation, cleansing, and modeling.\nLeverage Python to automate transformation tasks and implement custom logic in data workflows.\nBuild and manage DBT models for reusable, maintainable, and scalable data pipelines.\nAssemble reusable and scalable datasets aligned with business needs.\nEnsure data accuracy, consistency, and completeness through thorough validation, testing, and documentation.\nDevelop and maintain data storage solutions and implement transformation logic for analysis and reporting.\nCollaborate with cross-functional stakeholders to understand data requirements and provide actionable insights.\nTroubleshoot and resolve data-related technical issues and identify opportunities to improve data quality and reliability.\nDocument technical specifications and data transformation processes.\nAdhere to information security policies, ensure data compliance with PII, GDPR, and other relevant regulations.\nRequired Skills & Experience\n\nHands-on experience with Google Cloud Platform (GCP) and Google BigQuery.\nStrong expertise in SQL, database design, and query optimization.\nProven experience in designing and implementing ETL pipelines and data transformation flows.\nTechnical proficiency in data modeling, data mining, and segmentation techniques.\nExcellent numerical, analytical, and problem-solving skills.\nStrong attention to detail and a critical mindset for evaluating information.\nEffective stakeholder management and ability to handle multiple priorities.\nExcellent verbal and written communication skills.\nAbility to self-manage workload and work collaboratively within a team.\nDesirable Skills\n\nProficiency in SQL and Python programming.\nHands-on experience with DBT and version control tools like GitLab.\nFamiliarity with complex financial data models.\nExperience with data visualization tools such as Tableau or Google Data Studio.\nExposure to Salesforce and Salesforce Einstein Analytics.\nUnderstanding of Agile/Scrum methodologies.\nBachelor's degree in Computer Science, Information Technology, or a related field.\nData engineering certification and basic knowledge of AI/ML fundamentals are a plus.",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'Airflow', 'Google Cloud Platforms', 'ETL', 'SQL']",2025-06-13 05:12:12
Data Engineer Sr. Analyst,Accenture,2 - 3 years,Not Disclosed,['Kochi'],"Job Title - + +\n\n\n\nManagement Level:\n\n\n\nLocation:Kochi, Coimbatore, Trivandrum\n\n\n\nMust have skills:Python/Scala, Pyspark/Pytorch\n\n\n\n\nGood to have skills:Redshift\n\n\n\nJob\n\n\nSummary\n\nYoull capture user requirements and translate them into business and digitally enabled solutions across a range of industries. Your responsibilities will include:\n\n\n\nRoles and Responsibilities\nDesigning, developing, optimizing, and maintaining data pipelines that adhere to ETL principles and business goals\nSolving complex data problems to deliver insights that helps our business to achieve their goals.\nSource data (structured unstructured) from various touchpoints, format and organize them into an analyzable format.\nCreating data products for analytics team members to improve productivity\nCalling of AI services like vision, translation etc. to generate an outcome that can be used in further steps along the pipeline.\nFostering a culture of sharing, re-use, design and operational efficiency of data and analytical solutions\nPreparing data to create a unified database and build tracking solutions ensuring data quality\nCreate Production grade analytical assets deployed using the guiding principles of CI/CD.\n\n\nProfessional and Technical Skills\nExpert in Python, Scala, Pyspark, Pytorch, Javascript (any 2 at least)\nExtensive experience in data analysis (Big data- Apache Spark environments), data libraries (e.g. Pandas, SciPy, Tensorflow, Keras etc.), and SQL. 2-3 years of hands-on experience working on these technologies.\nExperience in one of the many BI tools such as Tableau, Power BI, Looker.\nGood working knowledge of key concepts in data analytics, such as dimensional modeling, ETL, reporting/dashboarding, data governance, dealing with structured and unstructured data, and corresponding infrastructure needs.\nWorked extensively in Microsoft Azure (ADF, Function Apps, ADLS, Azure SQL), AWS (Lambda,Glue,S3), Databricks analytical platforms/tools, Snowflake Cloud Datawarehouse.\n\n\n\n\nAdditional Information\nExperience working in cloud Data warehouses like Redshift or Synapse\nCertification in any one of the following or equivalent\nAWS- AWS certified data Analytics- Speciality\nAzure- Microsoft certified Azure Data Scientist Associate\nSnowflake- Snowpro core- Data Engineer\nDatabricks Data Engineering\n\nQualification\n\n\n\nExperience:3.5 -5 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scala', 'pyspark', 'pytorch', 'python', 'microsoft azure', 'glue', 'amazon redshift', 'sql', 'tensorflow', 'sql azure', 'spark', 'keras', 'big data', 'etl', 'scipy', 'snowflake', 'data analysis', 'azure data lake', 'power bi', 'data engineering', 'javascript', 'pandas', 'data bricks', 'tableau', 'lambda expressions', 'aws']",2025-06-13 05:12:14
Data Engineer - Senior Analyst,Accenture,2 - 3 years,Not Disclosed,['Kochi'],"Job Title - + +\n\n\n\nManagement Level:\n\n\n\nLocation:Kochi, Coimbatore, Trivandrum\n\n\n\nMust have skills:Python/Scala, Pyspark/Pytorch\n\n\n\n\nGood to have skills:Redshift\n\n\n\nExperience:3.5 -5 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)\n\n\n\nJob\n\n\nSummary\n\nYoull capture user requirements and translate them into business and digitally enabled solutions across a range of industries. Your responsibilities will include:\n\n\n\nRoles and Responsibilities\nDesigning, developing, optimizing, and maintaining data pipelines that adhere to ETL principles and business goals\nSolving complex data problems to deliver insights that helps our business to achieve their goals.\nSource data (structured unstructured) from various touchpoints, format and organize them into an analyzable format.\nCreating data products for analytics team members to improve productivity\nCalling of AI services like vision, translation etc. to generate an outcome that can be used in further steps along the pipeline.\nFostering a culture of sharing, re-use, design and operational efficiency of data and analytical solutions\nPreparing data to create a unified database and build tracking solutions ensuring data quality\nCreate Production grade analytical assets deployed using the guiding principles of CI/CD.\n\n\nProfessional and Technical Skills\nExpert in Python, Scala, Pyspark, Pytorch, Javascript (any 2 at least)\nExtensive experience in data analysis (Big data- Apache Spark environments), data libraries (e.g. Pandas, SciPy, Tensorflow, Keras etc.), and SQL. 2-3 years of hands-on experience working on these technologies.\nExperience in one of the many BI tools such as Tableau, Power BI, Looker.\nGood working knowledge of key concepts in data analytics, such as dimensional modeling, ETL, reporting/dashboarding, data governance, dealing with structured and unstructured data, and corresponding infrastructure needs.\nWorked extensively in Microsoft Azure (ADF, Function Apps, ADLS, Azure SQL), AWS (Lambda,Glue,S3), Databricks analytical platforms/tools, Snowflake Cloud Datawarehouse.\n\n\n\n\nAdditional Information\nExperience working in cloud Data warehouses like Redshift or Synapse\nCertification in any one of the following or equivalent\nAWS- AWS certified data Analytics- Speciality\nAzure- Microsoft certified Azure Data Scientist Associate\nSnowflake- Snowpro core- Data Engineer\nDatabricks Data Engineering\n\nQualification\n\n\n\nExperience:3.5 -5 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scala', 'pyspark', 'pytorch', 'python', 'microsoft azure', 'glue', 'amazon redshift', 'sql', 'tensorflow', 'sql azure', 'spark', 'keras', 'big data', 'etl', 'scipy', 'snowflake', 'data analysis', 'azure data lake', 'power bi', 'data engineering', 'javascript', 'pandas', 'data bricks', 'tableau', 'lambda expressions', 'aws']",2025-06-13 05:12:16
Senior PySpark Data Engineer,Synechron,7 - 12 years,Not Disclosed,"['Pune', 'Hinjewadi']","Job Summary\nSynechron is seeking an experienced and technically proficient Senior PySpark Data Engineer to join our data engineering team. In this role, you will be responsible for developing, optimizing, and maintaining large-scale data processing solutions using PySpark. Your expertise will support our organizations efforts to leverage big data for actionable insights, enabling data-driven decision-making and strategic initiatives.\nSoftware Requirements\nRequired Skills:\nProficiency in PySpark\nFamiliarity with Hadoop ecosystem components (e.g., HDFS, Hive, Spark SQL)\nExperience with Linux/Unix operating systems\nData processing tools like Apache Kafka or similar streaming platforms\nPreferred Skills:\nExperience with cloud-based big data platforms (e.g., AWS EMR, Azure HDInsight)\nKnowledge of Python (beyond PySpark), Java or Scala relevant to big data applications\nFamiliarity with data orchestration tools (e.g., Apache Airflow, Luigi)\nOverall Responsibilities\nDesign, develop, and optimize scalable data processing pipelines using PySpark.\nCollaborate with data engineers, data scientists, and business analysts to understand data requirements and deliver solutions.\nImplement data transformations, aggregations, and extraction processes to support analytics and reporting.\nManage large datasets in distributed storage systems, ensuring data integrity, security, and performance.\nTroubleshoot and resolve performance issues within big data workflows.\nDocument data processes, architectures, and best practices to promote consistency and knowledge sharing.\nSupport data migration and integration efforts across varied platforms.\nStrategic Objectives:\nEnable efficient and reliable data processing to meet organizational analytics and reporting needs.\nMaintain high standards of data security, compliance, and operational durability.\nDrive continuous improvement in data workflows and infrastructure.\nPerformance Outcomes & Expectations:\nEfficient processing of large-scale data workloads with minimum downtime.\nClear, maintainable, and well-documented code.\nActive participation in team reviews, knowledge transfer, and innovation initiatives.\nTechnical Skills (By Category)\nProgramming Languages:\nRequired: PySpark (essential); Python (needed for scripting and automation)\nPreferred: Java, Scala\nDatabases/Data Management:\nRequired: Experience with distributed data storage (HDFS, S3, or similar) and data warehousing solutions (Hive, Snowflake)\nPreferred: Experience with NoSQL databases (Cassandra, HBase)\nCloud Technologies:\nRequired: Familiarity with deploying and managing big data solutions on cloud platforms such as AWS (EMR), Azure, or GCP\nPreferred: Cloud certifications\nFrameworks and Libraries:\nRequired: Spark SQL, Spark MLlib (basic familiarity)\nPreferred: Integration with streaming platforms (e.g., Kafka), data validation tools\nDevelopment Tools and Methodologies:\nRequired: Version control systems (e.g., Git), Agile/Scrum methodologies\nPreferred: CI/CD pipelines, containerization (Docker, Kubernetes)\nSecurity Protocols:\nOptional: Basic understanding of data security practices and compliance standards relevant to big data management\nExperience Requirements\nMinimum of 7+ years of experience in big data environments with hands-on PySpark development.\nProven ability to design and implement large-scale data pipelines.\nExperience working with cloud and on-premises big data architectures.\nPreference for candidates with domain-specific experience in finance, banking, or related sectors.\nCandidates with substantial related experience and strong technical skills in big data, even from different domains, are encouraged to apply.\nDay-to-Day Activities\nDevelop, test, and deploy PySpark data processing jobs to meet project specifications.\nCollaborate in multi-disciplinary teams during sprint planning, stand-ups, and code reviews.\nOptimize existing data pipelines for performance and scalability.\nMonitor data workflows, troubleshoot issues, and implement fixes.\nEngage with stakeholders to gather new data requirements, ensuring solutions are aligned with business needs.\nContribute to documentation, standards, and best practices for data engineering processes.\nSupport the onboarding of new data sources, including integration and validation.\nDecision-Making Authority & Responsibilities:\nIdentify performance bottlenecks and propose effective solutions.\nDecide on appropriate data processing approaches based on project requirements.\nEscalate issues that impact project timelines or data integrity.\nQualifications\nBachelors degree in Computer Science, Information Technology, or related field. Equivalent experience considered.\nRelevant certifications are preferred: Cloudera, Databricks, AWS Certified Data Analytics, or similar.\nCommitment to ongoing professional development in data engineering and big data technologies.\nDemonstrated ability to adapt to evolving data tools and frameworks.\nProfessional Competencies\nStrong analytical and problem-solving skills, with the ability to model complex data workflows.\nExcellent communication skills to articulate technical solutions to non-technical stakeholders.\nEffective teamwork and collaboration in a multidisciplinary environment.\nAdaptability to new technologies and emerging trends in big data.\nAbility to prioritize tasks effectively and manage time in fast-paced projects.\nInnovation mindset, actively seeking ways to improve data infrastructure and processes.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['PySpark', 'S3', 'Unix operating systems', 'Spark SQL', 'Luigi', 'HDFS', 'AWS EMR', 'Apache Airflow', 'Hive', 'Linux', 'Azure HDInsight', 'Apache Kafka', 'AWS']",2025-06-13 05:12:18
Senior Data Engineer - Azure,Blend360 India,6 - 11 years,Not Disclosed,['Hyderabad'],"Job Description\nAs a Senior Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\nQualifications\n5+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar fie",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-13 05:12:20
Senior Data Engineer - Azure,Blend360 India,6 - 11 years,Not Disclosed,['Hyderabad'],"Job Description\nAs a Senior Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Access control', 'Data analysis', 'Team leading', 'Architecture', 'Analytical', 'Agile', 'data governance', 'Data processing', 'Mentor', 'Data quality']",2025-06-13 05:12:22
Hadoop Data Engineer/ Senior Software Engineer,Hsbc,2 - 11 years,Not Disclosed,['Pune'],"Some careers shine brighter than others.\nIf you re looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further.\nHSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions.\nWe are currently seeking an experienced professional to join our team in the role of Software Engineer\nIn this role you will be\nExpertise in Scala-Spark/Python-Spark development and should be able to Work with Agile application dev team to implement data strategies.\nDesign and implement scalable data architectures to support the banks data needs.\nDevelop and maintain ETL (Extract, Transform, Load) processes.\nEnsure the data infrastructure is reliable, scalable, and secure.\nOversee the integration of diverse data sources into a cohesive data platform.\nEnsure data quality, data governance, and compliance with regulatory requirements.\nMonitor and optimize data pipeline performance.\nTroubleshoot and resolve data-related issues promptly.\nImplement monitoring and alerting systems for data processes.\nTroubleshoot and resolve technical issues optimizing system performance ensuring reliability.\nCreate and maintain technical documentation for new and existing system ensuring that information is accessible to the team.\nImplementing and monitoring solutions that identify both system bottlenecks and production issues.\n\n\n\n\n\n\n\n\n\n\nRequirements\n\n\n\nTo be successful in this role, you should meet the following requirements:\nExperience in data engineering or related field and hands-on experience of building and maintenance of ETL Data pipelines\nGood experience in Designing and Developing Spark Applications using Scala or Python.\nGood experience with database technologies (SQL, NoSQL), data warehousing solutions, and big data technologies (Hadoop, Spark)\nProficiency in programming languages such as Python, Java, or Scala.\nOptimization and Performance Tuning of Spark Applications\nGIT Experience on creating, merging and managing Repos.\nPerform unit testing and performance testing.\nGood understanding of ETL processes and data pipeline orchestration tools like Airflow, Control-M.\nStrong problem-solving skills and ability to work under pressure.\nExcellent communication and interpersonal skills.",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Performance testing', 'Agile', 'Control-M', 'Data quality', 'Unit testing', 'Financial services', 'SQL', 'Python', 'Technical documentation']",2025-06-13 05:12:23
Senior Data Engineer,Qualcomm,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Area: Information Technology Group, Information Technology Group > IT Data Engineer\n\nGeneral Summary:\n\nDeveloper will play an integral role in the PTEIT Machine Learning Data Engineering team. Design, develop and support data pipelines in a hybrid cloud environment to enable advanced analytics. Design, develop and support CI/CD of data pipelines and services. - 5+ years of experience with Python or equivalent programming using OOPS, Data Structures and Algorithms - Develop new services in AWS using server-less and container-based services. - 3+ years of hands-on experience with AWS Suite of services (EC2, IAM, S3, CDK, Glue, Athena, Lambda, RedShift, Snowflake, RDS) - 3+ years of expertise in scheduling data flows using Apache Airflow - 3+ years of strong data modelling (Functional, Logical and Physical) and data architecture experience in Data Lake and/or Data Warehouse - 3+ years of experience with SQL databases - 3+ years of experience with CI/CD and DevOps using Jenkins - 3+ years of experience with Event driven architecture specially on Change Data Capture - 3+ years of Experience in Apache Spark, SQL, Redshift (or) Big Query (or) Snowflake, Databricks - Deep understanding building the efficient data pipelines with data observability, data quality, schema drift, alerting and monitoring. - Good understanding of the Data Catalogs, Data Governance, Compliance, Security, Data sharing - Experience in building the reusable services across the data processing systems. - Should have the ability to work and contribute beyond defined responsibilities - Excellent communication and inter-personal skills with deep problem-solving skills.\n\nMinimum Qualifications:\n3+ years of IT-related work experience with a Bachelor's degree in Computer Engineering, Computer Science, Information Systems or a related field.\nOR\n5+ years of IT-related work experience without a Bachelors degree.\n\n2+ years of any combination of academic or work experience with programming (e.g., Java, Python).\n1+ year of any combination of academic or work experience with SQL or NoSQL Databases.\n1+ year of any combination of academic or work experience with Data Structures and algorithms.\n5 years of Industry experience and minimum 3 years experience in Data Engineering development with highly reputed organizations- Proficiency in Python and AWS- Excellent problem-solving skills- Deep understanding of data structures and algorithms- Proven experience in building cloud native software preferably with AWS suit of services- Proven experience in design and develop data models using RDBMS (Oracle, MySQL, etc.)\n\nDesirable - Exposure or experience in other cloud platforms (Azure and GCP) - Experience working on internals of large-scale distributed systems and databases such as Hadoop, Spark - Working experience on Data Lakehouse platforms (One House, Databricks Lakehouse) - Working experience on Data Lakehouse File Formats (Delta Lake, Iceberg, Hudi)\n\nBachelor's or Master's degree in Computer Science, Software Engineering, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['algorithms', 'python', 'data quality', 'data structures', 'aws', 'schema', 'continuous integration', 'glue', 'amazon redshift', 'event driven architecture', 'ci/cd', 'data engineering', 'sql', 'alerts', 'java', 'data modeling', 'spark', 'devops', 'data flow', 'nosql databases', 'sql database']",2025-06-13 05:12:26
Senior Data Engineer - Azure,Blend360 India,3 - 6 years,Not Disclosed,['Hyderabad'],"Job Description\nAs a Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\nQualifications\n3+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar fiel",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-13 05:12:27
Senior Data Engineer - AWS,Blend360 India,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Description\nWe are looking for an experienced Senior Data Engineer with a strong foundation in Python, SQL, and Spark , and hands-on expertise in AWS, Databricks . In this role, you will build and maintain scalable data pipelines and architecture to support analytics, data science, and business intelligence initiatives. You ll work closely with cross-functional teams to drive data reliability, quality, and performance.\nResponsibilities:\nDesign, develop, and optimize scalable data pipelines using Databricks in AWS such as Glue, S3, Lambda, EMR, Databricks notebooks, workflows and jobs.\nBuilding data lake in WS Databricks.\nBuild and maintain robust ETL/ELT workflows using Python and SQL to handle structured and semi-structured data.\nDevelop distributed data processing solutions using Apache Spark or PySpark .\nPartner with data scientists and analysts to provide high-quality, accessible, and well-structured data.\nEnsure data quality, governance, security, and compliance across pipelines and data stores.\nMonitor, troubleshoot, and improve the performance of data systems and pipelines.\nParticipate in code reviews and help establish engineering best practices.\nMentor junior data engineers and support their technical development.\n\n\nQualifications\nRequirements\nBachelors or masters degree in computer science, Engineering, or a related field.\n5+ years of hands-on experience in data",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Version control', 'GIT', 'Workflow', 'Data quality', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-13 05:12:29
Senior Data Engineer,Fractal Analytics,8 - 10 years,Not Disclosed,['Mumbai'],"Job Description:\nAs a Backend (Java) Engineer, you would be part of the team consisting of Scrum Master, Cloud Engineers, AI/ML Engineers, and UI/UX Engineers to build end-to-end Data to Decision Systems.\nMandatory:\n8+ years of demonstrable experience designing, building, and working as a Java Developer for enterprise web applications\nIdeally, this would include the following:\no Expert-level proficiency with Java\no Expert-level proficiency with SpringBoot\nFamiliarity with common databases (RDBMS such as MySQL & NoSQL such as MongoDB) and data warehousing concepts (OLAP, OLTP)\nUnderstanding of REST concepts and building/interacting with REST APIs\nDeep understanding of core backend concepts:\no Develop and design RESTful services and APIs\no Develop functional databases, applications, and servers to support websites on the back end\no Performance optimization and multithreading concepts\no Experience with deploying and maintaining high traffic infrastructure (performance testing is a plus)\nIn addition, the ideal candidate would have great problem-solving skills, and familiarity with code versioning tools such as GitHub",,,,"['Backend', 'Multithreading', 'RDBMS', 'MySQL', 'Performance testing', 'OLAP', 'Scrum', 'MongoDB', 'Apache', 'OLTP']",2025-06-13 05:12:31
Senior - AWS Data Engineering,KPMG India,4 - 8 years,Not Disclosed,['Gurugram'],"KPMG India is looking for Senior - AWS Data Engineering to join our dynamic team and embark on a rewarding career journey Designs and builds scalable data pipelines using AWS servicesOptimizes data ingestion, storage, and processingCollaborates with data scientists and analystsEnsures performance, security, and compliance",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Networking', 'Focus', 'Manager Technology', 'professional services', 'AWS', 'international clients']",2025-06-13 05:12:32
Sr Analyst I Data Engineering,DXC Technology,9 - 12 years,Not Disclosed,['Hyderabad'],"Job Description:\nEssential Job Functions:\nParticipate in data engineering tasks, including data processing and integration activities.\nAssist in the development and maintenance of data pipelines.\nCollaborate with team members to collect, process, and store data.\nContribute to data quality assurance efforts and adherence to data standards.\nUse data engineering tools and techniques to analyze and generate insights from data.\nCollaborate with data engineers and other analysts on data-related projects.\nSeek out opportunities to enhance data engineering skills and domain knowledge.\nStay informed about data engineering trends and best practices.\n\nBasic Qualifications:\nBachelors degree in a relevant field or equivalent combination of education and experience\nTypically, 5+ years of relevant work experience in industry, with a minimum of 2 years in a similar role\nProven experience in data engineering\nProficiencies in data engineering tools and technologies\nA continuous learner that stays abreast with industry knowledge and technology\n\nOther Qualifications:\nAdvanced degree in a relevant field a plus\nRelevant certifications, such as Oracle Certified Professional, MySQL Database Administrator a plus\nRecruitment fraud is a scheme in which fictitious job opportunities are offered to job seekers typically through online services, such as false websites, or through unsolicited emails claiming to be from the company. These emails may request recipients to provide personal information or to make payments as part of their illegitimate recruiting process. DXC does not make offers of employment via social media networks and DXC never asks for any money or payments from applicants at any point in the recruitment process, nor ask a job seeker to purchase IT or other equipment on our behalf. More information on employment scams is available here .",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Usage', 'Manager Quality Assurance', 'Senior Analyst', 'Social media', 'Manager Technology', 'Data processing', 'Data quality', 'Oracle', 'mysql database administrator', 'Recruitment']",2025-06-13 05:12:34
Data Engineer,Luxoft,5 - 10 years,Not Disclosed,['Pune'],"Project description\nYou'll be working in the GM Business Analytics team located in Pune. The successful candidate will be a member of the global Distribution team, which has team members in London and Pune.\n\nWe work as part of a global team providing analytical solutions for IB distribution/sales people. Solutions deployed should be extensible globally with minimal localization.\n\nResponsibilities\n\nAre you passionate about data and analyticsAre you keen to be part of the journey to modernize a data warehouse/ analytics suite of application(s). Do you take pride in the quality of software delivered for each development iteration\n\nWe're looking for someone like that to join us and\n\nbe a part of a high-performing team on a high-profile project.\n\nsolve challenging problems in an elegant way\n\nmaster state-of-the-art technologies\n\nbuild a highly responsive and fast updating application in an Agile & Lean environment\n\napply best development practices and effectively utilize technologies\n\nwork across the full delivery cycle to ensure high-quality delivery\n\nwrite high-quality code and adhere to coding standards\n\nwork collaboratively with diverse team(s) of technologists\n\nYou are:\n\nCurious and collaborative, comfortable working independently, as well as in a team\n\nFocused on delivery to the business\n\nStrong in analytical skills. For example, the candidate must understand the key dependencies among existing systems in terms of the flow of data among them. It is essential that the candidate learns to understand the 'big picture' of how IB industry/business functions.\n\nAble to quickly absorb new terminology and business requirements\n\nAlready strong in analytical tools, technologies, platforms, etc. The candidate must also demonstrate a strong desire for learning and self-improvement.\n\nOpen to learning home-grown technologies, support current state infrastructure and help drive future state migrations. imaginative and creative with newer technologies\n\nAble to accurately and pragmatically estimate the development effort required for specific objectives\n\nYou will have the opportunity to work under minimal supervision to understand local and global system requirements, design and implement the required functionality/bug fixes/enhancements. You will be responsible for components that are developed across the whole team and deployed globally.\n\nYou will also have the opportunity to provide third-line support to the application's global user community, which will include assisting dedicated support staff and liaising with the members of other development teams directly, some of which will be local and some remote.\n\nSkills\nMust have\n\nA bachelor's or master's degree, preferably in Information Technology or a related field (computer science, mathematics, etc.), focusing on data engineering.\n\n5+ years of relevant experience as a data engineer in Big Data is required.\n\nStrong Knowledge of programming languages (Python / Scala) and Big Data technologies (Spark, Databricks or equivalent) is required.\n\nStrong experience in executing complex data analysis and running complex SQL/Spark queries.\n\nStrong experience in building complex data transformations in SQL/Spark.\n\nStrong knowledge of Database technologies is required.\n\nStrong knowledge of Azure Cloud is advantageous.\n\nGood understanding and experience with Agile methodologies and delivery.\n\nStrong communication skills with the ability to build partnerships with stakeholders.\n\nStrong analytical, data management and problem-solving skills.\n\nNice to have\n\nExperience working on the QlikView tool\n\nUnderstanding of QlikView scripting and data model\n\nOther\n\nLanguages\n\nEnglishC1 Advanced\n\nSeniority\n\nSenior",Industry Type: Legal,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analysis', 'data management', 'big data technologies', 'sql', 'spark', 'python', 'scala', 'mathematics', 'business analytics', 'data engineering', 'azure cloud', 'qlikview', 'data bricks', 'computer science', 'database creation', 'data transformation', 'agile', 'big data', 'agile methodology']",2025-06-13 05:12:36
Data Engineer,Mobile Programming,5 - 10 years,Not Disclosed,['Mumbai'],"Candidate Skill:Technical Skills - Data Engineering | ETL | SQL | Python | AWS | Azure | Google Cloud | Hadoop | Spark | Kafka | Data Warehousing | Data Modeling | NoSQL | Data Quality\nWe are looking for an experienced Data Engineer to join our team in Mumbai. As a Data Engineer, you will be responsible for designing, building, and maintaining efficient data pipelines that transform raw data into actionable insights. You will work closely with data scientists and analysts to ensure that data is accessible, reliable, and optimized for analysis. Your role will also involve handling large datasets, ensuring data quality, and implementing data processing frameworks.\nKey Responsibilities:Design and build scalable data pipelines for processing, transforming, and integrating large datasets.Develop and maintain ETL processes to extract, transform, and load data from multiple sources into the data warehouse.Collaborate with data scientists and analysts to ensure that the data is optimized for analysis and modeling.Ensure the quality, integrity, and security of data throughout its lifecycle.\nWork with cloud-based technologies for data storage and processing (AWS, Azure, GCP).Implement data processing frameworks for efficient handling of structured and unstructured data.Troubleshoot and resolve issues related to data pipelines and workflows.Automate data integration processes and ensure data consistency and accuracy across systems.\nRequired Skills:\n5+ years of experience in Data Engineering with hands-on experience in data pipeline development.Strong expertise in ETL processes, data integration, and data warehousing.Proficiency in SQL, Python, and other programming languages for data manipulation.Experience with cloud technologies such as AWS, Azure, or Google Cloud.Knowledge of big data technologies like Hadoop, Spark, or Kafka is a plus.Strong understanding of data modeling, data quality, and data governance.\nFamiliarity with NoSQL databases (e.g., MongoDB, Cassandra) and relational databases.Strong analytical and problem-solving skills with the ability to work with large, complex datasets.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Azure', 'Hadoop', 'Kafka', 'SQL', 'Data Quality', 'NoSQL', 'GCP', 'Spark', 'Data Warehousing', 'Data Modeling', 'ETL', 'AWS', 'Python']",2025-06-13 05:12:38
Data Engineer For a US based IT Company based in Hyderabad,GLOBAL INSTITUTE FOR STAFFING & TRAINING...,5 - 10 years,16-20 Lacs P.A.,['Hyderabad( Kokapeta Village )'],"We are Hiring Data Engineer for a US based IT Company Based in Hyderabad. Candidates with minimum 5 Years of experience in Data Engineering can apply.\n\nThis job is for 1 year contract only\n\nJob Title: Data Engineer\nLocation: Hyderabad\nCTC: Upto 20 LPA\nExperience: 5+ Years\n\nJob Overview:\nWe are looking for a seasoned Senior Data Engineer with deep hands-on experience in Talend and IBM DataStage to join our growing enterprise data team. This role will focus on designing and optimizing complex data integration solutions that support enterprise-wide analytics, reporting, and compliance initiatives.\nIn this senior-level position, you will collaborate with data architects, analysts, and key stakeholders to facilitate large-scale data movement, enhance data quality, and uphold governance and security protocols.\n\nKey Responsibilities:\nDevelop, maintain, and enhance scalable ETL pipelines using Talend and IBM DataStage\nPartner with data architects and analysts to deliver efficient and reliable data integration solutions\nReview and optimize existing ETL workflows for performance, scalability, and reliability\nConsolidate data from multiple sourcesboth structured and unstructuredinto data lakes and enterprise platforms\nImplement rigorous data validation and quality assurance procedures to ensure data accuracy and integrity\nAdhere to best practices for ETL development, including source control and automated deployment\nMaintain clear and comprehensive documentation of data processes, mappings, and transformation rules\nSupport enterprise initiatives around data migration, modernization, and cloud transformation\nMentor junior engineers and participate in code reviews and team learning sessions\nRequired Qualifications:\nMinimum 5 years of experience in data engineering or ETL development\nProficient with Talend (Open Studio and/or Talend Cloud) and IBM DataStage\nStrong skills in SQL, data profiling, and performance tuning\nExperience handling large datasets and complex data workflows\nSolid understanding of data warehousing, data modeling, and data lake architecture\nFamiliarity with version control systems (e.g., Git) and CI/CD pipelines\nStrong analytical and troubleshooting skills\nEffective verbal and written communication, with strong documentation habits\n\nPreferred Qualifications:\nPrior experience in banking or financial services\nExposure to cloud platforms such as AWS, Azure, or Google Cloud\nKnowledge of data governance tools (e.g., Collibra, Alation)\nAwareness of data privacy regulations (e.g., GDPR, CCPA)\nExperience working in Agile/Scrum environments\n\nFor further assistance contact/whatsapp: 9354909518 or write to priya@gist.org.in",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Data Engineering', 'Kafka', 'Snowflake', 'Java', 'Spark', 'mongo', 'Ci/Cd', 'Data Pipeline', 'Agile', 'Scrum', 'Data Modeling', 'Talend', 'Oracle', 'AWS', 'Data Governance', 'Gdpr', 'azure', 'Python', 'Shell Scripting', 'Postgresql', 'Ibm Datastage', 'Workflow', 'Data Framework', 'GCPA', 'SQL', 'GIT', 'Amazon Redshift', 'GCP', 'Data Lake', 'Data Warehousing', 'ETL']",2025-06-13 05:12:40
Data Engineer,Atyeti,2 - 4 years,Not Disclosed,['Pune'],"Role & responsibilities\n\nDevelop and Maintain Data Pipelines: Design, develop, and manage scalable ETL pipelines to process large datasets using PySpark, Databricks, and other big data technologies.\nData Integration and Transformation: Work with various structured and unstructured data sources to build efficient data workflows and integrate them into a central data warehouse.\nCollaborate with Data Scientists & Analysts: Work closely with the data science and business intelligence teams to ensure the right data is available for advanced analytics, machine learning, and reporting.",,,,"['Azure Synapse', 'Pyspark', 'ETL', 'Python']",2025-06-13 05:12:42
Data Engineer,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nAs part of the cybersecurity organization, the Data Engineer is responsible for designing, building, and maintaining data infrastructure to support data-driven decision-making. This role involves working with large datasets, developing reports, executing data governance initiatives, and ensuring data is accessible, reliable, and efficiently managed. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture, ETL processes, and cybersecurity data frameworks.",,,,"['Data engineering', 'data security', 'Agile', 'cloud data platforms', 'Databricks', 'data governance frameworks', 'ETL', 'AWS', 'SQL', 'Python']",2025-06-13 05:12:43
Data Engineer,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nDesign, develop, and maintain complex ETL/ELT data pipelines in Databricks using PySpark, Scala, and SQL to process large-scale datasets",,,,"['Data engineering', 'Maven', 'data validation', 'PySpark', 'Scala', 'APIs', 'SQL Server', 'SQL', 'Jenkins', 'Git', 'MySQL', 'troubleshooting', 'MongoDB', 'ETL']",2025-06-13 05:12:45
Data Engineer,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"Role Description:\nAs part of the cybersecurity organization, In this vital role you will be responsible for designing, building, and maintaining data infrastructure to support data-driven decision-making. This role involves working with large datasets, developing reports, executing data governance initiatives, and ensuring data is accessible, reliable, and efficiently managed. The role sits at the intersection of data infrastructure and business insight delivery, requiring the Data Engineer to design and build robust data pipelines while also translating data into meaningful visualizations for stakeholders across the organization. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture, ETL processes, and cybersecurity data frameworks.",,,,"['data engineering', 'data analysis', 'data modeling', 'analysis tools', 'data warehousing', 'troubleshooting', 'data architecture', 'data integration', 'etl process']",2025-06-13 05:12:47
Data Engineer,Luxoft,5 - 10 years,Not Disclosed,['Pune'],"Are you passionate about data and analytics? Are you keen to be part of the journey to modernize a data warehouse/ analytics suite of application(s). Do you take pride in the quality of software delivered for each development iteration?\nWere looking for someone like that to join us and\nbe a part of a high-performing team on a high-profile project.\nsolve challenging problems in an elegant way\nmaster state-of-the-art technologies\nbuild a highly responsive and fast updating application in an Agile & Lean environment\napply best development practices and effectively utilize technologies\nwork across the full delivery cycle to ensure high-quality delivery\nwrite high-quality code and adhere to coding standards\nwork collaboratively with diverse team(s) of technologists\nYou are:\nCurious and collaborative, comfortable working independently, as well as in a team\nFocused on delivery to the business\nStrong in analytical skills. For example, the candidate must understand the key dependencies among existing systems in terms of the flow of data among them. It is essential that the candidate learns to understand the big picture of how IB industry/business functions.\nAble to quickly absorb new terminology and business requirements\nAlready strong in analytical tools, technologies, platforms, etc. The candidate must also demonstrate a strong desire for learning and self-improvement.\nOpen to learning home-grown technologies, support current state infrastructure and help drive future state migrations. imaginative and creative with newer technologies\nAble to accurately and pragmatically estimate the development effort required for specific objectives\nYou will have the opportunity to work under minimal supervision to understand local and global system requirements, design and implement the required functionality/bug fixes/enhancements. You will be responsible for components that are developed across the whole team and deployed globally.\nYou will also have the opportunity to provide third-line support to the applications global user community, which will include assisting dedicated support staff and liaising with the members of other development teams directly, some of which will be local and some remote.\nSkills\nMust have\nA bachelors or masters degree, preferably in Information Technology or a related field (computer science, mathematics, etc.), focusing on data engineering.\n5+ years of relevant experience as a data engineer in Big Data is required.\nStrong Knowledge of programming languages (Python / Scala) and Big Data technologies (Spark, Databricks or equivalent) is required.\nStrong experience in executing complex data analysis and running complex SQL/Spark queries.\nStrong experience in building complex data transformations in SQL/Spark.\nStrong knowledge of Database technologies is required.\nStrong knowledge of Azure Cloud is advantageous.\nGood understanding and experience with Agile methodologies and delivery.\nStrong communication skills with the ability to build partnerships with stakeholders.\nStrong analytical, data management and problem-solving skills.\nNice to have\nExperience working on the QlikView tool\nUnderstanding of QlikView scripting and data model\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nBig Data Engineer (Scala/Java/Python)\nBigData Development\nUnited States of America\nStamford, US\nBig Data Engineer (Scala/Java/Python)\nBigData Development\nUnited States of America\nWeehawken\nData Engineer - PostgreSQL\nBigData Development\nPoland\nRemote Poland\nPune, India\nReq. VR-114879\nBigData Development\nBCM Industry\n05/06/2025\nReq. VR-114879\nApply for Data Engineer in Pune\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data management', 'Coding', 'Postgresql', 'Agile', 'Information technology', 'Analytics', 'SQL', 'Python']",2025-06-13 05:12:48
Data Engineer,Databeat,3 - 7 years,Not Disclosed,['Hyderabad( Rai Durg )'],"Experience Required: 3+ years\n\nTechnical knowledge: AWS, Python, SQL, S3, EC2, Glue, Athena, Lambda, DynamoDB, RedShift, Step Functions, Cloud Formation, CI/CD Pipelines, Github, EMR, RDS,AWS Lake Formation, GitLab, Jenkins and AWS CodePipeline.\n\n\n\nRole Summary: As a Senior Data Engineer,with over 3 years of expertise in Python, PySpark, SQL to design, develop and optimize complex data pipelines, support data modeling, and contribute to the architecture that supports big data processing and analytics to cutting-edge cloud solutions that drive business growth. You will lead the design and implementation of scalable, high-performance data solutions on AWS and mentor junior team members.This role demands a deep understanding of AWS services, big data tools, and complex architectures to support large-scale data processing and advanced analytics.\nKey Responsibilities:\nDesign and develop robust, scalable data pipelines using AWS services, Python, PySpark, and SQL that integrate seamlessly with the broader data and product ecosystem.\nLead the migration of legacy data warehouses and data marts to AWS cloud-based data lake and data warehouse solutions.\nOptimize data processing and storage for performance and cost.\nImplement data security and compliance best practices, in collaboration with the IT security team.\nBuild flexible and scalable systems to handle the growing demands of real-time analytics and big data processing.\nWork closely with data scientists and analysts to support their data needs and assist in building complex queries and data analysis pipelines.\nCollaborate with cross-functional teams to understand their data needs and translate them into technical requirements.\nContinuously evaluate new technologies and AWS services to enhance data capabilities and performance.\nCreate and maintain comprehensive documentation of data pipelines, architectures, and workflows.\nParticipate in code reviews and ensure that all solutions are aligned to pre-defined architectural specifications.\nPresent findings to executive leadership and recommend data-driven strategies for business growth.\nCommunicate effectively with different levels of management to gather use cases/requirements and provide designs that cater to those stakeholders.\nHandle clients in multiple industries at the same time, balancing their unique needs.\nProvide mentoring and guidance to junior data engineers and team members.\n\n\n\nRequirements:\n3+ years of experience in a data engineering role with a strong focus on AWS, Python, PySpark, Hive, and SQL.\nProven experience in designing and delivering large-scale data warehousing and data processing solutions.\nLead the design and implementation of complex, scalable data pipelines using AWS services such as S3, EC2, EMR, RDS, Redshift, Glue, Lambda, Athena, and AWS Lake Formation.\nBachelor's or Masters degree in Computer Science, Engineering, or a related technical field.\nDeep knowledge of big data technologies and ETL tools, such as Apache Spark, PySpark, Hadoop, Kafka, and Spark Streaming.\nImplement data architecture patterns, including event-driven pipelines, Lambda architectures, and data lakes.\nIncorporate modern tools like Databricks, Airflow, and Terraform for orchestration and infrastructure as code.\nImplement CI/CD using GitLab, Jenkins, and AWS CodePipeline.\nEnsure data security, governance, and compliance by leveraging tools such as IAM, KMS, and AWS CloudTrail.\nMentor junior engineers, fostering a culture of continuous learning and improvement.\nExcellent problem-solving and analytical skills, with a strategic mindset.\nStrong communication and leadership skills, with the ability to influence stakeholders at all levels.\nAbility to work independently as well as part of a team in a fast-paced environment.\nAdvanced data visualization skills and the ability to present complex data in a clear and concise manner.\nExcellent communication skills, both written and verbal, to collaborate effectively across teams and levels.\n\nPreferred Skills:\nExperience with Databricks, Snowflake, and machine learning pipelines.\nExposure to real-time data streaming technologies and architectures.\nFamiliarity with containerization and serverless computing (Docker, Kubernetes, AWS Lambda).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Aws Glue', 'SQL', 'Data Pipeline', 'Python', 'Amazon Ec2', 'Data Engineering', 'Data Bricks', 'Aws Lambda', 'Amazon Redshift', 'Azure Cloud', 'Data Lake', 'Data Modeling', 'Athena']",2025-06-13 05:12:50
Data Engineer,Xenonstack,2 - 5 years,Not Disclosed,['Mohali( Phase 8B Mohali )'],"At XenonStack, We committed to become the Most Value Driven Cloud Native, Platform Engineering and Decision Driven Analytics Company. Our Consulting Services and Solutions towards the Neural Company and its Key Drivers.\nXenonStacks DataOps team is looking for a Data Engineer who will be responsible for employing techniques to create and sustain structures that allow for the analysis of data while remaining familiar with dominant programming and deployment strategies in the field.\nYou should demonstrate flexibility, creativity, and the capacity to receive and utilize constructive criticism. The ideal candidate should be highly skilled in all aspects of Python, Java/Scala, SQL and analytical skills.\nJob Responsibilities:\nDevelop, construct, test and maintain Data Platform Architectures\nAlign Data Architecture with business requirements\nLiaising with co-workers and clients to elucidate the requirements for each task.\nScalable and High Performant Data Platform Infrastructure that allows big data to be accessed and analysed quickly by BI & AI Teams.\nReformulating existing frameworks to optimize their functioning.\nTransforming Raw Data into InSights for manipulation by Data Scientists.\nEnsuring that your work remains backed up and readily accessible to relevant co-workers.\nRemaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.\nRequirements:\nTechnical Requirements\nExperience of Python, Java/Scala\nGreat Statistical / SQL based Analytical Skills\nExperience of Data Analytics Architectural Design Patterns for Batch, Event Driven and Real-Time Analytics Use Cases\nUnderstanding of Data warehousing, ETL tools, machine learning, Data EPIs\nExcellent in Algorithms and Data Systems\nUnderstanding of Distributed System for Data Processing and Analytics\nFamiliarity with Popular Data Analytics Framework like Hadoop , Spark , Delta Lake , Time Series / Analytical Stores Stores.\nProfessional Attributes:\nExcellent communication skills & Attention to detail.\nAnalytical mind and problem-solving Aptitude with Strong Organizational skills & Visual Thinking.\nBenefits:\nDiscover the benefits of joining our team:\nDynamic and purposeful work culture in a people-oriented organization contributing to multi-million-dollar projects with guaranteed job security.\nOpen, authentic, and transparent communication fostering a warm work environment.\nRegular constructive feedback and exposure to diverse technologies.\nRecognition and rewards for exceptional performance achievements.\nAccess to certification courses & Skill Sessions to develop continually and refine your skills.\nAdditional allowances for team members assigned to specific projects.\nSpecial skill allowances to acknowledge and compensate for unique expertise.\nComprehensive medical insurance policy for your health and well-being.\nTo Learn more about the company -\nWebsite - http://www.xenonstack.com/",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hadoop', 'Spark', 'ETL', 'Python', 'SQL', 'Java', 'Data Processing', 'Machine Learning']",2025-06-13 05:12:52
Data Engineer,Luxoft,5 - 8 years,Not Disclosed,['Pune'],"Project description\nAre you passionate about leveraging the latest technologies for strategic changeDo you enjoy problem solving in clever waysAre you organized enough to drive change across complex data systemsIf so, you could be the right person for this role.\nAs an experienced data engineer, you will join a global data analytics team in our Group Chief Technology Officer / Enterprise Architecture organization supporting our strategic initiatives which ranges from portfolio health to integration.\n\nResponsibilities\n\nHelp Group Enterprise Architecture team to develop our suite of EA tools and workbenches\n\nWork in the development team to support the development of portfolio health insights\n\nBuild data applications from cloud infrastructure to visualization layer\n\nProduce clear and commented code\n\nProduce clear and comprehensive documentation\n\nPlay an active role with technology support teams and ensure deliverables are completed or escalated on time\n\nProvide support on any related presentations, communications, and trainings\n\nBe a team player, working across the organization with skills to indirectly manage and influence\n\nBe a self-starter willing to inform and educate others\n\nSkills\nMust have\n\nB.Sc./M.Sc. degree in computing or similar\n\n5-8+ years' experience as a Data Engineer, ideally in a large corporate environment\n\nIn-depth knowledge of SQL and data modelling/data processing\n\nStrong experience working with Microsoft Azure\n\nExperience with visualisation tools like PowerBI (or Tableau, QlikView or similar)\n\nExperience working with Git, JIRA, GitLab\n\nStrong flair for data analytics\n\nStrong flair for IT architecture and IT architecture metrics\n\nExcellent stakeholder interaction and communication skills\n\nUnderstanding of performance implications when making design decisions to deliver performant and maintainable software.\n\nExcellent end-to-end SDLC process understanding.\n\nProven track record of delivering complex data apps on tight timelines\n\nFluent in English both written and spoken.\n\nPassionate about development with focus on data and cloud\n\nAnalytical and logical, with strong problem solving skills\n\nA team player, comfortable with taking the lead on complex tasks\n\nAn excellent communicator who is adept in, handling ambiguity and communicating with both technical and non-technical audiences\n\nComfortable with working in cross-functional global teams to effect change\n\nPassionate about learning and developing your hard and soft professional skills\n\nNice to have\n\nExperience working in the financial industry\n\nExperience in complex metrics design and reporting\n\nExperience in using artificial intelligence for data analytics\n\nOther\n\nLanguages\n\nEnglishC1 Advanced\n\nSeniority\n\nSenior",Industry Type: Legal,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'data processing', 'microsoft azure', 'sql', 'data modeling', 'screening', 'it architecture', 'hiring', 'power bi', 'hrsd', 'knowledge of sql', 'data engineering', 'artificial intelligence', 'sourcing', 'qlikview', 'talent acquisition', 'tableau', 'git', 'recruitment', 'gitlab', 'sdlc', 'jira']",2025-06-13 05:12:54
Data Engineer,Shyftlabs,5 - 10 years,Not Disclosed,['Noida'],"Position Overview\nWe are looking for an experienced Lead Data Engineer to join our dynamic team. If you are passionate about building scalable software solutions, and work collaboratively with cross-functional teams to define requirements and deliver solutions we would love to hear from you.\nJob Responsibilities:\nDevelop and maintain data pipelines and ETL/ELT processes using Python\nDesign and implement scalable, high-performance applications\nWork collaboratively with cross-functional teams to define requirements and deliver solutions\nDevelop and manage near real-time data streaming solutions using Pub, Sub or Beam.\nContribute to code reviews, architecture discussions, and continuous improvement initiatives\nMonitor and troubleshoot production systems to ensure reliability and performance\nBasic Qualifications:\n5+ years of professional software development experience with Python\nStrong understanding of software engineering best practices (testing, version control, CI/CD)\nExperience building and optimizing ETL/ELT processes and data pipelines\nProficiency with SQL and database concepts\nExperience with data processing frameworks (e.g., Pandas)\nUnderstanding of software design patterns and architectural principles\nAbility to write clean, well-documented, and maintainable code\nExperience with unit testing and test automation\nExperience working with any cloud provider (GCP is preferred)\nExperience with CI/CD pipelines and Infrastructure as code\nExperience with Containerization technologies like Docker or Kubernetes\nBachelors degree in Computer Science, Engineering, or related field (or equivalent experience)\nProven track record of delivering complex software projects\nExcellent problem-solving and analytical thinking skills\nStrong communication skills and ability to work in a collaborative environment\nPreferred Qualifications:\nExperience with GCP services, particularly Cloud Run and Dataflow\nExperience with stream processing technologies (Pub/Sub)\nFamiliarity with big data technologies (Airflow)\nExperience with data visualization tools and libraries\nKnowledge of CI/CD pipelines with Gitlab and infrastructure as code with Terraform\nFamiliarity with platforms like Snowflake, Bigquery or Databricks,.\nGCP Data engineer certification",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Software design', 'Version control', 'Analytical', 'Data processing', 'Unit testing', 'data visualization', 'Continuous improvement', 'SQL', 'Python']",2025-06-13 05:12:55
Microsoft Fabric Data engineer,Bahwan CyberTek,12 - 14 years,20-30 Lacs P.A.,"['Indore', 'Hyderabad']","Microsoft Fabric Data engineer\n\nCTC Range 12 14 Years\nLocation – Hyderabad/Indore\nNotice Period - Immediate\n* Primary Skill\n\nMicrosoft Fabric\nSecondary Skill 1\n\nAzure Data Factory (ADF)\n12+ years of experience in Microsoft Azure Data Engineering for analytical projects.\nProven expertise in designing, developing, and deploying high-volume, end-to-end ETL pipelines for complex models, including batch, and real-time data integration frameworks using Azure, Microsoft Fabric and Databricks.\nExtensive hands-on experience with Azure Data Factory, Databricks (with Unity Catalog), Azure Functions, Synapse Analytics, Data Lake, Delta Lake, and Azure SQL Database for managing and processing large-scale data integrations.\nExperience in Databricks cluster optimization and workflow management to ensure cost-effective and high-performance processing.\nSound knowledge of data modelling, data governance, data quality management, and data modernization processes.\nDevelop architecture blueprints and technical design documentation for Azure-based data solutions.\nProvide technical leadership and guidance on cloud architecture best practices, ensuring scalable and secure solutions.\nKeep abreast of emerging Azure technologies and recommend enhancements to existing systems.\nLead proof of concepts (PoCs) and adopt agile delivery methodologies for solution development and delivery.\nwww.yash.com\n\n'Information transmitted by this e-mail is proprietary to YASH Technologies and/ or its Customers and is intended for use only by the individual or entity to which it is addressed, and may contain information that is privileged, confidential or exempt from disclosure under applicable law. If you are not the intended recipient or it appears that this mail has been forwarded to you without proper authority, you are notified that any use or dissemination of this information in any manner is strictly prohibited. In such cases, please notify us immediately at info@yash.com and delete this mail from your records.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Azure Data Factory', 'Azure Synapse', 'Microsoft Fabric', 'Microsoft Azure Data Engineering', 'Pyspark', 'Data Bricks', 'SQL', 'Azure Data Lake', 'Microsoft Azure', 'Spark', 'ETL', 'Python']",2025-06-13 05:12:57
Data Engineer,Konrad Group,3 - 7 years,15-30 Lacs P.A.,['Gurugram( Sector 42 Gurgaon )'],"Who We Are\n\nKonrad is a next generation digital consultancy. We are dedicated to solving complex business problems for our global clients with creative and forward-thinking solutions. Our employees enjoy a culture built on innovation and a commitment to creating best-in-class digital products in use by hundreds of millions of consumers around the world. We hire exceptionally smart, analytical, and hard working people who are lifelong learners.\nAbout The Role\nAs a Data Engineer youll be tasked with designing, building, and maintaining scalable data platforms and pipelines. Your deep knowledge of data platforms such as Azure Fabric, Databricks, and Snowflake will be essential as you collaborate closely with data analysts, scientists, and other engineers to ensure reliable, secure, and efficient data solutions.\n\nWhat Youll Do\n\nDesign, build, and manage robust data pipelines and data architectures.\nImplement solutions leveraging platforms such as Azure Fabric, Databricks, and Snowflake.\nOptimize data workflows, ensuring reliability, scalability, and performance.\nCollaborate with internal stakeholders to understand data needs and deliver tailored solutions.\nEnsure data security and compliance with industry standards and best practices.\nPerform data modelling, data extraction, transformation, and loading (ETL/ELT).\nIdentify and recommend innovative solutions to enhance data quality and analytics capabilities.\n\nQualifications\n\nBachelors degree or higher in Computer Science, Data Engineering, Information Technology, or a related field.\nAt least 3 years of professional experience as a Data Engineer or similar role.\nProficiency in data platforms such as Azure Fabric, Databricks, and Snowflake.\nHands-on experience with data pipeline tools, cloud services, and storage solutions.\nStrong programming skills in SQL, Python, or related languages.\nExperience with big data technologies and concepts (Spark, Hadoop, Kafka).\nExcellent analytical, troubleshooting, and problem-solving skills.\nAbility to effectively communicate technical concepts clearly to non-technical stakeholders.\nAdvanced English\n\nNice to have\n\nCertifications related to Azure Data Engineering, Databricks, or Snowflake.\nFamiliarity with DevOps practices and CI/CD pipelines.\n\nPerks and Benefits\n\nComprehensive Health & Wellness Benefits Package \nSocials, Outings & Retreats\nCulture of Learning & Development\nFlexible Working Hours\nWork from Home Flexibility\nService Recognition Programs\n\nKonrad is committed to maintaining a diverse work environment and is proud to be an equal opportunity employer. All qualified applicants, regardless of race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status will receive consideration for employment. If you have any accessibility requirements or concerns regarding the hiring process or employment with us, please notify us so we can provide suitable accommodation.\nWhile we sincerely appreciate all applications, only those candidates selected for an interview will be contacted.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hadoop', 'Azure Data Factory', 'Azure Databricks', 'Spark', 'Fabric', 'Python']",2025-06-13 05:12:59
Data Engineer,Infoobjects Inc.,3 - 6 years,Not Disclosed,['Jaipur'],"Role & responsibilities:\nDesign, develop, and maintain robust ETL/ELT pipelines to ingest and process data from multiple sources.\nBuild and maintain scalable and reliable data warehouses, data lakes, and data marts.\nCollaborate with data scientists, analysts, and business stakeholders to understand data needs and deliver solutions.\nEnsure data quality, integrity, and security across all data systems.\nOptimize data pipeline performance and troubleshoot issues in a timely manner.\nImplement data governance and best practices in data management.\nAutomate data validation, monitoring, and reporting processes.\n\n\n\nPreferred candidate profile:\nBachelor's or Masters degree in Computer Science, Engineering, Information Systems, or related field.\nProven experience (X+ years) as a Data Engineer or similar role.\nStrong programming skills in Python, Java, or Scala.\nProficiency with SQL and working knowledge of relational databases (e.g., PostgreSQL, MySQL).\nHands-on experience with big data technologies (e.g., Spark, Hadoop).\nFamiliarity with cloud platforms such as AWS, GCP, or Azure (e.g., S3, Redshift, BigQuery, Data Factory).\nExperience with orchestration tools like Airflow or Prefect.\nKnowledge of data modeling, warehousing, and architecture design principles.\nStrong problem-solving skills and attention to detail.\n\nPerks and benefits\nFree Meals\nPF and Gratuity\nMedical and Term Insurance",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SCALA', 'Kafka', 'AWS', 'Python', 'Pyspark', 'Java', 'Postgresql', 'Hadoop', 'Spark', 'ETL', 'SQL']",2025-06-13 05:13:00
Data Engineer,XL India Business Services Pvt. Ltd,1 - 7 years,Not Disclosed,['Gurugram'],"Senior Engineer, Data Modeling Gurgaon/Bangalore, India AXA XL recognizes data and information as critical business assets, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained industrious advantage\n\nOur Chief Data Office also known as our Innovation, Data Intelligence & Analytics team (IDA) is focused on driving innovation through optimizing how we leverage data to drive strategy and create a new business model - disrupting the insurance market\n\nAs we develop an enterprise-wide data and digital strategy that moves us toward greater focus on the use of data and data-driven insights, we are seeking a Data Engineer\n\nThe role will support the team s efforts towards creating, enhancing, and stabilizing the Enterprise data lake through the development of the data pipelines\n\nThis role requires a person who is a team player and can work well with team members from other disciplines to deliver data in an efficient and strategic manner\n\nWhat you ll be doing What will your essential responsibilities include? Act as a data engineering expert and partner to Global Technology and data consumers in controlling complexity and cost of the data platform, whilst enabling performance, governance, and maintainability of the estate\n\nUnderstand current and future data consumption patterns, architecture (granular level), partner with Architects to make sure optimal design of data layers\n\nApply best practices in Data architecture\n\nFor example, balance between materialization and virtualization, optimal level of de-normalization, caching and partitioning strategies, choice of storage and querying technology, performance tuning\n\nLeading and hands-on execution of research into new technologies\n\nFormulating frameworks for assessment of new technology vs business benefit, implications for data consumers\n\nAct as a best practice expert, blueprint creator of ways of working such as testing, logging, CI/CD, observability, release, enabling rapid growth in data inventory and utilization of Data Science Platform\n\nDesign prototypes and work in a fast-paced iterative solution delivery model\n\nDesign, Develop and maintain ETL pipelines using Py spark in Azure Databricks using delta tables\n\nUse Harness for deployment pipeline\n\nMonitor Performance of ETL Jobs, resolve any issue that arose and improve the performance metrics as needed\n\nDiagnose system performance issue related to data processing and implement solution to address them\n\nCollaborate with other teams to make sure successful integration of data pipelines into larger system architecture requirement\n\nMaintain integrity and quality across all pipelines and environments\n\nUnderstand and follow secure coding practice to make sure code is not vulnerable\n\nYou will report to the Application Manager\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Effective Communication skills\n\nBachelor s degree in computer science, Mathematics, Statistics, Finance, related technical field, or equivalent work experience\n\nRelevant years of extensive work experience in various data engineering & modeling techniques (relational, data warehouse, semi-structured, etc), application development, advanced data querying skills\n\nRelevant years of programming experience using Databricks\n\nRelevant years of experience using Microsoft Azure suite of products (ADF, synapse and ADLS)\n\nSolid knowledge on network and firewall concepts\n\nSolid experience writing, optimizing and analyzing SQL\n\nRelevant years of experience with Python\n\nAbility to break complex data requirements and architect solutions into achievable targets\n\nRobust familiarity with Software Development Life Cycle (SDLC) processes and workflow, especially Agile\n\nExperience using Harness\n\nTechnical lead responsible for both individual and team deliveries\n\nDesired Skills and Abilities: Worked in big data migration projects\n\nWorked on performance tuning both at database and big data platforms\n\nAbility to interpret complex data requirements and architect solutions\n\nDistinctive problem-solving and analytical skills combined with robust business acumen\n\nExcellent basics on parquet files and delta files\n\nEffective Knowledge of Azure cloud computing platform\n\nFamiliarity with Reporting software - Power BI is a plus\n\nFamiliarity with DBT is a plus\n\nPassion for data and experience working within a data-driven organization\n\nYou care about what you do, and what we do\n\nWho WE are AXA XL, the P&C and specialty risk division of AXA, is known for solving complex risks\n\nFor mid-sized companies, multinationals and even some inspirational individuals we don t just provide re/insurance, we reinvent it\n\nHow? By combining a comprehensive and efficient capital platform, data-driven insights, leading technology, and the best talent in an agile and inclusive workspace, empowered to deliver top client service across all our lines of business property, casualty, professional, financial lines and specialty\n\nWith an innovative and flexible approach to risk solutions, we partner with those who move the world forward\n\nLearn more at axaxl\n\ncom What we OFFER Inclusion AXA XL is committed to equal employment opportunity and will consider applicants regardless of gender, sexual orientation, age, ethnicity and origins, marital status, religion, disability, or any other protected characteristic\n\nAt AXA XL, we know that an inclusive culture and a diverse workforce enable business growth and are critical to our success\n\nThat s why we have made a strategic commitment to attract, develop, advance and retain the most diverse workforce possible, and create an inclusive culture where everyone can bring their full selves to work and can reach their highest potential\n\nIt s about helping one another and our business to move forward and succeed\n\nFive Business Resource Groups focused on gender, LGBTQ+, ethnicity and origins, disability and inclusion with 20 Chapters around the globe Robust support for Flexible Working Arrangements Enhanced family friendly leave benefits Named to the Diversity Best Practices Index Signatory to the UK Women in Finance Charter Learn more at axaxl\n\ncom / about-us / inclusion-and-diversity\n\nAXA XL is an Equal Opportunity Employer\n\nTotal Rewards AXA XL s Reward program is designed to take care of what matters most to you, covering the full picture of your health, wellbeing, lifestyle and financial security\n\nIt provides dynamic compensation and personalized, inclusive benefits that evolve as you do\n\nWe re committed to rewarding your contribution for the long term, so you can be your best self today and look forward to the future with confidence\n\nSustainability At AXA XL, Sustainability is integral to our business strategy\n\nIn an ever-changing world, AXA XL protects what matters most for our clients and communities\n\nWe know that sustainability is at the root of a more resilient future\n\nOur 2023-26 Sustainability strategy, called Roots of resilience , focuses on protecting natural ecosystems, addressing climate change, and embedding sustainable practices across our operations\n\nOur Pillars: Valuing nature: How we impact nature affects how nature impacts us\n\nResilient ecosystems - the foundation of a sustainable planet and society - are essential to our future\n\nWe re committed to protecting and restoring nature - from mangrove forests to the bees in our backyard - by increasing biodiversity awareness and inspiring clients and colleagues to put nature at the heart of their plans\n\nAddressing climate change: The effects of a changing climate are far reaching and significant\n\nUnpredictable weather, increasing temperatures, and rising sea levels cause both social inequalities and environmental disruption\n\nWere building a net zero strategy, developing insurance products and services, and mobilizing to advance thought leadership and investment in societal-led solutions\n\nIntegrating ESG: All companies have a role to play in building a more resilient future\n\nIncorporating ESG considerations into our internal processes and practices builds resilience from the roots of our business\n\nWe re training our colleagues, engaging our external partners, and evolving our sustainability governance and reporting\n\nAXA Hearts in Action: We have established volunteering and charitable giving programs to help colleagues support causes that matter most to them, known as AXA XL s Hearts in Action programs\n\nThese include our Matching Gifts program, Volunteering Leave, and our annual volunteering day - the Global Day of Giving\n\nFor more information, please see axaxl\n\ncom/sustainability",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Performance tuning', 'Data modeling', 'Coding', 'Agile', 'Workflow', 'Application development', 'SDLC', 'SQL', 'Python', 'Firewall']",2025-06-13 05:13:02
Data Engineer 4,Comcast,5 - 11 years,Not Disclosed,['Chennai'],".\nResponsible for designing, building and overseeing the deployment and operation of technology architecture, solutions and software to capture, manage, store and utilize structured and unstructured data from internal and external sources. Establishes and builds processes and structures based on business and technical requirements to channel data from multiple inputs, route appropriately and store using any combination of distributed (cloud) structures, local databases, and other applicable storage forms as required. Develops technical tools and programming that leverage artificial intelligence, machine learning and big-data techniques to cleanse, organize and transform data and to maintain, defend and update data structures and integrity on an automated basis. Creates and establishes design standards and assurance processes for software, systems and applications development to ensure compatibility and operability of data connections, flows and storage requirements. Reviews internal and external business and product requirements for data operations and activity and suggests changes and upgrades to systems and storage to accommodate ongoing needs. Work with data modelers/analysts to understand the business problems they are trying to solve then create or augment data assets to feed their analysis. Integrates knowledge of business and functional priorities. Acts as a key contributor in a complex and crucial environment. May lead teams or projects and shares expertise.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBachelors Degree\nWhile possessing the stated degree is preferred, Comcast also may consider applicants who hold some combination of coursework and experience, or who have extensive related professional experience.\n7-10 Years\nComcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, genetic information, or any other basis protected by applicable law.",,,,"['Engineering services', 'Assurance', 'Process optimization', 'MySQL', 'Machine learning', 'Data structures', 'Data quality', 'Troubleshooting', 'Downstream', 'Python']",2025-06-13 05:13:04
Data Engineer,IT Services Company,2 - 3 years,6-7 Lacs P.A.,['Pune'],"Data Engineer\nJob Description :\nJash Data Sciences: Letting Data Speak!\nDo you love solving real-world data problems with the latest and best techniques? And having fun while solving them in a team! Then come and join our high-energy team of passionate data people. Jash Data Sciences is the right place for you.\nWe are a cutting-edge Data Sciences and Data Engineering startup based in Pune, India. We believe in continuous learning and evolving together. And we let the data speak!\nWhat will you be doing?\nYou will be discovering trends in the data sets and developing algorithms to transform\nraw data for further analytics\nCreate Data Pipelines to bring in data from various sources, with different formats,\ntransform it, and finally load it to the target database.\nImplement ETL/ ELT processes in the cloud using tools like AirFlow, Glue, Stitch, Cloud\nData Fusion, and DataFlow.\nDesign and implement Data Lake, Data Warehouse, and Data Marts in AWS, GCP, or\nAzure using Redshift, BigQuery, PostgreSQL, etc.\nCreating efficient SQL queries and understanding query execution plans for tuning\nqueries on engines like PostgreSQL.\nPerformance tuning of OLAP/ OLTP databases by creating indices, tables, and views.\nWrite Python scripts for the orchestration of data pipelines\nHave thoughtful discussions with customers to understand their data engineering\nrequirements. Break complex requirements into smaller tasks for execution.\nWhat do we need from you?\nStrong Python coding skills with basic knowledge of algorithms/data structures and\ntheir application.\nStrong understanding of Data Engineering concepts including ETL, ELT, Data Lake, Data\nWarehousing, and Data Pipelines.\nExperience designing and implementing Data Lakes, Data Warehouses, and Data Marts\nthat support terabytes of scale data.\nA track record of implementing Data Pipelines on public cloud environments\n(AWS/GCP/Azure) is highly desirable\nA clear understanding of Database concepts like indexing, query performance\noptimization, views, and various types of schemas.\nHands-on SQL programming experience with knowledge of windowing functions,\nsubqueries, and various types of joins.\nExperience working with Big Data technologies like PySpark/ Hadoop\nA good team player with the ability to communicate with clarity\nShow us your git repo/ blog!\nQualification\n1-2 years of experience working on Data Engineering projects for Data Engineer I\n2-5 years of experience working on Data Engineering projects for Data Engineer II\n1-5 years of Hands-on Python programming experience\nBachelors/Masters' degree in Computer Science is good to have\nCourses or Certifications in the area of Data Engineering will be given a higher preference.\nCandidates who have demonstrated a drive for learning and keeping up to date with technology by continuing to do various courses/self-learning will be given high preference.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Airflow', 'Elt', 'Data Mart', 'Data Pipeline', 'ETL', 'Pyspark', 'Hadoop', 'Data Bricks', 'SQL', 'Data Fusion', 'Glue', 'GCP', 'Data Flow', 'Data Warehousing', 'Azzure', 'AWS']",2025-06-13 05:13:06
Data Engineer- MS Fabric,InfoCepts,5 - 9 years,Not Disclosed,['India'],"Position: Data Engineer – MS Fabric\n  Purpose of the Position: As an MS Fabric Data engineer you will be responsible for designing, implementing, and managing scalable data pipelines. Strong experience in implementation and management of lake House using MS Fabric Azure Tech stack (ADLS Gen2, ADF, Azure SQL) .\nProficiency in data integration techniques, ETL processes and data pipeline architectures. Well versed in Data Quality rules, principles and implementation.\n",,,,"['components', 'data', 'scala', 'delta', 'pyspark', 'data warehousing', 'rules', 'azure data factory', 'sql', 'parquet', 'analytics', 'sql azure', 'spark', 'oracle adf', 'data pipeline architecture', 'etl', 'python', 'azure synapse', 'microsoft azure', 'power bi', 'data bricks', 'data quality', 'system', 't', 'fabric', 'data integration', 'etl process']",2025-06-13 05:13:08
Deputy Manager - Data Engineer - Analytics,IBM,2 - 7 years,Not Disclosed,['Bengaluru'],"Develop, test and support future-ready data solutions for customers across industry verticals\nDevelop, test, and support end-to-end batch and near real-time data flows/pipelines\nDemonstrate understanding in data architectures, modern data platforms, big data, analytics, cloud platforms, data governance and information management and associated technologies\nCommunicates risks and ensures understanding of these risks.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nMinimum of 2+ years of related experience required\nExperience in modeling and business system designs\nGood hands-on experience on DataStage, Cloud based ETL Services\nHave great expertise in writing TSQL code\nWell versed with data warehouse schemas and OLAP techniques\n\n\nPreferred technical and professional experience\nAbility to manage and make decisions about competing priorities and resources.\nAbility to delegate where appropriate\nMust be a strong team player/leader\nAbility to lead Data transformation project with multiple junior data engineers\nStrong oral written and interpersonal skills for interacting and throughout all levels of the organization.\nAbility to clearly communicate complex business problems and technical solutions.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['information management', 'cloud platforms', 'data architecture', 'data governance', 'big data', 'schema', 'python', 'data analytics', 'datastage', 'microsoft azure', 'warehouse', 't-sql', 'data engineering', 'ansible', 'docker', 'sql', 'java', 'devops', 'linux', 'olap', 'jenkins', 'shell scripting', 'etl', 'aws']",2025-06-13 05:13:10
Data Engineer-Data Platforms-Google,IBM,5 - 7 years,Not Disclosed,['Bengaluru'],"A career in IBM Consulting is rooted by long-term relationships and close collaboration with clients across the globe.You'll work with visionaries across multiple industries to improve the hybrid cloud and Al journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio; including Software and Red Hat.\n\nIn your role, you will be responsible for:\nSkilled Multiple GCP services - GCS, BigQuery, Cloud SQL, Dataflow, Pub/Sub, Cloud Run, Workflow, Composer, Error reporting, Log explorer etc.\nMust have Python and SQL work experience & Proactive, collaborative and ability to respond to critical situation\nAbility to analyse data for functional business requirements & front face customer\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n5 to 7 years of relevant experience working as technical analyst with Big Query on GCP platform.\nSkilled in multiple GCP services - GCS, Cloud SQL, Dataflow, Pub/Sub, Cloud Run, Workflow, Composer, Error reporting, Log explorer\nAmbitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work\nYou love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies.\nEnd to End functional knowledge of the data pipeline/transformation implementation that the candidate has done, should understand the purpose/KPIs for which data transformation was done\n\n\nPreferred technical and professional experience\nExperience with AEM Core Technologies OSGI Services, Apache Sling ,Granite Framework., Java Content Repository API, Java 8+, Localization\nFamiliarity with building tools, Jenkin and Maven , Knowledge of version control tools, especially Git, Knowledge of Patterns and Good Practices to design and develop quality and clean code, Knowledge of HTML, CSS, and JavaScript , jQuery\nFamiliarity with task management, bug tracking, and collaboration tools like JIRA and Confluence",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'java', 'html', 'python', 'javascript', 'hive', 'css', 'confluence', 'aem', 'data warehousing', 'apache sling', 'jquery', 'gen', 'git', 'gcp', 'spark', 'jenkins', 'bigquery', 'data transformation', 'hadoop', 'big data', 'etl', 'jira', 'cloud sql', 'maven', 'airflow', 'osgi', 'granite', 'agile', 'sqoop', 'aws']",2025-06-13 05:13:12
Data Engineer-Data Platforms-Google,IBM,5 - 7 years,Not Disclosed,['Bengaluru'],"Skilled Multiple GCP services - GCS, BigQuery, Cloud SQL, Dataflow, Pub/Sub, Cloud Run, Workflow, Composer, Error reporting, Log explorer etc.\nMust have Python and SQL work experience & Proactive, collaborative and ability to respond to critical situation\nAbility to analyse data for functional business requirements & front face customer\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n5 to 7 years of relevant experience working as technical analyst with Big Query on GCP platform.\nSkilled in multiple GCP services - GCS, Cloud SQL, Dataflow, Pub/Sub, Cloud Run, Workflow, Composer, Error reporting, Log explorer\nYou love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies\nAmbitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work\n\n\nPreferred technical and professional experience\nCreate up to 3 bullets maxitive individual with an ability to manage change and proven time management\nProven interpersonal skills while contributing to team effort by accomplishing related results as needed\nUp-to-date technical knowledge by attending educational workshops, reviewing publications (encouraging then to focus on required skills)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'gcp', 'bigquery', 'cloud sql', 'python', 'hive', 'gen', 'java', 'postgresql', 'spark', 'linux', 'mysql', 'hadoop', 'big data', 'pubsub', 'airflow', 'application engine', 'machine learning', 'sql server', 'dataproc', 'cloud storage', 'bigtable', 'agile', 'sqoop', 'aws', 'data flow']",2025-06-13 05:13:14
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Bengaluru'],"As a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution.\n\nYour primary responsibilities include:\nLead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements.\nStrive for continuous improvements by testing the build solution and working under an agile framework.\nDiscover and implement the latest technologies trends to maximize and build creative solutions\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience with Apache Spark (PySpark)In-depth knowledge of Spark’s architecture, core APIs, and PySpark for distributed data processing.\nBig Data TechnologiesFamiliarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering\n\nSkills:\nStrong understanding of ETL pipelines, data modeling, and data warehousing concepts.\nStrong proficiency in PythonExpertise in Python programming with a focus on data processing and manipulation. Data Processing FrameworksKnowledge of data processing libraries such as Pandas, NumPy.\nSQL ProficiencyExperience writing optimized SQL queries for large-scale data analysis and transformation.\nCloud PlatformsExperience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems\n\n\nPreferred technical and professional experience\nDefine, drive, and implement an architecture strategy and standards for end-to-end monitoring.\nPartner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering,\nGood to have detection and prevention tools for Company products and Platform and customer-facing",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data processing', 'pyspark', 'data modeling', 'spark', 'data analysis', 'microsoft azure', 'data warehousing', 'cloud platforms', 'numpy', 'data engineering', 'sql', 'pandas', 'spring boot', 'etl pipelines', 'java', 'gcp', 'kafka', 'data warehousing concepts', 'hadoop', 'big data', 'aws', 'etl']",2025-06-13 05:13:15
Sales Excellence - COE - Data Engineering Specialist,Accenture,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Title -\n\n\n\nSales Excellence - COE - Data Engineering Specialist\n\n\n\nManagement Level:\n\n\n\n9-Team Lead/Consultant\n\n\n\nLocation:\n\n\n\nMumbai, MDC2C\n\n\n\nMust-have skills:Sales\n\n\n\n\nGood to have skills:Data Science, SQL, Automation, Machine Learning\n\n\n\nJob\n\n\nSummary:\n\nApply deep statistical tools and techniques to find relationships between variables\n\n\n\n\nRoles & Responsibilities:\n\n- Apply deep statistical tools and techniques to find relationships between variables.\n\n- Develop intellectual property for analytical methodologies and optimization techniques.\n\n- Identify data requirements and develop analytic solutions to solve business issues.\n\nJob Title - Analytics & Modelling Specialist\n\nManagement Level :9-Specialist\n\nLocation:Bangalore/ Gurgaon/Hyderabad/Mumbai\n\nMust have skills:Python, Data Analysis, Data Visualization, SQL\nGood to have skills:Machine Learning\n\nJob\n\n\nSummary:\n\nThe Center of Excellence (COE) makes sure that the sales and pricing methods and offerings of Sales Excellence are effective.\n\n- The COE supports salespeople through its business partners and Analytics and Sales Operations teams.\n\nThe Data Engineer helps manage data sources and environments, utilizing large data sets and maintaining their integrity to create models and apps that deliver insights to the organization.\nRoles & Responsibilities:\n\nBuild and manage data models that bring together data from different sources.\n\nHelp consolidate and cleanse data for use by the modeling and development teams.\n\nStructure data for use in analytics applications.\n\nLead a team of Data Engineers effectively.\nProfessional & Technical\n\n\n\n\nSkills:\nA bachelors degree or equivalent\n\nTotal experience Range:5-8 years in the relevant field\n\nA minimum of 3 years of GCP experience with exposure to machine learning/data science\n\nExperience in configuration the machine learning workflow in GCP.\n\nA minimum of 5 years Advanced SQL knowledge and experience working with relational databases\n\nA minimum of 3 years Familiarity and hands on experience in different SQL objects like stored procedures, functions, views etc.,\n\nA minimum of 3 years Building of data flow components and processing systems to extract, transform, load and integrate data from various sources.\n\nA minimum of 3 years Hands on experience in advanced excel topics such as cube functions, VBA Automation, Power Pivot etc.\n\nA minimum of 3 years Hands on experience in Python\nAdditional Information:\n\nUnderstanding of sales processes and systems.\n\nMasters degree in a technical field.\n\nExperience with quality assurance processes.\n\nExperience in project management.\n\nYou May Also Need:\n\nAbility to work flexible hours according to business needs.\n\nMust have good internet connectivity and a distraction-free environment for working at home, in accordance with local guidelines.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | AccentureQualification\n\n\n\nExperience:8 to 10 Years\n\n\n\n\nEducational Qualification:\n\n\n\nB.Com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'data analysis', 'sales', 'sql', 'data visualization', 'hive', 'advance sql', 'ssas', 'dbms', 'machine learning', 'data engineering', 'power pivot', 'sql server', 'vba automation', 'data science', 'gcp', 'spark', 'advanced excel', 'hadoop', 'ssis', 'etl', 'big data', 'data flow', 'sql joins']",2025-06-13 05:13:17
"Data Engineer II, SCOT - AIM",Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"SCOTs Automated Inventory Management (AIM) team seeks talented individuals passionate about solving complex problems and driving impactful business decisions for our executives. The AIM team owns critical Tier 1 metrics for Amazon Retail stores, providing key insights to improve store health monitoring. We focus on enhancing selection, product availability, inventory efficiency, and inventory readiness to fulfill customer orders (FastTrack) while enabling accelerated delivery Speed Fulfillment Worldwide. This improves both Customer Experience (CX) and Long-Term Free Cash Flow (LTFCF) outcomes. Our approach involves creating standardized, scalable, and automated systems and tools to identify and reduce supply chain defects in our systems and inputs, while driving operational leverage and scaling.\n\nAs a Data Engineer, you will analyze large-scale business data, solve real-world problems, and develop metrics and business cases to delight our customers worldwide. You will work closely with Scientists, Engineers, and Product Managers to build scalable, high-impact products, architect data pipelines, and transform data into actionable insights to manage business at scale. We are looking for people who are motivated by thinking big, moving fast, and exploring business insights. If you love to implement solutions to hard problems while working hard, having fun, and making history, this may be the opportunity for you.\n\nAbout the team\nSupply Chain Optimization Technologies (SCOT) is the name of a complex group of systems designed to make the best decisions when it comes to forecasting, buying, placing, and shipping inventory. Functionally these teams work together to drive in-stock, drive placement, drive inventory removal and manage the customer experience.\n\n3+ years of data engineering experience\n4+ years of SQL experience\nExperience with data modeling, warehousing and building ETL pipelines Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions\nExperience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)\nExperience with big data technologies such as: Hadoop, Hive, Spark, EMR",,,,"['Supply chain', 'data engineer ii', 'Data modeling', 'Inventory management', 'Cash flow', 'Customer experience', 'Forecasting', 'Operations', 'Monitoring', 'SQL']",2025-06-13 05:13:19
Data Engineer II,Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Amazon s Consumer Payments organization is seeking a highly quantitative, experienced Data Engineer to drive growth through analytics, automation of data pipelines, and enhancement of self-serve experiences. . You will succeed in this role if you are an organized self-starter who can learn new technologies quickly and excel in a fast-paced environment. In this position, you will be a key contributor and sparring partner, developing analytics and insights that global executive management teams and business leaders will use to define global strategies and deep dive businesses.\nYou will be part the team that is focused on acquiring new merchants from around the world to payments around the world. The position is based in India but will interact with global leaders and teams in Europe, Japan, US, and other regions. You should be highly analytical, resourceful, customer focused, team oriented, and have an ability to work independently under time constraints to meet deadlines. You will be comfortable thinking big and diving deep. A proven track record in taking on end-to-end ownership and successfully delivering results in a fast-paced, dynamic business environment is strongly preferred.\nResponsibilities include but not limited to:\nDesign, develop, implement, test, and operate large-scale, high-volume, high-performance data structures for analytics and Reporting.\nImplement data structures using best practices in data modeling, ETL/ELT processes, and SQL, AWS Redshift, and OLAP technologies, Model data and metadata for ad hoc and pre-built reporting.\nWork with product tech teams and build robust and scalable data integration (ETL) pipelines using SQL, Python and Spark.\nContinually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers.\nInterface with business customers, gathering requirements and delivering complete reporting solutions.\nCollaborate with Analysts, Business Intelligence Engineers and Product Managers to implement algorithms that exploit rich data sets for statistical analysis, and machine learning.\nParticipate in strategic tactical planning discussions, including annual budget processes.\nCommunicate effectively with product / business / tech-teams / other Data teams.\n3+ years of data engineering experience\nExperience with data modeling, warehousing and building ETL pipelines Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions\nExperience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)",,,,"['Automation', 'metadata', 'Data modeling', 'Machine learning', 'Data structures', 'OLAP', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-13 05:13:21
Data Engineer II,Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Amazon strives to be the worlds most customer-centric company, where customers can research and purchase anything they might want online\nWe set big goals and are looking for people who can help us reach and exceed them\nThe CPT Data Engineering & Analytics (DEA) team builds and maintains critical data infrastructure that enhances seller experience and protects the privacy of Amazon business partners throughout their lifecycle\nWe are looking for a strong Data Engineer to join our team\n\nThe Data Engineer I will work with well-defined requirements to develop and maintain data pipelines that help internal teams gather required insights for business decisions timely and accurately\nYou will collaborate with a team of Data Scientists, Business Analysts and other Engineers to build solutions that reduce investigation defects and assess the health of our Operations business while ensuring data quality and regulatory compliance\n\nThe ideal candidate must be passionate about building reliable data infrastructure, detail-oriented, and driven to help protect Amazons customers and business partners\nThey will be an individual contributor who works effectively with guidance from senior team members to successfully implement data solutions\nThe candidate must be proficient in SQL and at least one scripting language (e\ng\nPython, Perl, Scala), with strong understanding of data management fundamentals and distributed systems concepts\n\n\nBuild and optimize physical data models and data pipelines for simple datasets\nWrite secure, stable, testable, maintainable code with minimal defects\nTroubleshoot existing datasets and maintain data quality\nParticipate in team design, scoping, and prioritization discussions\nDocument solutions to ensure ease of use and maintainability\nHandle data in accordance with Amazon policies and security requirements Masters degree in computer science, engineering, analytics, mathematics, statistics, IT or equivalent\n3+ years of data engineering experience\nExperience with SQL\nExperience with data modeling, warehousing and building ETL pipelines\nKnowledge of distributed systems concepts from data storage and compute perspective\nAbility to work effectively in a team environment Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions\nFamiliarity with big data technologies (Hadoop, Spark, etc\n)\nKnowledge of data security and privacy best practices\nStrong problem-solving and analytical skills\nExcellent written and verbal communication skills",Industry Type: Internet,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data management', 'Data modeling', 'data security', 'Perl', 'Data quality', 'Distribution system', 'Analytics', 'SQL', 'Python']",2025-06-13 05:13:23
Data Engineer,Hinduja Tech,6 - 10 years,Not Disclosed,['Pune'],"Education: Bachelors or masters degree in computer science, Information Technology, Engineering, or a related field.Experience: 6-10 years\n8+ years of experience in data engineering or a related field.\nStrong hands-on experience with Azure Databricks, Spark, Python/Scala, CICD, Scripting for data processing.\nExperience working in multiple file formats like Parquet, Delta, and Iceberg.\nKnowledge of Kafka or similar streaming technologies for real-time data ingestion.",,,,"['Data Engineer', 'Azure Databricks', 'ETL', 'Pyspark', 'AWS', 'Python', 'SQL']",2025-06-13 05:13:25
Data Engineer,Diverse Lynx,3 - 6 years,Not Disclosed,['Noida'],"Responsibilities\nAs part of the Client delivery team, your primary role would be to interface with the client for quality assurance, issue resolution and ensuring high customer satisfaction.\nYou will understand requirements, create and review designs, validate the architecture and ensure high levels of service offerings to clients in the technology domain.\nYou will participate in project estimation, provide inputs for solution delivery, conduct technical risk planning, perform code reviews and unit test plan reviews.\nYou will lead and guide your teams towards developing optimized high quality code deliverables, continual knowledge management and adherence to the organizational guidelines and processes.\nYou would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Manager Quality Assurance', 'Project estimation', 'Architecture', 'Customer satisfaction', 'Test planning', 'Issue resolution', 'Unit testing', 'Management', 'Solution delivery', 'digital transformation']",2025-06-13 05:13:26
Data Engineer,Mobio Solutions,3 - 5 years,Not Disclosed,['Ahmedabad'],"Job Overview:\nWe are looking for a skilled and experienced Data Engineer to join our team. The ideal candidate will have a strong background in Azure Data Factory, Databricks, Pyspark, Python , Azure SQL and other Azure cloud services, and will be responsible for building and managing scalable data pipelines, data lakes, and data warehouses . Experience with Azure Synapse Analytics, Microsoft Fabric or PowerBI will be considered a strong advantage.\nKey Responsibilities:\nDesign, develop, and manage robust and scalable ETL/ELT pipelines using Azure Data Factory and Databricks\nWork with PySpark and Python to transform and process large datasets\nBuild and maintain data lakes and data warehouses on Azure Cloud\nCollaborate with data architects, analysts, and stakeholders to gather and translate requirements into technical solutions\nEnsure data quality, consistency, and integrity across systems\nOptimize performance and cost of data pipelines and cloud infrastructure\nImplement best practices for security, governance, and monitoring of data pipelines\nMaintain and document data workflows and architecture\nRequired Skills & Qualifications:\n3-5 years of experience in Data Engineering\nStrong hands-on experience with:\nAzure Data Factory (ADF)\nAzure Databricks\nAzure SQL\nPySpark and Python\nAzure Storage (Blob, Data Lake Gen2)\nHands-on experience with data warehouse/Lakehouse/data lake architecture\nFamiliarity with Delta Lake, MLflow, and Unity Catalog is a plus\nGood understanding of SQL and performance tuning\nKnowledge of CI/CD in Azure for data pipelines\nExcellent problem-solving skills and ability to work independently\nPreferred Skills:\nExperience with Azure Synapse Analytics\nFamiliarity with Microsoft Fabric\nWorking knowledge of Power BI for data visualization and dashboarding\nExposure to DevOps and infrastructure as code (IaC) in Azure\nUnderstanding of data governance and security best practices\nDatabricks certification (e.g., Databricks Certified Data Engineer Associate/Professional)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Cloud', 'data governance', 'Data quality', 'data visualization', 'microsoft', 'Analytics', 'Monitoring', 'SQL', 'Python']",2025-06-13 05:13:28
Data Engineer - Asset Lending,Investec Global Services,2 - 5 years,Not Disclosed,['Mumbai'],"About the role :\nPrimary function of the role is to deliver high quality data engineering solutions to business and end users across Asset Lending (Asset Finance, Working Capital and Asset Based Lending businesses either directly via self-service data products, or by working closely with the Analytics team, providing modelled data warehouses on which they can add reporting and analytics.\nReporting to the Head of ccc Technology, this role will fill a crucial role in bridging the gap between business needs, the requirements from the data analytics team and translating these into engineering delivery.\nKey Responsibilities :\nWork closely with end-users and Data Analysts to understand the business and their data requirements\nCarry out ad hoc data analysis and data wrangling using Synapse Analytics and Databricks\nBuilding dynamic meta-data driven data ingestion patterns using Azure Data Factory and Databricks\nBuild and maintain the Enterprise Data Warehouse (using Data Vault 2.0 methodology)\nBuild and maintain business focused data products and data marts\nBuild and maintain Azure Analysis Services databases and cubes\nShare support and operational duties within the wider engineering and data teams\nWork with Architecture and Engineering teams to deliver on these projects. and ensure that supporting code and infrastructure follows best practices outlined by these teams.\nHelp define test criteria to establish clear conditions for success and ensure alignment with business objectives.\nManage their user stories and acceptance criteria through to production into day-to-day support\nAssist in the testing and validation of new requirements and processes to ensure they meet business need\nWhat are we looking for?\nExcellent data analysis and exploration using T-SQL\nStrong SQL programming (stored procedures, functions)\nExtensive experience with SQL Server and SSIS\nKnowledge and experience of data warehouse modelling methodologies (Kimball, dimensional modelling, Data Vault 2.0)\nExperience in Azure one or more of the following: Data Factory, Databricks, Synapse Analytics, ADLS Gen2\nExperience in building robust and performant ETL processes\nBuild and maintain Analysis Services databases and cubes (both multidimensional and tabular)\nExperience in using source control & ADO\nUnderstanding and experience of deployment pipelines\n",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'Agile', 'Stored procedures', 'SSIS', 'Operations', 'Data warehousing', 'Analytics', 'Analysis services', 'SQL', 'google maps']",2025-06-13 05:13:30
Data Engineer,Smartavya Analytica,3 - 5 years,Not Disclosed,['Pune'],"Job Title: Data Engineer\nLocation: Pune, India (On-site)\nExperience: 3 5 years\nEmployment Type: Full-time\n\nJob Summary\nWe are looking for a hands-on Data Engineer who can design and build modern Lakehouse solutions on Microsoft Azure. You will own data ingestion from source-system APIs through Azure Data Factory into OneLake, curate bronze silver gold layers on Delta Lake, and deliver dimensional models that power analytics at scale.",,,,"['Azure Data Factory', 'Azure Synapse', 'Adls Gen2', 'Data Lake', 'Fabric']",2025-06-13 05:13:31
Data Engineer - SSIS - 5+ Years - Gurugram (Hybrid),Crescendo Global,5 - 10 years,Not Disclosed,['Gurugram'],"Data Engineer - SSIS - 5+ Years - Gurugram (Hybrid)\n\nAre you a skilled Data Engineer with expertise in SSIS and 5+ years of experience? Do you have a passion for analytics and want to work in a hybrid setup in Gurugram? Our client is seeking a talented individual to join their team and contribute to their data engineering projects.\n\nLocation : Gurugram (Hybrid)\n\nYour Future EmployerOur client is a leading organization in the analytics domain, known for fostering an inclusive and diverse work environment. They are committed to providing their employees with opportunities for growth and development.\n\nResponsibilities\nDesign, develop, and maintain data pipelines using SSIS for efficient data processing\nCollaborate with cross-functional teams to understand data requirements and provide effective data solutions\nOptimize data pipelines for performance and scalability\nEnsure data quality and integrity throughout the data engineering process\n\nRequirements\n5+ years of experience in data engineering with a strong focus on SSIS\nProficiency in data warehousing concepts and ETL processes\nHands-on experience with SQL databases and data modeling4.Strong analytical and problem-solving skills\nBachelor's degree in Computer Science, Engineering, or related field\n\nWhat's in it for you : In this role, you will have the opportunity to work on challenging projects and enhance your expertise in data engineering. The organization offers a competitive compensation package and a supportive work environment where your contributions are valued.\n\nReach us : If you feel this opportunity is well aligned with your career progression plans, please feel free to reach me with your updated profile at rohit.kumar@crescendogroup.in\n\nDisclaimer : Crescendo Global specializes in Senior to C-level niche recruitment. We are passionate about empowering job seekers and employers with an engaging memorable job search and leadership hiring experience. Crescendo Global does not discriminate on the basis of race, religion, color, origin, gender, sexual orientation, age, marital status, veteran status or disability status.\n\nNote : We receive a lot of applications on a daily basis so it becomes a bit difficult for us to get back to each candidate. Please assume that your profile has not been shortlisted in case you don't hear back from us in 1 week. Your patience is highly appreciated.\n\nScammers can misuse Crescendo Globals name for fake job offers. We never ask for money, purchases, or system upgrades. Verify all opportunities at www.crescendo-global.com and report fraud immediately. Stay alert!\n\nProfile keywords : Data Engineer, SSIS, Data Warehousing, ETL, SQL, Analytics",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'SSIS', 'SQL', 'Analytics']",2025-06-13 05:13:33
Data Engineer,Fortune India 500 Chemicals Firm,12 - 18 years,Not Disclosed,['Mumbai (All Areas)'],"Skills:\nData Management: Expertise in data warehousing, SQL/NoSQL, cloud platforms (AWS, Azure, GCP)\nETL Tools: Proficient in Informatica, Talend, Azure Data Factory\nModelling: Strong in dimensional modelling, star/snowflake schema\nGovernance & Compliance: Knowledge of GDPR, HIPAA, data governance frameworks\nLanguages: T-SQL, PL/SQL\nSoft Skills: Effective communicator, strong analytical and problem-solving skills\nKey Responsibilities:\nArchitecture: Designed scalable, high-performance data warehouse architectures and data models\nETL & Integration: Led ETL design/development for structured/unstructured data across platforms\nGovernance: Defined data quality standards and collaborated on data governance policy implementation\nCollaboration: Interfaced with BI, data science, and business teams to align data strategies\nPerformance & Security: Optimized queries/ETL jobs and ensured data security and compliance\nDocumentation: Maintained standards and documentation for architecture, ETL, and workflows",Industry Type: Chemicals,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Warehousing', 'GCP', 'Snowflake', 'Microsoft Azure', 'Dimensional Modeling', 'Data Modeling', 'ETL', 'AWS']",2025-06-13 05:13:35
Data Engineer,Centrilogic,15 - 20 years,Not Disclosed,['Hyderabad'],"Data Engineer\n\nPurpose:\n\nOver 15 years, we have become a premier global provider of multi-cloud management, cloud-native application development solutions, and strategic end-to-end digital transformation services.\nHeadquartered in Canada and with regional headquarters in the U.S. and the United Kingdom, Centrilogic delivers smart, streamlined solutions to clients worldwide.\n\nWe are looking for a passionate and experienced Data Engineer to work with our other 70 Software, Data and DevOps engineers to guide and assist our clients data modernization journey.\n\nOur team works with companies with ambitious missions - clients who are creating new, innovative products, often in uncharted markets. We work as embedded members and leaders of our clients development and data teams. We bring experienced senior engineers, leading-edge technologies and mindsets, and creative thinking. We show our clients how to move to the modern frameworks of data infrastructures and processing, and we help them reach their full potential with the power of data.\n\nIn this role, youll be the day-to-day primary point of contact with our clients to modernize their data infrastructures, architecture, and pipelines.\n\nPrincipal Responsibilities:\n\nConsulting clients on cloud-first strategies for core bet-the-company data initiatives\nProviding thought leadership on both process and technical matters\nBecoming a real champion and trusted advisor to our clients on all facets of Data Engineering\nDesigning, developing, deploying, and supporting the modernization and transformation of our client s end-to-end data strategy, including infrastructure, collection, transmission, processing, and analytics\nMentoring and educating clients teams to keep them up to speed with the latest approaches, tools and skills, and setting them up for continued success post-delivery\n\nRequired Experience and Skills:\n\nMust have either Microsoft Certified Azure Data Engineer Associate or Fabric Data Engineer Associate certification.\nMust have experience working in a consulting or contracting capacity on large data management and modernization programs.\nExperience with SQL Servers, data engineering, on platforms such as Azure Data Factory, Databricks, Data Lake, and Synapse.\nStrong knowledge and demonstrated experience with Delta Lake and Lakehouse Architecture.\nStrong knowledge of securing Azure environment, such as RBAC, Key Vault, and Azure Security Center.\nStrong knowledge of Kafka and Spark and extensive experience using them in a production environment.\nStrong and demonstrable experience as DBA in large-scale MS SQL environments deployed in Azure.\nStrong problem-solving skills, with the ability to get to the route of an issue quickly.\nStrong knowledge of Scala or Python.\nStrong knowledge of Linux administration and networking.\nScripting skills and Infrastructure as Code (IaC) experience using PowerShell, Bash, and ARM templates.\nUnderstanding of security and corporate governance issues related with cloud-first data architecture, as well as accepted industry solutions.\nExperience in enabling continuous delivery for development teams using scripted cloud provisioning and automated tooling.\nExperience working with Agile development methodology that is fit for purpose.\nSound business judgment and demonstrated leadership",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['MS SQL', 'Networking', 'Data management', 'Powershell', 'Consulting', 'Application development', 'microsoft', 'Analytics', 'Python', 'Data architecture']",2025-06-13 05:13:36
Data Engineer,zocket,4 - 5 years,Not Disclosed,['Chennai'],"About Zocket: AI Assistant for Marketers\nZocket empowers businesses to scale social media advertising seamlessly with advanced AI . From search to conversions, Zocket automates the entire workflow effortlessly.\n\nHeadquartered in Chennai and San Francisco , with 500+ customers across 25+ countries.\n\nOur GenAI Product Suite:\nCreative Studio: Make ad creatives 10x faster, 20x better\n\nAudience Assistant: Precision targeting, platform-wide\n\nSnoop AI: Competitive & industry insights in seconds\n\nInsights AI: 360 performance dashboard across channels\n\nGrowth Infra: Whitelisted infra for unbounded scale\n\nTrusted and Recognized:\nAWS GenAI Accelerator 2024 (Top 80 global startups)\n\nGoogle for Startups (GFSA Class VIII Ai-first)\n\nNASSCOM Gen AI Foundry\n\n4.9 Trustpilot | #1 Product of the Day & Week on Product Hunt\n\n\nKey Responsibilities\n\nDesign, build, and maintain robust ETL pipelines to process large-scale structured and unstructured data.\n\nWork with GraphQL databases and Vector DBs (e.g., Pinecone, Chroma) to power intelligent, AI-driven features.\n\nBuild and optimize scalable ML pipelines and contribute to model lifecycle management .\n\nImplement real-time and batch data streaming solutions using Kafka and Spark .\n\nWork with Airflow to schedule and orchestrate data workflows.\n\nDevelop efficient data models in PostgreSQL and ensure high-performance data access.\n\nIntegrate and optimize Elasticsearch for fast search and analytics.\n\nEnsure high data quality and reliability through monitoring and automation.\n\nCollaborate with Product, Data Science, and Engineering teams to deliver scalable data solutions.\n\nOwn the data infrastructure on AWS - from data lake to warehouse and analytics stack.\n\nDrive internal data tooling improvements and support analytics for business decision-making.\n\n\nKey Requirements\n\n4-5 years of experience as a Data Engineer or in a similar backend/data-intensive role.\n\nStrong programming experience with Python and familiarity with ML model development workflows .\n\nHands-on experience with GraphQL and Vector DBs (Pinecone, Chroma).\n\nProficiency in PostgreSQL and working knowledge of NoSQL and search systems like Elasticsearch .\n\nSolid understanding of data engineering on AWS (EC2, S3, RDS, Redshift, etc.).\n\nExperience with Apache Kafka , Apache Spark , and Airflow is essential.\n\nFamiliarity with modern ML Ops and data science integration is a plus.\n\nStrong analytical thinking and problem-solving skills.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'Backend', 'data science', 'Analytical', 'Social media', 'Workflow', 'Data quality', 'AWS', 'Analytics', 'Monitoring']",2025-06-13 05:13:38
Data Engineer,Trantor,5 - 10 years,Not Disclosed,[],"We are looking for a skilled and motivated Data Engineer with deep expertise in GCP,\nBigQuery, Apache Airflow to join our data platform team. The ideal candidate should have hands-on experience building scalable data pipelines, automating workflows, migrating large-scale datasets, and optimizing distributed systems. The candidate should have experience with building Web APIs using Python. This role will play a key part in designing and maintaining robust data engineering solutions across cloud and on-prem environments.\nKey Responsibilities\nBigQuery & Cloud Data Pipelines:\nDesign and implement scalable ETL pipelines for ingesting large-scale datasets.\nBuild solutions for efficient querying of tables in BigQuery.\nAutomated scheduled data ingestion using Google Cloud services and scheduled\nApache Airflow DAGs",,,,"['Airflow', 'Etl Pipelines', 'GCP', 'Bigquery', 'Python', 'SFTP', 'ETL', 'SQL']",2025-06-13 05:13:40
Data Engineer,Society Managers,3 - 5 years,Not Disclosed,['Mumbai (All Areas)'],"We are seeking a skilled and driven SDE-II (Data Engineering) to join our dynamic team. In this role, you will design, develop, and maintain scalable data pipelines, working with large, complex datasets. Youll collaborate closely with cross-functional teams to gather data requirements and contribute to the architecture of our data systems, leveraging your expertise in tools like Databricks, Spark, and SQL.\n\nRoles and responsibilities\nData Pipeline Development: Design,build, and maintain scalable data pipelines using Databricks, Python,and Spark.\nData Processing & Transformation: Handle large, complex datasets to ensure efficient data processing and transformations.\nCollaboration: Work with cross-functional teams to gather, understand, and implement data requirements.\nSQL & ETL: Write and optimize SQL queries for data extraction, transformation, and loading (ETL) processes.\nData Quality & Security: Ensure data accuracy, integrity, and security across all stages of the data lifecycle.\nSystem Design & Architecture: Contribute to the design and architecture of scalable data systems and solutions.\nRequired Skills and Qualification\nExperience: 3+ years of experience in data engineering or a related field.\nDatabricks & Spark: Strong expertise in Databricks and distributed data processing with Spark.\nProgramming: Proficiency in Python for data engineering tasks.\nSQL Optimization: Solid experience in writing and optimizing complex SQL queries.\nData Systems Knowledge: Hands-on experience with large-scale data systems and tools.\nDomain Knowledge: Familiarity with Capital Market/Private Equity is a plus (relaxation may apply).\nData Visualization: Experience with Tableau for creating insightful data visualizations and reports.\nPreferred skills\nCloud Platforms: Familiarity with cloud services like AWS, Azure, or GCP.\nData Warehousing & ETL:Experience with data warehousing concepts and ETL processes.\nAnalytical Skills: Strong problem-solving and analytical capabilities Analytics Tools: Hands-on experience with tools like Amplitude, PostHog, Google Analytics, or Mixpanel.\nAdditional Tools: Knowledge of Python for web scraping and frameworks like Django (good to have).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Microsoft Azure', 'Data Bricks', 'Spark', 'ETL', 'Python', 'SQL']",2025-06-13 05:13:41
"Data Engineer Openings at Advantum Health, Hyderabad",Advantum Health,3 - 5 years,Not Disclosed,['Hyderabad'],"Data Engineer openings at Advantum Health Pvt Ltd, Hyderabad.\nOverview:\nWe are looking for a Data Engineer to build and optimize robust data pipelines that support AI and RCM analytics. This role involves integrating structured and unstructured data from diverse healthcare systems into scalable, AI-ready datasets.\nKey Responsibilities:\nDesign, implement, and optimize data pipelines for ingesting and transforming healthcare and RCM data.\nBuild data marts and warehouses to support analytics and machine learning.\nEnsure data quality, lineage, and governance across AI use cases.\nIntegrate data from EMRs, billing platforms, claims databases, and third-party APIs.\nSupport data infrastructure in a HIPAA-compliant cloud environment.\nQualifications:\nBachelors in Computer Science, Data Engineering, or related field.\n3+ years of experience with ETL/ELT pipelines using tools like Apache Airflow, dbt, or Azure Data Factory.\nStrong SQL and Python skills.\nExperience with healthcare data standards (HL7, FHIR, X12) preferred.\nFamiliarity with data lake house architectures and AI integration best practices\nPh: 9177078628\nEmail id: jobs@advantumhealth.com\nAddress: Advantum Health Private Limited, Cyber gateway, Block C, 4th floor Hitech City, Hyderabad.\nDo follow us on LinkedIn, Facebook, Instagram, YouTube and Threads\nAdvantum Health LinkedIn Page:\nhttps://lnkd.in/gVcQAXK3\n\nAdvantum Health Facebook Page:\nhttps://lnkd.in/g7ARQ378\n\nAdvantum Health Instagram Page:\nhttps://lnkd.in/gtQnB_Gc\n\nAdvantum Health India YouTube link:\nhttps://lnkd.in/g_AxPaPp\n\nAdvantum Health Threads link:\nhttps://lnkd.in/gyq73iQ6",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'SQL', 'Python', 'Airflow', 'ETL', 'Elt']",2025-06-13 05:13:43
Data Engineer,Cloud Angles Digital Transformation,3 - 5 years,Not Disclosed,['Noida'],"Essential Functions/Responsibilities/Duties\n•       Work closely with Senior Business Intelligence engineer and BI architect to understand the schema objects and build BI reports and Dashboards\n•       Participation in sprint refinement, planning, and kick-off to understand the Agile process and Sprint priorities\n•       Develop necessary transformations and aggregate tables required for the reporting\\Dashboard needs\n•       Understand the Schema layer in MicroStrategy and business requirements\n•       Develop complex reports and Dashboards in MicroStrategy\n•       Investigate and troubleshoot issues with Dashboard and reports\n•       Proactively researching new technologies and proposing improvements to processes and tech stack\n•       Create test cases and scenarios to validate the dashboards and maintain data accuracy\nEducation and Experience\n•       3 years of experience in Business Intelligence and Data warehousing\n•       3+ years of experience in MicroStrategy Reports and Dashboard development\n•       2 years of experience in SQL\n•       Bachelors or masters degree in IT or Computer Science or ECE.\n•       Nice to have – Any MicroStrategy certifications\nRequired Knowledge, Skills, and Abilities\n•       Good in writing complex SQL, including aggregate functions, subqueries and complex date calculations and able to teach these concepts to others.\n•       Detail oriented and able to examine data and code for quality and accuracy.\n•       Self-Starter – taking initiative when inefficiencies or opportunities are seen.\n•       Good understanding of modern relational and non-relational models and differences between them\n•       Good understanding of Datawarehouse concepts, snowflake & star schema architecture and SCD concepts\n•       Good understanding of MicroStrategy Schema objects\n•       Develop Public objects such as metrics, filters, prompts, derived objects, custom groups and consolidations in MicroStrategy\n•       Develop complex reports and dashboards using OLAP and MTDI cubes\n•       Create complex dashboards with data blending\n•       Understand VLDB settings and report optimization\n•       Understand security filters and connection mappings in MSTR\nWork Environment\nAt Personify Health, we value and celebrate diversity and are committed to creating an inclusive environment for all employees. We believe in creating teams made up of individuals with various backgrounds, experiences, and perspectives. Diversity inspires innovation and collaboration and challenges us to produce better solutions. But more than this, diversity is our strength and a catalyst in our ability to change lives for the good. \nPhysical Requirements\n•       Constantly operates a computer and other office productivity machinery, such as copy machine, computer printer, calculator, etc.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Microstrategy', 'SQL', 'Dashboards']",2025-06-13 05:13:44
Data Engineer,Reputed Client,5 - 10 years,18-25 Lacs P.A.,[],"Data Engineer\n(Python, PySpark, SQL and Spark SQL)\n\nExperience - 5-10 Years\nMandate Skills: Python, PySpark, SQL and SparkSQL\nWorking Hours: 11:00 am to 8 pm\n\n(Candidate has to be flexible. 4-hour overlap with US business hours)\n\nSalary : 1.50 LPM to 2 LPM + Tax (Rate is not fixed, negotiable depending upon the candidate feedback)\n\nRemote / Hybrid (3 Days in a week WFO) (Pune, Bangalore, Noida, Mumbai, Hyderabad)\n\nNOTE: Need candidates within these cities, they have to collect assets from the office / need to be available for meetings - if they are working remotely)\n\nIt's a 6 months (C2H role).",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Python', 'PySpark', 'Spark', 'SQL']",2025-06-13 05:13:46
Data Engineer,Conversehr Business Solutions,4 - 7 years,15-30 Lacs P.A.,['Hyderabad'],"What are the ongoing responsibilities of Data Engineer responsible for?\nWe are building a growing Data and AI team. You will play a critical role in the efforts to centralize structured and unstructured data for the firm. We seek a candidate with skills in data modeling, data management and data governance, and can contribute first-hand towards firms data strategy. The ideal candidate is a self-starter with a strong technical foundation, a collaborative mindset, and the ability to navigate complex data challenges #ASSOCIATE\nWhat ideal qualifications, skills & experience would help someone to be successful?\nBachelors degree in computer science or computer applications; or equivalent experience in lieu of degree with 3 years of industry experience.\nStrong expertise in data modeling and data management concepts. Experience in implementing master data management is preferred.\nSound knowledge on Snowflake and data warehousing techniques.\nExperience in building, optimizing, and maintaining data pipelines and data management frameworks to support business needs.\nProficiency in at least one programming language, preferably python.\nCollaborate with cross-functional teams to translate business needs into scalable data and AI-driven solutions.\nTake ownership of projects from ideation to production, operating in a startup-like culture within an enterprise environment. Excellent communication, collaboration, and ownership mindset.\nFoundational Knowledge of API development and integration.\nKnowledge of Tableau, Alteryx is good-to-have.\nWork Shift Timings - 2:00 PM - 11:00 PM IST",Industry Type: Financial Services (Asset Management),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Master Data Management', 'Python', 'Etl Pipelines', 'Alteryx', 'ai', 'Data Modeling', 'Tableau', 'ETL']",2025-06-13 05:13:48
Cloud Data Engineer,PwC India,3 - 8 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Job Description:\n\nWe are seeking skilled and dynamic Cloud Data Engineers specializing in AWS, Azure, Databricks. The ideal candidate will have a strong background in data engineering, with a focus on data ingestion, transformation, and warehousing. They should also possess excellent knowledge of PySpark or Spark, and a proven ability to optimize performance in Spark job executions.\n\nKey Responsibilities:",,,,"['AWS OR Azure', 'Azure Data Engineer OR AWS Data Engineer', 'Azure', 'AWS']",2025-06-13 05:13:49
AWS Data Engineer,Infosys,5 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","AWS Data Engineer\n\nTo Apply, use the below link:\nhttps://career.infosys.com/jobdesc?jobReferenceCode=INFSYS-EXTERNAL-210775&rc=0\n\nJOB Profile:\nSignificant 5 to 9 years of experience in designing and implementing scalable data engineering solutions on AWS.\nStrong proficiency in Python programming language.\nExpertise in serverless architecture and AWS services such as Lambda, Glue, Redshift, Kinesis, SNS, SQS, and CloudFormation.\nExperience with Infrastructure as Code (IaC) using AWS CDK for defining and provisioning AWS resources.\nProven leadership skills with the ability to mentor and guide junior team members.\nExcellent understanding of data modeling concepts and experience with tools like ERStudio.\nStrong communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.\nExperience with Apache Airflow for orchestrating data pipelines is a plus.\nKnowledge of Data Lakehouse, dbt, or Apache Hudi data format is a plus.\n\n\nRoles and Responsibilities\nDesign, develop, test, deploy, and maintain large-scale data pipelines using AWS services such as S3, Glue, Lambda, Redshift.\nCollaborate with cross-functional teams to gather requirements and design solutions that meet business needs.\nDesired Candidate Profile\n5-9 years of experience in an IT industry setting with expertise in Python programming language (Pyspark).\nStrong understanding of AWS ecosystem including S3, Glue, Lambda, Redshift.\nBachelor's degree in Any Specialization (B.Tech/B.E.).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Aws Glue', 'AWS Data Engineer', 'Pyspark', 'Aws Lambda', 'Redshift Aws', 'Python']",2025-06-13 05:13:51
Data Engineer II,Flipkart,1 - 3 years,Not Disclosed,['Bengaluru'],"Skills Required :\nKafka, Spark Streaming. Proficiency in one of the programming languages preferably Java, Scala or Python.\nEducation/Qualification :\nBachelor's Degree in Computer Science, Engineering, Technology or related field\nDesirable Skills :\nKafka, Spark Streaming. Proficiency in one of the programming languages preferably Java, Scala or Python.",Industry Type: Courier / Logistics,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'java', 'Scala', 'Kafka', 'Spark Streaming', 'Python']",2025-06-13 05:13:53
S&C Global Network - AI - CG&S - Data Engineer Consultant,Accenture,3 - 7 years,Not Disclosed,['Bengaluru'],"Job Title:Industry & Function AI Data Engineer + S&C GN\n\n\n\nManagement Level:09 - Consultant\n\n\n\nLocation:Primary - Bengaluru, Secondary - Gurugram\n\n\n\nMust-Have Skills:Data Engineering expertise, Cloud platforms:AWS, Azure, GCP, Proficiency in Python, SQL, PySpark and ETL frameworks\n\n\n\nGood-to-Have Skills:LLM Architecture, Containerization tools:Docker, Kubernetes, Real-time data processing tools:Kafka, Flink, Certifications like AWS Certified Data Analytics Specialty, Google Professional Data Engineer,Snowflake,DBT,etc.\n\n\n\nJob\n\n\nSummary:\n\nAs a Data Engineer, you will play a critical role in designing, implementing, and optimizing data infrastructure to power analytics, machine learning, and enterprise decision-making. Your work will ensure high-quality, reliable data is accessible for actionable insights. This involves leveraging technical expertise, collaborating with stakeholders, and staying updated with the latest tools and technologies to deliver scalable and efficient data solutions.\n\n\n\n\nRoles & Responsibilities:\nBuild and Maintain Data Infrastructure:Design, implement, and optimize scalable data pipelines and systems for seamless ingestion, transformation, and storage of data.\nCollaborate with Stakeholders:Work closely with business teams, data analysts, and data scientists to understand data requirements and deliver actionable solutions.\nLeverage Tools and Technologies:Utilize Python, SQL, PySpark, and ETL frameworks to manage large datasets efficiently.\nCloud Integration:Develop secure, scalable, and cost-efficient solutions using cloud platforms such as Azure, AWS, and GCP.\nEnsure Data Quality:Focus on data reliability, consistency, and quality using automation and monitoring techniques.\nDocument and Share Best Practices:Create detailed documentation, share best practices, and mentor team members to promote a strong data culture.\nContinuous Learning:Stay updated with the latest tools and technologies in data engineering through professional development opportunities.\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\nStrong proficiency in programming languages such as Python, SQL, and PySpark\nExperience with cloud platforms (AWS, Azure, GCP) and their data services\nFamiliarity with ETL frameworks and data pipeline design\nStrong knowledge of traditional statistical methods, basic machine learning techniques.\nKnowledge of containerization tools (Docker, Kubernetes)\nKnowing LLM, RAG & Agentic AI architecture\nCertification in Data Science or related fields (e.g., AWS Certified Data Analytics Specialty, Google Professional Data Engineer)\n\n\n\n\n\nAdditional Information:\n\nThe ideal candidate has a robust educational background in data engineering or a related field and a proven track record of building scalable, high-quality data solutions in the Consumer Goods sector.\n\nThis position offers opportunities to design and implement cutting-edge data systems that drive business transformation, collaborate with global teams to solve complex data challenges and deliver measurable business outcomes and enhance your expertise by working on innovative projects utilizing the latest technologies in cloud, data engineering, and AI.\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience:Minimum 3-7 years in data engineering or related fields, with a focus on the Consumer Goods Industry\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Computer Science, Information Systems, Engineering, or a related field",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'pyspark', 'data engineering', 'sql', 'machine learning algorithms', 'kubernetes', 'snowflake', 'data analytics', 'microsoft azure', 'cloud platforms', 'machine learning', 'apache flink', 'artificial intelligence', 'docker', 'pipeline', 'data science', 'gcp', 'kafka', 'aws', 'etl', 'etl scripts']",2025-06-13 05:13:55
Data Engineer--Operations,Robert Bosch Engineering and Business Solutions Private Limited,2 - 6 years,Not Disclosed,['Bengaluru'],"As a Data engineer in Operations, you will work on the operational management, monitoring, and support of scalable data pipelines running in Azure Databricks, Hadoop and Radium. You will ensure the reliability, performance, and availability of data workflows and maintain production environments. You will collaborate closely with data engineers, architects, and platform teams to implement best practices in data pipeline operations and incident management to ensure data availability and data completeness.\nPrimary responsibilities:\nOperational support and incident management for Azure Databricks, Hadoop, Radium data pipelines.\nCollaborating with data engineering and platform teams to define and enforce operational standards, SLAs, and best practices.\nDesigning and implementing monitoring, alerting, and logging solutions for Azure Databricks pipelines.\nCoordinating with central teams to ensure compliance with organizational operational standards and security policies.\nDeveloping and maintaining runbooks, SOPs, and troubleshooting guides for pipeline issues.\nManaging the end-to-end lifecycle of data pipeline incidents, including root cause analysis and remediation.\nOverseeing pipeline deployments, rollbacks, and change management using CI/CD tools such as Azure DevOps.\nEnsuring data quality and validation checks are effectively monitored in production.\nWorking closely with platform and infrastructure teams to address pipeline and environment-related issues.\nProviding technical feedback and mentoring junior operations engineers.\nConducting peer reviews of operational scripts and automation code.\nAutomating manual operational tasks using Scala and Python scripts.\nManaging escalations and coordinating critical production issue resolution.\nParticipating in post-mortem reviews and continuous improvement initiatives for data pipeline operations.",Industry Type: Automobile,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Change management', 'SCALA', 'Incident management', 'Data quality', 'Troubleshooting', 'Operations', 'Monitoring', 'Python']",2025-06-13 05:13:57
Cloud Data Engineer,Wipro,8 - 12 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']",Role: Cloud Data Engineer\nLocation: Wipro PAN India\nHybrid 3 days in Wipro office\n\nJD:\nStrong - SQL\nStrong - Python,,,,"['SQL', 'Python', 'AZURE', 'GCP', 'AWS']",2025-06-13 05:13:58
Azure Data Engineer,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of the role is to support process delivery by ensuring daily performance of the Production Specialists, resolve technical escalations and develop technical capability within the Production Specialists.\n\nDo\nOversee and support process by reviewing daily transactions on performance parameters\n\n\n\nMandatory Skills: Azure Data Factory. Experience: 5-8 Years.",,,,"['Azure', 'azure databricks', 'azure data lake', 'ssas', 'ssrs', 'microsoft azure', 'azure data factory', 'ssis', 'msbi', 'sql server', 'sql']",2025-06-13 05:14:00
Data Engineer III,Expedia Group,5 - 10 years,Not Disclosed,['Bengaluru'],"Why Join Us?\nTo shape the future of travel, people must come first. Guided by our Values and Leadership Agreements, we foster an open culture where everyone belongs, differences are celebrated and know that when one of us wins, we all win.\nWe provide a full benefits package, including exciting travel perks, generous time-off, parental leave, a flexible work model (with some pretty cool offices), and career development resources, all to fuel our employees passion for travel and ensure a rewarding career journey. We re building a more open world. Join us.\nData Engineer III\nIntroduction to the Team\nExpedia Technology teams partner with our Product teams to create innovative products, services, and tools to deliver high-quality experiences for travelers, partners, and our employees. A singular technology platform powered by data and machine learning provides secure, differentiated, and personalized experiences that drive loyalty and traveler satisfaction.\nExpedia Group is seeking a skilled and motivated Data Engineer III to join our Finance Business Intelligence team supporting the Product & Technology Finance organization. In this role, you will help drive data infrastructure and analytics solutions that support strategic financial planning, reporting, and operational decision-making across the Global Finance community. You ll work closely with Finance and Technology partners to ensure data accuracy, accessibility, and usability in support of Expedia s business objectives.\nAs a Data Engineer III, you have strong experience working with a variety of datasets, data environments, tools, and analytical techniques. You enjoy a fun, collaborative and stimulating team environment. Successful candidates should be able to own projects end-to-end, including identifying problems and solutions, building and maintain data pipelines and dashboards, distilling key insights and communicate to stakeholders.\nIn this role, you will:\nDevelop new and improve existing end to end Business Intelligence products (data pipelines, Tableau dashboards, and Machine Learning predictive forecasting models).\nDrive internal efficiencies through streamline code/documentation/Tableau development to maintain high data integrity.\nTroubleshoot and resolve production issues with the team products (automation opportunities, optimizations, back-end data issues, data reconciliations).\nProactively reach out to subject matter experts /stakeholders and collaborate to solve problems.\nRespond to ad hoc data requests and conduct analysis to provide valuable insights to stakeholders.\nCollaborate and coordinate with team members/stakeholders to translate complex data into meaningful insights, that improve the analytical capabilities of the business.\nApply knowledge of database design to support migration of data pipelines from on prem to cloud environment (including data extraction, ingestion, processing of large data sets)\nSupport dashboard development on cloud environment to enable self-service reporting.\nCommunicate clearly on current work status and design considerations\nThink broadly and comprehend the how, why, and what behind data architecture designs\nExperience & Qualifications:\nBachelor s in Computer Science, Mathematics, Statistics, Information Systems, or related field\n5+ years experience in a Data Analyst, Data Engineer or Business Analyst role\nProven expertise in SQL, with practical experience utilizing query engines including SQL Server, Starburst, Trino, Querybook and data science tools such as Python/R, SparkSQL.\nProficient visualization skills (Tableau, Looker, or similar) and excel modeling/report automation.\nExceptional understanding of relational and dimensional datasets, data warehouse and data mining and applies database design principles to solve data requirements\nExperience building robust data extract, load and transform (ELT) processes, that source data from multiple databases.\nDemonstrated record of defining and executing key analysis and solving problems with minimal supervision.\nDynamic individual contributor who consistently enhances operational playbooks to address business problems.\n3+ year working in a hybrid environment that uses both on-premise and cloud technologies is preferred.\nExperience working in an environment that manipulates large datasets on the cloud platform preferred.\nBackground in analytics, finance or a comparable reporting and analytics role preferred.\nAccommodation requests\nIf you need assistance with any part of the application or recruiting process due to a disability, or other physical or mental health conditions, please reach out to our Recruiting Accommodations Team through the Accommodation Request .\nWe are proud to be named as a Best Place to Work on Glassdoor in 2024 and be recognized for award-winning culture by organizations like Forbes, TIME, Disability:IN, and others.\nExpedia Groups family of brands includes: Brand Expedia , Hotels.com , Expedia Partner Solutions, Vrbo , trivago , Orbitz , Travelocity , Hotwire , Wotif , ebookers , CheapTickets , Expedia Group Media Solutions, Expedia Local Expert , CarRentals.com , and Expedia Cruises . 2024 Expedia, Inc. All rights reserved. Trademarks and logos are the property of their respective owners. . Never provide sensitive, personal information to someone unless you re confident who the recipient is. Expedia Group does not extend job offers via email or any other messaging tools to individuals with whom we have not made prior contact. Our email domain is @expediagroup.com. The official website to find and apply for job openings at Expedia Group is careers.expediagroup.com/jobs .\nExpedia is committed to creating an inclusive work environment with a diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, gender, sexual orientation, national origin, disability or age.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Database design', 'Machine learning', 'Business intelligence', 'Data mining', 'Analytics', 'SQL', 'Python', 'Data architecture']",2025-06-13 05:14:02
Azure Data Engineer,Infosys,5 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Job description\nWe are looking for Azure Data Engineer's resources having minimum 5 to 9 years of Experience.\n\nRole & responsibilities\nBlend of technical expertise with 5 to 9 year of experience, analytical problem-solving, and collaboration with cross-functional teams. Design and implement Azure data engineering solutions (Ingestion & Curation)\nCreate and maintain Azure data solutions including Azure SQL Database, Azure Data Lake, and Azure Blob Storage.\nDesign, implement, and maintain data pipelines for data ingestion, processing, and transformation in Azure.\nUtilizing Azure Data Factory or comparable technologies, create and maintain ETL (Extract, Transform, Load) operations\nUse Azure Data Factory and Databricks to assemble large, complex data sets\nImplementing data validation and cleansing procedures will ensure the quality, integrity, and dependability of the data.\nEnsure data quality / security and compliance.\nOptimize Azure SQL databases for efficient query performance.\nCollaborate with data engineers, and other stakeholders to understand requirements and translate them into scalable and reliable data platform architectures.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Azure Data Factory', 'Azure Synapse', 'Pyspark', 'Azure Data Lake']",2025-06-13 05:14:04
AWS Data Engineer,Infosys,5 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Significant 5 to 9 years of experience in designing and implementing scalable data engineering solutions on AWS.\nStrong proficiency in Python programming language.\nExpertise in serverless architecture and AWS services such as Lambda, Glue, Redshift, Kinesis, SNS, SQS, and CloudFormation.\nExperience with Infrastructure as Code (IaC) using AWS CDK for defining and provisioning AWS resources.\nProven leadership skills with the ability to mentor and guide junior team members.\nExcellent understanding of data modeling concepts and experience with tools like ERStudio.\nStrong communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.\nExperience with Apache Airflow for orchestrating data pipelines is a plus.\nKnowledge of Data Lakehouse, dbt, or Apache Hudi data format is a plus.\nRoles and Responsibilities\nDesign, develop, test, deploy, and maintain large-scale data pipelines using AWS services such as S3, Glue, Lambda, Redshift.\nCollaborate with cross-functional teams to gather requirements and design solutions that meet business needs.\nDesired Candidate Profile\n5-9 years of experience in an IT industry setting with expertise in Python programming language (Pyspark).\nStrong understanding of AWS ecosystem including S3, Glue, Lambda, Redshift.\nBachelor's degree in Any Specialization (B.Tech/B.E.).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Aws Glue', 'Pyspark', 'Aws Lambda', 'Amazon Redshift', 'Athena', 'Python']",2025-06-13 05:14:06
Azure Data Engineer,Hexaware Technologies,6 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","Hi\n\nWork Location : Chennai AND Bangalore\nWork location : Imm - 30 days\n\nPrimary: Azure Databricks,ADF, Pyspark SQL",,,,"['Pyspark', 'Azure Databricks', 'SQL', 'Azure Data Factory']",2025-06-13 05:14:07
Cloud Data Engineer,PwC India,5 - 8 years,10-20 Lacs P.A.,['Bengaluru'],"Role & responsibilities\nStrong hands-on experience with multi cloud (AWS, Azure, GCP)  services such as GCP BigQuery, Dataform AWS Redshift, \nProficient in PySpark and SQL for building scalable data processing pipelines\nKnowledge of utilizing serverless technologies like AWS Lambda, and Google Cloud Functions \nExperience in orchestration frameworks like Apache Airflow, Kubernetes, and Jenkins to manage and orchestrate data pipelines",,,,"['Pyspark', 'Python', 'SQL', 'Datafactory', 'GCP', 'Microsoft Azure', 'AWS', 'Data Bricks']",2025-06-13 05:14:09
Lead Big Data Engineer - Python & Spark,Hubnex,7 - 12 years,Not Disclosed,['Bengaluru'],"Job Title: Lead Big Data Engineer - Python & Spark\nLocation: Gurgaon, India\nExperience: 7+ Years\nEmployment Type: Full-Time | Onsite\nDepartment: Data Engineering\nAbout Hubnex Labs:\nHubnex Labs is a forward-looking IT consulting and software services company, building next-generation data platforms, AI systems, and enterprise-grade applications. We re looking for a Lead Big Data Engineer to drive the development and deployment of high-performance data processing solutions.\nRole Overview:\nAs a Lead Big Data Engineer , you will be responsible for architecting and implementing scalable big data solutions using Spark, Python, Hive, and related technologies. You will mentor a team of developers and work closely with cross-functional stakeholders to ensure on-time, error-free software delivery.\nKey Responsibilities:\nLead and mentor a team of Python and big data developers to deliver robust data-driven applications\nDesign, develop, and maintain scalable data processing pipelines using Spark (Scala/PySpark), Hive, and Hadoop ecosystems\nWrite efficient, reusable, and well-documented code in Python, SQL, and shell scripting\nOptimize Spark applications for performance and scalability; tune existing Hadoop-based systems\nCollaborate with QA, DevOps, and business teams to ensure high-quality software delivery\nPerform code reviews , enforce coding standards, and contribute to overall architectural decisions\nActively participate in daily Scrum meetings , sprint planning, retrospectives, and release cycles\nTroubleshoot complex issues across the full data pipeline\nRequired Qualifications:\n7+ years of hands-on experience in software development, with a strong background in big data frameworks\nDeep expertise in Hadoop ecosystem including HDFS, Hive, HQL, Spark (Scala/PySpark), Sqoop\n4+ years of programming experience using Python , SQL , and Unix shell scripting\nProven experience in leading development teams and delivering enterprise-level solutions\nStrong grasp of Spark architecture , performance tuning, and data frame APIs\nExcellent understanding of database concepts , data structures , and distributed systems\nBachelors degree in Computer Science , Engineering , or a related field\nExceptional communication, problem-solving, and leadership skills\nPreferred Skills:\nExperience with CI/CD pipelines for data projects\nFamiliarity with cloud-based data platforms (AWS EMR, GCP DataProc, or Azure HDInsight)\nWorking knowledge of Kafka , Airflow , or other data orchestration tools\nWhy Join Hubnex Labs?\nWork on mission-critical big data solutions that impact businesses globally\nBe part of an innovative, agile, and supportive team\nLeadership opportunities and exposure to latest technologies in AI and cloud computing\nCompetitive salary, performance incentives, and professional growth support",Industry Type: Internet,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Performance tuning', 'Cloud computing', 'Coding', 'Agile', 'Data structures', 'Scrum', 'Unix shell scripting', 'SQL', 'Python']",2025-06-13 05:14:11
Data Engineer-Data Integration,IBM,2 - 5 years,Not Disclosed,['Pune'],"As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\n\n\n Your primary responsibilities include: \nDevelop & maintain data pipelines for batch & stream processing using informatica power centre or cloud ETL/ELT tools.\nLiaise with business team and technical leads, gather requirements, identify data sources, identify data quality issues, design target data structures, develop pipelines and data processing routines, perform unit testing and support UAT.\nWork with data scientist and business analytics team to assist in data ingestion and data-related technical issues.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExpertise in Data warehousing/ information Management/ Data Integration/Business Intelligence using ETL tool Informatica PowerCenter\nKnowledge of Cloud, Power BI, Data migration on cloud skills.\nExperience in Unix shell scripting and python\nExperience with relational SQL, Big Data etc\n\n\nPreferred technical and professional experience\nKnowledge of MS-Azure Cloud\nExperience in Informatica PowerCenter\nExperience in Unix shell scripting and python",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['information management', 'data warehousing', 'business intelligence', 'etl', 'data integration', 'python', 'informatica powercenter', 'power bi', 'relational sql', 'data migration', 'azure cloud', 'sql', 'elastic search', 'unix shell scripting', 'splunk', 'agile', 'big data', 'informatica']",2025-06-13 05:14:13
Data Engineer-Data Integration,IBM,2 - 5 years,Not Disclosed,['Pune'],"As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExpertise in Data warehousing/ information Management/ Data Integration/Business Intelligence using ETL tool Informatica PowerCenter\nKnowledge of Cloud, Power BI, Data migration on cloud skills.\nExperience in Unix shell scripting and python\nExperience with relational SQL, Big Data etc\n\n\nPreferred technical and professional experience\nKnowledge of MS-Azure Cloud\nExperience in Informatica PowerCenter\nExperience in Unix shell scripting and python",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['information management', 'data warehousing', 'business intelligence', 'etl', 'data integration', 'python', 'data management', 'informatica powercenter', 'power bi', 'relational sql', 'data migration', 'azure cloud', 'sql', 'unix shell scripting', 'java', 'etl tool', 'big data', 'informatica', 'unix']",2025-06-13 05:14:14
Senior Data Engineer,Grid Dynamics,8 - 13 years,15-25 Lacs P.A.,['Bengaluru'],"We are looking for an enthusiastic and technology-proficient Big Data Engineer, who is eager to participate in the design and implementation of a top-notch Big Data solution to be deployed at massive scale.\nOur customer is one of the world's largest technology companies based in Silicon Valley with operations all over the world. On this project we are working on the bleeding-edge of Big Data technology to develop high performance data analytics platform, which handles petabytes datasets.\nEssential functions\nParticipate in design and development of Big Data analytical applications.\nDesign, support and continuously enhance the project code base, continuous integration pipeline, etc.\nWrite complex ETL processes and frameworks for analytics and data management.\nImplement large-scale near real-time streaming data processing pipelines.\nWork inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale.\nQualifications\nStrong coding experience with Scala, Spark,Hive, Hadoop.\nIn-depth knowledge of Hadoop and Spark, experience with data mining and stream processing technologies (Kafka, Spark Streaming, Akka Streams).\nUnderstanding of the best practices in data quality and quality engineering.\nExperience with version control systems, Git in particular.\nDesire and ability for quick learning of new tools and technologies.\nWould be a plus\nKnowledge of Unix-based operating systems (bash/ssh/ps/grep etc.).\nExperience with Github-based development processes.\nExperience with JVM build systems (SBT, Maven, Gradle).\nWe offer\nOpportunity to work on bleeding-edge projects\nWork with a highly motivated and dedicated team\nCompetitive salary\nFlexible schedule\nBenefits package - medical insurance, sports\nCorporate social events\nProfessional development opportunities\nWell-equipped office\nAbout us\nGrid Dynamics (NASDAQ: GDYN) is a leading provider of technology consulting, platform and product engineering, AI, and advanced analytics services. Fusing technical vision with business acumen, we solve the most pressing technical challenges and enable positive business outcomes for enterprise companies undergoing business transformation. A key differentiator for Grid Dynamics is our 8 years of experience and leadership in enterprise AI, supported by profound expertise and ongoing investment in data, analytics, cloud & DevOps, application modernization and customer experience. Founded in 2006, Grid Dynamics is headquartered in Silicon Valley with offices across the Americas, Europe, and India.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hive', 'SCALA', 'Hadoop', 'Big Data', 'Spark']",2025-06-13 05:14:16
Data Engineer,Meritus Management Service,4 - 9 years,9-18 Lacs P.A.,"['Pune', 'Gurugram']","The first Data Engineer specializes in traditional ETL with SAS DI and Big Data (Hadoop, Hive). The second is more versatile, skilled in modern data engineering with Python, MongoDB, and real-time processing.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Etl Pipelines', 'Big Data', 'Informatica', 'SAS DI', 'SQL', 'Hive', 'Hadoop', 'Talend', 'ETL Tool', 'Python']",2025-06-13 05:14:18
Data Engineer - ETL/ Python,Meritus Management Service,5 - 7 years,10-14 Lacs P.A.,['Indore'],"Focus on Python, you'll play a crucial role in designing, developing, and maintaining data pipelines and ETL processes. Python to manage large datasets, automate data workflows, and ensure data accuracy and efficiency across our organization.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pandas', 'MySQL', 'Sqlalchemy', 'Numpy', 'Python', 'Azure Synapse', 'Postgresql', 'Etl Process', 'SQL']",2025-06-13 05:14:20
Big Data Engineer,Apexon,11 - 16 years,Not Disclosed,['Bengaluru'],"We enable #HumanFirstDigital\n\nJob Summary:\nWe are looking for a highly experienced and strategic Data Engineer to drive the design, development, and optimization of our enterprise data platform. This role requires deep technical expertise in AWS, StreamSets, and Snowflake, along with solid experience in Kubernetes, Apache Airflow, and unit testing. The ideal candidate will lead a team of data engineers and play a key role in delivering scalable, secure, and high-performance data solutions for both historical and incremental data loads.\nKey Responsibilities:\nLead the architecture, design, and implementation of end-to-end data pipelines using StreamSets and Snowflake.\nOversee the development of scalable ETL/ELT processes for historical data migration and incremental data ingestion.\nGuide the team in leveraging AWS services (S3, Lambda, Glue, IAM, etc.) to build cloud-native data solutions.\nProvide technical leadership in deploying and managing containerized applications using Kubernetes.\nDefine and implement workflow orchestration strategies using Apache Airflow.\nEstablish best practices for unit testing, code quality, and data validation.\nCollaborate with data architects, analysts, and business stakeholders to align data solutions with business goals.\nMentor junior engineers and foster a culture of continuous improvement and innovation.\nMonitor and optimize data workflows for performance, scalability, and cost-efficiency.\nRequired Skills & Qualifications:\nHigh proficiency in AWS, including hands-on experience with core services (S3, Lambda, Glue, IAM, CloudWatch).\nExpert-level experience with StreamSets, including Data Collector, Transformer, and Control Hub.\nStrong Snowflake expertise, including data modeling, SnowSQL, and performance tuning.\nMedium-level experience with Kubernetes, including container orchestration and deployment.\nWorking knowledge of Apache Airflow for workflow scheduling and monitoring.\nExperience with unit testing frameworks and practices in data engineering.\nProven experience in building and managing ETL pipelines for both batch and real-time data.\nStrong command of SQL and scripting languages such as Python or Shell.\nExperience with CI/CD pipelines and version control tools (e.g., Git, Jenkins).\nPreferred Qualifications:\nAWS certification (e.g., AWS Certified Data Analytics, Solutions Architect).\nExperience with data governance, security, and compliance frameworks.\nFamiliarity with Agile methodologies and tools like Jira and Confluence.\nPrior experience in a leadership or mentoring role within a data engineering team.\nOur Commitment to Diversity & Inclusion:\nOur Perks and Benefits:\nOur benefits and rewards program has been thoughtfully designed to recognize your skills and contributions, elevate your learning/upskilling experience and provide care and support for you and your loved ones. As an Apexon Associate, you get continuous skill-based development, opportunities for career advancement, and access to comprehensive health and well-being benefits and assistance.\nWe also offer:\no Group Health Insurance covering family of 4\no Term Insurance and Accident Insurance\no Paid Holidays & Earned Leaves\no Paid Parental LeaveoLearning & Career Development\no Employee Wellness\nJob Location : Bengaluru, India",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Data modeling', 'Agile', 'Wellness', 'Workflow', 'Healthcare', 'Unit testing', 'Apache', 'SQL', 'Python']",2025-06-13 05:14:22
Big Data Engineer,Rarr Technologies,6 - 8 years,Not Disclosed,['Bengaluru'],Job description\n\nProven experience working with data pipelines ETL BI regardless of the technology\nProven experience working with AWS including at least 3 of RedShift S3 EMR Cloud Formation DynamoDB RDS lambda\nBig Data technologies and distributed systems one of Spark Presto or Hive\nPython language scripting and object oriented\nFluency in SQL for data warehousing RedShift in particular is a plus\nGood understanding on data warehousing and Data modelling concepts\nFamiliar with GIT Linux CICD pipelines is a plus\nStrong systems process orientation with demonstrated analytical thinking organization skills and problem solving skills\nAbility to self manage prioritize and execute tasks in a demanding environment\nStrong consultancy orientation and experience with the ability to form collaborative\nproductive working relationships across diverse teams and cultures is a must\nWillingness and ability to train and teach others\nAbility to facilitate meetings and follow up with resulting action items,Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['python scripting', 'Big Data Technologies', 'ETL', 'AWS']",2025-06-13 05:14:24
Big Data Engineer,Client of Hiresquad Resources,5 - 8 years,22.5-30 Lacs P.A.,"['Noida', 'Hyderabad', 'Bengaluru']","Role: Data Engineer\nExp: 5 to 8 Years\nLocation: Bangalore, Noida, and Hyderabad (Hybrid, weekly 2 Days office must)\nNP: Immediate to 15 Days (Try to find only immediate joiners)\n\n\nNote:\nCandidate must have experience in Python, Kafka Streams, Pyspark, and Azure Databricks.\nNot looking for candidates who have only Exp in Pyspark and not in Python.\n\n\nJob Title: SSE Kafka, Python, and Azure Databricks (Healthcare Data Project)\nExperience:  5 to 8 years\n\nRole Overview:\nWe are looking for a highly skilled with expertise in Kafka, Python, and Azure Databricks (preferred) to drive our healthcare data engineering projects. The ideal candidate will have deep experience in real-time data streaming, cloud-based data platforms, and large-scale data processing. This role requires strong technical leadership, problem-solving abilities, and the ability to collaborate with cross-functional teams.\n\nKey Responsibilities:\nLead the design, development, and implementation of real-time data pipelines using Kafka, Python, and Azure Databricks.\nArchitect scalable data streaming and processing solutions to support healthcare data workflows.\nDevelop, optimize, and maintain ETL/ELT pipelines for structured and unstructured healthcare data.\nEnsure data integrity, security, and compliance with healthcare regulations (HIPAA, HITRUST, etc.).\nCollaborate with data engineers, analysts, and business stakeholders to understand requirements and translate them into technical solutions.\nTroubleshoot and optimize Kafka streaming applications, Python scripts, and Databricks workflows.\nMentor junior engineers, conduct code reviews, and ensure best practices in data engineering.\nStay updated with the latest cloud technologies, big data frameworks, and industry trends.\n\n\nRequired Skills & Qualifications:\n4+ years of experience in data engineering, with strong proficiency in Kafka and Python.\nExpertise in Kafka Streams, Kafka Connect, and Schema Registry for real-time data processing.\nExperience with Azure Databricks (or willingness to learn and adopt it quickly).\nHands-on experience with cloud platforms (Azure preferred, AWS or GCP is a plus).\nProficiency in SQL, NoSQL databases, and data modeling for big data processing.\nKnowledge of containerization (Docker, Kubernetes) and CI/CD pipelines for data applications.\nExperience working with healthcare data (EHR, claims, HL7, FHIR, etc.) is a plus.\nStrong analytical skills, problem-solving mindset, and ability to lead complex data projects.\nExcellent communication and stakeholder management skills.\n\n\n\nEmail: Sam@hiresquad.in",Industry Type: Medical Services / Hospital,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Kafka', 'Azure Databricks', 'Python', 'Etl Pipelines', 'Pyspark', 'spark architecture', 'Data Engineering', 'opps concepts', 'Data Streaming', 'Medallion Architecture', 'python scripts', 'schema registry', 'SQL Database', 'Nosql Databases', 'spark tuning', 'Kafka Streams', 'kafka connect']",2025-06-13 05:14:26
"Senior Data Engineer (Snowflake, DBT)",Allegis Global Solutions (AGS),5 - 10 years,Not Disclosed,[],"Senior Data Engineer (Snowflake, DBT, Azure)\nJob Location: Hyderabad / Bangalore / Chennai / Kolkata / Noida/ Gurgaon / Pune / Indore / Mumbai\n\nJob Details\nTechnical Expertise:\nStrong proficiency in Snowflake architecture, including data sharing, partitioning, clustering, and materialized views.\nAdvanced experience with DBT for data transformations and workflow management.\nExpertise in Azure services, including Azure Data Factory, Azure Data Lake, Azure Synapse, and Azure Functions.\nData Engineering:\nProficiency in SQL, Python, or other relevant programming languages.\nStrong understanding of data modeling concepts, including star schema and normalization.\nHands-on experience with ETL/ELT pipelines and data integration tools.\n\nPreferred Qualifications:\nCertifications in Snowflake, DBT, or Azure Data Engineering.\nFamiliar with data visualization tools like Power BI or Tableau.\nKnowledge of CI/CD pipelines and DevOps practices for data workflows.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Data Build Tool', 'Azure']",2025-06-13 05:14:28
Architect (Data Engineering),Amgen Inc,9 - 12 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\n\nRole Description:\n\nWe are seeking a Data Solutions Architect with deep expertise in Biotech/Pharma to design, implement, and optimize scalable and high-performance data solutions that support enterprise analytics, AI-driven insights, and digital transformation initiatives. This role will focus on data strategy, architecture, governance, security, and operational efficiency, ensuring seamless data integration across modern cloud platforms. The ideal candidate will work closely with engineering teams, business stakeholders, and leadership to establish a future-ready data ecosystem, balancing performance, cost-efficiency, security, and usability. This position requires expertise in modern cloud-based data architectures, data engineering best practices, and Scaled Agile methodologies.",,,,"['Data Engineering', 'continuous integration', 'technical leadership', 'metadata management', 'presentation skills', 'ci/cd', 'distributed computing', 'sql', 'data bricks', 'git', 'data modeling', 'spark', 'devops', 'data governance', 'jenkins', 'troubleshooting', 'access control', 'etl']",2025-06-13 05:14:30
Azure Data Engineer,HTC Global Services,4 - 8 years,Not Disclosed,['Bengaluru( Murugeshpalya )'],"Job Summary:\nWe are looking for a highly skilled Azure Data Engineer with experience in building and managing scalable data pipelines using Azure Data Factory, Synapse, and Databricks. The ideal candidate should be proficient in big data tools and Azure services, with strong programming knowledge and a solid understanding of data architecture and cloud platforms.\n\nKey Responsibilities:",,,,"['Power Bi', 'Azure Databricks', 'Azure Data Factory', 'Synapse', 'Python', 'Java', 'Scala', 'Kafka', 'big data tools', 'SQL', 'EventHub', 'Azure cloud services', 'Spark']",2025-06-13 05:14:32
Tech Lead-Data Engineering,Ameriprise Financial,7 - 10 years,Not Disclosed,['Hyderabad'],"Key Responsibilities\nDesign and develop high-volume, data engineering solutions for mission-critical systems with quality.\nMaking enhancements to various applications that meets business and auditing requirements.\nResearch and evaluate alternative solutions and make recommendations on improving the product to meet business and information risk requirements.\nEvaluate service level issues and suggested enhancements to diagnose and address underlying system problems and inefficiencies.\nParticipate in full development lifecycle activities for the product (coding, testing, release activities).\nSupport Release activities on weekends as required.\nSupport any application issues reported during weekends.\nCoordinating day-To-day activities for multiple projects with onshore and offshore team members. Ensuring the availability of platform in lower environments\n\nRequired Qualifications\n7+ years of overall IT experience, which includes hands on experience in Big Data technologies.\nMandatory - Hands on experience in Python and PySpark.\nBuild pySpark applications using Spark Dataframes in Python using Jupyter notebook and PyCharm(IDE).\nWorked on optimizing spark jobs that processes huge volumes of data.\nHands on experience in version control tools like Git.\nWorked on Amazon s Analytics services like Amazon EMR, Amazon Athena, AWS Glue.\nWorked on Amazon s Compute services like Amazon Lambda, Amazon EC2 and Amazon s Storage service like S3 and few other services like SNS.\nExperience/knowledge of bash/shell scripting will be a plus.\nHas built ETL processes to take data, copy it, structurally transform it etc. involving a wide variety of formats like CSV, TSV, XML and JSON.\nExperience in working with fixed width, delimited , multi record file formats etc.\nGood to have knowledge of datawarehousing concepts - dimensions, facts, schemas- snowflake, star etc.\nHave worked with columnar storage formats- Parquet,Avro,ORC etc. Well versed with compression techniques - Snappy, Gzip.\nGood to have knowledge of AWS databases (atleast one) Aurora, RDS, Redshift, ElastiCache, DynamoDB.\nHands on experience in tools like Jenkins to build, test and deploy the applications\nAwareness of Devops concepts and be able to work in an automated release pipeline environment.\nExcellent debugging skills.\n\nPreferred Qualifications\nExperience working with US Clients and Business partners.\nKnowledge on Front end frameworks.\nExposure to BFSI domain is a good to have.\nHands on experience on any API Gateway and management platform.\nAWMPO AWMPS Presidents Office\nTechnology",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Front end', 'Version control', 'Coding', 'XML', 'Shell scripting', 'Debugging', 'JSON', 'Asset management', 'Analytics', 'Python']",2025-06-13 05:14:34
Lead Data Engineer - Azure,Blend360 India,7 - 12 years,Not Disclosed,['Hyderabad'],"Job Description\nAs a Sr Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\nQualifications\n7+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar fiel",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-13 05:14:36
Lead Data Engineer - Azure,Blend360 India,7 - 12 years,Not Disclosed,['Hyderabad'],"Job Description\nAs a Lead Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\nQualifications\n7+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar fiel",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-13 05:14:38
Sr Data Engineer - Lead,Clifyx Technology,7 - 11 years,Not Disclosed,['Bengaluru'],"Developing Scala Spark pipelines that are resilient, modular and tested.\nHelp automate and scale governance through technology enablement\nEnable users finding the ""right data for the ""right use case\nParticipate in identifying and proposing solutions to data quality issues, and data management solutions\nSupport technical implementation of solutions through data pipeline development\nMaintain technical processes and procedures for data management\nVery good understanding of MS Azure Data Lake and associated setups\nETL knowledge to build semantic layers for reporting\nCreation / modification of pipelines based on source and target systems\nUser and access Management and Training end users",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Training', 'Usage', 'Data management', 'Access management', 'spark', 'Billing', 'SCALA', 'Manager Technology', 'Data quality', 'Testing']",2025-06-13 05:14:40
Senior Developer / Lead Data Engineer - Incorta,KPI Partners,4 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","About Us:\nKPI Partners is a leading provider of data analytics and performance management solutions, dedicated to helping organizations harness the power of their data to drive business success. Our team of experts is at the forefront of the data revolution, delivering innovative solutions to our clients. We are currently seeking a talented and experienced Senior Developer / Lead Data Engineer with expertise in Incorta to join our dynamic team.\n\n",,,,"['python', 'oracle', 'data analytics', 'analytical', 'scala', 'pyspark', 'microsoft azure', 'cloud platforms', 'data pipeline', 'relational databases', 'data engineering', 'sql server', 'sql', 'analytics', 'java', 'data modeling', 'collaboration', 'data integration tools', 'mysql', 'etl', 'aws', 'programming', 'communication skills']",2025-06-13 05:14:42
Data Engineer-Data Platforms,IBM,5 - 10 years,Not Disclosed,['Navi Mumbai'],"As a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\n\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.\nGood working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\n\n\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['scala', 'hadoop spark', 'spark', 'big data', 'python', 'hive', 'cloudera', 'pyspark', 'sql', 'java', 'git', 'postgresql', 'devops', 'jenkins', 'data ingestion', 'mysql', 'hadoop', 'etl', 'hbase', 'data analysis', 'dynamo db', 'oozie', 'microsoft azure', 'impala', 'data engineering', 'lambda expressions', 'kafka', 'sqoop', 'aws']",2025-06-13 05:14:44
Data Engineer Specialist,Accenture,3 - 4 years,Not Disclosed,['Kochi'],"Job Title - + +\n\n\n\nManagement Level :\n\n\n\nLocation:Kochi, Coimbatore, Trivandrum\n\n\n\nMust have skills:Python, Pyspark\n\n\n\n\nGood to have skills:Redshift\n\n\n\nJob\n\n\nSummary: We are seeking a highly skilled and experienced Senior Data Engineer to join our growing Data and Analytics team. The ideal candidate will have deep expertise in Databricks and cloud data warehousing, with a proven track record of designing and building scalable data pipelines, optimizing data architectures, and enabling robust analytics capabilities. This role involves working collaboratively with cross-functional teams to ensure the organization leverages data as a strategic asset. Your responsibilities will include:\n\n\n\n\nRoles & Responsibilities\nDesign, build, and maintain scalable data pipelines and ETL processes using Databricks and other modern tools.\nArchitect, implement, and manage cloud-based data warehousing solutions on Databricks (Lakehouse Architecture)\nDevelop and maintain optimized data lake architectures to support advanced analytics and machine learning use cases.\nCollaborate with stakeholders to gather requirements, design solutions, and ensure high-quality data delivery.\nOptimize data pipelines for performance and cost efficiency.\nImplement and enforce best practices for data governance, access control, security, and compliance in the cloud.\nMonitor and troubleshoot data pipelines to ensure reliability and accuracy.\nLead and mentor junior engineers, fostering a culture of continuous learning and innovation.\nExcellent communication skills\nAbility to work independently and along with client based out of western Europe\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\nDesigning, developing, optimizing, and maintaining data pipelines that adhere to ETL principles and business goals\nSolving complex data problems to deliver insights that helps our business to achieve their goals.\nSource data (structured unstructured) from various touchpoints, format and organize them into an analyzable format.\nCreating data products for analytics team members to improve productivity\nCalling of AI services like vision, translation etc. to generate an outcome that can be used in further steps along the pipeline.\nFostering a culture of sharing, re-use, design and operational efficiency of data and analytical solutions\nPreparing data to create a unified database and build tracking solutions ensuring data quality\nCreate Production grade analytical assets deployed using the guiding principles of CI/CD.\n\n\n\nProfessional and Technical Skills\nExpert in Python, Scala, Pyspark, Pytorch, Javascript (any 2 at least)\nExtensive experience in data analysis (Big data- Apache Spark environments), data libraries (e.g. Pandas, SciPy, Tensorflow, Keras etc.), and SQL. 3-4 years of hands-on experience working on these technologies.\nExperience in one of the many BI tools such as Tableau, Power BI, Looker.\nGood working knowledge of key concepts in data analytics, such as dimensional modeling, ETL, reporting/dashboarding, data governance, dealing with structured and unstructured data, and corresponding infrastructure needs.\nWorked extensively in Microsoft Azure (ADF, Function Apps, ADLS, Azure SQL), AWS (Lambda,Glue,S3), Databricks analytical platforms/tools, Snowflake Cloud Datawarehouse.\n\n\n\n\nAdditional Information\nExperience working in cloud Data warehouses like Redshift or Synapse\nCertification in any one of the following or equivalent\nAWS- AWS certified data Analytics- Speciality\nAzure- Microsoft certified Azure Data Scientist Associate\nSnowflake- Snowpro core- Data Engineer\nDatabricks Data Engineering\n\nQualification\n\n\n\nExperience:5-8 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scala', 'pyspark', 'pytorch', 'python', 'data bricks', 'glue', 'amazon redshift', 'data warehousing', 'sql', 'tensorflow', 'sql azure', 'spark', 'keras', 'big data', 'etl', 'snowflake', 'scipy', 'data analysis', 'azure data lake', 'microsoft azure', 'power bi', 'javascript', 'pandas', 'tableau', 'lambda expressions', 'aws']",2025-06-13 05:14:46
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Mumbai'],"As a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution.\nYour primary responsibilities include\nLead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements.\nStrive for continuous improvements by testing the build solution and working under an agile framework.\nDiscover and implement the latest technologies trends to maximize and build creative solutions\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience with Apache Spark (PySpark)In-depth knowledge of Spark’s architecture, core APIs, and PySpark for distributed data processing.\nBig Data TechnologiesFamiliarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering\n\nSkills:\nStrong understanding of ETL pipelines, data modeling, and data warehousing concepts.\nStrong proficiency in PythonExpertise in Python programming with a focus on data processing and manipulation. Data Processing FrameworksKnowledge of data processing libraries such as Pandas, NumPy.\nSQL ProficiencyExperience writing optimized SQL queries for large-scale data analysis and transformation.\nCloud PlatformsExperience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems\n\n\nPreferred technical and professional experience\nDefine, drive, and implement an architecture strategy and standards for end-to-end monitoring.\nPartner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering,\nGood to have detection and prevention tools for Company products and Platform and customer-facing",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data processing', 'pyspark', 'data modeling', 'spark', 'data analysis', 'microsoft azure', 'data warehousing', 'cloud platforms', 'numpy', 'data engineering', 'sql', 'pandas', 'spring boot', 'etl pipelines', 'java', 'gcp', 'kafka', 'data warehousing concepts', 'hadoop', 'big data', 'aws', 'etl']",2025-06-13 05:14:48
Data Engineer-Data Platforms-Google,IBM,2 - 5 years,Not Disclosed,['Hyderabad'],"As an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\n\n\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['elastic search', 'gcp', 'splunk', 'hadoop', 'big data', 'hive', 'python', 'data management', 'presentation skills', 'microsoft azure', 'machine learning', 'javascript', 'sql', 'docker', 'java', 'git', 'spark', 'linux', 'jenkins', 'html', 'mysql', 'aws']",2025-06-13 05:14:50
Data Engineer-Business Intelligence,IBM,3 - 8 years,Not Disclosed,['Pune'],"Provide expertise in analysis, requirements gathering, design, coordination, customization, testing and support of reports, in client’s environment\nDevelop and maintain a strong working relationship with business and technical members of the team\nRelentless focus on quality and continuous improvement\nPerform root cause analysis of reports issues\nDevelopment / evolutionary maintenance of the environment, performance, capability and availability.\nAssisting in defining technical requirements and developing solutions\nEffective content and source-code management, troubleshooting and debugging.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nCognos Developer & Admin Required. EducationThe resource should be full time MCA/M. Tech/B. Tech/B.E. and should preferably have relevant certifications\nExperienceThe resource should have a minimum of 3 years of experience of working in the in BI DW projects in areas pertaining to reporting and visualization using cognos.\nThe resources shall have worked in at least two projects where they were involved in developing reporting/ visualization\nHe shall have good understanding of UNIX.\nShould be well conversant in English and should have excellent writing, MIS, communication, time management and multi-tasking skill\n\n\nPreferred technical and professional experience\nExperience with various cloud and integration platforms (e.g. AWS, Google, Azure)\nAgile mindset – ability to process changes of priorities and requests, ownership, critical thinking\nExperience with an ETL/Data Integration tool (eg. IBM InfoSphere DataStage, Azure Data Factory, Informatica PowerCenter)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['informatica powercenter', 'microsoft azure', 'mis', 'bi dw', 'etl', 'datastage', 'bi', 'azure data factory', 'cognos', 'root cause analysis', 'business intelligence', 'sql', 'dw', 'ibm datastage', 'troubleshooting', 'debugging', 'agile', 'data visualization', 'aws', 'data integration', 'informatica', 'unix']",2025-06-13 05:14:52
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Mumbai'],"Experience with Scala object-oriented/object function Strong SQL background.\nExperience in Spark SQL, Hive, Data Engineer.\nSQL Experience with data pipelines & Data Lake Strong background in distributed comp.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nSQL Experience with data pipelines & Data Lake Strong background in distributed comp\nExperience with Scala object-oriented/object function Strong SQL background\n\n\nPreferred technical and professional experience\nCore Scala Development Experience",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'scala', 'sql', 'spark', 'data lake', 'amazon redshift', 'pyspark', 'data warehousing', 'emr', 'java', 'data modeling', 'mysql', 'hadoop', 'big data', 'etl', 'python', 'microsoft azure', 'machine learning', 'data engineering', 'sql server', 'nosql', 'amazon ec2', 'kafka', 'sqoop', 'aws']",2025-06-13 05:14:54
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Pune'],"As a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution.\nYour primary responsibilities include:\nLead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements.\nStrive for continuous improvements by testing the build solution and working under an agile framework.\nDiscover and implement the latest technologies trends to maximize and build creative solutions\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience with Apache Spark (PySpark)In-depth knowledge of Spark’s architecture, core APIs, and PySpark for distributed data processing.\nBig Data TechnologiesFamiliarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering\n\nSkills:\nStrong understanding of ETL pipelines, data modeling, and data warehousing concepts.\nStrong proficiency in PythonExpertise in Python programming with a focus on data processing and manipulation. Data Processing FrameworksKnowledge of data processing libraries such as Pandas, NumPy.\nSQL ProficiencyExperience writing optimized SQL queries for large-scale data analysis and transformation.\nCloud PlatformsExperience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems\n\n\nPreferred technical and professional experience\nDefine, drive, and implement an architecture strategy and standards for end-to-end monitoring.\nPartner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering,\nGood to have detection and prevention tools for Company products and Platform and customer-facing",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data processing', 'pyspark', 'data modeling', 'spark', 'data analysis', 'microsoft azure', 'data warehousing', 'cloud platforms', 'numpy', 'data engineering', 'sql', 'pandas', 'spring boot', 'etl pipelines', 'java', 'gcp', 'kafka', 'data warehousing concepts', 'hadoop', 'big data', 'aws', 'etl']",2025-06-13 05:14:56
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Navi Mumbai'],"As a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution.\n\nYour primary responsibilities include:\nLead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements.\nStrive for continuous improvements by testing the build solution and working under an agile framework.\nDiscover and implement the latest technologies trends to maximize and build creative solutions\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience with Apache Spark (PySpark)In-depth knowledge of Spark’s architecture, core APIs, and PySpark for distributed data processing.\nBig Data TechnologiesFamiliarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering\n\nSkills:\nStrong understanding of ETL pipelines, data modeling, and data warehousing concepts.\nStrong proficiency in PythonExpertise in Python programming with a focus on data processing and manipulation. Data Processing FrameworksKnowledge of data processing libraries such as Pandas, NumPy.\nSQL ProficiencyExperience writing optimized SQL queries for large-scale data analysis and transformation.\nCloud PlatformsExperience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems\n\n\nPreferred technical and professional experience\nDefine, drive, and implement an architecture strategy and standards for end-to-end monitoring.\nPartner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering,\nGood to have detection and prevention tools for Company products and Platform and customer-facing",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data processing', 'pyspark', 'data modeling', 'spark', 'data analysis', 'microsoft azure', 'data warehousing', 'cloud platforms', 'numpy', 'data engineering', 'sql', 'pandas', 'spring boot', 'etl pipelines', 'java', 'gcp', 'kafka', 'data warehousing concepts', 'hadoop', 'big data', 'aws', 'etl']",2025-06-13 05:14:58
Data Engineer-Data Platforms-AWS,IBM,4 - 9 years,Not Disclosed,['Kochi'],"As Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in the development of data solutions using Spark Framework with Python or Scala on Hadoop and Azure Cloud Data Platform\n\nResponsibilities:\nExperienced in building data pipelines to Ingest, process, and transform data from files, streams and databases. Process the data with Spark, Python, PySpark and Hive, Hbase or other NoSQL databases on Azure Cloud Data Platform or HDFS\nExperienced in develop efficient software code for multiple use cases leveraging Spark Framework / using Python or Scala and Big Data technologies for various use cases built on the platform\nExperience in developing streaming pipelines\nExperience to work with Hadoop / Azure eco system components to implement scalable solutions to meet the ever-increasing data volumes, using big data/cloud technologies Apache Spark, Kafka, any Cloud computing etc\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nMinimum 4+ years of experience in Big Data technologies with extensive data engineering experience in Spark / Python or Scala;\nMinimum 3 years of experience on Cloud Data Platforms on Azure;\nExperience in DataBricks / Azure HDInsight / Azure Data Factory, Synapse, SQL Server DB\nGood to excellent SQL skills\nExposure to streaming solutions and message brokers like Kafka technologies\n\n\nPreferred technical and professional experience\nCertification in Azure and Data Bricks or Cloudera Spark Certified developers",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'big data technologies', 'data engineering', 'sql', 'spark', 'hive', 'cloudera', 'scala', 'pyspark', 'microsoft azure', 'azure data factory', 'azure hdinsight', 'azure cloud', 'sql server', 'nosql', 'data bricks', 'apache', 'kafka', 'hadoop', 'big data', 'aws', 'cloud computing', 'hbase', 'nosql databases']",2025-06-13 05:15:00
"Data Engineer, Alexa AI Developer Tech",Amazon,3 - 8 years,Not Disclosed,['Pune'],"Alexa+ is our next-generation assistant powered by generative AI. Alexa+ is more conversational, smarter, personalized, and gets things done.\n\nOur goal is make Alexa+ an instantly familiar personal assistant that is always ready to help or entertain on any device. At the core of this vision is Alexa AI Developer Tech, a close-knit team that s dedicated to providing software developers with the tools, primitives, and services they need to easily create engaging customer experiences that expand the wealth of information, products and services available on Alexa+.\n\nYou will join a growing organization working on top technology using Generative AI and have an enormous opportunity to make an impact on the design, architecture, and implementation of products used every day, by people you know.\n\nWe re working hard, having fun, and making history; come join us!\n\n\nWork with a team of product and program managers, engineering leaders, and business leaders to build data architectures and platforms to support business\nDesign, develop, and operate high-scalable, high-performance, low-cost, and accurate data pipelines in distributed data processing platforms\nRecognize and adopt best practices in data processing, reporting, and analysis: data integrity, test design, analysis, validation, and documentation\nKeep up to date with big data technologies, evaluate and make decisions around the use of new or existing software products to design the data architecture\nDesign, build and own all the components of a high-volume data warehouse end to end.\nProvide end-to-end data engineering support for project lifecycle execution (design, execution and risk assessment)\nContinually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers\nInterface with other technology teams to extract, transform, and load (ETL) data from a wide variety of data sources\nOwn the functional and nonfunctional scaling of software systems in your ownership area. *Implement big data solutions for distributed computing.\n\nAbout the team\nAlexa AI Developer Tech is an organization within Alexa on a mission to empower developers to create delightful and engaging experiences by making Alexa more natural, accurate, conversational, and personalized. 3+ years of data engineering experience\n4+ years of SQL experience\nExperience with data modeling, warehousing and building ETL pipelines Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions\nExperience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)",,,,"['Data modeling', 'Business design', 'Risk assessment', 'Test design', 'Architectural design', 'Data processing', 'data integrity', 'Design analysis', 'SQL', 'Data architecture']",2025-06-13 05:15:02
"Data Engineer II, TFAW/Sherlock",Amazon,3 - 8 years,Not Disclosed,['Hyderabad'],"As a Data Engineer you will be working on building and maintaining complex data pipelines, assemble large and complex datasets to generate business insights and to enable data driven decision making and support the rapidly growing and dynamic business demand for data.\nYou will have an opportunity to collaborate and work with various teams of Business analysts, Managers, Software Dev Engineers, and Data Engineers to determine how best to design, implement and support solutions. You will be challenged and provided with tremendous growth opportunity in a customer facing, fast paced, agile environment.\n\n\nDesign, implement and support an analytical data platform solutions for data driven decisions and insights\nDesign data schema and operate internal data warehouses SQL/NOSQL database systems\nWork on different data model designs, architecture, implementation, discussions and optimizations\nInterface with other teams to extract, transform, and load data from a wide variety of data sources using AWS big data technologies like EMR, RedShift, Elastic Search etc.\nWork on different AWS technologies such as S3, RedShift, Lambda, Glue, etc.. and Explore and learn the latest AWS technologies to provide new capabilities and increase efficiency\nWork on data lake platform and different components in the data lake such as Hadoop, Amazon S3 etc.\nWork on SQL technologies on Hadoop such as Spark, Hive, Impala etc..\nHelp continually improve ongoing analysis processes, optimizing or simplifying self-service support for customers\nMust possess strong verbal and written communication skills, be self-driven, and deliver high quality results in a fast-paced environment.\nRecognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.\nEnjoy working closely with your peers in a group of talented engineers and gain knowledge.\nBe enthusiastic about building deep domain knowledge on various Amazon s business domains.\nOwn the development and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions. 3+ years of data engineering experience\n4+ years of SQL experience\nExperience with data modeling, warehousing and building ETL pipelines Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions\nExperience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)",,,,"['NoSQL', 'data engineer ii', 'Business Analyst', 'Analytical', 'Schema', 'Test design', 'Agile', 'data integrity', 'Design analysis', 'SQL']",2025-06-13 05:15:03
Azure Cloud Data Engineering Consultant,Optum,7 - 10 years,17-27.5 Lacs P.A.,['Gurugram'],"Primary Responsibilities:\nDesign and develop applications and services running on Azure, with a strong emphasis on Azure Databricks, ensuring optimal performance, scalability, and security.\nBuild and maintain data pipelines using Azure Databricks and other Azure data integration tools.\nWrite, read, and debug Spark, Scala, and Python code to process and analyze large datasets.\nWrite extensive query in SQL and Snowflake\nImplement security and access control measures and regularly audit Azure platform and infrastructure to ensure compliance.\nCreate, understand, and validate design and estimated effort for given module/task, and be able to justify it.\nPossess solid troubleshooting skills and perform troubleshooting of issues in different technologies and environments.\nImplement and adhere to best engineering practices like design, unit testing, functional testing automation, continuous integration, and delivery.\nMaintain code quality by writing clean, maintainable, and testable code.\nMonitor performance and optimize resources to ensure cost-effectiveness and high availability.\nDefine and document best practices and strategies regarding application deployment and infrastructure maintenance.\nProvide technical support and consultation for infrastructure questions.\nHelp develop, manage, and monitor continuous integration and delivery systems.\nTake accountability and ownership of features and teamwork.\nComply with the terms and conditions of the employment contract, company policies and procedures, and any directives.\nRequired Qualifications:\nB.Tech/MCA (Minimum 16 years of formal education)\nOverall 7+ years of experience.\nMinimum of 3 years of experience in Azure (ADF), Databricks and DevOps.\n5 years of experience in writing advanced level SQL.\n2-3 years of experience in writing, reading, and debugging Spark, Scala, and Python code.\n3 or more years of experience in architecting, designing, developing, and implementing cloud solutions on Azure.\nProficiency in programming languages and scripting tools.\nUnderstanding of cloud data storage and database technologies such as SQL and NoSQL.\nProven ability to collaborate with multidisciplinary teams of business analysts, developers, data scientists, and subject-matter experts.\nFamiliarity with DevOps practices and tools, such as continuous integration and continuous deployment (CI/CD) and Teraform.\nProven proactive approach to spotting problems, areas for improvement, and performance bottlenecks.\nProven excellent communication, writing, and presentation skills.\nExperience in interacting with international customers to gather requirements and convert them into solutions using relevant skills.\nPreferred Qualifications:\nKnowledge of AI/ML or LLM (GenAI).\nKnowledge of US Healthcare domain and experience with healthcare data.\nExperience and skills with Snowflake.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Databricks', 'ETL', 'SQL', 'Python', 'Airflow', 'Pyspark', 'Snowflake', 'SCALA', 'Spark', 'Data Bricks']",2025-06-13 05:15:05
SNOWFLAKE DATA ENGINEER,Capgemini,6 - 11 years,Not Disclosed,['Chennai'],"Your Role \n\nWorks in the area of Software Engineering, which encompasses the development, maintenance and optimization of software solutions/applications.\n1. Applies scientific methods to analyse and solve software engineering problems.\n2. He/she is responsible for the development and application of software engineering practice and knowledge, in research, design, development and maintenance.\n3. His/her work requires the exercise of original thought and judgement and the ability to supervise the technical and administrative work of other software engineers.\n4. The software engineer builds skills and expertise of his/her software engineering discipline to reach standard software engineer skills expectations for the applicable role, as defined in Professional Communities.\n5. The software engineer collaborates and acts as team player with other software engineers and stakeholders.\nWorks in the area of Software Engineering, which encompasses the development, maintenance and optimization of software solutions/applications.1. Applies scientific methods to analyse and solve software engineering problems.2. He/she is responsible for the development and application of software engineering practice and knowledge, in research, design, development and maintenance.3. His/her work requires the exercise of original thought and judgement and the ability to supervise the technical and administrative work of other software engineers.4. The software engineer builds skills and expertise of his/her software engineering discipline to reach standard software engineer skills expectations for the applicable role, as defined in Professional Communities.5. The software engineer collaborates and acts as team player with other software engineers and stakeholders.\n\n Your Profile \n4+ years of experience in data architecture, data warehousing, and cloud data solutions.\nMinimum 3+ years of hands-on experience with End to end Snowflake implementation.\nExperience in developing data architecture and roadmap strategies with knowledge to establish data governance and quality frameworks within Snowflake\nExpertise or strong knowledge in Snowflake best practices, performance tuning, and query optimisation.\nExperience with cloud platforms like AWS or Azure and familiarity with Snowflakes integration with these environments.\nStrong knowledge in at least one cloud(AWS or Azure) is mandatory\nSolid understanding of SQL, Python, and scripting for data processing and analytics.\nExperience in leading teams and managing complex data migration projects.\nStrong communication skills, with the ability to explain technical concepts to non-technical stakeholders.\n\nKnowledge on new Snowflake features,AI capabilities and industry trends to drive innovation and continuous improvement.\n\n  \n\n Skills (competencies) \n\nVerbal Communication",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'python', 'microsoft azure', 'sql', 'aws', 'head hunting', 'screening', 'performance tuning', 'data processing', 'data warehousing', 'hrsd', 'data architecture', 'data migration', 'sourcing', 'talent acquisition', 'it recruitment', 'technical recruitment', 'recruitment', 'data governance']",2025-06-13 05:15:07
Data Engineer-Business Intelligence,IBM,5 - 10 years,Not Disclosed,['Hyderabad'],"Provide expertise in analysis, requirements gathering, design, coordination, customization, testing and support of reports, in client’s environment\nDevelop and maintain a strong working relationship with business and technical members of the team\nRelentless focus on quality and continuous improvement\nPerform root cause analysis of reports issues\nDevelopment / evolutionary maintenance of the environment, performance, capability and availability.\nAssisting in defining technical requirements and developing solutions\nEffective content and source-code management, troubleshooting and debugging\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n5+ years of experience with BI tools, with expertise and/or certification in at least one major BI platform – Tableau preferred.\nAdvanced knowledge of SQL, including the ability to write complex stored procedures, views, and functions.\nProven capability in data storytelling and visualization, delivering actionable insights through compelling presentations.\nExcellent communication skills, with the ability to convey complex analytical findings to non-technical stakeholders in a clear, concise, and meaningful way.\n5.Identifying and analyzing industry trends, geographic variations, competitor strategies, and emerging customer behavior\n\n\nPreferred technical and professional experience\nTroubleshooting capabilities to debug Data controls Capable of converting business requirements into workable model.\nGood communication skills, willingness to learn new technologies, Team Player, Self-Motivated, Positive Attitude.\nMust have thorough understanding of SQL & advance SQL (Joining & Relationships)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['advance sql', 'sql', 'bi tools', 'debugging', 'troubleshooting', 'python', 'data analysis', 'data analytics', 'bi', 'data warehousing', 'power bi', 'business analysis', 'machine learning', 'business intelligence', 'sql server', 'qlikview', 'tableau', 'r', 'data visualization', 'etl', 'ssis']",2025-06-13 05:15:10
Cloud Data Engineer - GCP,Synechron,2 - 3 years,Not Disclosed,"['Hyderabad', 'Gachibowli']","Job Summary\nSynechron is seeking a highly motivated and skilled Senior Cloud Data Engineer GCP to join our cloud solutions team. In this role, you will collaborate closely with clients and internal stakeholders to design, implement, and manage scalable, secure, and high-performance cloud-based data solutions on Google Cloud Platform (GCP). You will leverage your technical expertise to ensure the integrity, security, and efficiency of cloud data architectures, enabling the organization to derive maximum value from cloud data assets. This role contributes directly to our mission of delivering innovative digital transformation solutions and supports the organizations strategic objectives of scalable and sustainable cloud infrastructure.\nSoftware Requirements\nOverall Responsibilities\nTechnical Skills (By Category)\nExperience Requirements\nDay-to-Day Activities\nQualifications\nProfessional Competencies",,,,"['GCP', 'Jenkins', 'Java', 'NoSQL', 'Bash scripts', 'Data Studio', 'Data Management', 'CI/CD', 'Apache Beam', 'MongoDB', 'Cloud Build']",2025-06-13 05:15:12
Data Engineer III,Expedia Group,6 - 11 years,Not Disclosed,['Gurugram'],"Why Join Us?\nTo shape the future of travel, people must come first. Guided by our Values and Leadership Agreements, we foster an open culture where everyone belongs, differences are celebrated and know that when one of us wins, we all win.\nWe provide a full benefits package, including exciting travel perks, generous time-off, parental leave, a flexible work model (with some pretty cool offices), and career development resources, all to fuel our employees passion for travel and ensure a rewarding career journey. We re building a more open world. Join us.\nData Engineer III\nExpedia Group s CTO Enablement team is looking for a highly motivated Data Engineer III to lead the design, delivery, and stewardship of business-critical data infrastructure that powers our Capitalization program and Business Operations functions . This role is at the intersection of finance, strategy, and engineering , where data precision and operational rigor directly support the company s financial integrity and execution effectiveness.\nYou will collaborate with stakeholders across Finance, BizOps, and Technology to build scalable data solutions that ensure capitalization accuracy, enable deep operational analytics, and streamline financial and business reporting at scale.\nWhat you will do:\nDesign, build, and maintain high-scale data pipelines and transformation logic to support CapEx/OpEx classification, capitalization tracking, and operational data modeling.\nDeliver clean, well-documented, governed datasets that drive finance reporting, strategic planning, and key operational dashboards.\nPartner with cross-functional teams (Finance, Engineering, Strategy) to translate business and compliance requirements into technical solutions.\nLead the development of data models and ETL processes to support performance monitoring, workforce utilization, project financials, and business KPIs.\nEstablish and enforce data quality, lineage, and access control standards to ensure trust in business-critical data.\nProactively identify and resolve data reliability issues related to financial close processes, budget tracking, and capitalization rules.\nServe as a technical advisor to BizOps and Finance stakeholders, recommending improvements in tooling, architecture, and process automation.\nMentor other engineers and contribute to the growth of a high-performance data team culture.\nWho you are:\n6+ years of experience in data engineering , analytics engineering , or data infrastructure roles with a focus on operational and financial data.\nExpertise in SQL and Python , and experience with data pipeline orchestration tools such as Airflow , dbt , or equivalent.\nStrong understanding of cloud-based data platforms (e.g., Snowflake, BigQuery, Redshift, or Databricks).\nDeep familiarity with capitalization standards , CapEx/OpEx distinction, and operational reporting in a tech-driven environment.\nDemonstrated ability to build scalable, reliable ETL/ELT workflows that serve diverse analytical and reporting needs.\nExperience working cross-functionally in complex organizations with multiple stakeholder groups.\nPassion for operational excellence, data governance, and driving actionable business insights from data.\nPreferred qualifications:\n- Experience supporting BizOps , FP&A , or Product Finance teams with data tooling and reporting.\n- Familiarity with BI platforms like Looker , Power BI , or Tableau .\n- Exposure to agile delivery frameworks and enterprise-level operational rhythms.\nAccommodation requests\nIf you need assistance with any part of the application or recruiting process due to a disability, or other physical or mental health conditions, please reach out to our Recruiting Accommodations Team through the Accommodation Request .\nWe are proud to be named as a Best Place to Work on Glassdoor in 2024 and be recognized for award-winning culture by organizations like Forbes, TIME, Disability:IN, and others.\nExpedia Groups family of brands includes: Brand Expedia , Hotels.com , Expedia Partner Solutions, Vrbo , trivago , Orbitz , Travelocity , Hotwire , Wotif , ebookers , CheapTickets , Expedia Group Media Solutions, Expedia Local Expert , CarRentals.com , and Expedia Cruises . 2024 Expedia, Inc. All rights reserved. Trademarks and logos are the property of their respective owners. . Never provide sensitive, personal information to someone unless you re confident who the recipient is. Expedia Group does not extend job offers via email or any other messaging tools to individuals with whom we have not made prior contact. Our email domain is @expediagroup.com. The official website to find and apply for job openings at Expedia Group is careers.expediagroup.com/jobs .\nExpedia is committed to creating an inclusive work environment with a diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, gender, sexual orientation, national origin, disability or age.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Operational excellence', 'Data modeling', 'Talent acquisition', 'Analytical', 'Strategic planning', 'Data quality', 'Operations', 'Analytics', 'SQL', 'Business operations']",2025-06-13 05:15:13
Collibra Data Governance Engineer,Allegis Group,6 - 11 years,Not Disclosed,[],"Collibra Data Governance Engineer\nJob Location : Hyderabad / Bangalore / Chennai / Noida/ Gurgaon / Pune / Indore / Mumbai/ Kolkata\nRequired Skills\n5+ years of experience in data governance and/or metadata management.\nHands-on experience with Collibra Data Governance Center (Collibra DGC), including workflow configuration, cataloging, and operating model customization.\nStrong knowledge of metadata management, data lineage, and data quality principles.\nHands-on experience with Snowflake\nFamiliarity with data integration tools and AWS cloud platform\nExperience with SQL and working knowledge of relational databases.\nUnderstanding of data privacy regulations (e.g., GDPR, CCPA) and compliance frameworks.\nPreferred Skills\nCertifications such as Collibra Certified Solution Architect.\nExperience integrating Collibra with tools like Snowflake, Tableau or other BI/analytics platforms.\nExposure to DataOps, MDM (Master Data Management), and data governance frameworks like DAMA-DMBOK.\nStrong communication and stakeholder management skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Collibra', 'Metadata', 'Data Governance']",2025-06-13 05:15:15
Azure Senior Data Engineers,IT Services & Consulting,5 - 9 years,14-24 Lacs P.A.,['Bengaluru'],"Job Title: Senior Data Engineer Azure\nLocation: Bengaluru\nExperience: 6+ years (3+ years on Azure data services preferred)\nDepartment: Data Engineering / IT\nJob Summary:\nWe are seeking a highly skilled Senior Data Engineer with expertise in Microsoft Azure to design, develop, and optimize data pipelines, data lakes, and warehouse solutions. The ideal candidate will play a key role in building scalable and secure data platforms to support business intelligence, analytics, and machine learning use cases.\nKey Responsibilities:\nDesign, build, and maintain scalable data pipelines using Azure Data Factory, Databricks, Synapse Analytics, and related tools.\nDevelop and optimize ETL/ELT processes for structured and unstructured data.\nImplement data lake and data warehouse solutions following best practices and security standards.\nCollaborate with data scientists, analysts, and business stakeholders to understand data requirements.\nEnsure data quality, lineage, and governance using tools like Purview, Azure Monitor, and Data Catalog.\nMonitor and troubleshoot performance issues across data flows and batch processing pipelines.\nSupport real-time data integration and streaming solutions using Azure Event Hubs, Stream Analytics, or Kafka.\nMaintain and enhance CI/CD pipelines for data solutions using Azure DevOps or GitHub Actions.\nLead and mentor junior engineers in best practices for Azure data engineering.\nRequired Skills & Qualifications:\nBachelor’s or Master’s degree in Computer Science, Engineering, or related field.\n6+ years of experience in data engineering roles, with 3+ years working with Azure data services.\nProficiency in SQL, Python or Scala.\nExperience with tools like Azure Data Factory, Azure Synapse Analytics, Azure Databricks, Azure SQL Database, and Azure Blob Storage.\nStrong understanding of data modeling, data warehousing, and data lake architecture.\nFamiliarity with DevOps practices, infrastructure-as-code (IaC) tools (ARM, Bicep, Terraform), and CI/CD pipelines.\nKnowledge of data governance and data security best practices on cloud platforms.\nExcellent communication and documentation skills.\nPreferred Qualifications:\nAzure certification (e.g., Azure Data Engineer Associate, Azure Solutions Architect, etc.)\nExperience with big data frameworks (e.g., Spark, Hadoop).\nKnowledge of machine learning pipelines or MLOps in Azure.\nExperience with Power BI or integration with other visualization tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Senior Data Engineers', 'Azure Data Factory', 'azure']",2025-06-13 05:15:17
Senior Data Engineer -Bangalore,Happiest Minds Technologies,6 - 10 years,Not Disclosed,['Bengaluru'],"Job Overview:\nThe primary purpose of this role is to translate business requirements and functional specifications into logical program designs and to deliver dashboards, schema, data pipelines, and software solutions. This includes developing, configuring, or modifying data components within various complex business and/or enterprise application solutions in various computing environments. You will partner closely with multiple Business partners, Product Owners, Data Strategy, Data Platform, Data Science and Machine Learning (MLOps) teams to drive innovative data products for end users. Additionally, you will help shape overall solution & data products, develop scalable solutions through best-in-class engineering practices.",,,,"['NoSQL', 'big data systems', 'Data Pipeline', 'MongoDB', 'SQL', 'Hive', 'GIT', 'Hadoop', 'Kafka', 'Agile', 'MQL', 'Ci/Cd']",2025-06-13 05:15:19
Senior Data Engineer,PulseData labs Pvt Ltd,7 - 10 years,Not Disclosed,['Bengaluru'],"Company name: PulseData labs Pvt Ltd (captive Unit for URUS, USA)\n\nAbout URUS\nWe are the URUS family (US), a global leader in products and services for Agritech.\n\nSENIOR DATA ENGINEER\nThis role is responsible for the design, development, and maintenance of data integration and reporting solutions. The ideal candidate will possess expertise in Databricks and strong skills in SQL Server, SSIS and SSRS, and experience with other modern data engineering tools such as Azure Data Factory. This position requires a proactive and results-oriented individual with a passion for data and a strong understanding of data warehousing principles.\n\nResponsibilities\nData Integration\nDesign, develop, and maintain robust and efficient ETL pipelines and processes on Databricks.\nTroubleshoot and resolve Databricks pipeline errors and performance issues.\nMaintain legacy SSIS packages for ETL processes.\nTroubleshoot and resolve SSIS package errors and performance issues.\nOptimize data flow performance and minimize data latency.\nImplement data quality checks and validations within ETL processes.\nDatabricks Development\nDevelop and maintain Databricks pipelines and datasets using Python, Spark and SQL.\nMigrate legacy SSIS packages to Databricks pipelines.\nOptimize Databricks jobs for performance and cost-effectiveness.\nIntegrate Databricks with other data sources and systems.\nParticipate in the design and implementation of data lake architectures.\nData Warehousing\nParticipate in the design and implementation of data warehousing solutions.\nSupport data quality initiatives and implement data cleansing procedures.\nReporting and Analytics\nCollaborate with business users to understand data requirements for department driven reporting needs.\nMaintain existing library of complex SSRS reports, dashboards, and visualizations.\nTroubleshoot and resolve SSRS report issues, including performance bottlenecks and data inconsistencies.\nCollaboration and Communication\nComfortable in entrepreneurial, self-starting, and fast-paced environment, working both independently and with our highly skilled teams.\nCollaborate effectively with business users, data analysts, and other IT teams.\nCommunicate technical information clearly and concisely, both verbally and in writing.\nDocument all development work and procedures thoroughly.\nContinuous Growth\nKeep abreast of the latest advancements in data integration, reporting, and data engineering technologies.\nContinuously improve skills and knowledge through training and self-learning.\nThis job description reflects managements assignment of essential functions; it does not prescribe or restrict the tasks that may be assigned.\n\nRequirements\nBachelor's degree in computer science, Information Systems, or a related field.\n7+ years of experience in data integration and reporting.\nExtensive experience with Databricks, including Python, Spark, and Delta Lake.\nStrong proficiency in SQL Server, including T-SQL, stored procedures, and functions.\nExperience with SSIS (SQL Server Integration Services) development and maintenance.\nExperience with SSRS (SQL Server Reporting Services) report design and development.\nExperience with data warehousing concepts and best practices.\nExperience with Microsoft Azure cloud platform and Microsoft Fabric desirable.\nStrong analytical and problem-solving skills.\nExcellent communication and interpersonal skills.\nAbility to work independently and as part of a team.\nExperience with Agile methodologies.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Etl Development', 'Azure Databricks', 'Spark', 'SQL Server', 'Databricks Engineer', 'Data Warehousing', 'Pythonspark']",2025-06-13 05:15:21
"Senior Data Engineer - Airflow, PLSQL",Relanto Global,5 - 10 years,Not Disclosed,['Bengaluru'],"PositionSenior Data Engineer - Airflow, PLSQL \n\n Experience5+ Years \n\n LocationBangalore/Hyderabad/Pune \n\n\n\nSeeking a Senior Data Engineer with strong expertise in Apache Airflow and Oracle PL/SQL, along with working experience in Snowflake and Agile methodologies. The ideal candidate will also take up Scrum Master responsibilities and lead a data engineering scrum team to deliver robust, scalable data solutions.\n\n\n Key Responsibilities: \nDesign, develop, and maintain scalable data pipelines using Apache Airflow.\nWrite and optimize complex PL/SQL queries, procedures, and packages on Oracle databases.\nCollaborate with cross-functional teams to design efficient data models and integration workflows.\nWork with Snowflake for data warehousing and analytics use cases.\nOwn the delivery of sprint goals, backlog grooming, and facilitation of agile ceremonies as the Scrum Master.\nMonitor pipeline health and troubleshoot production data issues proactively.\nEnsure code quality, documentation, and best practices across the team.\nMentor junior data engineers and promote a culture of continuous improvement.\n\n\n Required Skills and Qualifications: \n5+ years of experience as a Data Engineer in enterprise environments.\nStrong expertise in  Apache Airflow  for orchestrating workflows.\nExpert in  Oracle PL/SQL  - stored procedures, performance tuning, debugging.\nHands-on experience with  Snowflake  - data modeling, SQL, optimization.\nWorking knowledge of version control (Git) and CI/CD practices.\nPrior experience or certification as a  Scrum Master  is highly desirable.\nStrong analytical and problem-solving skills with attention to detail.\nExcellent communication and leadership skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'plsql', 'stored procedures', 'oracle pl', 'performance tuning', 'hive', 'continuous integration', 'ci/cd', 'data warehousing', 'pyspark', 'git', 'apache', 'data modeling', 'spark', 'debugging', 'hadoop', 'big data', 'snowflake', 'python', 'oracle', 'sql queries', 'airflow', 'data engineering', 'agile', 'sqoop']",2025-06-13 05:15:23
Sr Data Engineer,Lowes Services India Private limited,5 - 10 years,Not Disclosed,['Bengaluru'],"We are seeking a seasoned Senior Data Engineer to join our Marketing Data Platform team. This role is pivotal in designing, building, and optimizing scalable data pipelines and infrastructure that support our marketing analytics and customer engagement strategies. The ideal candidate will have extensive experience with big data technologies, cloud platforms, and a strong understanding of marketing data dynamics.\n\nData Pipeline Development & Optimization\nDesign, develop, and maintain robust ETL/ELT pipelines using Apache PySpark on GCP services like Dataproc and Cloud Composer.\nEnsure data pipelines are scalable, efficient, and reliable to handle large volumes of marketing data.\nData Warehousing & Modeling\nImplement and manage data warehousing solutions using BigQuery, ensuring optimal performance and cost-efficiency.\nDevelop and maintain data models that support marketing analytics and reporting needs.\nCollaboration & Stakeholder Engagement\nWork closely with marketing analysts, data scientists, and cross-functional teams to understand data requirements and deliver solutions that drive business insights.\nTranslate complex business requirements into technical specifications and data architecture.\nData Quality & Governance\nImplement data quality checks and monitoring to ensure the accuracy and integrity of marketing data.\nAdhere to data governance policies and ensure compliance with data privacy regulations.\nContinuous Improvement & Innovation\nStay abreast of emerging technologies and industry trends in data engineering and marketing analytics.\nPropose and implement improvements to existing data processes and infrastructure\n  Years of Experience\n5 Years in Data Engineer space\n  Education Qualification & Certifications\nB.Tech or MCA\n  Experience\nProven experience with Apache PySpark, GCP (including Dataproc, BigQuery, Cloud Composer), and data pipeline orchestration.\nTechnical Skills\nProficiency in SQL and Python.\nExperience with data modeling, ETL/ELT processes, and data warehousing concepts.",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['orchestration', 'Data modeling', 'data governance', 'Data quality', 'Apache', 'Continuous improvement', 'Monitoring', 'SQL', 'Python', 'Data architecture']",2025-06-13 05:15:24
Senior Data Engineer,Talentien Global Solutions,4 - 8 years,12-18 Lacs P.A.,"['Hyderabad', 'Chennai', 'Coimbatore']","We are seeking a skilled and motivated Data Engineer to join our dynamic team. The ideal candidate will have experience in designing, developing, and maintaining scalable data pipelines and architectures using Hadoop, PySpark, ETL processes, and Cloud technologies.\n\nResponsibilities:\nDesign, develop, and maintain data pipelines for processing large-scale datasets.\nBuild efficient ETL workflows to transform and integrate data from multiple sources.\nDevelop and optimize Hadoop and PySpark applications for data processing.\nEnsure data quality, governance, and security standards are met across systems.\nImplement and manage Cloud-based data solutions (AWS, Azure, or GCP).\nCollaborate with data scientists and analysts to support business intelligence initiatives.\nTroubleshoot performance issues and optimize query executions in big data environments.\nStay updated with industry trends and advancements in big data and cloud technologies.\nRequired Skills:\nStrong programming skills in Python, Scala, or Java.\nHands-on experience with Hadoop ecosystem (HDFS, Hive, Spark, etc.).\nExpertise in PySpark for distributed data processing.\nProficiency in ETL tools and workflows (SSIS, Apache Nifi, or custom pipelines).\nExperience with Cloud platforms (AWS, Azure, GCP) and their data-related services.\nKnowledge of SQL and NoSQL databases.\nFamiliarity with data warehousing concepts and data modeling techniques.\nStrong analytical and problem-solving skills.\n\nInterested can reach us at +91 7305206696/ saranyadevib@talentien.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Hadoop', 'Spark', 'ETL', 'Airflow', 'Etl Pipelines', 'Big Data', 'EMR', 'Gcp Cloud', 'Data Bricks', 'Azure Cloud', 'Data Pipeline', 'SCALA', 'Snowflake', 'Data Lake', 'Data Warehousing', 'Data Modeling', 'AWS', 'Python']",2025-06-13 05:15:26
Senior Data Engineer,SPAN.IO,5 - 10 years,Not Disclosed,['Bengaluru( Indira Nagar )'],"Senior Data Engineer\n\nOur Mission\n\nSPAN is enabling electrification for all\nWe are a mission-driven company designing, building, and deploying products that electrify the built environment, reduce carbon emissions, and slow the effects of climate change.\nDecarbonization is the process to reduce or remove greenhouse gas emissions, especially carbon dioxide, from entering our atmosphere.\nElectrification is the process of replacing fossil fuel appliances that run on gas or oil with all-electric upgrades for a cleaner way to power our lives.\n\nAt SPAN, we believe in:\nEnabling homes and vehicles powered by clean energy\nMaking electrification upgrades possible\nBuilding more resilient homes with reliable backup\nDesigning a flexible and distributed electrical grid\n\nThe Role\nAs a Data Engineer you would be working to design, build, test and create infrastructure necessary for real time analytics and batch analytics pipelines. You will work with multiple teams within the org to provide analysis, insights on the data. You will also be involved in writing ETL processes that support data ingestion. You will also guide and enforce best practices for data management, governance and security. You will build infrastructure to monitor these data pipelines / ETL jobs / tasks and create tooling/infrastructure for providing visibility into these.\n\nResponsibilities\nWe are looking for a Data Engineer with passion for building data pipelines, working with product, data science and business intelligence teams and delivering great solutions. As a part of the team you:-\nAcquire deep business understanding on how SPAN data flows from IoT device to cloud through the system and build scalable and optimized data solutions that impact many stakeholders.\nBe an advocate for data quality and excellence of our platform.\nBuild tools that help streamline the management and operation of our data ecosystem.\nEnsure best practices and standards in our data ecosystem are shared across teams.\nWork with teams within the company to build close relationships with our partners to understand the value our platform can bring and how we can make it better.\nImprove data discovery by creating data exploration processes and promoting adoption of data sources across the company.\nHave a desire to write tools and applications to automate work rather than do everything by hand.\nAssist internal teams in building out data logging, alerting and monitoring for their applications\nAre passionate about CI/CD process.\nDesign, develop and establish KPIs to monitor analysis and provide strategic insights to drive growth and performance.\n\nAbout You\n\nRequired Qualifications\nBachelor's Degree in a quantitative discipline: computer science, statistics, operations research, informatics, engineering, applied mathematics, economics, etc.\n5+ years of relevant work experience in data engineering, business intelligence, research or related fields.\nExpert level production-grade, programming experience in at least one of these languages (Python, Kotlin, or other JVM based languages)\nExperience in writing clean, concise and well structured code in one of the above languages.\nExperience working with Infrastructure-as-code tools: Pulumi, Terraform, etc.\nExperience working with CI/CD systems: Circle-CI, Github Actions, Argo-CD, etc.\nExperience managing data engineering infrastructure through Docker and Kubernetes\nExperience working with latency data processing solutions like Flink, Prefect, AWS Kinesis, Kafka, Spark Stream processing etc.\nExperience with SQL/Relational databases, OLAP databases like Snowflake.\nExperience working in AWS: S3, Glue, Athena, MSK, EMR, ECR etc.\n\nBonus Qualifications\nExperience with the Energy industry\nExperience with building IoT and/or hardware products\nUnderstanding of electrical systems and residential loads\nExperience with data visualization using Tableau.\nExperience in Data loading tools like FiveTran as well as data debugging tools such as DataDog\n\nLife at SPAN\nOur Bengaluru team plays a pivotal role in SPANs continued growth and expansion. Together, were driving engineering, product development, and operational excellence to shape the future of home energy solutions.\nAs part of our team in India, youll have the opportunity to collaborate closely with our teams in the US and across the globe. This international collaboration fosters innovation, learning, and growth, while helping us achieve our bold mission of electrifying homes and advancing clean energy solutions worldwide.\nOur in-office culture offers the chance for dynamic interactions and hands-on teamwork, making SPAN a truly collaborative environment where every team members contribution matters.\nOur climate-focused culture is driven by a team of forward-thinkers, engineers, and problem-solvers who push boundaries every day.\nDo mission-driven work: Every role at SPAN directly advances clean energy adoption.\nBring powerful ideas to life: We encourage diverse ideas and perspectives to drive stronger products.\nNurture an innovation-first mindset: We encourage big thinking and bold action.\nDeliver exceptional customer value: We value hard work, and the ability to deliver exceptional customer value.\n\nBenefits at SPAN India\nGenerous paid leave\nComprehensive Insurance & Health Benefits\nCentrally located office in Bengaluru with easy access to public transit, dining, and city amenities\n\nInterested in joining our team? Apply today and well be in touch with the next steps!",Industry Type: Electronics Manufacturing,"Department: Production, Manufacturing & Engineering","Employment Type: Full Time, Permanent","['Terraform', 'Snowflake', 'AWS', 'Python', 'SQL', 'Java', 'Apache Flink', 'Kotlin']",2025-06-13 05:15:28
Senior Data Engineer,Epsilon,5 - 9 years,Not Disclosed,['Bengaluru'],"This position in the Engineering team under the Digital Experience organization. We drive the first mile of the customer experience through personalization of offers and content. We are currently on the lookout for a smart, highly driven engineer.\nYou will be part of a team that is focused on building & managing solutions, pipelines using marketing technology stacks. You will also be expected to Identify and implement improvements including for optimizing data delivery and automate processes/pipelines.\nThe incumbent is also expected to partner with various stakeholders, bring scientific rigor to design and develop high quality solutions.\nCandidate must have excellent verbal and written communication skills and be comfortable working in an entrepreneurial, startup environment within a larger company.\nClick here to view how Epsilon transforms marketing with 1 View, 1 Vision and 1 Voice.\n\nBrief Description of Role:\nExperience with both structured and unstructured data\nExperience working on AdTech or MarTech technologies.\nExperience in relational and non-relational databases and SQL (NoSQL is a plus).\nUnderstanding of Data Modeling, Data Catalog concepts and tools\nAbility to deal with data imperfections such as missing values, outliers, inconsistent formatting, etc.\nCollaborate with other members of the team to ensure high quality deliverables\nLearning and implementing the latest design patterns in data engineering\n\nData Management\nExperience with both structured and unstructured data\nExperience building Data and CI/CD pipelines\nExperience working on AdTech or MarTech technologies is added advantage\nExperience in relational and non-relational databases and SQL (NoSQL is a plus).\nHands on experience building ETL workflows/pipelines on large volumes of data\nGood understanding of Data Modeling, Data Warehouse, Data Catalog concepts and tools\nAble to identify, join, explore, and examine data from multiple disparate sources and formats\nAbility to reduce large quantities of unstructured or formless data and get it into a form in which it can be analyzed\nAbility to deal with data imperfections such as missing values, outliers, inconsistent formatting, etc.\nDevelopment\nAbility to write code in programming languages such as Python and shell script on Linux\nFamiliarity with development methodology such as Agile/Scrum\nLove to learn new technologies, keep abreast of the latest technologies within the cloud architecture, and drive your organization to adapt to emerging best practices\nGood knowledge of working in UNIX/LINUX systems\nQualifications\nBachelors degree in computer science with 5+ years of similar experience\nTech Stack: Python, SQL, Scripting language (preferably JavaScript)\nExperience or knowledge on Adobe Experience Platform (RT-CDP/AEP)\nExperience working in Cloud Platforms (GCP or AWS)\nFamiliarity with automated unit/integration test frameworks\nGood written and spoken communication skills, team player.\nStrong analytic thought process and ability to interpret findings",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Data Bricks', 'Python', 'SQL', 'Azure Aws', 'AWS']",2025-06-13 05:15:30
Senior Data Engineer,Suzva Software Technologies,6 - 8 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","JobOpening Senior Data Engineer (Remote, Contract 6 Months)\nRemote | Contract Duration: 6 Months | Experience: 6-8 Years\n\nWe are hiring a Senior Data Engineer for a 6-month remote contract position. The ideal candidate is highly skilled in building scalable data pipelines and working within the Azure cloud ecosystem, especially Databricks, ADF, and PySpark. You'll work closely with cross-functional teams to deliver enterprise-level data engineering solutions.\n\n#KeyResponsibilities\nBuild scalable ETL pipelines and implement robust data solutions in Azure.\n\nManage and orchestrate workflows using ADF, Databricks, ADLS Gen2, and Key Vaults.\n\nDesign and maintain secure and efficient data lake architecture.\n\nWork with stakeholders to gather data requirements and translate them into technical specs.\n\nImplement CI/CD pipelines for seamless data deployment using Azure DevOps.\n\nMonitor data quality, performance bottlenecks, and scalability issues.\n\nWrite clean, organized, reusable PySpark code in an Agile environment.\n\nDocument pipelines, architectures, and best practices for reuse.\n\n#MustHaveSkills\nExperience: 6+ years in Data Engineering\n\nTech Stack: SQL, Python, PySpark, Spark, Azure Databricks, ADF, ADLS Gen2, Azure DevOps, Key Vaults\n\nCore Expertise: Data Warehousing, ETL, Data Pipelines, Data Modelling, Data Governance\n\nAgile, SDLC, Containerization (Docker), Clean coding practices\n\n#GoodToHaveSkills\nEvent Hubs, Logic Apps\n\nPower BI\n\nStrong logic building and competitive programming background\n\nMode: Remote\n\nDuration: 6 Months\nLocations : Mumbai, Delhi / NCR, Bengaluru , Kolkata, Chennai, Hyderabad, Ahmedabad, Pune, Remote",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Databricks', 'Data Modelling', 'Data Pipelines', 'ADF', 'PySpark', 'Data Warehousing', 'ETL', 'Data Governance']",2025-06-13 05:15:32
"Senior Data Engineer Databricks, ADF, PySpark",Suzva Software Technologies,6 - 11 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","Senior Data Engineer (Remote, Contract 6 Months) Databricks, ADF, and PySpark.\nWe are hiring a Senior Data Engineer for a 6-month remote contract position. The ideal candidate is highly skilled in building scalable data pipelines and working within the Azure cloud ecosystem, especially Databricks, ADF, and PySpark. You'll work closely with cross-functional teams to deliver enterprise-level data engineering solutions.\n\nKeyResponsibilities\nBuild scalable ETL pipelines and implement robust data solutions in Azure.\n\nManage and orchestrate workflows using ADF, Databricks, ADLS Gen2, and Key Vaults.\n\nDesign and maintain secure and efficient data lake architecture.\n\nWork with stakeholders to gather data requirements and translate them into technical specs.\n\nImplement CI/CD pipelines for seamless data deployment using Azure DevOps.\n\nMonitor data quality, performance bottlenecks, and scalability issues.\n\nWrite clean, organized, reusable PySpark code in an Agile environment.\n\nDocument pipelines, architectures, and best practices for reuse.\n\nMustHaveSkills\nExperience: 6+ years in Data Engineering\n\nTech Stack: SQL, Python, PySpark, Spark, Azure Databricks, ADF, ADLS Gen2, Azure DevOps, Key Vaults\n\nCore Expertise: Data Warehousing, ETL, Data Pipelines, Data Modelling, Data Governance\n\nAgile, SDLC, Containerization (Docker), Clean coding practices\n\nGoodToHaveSkills\nEvent Hubs, Logic Apps\n\nPower BI\n\nStrong logic building and competitive programming background\n\nLocation : - Remote,Mumbai, Delhi / NCR, Bengaluru , Kolkata, Chennai, Hyderabad, Ahmedabad, Pune",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Databricks', 'ADF', 'PySpark', 'ADLS Gen2', 'Azure Databricks', 'Key Vaults', 'Spark', 'Azure DevOps', 'SQL', 'Python']",2025-06-13 05:15:34
Senior Data Engineer,Eurofins Digital Testing,7 - 10 years,Not Disclosed,['Bengaluru'],"Job Description\nSenior Data Engineer - Quality Engineering\nExperience Range: 7-10 Years\nLocation: Bangalore (Hybrid Mode)\nResillion, a leading quality engineering company with offices around the world, is seeking a talented Data Engineer to join our growing India team. In this role, you will play a critical part in building and maintaining the data infrastructure that supports our AI-powered testing tools and analytics solutions. You will have technical responsibility for the entire data lifecycle, from data acquisition and ingestion to storage, processing, and analysis.\nResponsibilities:\nCollaborate with GenAI engineers, Software Engineers, Test automation engineers and other stakeholders within Resillion to understand data needs and translate them into technical solutions, including designing data pipelines for training & deploying models and data pre-processing for AI, including generative AI applications.\nDesign, implement, and advise on data migration testing strategies and data quality assurance strategies for Resillion customers, ensuring a smooth transition of data to new customer systems.\nDesign, develop, and implement scalable data pipelines using cloud-based data engineering tools and technologies, with a focus on both Microsoft Azure solutions (e.g., Azure Data Factory, Azure Databricks) and Google Cloud Platform (GCP) solutions (e.g., Google Cloud Dataflow, Google Cloud Dataproc).\nWrite efficient and maintainable code to extract, transform, and load data from various sources, leveraging your expertise in Azure Data Lake Storage and other relevant Azure services, as well as Google Cloud Storage and other relevant GCP services.\nBuild and manage data warehouses and data lakes for quality engineering data, utilizing your knowledge of Azure Synapse Analytics or similar technologies, and Google BigQuery or similar technologies.\nDevelop and implement data quality checks and monitoring procedures to ensure data integrity using Azure Data Catalog or other appropriate tools, as well as Google Cloud Data Catalog or other appropriate tools.\nAutomate data engineering tasks and workflows using Azure automation tools and GCP automation tools (e.g., Cloud Functions, Cloud Composer).\nSet up quality intelligence dashboards for quality assurance data using Microsoft Power BI to provide stakeholders with clear and actionable insights.\nStay up-to-date with the latest data engineering tools and technologies, including advancements in AI, Machine Learning (MLOps), and generative AI for data processing in both the Azure and GCP environments.\nAdvise on and implement test data generation strategies and solutions for various testing needs.\n\n\nQualifications\nMinimum 7+ years of experience in data engineering or a related field\nProven experience in designing, developing, and deploying data pipelines",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Engineering services', 'Manager Quality Assurance', 'GCP', 'Quality engineering', 'Machine learning', 'Quality Engineer', 'Monitoring', 'Analytics', 'SQL', 'Python']",2025-06-13 05:15:35
Senior Data Engineer,Wavicle Data Solutions,6 - 11 years,15-25 Lacs P.A.,"['Chennai', 'Coimbatore', 'Bengaluru']","Hi Professionals,\n\nWe are looking for Senior Data Engineer for Permanent Role\n\nWork Location: Hybrid Chennai, Coimbatore or Bangalore\n\nExperience: 6 to 12 Years\n\nNotice Period: 0 TO 15 Days or Immediate Joiner.\n\nSkills:\n1. Python\n2. Pyspark\n3. SQL\n4. AWS\n5. GCP\n6. MLOps\n\nInterested can send your resume to gowtham.veerasamy@wavicledata.com.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'GCP', 'AWS', 'Ml']",2025-06-13 05:15:37
Senior Software Engineer - Data Engineering,Zendesk,2 - 6 years,Not Disclosed,['Bengaluru'],"Design, build, and maintain data quality systems and pipelines.\nWork with tools such as Snowflake, Docker/Kubernetes, and Kafka to enable scalable, observable data movement.\nCollaborate cross-functionally to close skill gaps in DQ and data platform tooling.\nContribute to building internal tooling that supports schema validation, data experimentation, and automated checks.\nCollaborate cross-functionally with data producers, analytics engineers, platform teams, and business stakeholders.\nOwn the reliability, scalability, and performance of ingestion systems deployed on AWS\nArchitect and build core components of our real-time ingestion platform using Kafka, Snowpipe Streaming.\nChampion software engineering excellence including testing, observability, CI/CD, and automation\nDrive the development of platform tools that ensure data quality, observability, and lineage through Protobuf-based schema management..\nParticipate in the implementation of ingestion best practices and reusable frameworks across data and software engineering teams.\nCore Skills:\nSolid programming experience (preferably in Java )\nExperience with distributed data systems ( Kafka, Snowflake )\nFamiliarity with Data Quality tooling and concepts\nGood working knowledge of SQL (especially for diagnostics and DQ workflows)\nExperience with containerization (Docker, Kubernetes )\nStrong debugging, observability, and pipeline reliability practices\n\nWhat You Bring:\nA systems mindset with strong software engineering fundamentals.\nPassion for building resilient, high-throughput, real-time platforms.\nAbility to influence technical direction across teams and drive alignment.\nStrong communication and mentoring skills.\nA bias toward automation, continuous improvement, and platform thinking.\n\nNice to Haves:\nExperience with GenAI tools or supporting ML/AI data workflows\nFamiliarity with cloud-native data platforms (e.g., AWS, GCP)\nExposure to dbt or ELT frameworks",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'GCP', 'Quality systems', 'Debugging', 'Data quality', 'Customer service', 'Continuous improvement', 'Monitoring', 'Analytics', 'SQL']",2025-06-13 05:15:39
Senior GCP Data Engineer,Swits Digital,6 - 9 years,Not Disclosed,['Bengaluru'],"Job Title: Senior GCP Data Engineer\nLocation: Chennai, Bangalore, Hyderabad\nExperience: 6-9 Years\nJob Summary:\nWe are seeking a GCP Data & Cloud Engineer with strong expertise in Google Cloud Platform services, including BigQuery, Cloud Run, Cloud Storage , and Pub/Sub . The ideal candidate will have deep experience in SQL coding , data pipeline development, and deploying cloud-native solutions.\nKey Responsibilities:\nDesign, implement, and optimize scalable data pipelines and services using GCP\nBuild and manage cloud-native applications deployed via Cloud Run\nDevelop complex and performance-optimized SQL queries for analytics and data transformation\nManage and automate data storage, retrieval, and archival using Cloud Storage\nImplement event-driven architectures using Google Pub/Sub\nWork with large datasets in BigQuery , including ETL/ELT design and query optimization\nEnsure security, monitoring, and compliance of cloud-based systems\nCollaborate with data analysts, engineers, and product teams to deliver end-to-end cloud solutions\nRequired Skills & Experience:\n3+ years of experience working with Google Cloud Platform (GCP)\nStrong proficiency in SQL coding , query tuning, and handling complex data transformations\nHands-on experience with:\nBigQuery\nCloud Run\nCloud Storage\nPub/Sub\nUnderstanding of data pipeline and ETL/ELT workflows in cloud environments\nFamiliarity with containerized services and CI/CD pipelines\nExperience in scripting languages (e.g., Python, Shell) is a plus\nStrong analytical and problem-solving skills",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SUB', 'query optimization', 'GCP', 'Analytical', 'Cloud', 'query', 'cloud storage', 'Security monitoring', 'SQL coding', 'Python']",2025-06-13 05:15:40
"Lead Engineer, Data Engineering (J2EE/Angular/React/React Full Stack)",S&P Global Market Intelligence,10 - 15 years,Not Disclosed,"['Mumbai', 'Maharastra']","About the Role:\nGrade Level (for internal use): 11\nThe Team\nYou will be an expert contributor and part of the Rating Organizations Data Services Product Engineering Team. This team, who has a broad and expert knowledge on Ratings organizations critical data domains, technology stacks and architectural patterns, fosters knowledge sharing and collaboration that results in a unified strategy. All Data Services team members provide leadership, innovation, timely delivery, and the ability to articulate business value. Be a part of a unique opportunity to build and evolve S&P Ratings next gen analytics platform.\nResponsibilities:\nArchitect, design, and implement innovative software solutions to enhance S&P Ratings' cloud-based analytics platform.\nMentor a team of engineers (as required), fostering a culture of trust, continuous growth, and collaborative problem-solving.\nCollaborate with business partners to understand requirements, ensuring technical solutions align with business goals.\nManage and improve existing software solutions, ensuring high performance and scalability.\nParticipate actively in all Agile scrum ceremonies, contributing to the continuous improvement of team processes.\nProduce comprehensive technical design documents and conduct technical walkthroughs.\nExperience & Qualifications:\nBachelors degree in computer science, Information Systems, Engineering, equivalent or more is required\nProficient with software development lifecycle (SDLC) methodologies like Agile, Test-driven development\n10+ years of experience with 4+ years designing/developing enterprise products, modern tech stacks and data platforms\n4+ years of hands-on experience contributing to application architecture & designs, proven software/enterprise integration design patterns and full-stack knowledge including modern distributed front end and back-end technology stacks\n5+ years full stack development experience in modern web development technologies, Java/J2EE, UI frameworks like Angular, React, SQL, Oracle, NoSQL Databases like MongoDB\nExperience designing transactional/data warehouse/data lake and data integrations with Big data eco system leveraging AWS cloud technologies\nThorough understanding of distributed computing\nPassionate, smart, and articulate developer\nQuality first mindset with a strong background and experience with developing products for a global audience at scale\nExcellent analytical thinking, interpersonal, oral and written communication skills with strong ability to influence both IT and business partners\nSuperior knowledge of system architecture, object-oriented design, and design patterns.\nGood work ethic, self-starter, and results-oriented\nExcellent communication skills are essential, with strong verbal and writing proficiencies\nExp. with Delta Lake systems like Databricks using AWS cloud technologies and PySpark is a plus\nAdditional Preferred Qualifications:\nExperience working AWS\nExperience with SAFe Agile Framework\nBachelor's/PG degree in Computer Science, Information Systems or equivalent.\nHands-on experience contributing to application architecture & designs, proven software/enterprise integration design principles\nAbility to prioritize and manage work to critical project timelines in a fast-paced environment\nExcellent Analytical and communication skills are essential, with strong verbal and writing proficiencies\nAbility to train and mentor",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'AWS cloud technologies', 'PySpark', 'J2EE', 'React Full Stack', 'Databricks', 'React', 'Angular']",2025-06-13 05:15:43
Data Engineer IV - Big Data / Spark,Sadup Soft,5 - 7 years,Not Disclosed,['Chennai'],"Must have skills :\n\n- Minimum of 5-7 years of experience in software development, with a focus on Java and infrastructure tools.\n\n- Min 6+ years of experience as a Data Engineer.\n\n- Good Experience in handling Big Data Spark, Hive SQL, BigQuery, SQL.\n\n- Candidate worked on cloud platforms and GCP would be an added advantage.\n\n- Good understanding of Hadoop based ecosystem including hard sequel, HDFS would be very essential.\n\n- Very good professional knowledge of PySpark or using Scala\n\nResponsibilities :\n\n- Collaborate with cross-functional teams such as Data Scientists, Product Partners and Partner Team Developers to identify opportunities for Big Data, Query ( Spark, Hive SQL, BigQuery, SQL ) tuning opportunities that can be solved using machine learning and generative AI.\n\n- Write clean, high-performance, high-quality, maintainable code.\n\n- Design and develop Big Data Engineering Solutions Applications for above ensuring scalability, efficiency, and maintainability of such solutions.\n\nRequirements :\n\n- A Bachelor or Master's degree in Computer Science or a related field.\n\n- Proven experience working as a Big Data & MLOps Engineer, with a focus on Spark, Scala Spark or PySpark, Spark SQL, BigQuery, Python, Google Cloud,.\n\n- Deep understanding and experience in tuning Dataproc, BigQuery, Spark Applications.\n\n- Solid knowledge of software engineering best practices, including version control systems (e.g Git), code reviews, and testing methodologies.\n\n- Strong communication skills to effectively collaborate and present findings to both technical and non-technical stakeholders.\n\n- Proven ability to adapt and learn new technologies and frameworks quickly.\n\n- A proactive mindset with a passion for continuous learning and research.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Data Engineering', 'BigQuery', 'GCP', 'Spark', 'Machine Learning', 'Python', 'SQL']",2025-06-13 05:15:44
Lead AWS Glue Data Engineer,Allegis Group,8 - 13 years,Not Disclosed,[],"Lead AWS Glue Data Engineer\nJob Location : Hyderabad / Bangalore / Chennai / Noida/ Gurgaon / Pune / Indore / Mumbai/ Kolkata\n\nWe are seeking a skilled Lead AWS Data Engineer with 8+ years of strong programming and SQL skills to join our team. The ideal candidate will have hands-on experience with AWS Data Analytics services and a basic understanding of general AWS services. Additionally, prior experience with Oracle and Postgres databases and secondary skills in Python and Azure DevOps will be an advantage.\n\nKey Responsibilities:\nDesign, develop, and optimize data pipelines using AWS Data Analytics services such as RDS, DMS, Glue, Lambda, Redshift, and Athena.\nImplement data migration and transformation processes using AWS DMS and Glue.\nWork with SQL (Oracle & Postgres) to query, manipulate, and analyse large datasets.\nDevelop and maintain ETL/ELT workflows for data ingestion and transformation.\nUtilize AWS services like S3, IAM, CloudWatch, and VPC to ensure secure and efficient data operations.\nWrite clean and efficient Python scripts for automation and data processing.\nCollaborate with DevOps teams using Azure DevOps for CI/CD pipelines and infrastructure management.\nMonitor and troubleshoot data workflows to ensure high availability and performance.\n\nPreferred Qualifications:\nAWS certifications in Data Analytics, Solutions Architect, or DevOps.\nExperience with data warehousing concepts and data lake implementations.\nHands-on experience with Infrastructure as Code (IaC) tools like Terraform or CloudFormation.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['RDS', 'Glue', 'DMS', 'Lambda', 'Redshift', 'Athena']",2025-06-13 05:15:46
Senior Data Engineer,The Main Stage Productions,4 - 6 years,Not Disclosed,['Bengaluru'],"Design and implement cloud-native data architectures on AWS, including data lakes, data warehouses, and streaming pipelines using services like S3, Glue, Redshift, Athena, EMR, Lake Formation, and Kinesis.\nDevelop and orchestrate ETL/ELT pipelines\n\nRequired Candidate profile\nParticipate in pre-sales and consulting activities such as:\nEngaging with clients to gather requirements and propose AWS-based data engineering solutions.\nSupporting RFPs/RFIs, technical proposals",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AWS Glue', 'GitHub Actions', 'PySpark', 'Scala', 'CodePipeline', 'Step Functions', 'data engineering']",2025-06-13 05:15:48
Data Engineering - Senior Developer with Salesforce,Job Delights,5 - 10 years,25-27.5 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Data Engineering with SQL, Python, ETL & SalesForce Marketing Cloud (Must)",Industry Type: Software Product,Department: Other,"Employment Type: Full Time, Permanent","['Salesforce Marketing Cloud', 'SQL', 'Marketing Cloud', 'Salesforce', 'Python']",2025-06-13 05:15:50
Data Engineer - SAS Migration,Crisil,2 - 4 years,Not Disclosed,['Mumbai'],"The SAS to Databricks Migration Developer will be responsible for migrating existing SAS code, data processes, and workflows to the Databricks platform\n\nThis role requires expertise in both SAS and Databricks, with a focus on converting SAS logic into scalable PySpark and Python code\n\nThe developer will design, implement, and optimize data pipelines, ensuring seamless integration and functionality within the Databricks environment\n\nCollaboration with various teams is essential to understand data requirements and deliver solutions that meet business needs",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['hive', 'scala', 'pyspark', 'data warehousing', 'data migration', 'azure data factory', 'sql', 'sql azure', 'java', 'spark', 'mysql', 'hadoop', 'big data', 'etl', 'python', 'sas', 'microsoft azure', 'power bi', 'machine learning', 'sql server', 'data bricks', 'migration', 'sqoop', 'aws', 'ssis']",2025-06-13 05:15:52
IT Manager - Data Engineering & Analytics,ZS,12 - 15 years,Not Disclosed,['Pune'],"IT MANAGER, DATA ENGINEERING AND ANALYTICS will lead a team of data engineers and analysts responsible for designing, developing, and maintaining robust data systems and integrations. This role is critical for ensuring the smooth collection, transformation, integration and visualization of data, making it easily accessible for analytics and decision-making across the organization. The Manager will collaborate closely with analysts, developers, business leaders and other stakeholders to ensure that the data infrastructure meets business needs and is scalable, reliable, and efficient.\n",,,,"['Data modeling', 'Project management', 'Analytical', 'Financial planning', 'Management consulting', 'Data quality', 'Troubleshooting', 'Stakeholder management', 'Analytics', 'SQL']",2025-06-13 05:15:54
IN Senior Associate GenAI S/W Engineer- Data and Analytics,PwC Service Delivery Center,1 - 7 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nData, Analytics & AI\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\n& Summary\nJob Overview\nWe are seeking a highly skilled and versatile polyglot Full Stack Developer with expertise in modern frontend and backend technologies, cloudbased solutions, AI/ML and Gen AI. The ideal candidate will have a strong foundation in fullstack development, cloud platforms (preferably Azure), and handson experience in Gen AI, AI and machine learning technologies.\nKey Responsibilities\nDevelop and maintain web applications using Angular / React.js , .NET , and Python .\nDesign, deploy, and optimize Azure native PaaS and SaaS services, including but not limited to Function Apps , Service Bus , Storage Accounts , SQL Databases , Key vaults, ADF, Data Bricks and REST APIs with Open API specifications.\nImplement security best practices for data in transit and rest. Authentication best practices SSO, OAuth 2.0 and Auth0.\nUtilize Python for developing data processing and advanced AI/ML models using libraries like pandas , NumPy , scikitlearn and Langchain , Llamaindex , Azure OpenAI SDK\nLeverage Agentic frameworks like Crew AI, Autogen etc.\nWell versed with RAG and Agentic Architecture.\nStrong in Design patterns Architectural, Data, Object oriented\nLeverage azure serverless components to build highly scalable and efficient solutions.\nCreate, integrate, and manage workflows using Power Platform , including Power Automate , Power Pages , and SharePoint .\nApply expertise in machine learning , deep learning , and Generative AI to solve complex problems.\nPrimary Skills\nProficiency in React.js , .NET , and Python .\nStrong knowledge of Azure Cloud Services , including serverless architectures and data security.\nExperience with Python Data Analytics libraries\npandas\nNumPy\nscikitlearn\nMatplotlib\nSeaborn\nExperience with Python Generative AI Frameworks\nLangchain\nLlamaIndex\nCrew AI\nAutoGen\nFamiliarity with REST API design , Swagger documentation , and authentication best practices .\nSecondary Skills\nExperience with Power Platform tools such as Power Automate, Power Pages, and SharePoint integration.\nKnowledge of Power BI for data visualization (preferred).\nPreferred Knowledge Areas Nice to have\nIndepth understanding of Machine Learning , deep learning, supervised, unsupervised algorithms.\nMandatory skill sets\nAI, ML\nPreferred skill sets\nAI, ML\nYears of experience required\n3 7 years\nEducation qualification\nBE/BTECH, ME/MTECH, MBA, MCA\nEducation\nDegrees/Field of Study required Bachelor of Technology, Master of Business Administration, Bachelor of Engineering, Master of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nGame AI\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 28 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Front end', 'Architecture', 'Data modeling', 'data security', 'Machine learning', 'data visualization', 'Apache', 'SQL', 'Python', 'Data architecture']",2025-06-13 05:15:55
Linux Kernel Engineer Senior For Data Center SoC,Qualcomm,2 - 7 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nAs a Senior Software Engineer, you will play a pivotal role in designing, developing, optimizing, and commercializing software solutions for Qualcomms next-generation data center platforms. You will collaborate closely with cross-functional teams to advance critical technologies such as virtualization, memory management, scheduling, and the Linux Kernel.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\nCollaborate within the team and across teams to design, develop, and release our software, tooling, and practices to meet community standards and internal and external requirements.\nBring up platform solutions across the Qualcomm chipset portfolio.\nTriage software build, tooling, packaging, functional, or stability failures.\nGuide and support development teams inside and outside the Linux organization, focusing on Linux userspace software functionality, integration, and maintenance.\nWork with development and product teams as necessary for issue resolution.\n\n\nPreferred Qualifications:\nMaster's Degree in Engineering, Information Systems, Computer Science, or a related field.\nStrong background in Computer Science and software fundamentals.\nWorking knowledge of C, C++, and proficiency in scripting languages (Bash, Python, etc.).\nExperience using git/gerrit.\nStrong understanding of the Linux kernel, configuration techniques like ACPI and device tree, system services, and various components that make up a Linux distribution.\nExperience with Linux distributions such as Debian, Ubuntu, RedHat, Yocto, etc.\nFamiliarity with package managers and their workings is crucial.\nFamiliarity with CI/CD tools.\nProven ability and interest in debugging complex compute and data center systems.\nStrong ability to solve problems in a non-linear fashion.\nQuick learner; able to grasp concepts with only basic training and the initiative to ask questions and investigate new areas and concepts as needed.\nPrior experience with Qualcomm software platforms is a plus.\nMature interpersonal skills with an ability to collaboratively work within the team and with many varied teams to resolve problems spanning many disciplines.\nProven ability to work in a dynamic, multi-tasked environment.\nExcellent written and verbal communication skills are required.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['computer science', 'linux', 'software engineering', 'scripting languages', 'linux kernel', 'continuous integration', 'c++', 'redhat linux', 'ci/cd', 'gerrit', 'git', 'java', 'yocto', 'embedded systems', 'debian', 'html', 'mysql', 'python', 'c', 'ubuntu', 'javascript', 'data center', 'embedded c', 'bash', 'aws']",2025-06-13 05:15:57
"Data Engineering : Sr Software Engineer, Tech Lead & Sr Tech Lead",Reflion Tech,7 - 12 years,22.5-37.5 Lacs P.A.,"['Mumbai( Ghansoli )', 'Navi Mumbai', 'Mumbai (All Areas)']","Hiring: Data Engineering Senior Software Engineer / Tech Lead / Senior Tech Lead\n\n- Hybrid (3 Days from office) | Shift: 2 PM 11 PM IST\n- Experience: 5 to 12+ years (based on role & grade)\n\nOpen Grades/Roles:\nSenior Software Engineer: 58 Years\nTech Lead: 7–10 Years\nSenior Tech Lead: 10–12+ Years\n\nJob Description – Data Engineering Team\n\nCore Responsibilities (Common to All Levels):\n\nDesign, build and optimize ETL/ELT pipelines using tools like Pentaho, Talend, or similar\nWork on traditional databases (PostgreSQL, MSSQL, Oracle) and MPP/modern systems (Vertica, Redshift, BigQuery, MongoDB)\nCollaborate cross-functionally with BI, Finance, Sales, and Marketing teams to define data needs\nParticipate in data modeling (ER/DW/Star schema), data quality checks, and data integration\nImplement solutions involving messaging systems (Kafka), REST APIs, and scheduler tools (Airflow, Autosys, Control-M)\nEnsure code versioning and documentation standards are followed (Git/Bitbucket)\n\nAdditional Responsibilities by Grade\n\nSenior Software Engineer (5–8 Yrs):\nFocus on hands-on development of ETL pipelines, data models, and data inventory\nAssist in architecture discussions and POCs\nGood to have: Tableau/Cognos, Python/Perl scripting, GCP exposure\n\nTech Lead (7–10 Yrs):\nLead mid-sized data projects and small teams\nDecide on ETL strategy (Push Down/Push Up) and performance tuning\nStrong working knowledge of orchestration tools, resource management, and agile delivery\n\nSenior Tech Lead (10–12+ Yrs):\nDrive data architecture, infrastructure decisions, and internal framework enhancements\nOversee large-scale data ingestion, profiling, and reconciliation across systems\nMentoring junior leads and owning stakeholder delivery end-to-end\nAdvantageous: Experience with AdTech/Marketing data, Hadoop ecosystem (Hive, Spark, Sqoop)\n\n- Must-Have Skills (All Levels):\n\nETL Tools: Pentaho / Talend / SSIS / Informatica\nDatabases: PostgreSQL, Oracle, MSSQL, Vertica / Redshift / BigQuery\nOrchestration: Airflow / Autosys / Control-M / JAMS\nModeling: Dimensional Modeling, ER Diagrams\nScripting: Python or Perl (Preferred)\nAgile Environment, Git-based Version Control\nStrong Communication and Documentation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'SQL', 'ETL', 'Orchestration', 'Postgresql', 'Peri', 'Informatica', 'ETL Tool', 'SSIS', 'Elt', 'Modeling', 'MongoDB', 'Data Architecture', 'Talend', 'Pentaho', 'Python']",2025-06-13 05:15:59
Senior / Lead Data Engineer,Atlas,1 - 2 years,Not Disclosed,['Pune'],"1. Designing and developing data pipelines: Lead data engineers are responsible for designing and developing data pipelines that move data from various sources to storage and processing systems.\n2. Building and maintaining data infrastructure: Lead data engineers are responsible for building and maintaining data infrastructure, such as data warehouses, data lakes, and data marts.\n3. Ensuring data quality and integrity: Lead data engineers are responsible for ensuring data quality and integrity, by setting up data validation processes and implementing data quality checks.\n4. Managing data storage and retrieval: Lead data engineers are responsible for managing data storage and retrieval, by designing and implementing data storage systems, such as NoSQL databases or Hadoop clusters.\n5. Developing and maintaining data models: Lead data engineers are responsible for developing and maintaining data models, such as data dictionaries and entity-relationship diagrams, to ensure consistency in data architecture.\n6. Managing data security and privacy: Lead data engineers are responsible for managing data security and privacy, by implementing security measures, such as access controls and encryption, to protect sensitive data.\n7. Leading and managing a team: Lead data engineers may be responsible for leading and managing a team of data engineers, providing guidance and support for their work.",Industry Type: Industrial Equipment / Machinery,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent",['Senior Lead'],2025-06-13 05:16:00
Data Streaming Engineer,Data Streaming Engineer,5 - 10 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","Hello Candidates,\n\nWe are Hiring !!\n\nJob Position - Data Streaming Engineer\nExperience - 5+ years\nLocation - Mumbai, Pune , Chennai , Bangalore\nWork mode - Hybrid ( 3 days WFO)\n\nJOB DESCRIPTION\n\nRequest for Data Streaming Engineer Data Streaming @ offshore :\n• Flink , Python Language.\n• Data Lake Systems. (OLAP Systems).\n• SQL (should be able to write complex SQL Queries)\n• Orchestration (Apache Airflow is preferred).\n• Hadoop (Spark and Hive: Optimization of Spark and Hive apps).\n• Snowflake (good to have).\n• Data Quality (good to have).\n• File Storage (S3 is good to have)\n\nNOTE - Candidates can share their resume on - shrutia.talentsketchers@gmail.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Flink', 'Apache Airflow', 'Data Quality', 'Hadoop', 'Snowflake', 'Data Lake', 'orchastration', 'Python', 'SQL']",2025-06-13 05:16:02
Data Lineage Engineers,Altimetrik,5 - 10 years,15-30 Lacs P.A.,"['Pune', 'Chennai', 'Bengaluru']","Role & responsibilities\nSkill need :  Data Lineage with Ab-initio - Metadata hub\n\nSkills & Experience:\nExpertise in mHub or similar tools, data pipelines, and cloud platforms.\nProficiency in Python, Oracle, SQL, Java, and ETL tools.\n5-10 years of experience in data engineering and governance.",,,,"['Ab-initio', 'Metadata hub', 'Python', 'mhub', 'ETL', 'Oracle', 'SQL']",2025-06-13 05:16:04
Consultant - Lead Data Engineer,Affine Analytics,5 - 8 years,Not Disclosed,['Bengaluru'],"Strong experience with Python, SQL, pySpark, AWS Glue. Good to have - Shell Scripting, Kafka\nGood knowledge of DevOps pipeline usage (Jenkins, Bitbucket, EKS, Lightspeed)\nExperience of AWS tools (AWS S3, EC2, Athena, Redshift, Glue, EMR, Lambda, RDS, Kinesis, DynamoDB, QuickSight etc.).\nOrchestration using Airflow\nGood to have - Streaming technologies and processing engines, Kinesis, Kafka, Pub/Sub and Spark Streaming\nGood debugging skills",,,,"['Python', 'RDS', 'Shell Scripting', 'Kafka', 'AWS Glue', 'DynamoDB', 'Lightspeed', 'EMR', 'EKS', 'pySpark', 'Redshift', 'SQL', 'Jenkins', 'QuickSight', 'Glue', 'EC2', 'Kinesis', 'AWS S3', 'Bitbucket', 'Athena', 'Lambda']",2025-06-13 05:16:06
Senior Data Engineer,ANS Group,2 - 6 years,Not Disclosed,['Ahmedabad'],"ANS Group is looking for Senior Data Engineer\nThe job responsibilities of a Senior Data Engineer may include:1\n\nDesigning and implementing scalable and reliable data pipelines, data models, and data infrastructure for processing large and complex datasets\n\n2\n\nDeveloping and maintaining databases, data warehouses, and data lakes that store and manage the organization's data\n\n3\n\nDeveloping and implementing data integration and ETL (Extract, Transform, Load) processes to ensure that data flows smoothly and accurately between different systems and data sources\n\n4\n\nEnsuring data quality, consistency, and accuracy through data profiling, cleansing, and validation\n\n5\n\nBuilding and maintaining data processing and analytics systems that support business intelligence, machine learning, and other data-driven applications\n\n6\n\nOptimizing the performance and scalability of data systems and infrastructure to ensure that they can handle the organization's growing data needs\n\nTo be a successful Senior Data Engineer, one must have in-depth knowledge of database architecture, data modeling, data integration, and ETL processes\n\nThey should also be proficient in programming languages such as Python, Java, or SQL and have experience working with big data technologies like Hadoop, Spark, and NoSQL databases\n\nStrong communication and leadership skills",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data warehousing', 'data pipeline', 'machine learning', 'data engineering', 'sql', 'nosql', 'database design', 'data quality', 'java', 'data modeling', 'spark', 'leadership', 'hadoop', 'etl', 'data integration', 'programming', 'data lake', 'etl process', 'communication skills', 'data profiling']",2025-06-13 05:16:08
Senior Data Engineer,Amgen Inc,9 - 12 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.",,,,"['Data engineering', 'performance tuning', 'data security', 'data processing', 'Hadoop', 'Apache Spark', 'SQL', 'CI/CD', 'troubleshooting', 'big data', 'aws', 'ETL', 'Python']",2025-06-13 05:16:10
Senior Data Engineer,Amgen Inc,3 - 8 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.",,,,"['Data Engineering', 'Git', 'PySpark', 'CI/CD', 'Databricks', 'ETL', 'NOSQL', 'AWS', 'data integration', 'SQL', 'Apache Spark', 'Python']",2025-06-13 05:16:11
Senior Data Engineer,Amgen Inc,3 - 7 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.",,,,"['Data Engineering', 'Maven', 'SparkSQL Apache Spark', 'PySpark', 'Subversion', 'OLAP', 'Scaled Agile methodologies', 'SQL', 'Scaled Agile Framework', 'Jenkins', 'NOSQL database', 'Git', 'Databricks', 'Data Fabric', 'Data Mesh', 'AWS', 'Python']",2025-06-13 05:16:13
Senior Data Engineer (Data Architect),Adastra Corp,8 - 13 years,Not Disclosed,[],"Join our innovative team and architect the future of data solutions on Azure, Synapse, and Databricks!\nSenior Data Engineer (Data Architect)\nAdditional Details:\nNotice Period: 30 days (maximum)\nLocation: Remote\nAbout the Role\nDesign and implement scalable data pipelines, data warehouses, and data lakes that drive business growth. Collaborate with stakeholders to deliver data-driven insights and shape the data landscape.\nRequirements\n8+ years of experience in data engineering and data architecture\nStrong expertise in Azure services (Synapse Analytics, Databricks, Storage, Active Directory)\nProven experience in designing and implementing data pipelines, data warehouses, and data lakes\nStrong understanding of data governance, data quality, and data security\nExperience with infrastructure design and implementation, including DevOps practices and tools",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Azure', 'DESIGN', 'Architecture', 'Synapse Analytics']",2025-06-13 05:16:14
Senior Data Engineer,Ensemble Health Partners,5 - 9 years,Not Disclosed,[],"Company Overview:\nAs Ensemble Health Partners Company, we're at the forefront of innovation, leveraging cutting-edge technology to drive meaningful impact in the Revenue Cycle Management landscape. Our future-forward technology combines tightly integrated data ingestion, workflow automation and business intelligence solutions on a modern cloud architecture. We have the second-largest share in the RCM space in the US Market with 10000+ professionals working in the organization. With 10 Technology Patents in our name, we believe the best results come from a combination of skilled and experienced team, proven and repeatable processes, and modern and flexible technologies. As a leading player in the industry, we offer an environment that fosters growth, creativity, and collaboration, where your expertise will be valued, and your contributions will make a difference.\n\nRole & responsibilities :\nExperience : 5-9 Years\nLocation : remote/wfh\n\nPosition Summary :\nDesign and maintain scalable data pipelines, manage ETL processes and data warehouses, ensure data quality and governance, collaborate with cross-functional teams, support machine learning deployment, lead projects, mentor juniors, work with big data and cloud technologies, and bring expertise in Spark, Databricks, Streaming/Reactive/Event-driven systems, Agentic programming, and LLM application development.\n\nRequired Skills :\nSpark, Databricks, Streaming/Reactive /Event driven, Agentic programming & LLM Application Experience\n5+ years of coding experience with Microsoft SQL.\n3+ years working with big data technologies including but not limited to Databricks, Apache Spark, Python, Microsoft Azure (Data Factory, Dataflows, Azure Functions, Azure Service Bus) with a willingness and ability to learn new ones\nExcellent understanding of engineering fundamentals: testing automation, code reviews, telemetry, iterative delivery and DevOps\nExperience with polyglot storage architectures including relational, columnar, key-value, graph or equivalent\nExperience with Delta tables as well as Parquet files stored in ADLS\nExperience delivering applications using componentized and distributed architectures using event driven patterns\nDemonstrated ability to communicate effectively to both technical and non-technical, globally distributed audiences\nSolid foundations in formal architecture, design patterns and best practices\nExperience working with healthcare datasets\n\nWhy Join US?\nWe adapt emerging technologies to practical uses to deliver concrete solutions that bring maximum impact to providers bottom line. We currently have 10 Technology Patents in our name.\nWe offer you a great organization to work for, where you will get to do best work of your career and grow with the team that is shaping the future of Revenue Cycle Management.\nWe have our strong focus on Learning and development. We have the best Industry standard professional development policies to support the learning goals of our associates.\nWe have flexible/ remote working/ working from home options\nBenefits\nHealth Benefits and Insurance Coverage for family and parents. Accidental Insurance for the associate.\nCompliant with all Labor Laws- Maternity benefits, Paternity Leaves.\nCompany Swags- Welcome Packages, Work Anniversary Kits\nExclusive Referral Policy\nProfessional Development Program and Reimbursements.\nRemote work flexibility to work from home.\nPlease share your resume on yash.arora@ensemblehp.com with current ctc, expected ctc, notice period.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Pyspark', 'Azure Functions', 'Azure Databricks']",2025-06-13 05:16:16
"Walk-In Senior Data Engineer - DataStage, Azure & Power BI",Net Connect,6 - 10 years,5-11 Lacs P.A.,['Hyderabad( Madhapur )'],"Greetings from NCG!\n\nWe have a opening for Snowflake Developer role in Hyderabad office!\nBelow JD for your reference\n\nJob Description:\n\nWe are hiring an experienced Senior Data Engineer with strong expertise in IBM DataStage, , and . The ideal candidate will be responsible for end-to-end data integration, transformation, and reporting solutions that drive business decisions.",,,,"['Azure Data Factory', 'Datastage', 'Etl Datastage']",2025-06-13 05:16:18
Senior Data Engineer,Amgen Inc,3 - 7 years,Not Disclosed,['Hyderabad'],"What you will do\nRole Description:\nWe are seeking a Senior Data Engineer with expertise in Graph Data technologies to join our data engineering team and contribute to the development of scalable, high-performance data pipelines and advanced data models that power next-generation applications and analytics. This role combines core data engineering skills with specialized knowledge in graph data structures, graph databases, and relationship-centric data modeling, enabling the organization to leverage connected data for deep insights, pattern detection, and advanced analytics use cases. The ideal candidate will have a strong background in data architecture, big data processing, and Graph technologies and will work closely with data scientists, analysts, architects, and business stakeholders to design and deliver graph-based data engineering solutions.",,,,"['Data Engineering', 'SPARQL', 'Maven', 'PySpark', 'GSQL', 'Subversion', 'AWS services', 'Stardog', 'Cypher', 'SAFe', 'Jenkins', 'DevOps', 'Git', 'Neo4j', 'Delta Lake', 'Graph Databases', 'Spark', 'Marklogic', 'Gremlin']",2025-06-13 05:16:19
"Senior data engineer - Python, Pyspark, AWS - 5+ years Gurgaon",One of the largest insurance providers.,5 - 10 years,Not Disclosed,['Gurugram'],"Senior data engineer - Python, Pyspark, AWS - 5+ years Gurgaon\n\nSummary: An excellent opportunity for someone having a minimum of five years of experience with expertise in building data pipelines. A person must have experience in Python, Pyspark and AWS.\n\nLocation- Gurgaon (Hybrid)\n\nYour Future Employer- One of the largest insurance providers.\n\nResponsibilities-\nTo design, develop, and maintain large-scale data pipelines that can handle large datasets from multiple sources.\nReal-time data replication and batch processing of data using distributed computing platforms like Spark, Kafka, etc.\nTo optimize the performance of data processing jobs and ensure system scalability and reliability.\nTo collaborate with DevOps teams to manage infrastructure, including cloud environments like AWS.\nTo collaborate with data scientists, analysts, and business stakeholders to develop tools and platforms that enable advanced analytics and reporting.\n\nRequirements-\nHands-on experience with AWS services such as S3, DMS, Lambda, EMR, Glue, Redshift, RDS (Postgres) Athena, Kinesics, etc.\nExpertise in data modeling and knowledge of modern file and table formats.\nProficiency in programming languages such as Python, PySpark, and SQL/PLSQL for implementing data pipelines and ETL processes.\nExperience data architecting or deploying Cloud/Virtualization solutions (Like Data Lake, EDW, Mart ) in the enterprise.\nCloud/hybrid cloud (preferably AWS) solution for data strategy for Data lake, BI and Analytics.\nWhat is in for you-\nA stimulating working environment with equal employment opportunities.\nGrowing of skills while working with industry leaders and top brands.\nA meritocratic culture with great career progression.\n\nReach us- If you feel that you are the right fit for the role please share your updated CV at randhawa.harmeen@crescendogroup.in\n\nDisclaimer- Crescendo Global specializes in Senior to C-level niche recruitment. We are passionate about empowering job seekers and employers with an engaging memorable job search and leadership hiring experience. Crescendo Global does not discriminate on the basis of race, religion, color, origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Pipeline', 'AWS', 'Data Ingestion', 'Data Engineering', 'Data Processing']",2025-06-13 05:16:21
Senior Data Engineer with Dot Net,Swits Digital,4 - 7 years,Not Disclosed,['Gurugram'],"Job Title: Senior Data Engineer with .NET\n\nExperience: 6+ Years\n\nLocation: Gurgaon, India\n\n\n\nRequired Skills:\n\n\n\n\n5+ years of experience in distributed computing (Spark) and software development\n\n\n\n3+ years of hands-on experience in Spark-Scala\n\n\n\n5+ years of experience in Data Engineering\n\n\n\n5+ years of experience in Python\n\n\n\n5+ years of experience in .NET Core\n\n\n\nExperience with Dapper, SQL, XUnit, NUnit, and RabbitMQ\n\n\n\nProficiency in working with databases (preferably Postgres)\n\n\n\nSolid understanding of Object-Oriented Programming principles\n\n\n\nExperience in Agile development environments (Scrum/Kanban)\n\n\n\nExperience with version control tools (preferably Git)\n\n\n\nCI/CD pipeline experience\n\n\n\nStrong exposure to automated testing including Integration/Delta, Load, and Performance testing\n\n\n\n\nGood to Have Skills:\n\n\n\n\nExperience with Docker and containerized deployments\n\n\n\nExperience with Kubernetes\n\n\n\nFamiliarity with Airflow\n\n\n\nExperience working in cloud environments (GCP and Azure)\n\n\n\nExposure to TeamCity CI and Octopus Deploy",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation testing', 'GIT', 'Version control', 'spark', 'Agile development', 'Performance testing', 'Deployment', 'Object oriented programming', 'SQL', 'Python']",2025-06-13 05:16:23
Senior Data Engineer - Azure,Blend360 India,6 - 11 years,Not Disclosed,['Hyderabad'],"As a Senior Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Access control', 'Data analysis', 'Team leading', 'Architecture', 'Analytical', 'Agile', 'data governance', 'Data processing', 'Mentor', 'Data quality']",2025-06-13 05:16:24
Senior Data Engineer - Azure,Blend360 India,5 - 11 years,Not Disclosed,['Hyderabad'],"As a Senior Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\n5+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar field\nMust have experience e",Industry Type: Industrial Automation,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-13 05:16:26
Data Engineer / Sr. Data Entry Specialist,Techwaukee,2 - 4 years,Not Disclosed,[],Role Purpose\nThe purpose of this role is to execute the process and drive the performance of the team on the key metrices of the process.\nJob Details\nCountry/Region: India\nEmployment Type: Remote\nWork Type: Contract,,,,"['Product management', 'ERP', 'Data modeling', 'XML', 'MySQL', 'JSON', 'Informatica', 'Information technology', 'SQL', 'Python']",2025-06-13 05:16:28
Senior Data engineer,Wissen Technology,10 - 17 years,Not Disclosed,['Pune'],"Wissen Technology is Hiring fo r Senior Data engineer\n\nAbout Wissen Technology:\nWissen Technology is a globally recognized organization known for building solid technology teams, working with major financial institutions, and delivering high-quality solutions in IT services. With a strong presence in the financial industry, we provide cutting-edge solutions to address complex business challenges\n\nRole Overview : We are looking for a highly skilled and experienced Senior Data Engineer to join our growing data engineering team in Bangalore . In this role, you will be responsible for designing, developing, and maintaining scalable and efficient data pipelines and data architectures. You will collaborate closely with cross-functional teams to ensure data accessibility, reliability, and performance to support business intelligence, analytics, and data science initiatives\n\nExperience : 9-13 Years\nLocation: Bangalore\nKey Responsibilities\nDesign , build, and maintain scalable and reliable data pipelines for ingesting, transforming, and loading structured and unstructured data from diverse sources.\nDevelop optimized SQL queries and modular scripts for data manipulation and transformation.\nCreate and maintain data models and schemas to support advanced analytics, reporting, and operational processes.\nAutomate routine data engineering tasks and data quality checks using scripting (Python, Shell, etc.) and orchestration tools.\nCollaborate with engineering, product, and data science teams to understand data requirements and deliver high-quality solutions.\nAct as a liaison between engineering and business teams to translate business needs into technical solutions.\nImplement monitoring, alerting, and troubleshooting mechanisms for data pipelines and dashboards to ensure data integrity and availability.\nDefine and implement best practices for data validation, data governance, and compliance.\nManage test data and QA environments, supporting test data management processes.\nWork in an Agile environment and contribute to continuous improvement initiatives across the data engineering landscape.\n\n\nRequired Skills and Qualification\nBachelors or masters degree in computer science , Information Systems, or a related field.\n9+ years of professional experience in data engineering or a related field.\nStrong expertise in SQL development and performance tuning for both RDBMS and cloud-based databases.\nHands-on experience with cloud platforms, particularly AWS (e.g., S3, Glue, Lambda, Redshift, EMR).\nExperience with modern data warehouse technologies like Snowflake and Amazon Redshift .\nStrong background in data modeling , ETL/ELT architecture, and data warehousing concepts.\nProficiency in programming languages such as Python , Scala , or Java .\nExperience working with",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Telecom', 'Performance tuning', 'Manager Quality Assurance', 'Data management', 'Consulting', 'Manager Technology', 'Healthcare', 'Business intelligence', 'Monitoring', 'Python']",2025-06-13 05:16:30
Sr. Data Engineer - Snowflake,Freelancer Monika,5 - 10 years,15-25 Lacs P.A.,['Pune'],"Role & responsibilities\nDesigned and implemented end-to-end data pipeline using DBT, Snowflake  \nCreated and structure DBT models like staging, transformation, marts, YAML configurations for models and tests, dbt  seeds.  \n\nHands-on experience on DBT Jinja templating, macro development, dbt jobs and snapshot management for Slowly  changing dimensions.  \n\nDevelop python script for data cleaning, transformation and automation of repetitive task.  \nExperienced in loading structured and semi-structured data from AWS S3 to Snowflake by designing file formats,  \nconfiguring storage integration, and automating data loads using Snow pipe.  \nDesigned scalable incremental models for handling large datasets, reducing resource usage\n\nPreferred candidate profile\nCandidate must have 5+ Yrs experience.\nEarly joiner, who can join within a month",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'DBT', 'Data Engineer', 'Snowflake Sql', 'Snow Flake Schema', 'Data Build Tool', 'Snowflake Db', 'ETL', 'SQL']",2025-06-13 05:16:31
Senior Azure Data Engineer,Cloud Angles Digital Transformation,8 - 12 years,Not Disclosed,['Hyderabad'],"Job Summary:\nWe are seeking a highly skilled Data Engineer with expertise in leveraging Data Lake architecture and the Azure cloud platform to develop, deploy, and optimise data-driven solutions. . You will play a pivotal role in transforming raw data into actionable insights, supporting strategic decision-making across the organisation.\nResponsibilities\nDesign and implement scalable data science solutions using Azure Data Lake, Azure Data Bricks, Azure Data Factory and related Azure services.\nDevelop, train, and deploy machine learning models to address business challenges.\nCollaborate with data engineering teams to optimise data pipelines and ensure seamless data integration within Azure cloud infrastructure.\nConduct exploratory data analysis (EDA) to identify trends, patterns, and insights.\nBuild predictive and prescriptive models to support decision-making processes.\nExpertise in developing end-to-end Machine learning lifecycle utilizing crisp-DM which includes of data collection, cleansing, visualization, preprocessing, model development, model validation and model retraining\nProficient in building and implementing RAG systems that enhance the accuracy and relevance of model outputs by integrating retrieval mechanisms with generative models.\nEnsure data security, compliance, and governance within the Azure cloud ecosystem.\nMonitor and optimise model performance and scalability in production environments.\nPrepare clear and concise documentation for developed models and workflows.\nSkills Required:\nGood experience in using Pyspark, Python, MLops (Optional), ML flow (Optional), Azure Data Lake Storage. Unity Catalog\nWorked and utilized data from various RDBMS like MYSQL, SQL Server, Postgres and NoSQL databases like MongoDB, Cassandra, Redis and graph DB like Neo4j, Grakn.\nProven experience as a Data Engineer with a strong focus on Azure cloud platform and Data Lake architecture.\nProficiency in Python, Pyspark,\nHands-on experience with Azure services such as Azure Data Lake, Azure Synapse Analytics, Azure Machine Learning, Azure Databricks, and Azure Functions.\nStrong knowledge of SQL and experience in querying large datasets from Data Lakes.\nFamiliarity with data engineering tools and frameworks for data ingestion and transformation in Azure.\nExperience with version control systems (e.g., Git) and CI/CD pipelines for machine learning projects.\nExcellent problem-solving skills and the ability to work collaboratively in a team environment.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Data Engineering', 'Azure Databricks', 'Pyspark', 'Azure Data Lake', 'Python']",2025-06-13 05:16:33
Sr Data Engineering Manager,Amgen Inc,12 - 15 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nWe are seeking a Senior Data Engineering Manager with a strong background in Regulatory or Integrated Product Teams within the Biotech or Pharmaceutical domain. This role will lead the end-to-end data strategy and execution for regulatory product submissions, lifecycle management, and compliance reporting, ensuring timely and accurate delivery of regulatory data assets across global markets.You will be embedded in a cross-functional Regulatory Integrated Product Team (IPT) and serve as the data and technology lead, driving integration between scientific, regulatory, and engineering functions to support submission-ready data and regulatory intelligence solutions.",,,,"['Data Engineering', 'engineering strategy', 'DevOps', 'Project Management', 'DataOps', 'Agile', 'data strategy']",2025-06-13 05:16:35
"Sr. Data Engineer, R&D Data Catalyst Team",Amgen Inc,7 - 9 years,Not Disclosed,['Hyderabad'],"The R&D Data Catalyst Team is responsible for buildingData Searching, Cohort Building, and Knowledge Management tools that provide the Amgen scientific community with visibility to Amgens wealth of human datasets, projects and study histories, and knowledge over various scientific findings. These solutions are pivotal tools in Amgens goal to accelerate the speed of discovery, and speed to market of advanced precision medications.\nThe Data Engineer will be responsible for the end-to-end development of an enterprise analytics and data mastering solution leveraging Databricks and Power BI. This role requiresexpertise in both data architecture and analytics, with the ability to create scalable, reliable, and high-performing",,,,"['Data Engineering', 'data analysis', 'ETL processes', 'DAX', 'Business Objects', 'data warehouse design', 'ETL', 'PowerBI Models', 'AWS', 'Power Query']",2025-06-13 05:16:37
Senior Data Engineer,Keeptruckin,6 - 11 years,Not Disclosed,[],"Who we are:\nMotive empowers the people who run physical operations with tools to make their work safer, more productive, and more profitable. For the first time ever, safety, operations and finance teams can manage their drivers, vehicles, equipment, and fleet related spend in a single system. Combined with industry leading AI, the Motive platform gives you complete visibility and control, and significantly reduces manual workloads by automating and simplifying tasks.\nMotive serves more than 120,000 customers - from Fortune 500 enterprises to small businesses - across a wide range of industries, including transportation and logistics, construction, energy, field service, manufacturing, agriculture, food and beverage, retail, and the public sector.\nVisit gomotive.com to learn more.\nAbout the Role:\nAs a Senior Data Engineer you will be part of the core data team building out world class data models and pipelines that will feed into our products. You will be working on the intersection of all data streams in the company from our IOT data consuming 100s of thousands of data points per minute to our user data. You will partner closely with both the Product, Engineering, as well as the Strategic Analytics teams. We are seeking strong team players who thrive on innovation and continuous improvement. We pride ourselves on our culture, and ability to work effectively across a highly diversified team.\nWhat Youll Do:\nBuild data pipelines based on product data which will go into our product for Enterprise customers\nArchitect and design data models in collaboration with data and product teams\nCommunicate effectively across multiple teams and projects.\nActively work on deploying Data Ops into Motive, driving the most robust data models using the latest tools in the market\nYou will be working with airflow, aws, great expectations and table creation frameworks similar to dbt.\nWhat Were Looking For:\nBachelors degree or higher in a quantitative field, e.g. Computer Science, Math, Economics, or Statistics\n6+ years experience in Data Engineering, including experience building modeled tables\nExpertise with data engineering stack, dBt, Snowflake, airflow, data observability tools, AWS.\nExpertise in SQL and Python\nWillingness to learn new technologies\nSolid communication, collaboration, and people skills\nCreating a diverse and inclusive workplace is one of Motives core values. We are an equal opportunity employer and welcome people of different backgrounds, experiences, abilities and perspectives.\nPlease review our Candidate Privacy Notice here .\nUK Candidate Privacy Notice here .\nThe applicant must be authorized to receive and access those commodities and technologies controlled under U.S. Export Administration Regulations. It is Motives policy to require that employees be authorized to receive access to Motive products and technology.\n#LI-Remote",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Product engineering', 'Manager Technology', 'Continuous improvement', 'AWS', 'Analytics', 'Team building', 'SQL', 'Python', 'Logistics']",2025-06-13 05:16:38
Senior Data Engineer (Identity),Kargo,6 - 11 years,Not Disclosed,[],"Success takes all kinds. Diversity describes our workforce. Inclusion defines our culture. We do not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, marital status, age, national origin, protected veteran status, disability or other legally protected status. Individuals with disabilities are provided reasonable accommodation to participate in the job application process, perform essential job functions, and receive other benefits and privileges of employment.\nTitle: Senior Data Engineer\nJob Type: Permanent\n\nJob Location: Remote\nThe Opportunity\nAt Kargo, we are rapidly evolving our data infrastructure and capabilities to address challenges of data scale, new methodologies for onboarding and targeting, and rigorous privacy standards. Were looking for an experienced Senior Data Engineer to join our team, focusing on hands-on implementation, creative problem-solving, and exploring new technical approaches. Youll work collaboratively with our technical leads and peers, actively enhancing and scaling the data processes that drive powerful targeting systems.\nThe Daily To-Do\nIndependently implement, optimize, and maintain robust ETL/ELT pipelines using Python, Airflow, Spark, Iceberg, Snowflake, Aerospike, Docker, Kubernetes (EKS), AWS, and real-time streaming technologies like Kafka and Flink.\nEngage proactively in collaborative design and brainstorming sessions, contributing technical insights and innovative ideas for solving complex data engineering challenges.\nSupport the definition and implementation of robust testing strategies, and guide the team in adopting disciplined CI/CD practices using ArgoCD to enable efficient and reliable deployments.\nMonitor and optimize data systems and infrastructure to ensure operational reliability, performance efficiency, and cost-effectiveness.\nActively contribute to onboarding new datasets, enhancing targeting capabilities, and exploring modern privacy-compliant methodologies.\nMaintain thorough documentation of technical implementations, operational procedures, and best practices for effective knowledge sharing and onboarding.\nQualifications:\nStrong expertise in implementing, maintaining, and optimizing large-scale data systems with minimal oversight.\nDeep proficiency in Python, Spark, and Iceberg, with a clear understanding of data structuring for efficiency and performance.\nExperience with Airflow for building robust data workflows is strongly preferred.\nExtensive DevOps experience, particularly with AWS (including EKS), Docker, Kubernetes, CI/CD automation using ArgoCD, and monitoring via Prometheus.\nFamiliarity with Snowflake, including writing and optimizing SQL queries and understanding Snowflakes performance and cost dynamics.\nComfort with Agile methodologies, including regular use of Jira and Confluence for task management and documentation.\nProven ability to independently drive implementation and problem-solving, turning ambiguity into clearly defined actions.\nExcellent communication skills to effectively engage in discussions with technical teams and stakeholders.\nFamiliarity with identity, privacy, and targeting methodologies in AdTech is required.\nFollow Our Lead\nBig Picture: kargo.com\nThe Latest: Instagram ( @kargomobile ) and LinkedIn ( Kargo )",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL queries', 'Automation', 'CTV', 'spark', 'Agile', 'JIRA', 'Operations', 'Cost', 'AWS', 'Python']",2025-06-13 05:16:40
"Senior Data Engineer II, Business Intelligence & Reporting",XL India Business Services Pvt. Ltd,3 - 7 years,Not Disclosed,['Gurugram'],"Senior Data Engineer II, Business Intelligence & Reporting Gurgaon, Haryana, India AXA XL recognizes digital, data, and information assets are critical for the business, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained dynamic advantage\n\nOur Data, Intelligence & Analytics function is focused on driving innovation by optimizing how we leverage digital, data, and AI to drive strategy and differentiate ourselves from the competition\n\nAs we develop an enterprise-wide data and digital strategy that moves us toward a greater focus on the use of data and strengthening our digital, AI capabilities, we are seeking a Deputy Manager, BI and Reporting\n\nIn this role, you will support/manage BI & reporting\n\nWhat you ll be DOING Your essential responsibilities include: BI & Reporting Management: Oversee and support Business Intelligence (BI) and Reporting products, ensuring their effectiveness and alignment with organizational goals\n\nStakeholder Engagement: Manage Business as Usual (BAU) activities for BI and Reporting, fostering effective communication and relationships with stakeholders to understand their needs and expectations\n\nModel Integration: Energize and synergize various Business Intelligence models and reporting systems to enhance data insights and reporting capabilities\n\nStrategic Initiative Support: Collaborate with the Data Intelligence and Analytics (DIA) team on various strategic initiatives, enabling the development of BI and Reporting functions and related capabilities\n\nTalent Development: Foster the growth of BI and Reporting talent across AXA XL by promoting an inclusive and diverse environment that encourages the utilization and value creation of our strategic digital, data, and analytics assets\n\nCustomer-Centric Culture: Instill a customer-first mindset within the team, prioritizing exceptional service for business stakeholders and ensuring their needs are met\n\nTeam Development: Contribute to the enhancement of the Business Intelligence teams tools, skills, and culture, driving positive impacts on team performance and outcomes\n\nYou will report to Senior Manager, Business Intelligence & Reporting\n\nWhat you will BRING At AXA XL, we view individuals holistically through their People, Business, and Technical Skills\n\nWe re interested in what you bring, how you think, and your potential for growth\n\nWe value diverse backgrounds and perspectives, recognizing that each person contributes uniquely to our teams success\n\nWe value relevant education and experience in a related field\n\nAdditionally, we encourage candidates with diverse educational backgrounds or equivalent experience to apply\n\nHere are some of the key skills important for the role: People Skills Customer Centricity: Brings a collaborative spirit, a can-do attitude, and a Customer First mindset, ensuring that stakeholder needs are prioritized\n\nCross-Functional Collaboration: Ability to communicate effectively with teams, peers, and stakeholders across the globe, fostering collaboration and understanding\n\nAble to help and guide team members on technical issues, fostering their development and promoting self-directed problem-solving\n\nGrowth Mindset: Passion for digital, data, and AI, along with a commitment to personal and team development in a digital and data-driven organization\n\nResilience: Ability to lead a project or team, demonstrating adaptability and leadership under various circumstances\n\nAnalytical & Strategic Mindset: Ability to analyze data effectively and develop strategic insights that drive decision-making and improve business outcomes\n\nPerformance Excellence: Commitment to delivering high-quality results and continuously improving processes and performance metrics within the team\n\nBUSINESS Skills Business & Insurance Acumen: Ability to showcase relevant industry knowledge supporting multiple specialty areas of Data and Analytics\n\nStakeholder Management: Ability to manage stakeholders effectively, understanding their needs and ensuring clear communication and support\n\nSimplifies Complexity: Ability to distill complex data concepts and analyses into clear, actionable insights for stakeholders\n\nEnsuring that technical information is accessible, enabling informed decision-making and fostering collaboration between technical and non-technical teams\n\nTECHNICAL Skills Data Visualization: Experience with end-user BI tools like Power BI, enabling effective presentation and visualization of data insights\n\nReporting Tools: Proficincy in SQL, Advanced Excel, MS Access, and VBA, allowing for effective data manipulation and reporting\n\nData Analytics: Ability to help and guide team members on technical issues, fostering skill development within the team to self-directedly manage data analytics tasks",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Stakeholder Engagement', 'MS Access', 'Analytical', 'Agile', 'digital strategy', 'Business strategy', 'data visualization', 'Stakeholder management', 'Reporting tools', 'SQL']",2025-06-13 05:16:42
Senior Data Engineer - AWS,Blend360 India,6 - 10 years,Not Disclosed,['Hyderabad'],"We are looking for an experienced Senior Data Engineer with a strong foundation in Python, SQL, and Spark , and hands-on expertise in AWS, Databricks . In this role, you will build and maintain scalable data pipelines and architecture to support analytics, data science, and business intelligence initiatives. You ll work closely with cross-functional teams to drive data reliability, quality, and performance.\nResponsibilities:\nDesign, develop, and optimize scalable data pipelines using Databricks in AWS such as Glue, S3, Lambda, EMR, Databricks notebooks, workflows and jobs.\nBuilding data lake in WS Databricks.\nBuild and maintain robust ETL/ELT workflows using Python and SQL to handle structured and semi-structured data.\nDevelop distributed data processing solutions using Apache Spark or PySpark .\nPartner with data scientists and analysts to provide high-quality, accessible, and well-structured data.\nEnsure data quality, governance, security, and compliance across pipelines and data stores.\nMonitor, troubleshoot, and improve the performance of data systems and pipelines.\nParticipate in code reviews and help establish engineering best practices.\nMentor junior data engineers and support their technical development.\n\n\nRequirements\nBachelors or masters degree in computer science, Engineering, or a related field.\n5+ years of hands-on experience in data engineering , with at lea",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Version control', 'GIT', 'Workflow', 'Data quality', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-13 05:16:44
Senior Data Engineer - Azure,Blend360 India,6 - 11 years,Not Disclosed,['Hyderabad'],"As a Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\n3+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar field\nMust have experience e",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-13 05:16:45
Senior Data Engineer,Neal Analytics,10 - 15 years,Not Disclosed,['Mumbai'],"Its fun to work in a company where people truly BELIEVE in what they are doing!\nWere committed to bringing passion and customer focus to the business.\nJob Description:\nAs a Backend (Java) Engineer, you would be part of the team consisting of Scrum Master, Cloud Engineers, AI/ML Engineers, and UI/UX Engineers to build end-to-end Data to Decision Systems.\nMandatory:\n8+ years of demonstrable experience designing, building, and working as a Java Developer for enterprise web applications\nIdeally, this would include the following:\no Expert-level proficiency with Java\no Expert-level proficiency with SpringBoot\nFamiliarity with common databases (RDBMS such as MySQL & NoSQL such as MongoDB) and data warehousing concepts (OLAP, OLTP)\nUnderstanding of REST concepts and building/interacting with REST APIs\nDeep understanding of core backend concepts:\no Develop and design RESTful services and APIs\no Develop functional databases, applications, and servers to support websites on the back end\no Performance optimization and multithreading concepts\no Experience with deploying and maintaining high traffic infrastructure (performance testing is a plus)\nIn addition, the ideal candidate would have great problem-solving skills, and familiarity with code versioning tools such as GitHub\nGood to have:\nFamiliarity with Microsoft Azure Cloud Services (particularly Azure Web App, Storage and VM), or familiarity with AWS (EC2 containers) or GCP Services.\nExperience with Microservices, Messaging Brokers (e.g., RabbitMQ)\nExperience with fine-tuning reverse proxy engines such as Nginx, Apache HTTPD\nIf you like wild growth and working with happy, enthusiastic over-achievers, youll enjoy your career with us!\nNot the right fit? Let us know youre interested in a future opportunity by clicking Introduce Yourself in the top-right corner of the page or create an account to set up email alerts as new job postings become available that meet your interest!",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Backend', 'Multithreading', 'RDBMS', 'MySQL', 'Performance testing', 'OLAP', 'Scrum', 'MongoDB', 'Apache', 'OLTP']",2025-06-13 05:16:47
Senior Data Engineer : 7+ Years,Jayam Solutions Pvt Ltd - CMMI Level III Company,5 - 9 years,Not Disclosed,['Hyderabad( Madhapur )'],"Job Description:\nPosition: Sr.Data Engineer\nExperience: Minimum 7 years\nLocation: Hyderabad\nJob Summary:\n\nWhat Youll Do\n\nDesign and build efficient, reusable, and reliable data architecture leveraging technologies like Apache Flink, Spark, Beam and Redis to support large-scale, real-time, and batch data processing.\nParticipate in architecture and system design discussions, ensuring alignment with business objectives and technology strategy, and advocating for best practices in distributed data systems.\nIndependently perform hands-on development and coding of data applications and pipelines using Java, Scala, and Python, including unit testing and code reviews.\nMonitor key product and data pipeline metrics, identify root causes of anomalies, and provide actionable insights to senior management on data and business health.\nMaintain and optimize existing datalake infrastructure, lead migrations to lakehouse architectures, and automate deployment of data pipelines and machine learning feature engineering requests.\nAcquire and integrate data from primary and secondary sources, maintaining robust databases and data systems to support operational and exploratory analytics.\nEngage with internal stakeholders (business teams, product owners, data scientists) to define priorities, refine processes, and act as a point of contact for resolving stakeholder issues.\nDrive continuous improvement by establishing and promoting technical standards, enhancing productivity, monitoring, tooling, and adopting industry best practices.\n\nWhat Youll Bring\n\nBachelors degree or higher in Computer Science, Engineering, or a quantitative discipline, or equivalent professional experience demonstrating exceptional ability.\n7+ years of work experience in data engineering and platform engineering, with a proven track record in designing and building scalable data architectures.\nExtensive hands-on experience with modern data stacks, including datalake, lakehouse, streaming data (Flink, Spark), and AWS or equivalent cloud platforms.\nCloud - AWS\nApache Flink/Spark , Redis\nDatabase platform- Databricks.\nProficiency in programming languages such as Java, Scala, and Python(Good to have) for data engineering and pipeline development.\nExpertise in distributed data processing and caching technologies, including Apache Flink, Spark, and Redis.\nExperience with workflow orchestration, automation, and DevOps tools (Kubernetes,git,Terraform, CI/CD).\nAbility to perform under pressure, managing competing demands and tight deadlines while maintaining high-quality deliverables.\nStrong passion and curiosity for data, with a commitment to data-driven decision making and continuous learning.\nExceptional attention to detail and professionalism in report and dashboard creation.\nExcellent team player, able to collaborate across diverse functional groups and communicate complex technical concepts clearly.\nOutstanding verbal and written communication skills to effectively manage and articulate the health and integrity of data and systems to stakeholders.\n\nPlease feel free to contact us: 9440806850\nEmail ID : careers@jayamsolutions.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Apache Flink', 'Redis', 'Spark', 'Python', 'SCALA', 'Ci/Cd', 'Devops', 'AWS']",2025-06-13 05:16:49
Senior Data Engineer,Conversehr Business Solutions,7 - 12 years,30-45 Lacs P.A.,['Hyderabad'],"What is the Data team responsible for?\nAs a Senior Data Engineer, youll be a key member of the Data & AI team. This team is responsible for designing and delivering data engineering, analytics, and generative AI solutions that drive meaningful business impact. Were looking for a pragmatic, results-driven problem solver who thrives in a fast-paced environment and is passionate about building solutions on a scale #MID_SENIOR_LEVEL\nThe ideal candidate has a strong technical foundation, a collaborative mindset, and the ability to navigate complex challenges. You should be comfortable working in a fast-moving, startup-like environment within an established enterprise, and should bring strong skill sets to adapt to new solutions fast. You will play a pivotal role in optimizing data infrastructure, enabling data-driven decision-making and integrating AI across the organization.\nWhat is the Lead Software Engineer (Senior Data Engineer) responsible for?\nServe as a hands-on technical lead, driving project execution and delivery in our growing data team based in the Hyderabad office.\nCollaborate closely with the U.S.-based team and cross-functional stakeholders to understand business needs and deliver scalable solutions.\nLead the initiative to build firmwide data models and master data management solutions for structured data (in Snowflake) and manage unstructured data using vector embeddings.\nBuild, maintain, and optimize robust data pipelines and frameworks to support business intelligence and operational workflows.\nDevelop dashboards and data visualizations that support strategic business decisions.\nStay current with emerging trends in data engineering and help implement best practices within the team.\nMentor and support junior engineers, fostering a culture of learning and technical excellence.\nWhat ideal qualifications, skills & experience would help someone to be successful?\nBachelors or master’s degree in computer science, data science, engineering, or a related field.\n7+ years of experience in data engineering including 3+ years in a technical leadership role.\nStrong SQL skills and hands-on experience with modern data pipeline technologies (e.g., Spark, Flink).\nDeep expertise in the Snowflake ecosystem, including data modeling, data warehousing, and master data management.\nProficiency in at least one programming language - Python preferred.\nExperience with Tableau and Alteryx is a plus.\nSelf-starter with a passion for learning new tools and technologies.\nStrong communication skills and a collaborative, ownership-driven mindset.\nWork Shift Timings - 2:00 PM - 11:00 PM IST",Industry Type: Investment Banking / Venture Capital / Private Equity,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'Snowflake', 'Python', 'flink', 'Data Pipeline', 'Spark', 'SQL']",2025-06-13 05:16:51
Senior Data Engineer,Jeavio,5 - 10 years,Not Disclosed,[],"We are seeking an experienced Senior Data Engineer to join our team. The ideal candidate will have a strong background in data engineering and AWS infrastructure, with hands-on experience in building and maintaining data pipelines and the necessary infrastructure components. The role will involve using a mix of data engineering tools and AWS services to design, build, and optimize data architecture.\n\nKey Responsibilities:\nDesign, develop, and maintain data pipelines using Airflow and AWS services.\nImplement and manage data warehousing solutions with Databricks and PostgreSQL.\nAutomate tasks using GIT / Jenkins.\nDevelop and optimize ETL processes, leveraging AWS services like S3, Lambda, AppFlow, and DMS.\nCreate and maintain visual dashboards and reports using Looker.\nCollaborate with cross-functional teams to ensure smooth integration of infrastructure components.\nEnsure the scalability, reliability, and performance of data platforms.\nWork with Jenkins for infrastructure automation.\n\nTechnical and functional areas of expertise:\nWorking as a senior individual contributor on a data intensive project\nStrong experience in building high performance, resilient & secure data processing pipelines preferably using Python based stack.\nExtensive experience in building data intensive applications with a deep understanding of querying and modeling with relational databases preferably on time-series data.\nIntermediate proficiency in AWS services (S3, Airflow)\nProficiency in Python and PySpark\nProficiency with ThoughtSpot or Databricks.\nIntermediate proficiency in database scripting (SQL)\nBasic experience with Jenkins for task automation\n\nNice to Have :\nIntermediate proficiency in data analytics tools (Power BI / Tableau / Looker / ThoughSpot)\nExperience working with AWS Lambda, Glue, AppFlow, and other AWS transfer services.\nExposure to PySpark and data automation tools like Jenkins or CircleCI.\nFamiliarity with Terraform for infrastructure-as-code.\nExperience in data quality testing to ensure the accuracy and reliability of data pipelines.\nProven experience working directly with U.S. client stakeholders.\nAbility to work independently and take the lead on tasks.\n\nEducation and experience:\nBachelors or masters in computer science or related fields.\n5+ years of experience\n\nStack/Skills needed:\nDatabricks\nPostgreSQL\nPython & Pyspark\nAWS Stack\nPower BI / Tableau / Looker / ThoughSpot\nFamiliarity with GIT and/or CI/CD tools",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'AWS', 'Data Bricks', 'Python', 'Etl Pipelines', 'Airflow', 'Database Scripting', 'Postgresql', 'Looker', 'SQL']",2025-06-13 05:16:52
Data Engineer sr Associate,Product base company,8 - 13 years,Not Disclosed,['Hyderabad( Gachibowli )'],"Bachelors degree in computer science, engineering, or a related field. Master’s degree preferred.\nData: 5+ years of experience with data analytics and data warehousing. Sound knowledge of data warehousing concepts.\nSQL: 5+ years of hands-on experience on SQL and query optimization for data pipelines.\nELT/ETL: 5+ years of experience in Informatica/ 3+ years of experience in IICS/IDMC\nMigration Experience: Experience Informatica on prem to IICS/IDMC migration\nCloud: 5+ years’ experience working in AWS cloud environment\nPython: 5+ years of hands-on experience of development with Python\nWorkflow: 4+ years of experience in orchestration and scheduling tools (e.g. Apache Airflow)\nAdvanced Data Processing: Experience using data processing technologies such as Apache Spark or Kafka\nTroubleshooting: Experience with troubleshooting and root cause analysis to determine and remediate potential issues\nCommunication: Excellent communication, problem-solving and organizational and analytical skills\nAble to work independently and to provide leadership to small teams of developers.\nReporting: Experience with data reporting (e.g. MicroStrategy, Tableau, Looker) and data cataloging tools (e.g. Alation)\nExperience in Design and Implementation of ETL solutions with effective design and optimized performance, ETL Development with industry standard recommendations for jobs recovery, fail over, logging, alerting mechanisms. Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Airflow', 'Unix', 'IICS', 'Informatica', 'Python', 'Informatica Cloud', 'Data Engineer', 'AWS', 'SQL']",2025-06-13 05:16:54
Senior AWS Data Engineer,Exavalu,7 - 12 years,Not Disclosed,[],"7-12 years of experience using AWS Data Landscape and data Ingestion pipeline.\nAble to understand and explain the data ingestion from different sources like file, database, applications etc\nBuild and enhance the Python, PySpark Based Framework for ingestion.Data engineering experience in AWS Data Services like Glue, EMR, Airflow, CloudWatch, Lambda, Step functions, Event triggers.\nAble to work as a senior engineer with sole interaction point with different business functional teams.\nRequirements\n7 to 12 years of experience in ETL and Data Engineering roles.\nAWS Glue, PySpark, and Amazon Redshift.\nStrong command of SQL and procedural programming in cloud or enterprise databases.\nDeep understanding of data warehousing concepts and data modeling.\nProven ability to deliver efficient, we'll-documented, and scalable data pipelines on AWS.\nFamiliarity with Airflow, AWS Lambda, and other orchestration tools is a plus.\nAWS Certification (eg, AWS Data Analytics Specialty) is an advantage.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data services', 'Data modeling', 'amazon redshift', 'Database', 'Programming', 'Data analytics', 'AWS', 'Data warehousing', 'SQL', 'Python']",2025-06-13 05:16:55
Senior Databricks Data Engineer,"Creative Capsule, LLC",5 - 10 years,Not Disclosed,['Panaji'],"This position will primarily be responsible for designing, developing, and maintaining robust ETL/ELT pipelines for data ingestion, transformation, and storage. The role also involves designing and developing scalable data solutions.\nYou will work with a team responsible for ensuring the availability, reliability, and performance of data systems and requires a good understanding of infrastructure management, cost optimization, and performance tuning in Databricks environments. The candidate will design, develop, and maintain scalable data pipelines of Databricks on cloud platforms (Databricks, Azure, AWS, or GCP).\nThe person will also work with a collaborative team responsible for driving client performance by combining data-driven insights and strategic thinking to solve business challenges. The candidate should have strong organizational, critical thinking, and communication skills to interact effectively with stakeholders.\nResponsibilities:\nDesign, develop, and manage end-to-end data pipelines on Databricks using Spark, Delta Lake, and related technologies\nImplement and optimize ETL/ELT workflows for ingesting, transforming, and storing large volumes of structured and semi-structured data\nManage and monitor Databricks infrastructure, including cluster configuration, auto-scaling, job execution, and resource utilization\nIdentify and implement cost-saving strategies across Databricks workspaces through efficient pipeline design, job scheduling, and infrastructure tuning\nOrchestrate workflows using tools like Apache Airflow, Azure Data Factory, or similar orchestration frameworks\nCollaborate with data architects and platform engineers to ensure efficient data architecture and performance tuning\nMonitor data quality, integrity, and lineage across all pipelines and proactively troubleshoot data issues\nDevelop, maintain, and optimize data models and storage layers that support BI/reporting and analytics use cases\nPartner with business stakeholders to translate business needs into technical specifications and scalable data solutions\nMaintain comprehensive documentation of pipeline architecture, configurations, and operational best practices\nTechnical Qualifications:\nExperience in infrastructure management on Databricks: cluster setup, scaling, security roles, workspace configuration\nExperience in cost optimization techniques in Databricks (e.g., job cluster vs. all-purpose cluster usage, cost/performance tradeoffs)\nExperience with orchestration tools such as Apache Airflow, Azure Data Factory, or AWS Glue Workflows\nExperience with relational (e.g., PostgreSQL, SQL Server) and NoSQL (e.g., MongoDB) databases\nStrong proficiency in Apache Spark and Delta Lake within the Databricks ecosystem\nProficient in Python and SQL for data processing and pipeline development\nFamiliarity with data warehousing, data lakes, and distributed data processing frameworks\nUnderstanding of BI tools like Power BI, Tableau, or Looker is a plus\nPersonal Skills:\nStrong analytical skills: ability to read business requirements, analyze problems, and propose solutions\nAbility to identify alternatives and find optimal solutions\nAbility to follow through and ensure logical implementation\nQuick learner with the ability to adapt to new concepts and software\nAbility to work effectively in a team environment\nStrong time management skills, capable of handling multiple tasks and competing deadlines\nEffective written and verbal communication skills\nEducation and Work Experience:\nBackground in Computer Science, Information Technology, Data Science, or a related field preferred\nMinimum 5 years of experience in Data Engineering with at least 2 years of hands-on experience with Databricks (Azure, AWS, or GCP)\nCertification in Databricks, Azure Data Engineering, or any related data technology is an added advantage",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Performance tuning', 'Infrastructure management', 'Postgresql', 'Scheduling', 'Data quality', 'Information technology', 'Analytics', 'SQL', 'Python']",2025-06-13 05:16:57
Data Engineer - GCP,Happiest Minds Technologies,5 - 10 years,8-18 Lacs P.A.,"['Pune', 'Bengaluru']","Key Responsibilities:\n• Data Pipeline Development: Designing, building, and maintaining robust data pipelines to move data from various sources (e.g., databases, external APIs, logs) to centralized data systems, such as data lakes or warehouses.\n• Data Integration: Integrating data from multiple sources and ensuring it's processed in a consistent, usable format. This involves transforming, cleaning, and validating data to meet the needs of products, analysts and data scientists.\n• Database Management: Creating, managing, and optimizing databases for storing large amounts of structured and unstructured data. Ensuring high availability, scalability, and security of data storage solutions.\nIdentifying and resolving issues related to the speed and efficiency of data systems. This could include optimizing queries, storage systems, and improving overall system architecture.\n• : Automating routine tasks, such as data extraction, transformation, and loading (ETL), to ensure smooth data flows with minimal manual intervention.\n• : Working closely with Work closely with product managers, UX/UI designers, and other stakeholders to understand data requirements and ensure data is in the right format for analysis and modeling.\n• : Ensuring data integrity and compliance with data governance policies, including data quality standards, privacy regulations (e.g., GDPR), and security protocols.\n: Continuously monitoring data pipelines and databases for any disruptions or errors and troubleshooting any issues that arise to ensure continuous data flow.\n• : Staying up to date with emerging data tools, technologies, and best practices in order to improve data systems and infrastructure.\n• : Documenting data systems, pipeline processes, and data architectures, providing clear instructions for the team to follow, and ensuring that the architecture is understandable for stakeholders.",,,,"['Data Engineering', 'gcp', 'Python', 'SQL', 'Pyspark', 'Bigquery', 'Google Cloud Platforms']",2025-06-13 05:16:59
Azure Data Engineer,JRD Systems,7 - 12 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","Cloud Data Engineer\n\nThe Cloud Data Engineer will be responsible for developing the data lake platform and all applications on Azure cloud. Proficiency in data engineering, data modeling, SQL, and Python programming is essential. The Data Engineer will provide design and development solutions for applications in the cloud.\nEssential Job Functions:\nUnderstand requirements and collaborate with the team to design and deliver projects.\nDesign and implement data lake house projects within Azure.\nDevelop application lifecycle utilizing Microsoft Azure technologies.\nParticipate in design, planning, and necessary documentation.\nEngage in Agile ceremonies including daily standups, scrum, retrospectives, demos, and code reviews.\nHands-on experience with Python/SQL development and Azure data pipelines.\nCollaborate with the team to develop and deliver cross-functional products.\nKey Skills:\na. Data Engineering and SQL\nb. Python\nc. PySpark\nd. Azure Data Lake and ADF\ne. Databricks\nf. CI/CD\ng. Strong communication\nOther Responsibilities:\nDocument and maintain project artifacts.\nMaintain comprehensive knowledge of industry standards, methodologies, processes, and best practices.\nComplete training as required for Privacy, Code of Conduct, etc.\nPromptly report any known or suspected loss, theft, or unauthorized disclosure or use of PI to the General Counsel/Chief Compliance Officer or Chief Information Officer.\nAdhere to the company's compliance program.\nSafeguard the company's intellectual property, information, and assets.\nOther duties as assigned.\nMinimum Qualifications and Job Requirements:\nBachelor's degree in Computer Science.\n7 years of hands-on experience in designing and developing distributed data pipelines.\n5 years of hands-on experience in Azure data service technologies.\n5 years of hands-on experience in Python, SQL, Object-oriented programming, ETL, and unit testing.\nExperience with data integration with APIs, Web services, Queues.\nExperience with Azure DevOps and CI/CD as well as agile tools and processes including JIRA, Confluence.\n*Required: Azure data engineering associate and databricks data engineering certification",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Delta Table', 'Azure Databricks', 'SQL', 'Python', 'SCALA', 'Big Data', 'Kafka', 'Azure Data Lake', 'Spark', 'ETL', 'Data Bricks']",2025-06-13 05:17:00
Azure Data Engineer,Big4,7 - 12 years,18-30 Lacs P.A.,['Bengaluru'],"Urgently Hiring for Senior Azure Data Engineer\n\nJob Location- Bangalore\nMinimum exp - Total 7+yrs with min 4 years relevant exp\n\nKeywords Databricks, Pyspark, SCALA, SQL, Live / Streaming data, batch processing data\n\nShare CV siddhi.pandey@adecco.com\nOR Call 6366783349\n\nRoles and Responsibilities:\nThe Data Engineer will work on data engineering projects for various business units, focusing on delivery of complex data management solutions by leveraging industry best practices. They work with the project team to build the most efficient data pipelines and data management solutions that make data easily available for consuming applications and analytical solutions. A Data engineer is expected to possess strong technical skills\n\nKey Characteristics\nTechnology champion who constantly pursues skill enhancement and has inherent curiosity to understand work from multiple dimensions\nInterest and passion in Big Data technologies and appreciates the value that can be brought in with an effective data management solution\nHas worked on real data challenges and handled high volume, velocity, and variety of data.\nExcellent analytical & problem-solving skills, willingness to take ownership and resolve technical challenges.\nContributes to community building initiatives like CoE, CoP.\nMandatory skills:\nAzure - Master\nELT - Skill\nData Modeling - Skill\nData Integration & Ingestion - Skill\nData Manipulation and Processing - Skill\nGITHUB, Action, Azure DevOps - Skill\nData factory, Databricks, SQL DB, Synapse, Stream Analytics, Glue, Airflow, Kinesis, Redshift, SonarQube, PyTest - Skill\nOptional skills:\nExperience in project management, running a scrum team.\nExperience working with BPC, Planning.\nExposure to working with external technical ecosystem.\nMKDocs documentation\n\nShare CV siddhi.pandey@adecco.com\nOR Call 6366783349",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['databricks', 'Azure Synapse', 'Pyspark', 'Stream Analytics', 'SCALA', 'SQL Azure', 'Data Bricks', 'SQL']",2025-06-13 05:17:02
Data Engineer I,Swiss Re,1 - 3 years,Not Disclosed,['Bengaluru'],"About the Role:\nAs a Data Engineer, you will be responsible for implementing data pipelines and analytics\nsolutions to support key decision-making processes in our Life Health Reinsurance business. You will become part of a project that is leveraging cutting edge technology that applies Big Data and Machine Learning to solve new and emerging problems for Swiss Re. You will be expected to gain a full understanding of the reinsurance data and business logic required to deliver analytics solutions.\nKey responsibilities include:\nWork closely with Product Owners and Engineering Leads to understand requirements and evaluate the implementation effort.\nDevelop and maintain scalable data transformation pipelines\nImplement analytics models and visualizations to provide actionable data insights\nCollaborate within a global development team to design and deliver solutions.\nAbout the Team:\nLife Health Data Analytics Engineering is a key tech partner for our Life Health Reinsurance division, supporting in the transformation of the data landscape and the creation of innovative analytical products and capabilities. A large globally distributed team working in an agile development landscape, we deliver solutions to make better use of our reinsurance data and enhance our ability to make data-driven decisions across the business value chain.\nAbout You:\nAre you eager to disrupt the industry with us and make an impactDo you wish to have your talent recognized and rewardedThen join our growing team and become part of the next wave of data\ninnovation. Key qualifications include:\nBachelors degree level or equivalent in Computer Science, Data Science or similar discipline\nAt least 1-3 years of experience working with large scale software systems\nProficient in Python/PySpark\nProficient in SQL (Spark SQL preferred)\nPalantir Foundry experience is a strong plus.\nExperience working with large data sets on enterprise data platforms and distributed computing (Spark/Hive/Hadoop preferred)\nExperience with JavaScript/HTML/CSS a plus\nExperience working in a Cloud environment such as AWS or Azure is a plus\nStrong analytical and problem-solving skills\nEnthusiasm to work in a global and multicultural environment of internal and external professionals\nStrong interpersonal and communication skills, demonstrating a clear and articulate standard of written and verbal communication in complex environments\nAbout Swiss Re\n.\n\n\n\nIf you are an experienced professional returning to the workforce after a career break, we encourage you to apply for open positions that match your skills and experience.\nKeywords:\nReference Code: 134085",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Reinsurance', 'data science', 'spark', 'Analytical', 'Machine learning', 'Javascript', 'HTML', 'SQL', 'Python']",2025-06-13 05:17:03
"Data Engineer, Product Analytics",Meta,2 - 7 years,Not Disclosed,['Bengaluru'],"Apply to this job\nAs a Data Engineer at Meta, you will shape the future of people-facing and business-facing products we build across our entire family of applications (Facebook, Instagram, Messenger, WhatsApp, Reality Labs, Threads). Your technical skills and analytical mindset will be utilized designing and building some of the worlds most extensive data sets, helping to craft experiences for billions of people and hundreds of millions of businesses worldwide.In this role, you will collaborate with software engineering, data science, and product management teams to design/build scalable data solutions across Meta to optimize growth, strategy, and user experience for our 3 billion plus users, as well as our internal employee community.You will be at the forefront of identifying and solving some of the most interesting data challenges at a scale few companies can match. By joining Meta, you will become part of a world-class data engineering community dedicated to skill development and career growth in data engineering and beyond.Data Engineering: You will guide teams by building optimal data artifacts (including datasets and visualizations) to address key questions. You will refine our systems, design logging solutions, and create scalable data models. Ensuring data security and quality, and with a focus on efficiency, you will suggest architecture and development approaches and data management standards to address complex analytical problems.Product leadership: You will use data to shape product development, identify new opportunities, and tackle upcoming challenges. Youll ensure our products add value for users and businesses, by prioritizing projects, and driving innovative solutions to respond to challenges or opportunities.Communication and influence: You wont simply present data, but tell data-driven stories. You will convince and influence your partners using clear insights and recommendations. You will build credibility through structure and clarity, and be a trusted strategic partner.\nData Engineer, Product Analytics Responsibilities\nCollaborate with engineers, product managers, and data scientists to understand data needs, representing key data insights in a meaningful way\nDesign, build, and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains\nDefine and manage Service Level Agreements for all data sets in allocated areas of ownership\nSolve challenging data integration problems, utilizing optimal Extract, Transform, Load (ETL) patterns, frameworks, query techniques, sourcing from structured and unstructured data sources\nImprove logging\nAssist in owning existing processes running in production, optimizing complex code through advanced algorithmic concepts\nOptimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts\nInfluence product and cross-functional teams to identify data opportunities to drive impact\nMinimum Qualifications\nBachelors degree in Computer Science, Computer Engineering, relevant technical field, or equivalent\n2+ years of experience where the primary responsibility involves working with data. This could include roles such as data analyst, data scientist, data engineer, or similar positions\n2+ years of experience with SQL, ETL, data modeling, and at least one programming language (e.g., Python, C++, C#, Scala or others.)\nPreferred Qualifications\nMasters or Ph.D degree in a STEM field\nAbout Meta\n.\n\n\nEqual Employment Opportunity\n.\n\nMeta is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, fill out the Accommodations request form .",Industry Type: Internet,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Product management', 'Computer science', 'C++', 'Data management', 'Data modeling', 'data security', 'Analytical', 'Analytics', 'SQL', 'Python']",2025-06-13 05:17:05
Data Engineer II - Marketplace (Experimentation Track),Booking Holdings,5 - 10 years,Not Disclosed,['Bengaluru'],"We are looking for a Data Engineer to join our team and help us to improve the platform that supports one of the best experimentation tools in the world.\nYou will work side by side with other data engineers and site reliability engineers to improve the reliability, scalability, maintenance and operations of all the data products that are part of the experimentation tool at Booking.com.\nYour day to day work includes but is not limited to: maintenance and operations of data pipelines and products that handles data at big scale; the development of capabilities for monitoring, alerting, testing and troubleshooting of the data ecosystem of the experiment platform; and the delivery of data products that produce metrics for experimentation at scale. You will collaborate with colleagues in Amsterdam to achieve results the right way. This will include engineering managers, product managers, engineers and data scientists.",,,,"['Data Engineering', 'Airflow', 'Java', 'CDC', 'NoSQL', 'Snowflake', 'DBT', 'Kafka', 'Python']",2025-06-13 05:17:07
Data Engineering Manager,NOVARTIS,6 - 8 years,Not Disclosed,['Hyderabad'],"Summary\nWe are seeking a highly skilled and motivated GCP Data Engineering Manager to join our dynamic team. As a Data Engineering manager specializing in Google Cloud Platform (GCP), you will play a crucial role in designing, implementing, and maintaining scalable data pipelines and\nsystems. You will leverage your expertise in Google Big Query, SQL, Python, and analytical skills to drive data-driven decision-making processes and support various business functions.\nAbout the Role\nKey Responsibilities:\nData Pipeline Development: Design, develop, and maintain robust data pipelines using GCP services like Dataflow, Dataproc, ensuring high performance and scalability.\nGoogle Big Query Expertise: Utilize your hands-on experience with Google Big Query to manage and optimize data storage, retrieval, and processing.\nSQL Proficiency: Write and optimize complex SQL queries to transform and analyze large datasets, ensuring data accuracy and integrity.\nPython Programming: Develop and maintain Python scripts for data processing, automation, and integration with other systems and tools.\nData Integration: Collaborate with data analysts, and other stakeholders to integrate data from various sources, ensuring seamless data flow and consistency.\nData Quality and Governance: Implement data quality checks, validation processes, and governance frameworks to maintain high data standards.\nPerformance Tuning: Monitor and optimize the performance of data pipelines, queries, and storage solutions to ensure efficient data processing.\nDocumentation: Create comprehensive documentation for data pipelines, processes, and best practices to facilitate knowledge sharing and team collaboration.\nMinimum Qualifications:\nProven experience (minimum 6 - 8 yrs) in Data Engineer, with significant hands-on experience in Google Cloud Platform (GCP) and Google Big Query.\nProficiency in SQL for data transformation, analysis and performance optimization.\nStrong programming skills in Python, with experience in developing data processing scripts and automation.\nProven analytical skills with the ability to interpret complex data and provide actionable insights.\nExcellent problem-solving abilities and attention to detail.\nStrong communication and collaboration skills, with the ability to work effectively in a team enviro\nDesired Skills :\nExperience with Google Analytics data and understanding of digital marketing data.\nFamiliarity with other GCP services such as Cloud Storage, Dataflow, Pub/Sub, and Dataproc.\nKnowledge of data visualization tools such as Looker, Tableau, or Data Studio.\nExperience with machine learning frameworks and libraries.\nWhy Novartis: Helping people with disease and their families takes more than innovative science. It takes a community of smart, passionate people like you. Collaborating, supporting and inspiring each other. Combining to achieve breakthroughs that change patients lives. Ready to create a brighter future together? https://www. novartis. com / about / strategy / people-and-culture\nJoin our Novartis Network: Not the right Novartis role for you? Sign up to our talent community to stay connected and learn about suitable career opportunities as soon as they come up: https://talentnetwork. novartis. com/network\nBenefits and Rewards: Read our handbook to learn about all the ways we ll help you thrive personally and professionally:",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Performance tuning', 'Automation', 'Google Analytics', 'Machine learning', 'Data processing', 'Data quality', 'data visualization', 'Digital marketing', 'SQL', 'Python']",2025-06-13 05:17:09
Immediate Joiner- Data Engineer,Healthedge,1 - 4 years,Not Disclosed,['Bengaluru'],"Data Engineer\nYou will be working with agile cross functional software development teams developing cutting age software to solve a significant problem in the Provider Data Management space. This hire will have experience building large scale complex data systems involving multiple cross functional data sets and teams. The ideal candidate will be excited about working on new product development, is comfortable pushing the envelope and challenging the status quo, sets high standards for him/herself and the team, and works well with ambiguity.\nWhat you will do:\nBuild data pipelines to assemble large, complex sets of data that meet non-functional and functional business requirements.\nWork closely with data architect, SMEs and other technology partners to develop & execute data architecture and product roadmap.\nBuild analytical tools to utilize the data pipeline, providing actionable insight into key business performance including operational efficiency and business metrics.\nWork with stakeholders including the leadership, product, customer teams to support their data infrastructure needs while assisting with data-related technical issues.\nAct as a subject matter expert to other team members for technical guidance, solution design and best practices within the customer organization.\nKeep current on big data and data visualization technology trends, evaluate, work on proof-of-concept and make recommendations on cloud technologies.\nWhat you bring:\n2+ years of data engineering experience working in partnership with large data sets (preferably terabyte scale)\nExperience in building data pipelines using any of the ETL tools such as Glue, ADF, Notebooks, Stored Procedures, SQL/Python constructs or similar.\nDeep experience working with industry standard RDBMS such Postgres, SQL Server, Oracle, MySQL etc. and any of the analytical cloud databases such as Big Query, Redshift, Snowflake or similar\nAdvanced SQL expertise and solid programming experience with Python and/or Spark\nExperience working with orchestration tools such as Airflow and building complex dependency workflows.\nExperience, developing and implementing Data Warehouse or Data Lake Architectures, OLAP technologies, data modeling with star/snowflake-schemas to enable analytics & reporting.\nGreat problem-solving capabilities, troubleshooting data issues and experience in stabilizing big data systems.\nExcellent communication and presentation skills as youll be regularly interacting with stakeholders and engineering leadership.\nBachelors or master's in quantitative disciplines such as Computer Science, Computer Engineering, Analytics, Mathematics, Statistics, Information Systems, or other scientific fields.\nBonus points:\nHands-on deep experience with cloud data migration, and experience working with analytic platforms like Fabric, Databricks on the cloud.\nCertification in one of the cloud platforms (AWS/GCP/Azure)\nExperience or demonstrated understanding with real-time data streaming tools like Kafka, Kinesis or any similar tools.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ETL', 'SQL', 'Pyspark', 'Cloud', 'Python']",2025-06-13 05:17:11
Data Engineer - AWS,Happiest Minds Technologies,6 - 10 years,Not Disclosed,"['Pune', 'Bengaluru']","Role & responsibilities\nEssential Skills: Experience: 6 to 10 yrs\n- Technical Expertise: Proficiency in AWS services such as Amazon S3, Redshift, EMR, Glue, Lambda, and Kinesis. Strong skills in SQL and experience with scripting languages like Python or Java.\n- Data Engineering Experience: Hands on experience in building and maintaining data pipelines, data modeling, and working with big data technologies.\n- Problem-Solving Skills: Ability to analyze complex data issues and develop effective solutions to optimize data processing and storage.",,,,"['Data Engineering', 'Pyspark', 'Aws Lambda', 'Amazon Redshift', 'Aws Glue', 'Athena', 'AWS', 'Python', 'SQL']",2025-06-13 05:17:12
Tech. PM - Data Engineering-Data Analytics@ Gurgaon/Blore_Urgent,A global leader in delivering innovative...,5 - 10 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Job Title - Technical Project Manager\n\nLocation - Gurgaon/ Bangalore\n\nNature of Job - Permanent\n\nDepartment - data analytics\n\nWhat you will be doing\n\n\nDemonstrated client servicing and business analytics skills with at least 5 - 9 years of experience as data engineer, BI developer, data analyst, technical project manager, program manager etc.\nTechnical project management- drive BRD, project scope, resource allocation, team\ncoordination, stakeholder communication, UAT, Prod fix, change requests, project governance\nSound knowledge of banking industry (payments, retail operations, fraud etc.)\nStrong ETL experience or experienced Teradata developer\nManaging team of business analysts, BI developers, ETL developers to ensure that projects are completed on time\nResponsible for providing thought leadership and technical advice on business issues\nDesign methodological frameworks and solutions.\n\n\nWhat were looking for\n\n\nBachelors/masters degree in computer science/data science/AI/statistics, Certification in Gen AI. Masters degree Preferred.\nManage multiple projects, at a time, from inception to delivery\nSuperior problem-solving, analytical, and quantitative skills\nEntrepreneurial mindset, coupled with a “can do” attitude\nDemonstrated ability to collaborate with cross-functional, cross-border teams and coach / mentor colleagues.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Technical Project Manager', 'Data Engineering', 'multiple projects', 'Technical project management', 'Data Analytics', 'project scope', 'ETL Pipeline', 'team coordination', 'resource allocation', 'Prod fix', 'drive BRD', 'program manager', 'Big data']",2025-06-13 05:17:14
Expert Cloud Data Engineer - (AWS/Python/Pyspark/Kafka/SQL),BMW Techworks India,9 - 13 years,15-27.5 Lacs P.A.,"['Chennai', 'Bengaluru']","Job Description\nRole Expert Cloud Data Engineer\nLocation Pune/Chennai\nExperience: 9 to 12 years and above.\nWhat awaits you/ Job Profile\nYou will lead strategic data engineering initiatives, ensuring that cloud data solutions align with business objectives. You will oversee large-scale projects, mentor teams, and drive innovation in cloud-based data engineering.\nKey Responsibilities:\nArchitect and implement enterprise-level data solutions that are scalable and secure.\nLead cross-functional teams to execute large-scale data projects.\nDevelop data governance strategies and lifecycle management best practices.\nExplore and integrate cutting-edge cloud and AI technologies to enhance data capabilities.\nMentor junior and senior engineers, fostering a culture of technical excellence.\nConduct high-level data analysis and make strategic recommendations.\nDrive cost optimization and performance tuning across cloud-based data platforms.\nWhat should you bring along\n8+ years of experience in data engineering, AWS Cloud Architecture, and DevOps.\nStrong understanding of data security, compliance, and governance in cloud environments.\nAbility to evaluate and implement emerging technologies in data engineering.\nExcellent knowledge of Terraform and GitHub Actions.\nExperienced in being in the lead of a feature team.\nMust have technical skill\nAdvanced Python, PySpark, SQL, Java/Scala (preferred)\nAWS (advanced expertise in Glue, Redshift, Athena, Kinesis, EMR), Cloud Architect Certification\nKafka, Flink, Spark Streaming\nGood to have technical skills\nTerraform, AWS CloudFormation\nGitHub Actions, Jenkins, Kubernetes\nIntegration of ML models into cloud data pipelines\nData Governance & Security: Role-based access control (RBAC), encryption, compliance frameworks",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Terraform', 'Kafka', 'AWS', 'Python']",2025-06-13 05:17:16
Azure Data Engineer ( Azure Databricks),Apex One,4 - 8 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","Job Summary\nWe are seeking a skilled Azure Data Engineer with 4 years of overall experience, including at least 2 years of hands-on experience with Azure Databricks (Must). The ideal candidate will have strong expertise in building and maintaining scalable data pipelines and working across cloud-based data platforms.\nKey Responsibilities\nDesign, develop, and optimize large-scale data pipelines using Azure Data Factory, Azure Databricks, and Azure Synapse.\nImplement data lake solutions and work with structured and unstructured datasets in Azure Data Lake Storage (ADLS).\nCollaborate with data scientists, analysts, and engineering teams to design and deliver end-to-end data solutions.\nDevelop ETL/ELT processes and integrate data from multiple sources.\nMonitor, debug, and optimize workflows for performance and cost-efficiency.\nEnsure data governance, quality, and security best practices are maintained.\nMust-Have Skills\n4+ years of total experience in data engineering.\n2+ years of experience with Azure Databricks (PySpark, Notebooks, Delta Lake).\nStrong experience with Azure Data Factory, Azure SQL, and ADLS.\nProficient in writing SQL queries and Python/Scala scripting.\nUnderstanding of CI/CD pipelines and version control systems (e.g., Git).\nSolid grasp of data modeling and warehousing concepts.",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'Azure Data Factory', 'SQL queries', 'PySpark', 'Delta Lake', 'Azure Databricks', 'Notebooks', 'Azure SQL']",2025-06-13 05:17:17
Jr Data Engineer,BMW Techworks India,4 - 6 years,Not Disclosed,"['Chennai', 'Bengaluru']","What awaits you/ Job Profile\nAnalyze, Organize data and Build data systems with multiple data sources\nWorking closely with business and market team for the Data analysis.\nWhat should you bring along\nBuild data systems and pipelines\nStandardize the data assets which maximize data reusability\nEvaluate business needs and objectives\nImplementing data pipeline to ingest and cleanse raw data from various source systems including SaaS, Cloud based and On-prep databases\nCombine raw information from different sources\nExplore ways to enhance data quality and reliability\nImplementing functional requirements including incremental loads, data snapshots and identity resolution\nOptimizing data pipelines to improve performance and cost, while ensuring a high quality of data within the data lake:\nMonitoring services and jobs for cost and performance, ensuring continual operations of data pipelines, and fixing of defects.\nConstantly looking for opportunities to optimize data pipelines to improve performance.\nMust have technical skill\nMust have coding skills in Spark/Pyspark, Python and SQL\nMust have Knowledge of AWS tools, Glue, Athena ,step functions and S3\nGood to have technical skills\nGood knowledge of CI/CD (Github / Github Actions, Terraform)\nKnowledge on Agile methodologies.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'github', 'Coding', 'Agile', 'Data quality', 'Cost', 'AWS', 'Monitoring', 'SQL', 'Python']",2025-06-13 05:17:19
Mid-Level Data Engineer,BMW Techworks India,6 - 8 years,Not Disclosed,"['Chennai', 'Bengaluru']","What awaits you/ Job Profile\nProvide estimates for requirements, analyses and develop as per the requirement.\nDeveloping and maintaining data pipelines and ETL (Extract, Transform, Load) processes to extract data efficiently and reliably from various sources, transform it into a usable format, and load it into the appropriate data repositories.\nCreating and maintaining logical and physical data models that align with the organizations data architecture and business needs. This includes defining data schemas, tables, relationships, and indexing strategies for optimal data retrieval and analysis.\nCollaborating with cross-functional teams and stakeholders to ensure data security, privacy, and compliance with regulations.\nCollaborate with downstream application to understand their needs and build the data storage and optimize as per their need.\nWorking closely with other stakeholders and Business to understand data requirements and translate them into technical solutions.\nFamiliar with Agile methodologies and have prior experience working with Agile teams using Scrum/Kanban\nLead Technical discussions with customers to find the best possible solutions.\nProactively identify and implement opportunities to automate tasks and develop reusable frameworks.\nOptimizing data pipelines to improve performance and cost, while ensuring a high quality of data within the data lake.\nMonitoring services and jobs for cost and performance, ensuring continual operations of data pipelines, and fixing of defects.\nConstantly looking for opportunities to optimize data pipelines to improve performance\nWhat should you bring along\nMust Have:\nHand on Expertise of 6- 8 years in AWS services like S3, Lambda, Glue, Athena, RDS, Step functions, SNS, SQS, API Gateway, Security, Access and Role permissions, Logging and monitoring Services.\nGood hand on knowledge on Python, Spark, Hive and Unix, AWS CLI\nPrior experience in working with streaming solution like Kafka\nPrior experience in implementing different file storage types like Delta-lake / Ice-berg.\nExcellent knowledge in Data modeling and Designing ETL pipeline.\nMust have strong knowledge in using different databases such as MySQL, Oracle and Writing complex queries.\nStrong experience working in a continuous integration and Deployment process.\nNice to Have:\nHand on experience in the Terraform, GIT, GIT Actions. CICD pipeline and Amazon Q.\nMust have technical skill\nPyspark, AWS ,SQL, Kafka, Glue, IAM. S3, Lambda, Step Function, Athena\nGood to have Technical skills\nTerraform, GIT, GIT Actions. CICD pipeline , AI",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Data modeling', 'data security', 'MySQL', 'Agile', 'Scrum', 'Oracle', 'Monitoring', 'SQL', 'Python']",2025-06-13 05:17:21
Aws Data Engineer,Hiring for Leading MNC Company,4 - 6 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Warm Greetings from SP Staffing!!\n\nRole:AWS Data Engineer\nExperience Required :4 to 6 yrs\nWork Location :Bangalore/Pune/Hyderabad/Chennai\n\nRequired Skills,\n\nPyspark\nAWS Glue\n\nInterested candidates can send resumes to nandhini.spstaffing@gmail.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Aws Glue', 'python', 'EMR', 'Aws Emr', 'AWS Data Engineer', 'Aws Lambda', 'Lakehouse', 'spark', 'Data Engineer', 'ETL', 'AWS', 'Athena', 'gateway']",2025-06-13 05:17:23
Gcp Data Engineer,Saama Technologies,3 - 8 years,Not Disclosed,"['Pune', 'Chennai', 'Coimbatore']","We are looking for immediate joiners only.\nPosition: GCP Data Engineer\nWe are seeking a skilled and experienced GCP Data Engineer to join our dynamic team. The ideal candidate will have a strong background in Google Cloud Platform (GCP), BigQuery, Dataform, and data warehouse concepts. Experience with Airflow/Cloud Composer and cloud computing knowledge will be a significant advantage.\nResponsibilities:\n- Designing, developing, and maintaining data pipelines and workflows on the Google Cloud Platform.",,,,"['Pyspark', 'GCP', 'Python', 'SQL', 'Google Cloud Platforms']",2025-06-13 05:17:24
GCP Data Engineer,TVS Next,3 - 5 years,Not Disclosed,['Bengaluru'],"What you’ll be doing:\nAssist in developing machine learning models based on project requirements\nWork with datasets by preprocessing, selecting appropriate data representations, and ensuring data quality.\nPerforming statistical analysis and fine-tuning using test results.\nSupport training and retraining of ML systems as needed.\nHelp build data pipelines for collecting and processing data efficiently.",,,,"['kubernetes', 'pyspark', 'data pipeline', 'sql', 'docker', 'cloud', 'tensorflow', 'java', 'spark', 'gcp', 'pytorch', 'bigquery', 'programming', 'ml', 'cloud sql', 'cd', 'python', 'airflow', 'cloud spanner', 'cloud pubsub', 'application engine', 'machine learning', 'apache flink', 'data engineering', 'dataproc', 'kafka', 'cloud storage', 'terraform', 'bigtable']",2025-06-13 05:17:26
GCP Data Engineer,Swits Digital,4 - 6 years,Not Disclosed,['Bengaluru'],"Job Title: GCP Data Engineer\nLocation: Chennai, Bangalore, Hyderabad\nExperience: 4-6 Years\nJob Summary:\nWe are seeking a GCP Data & Cloud Engineer with strong expertise in Google Cloud Platform services, including BigQuery, Cloud Run, Cloud Storage , and Pub/Sub . The ideal candidate will have deep experience in SQL coding , data pipeline development, and deploying cloud-native solutions.\nKey Responsibilities:\nDesign, implement, and optimize scalable data pipelines and services using GCP\nBuild and manage cloud-native applications deployed via Cloud Run\nDevelop complex and performance-optimized SQL queries for analytics and data transformation\nManage and automate data storage, retrieval, and archival using Cloud Storage\nImplement event-driven architectures using Google Pub/Sub\nWork with large datasets in BigQuery , including ETL/ELT design and query optimization\nEnsure security, monitoring, and compliance of cloud-based systems\nCollaborate with data analysts, engineers, and product teams to deliver end-to-end cloud solutions\nRequired Skills & Experience:\n4 years of experience working with Google Cloud Platform (GCP)\nStrong proficiency in SQL coding , query tuning, and handling complex data transformations\nHands-on experience with:\nBigQuery\nCloud Run\nCloud Storage\nPub/Sub\nUnderstanding of data pipeline and ETL/ELT workflows in cloud environments\nFamiliarity with containerized services and CI/CD pipelines\nExperience in scripting languages (e.g., Python, Shell) is a plus\nStrong analytical and problem-solving skills",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SUB', 'query optimization', 'GCP', 'Analytical', 'Cloud', 'query', 'cloud storage', 'Analytics', 'SQL coding', 'Python']",2025-06-13 05:17:28
Python Data Engineer with AI/ML,Expleo,5 - 6 years,Not Disclosed,['Pune'],"Overview\nWe are looking for a Python Data Engineer with expertise in real-time data monitoring, extraction, transformation, and visualization. The ideal candidate will have experience working with Oracle SQL databases, multithreading, and AI/ML techniques and should be proficient in deploying Python applications on IIS servers . The role involves developing a system to monitor live files and folders, extract data, transform it using various techniques, and display insights on a Plotly Dash-based dashboard .\nResponsibilities",,,,"['Computer science', 'IIS', 'Data analysis', 'Backend', 'Multithreading', 'Debugging', 'Data processing', 'Troubleshooting', 'Python', 'Data extraction']",2025-06-13 05:17:29
Data Engineering Specialist,Overture Rede,10 - 15 years,Not Disclosed,"['Mumbai', 'Hyderabad', 'Gurugram', 'Bengaluru']","Job Title: Sales Excellence COE Data Engineering Specialist\nLocations: Mumbai / Bangalore / Gurgaon / Hyderabad\nExperience: 1012 Years Level: Team Lead / Specialist (Level 9)\n\nJob Role\nLead data engineering efforts to support sales insights through scalable pipelines, statistical modeling, and ML workflows across cloud platforms.\n\nRequired Skills\nProficiency in Python\nAdvanced SQL (Views, Functions, Procedures)\nExperience with Google Cloud Platform (GCP) ML workflow setup\nStrong in Data Modeling and ETL Development\nExcel skills including VBA, Power Pivot, Cube Functions\nSolid understanding of Sales Processes\n\n\n",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Statistical modeling', 'Sales', 'Excel', 'VBA', 'Data modeling', 'GCP', 'Cloud', 'Workflow', 'SQL', 'Python']",2025-06-13 05:17:31
Data Engineer KL-BL,Puresoftware,5 - 12 years,Not Disclosed,['Bengaluru'],"Core Competences Required and Desired Attributes:\nBachelors degree in computer science, Information Technology, or a related field.\nProficiency in Azure Data Factory, Azure Databricks and Unity Catalog, Azure SQL Database, and other Azure data services.\nStrong programming skills in SQL, Python and PySpark languages.\nExperience in the Asset Management domain would be preferable.\nStrong proficiency in data analysis and data modelling, with the ability to extract insights from complex data sets.\nHands-on experience in Power BI, including creating custom visuals, DAX expressions, and data modelling.\nFamiliarity with Azure Analysis Services, data modelling techniques, and optimization.\nExperience with data quality and data governance frameworks with an ability to debug, fine tune and optimise large scale data processing jobs.\nStrong analytical and problem-solving skills, with a keen eye for detail.\nExcellent communication and interpersonal skills, with the ability to work collaboratively in a team environment.\nProactive and self-motivated, with the ability to manage multiple tasks and deliver high-quality results within deadlines.",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Interpersonal skills', 'Data modeling', 'Analytical', 'data governance', 'Data quality', 'Asset management', 'Information technology', 'SQL', 'Python']",2025-06-13 05:17:33
Data Engineer (AWS),Neoware Technology Solutions,4 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","Data Engineer (AWS) - Neoware Technology Solutions Private Limited Data Engineer (AWS)\nRequirements\n4 - 10 years of hands-on experience in designing, developing and implementing data engineering solutions.\nStrong SQL development skills, including performance tuning and query optimization.\nGood understanding of data concepts.\nProficiency in Python and a solid understanding of programming concepts.\nHands-on experience with PySpark or Spark Scala for building data pipelines.\nUnderstanding of streaming data pipelines for near real-time analytics.\nExperience with Azure services including Data Factory, Functions, Databricks, Synapse Analytics, Event Hub, Stream Analytics and Data Lake Storage.\nFamiliarity with at least one NoSQL database.\nKnowledge of modern data architecture patterns and industry trends in data engineering.\nUnderstanding of data governance concepts for data platforms and analytical solutions.\nExperience with Git for managing version control for source code.\nExperience with DevOps processes, including experience implementing CI/CD pipelines for data engineering solutions.\nStrong analytical and problem-solving skills.\nExcellent communication and teamwork skills.\nResponsibilities\nAzure Certifications related to Data Engineering are highly preferred.\nExperience with Amazon AppFlow, EKS, API Gateway, NoSQL database services.\nStrong understanding and experience with BI/visualization tools like Power BI.\nChennai, Bangalore\nFull time\n4+ years\nOther positions Principal Architect (Data and Cloud) Development",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Performance tuning', 'Version control', 'GIT', 'NoSQL', 'query optimization', 'Analytical', 'data governance', 'Analytics', 'Python', 'Data architecture']",2025-06-13 05:17:35
Data Engineer (Azure),Neoware Technology Solutions,4 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","Data Engineer (Azure) - Neoware Technology Solutions Private Limited Data Engineer (Azure)\nRequirements\n4 - 10 years of hands-on experience in designing, developing and implementing data engineering solutions.\nStrong SQL development skills, including performance tuning and query optimization.\nGood understanding of data concepts.\nProficiency in Python and a solid understanding of programming concepts.\nHands-on experience with PySpark or Spark Scala for building data pipelines.\nEnsure data consistency and address ambiguities or inconsistencies across datasets.\nUnderstanding of streaming data pipelines for near real-time analytics.\nExperience with Azure services including Data Factory, Functions, Databricks, Synapse Analytics, Event Hub, Stream Analytics and Data Lake Storage.\nFamiliarity with at least one NoSQL database.\nKnowledge of modern data architecture patterns and industry trends in data engineering.\nUnderstanding of data governance concepts for data platforms and analytical solutions.\nExperience with Git for managing version control for source code.\nExperience with DevOps processes, including experience implementing CI/CD pipelines for data engineering solutions.\nStrong analytical and problem-solving skills.\nExcellent communication and teamwork skills.\nResponsibilities\nAzure Certifications related to Data Engineering are highly preferred.\nExperience with Azure Kubernetes Service (AKS), Container Apps and API Management.\nStrong understanding and experience with BI/visualization tools like Power BI.\nChennai, Bangalore\nFull time\n4+ years\nOther positions\nChennai / Bangalore / Mumbai\n3+ years\nPrincipal Architect (Data and Cloud) Development",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Performance tuning', 'Version control', 'GIT', 'query optimization', 'NoSQL', 'Analytical', 'SCALA', 'Analytics', 'Python', 'Data architecture']",2025-06-13 05:17:37
Director - Data Engineering,Blend360 India,10 - 15 years,Not Disclosed,['Bengaluru'],"We are seeking a strategic Director of Data & AI Engineering to lead the growth and evolution of our data engineering function. This role will play a pivotal part in designing scalable platforms, enabling advanced AI and ML applications, and building a world-class engineering team. The ideal candidate is both a seasoned technical leader and a visionary strategist who thrives at the intersection of innovation, execution, and impact. You will collaborate with internal and client teams to develop systems and infrastructure that power intelligent products and data-driven decision-making. You will also build and mentor a high-performing team of Data and AI Engineers, foster a modern data culture, and champion best practices for scalable architecture, AI integration, and engineering excellence.\nResponsibilities:\nSolution Design & Engineering Leadership\nArchitect and build scalable, high-performance data and AI pipelines using tools such as Spark, PySpark, SQL, Python, DBT, Airflow, and cloud-native platforms (AWS, GCP, or Azure).\nLead the design of hybrid/on-prem data platforms, incorporating security, governance, and performance optimization.\nStrategic Client & Stakeholder Engagement\nServe as a trusted technical advisor to internal and external stakeholders.\nTranslate complex business needs into practical engineering solutions and oversee the end-to-end delivery lifecycle.\nAI-Driven Productivity & Innovation\nIntroduce and scale AI-driven tools and practices to accelerate development and enhance data quality, resilience, and maintainability.\nChampion the adoption of generative AI and foundation models to enable intelligent automation and insight generation.\nGrowth & Team Leadership\nBuild, lead, and inspire a diverse team of Data Engineers, ML Engineers, and AI Specialists.\nSet a clear vision and goals, provide mentorship, and cultivate a strong engineering culture and standards.\nPlatform and Data Strategy\nLead initiatives to modernize data infrastructure, improve data discoverability, and support real-time analytics and experimentation.\nCollaborate cross-functionally to shape product data strategies and influence the overall AI roadmap.\nPresales & Business Development Support\nPartner with sales and solution teams to craft compelling proposals, technical solutions, and client presentations.\nRepresent the engineering function in client discussions, workshops, and RFP responses to articulate value and differentiation.\nSupport opportunity scoping, estimation, and roadmap planning for prospective engagements.\n\n\n10+ years of experience in data engineering, AI/ML engineering, or product/platform engineering.\nProven track record of leading high-performing teams and managing senior engineers and managers.\nBachelor s or masters degre",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'GCP', 'Business Development Manager', 'Social media', 'Data quality', 'RFP', 'Analytics', 'SQL', 'Python']",2025-06-13 05:17:38
Snowflake Data Engineer,Tredence,3 - 8 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\n\nDesign, build, and maintain scalable data pipelines using DBT and Airflow.\nDevelop and optimize SQL queries and data models in Snowflake.\nImplement ETL/ELT workflows, ensuring data quality, performance, and reliability.\nWork with Python for data processing, automation, and integration tasks.\nHandle JSON data structures for data ingestion, transformation, and APIs.\nLeverage AWS services (e.g., S3, Lambda, Glue, Redshift) for cloud-based data solutions. Collaborate with data analysts, engineers, and business teams to deliver high-quality data products.",,,,"['Snowflake', 'DBT', 'SQL']",2025-06-13 05:17:40
"Data Engineer( Python, AWS, Databricks, EKS, Airflow)",Banking,5 - 9 years,Not Disclosed,['Bengaluru'],"Exprence 5-8 Years\nLocation - Bangalore\nMode C2H\n\nHands on data engineering experience.\nHands on experience with Python programming\nHands-on Experience with AWS & EKS\nWorking knowledge of Unix, Databases, SQL\nWorking Knowledge on Databricks\nWorking Knowledge on Airflow and DBT",Industry Type: Investment Banking / Venture Capital / Private Equity,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Airflow', 'Data Engineering', 'AWS', 'Python', 'SQL', 'Databricks', 'Eks']",2025-06-13 05:17:42
Urgent hiring For Cloud Data Engineer,Wowjobs,7 - 10 years,30-45 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities\n*  Design, Build, and Maintain ETL Pipelines: Develop robust, scalable, and efficient ETL workflows to ingest, transform, and load data into distributed data products within the Data Mesh architecture.\n*   Data Transformation with dbt: Use dbt to build modular, reusable transformation workflows that align with the principles of Data Products.\n*   Cloud Expertise: Leverage Google Cloud Platform (GCP) services such as BigQuery, Cloud Storage, Pub/Sub, and Dataflow to implement highly scalable data solutions.\n*   Data Quality & Governance: Enforce strict data quality standards by implementing validation checks, anomaly detection mechanisms, and monitoring frameworks.\n*   Performance Optimization: Continuously optimize ETL pipelines for speed, scalability, and cost efficiency.\n*   Collaboration & Ownership: Work closely with data product owners, BI developers, and stakeholders to understand requirements and deliver on expectations. Take full ownership of your deliverables.\n*   Documentation & Standards: Maintain detailed documentation of ETL workflows, enforce coding standards, and adhere to best practices in data engineering.\n*   Troubleshooting & Issue Resolution: Proactively identify bottlenecks or issues in pipelines and resolve them quickly with minimal disruption.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python coding', 'Cloud data engineer', 'ETL Workflow']",2025-06-13 05:17:44
Gcp Data Engineer,Royal Cyber,9 - 14 years,Not Disclosed,[],"Minimum 7+ years in data engineering with 5+ years of hands-on experience on GCP.\nProven track record with tools and services like BigQuery, Cloud Composer (Apache Airflow), Cloud Functions, Pub/Sub, Cloud Storage, Dataflow, and IAM/VPC.\nDemonstrated expertise in Apache Spark (batch and streaming), PySpark, and building scalable API integrations.\nAdvanced Airflow skills including custom operators, dynamic DAGs, and workflow performance tuning.\nCertifications\nGoogle Cloud Professional Data Engineer certification preferred.\nKey Skills\nMandatory Technical Skills\nAdvanced Python (PySpark, Pandas, pytest) for automation and data pipelines.\nStrong SQL with experience in window functions, CTEs, partitioning, and optimization.\nProficiency in GCP services including BigQuery, Dataflow, Cloud Composer, Cloud Functions, and Cloud Storage.\nHands-on with Apache Airflow, including dynamic DAGs, retries, and SLA enforcement.\nExpertise in API data ingestion, Postman collections, and REST/GraphQL integration workflows.\nFamiliarity with CI/CD workflows using Git, Jenkins, or Bitbucket.\nExperience with infrastructure security and governance using IAM and VPC.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Part Time, Temporary/Contractual","['GCP', 'Bigquery', 'Google Cloud Platforms', 'Cloud Storage', 'Data Flow']",2025-06-13 05:17:45
Aws Data Engineer,Hiring for Leading MNC Company!!,8 - 13 years,Not Disclosed,['Bengaluru'],"Warm Greetings from SP Staffing!!\n\nRole:AWS Data Engineer\nExperience Required :8 to 15 yrs\nWork Location :Bangalore\n\nRequired Skills,\n\nTechnical knowledge of data engineering solutions and practices. Implementation of data pipelines using tools like EMR, AWS Glue, AWS Lambda, AWS Step Functions, API Gateway, Athena\nProficient in Python and Spark, with a focus on ETL data processing and data engineering practices.\n\nInterested candidates can send resumes to nandhini.spstaffing@gmail.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineer', 'AWS', 'Pyspark', 'python', 'EMR', 'Aws Glue', 'Aws Emr', 'AWS Data Engineer', 'Aws Lambda', 'Lakehouse', 'spark', 'ETL', 'Athena', 'gateway']",2025-06-13 05:17:47
Data Engineer-Having Stratup-Mid-Size company Exp.@ Bangalore_Urgent,"As a leader in this space, we deliver wo...",8 - 13 years,Not Disclosed,['Bengaluru'],"Data Engineer\n\nLocation: Bangalore - Onsite\nExperience: 8 - 15 years\nType: Full-time\n\nRole Overview\n\nWe are seeking an experienced Data Engineer to build and maintain scalable, high-performance data pipelines and infrastructure for our next-generation data platform. The platform ingests and processes real-time and historical data from diverse industrial sources such as airport systems, sensors, cameras, and APIs. You will work closely with AI/ML engineers, data scientists, and DevOps to enable reliable analytics, forecasting, and anomaly detection use cases.\nKey Responsibilities\nDesign and implement real-time (Kafka, Spark/Flink) and batch (Airflow, Spark) pipelines for high-throughput data ingestion, processing, and transformation.\nDevelop data models and manage data lakes and warehouses (Delta Lake, Iceberg, etc) to support both analytical and ML workloads.\nIntegrate data from diverse sources: IoT sensors, databases (SQL/NoSQL), REST APIs, and flat files.\nEnsure pipeline scalability, observability, and data quality through monitoring, alerting, validation, and lineage tracking.\nCollaborate with AI/ML teams to provision clean and ML-ready datasets for training and inference.\nDeploy, optimize, and manage pipelines and data infrastructure across on-premise and hybrid environments.\nParticipate in architectural decisions to ensure resilient, cost-effective, and secure data flows.\nContribute to infrastructure-as-code and automation for data deployment using Terraform, Ansible, or similar tools.\n\n\nQualifications & Required Skills\n\nBachelors or Master’s in Computer Science, Engineering, or related field.\n6+ years in data engineering roles, with at least 2 years handling real-time or streaming pipelines.\nStrong programming skills in Python/Java and SQL.\nExperience with Apache Kafka, Apache Spark, or Apache Flink for real-time and batch processing.\nHands-on with Airflow, dbt, or other orchestration tools.\nFamiliarity with data modeling (OLAP/OLTP), schema evolution, and format handling (Parquet, Avro, ORC).\nExperience with hybrid/on-prem and cloud platforms (AWS/GCP/Azure) deployments.\nProficient in working with data lakes/warehouses like Snowflake, BigQuery, Redshift, or Delta Lake.\nKnowledge of DevOps practices, Docker/Kubernetes, Terraform or Ansible.\nExposure to data observability, data cataloging, and quality tools (e.g., Great Expectations, OpenMetadata).\nGood-to-Have\nExperience with time-series databases (e.g., InfluxDB, TimescaleDB) and sensor data.\nPrior experience in domains such as aviation, manufacturing, or logistics is a plus.\n\nRole & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['aviation', 'Data Modeling', 'Python', 'OLAP', 'Cloud', 'ORC', 'logistics', 'Avro', 'Terraform', 'Snowflake', 'manufacturing', 'AWS', 'Parquet', 'Java', 'Azure', 'BigQuery', 'Data', 'Redshift', 'SQL', 'TimescaleDB', 'GCP', 'InfluxDB', 'dbt', 'Ansible', 'OLTP', 'Kubernetes']",2025-06-13 05:17:49
Azure Data Engineer/Lead/Architect (5 - 20 Years) (Pan India Location),Allegis Group,5 - 10 years,Not Disclosed,[],"Azure Data Engineer/Lead/Architect (5 - 20 Years) (Pan India Location)\nJob Location : Hyderabad / Bangalore / Chennai / Kolkata / Noida/ Gurgaon / Pune / Indore / Mumbai\n\n\n5 -20 years of relevant hands on development experience. And 4+ years as Azure Data Engineering role\nProficient in Azure technologies like ADB, ADF, SQL(capability of writing complex SQL queries), ADB, PySpark, Python, Synapse, Delta Tables, Unity Catalog\nHands on in Python, PySpark or Spark SQL\nHands on in Azure Analytics and DevOps\nTaking part in Proof of Concepts (POCs) and pilot solutions preparation\nAbility to conduct data profiling, cataloguing, and mapping for technical design and construction of technical data flows\nExperience in business processing mapping of data and analytics solutions",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Analytics', 'Azure Data Engineering', 'Azure Databricks', 'Devops', 'Python', 'Azure Data Factory', 'Pyspark', 'Azure', 'Adb']",2025-06-13 05:17:51
Senior Data Engineer,Binary Infoways,5 - 9 years,12-19.2 Lacs P.A.,['Hyderabad'],"Responsibilities:\n* Design, develop & maintain data pipelines using Airflow, Python & SQL.\n* Optimize performance through Spark & Splunk analytics.\n* Collaborate with cross-functional teams on big data initiatives.\n* AWS",Industry Type: BPM / BPO,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Airflow', 'Big Data Technologies', 'ETL', 'AWS', 'Python', 'Glue', 'Snowflake', 'Spark', 'Splunk', 'SQL']",2025-06-13 05:17:53
Senior Data Engineer,Dfcs Technologies,8 - 13 years,Not Disclosed,['Chennai'],"Architect & Build Scalable Systems: Design and implement a petabyte-scale lakehouse\nArchitectures to unify data lakes and warehouses. Real-Time Data Engineering: Develop and optimize streaming pipelines using Kafka, Pulsar, and Flink.\n\nRequired Candidate profile\nData engineering experience with large-scale systems• Expert proficiency in Java for data-intensive applications. Handson experience with lakehouse architectures, stream processing, & event streaming",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Java', 'Terraform', 'Delta Lake', 'Data Engineer', 'Streaming', 'Streaming Framework', 'Pyspark', 'Apache Iceberg', 'lakehouse', 'Apache Flink', 'Kafka', 'Spark Streaming', 'Stream Processing', 'Spark Core', 'Apache Storm', 'Apache Pulsar', 'Kafta', 'Data Pipeline', 'Streaming Kafka', 'Clickhouse', 'Spark', 'AWS', 'Kafka Streams', 'Streams']",2025-06-13 05:17:55
Sr Data Engineer - Fully Remote & Immediate Opportunity,Zealogics.com,10 - 15 years,Not Disclosed,[],"10 yrs of exp working in cloud-native data (Azure Preferred),Databricks, SQL,PySpark, migrating from Hive Metastore to Unity Catalog, Unity Catalog, implementing Row-Level Security (RLS), metadata-driven ETL design patterns,Databricks certifications",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Azure', 'Metadata', 'Data Bricks', 'Unity Catalog', 'ETL design', 'SQL']",2025-06-13 05:17:57
Senior Data Engineer,Conviction HR,8 - 10 years,Not Disclosed,"['Kolkata', 'Hyderabad', 'Pune( Malad )']","Must have -Azure Data Factory (Mandatory). Azure Databricks, Pyspark and Python and advance SQL Azure eco-system. 1) Advanced SQL Skills. 2)Data Analysis. 3) Data Models. 4) Python (Desired). 5) Automation - Experience required : 8 to 10 years.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Data Engineering', 'Python', 'Azure Databricks', 'Data Modeling', 'Data Bricks', 'SQL']",2025-06-13 05:17:58
Azure Data Engineer,Jurist Associates,5 - 10 years,7-12 Lacs P.A.,"['Kochi', 'Hyderabad', 'Bengaluru']","Design, build, and maintain scalable and efficient data pipelines using Azure services such as Azure Data Factory (ADF), Azure Databricks, and Azure Synapse Analytics. Develop and optimize ETL/ELT workflows for ingestion, cleansing, transformation,\n\nRequired Candidate profile\nStrong understanding of data warehouse architecture, data lakes, and big data frameworks. Candidates who have atleast 5 years of experience should only apply.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Engineering', 'Azure Data Factory', 'GenAI Tools', 'Azure Data Warehouse', 'Azure data lake Gen2', 'Azure Synapse Analytics', 'Azure Databricks', 'Azure Data Lake', 'Nosql Databases', 'Data Modeling', 'Semantic Analytics', 'ML/DL Models']",2025-06-13 05:18:00
Data Engineer - Financial Analytics Specialist,RWS Group,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Purpose -\nWe are seeking a Data Engineer Financial Analytics Specialist to join our Data and Analytics team at RWS. This role combines advanced technical skills in SQL and ETL with analytical thinking and financial business acumen. You will be instrumental in designing and implementing data transformations and pipelines to support complex business requirements, particularly in financial data systems. This is a hands-on technical role with a significant focus on problem-solving, mathematical reasoning, and creating solutions for intricate business processes.\nThis position is ideal for someone with a quantitative background in computer science, mathematics, or a related field, who thrives on developing innovative solutions, optimizing data workflows, and ensuring data accuracy. We are looking for a candidate who is self-motivated, detail-oriented, and possesses exceptional technical skills, alongside a strong sense of ownership and accountability. The primary focus of this role will be financial data projects within a collaborative and dynamic team environment.\n\nAbout Group Technology-\nGroup Technology enables the organization to achieve its strategic direction whilst driving shareholder value. The division establishes common standards and IT governance across the business. It further develops and manages core applications enabling smooth operational running of the organization across all functions. We drive and deliver future roadmaps aligned to the overall strategic direction of the business. Group Technology support services to over 7500 end users across the globe, manage the information security operation and safeguard all our assets. Our core Group Technology functions include Technical Architecture, Network & Voice, IT Security, Service Delivery, Solutions Delivery and Asset Management. Group Technology has a global presence across all regions with over 400 staff.\nOur Data Engineering team is responsible for developing, building, maintaining, and managing data pipelines. This requires working with large datasets, databases, and the software used to analyse them including cloud systems like AWS or Azure.\n\nJob Overview\nKey Responsibilities\nWe are seeking a Data Engineer Financial Analytics Specialist to join our Data and Analytics team at RWS. This role combines advanced technical skills in SQL and ETL with analytical thinking and financial business acumen. You will be instrumental in designing and implementing data transformations and pipelines to support complex business requirements, particularly in financial data systems. This is a hands-on technical role with a significant focus on problem-solving, mathematical reasoning, and creating solutions for intricate business processes.\nThis position is ideal for someone with a quantitative background in computer science, mathematics, or a related field, who thrives on developing innovative solutions, optimizing data workflows, and ensuring data accuracy. We are looking for a candidate who is self-motivated, detail-oriented, and possesses exceptional technical skills, alongside a strong sense of ownership and accountability. The primary focus of this role will be financial data projects within a collaborative and dynamic team environment.\nKey Responsibilities\nWrite advanced T-SQL queries, stored procedures, and functions to handle complex data transformations, ensuring optimal performance and scalability.\nPerform query performance optimization, including indexing strategies and tuning for large datasets.\nDesign, develop, and maintain ETL/ELT pipelines using tools like SSIS, ADF, or equivalent, ensuring seamless data integration and transformation.\nBuild and automate data workflows to support accurate and efficient data operations.\nCollaborate with stakeholders to understand financial business processes and requirements.\nDevelop tailored data solutions to address challenges such as cost allocation, weighted distributions, and revenue recognition using mathematical and logical reasoning.\nConduct root cause analysis for data issues, resolving them swiftly to ensure data accuracy and reliability.\nApply critical thinking and problem-solving to transform business logic into actionable data workflows.\nPartner with cross-functional teams to ensure data solutions align with business objectives.\nDocument data workflows, pipelines, and processes to support ongoing maintenance and scalability.\nRequired Skills and Experiences\n\nMinimum 5+ years of experience require in the similar role or same skillset\nAdvanced proficiency in T-SQL and SQL Server, including tools like SSMS, SQL Profiler, and SQL Agent.\nProven experience developing and optimizing complex queries, stored procedures, and ETL pipelines.\nProficiency with SSIS for data integration and transformation.\nStrong critical thinking and mathematical reasoning skills to design efficient solutions for business problems.\nDemonstrated ability to handle complex financial data challenges, including proportional distributions and other mathematical transformations.\nSolid understanding of financial concepts and processes, enabling seamless translation of business needs into data solutions.\nAbility to work effectively with business stakeholders, translating requirements into technical solutions.\nExcellent interpersonal and communication skills, fostering collaboration across teams.\nFamiliarity with Python, C#, or similar scripting languages for backend integration.\nExperience with cloud ETL tools such as Azure Data Factory (ADF), Fivetran, or Airbyte.\nKnowledge of data governance and compliance principles.\n\nShift Timings-\nThere will be UK shift timings - 1:30 PM 9:30 PM (IST)\n\nRWS Values -\nGet the 3Ps right Partner, Pioneer, Progress and well Deliver together as One RWS.\nFor further information, please visit: RWS\nRWS embraces DEI and promotes equal opportunity, we are an Equal Opportunity Employer and prohibit discrimination and harassment of any kind. RWS is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at RWS are based on business needs, job requirements and individual qualifications, without regard to race, religion, nationality, ethnicity, sex, age, disability, or sexual orientation. RWS will not tolerate discrimination based on any of these characteristics\nRecruitment Agencies: RWS Holdings PLC does not accept agency resumes. Please do not forward any unsolicited resumes to any RWS employees. Any unsolicited resume received will be treated as the property of RWS and Terms & Conditions associated with the use of such resume will be considered null and void.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['T-SQL', 'Analytical Skills', 'Finance', 'SSIS', 'SQL', 'Critical Thinking', 'Azure', 'Analytical Ability', 'SSRS', 'ETL']",2025-06-13 05:18:02
Associate Data Engineer,Amgen Inc,0 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role We are seeking a Associate Data Engineer to design, build, and maintain scalable data solutions that drive business insights. You will work with large datasets, cloud platforms (AWS preferred), and big data technologies to develop ETL pipelines, ensure data quality, and support data governance initiatives.\nDevelop and maintain data pipelines, ETL/ELT processes, and data integration solutions.\nDesign and implement data models, data dictionaries, and documentation for accuracy and consistency.",,,,"['Data Engineering', 'data analysis', 'data modeling', 'data warehousing', 'data visualization', 'Databricks', 'ETL', 'AWS', 'SQL', 'Apache Spark', 'Python']",2025-06-13 05:18:04
Associate Data Engineer,Amgen Inc,0 - 2 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for an Associate Data Engineer with deep expertise in writing data pipelines to build scalable, high-performance data solutions. The ideal candidate will be responsible for developing, optimizing and maintaining complex data pipelines, integration frameworks, and metadata-driven architectures that enable seamless access and analytics. This role prefers deep understanding of the big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.",,,,"['Data Engineering', 'Maven', 'test automation', 'data engineering lifecycle', 'Scaled Agile methodologies', 'JIRA', 'SQL', 'Apache Spark', 'Jenkins', 'Agile DevOps tools', 'ETL platform', 'Confluence', 'Scaled Agile', 'Delta Lake', 'Agile', 'Databricks', 'AWS', 'Python']",2025-06-13 05:18:06
Software Engineer (Java + Big Data),Impetus Technologies,5 - 7 years,Not Disclosed,['Chennai'],"Job: Software Developer (Java + Big Data)\nLocation: Indore\nYears of experience: 5-7 years\n\nRequisition Description\n1. Problem solving and analytical skills\n2. Good verbal and written communication skills\n\nRoles and Responsibilities\n\n1. Design and develop high performance, scale-able applications with Java + Bigdata  as minimum required skill .\nJava, Microservices , Spring boot, API ,Bigdata-Hive, Spark, Pyspark\n2. Build and maintain efficient data pipelines to process large volumes of structured and unstructured data.\n3. Develop micro-services, API and distributed systems\n4. Worked on Spark, HDFS, CEPh, Solr/Elastic search, Kafka, Deltalake\n5. Mentor and Guide junior members",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Java', 'Hive', 'Big Data', 'Spring Boot', 'Microservices', 'Hdfs', 'Spark Streaming']",2025-06-13 05:18:08
"Data Engineer - Snowflake, Azure Data Factory (ADF)",Suzva Software Technologies,0 - 1 years,Not Disclosed,['Mumbai'],"We are seeking an experienced Data Engineer to join our team for a 6-month contract assignment. The ideal candidate will work on data warehouse development, ETL pipelines, and analytics enablement using Snowflake, Azure Data Factory (ADF), dbt, and other tools.\n\nThis role requires strong hands-on experience with data integration platforms, documentation, and pipeline optimizationespecially in cloud environments such as Azure and AWS.\n\n#KeyResponsibilities\nBuild and maintain ETL pipelines using Fivetran, dbt, and Azure Data Factory\n\nMonitor and support production ETL jobs\n\nDevelop and maintain data lineage documentation for all systems\n\nDesign data mapping and documentation to aid QA/UAT testing\n\nEvaluate and recommend modern data integration tools\n\nOptimize shared data workflows and batch schedules\n\nCollaborate with Data Quality Analysts to ensure accuracy and integrity of data flows\n\nParticipate in performance tuning and improvement recommendations\n\nSupport BI/MDM initiatives including Data Vault and Data Lakes\n\n#RequiredSkills\n7+ years of experience in data engineering roles\n\nStrong command of SQL, with 5+ years of hands-on development\n\nDeep experience with Snowflake, Azure Data Factory, dbt\n\nStrong background with ETL tools (Informatica, Talend, ADF, dbt, etc.)\n\nBachelor's in CS, Engineering, Math, or related field\n\nExperience in healthcare domain (working with PHI/PII data)\n\nFamiliarity with scripting/programming (Python, Perl, Java, Linux-based environments)\n\nExcellent communication and documentation skills\n\nExperience with BI tools like Power BI, Cognos, etc.\n\nOrganized, self-starter with strong time-management and critical thinking abilities\n\n#NiceToHave\nExperience with Data Lakes and Data Vaults\n\nQA & UAT alignment with clear development documentation\nMulti-cloud experience (especially Azure, AWS)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Java', 'Azure', 'Power BI', 'UAT', 'Perl', 'QA', 'Azure Data Factory', 'Linux', 'Cognos', 'Snowflake', 'ETL', 'AWS', 'Python']",2025-06-13 05:18:09
Lead Security Engineer (Data Protection),Flipkart,7 - 11 years,Not Disclosed,['Bengaluru'],"Skills Required :\n7+ years of experience in the field of information security. Strong technical expertise in DLP and data classification methodologies\nDesirable Skills :\nWork done in AI, automations",Industry Type: Courier / Logistics,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['AI', 'automations', 'Data Protection', 'information security', 'DLP']",2025-06-13 05:18:11
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\nRoles & Responsibilities:- Assist with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data pipelines and ETL processes using Databricks Unified Data Analytics Platform.- Design and implement data security and access controls for the data platform.- Troubleshoot and resolve issues related to the data platform and data pipelines.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nExperience with Databricks Unified Data Analytics Platform.- Must To Have\n\n\n\n\nSkills:\nStrong understanding of data modeling and database design principles.- Good To Have\n\n\n\n\nSkills:\nExperience with cloud-based data platforms such as AWS or Azure.- Good To Have\n\n\n\n\nSkills:\nExperience with data security and access controls.- Good To Have\n\n\n\n\nSkills:\nExperience with data visualization tools such as Tableau or Power BI.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions.- This position is based at our Bangalore, Hyderabad, Chennai and Pune Offices.- Mandatory office (RTO) for 2- 3 days and have to work on 2 shifts (Shift A- 10:00am to 8:00pm IST and Shift B - 12:30pm to 10:30 pm IST)\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['database design', 'data modeling', 'data analytics', 'microsoft azure', 'design principles', 'hive', 'sql', 'java', 'spark', 'design patterns', 'oops', 'mysql', 'hadoop', 'etl', 'big data', 'c#', 'rest', 'python', 'data security', 'power bi', 'javascript', 'sql server', 'data bricks', 'tableau', 'kafka', 'sqoop', 'aws']",2025-06-13 05:18:13
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in shaping the data platform components.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Lead the implementation of data platform solutions.- Conduct performance tuning and optimization of data platform components.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of cloud-based data platforms.- Experience in designing and implementing data pipelines.- Knowledge of data governance and security best practices.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['hive', 'data analytics', 'data modeling', 'spark', 'data governance', 'python', 'amazon redshift', 'data warehousing', 'microsoft azure', 'emr', 'machine learning', 'sql', 'nosql', 'amazon ec2', 'java', 'kafka', 'mysql', 'hadoop', 'sqoop', 'big data', 'aws', 'etl']",2025-06-13 05:18:15
Data Platform Engineer,Accenture,12 - 15 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Collibra Data Governance\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, encompassing the relevant data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models, while also engaging in discussions to refine and enhance the overall data architecture. You will be involved in various stages of the data platform lifecycle, ensuring that all components work harmoniously to support the organization's data needs and objectives.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Facilitate knowledge sharing sessions to enhance team capabilities and foster a culture of continuous improvement.- Monitor and evaluate team performance, providing constructive feedback to ensure alignment with project goals.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Collibra Data Governance.- Strong understanding of data governance frameworks and best practices.- Experience with data integration tools and techniques.- Familiarity with data modeling concepts and methodologies.- Ability to analyze and interpret complex data sets to inform decision-making.\nAdditional Information:- The candidate should have minimum 12 years of experience in Collibra Data Governance.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'data architecture', 'sql', 'data modeling', 'data governance', 'data analysis', 'oracle', 'data management', 'data warehousing', 'business analysis', 'machine learning', 'business intelligence', 'javascript', 'sql server', 'data quality', 'tableau', 'java', 'html', 'mysql', 'etl', 'informatica']",2025-06-13 05:18:17
Data Platform Engineer,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in shaping the data platform components.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with Integration Architects and Data Architects to design and implement data platform components.- Ensure seamless integration between various systems and data models.- Develop and maintain data platform blueprints.- Optimize data platform performance and scalability.- Provide technical guidance and support to team members.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of data platform architecture and design principles.- Experience with cloud-based data platforms like AWS or Azure.- Hands-on experience with data integration tools and technologies.- Knowledge of data governance and security best practices.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'microsoft azure', 'platform architecture', 'design principles', 'aws', 'kubernetes', 'c++', 'oracle', 'enterprise architecture', 'microservices', 'docker', 'infrastructure architecture', 'java', 'data modeling', 'gcp', 'design patterns', 'data governance', 'agile', 'hadoop']",2025-06-13 05:18:18
Hadoop & UNIX Shell Scripting Engineer (Data Management),Synechron,5 - 10 years,Not Disclosed,['Bengaluru'],"Job Summary\nSynechron is seeking a dedicated and technically skilled Hadoop Shell Scripting Engineer to manage and optimize our Hadoop ecosystem. The role involves developing automation utilities, troubleshooting complex issues, and collaborating with vendors for platform enhancements. Your expertise will directly support enterprise data processing, performance tuning, and cloud migration initiatives, ensuring reliable and efficient data infrastructure that aligns with organizational goals.\nSoftware Requirements\nRequired Skills:\nStrong proficiency in UNIX Shell scripting with hands-on experience in developing automation utilities\nIn-depth understanding of Hadoop architecture and ecosystem components (HDFS, Hive, Spark)\nExperience with SQL querying and database systems\nFamiliarity with Git and enterprise version control practices\nWorking knowledge of DevOps and CI/CD tools and processes\nPreferred Skills:\nExperience with Python scripting for automation and utility development\nKnowledge of Java programming language\nFamiliarity with cloud platforms (AWS, Azure, GCP) related to Hadoop ecosystem support\nExposure to Hadoop vendor support and collaboration processes (e.g., Cloudera)\nOverall Responsibilities\nDevelop, maintain, and enhance scripts and utilities to automate Hadoop cluster management and data processing tasks\nServe as the Level 3 point of contact for issues related to Hadoop and Spark platforms\nPerform performance tuning and capacity planning to support enterprise data workloads\nConduct proof-of-concept tests for emerging technologies and evaluate their suitability for cloud migration projects\nCollaborate with vendor support teams and internal stakeholders for issue resolution, feature requests, and platform improvements\nReview and validate all changes going into production to ensure stability and performance\nContinuously analyze process inefficiencies and develop new automation utilities to enhance productivity\nAssist in capacity management and performance monitoring activities\nTechnical Skills (By Category)\nProgramming Languages:\nEssential: UNIX Shell scripting\nPreferred: Python, Java\nDatabases & Data Management:\nEssential: Knowledge of SQL querying and database systems\nPreferred: Experience with Hive, HDFS\nCloud Technologies:\nPreferred: Basic familiarity with cloud platforms for Hadoop ecosystem support and migration\nFrameworks & Libraries:\nNot specifically applicable; focus on scripting and platform tools\nDevelopment Tools & Methodologies:\nEssential: Git, version control, DevOps practices, CI/CD pipelines\nPreferred: Automation frameworks, monitoring tools\nSecurity Protocols:\nNot explicitly specified but familiarity with secure scripting and data access controls is advantageous\nExperience Requirements\nMinimum of 5+ years of hands-on experience working with Hadoop clusters and scripting in UNIX shell\nProven experience in managing enterprise Hadoop/Spark environments\nExperience in performance tuning, capacity planning, and utility development\nExposure to cloud migrations or proof-of-concept evaluations is a plus\nBackground in data engineering or platform support roles preferred\nDay-to-Day Activities\nDevelop and enhance UNIX shell scripts for Hadoop automation and utility management\nTroubleshoot and resolve complex platform issues as the Level 3 point of contact\nWork with application teams to optimize queries and data workflows\nEngage with vendor support teams for platform issues and feature requests\nPerform system performance reviews, capacity assessments, and tuning activities\nLead initiatives for process automation, efficiency improvement, and new technology evaluations\nDocument procedures, scripts, and platform configurations\nParticipate in team meetings, provide technical feedback, and collaborate across teams on platform health\nQualifications\nEducational Requirements:\nBachelor's degree in Computer Science, Information Technology, or related field\nEquivalent professional experience in data engineering, platform support, or Hadoop administration\nCertifications (Preferred):\nCertificates in Hadoop ecosystem, Linux scripting, or cloud platform certifications\nTraining & Professional Development:\nOngoing learning related to big data platforms, automation, and cloud migration\nProfessional Competencies\nStrong analytical and troubleshooting skills\nExcellent written and verbal communication skills\nProven ability to work independently with minimal supervision\nCollaborative team player with a positive attitude\nAbility to prioritize tasks effectively and resolve issues swiftly\nAdaptability to evolving technologies and environments\nFocus on quality, security, and process improvement",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hadoop', 'Java', 'Automation', 'DevOps', 'Data Management', 'CI/CD', 'Performance Tuning', 'UNIX Shell Scripting', 'Python', 'SQL']",2025-06-13 05:18:20
Data Platform Engineer,Accenture,5 - 10 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data bricks', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-13 05:18:22
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data bricks', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-13 05:18:24
Data Platform Engineer,Accenture,5 - 10 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :A Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:Overall 7+ years of experience In Industry including 4 Years of experience As Developer using Big Data Technologies like Databricks/Spark and Hadoop Ecosystems - Hands on experience on Unified Data Analytics with Databricks, Databricks Workspace User Interface, Managing Databricks Notebooks, Delta Lake with Python, Delta Lake with Spark SQL - Good understanding of Spark Architecture with Databricks, Structured Streaming.\nSetting Up cloud platform with Databricks, Databricks Workspace- Working knowledge on distributed processing, data warehouse concepts, NoSQL, huge amount of data processing, RDBMS, Testing, Data management principles, Data mining and Data modellingAs a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\nRoles & Responsibilities:- Assist with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data pipelines using Databricks Unified Data Analytics Platform.- Troubleshoot and resolve issues related to data pipelines and data platform components.- Ensure data quality and integrity by implementing data validation and testing procedures.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nExperience with Databricks Unified Data Analytics Platform.- Must To Have\n\n\n\n\nSkills:\nStrong understanding of data modeling and database design principles.- Good To Have\n\n\n\n\nSkills:\nExperience with Apache Spark and Hadoop.- Good To Have\n\n\n\n\nSkills:\nExperience with cloud-based data platforms such as AWS or Azure.- Proficiency in programming languages such as Python or Java.- Experience with data integration and ETL tools such as Apache NiFi or Talend.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Databricks Unified Data Analytics Platform.- The ideal candidate will possess a strong educational background in computer science, software engineering, or a related field, along with a proven track record of delivering impactful data-driven solutions.- This position is based at our Chennai, Bengaluru, Hyderabad and Pune office.\n\nQualification\n\nA Engineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'software engineering', 'design principles', 'python', 'rdbms', 'data management', 'talend', 'microsoft azure', 'nosql', 'spark programming', 'data bricks', 'data quality', 'java', 'apache nifi', 'spark', 'hadoop', 'big data', 'aws', 'etl', 'data integration']",2025-06-13 05:18:26
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data bricks', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-13 05:18:28
Specialist Data/AI Engineering,ATT Communication Services,4 - 9 years,Not Disclosed,['Bengaluru'],"Key Roles and Responsibilities\nDevelop, enhance, and support assigned applications, ensuring seamless functionality and high performance.\nProvide subject matter expertise, acting as a go-to resource for application-specific knowledge.\nCollaborate and communicate effectively with team members, stakeholders, and end-users.\nTroubleshoot issues, monitor performance, and optimize processes for continuous improvement.\nStay current with industry trends and share best practices with the team.\nAdhere to company policies, procedures, and security requirements while maintaining compliance standards.\nFlexible with shifts and occasional weekend support.\nKey Competencies\nFull life-cycle experience on enterprise software development projects.\nExperience in relational databases/ data marts/data warehouses and complex SQL programming.\nExtensive experience in ETL, shell or python scripting, data modelling, analysis, and preparation\nExperience in Unix/Linux system, files systems, shell scripting.\nGood to have knowledge on any cloud platforms like Azure, Databricks etc.\nGood to have experience in BI Reporting tools Power BI\nGood problem-solving and analytical skills used to resolve technical problems.\nMust possess a good understanding of Compliance and Security Standards (preferably US standards).\nAbility to work independently but must be a team player. Should be able to drive business decisions and take ownership of their work.\nExperience in presentation design, development, delivery, and good communication skills to present analytical results and recommendations for action-oriented data driven decisions and associated operational and financial impacts.\nRequired/Desired Skills\nApplication: Databricks\nLanguages: Python, Shell Scripting, PLSQL, SQL\nCloud Technologies: Azure.\nTools: Jupyter Notebooks, SQL Developer, Postman, IDE.\nDatabase: Oracle, Snowflake and SQL Server.\nDevops: Jenkins, Kubernetes and Docker.\nBachelor s degree in computer science, Information Systems or related field.\n4+ years of experience in working in Engineering or Development roles with Compliance Standards\n4+ years of experience building high transaction defensive web applications and Python applications\n2+ years of experience in cloud technologies: AWS, Azure, OpenStack, Ansible, Chef or Terraform\n2+ years of experience in creating application for container services Docker, Kubernetes,\n2+ years of experience in build and CICD technologies: GitHub, Maven, Jenkins, Nexus or Sonar\nProficiency in Unix/Linux command line\n1+ years of experience in building applications using Large Language Models.\n1+ years of experience in building intelligent automation using Power Automate\n1+ years of experience in using Agentic framework\n1+ years of experience in building accurate prompts for AI tools.\nExpert knowledge and experience working with asynchronous message processing, stream processing and event driven computing.\nExperience working within Agile/Scrum/Kanban development team\nFamiliarity with HTML5, JavaScript frameworks, and CSS3\nCertified in Java, Spring or Azure technologies\nExcellent written and verbal communication skills with demonstrated ability to present complex technical information in a clear manner to peers, developers, and senior leaders\nEducation & Qualifications\nUniversity Degree in Computer Science and/or Analytics\nMinimum Experience required: 6-9 years in relational database design & development, ETL development, GenAI\nAdditional Details\nShift timing (if any): 12.30 to 9.30 IST(Bangalore)\nWork mode: Hybrid (3 days mandatory in office)\nLocation: Bangalore\n#DataPlatform\n#DataScience\nJob ID R-61633 Date posted 06/03/2025",Industry Type: Telecom / ISP,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Computer science', 'Automation', 'Linux', 'Database design', 'Shell scripting', 'Javascript', 'Design development', 'SQL', 'Python']",2025-06-13 05:18:30
Graph Engineer- Data Science,HARMAN,4 - 9 years,Not Disclosed,['Bengaluru'],"Job Description\nIntroduction: Digital Transformation Solutions (DTS)\n.\nExtensive experience in defining, developing, and implementing security software, ideally with a strong embedded firmware development background\nAbout the Role\nThis position offers an opportunity to work in a globally distributed team where you will get a unique opportunity of personal development in a multi-cultural environment. You will also get a challenging environment to develop expertise in the technologies useful in the industry.",,,,"['Computer science', 'Product quality', 'UML', 'XML', 'Relationship', 'Javascript', 'HTML', 'Oracle', 'Automotive', 'Python']",2025-06-13 05:18:32
Site Reliability Engineer - Cloud & Data Center Networking,IBM,5 - 10 years,Not Disclosed,['Bengaluru'],"- Manage, optimise and resolve issues observed in the VPC environment through detailed debugging\n\n- Manage extremely good timelines for the events and actions taken in our incident records for future reference for improvements\n\n- Manage and optimize underlay network infrastructure including routing, switching, and physical connectivity in data centers\n\n- Collaborate with cloud architects to troubleshoot and resolve networking issues across the Cloud Infrastructure\n\n- Monitor network performance and proactively resolve issues using tools like Splunk, AppNeta or equivalent\n\n- Document procedures and call out anomalies when observed during run-book executions\n\n- Also improve run-books as and when issues are discovered\n\n- Participate in on-call rotations and incident response as needed\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n- Bachelor’s degree in Computer Science, Information Technology, or related field (or equivalent experience).\n\n- 5+ years of experience in enterprise networking, with a strong focus on both cloud and data center networking environments\n\n- Proficiency in overlay technologiesVXLAN, NVGRE, VPC\n\n- Strong understanding of underlay technologiesBGP, OSPF, MPLS, Ethernet, Spine-Leaf architectures\n\n- Relevant certifications (e.g., CCNP, CCIE, AWS Advanced Networking) are a plus\n\n\nPreferred technical and professional experience\n\n- Exposure to container networking (Kubernetes)\n\n- Familiarity with network automation tools (e.g., Ansible, Terraform, Python scripting)",Industry Type: IT Services & Consulting,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['networking', 'ospf', 'ethernet', 'data center', 'mpls', 'container', 'kubernetes', 'python', 'network infrastructure', 'reliability', 'site reliability engineering', 'ansible', 'docker', 'automation tools', 'java', 'devops', 'linux', 'jenkins', 'splunk', 'cloud infrastructure', 'debugging', 'terraform', 'aws']",2025-06-13 05:18:33
Data Platform Engineer,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Hyderabad office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'data analysis', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-13 05:18:35
Data Platform Engineer,Accenture,3 - 8 years,Not Disclosed,['Chennai'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, encompassing the relevant data platform components. You will collaborate with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Your typical day will involve working on the data platform blueprint and design, collaborating with architects, and ensuring seamless integration between systems and data models.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects.- Ensure cohesive integration between systems and data models.- Implement data platform components.- Troubleshoot and resolve data platform issues.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'data architecture', 'machine learning', 'sql', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining']",2025-06-13 05:18:37
AM Client Workflows & Ref Data - Associate - Software Engineering,Goldman Sachs,2 - 5 years,Not Disclosed,['Bengaluru'],"Work with a global team of highly motivated platform engineers and software developers building integrated architectures for secure, scalable infrastructure services serving a diverse set of use cases.\nPartner with colleagues from across technology and risk to ensure an outstanding platform is delivered.\nHelp to provide frictionless integration with the firm s runtime, deployment and SDLC technologies.\nCollaborate on feature design and problem solving.\nHelp to ensure reliability, define, measure, and meet service level objectives.\nQuality coding & integration, testing, release, and demise of software products supporting AWM functions.\nEngage in quality assurance and production troubleshooting.\nHelp to communicate and promote best practices for software engineering across the Asset Management tech stack.\nBasic Qualifications\nA strong grounding in software engineering concepts and implementation of architecture design patterns.\nA good understanding of multiple aspects of software development in microservices architecture, full stack development experience, Identity / access management and technology risk.\nSound SDLC and practices and tooling experience - version control, CI/CD and configuration management tools.\nAbility to communicate technical concepts effectively, both written and orally, as we'll as interpersonal skills required to collaborate effectively with colleagues across diverse technology teams.\nExperience meeting demands for high availability and scalable system requirements.\nAbility to reason about performance, security, and process interactions in complex distributed systems.\nAbility to understand and effectively debug both new and existing software.\nExperience with metrics and monitoring tooling, including the ability to use metrics to rationally derive system health and availability information.\nExperience in auditing and supporting software based on sound SRE principles.\nPreferred Qualifications\n3+ Years of Experience using and/or supporting Java based frameworks & SQL / NOSQL data stores.\nExperience with deploying software to containerized environments - Kubernetes/Docker.\nScripting skills using Python, Shell or bash.\nExperience with Terraform or similar infrastructure-as-code platforms.\nExperience building services using public cloud providers such as AWS, Azure or GCP.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Manager Quality Assurance', 'Coding', 'Configuration management', 'Investment banking', 'Asset management', 'Troubleshooting', 'SDLC', 'Monitoring', 'SQL', 'Python']",2025-06-13 05:18:39
"Associate Director, Data Science/Software Engineering",ATT Communication Services,10 - 15 years,Not Disclosed,['Bengaluru'],"Associate Director, Data Science/Software Engineering:\nAT&T is one of the leading service providers in the telecommunication sector and propelling it into the data and AI driven era is powered by CDO (Chief Data Office) . CDO is empowering AT&T, through execution, self-service, and as a data and AI center of excellence, to unlock transformative insights and actions that drive value for the company and its customers.\nEmployees in CDO imagine, innovate, and unlock data & AI driven insights and actions that create value for our customers and the enterprise. Part of the work, we govern data collection and use, mitigate for potential bias in machine learning models, and encourage an enterprise culture of responsible AI.\nAT&T s Chief Data Office (CDO) is harnessing data and making AT&T s data assets and ground-breaking AI functionality accessible to employees across the firm. In addition, our talented employees are a significant component that contributes to AT&T s place as the U.S. company with the sixth most AI-related patents. CDO also maintains academic and tech partnerships to cultivate the next generation of experts in statistics and machine learning, statistical computing, data visualization, text mining, time series modelling, data stream and database management, data quality and anomaly detection, data privacy, and more.\nWe are looking for an accomplished and visionary professional for the role of Associate Director, Data Science/Software Engineering to join our team and lead the development of cutting-edge software solutions. This is a hands-on leadership position that requires the fine balance of supervising and leading people while providing significant technical contributions to the projects you will be responsible for. As a key technical leader, you will leverage your expertise in full-stack development, DevOps best practices, Data analysis, AI/ML and Generative AI to lead your team in creating scalable, reliable, and efficient systems.\nThis role demands a strategic thinker and hands-on contributor who can work across multiple teams, drive innovation, and ensure technical excellence. You will be instrumental in shaping the technical roadmap, mentoring teams, and delivering transformative solutions that align with business objectives.\nKey Responsibilities:\nTechnical Leadership:\nDefine and drive the technical vision and architecture for scalable, resilient, and secure full-stack applications utilizing data powered insights.\nLead end-to-end software development projects from concept to deployment and maintenance.\nCollaborate with cross-functional teams to translate business requirements into technical solutions.\nServe as a mentor and technical advisor to engineering teams, fostering a culture of innovation and excellence.\nFull-Stack Development:\nDesign and implement scalable and high-performance web applications using modern front-end and back-end frameworks (e.g., React, Angular, Node.js, Python, Java).\nDevelop modular and reusable APIs (RESTful or GraphQL) with an emphasis on maintainability and performance.\nEnsure seamless integration of front-end and back-end systems while maintaining best practices for UI/UX design.\nOptimize database structures and queries for both relational (e.g., MySQL, PostgreSQL) and non-relational (e.g., MongoDB, DynamoDB) databases.\nDevOps and Automation:\nArchitect and implement CI/CD pipelines to streamline build, test, and deployment processes.\nEnsure seamless deployment and scalability of applications through containerization tools (e.g., Docker) and orchestration platforms (e.g., Kubernetes).\nLeverage infrastructure-as-code solutions (e.g., Terraform, Ansible) to automate infrastructure provisioning and management.\nMonitor application performance, troubleshoot issues, and ensure high availability through tools like Prometheus, Grafana, or New Relic.\nShell Scripting and Automation:\nDevelop and maintain shell scripts to automate routine tasks, system monitoring, and application deployments.\nDebug and troubleshoot production issues using scripting techniques to ensure minimal downtime.\nEnhance system efficiency by automating log analysis, error detection, and reporting.\nStrategic Contribution:\nCollaborate with stakeholders to align technical priorities with business goals.\nEvaluate emerging technologies and tools to recommend and implement solutions that advance the organization s technical capabilities.\nEstablish and enforce software engineering best practices, ensuring robust security, scalability, and maintainability.\nQualifications:\nEducation:\nBachelor s or Master s degree in Computer Science, Software Engineering, or a related field. A Ph.D. is a plus.\nExperience:\n13+ years of experience in software engineering, including hands-on experience with full-stack development and DevOps practices.\nProven track record of delivering large-scale, high-impact software solutions in a leadership capacity.\nTechnical Expertise:\nAdvanced proficiency in front-end frameworks (React, Angular, or Vue.js) and back-end technologies (Node.js, Python, Java, Go, etc.).\nStrong experience with DevOps tools (Jenkins, GitLab CI/CD, Docker, Kubernetes).\nDeep understanding of cloud platforms (AWS, Azure, GCP), including architecture and deployment strategies.\nSolid grasp of database technologies (SQL and NoSQL) and optimization techniques.\nProficiency in writing, debugging, and maintaining shell scripts for automation and system monitoring.\nStrong knowledge of microservices architecture, API gateways, and distributed systems.\nSoft Skills:\nExceptional problem-solving and critical-thinking abilities.\nStrong leadership and mentoring skills, with the ability to inspire and guide teams.\nExcellent communication skills, both written and verbal, to collaborate effectively with technical and non-technical stakeholders.\nStrategic mindset, capable of balancing technical depth with business impact.\nPreferred Qualifications:\nExperience with serverless computing frameworks (e.g., AWS Lambda).\nCertifications in cloud platforms (e.g., AWS Certified Solutions Architect, Azure DevOps Engineer Expert).\nKnowledge of security best practices in software development and DevOps.\n#DataEngineering\nLocation:\nIND:KA:Bengaluru / Innovator Building, Itpb, Whitefield Rd - Adm: Intl Tech Park, Innovator Bldg\nJob ID R-66889 Date posted 05/14/2025",Industry Type: Telecom / ISP,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Data analysis', 'Front end', 'Postgresql', 'MySQL', 'Shell scripting', 'Telecommunication', 'SQL', 'Python']",2025-06-13 05:18:41
Lead Data Engineer,Moreyeahs,7 - 9 years,Not Disclosed,['Indore'],"We are seeking a highly skilled and experienced Lead Data Engineer (7+ years) to join our dynamic team. As a Lead Data Engineer, you will play a crucial role in designing, developing, and maintaining our data infrastructure. You will be responsible for ensuring the efficient and reliable collection, storage, and transformation of large-scale data to support business intelligence, analytics, and data-driven decision-making.\n\nKey Responsibilities :\n\nData Architecture & Design :\n- Lead the design and implementation of robust data architectures that support data warehousing (DWH), data integration, and analytics platforms.\n- Develop and maintain ETL (Extract, Transform, Load) pipelines to ensure the efficient processing of large datasets.\n\nETL Development :\n- Design, develop, and optimize ETL processes using tools like Informatica Power Center, Intelligent Data Management Cloud (IDMC), or custom Python scripts.\n- Implement data transformation and cleansing processes to ensure data quality and consistency across the enterprise.\n\nData Warehouse Development :\n- Build and maintain scalable data warehouse solutions using Snowflake, Databricks, Redshift, or similar technologies.\n\n- Ensure efficient storage, retrieval, and processing of structured and semi-structured data.\n\nBig Data & Cloud Technologies :\n- Utilize AWS Glue and PySpark for large-scale data processing and transformation.\n- Implement and manage data pipelines using Apache Airflow for orchestration and scheduling.\n- Leverage cloud platforms (AWS, Azure, GCP) for data storage, processing, and analytics.\n\nData Management & Governance :\n- Establish and enforce data governance and security best practices.\n- Ensure data integrity, accuracy, and availability across all data platforms.\n- Implement monitoring and alerting systems to ensure data pipeline reliability.\n\nCollaboration & Leadership :\n\n- Work closely with data Stewards, analysts, and business stakeholders to understand data requirements and deliver solutions that meet business needs.\n- Mentor and guide junior data engineers, fostering a culture of continuous learning and development within the team.\n- Lead data-related projects from inception to delivery, ensuring alignment with business objectives and timelines.\n\nDatabase Management :\n- Design and manage relational databases (RDBMS) to support transactional and analytical workloads.\n- Optimize SQL queries for performance and scalability across various database platforms.\n\nRequired Skills & Qualifications :\nEducation: Bachelors or Masters degree in Computer Science, Information Systems, Engineering, or a related field.\n\nExperience :\n- Minimum of 7+ years of experience in data engineering, ETL, and data warehouse development.\n- Proven experience with ETL tools like Informatica Power Center or IDMC.\n- Strong proficiency in Python and PySpark for data processing.\n- Experience with cloud-based data platforms such as AWS Glue, Snowflake, Databricks, or Redshift.\n- Hands-on experience with SQL and RDBMS platforms (e.g., Oracle, MySQL, PostgreSQL).\n- Familiarity with data orchestration tools like Apache Airflow.\nTechnical Skills :\n- Advanced knowledge of data warehousing concepts and best practices.\n- Strong understanding of data modeling, schema design, and data governance.\n- Proficiency in designing and implementing scalable ETL pipelines.\n- Experience with cloud infrastructure (AWS, Azure, GCP) for data storage and processing.\n\nSoft Skills :\n- Excellent communication and collaboration skills.\n- Ability to lead and mentor a team of engineers.\n- Strong problem-solving and analytical thinking abilities.\n- Ability to manage multiple projects and prioritize tasks effectively.\n\nPreferred Qualifications :\n- Experience with machine learning workflows and data science tools.\n- Certification in AWS, Snowflake, Databricks, or relevant data engineering technologies.\n- Experience with Agile methodologies and DevOps practices.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data modeling', 'RDBMS', 'MySQL', 'Agile', 'Informatica', 'Oracle', 'Apache', 'Business intelligence', 'Analytics', 'Python']",2025-06-13 05:18:43
Lead Data Engineer,Shyftlabs,5 - 10 years,Not Disclosed,['Noida'],"Position Overview\nWe are looking for an experienced Lead Data Engineer to join our dynamic team. If you are passionate about building scalable software solutions, and work collaboratively with cross-functional teams to define requirements and deliver solutions we would love to hear from you.\nJob Responsibilities:\nDevelop and maintain data pipelines and ETL/ELT processes using Python\nDesign and implement scalable, high-performance applications\nWork collaboratively with cross-functional teams to define requirements and deliver solutions\nDevelop and manage near real-time data streaming solutions using Pub, Sub or Beam.\nContribute to code reviews, architecture discussions, and continuous improvement initiatives\nMonitor and troubleshoot production systems to ensure reliability and performance\nBasic Qualifications:\n5+ years of professional software development experience with Python\nStrong understanding of software engineering best practices (testing, version control, CI/CD)\nExperience building and optimizing ETL/ELT processes and data pipelines\nProficiency with SQL and database concepts\nExperience with data processing frameworks (e.g., Pandas)\nUnderstanding of software design patterns and architectural principles\nAbility to write clean, well-documented, and maintainable code\nExperience with unit testing and test automation\nExperience working with any cloud provider (GCP is preferred)\nExperience with CI/CD pipelines and Infrastructure as code\nExperience with Containerization technologies like Docker or Kubernetes\nBachelors degree in Computer Science, Engineering, or related field (or equivalent experience)\nProven track record of delivering complex software projects\nExcellent problem-solving and analytical thinking skills\nStrong communication skills and ability to work in a collaborative environment\nPreferred Qualifications:\nExperience with GCP services, particularly Cloud Run and Dataflow\nExperience with stream processing technologies (Pub/Sub)\nFamiliarity with big data technologies (Airflow)\nExperience with data visualization tools and libraries\nKnowledge of CI/CD pipelines with Gitlab and infrastructure as code with Terraform\nFamiliarity with platforms like Snowflake, Bigquery or Databricks,.\nGCP Data engineer certification",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Software design', 'Version control', 'Analytical', 'Data processing', 'Unit testing', 'data visualization', 'Continuous improvement', 'SQL', 'Python']",2025-06-13 05:18:45
Lead Data Engineer,anblicks,6 - 8 years,Not Disclosed,['Ahmedabad'],"Job Summary:\nWe are seeking a Senior Data Engineer with hands-on experience building scalable data pipelines using Microsoft Fabric. The role focuses on delivering ingestion, transformation, and enrichment workflows across medallion architecture.\nKey Responsibilities:\nDevelop and maintain data pipelines using Microsoft Fabric Data Factory and OneLake.\nDesign and build ingestion and transformation pipelines for structured and unstructured data.\nImplement frameworks for metadata tagging, version control, and batch tracking.\nEnsure security, quality, and compliance of data pipelines.\nContribute to CI/CD integration, observability, and documentation.\nCollaborate with data architects and analysts to meet business requirements.\nQualifications:\n6+ years of experience in data engineering; 2+ years working on Microsoft Fabric or Azure Data services.\nHands-on with tools like Azure Data Factory, Fabric, Databricks, or Synapse.\nStrong SQL and data processing skills (e.g., PySpark, Python).\nExperience with data cataloging, lineage, and governance frameworks.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['metadata', 'data services', 'Version control', 'Compliance', 'Architecture', 'Data processing', 'microsoft', 'SQL', 'Python']",2025-06-13 05:18:47
Lead Data Engineer (Immediate joiner),Decision Point,4 - 9 years,15-30 Lacs P.A.,"['Gurugram', 'Chennai']","Role & responsibilities\n• Assume ownership of Data Engineering projects from inception to completion.\nImplement fully operational Unified Data Platform solutions in production environments using technologies like Databricks, Snowflake, Azure Synapse etc.\nShowcase proficiency in Data Modelling and Data Architecture\nUtilize modern data transformation tools such as DBT (Data Build Tool) to streamline and automate data pipelines (nice to have).",,,,"['Pyspark', 'Azure Databricks', 'SQL', 'Azure Synapse', 'Python', 'Etl Pipelines', 'Airflow', 'Bigquery', 'Advance Sql', 'Azure Cloud', 'GCP', 'Data Modeling', 'Data Architecture', 'AWS']",2025-06-13 05:18:48
Lead Data Engineer - Azure,Blend360 India,7 - 12 years,Not Disclosed,['Hyderabad'],"As a Sr Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\n7+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar field\nMust have experience e",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-13 05:18:50
Lead Data Engineer,Prolegion,8 - 12 years,Not Disclosed,['Hyderabad'],"Job Summary:\nWe are seeking a highly skilled Lead Data Engineer/Associate Architect to lead the design, implementation, and optimization of scalable data architectures. The ideal candidate will have a deep understanding of data modeling, ETL processes, cloud data solutions, and big data technologies. You will work closely with cross-functional teams to build robust, high-performance data pipelines and infrastructure to enable data-driven decision-making.\n\nExperience: 8 - 12+ years\nWork Location: Hyderabad (Hybrid)\nMandatory skills: Python, SQL, Snowflake\n\nResponsibilities:\nDesign and Develop scalable and resilient data architectures that support business needs, analytics, and AI/ML workloads.\nData Pipeline Development: Design and implement robust ETL/ELT processes to ensure efficient data ingestion, transformation, and storage.\nBig Data & Cloud Solutions: Architect data solutions using cloud platforms like AWS, Azure, or GCP, leveraging services such as Snowflake, Redshift, BigQuery, and Databricks.\nDatabase Optimization: Ensure performance tuning, indexing strategies, and query optimization for relational and NoSQL databases.\nData Governance & Security: Implement best practices for data quality, metadata management, compliance (GDPR, CCPA), and security.\nCollaboration & Leadership: Work closely with data engineers, analysts, and business stakeholders to translate business requirements into scalable solutions.\nTechnology Evaluation: Stay updated with emerging trends, assess new tools and frameworks, and drive innovation in data engineering.\n\nRequired Skills:\nEducation: Bachelors or Masters degree in Computer Science, Data Engineering, or a related field.\nExperience: 8 - 12+ years of experience in data engineering\nCloud Platforms: Strong expertise in AWS data services.\nBig Data Technologies: Experience with Hadoop, Spark, Kafka, and related frameworks.\nDatabases: Hands-on experience with SQL, NoSQL, and columnar databases such as PostgreSQL, MongoDB, Cassandra, and Snowflake.\nProgramming: Proficiency in Python, Scala, or Java for data processing and automation.\nETL Tools: Experience with tools like Apache Airflow, Talend, DBT, or Informatica.\nMachine Learning & AI Integration (Preferred): Understanding of how to architect data solutions for AI/ML applications\n\n,",Industry Type: Defence & Aerospace,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Performance tuning', 'Automation', 'Data modeling', 'Postgresql', 'Informatica', 'Apache', 'Analytics', 'SQL', 'Python']",2025-06-13 05:18:52
Data Engineering Lead,Yotta Techports,10 - 15 years,30-35 Lacs P.A.,['Hyderabad'],"Responsibilities:\nLead and manage an offshore team of data engineers, providing strategic guidance, mentorship, and support to ensure the successful delivery of projects and the development of team members.\nCollaborate closely with onshore stakeholders to understand project requirements, allocate resources efficiently, and ensure alignment with client expectations and project timelines.\nDrive the technical design, implementation, and optimization of data pipelines, ETL processes, and data warehouses, ensuring scalability, performance, and reliability.\nDefine and enforce engineering best practices, coding standards, and data quality standards to maintain high-quality deliverables and mitigate project risks.\nStay abreast of emerging technologies and industry trends in data engineering, and provide recommendations for tooling, process improvements, and skill development.\nAssume a data architect role as needed, leading the design and implementation of data architecture solutions, data modeling, and optimization strategies.\nDemonstrate proficiency in AWS services such as:\nExpertise in cloud data services, including AWS services like Amazon Redshift, Amazon EMR, and AWS Glue, to design and implement scalable data solutions.\nExperience with cloud infrastructure services such as AWS EC2, AWS S3, to optimize data processing and storage.\nKnowledge of cloud security best practices, IAM roles, and encryption mechanisms to ensure data privacy and compliance.\nProficiency in managing or implementing cloud data warehouse solutions, including data modeling, schema design, performance tuning, and optimization techniques.\nDemonstrate proficiency in modern data platforms such as Snowflake and Databricks, including:\nDeep understanding of Snowflake's architecture, capabilities, and best practices for designing and implementing data warehouse solutions.\nHands-on experience with Databricks for data engineering, data processing, and machine learning tasks, leveraging Spark clusters for scalable data processing.\nAbility to optimize Snowflake and Databricks configurations for performance, scalability, and cost-effectiveness.\nManage the offshore team's performance, including resource allocation, performance evaluations, and professional development, to maximize team productivity and morale.\n\nQualifications:\nBachelor's degree in Computer Science, Engineering, or a related field; advanced degree preferred.\n10+ years of experience in data engineering, with a proven track record of leadership and technical expertise in managing complex data projects.\nProficiency in programming languages such as Python, Java, or Scala, as well as expertise in SQL and relational databases (e.g., PostgreSQL, MySQL).\nStrong understanding of distributed computing, cloud technologies (e.g., AWS), and big data frameworks (e.g., Hadoop, Spark).\nExperience with data architecture design, data modeling, and optimization techniques.\nExcellent communication, collaboration, and leadership skills, with the ability to effectively manage remote teams and engage with onshore stakeholders.\nProven ability to adapt to evolving project requirements and effectively prioritize tasks in a fast-paced environment.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Team Handling', 'Snowflake', 'Data Services', 'Cloud Infrastructure', 'Data Bricks']",2025-06-13 05:18:53
Data Engineering Lead Microsoft Power BI,Client of Techs To Suit,8 - 12 years,25-32.5 Lacs P.A.,"['Indore', 'Hyderabad', 'Ahmedabad']","Poistion - Data Engineering Lead\nExp - 8 to 12 Years\nJob Location: Hyderabad, Ahmedabad, Indore, India.\n\nMust be Able to join in 30 days\nJob Summary:\nAs a Data Engineering Lead, your role will involve designing, developing, and implementing\ninteractive dashboards and reports using data engineering tools. You will work closely with\nstakeholders to gather requirements and translate them into effective data visualizations that\nprovide valuable insights. Additionally, you will be responsible for extracting, transforming, and\nloading data from multiple sources into Power BI, ensuring its accuracy and integrity. Your\nexpertise in Power BI and data analytics will contribute to informed decision-making and\nsupport the organization in driving data-centric strategies and initiatives.\nWe are looking for you!\nAs an ideal candidate for the Data Engineering Lead position, you embody the qualities of a\nteam player with a relentless get-it-done attitude. Your intellectual curiosity and customer\nfocus drive you to continuously seek new ways to add value to your job accomplishments. You\nthrive under pressure, maintaining a positive attitude and understanding that your career is a\njourney. You are willing to make the right choices to support your growth. In addition to your\nexcellent communication skills, both written and verbal, you have a proven ability to create\nvisually compelling designs using tools like Power BI and Tableau that effectively\ncommunicate our core values.\nYou build high-performing, scalable, enterprise-grade applications and teams. Your creativity\nand proactive nature enable you to think differently, find innovative solutions, deliver high-\nquality outputs, and ensure customers remain referenceable. With over eight years of\nexperience in data engineering, you possess a strong sense of self-motivation and take\nownership of your responsibilities. You prefer to work independently with little to no\nsupervision.\nYou are process-oriented, adopt a methodical approach, and demonstrate a quality-first\nmindset. You have led mid to large-size teams and accounts, consistently using constructive\nfeedback mechanisms to improve productivity, accountability, and performance within the\nteam. Your track record showcases your results-driven approach, as you have consistently\ndelivered successful projects with customer case studies published on public platforms.\nOverall, you possess a unique combination of skills, qualities, and experiences that make you\nan ideal fit to lead our data engineering team(s). You value inclusivity and want to join a culture\nthat empowers you to show up as your authentic self.\nYou know that success hinges on commitment, our differences make us stronger, and the\nfinish line is always sweeter when the whole team crosses together. In your role, you shouldbe driving the team using data, data, and more data. You will manage multiple teams, oversee\nagile stories and their statuses, handle escalations and mitigations, plan ahead, identify hiring\nneeds, collaborate with recruitment teams for hiring, enable sales with pre-sales teams, and\nwork closely with development managers/leads for solutioning and delivery statuses, as well\nas architects for technology research and solutions.\nWhat You Will Do:\nAnalyze Business Requirements.\nAnalyze the Data Model and do GAP analysis with Business Requirements and Power\nBI. Design and Model Power BI schema.\nTransformation of Data in Power BI/SQL/ETL Tool.\nCreate DAX Formula, Reports, and Dashboards. Able to write DAX formulas.\nExperience writing SQL Queries and stored procedures.\nDesign effective Power BI solutions based on business requirements.\nManage a team of Power BI developers and guide their work.\nIntegrate data from various sources into Power BI for analysis.\nOptimize performance of reports and dashboards for smooth usage.\nCollaborate with stakeholders to align Power BI projects with goals.\nKnowledge of Data Warehousing(must), Data Engineering is a plus",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power Bi Dashboards', 'Microsoft Power Bi', 'SQL Queries', 'Azure Databricks', 'Dax', 'Azure Data Factory']",2025-06-13 05:18:55
Lead Data Engineer - Azure,Blend360 India,7 - 12 years,Not Disclosed,['Hyderabad'],"As a Lead Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\n7+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar field\nMust have experience e",Industry Type: Industrial Automation,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-13 05:18:57
Senior Engineer - Data Science,Sasken Technologies,2 - 5 years,Not Disclosed,['Bengaluru'],"Job Summary\nPerson at this position has gained significant work experience to be able to apply their knowledge effectively and deliver results. Person at this position is also able to demonstrate the ability to analyse and interpret complex problems and improve change or adapt existing methods to solve the problem.\nPerson at this position regularly interacts with interfacing groups / customer on technical issue clarification and resolves the issues. Also participates actively in important project/ work related activities and contributes towards identifying important issues and risks. Reaches out for guidance and advice to ensure high quality of deliverables.\nPerson at this position consistently seek opportunities to enhance their existing skills, acquire more complex skills and work towards enhancing their proficiency level in their field of specialisation.\nWorks under limited supervision of Team Lead/ Project Manager.\n\n\nRoles & Responsibilities\nResponsible for design, coding, testing, bug fixing, documentation and technical support in the assigned area. Responsible for on time delivery while adhering to quality and productivity goals. Responsible for adhering to guidelines and checklists for all deliverable reviews, sending status report to team lead and following relevant organizational processes. Responsible for customer collaboration and interactions and support to customer queries. Expected to enhance technical capabilities by attending trainings, self-study and periodic technical assessments. Expected to participate in technical initiatives related to project and organization and deliver training as per plan and quality.\n\n\nEducation and Experience Required\nEngineering graduate, MCA, etc Experience: 2-5 years\n\nCompetencies Description\nData Science TCB is applicable to one who\n1) Analyses data to arrive at patterns/Insights/models\n2) Come up with models based on the data to provide recommendations, predictive analytics etc\n3) Provides implementation of the models in R, Matlab etc\n4) Can understand and apply machine learning/AI techniques\nPlatforms-\nUnix\nTechnology Standard-\nNA\nTools-\nR, Matlab, Spark Machine Learning, Python-ML, SPSS, SAS\nLanguages-\nR, Perl, Python, Scala\nSpecialization-\nCOGNITIVE ANALYTICS INCLUDING COMPUTER VISION, AI and ML, STATISTICS.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'R', 'SAS', 'Scala', 'Perl', 'SPSS', 'Spark', 'machine learning', 'Python']",2025-06-13 05:18:58
Gcp Data Engineer,Estuate Software,6 - 11 years,Not Disclosed,['Hyderabad'],"Design, build,\nJob Title: Data Engineer / Integration Engineer\nJob Summary:\nWe are seeking a highly skilled Data Engineer / Integration Engineer to join our team. The ideal candidate will have expertise in Python, workflow orchestration, cloud platforms (GCP/Google BigQuery), big data frameworks (Apache Spark or similar), API integration, and Oracle EBS. The role involves designing, developing, and maintaining scalable data pipelines, integrating various systems, and ensuring data quality and consistency across platforms. Knowledge of Ascend.io is a plus.\nKey Responsibilities:\nDesign, build, and maintain scalable data pipelines and workflows.\nDevelop and optimize ETL/ELT processes using Python and workflow automation tools.\nImplement and manage data integration between various systems, including APIs and Oracle EBS.\nWork with Google Cloud Platform (GCP) or Google BigQuery (GBQ) for data storage, processing, and analytics.\nUtilize Apache Spark or similar big data frameworks for efficient data processing.\nDevelop robust API integrations for seamless data exchange between applications.\nEnsure data accuracy, consistency, and security across all systems.\nMonitor and troubleshoot data pipelines, identifying and resolving performance issues.\nCollaborate with data analysts, engineers, and business teams to align data solutions with business goals.\nDocument data workflows, processes, and best practices for future reference.\nRequired Skills & Qualifications:\nStrong proficiency in Python for data engineering and workflow automation.\nExperience with workflow orchestration tools (e.g., Apache Airflow, Prefect, or similar).\nHands-on experience with Google Cloud Platform (GCP) or Google BigQuery (GBQ).\nExpertise in big data processing frameworks, such as Apache Spark.\nExperience with API integrations (REST, SOAP, GraphQL) and handling structured/unstructured data.\nStrong problem-solving skills and ability to optimize data pipelines for performance.\nExperience working in an agile environment with CI/CD processes.\nStrong communication and collaboration skills.\nPreferred Skills & Nice-to-Have:\nExperience with Ascend.io platform for data pipeline automation.\nKnowledge of SQL and NoSQL databases.\nFamiliarity with Docker and Kubernetes for containerized workloads.\nExposure to machine learning workflows is a plus.\nWhy Join Us?\nOpportunity to work on cutting-edge data engineering projects.\nCollaborative and dynamic work environment.\nCompetitive compensation and benefits.\nProfessional growth opportunities with exposure to the latest technologies.\n\nHow to Apply:\nInterested candidates can apply by sending their resume to 8892751405 / deekshith.naidu@estuate.com or through Naukri",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Airflow', 'GCP', 'Bigquery', 'SQL']",2025-06-13 05:19:00
Manager Data Engineer - Research Data and Analytics,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will create and develop data lake solutions for scientific data that drive business decisions for Research. You will build scalable and high-performance data engineering solutions for large scientific datasets and collaborate with Research collaborators. You will also provide technical leadership to junior team members. The ideal candidate possesses experience in the pharmaceutical or biotech industry, demonstrates deep technical skills, is proficient with big data technologies, and has a deep understanding of data architecture and ETL processes.\nRoles & Responsibilities:",,,,"['Data Engineering', 'Spotfire', 'PySpark', 'PostgreSQL', 'Plotly', 'SparkSQL', 'SQL server', 'SQL', 'process mapping', 'Dash', 'MySQL', 'ETL', 'Oracle', 'data governance frameworks', 'Python']",2025-06-13 05:19:02
Azure Data Engineer,Arges Global,2 - 5 years,8-18 Lacs P.A.,['Pune( Baner )'],"Scope of Work:\nCollaborate with the lead Business / Data Analyst to gather and analyse business requirements for data processing and reporting solutions.\nMaintain and run existing Python code, ensuring smooth execution and troubleshooting any issues that arise.\nDevelop new features and enhancements for data processing, ingestion, transformation, and report building.\nImplement best coding practices to improve code quality, maintainability, and efficiency.\nWork within Microsoft Fabric to manage data integration, warehousing, and analytics, ensuring optimal performance and reliability.\nSupport and maintain CI/CD workflows using Git-based deployments or other automated deployment tools, preferably in Fabric.\nDevelop complex business rules and logic in Python to meet functional specifications and reporting needs.\nParticipate in an agile development environment, providing feedback, iterating on improvements, and supporting continuous integration and delivery processes.\nRequirements:\nThis person will be an individual contributor responsible for programming, maintenance support, and troubleshooting tasks related to data movement, processing, ingestion, transformation, and report building.\nAdvanced-level Python developer.\nModerate-level experience in working in Microsoft Fabric environment (at least one and preferably two or more client projects in Fabric).\nWell-versed with understanding of modelling, databases, data warehousing, data integration, and technical elements of business intelligence technologies.\nAbility to understand business requirements and translate them into functional specifications for reporting applications.\nExperience in GIT-based deployments or other CI/CD workflow options, preferably in Fabric.\nStrong verbal and written communication skills.\nAbility to perform in an agile environment where continual development is prioritized.\nWorking experience in the financial industry domain and familiarity with financial accounting terms and statements like general ledger, balance sheet, and profit & loss statements would be a plus.\nAbility to create Power BI dashboards, KPI scorecards, and visual reports would be a plus.\nDegree in Computer Science or Information Systems, along with a good understanding of financial terms or working experience in banking/financial institutions, is preferred.",Industry Type: Financial Services (Asset Management),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power Bi', 'Microsoft Azure', 'Python', 'Azure Data Factory', 'Microsoft Fabric', 'Azure Databricks', 'Azure Data Lake']",2025-06-13 05:19:03
Data Engineering Manager,Amgen Inc,8 - 12 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\n\n\nRole Description:\n\nWe are seeking a seasoned Engineering Manager (Data Engineering) to lead the end-to-end management of enterprise data assets and operational data workflows. This role is critical in ensuring the availability, quality, consistency, and timeliness of data across platforms and functions, supporting analytics, reporting, compliance, and digital transformation initiatives. You will be responsible for the day-to-day data operations, manage a team of data professionals, and drive process excellence in data intake, transformation, validation, and delivery. You will work closely with cross-functional teams including data engineering, analytics, IT, governance, and business stakeholders to align operational data capabilities with enterprise needs.",,,,"['Data Engineering', 'fullstack development', 'logging framework', 'stakeholder engagement', 'troubleshooting', 'cloud platforms']",2025-06-13 05:19:06
AWS Data Engineer,DATATERRAIN LTD,10 - 15 years,Not Disclosed,['Hyderabad'],"Role AWS Data Engineer Experience 10+ Years Hyderabad Salary As per Industry Job Summary :\nWe are looking for a highly skilled Data Engineer to join our team. The ideal candidate should have strong experience working in an AWS environment, proficiency in ETL tools, and expertise in reporting tools. The candidate must be able to work onsite at the Hyderabad office on a daily basis.\nKey Responsibilities:\n1. Design, develop, and maintain data pipelines using AWS technologies.\n2. Work with ETL tools to process and transform large datasets efficiently.\n3. Develop and optimize reports and dashboards using reporting tools.\n4. Ensure data integrity, security, and governance across all data pipelines.\n5. Collaborate with cross-functional teams to understand data requirements.\n6. Troubleshoot and resolve data-related issues proactively.\nRequired Skills & Qualifications:\n1. Strong experience in AWS services\n2. Proficiency in any ETL tool\n3. Proficiency in any reporting tool\n4. Strong understanding of data modeling, warehousing, and big data concepts .\n5. Ability to work onsite daily at the Hyderabad clients office.\n6. Bachelors degree in Computer Science, Software Engineering, or a related field\nNote: Only shortlisted candidates will be contacted for further steps in the selection process.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Selection process', 'Warehouse', 'Data modeling', 'data integrity', 'Troubleshooting', 'big data', 'ETL tool', 'AWS', 'Reporting tools']",2025-06-13 05:19:07
AWS Data Engineer,Exavalu,2 - 5 years,Not Disclosed,[],"Exavalu is looking for AWS Data Engineer to join our dynamic team and embark on a rewarding career journey\nBe responsible for the planning, implementation, and growth of the AWS cloud infrastructure\nBuild, release, and manage the configuration of all production systems\nManage a continuous integration and deployment methodology for server-based technologies\nWork alongside architecture and engineering teams to design and implement any scalable software services\nEnsure necessary system security by using best in class cloud security solutionsImplement continuous integration/continuous delivery (CI/CD) pipelines when necessary\nRecommend process and architecture improvements\nTroubleshoot the system and solve problems across all platform and application domains\nOversee pre-production acceptance testing to ensure the high quality of a companys services and products",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent",['AWS Data Engineer'],2025-06-13 05:19:09
Data Engineering Automation Tester,Swits Digital,3 - 6 years,Not Disclosed,['Gurugram'],"Job Title: Data Engineering Automation Tester\n\nExperience: 6+ Years\n\nLocation: Gurgaon, India\n\n\nMandatory Skills:\n\n\n\n\nStrong experience in distributed computing (Spark) and software development.\n\n\n\nProficiency in working with databases (preferably Postgres).\n\n\n\nSolid understanding of Object-Oriented Programming and development principles.\n\n\n\nExperience in Agile development methodologies (Scrum/Kanban).\n\n\n\nHands-on experience with version control tools (preferably Git).\n\n\n\nExposure to CI/CD pipelines.\n\n\n\nStrong background in automated testing including Integration/Delta, Load, and Performance testing.\n\n\n\nExtensive experience in database testing (preferably Postgres).\n\n\n\n\nGood to Have Skills:\n\n\n\n\nExposure to Docker and containerized environments.\n\n\n\nExperience with Spark-Scala.\n\n\n\nKnowledge of Data Engineering principles.\n\n\n\nExperience in Python and .NET Core.\n\n\n\nFamiliarity with Kubernetes.\n\n\n\nExperience with Airflow.\n\n\n\nWorking knowledge of cloud platforms (GCP and Azure).\n\n\n\nExperience with TeamCity CI and Octopus Deploy.",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database testing', 'Automation', 'Automation testing', 'GIT', 'Version control', 'spark', 'Agile development', 'Performance testing', 'Object oriented programming', 'Python']",2025-06-13 05:19:11
GCP Data Engineer,Swits Digital,2 - 7 years,Not Disclosed,['Chennai'],"Job Title: GCP Data Engineer\nLocation: Chennai, India\nJob type: FTE\nMandatory Skills: Google Cloud Platform - Biq Query, Data Flow, Dataproc, Data Fusion, TERRAFORM, Tekton,Cloud SQL, AIRFLOW, POSTGRES, Airflow PySpark, Python, API\nJob Description:\n2+Years in GCP Services, Biq Query, Data Flow, Dataproc, DataPlex,DataFusion, Terraform, Tekton, Cloud SQL, Redis Memory, Airflow, Cloud Storage\n2+ Years inData Transfer Utilities\n2+ Years in Git / any other version control tool\n2+ Years in Confluent Kafka\n1+ Years of Experience in API Development\n2+ Years in Agile Framework\n4+ years of strong experience in python, Pyspark development.\n4+ years of shell scripting to develop the adhoc jobsfor data importing/exporting",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Version control', 'GIT', 'GCP', 'Shell scripting', 'query', 'Agile', 'cloud storage', 'SQL', 'Python']",2025-06-13 05:19:12
GCP Data Engineer,Product based Fortune Global 500 MNC in ...,4 - 8 years,6-16 Lacs P.A.,"['Hyderabad', 'Chennai']","Role & responsibilities\nBachelors degree or four or more years of work experience.\nFour or more years of work experience.\nExperience with Data Warehouse concepts and Data Management life cycle.\nExperience in any DBMS\nExperience in Shell scripting, Spark, Scala.\nExperience in GCP/Big Query, composer, Airflow.\nExperience in real time streaming\nExperience in DevOps",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GCP', 'Bigquery', 'Cloud Storage', 'Pubsub', 'Data Flow', 'Dataproc', 'Spark', 'Composing', 'Dataprep', 'Python']",2025-06-13 05:19:14
Aws Data Engineer,Astrosoft Technologies,2 - 4 years,6.5-10 Lacs P.A.,['Hyderabad( Gachibowli )'],"Company: AstroSoft Technologies (https://www.astrosofttech.com/)\nAstrosoft is an award-winning company that specializes in the areas of Data, Analytics, Cloud, AI/ML, Innovation, Digital. We have a customer first mindset and take extreme ownership in delivering solutions and projects for our customers and have consistently been recognized by our clients as the premium partner to work with. We bring to bear top tier talent, a robust and structured project execution framework, our significant experience over the years and have an impeccable record in delivering solutions and projects for our clients.\nFounded in 2004, Headquarters in FL,USA, Corporate Office - India, Hyderabad\nBenefits from Astrosoft Technologies\nH1B Sponsorship (Depends on Project & Performance)\nLunch & Dinner (Every day)\nHealth Insurance Coverage- Group\nIndustry Standards Leave Policy\nSkill Enhancement Certification\nHybrid Mode\nRole & responsibilities\nJob Title: AWS Engineer\nRequired Skills:\nMinimum of 2+ years direct experience with AWS Data Engineer\nStrong Experience in AWS Services like Redshift & ETL Glue, Spark, Python, Lambda,Kafka,S3, EMR Etc experience is a must.\nMonitoring tools - Cloudwatch\nExperience in Development & Support Projects as well.\nStrong verbal and written communication skills\nStrong experience and understanding of streaming architecture and development practices using kafka, spark, flink etc,\nStrong AWS development experience using S3, SNS, SQS, MWAA (Airflow) Glue, DMS and EMR.\nStrong knowledge of one or more programing languages Python/Java/Scala (ideally Python)\nExperience using Terraform to build IAC components in AWS.\nStrong experience with ETL Tools in AWS; ODI experience is as plus.\nStrong experience with Database Platforms: Oracle, AWS Redshift\nStrong experience in SQL tuning, tuning ETL solutions, physical optimization of databases.\nVery familiar with SRE concepts which includes evaluating and implementing monitoring and observability tools like Splunk, Data Dog, CloudWatch and other job, log or dashboard concepts for customer support and application health checks.\nAbility to collaborate with our business partners to understand and implement their requirements.\nExcellent interpersonal skills and be able to build consensus across teams.\nStrong critical thinking and ability to think out-of-the box.\nSelf-motivated and able to perform under pressure.\nAWS certified (preferred)\nQualifications:\nEducation: Bachelor's degree in Computer Science, Information Technology, or a related field (or equivalent work experience).\nExperience:\nProven experience as an AWS Support Engineer or in a similar role.\nHands-on experience with a wide range of AWS services (e.g., EC2, S3, RDS, Lambda, CloudFormation, IAM).\nSoft Skills:\nExcellent communication and interpersonal skills.\nStrong problem-solving and analytical skills.\nAbility to work independently and as part of a team.\nCustomer-focused mindset with a commitment to delivering high-quality support.\nWhat We Offer:\nCompetitive salary and benefits package.\nOpportunities for professional growth and development.\nA collaborative and supportive work environment.\nAccess to the latest AWS technologies and training resources.\nIf you are passionate about cloud technology and enjoy helping customers solve complex technical challenges, we would love to hear from you!\nAcknowledge the mail with your updated cv,\nJulekha.Gousiya@astrosofttech.com\n\n\n\nDetails As we discussed, Please revert with your acknowledgment.\nTotal Experience-\nAws\nRedshift\nGlue-\nCurrent Location-\nCurrent Company-\nC-CTC-\nEx-CTC –\nOffer –\nNP –\nReady to Relocate Hyderabad (Y/N) – Yes\n(Hybrid) –(Y/N) - Yes",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AWS', 'Glue', 'redshift', 'Spark']",2025-06-13 05:19:16
DBA DATA Engineer || 12 Lakhs CTC,Robotics Technologies,6 - 10 years,12 Lacs P.A.,['Hyderabad( Banjara hills )'],"Dear Candidate,\nWe are seeking a skilled and experienced DBA Data Engineer to join our growing data team. The ideal candidate will play a key role in designing, implementing, and maintaining our databases and data pipeline architecture. You will collaborate with software engineers, data analysts, and DevOps teams to ensure efficient data flow, data integrity, and optimal database performance across all systems. This role requires a strong foundation in database administration, SQL performance tuning, data modeling, and experience with both on-prem and cloud-based environments.\n\nRequirements:\nBachelors degree in Computer Science, Information Systems, or a related field.\n6+ years of experience in database administration and data engineering.\nProven expertise in RDBMS (Oracle, MySQL, and PostgreSQL) and NoSQL systems (MongoDB, Cassandra).\nExperience managing databases in cloud environments (AWS, Azure, or GCP).\nProficiency in ETL processes and tools (e.g., Apache NiFi, Talend, Informatica, AWS Glue).\nStrong experience with scripting languages such as Python, Bash, or PowerShell.\n\nDBA Data Engineer Roles & Responsibilities:\nDesign and maintain scalable and high-performance database architectures.\nMonitor and optimize database performance using tools like CloudWatch, Oracle Enterprise Manager, pgAdmin, Mongo Compass, or Dynatrace.\nDevelop and manage ETL/ELT pipelines to support business intelligence and analytics.\nEnsure data integrity and security through best practices in backup, recovery, and encryption.\nAutomate regular database maintenance tasks using scripting and scheduled jobs.\nImplement high availability, failover, and disaster recovery strategies.\nConduct performance tuning of queries, stored procedures, indexes, and table structures.\nCollaborate with DevOps to automate database deployments using CI/CD and IaC tools (e.g., Terraform, AWS CloudFormation).\nDesign and implement data models, including star/snowflake schemas for data warehousing.\nDocument data flows, data dictionaries, and database configurations.\nManage user access and security policies using IAM roles or database-native permissions.\nAnalyze existing data systems and propose modernization or migration plans (on-prem to cloud, SQL to NoSQL, etc.).\nUse AWS RDS, Amazon Redshift, Azure SQL Database, or Google BigQuery as needed.\nStay up-to-date with emerging database technologies and make recommendations.\n\nMust-Have Skills:\nDeep knowledge of SQL and database performance tuning.\nHands-on experience with database migrations and replication strategies.\nFamiliarity with data governance, data quality, and compliance frameworks (GDPR, HIPAA, etc.).\nStrong problem-solving and troubleshooting skills.\nExperience with data streaming platforms such as Apache Kafka, AWS Kinesis, or Apache Flink is a plus.\nExperience with data lake and data warehouse architectures.\nExcellent communication and documentation skills.\n\nSoft Skills:\nProblem-Solving: Ability to analyze complex problems and develop effective solutions.\nCommunication Skills: Strong verbal and written communication skills to effectively collaborate with cross-functional teams.\nAnalytical Thinking: Ability to think critically and analytically to solve technical challenges.\nTime Management: Capable of managing multiple tasks and deadlines in a fast-paced environment.\nAdaptability: Ability to quickly learn and adapt to new technologies and methodologies.\n\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database Administration', 'Database Migration', 'Dba Skills', 'High Availability', 'Db Administration', 'Hadr', 'Database Security', 'Disaster Recovery', 'Dr Testing', 'DR', 'Luw', 'Db Upgrade', 'Brtools', 'UDDI', 'Os Migration', 'Dbase', 'DBMS', 'IBM DB2', 'Db Migration', 'Disaster Recovery Planning', 'Db Dba', 'Backup And Recovery']",2025-06-13 05:19:17
Assoc. Data Engineer - R&D Precision Medicine Team,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\n\n\nThe R&D Precision Medicine team is responsible for Data Standardization, Data Searching, Cohort Building, and Knowledge Management tools that provide the Amgen scientific community with access to Amgens wealth of human datasets, projects and study histories, and knowledge over various scientific findings. These data include clinical data, omics, and images. These solutions are pivotal tools in Amgens goal to accelerate the speed of discovery, and speed to market of advanced precision medications.\n\nThe Data Engineer will be responsible for full stack development of enterprise analytics and data mastering solutions leveraging Databricks and Power BI. This role requires expertise in both data architecture and analytics, with the ability to create scalable, reliable, and high-performing enterprise solutions that support research cohort-building and advanced AI pipelines. The ideal candidate will have experience creating and surfacing large unified repositories of human data, based on integrations from multiple repositories and solutions, and be exceptionally skilled with data analysis and profiling.\n\nYou will collaborate closely with partners, product team members, and related IT teams, to design and implement data models, integrate data from various sources, and ensure best practices for data governance and security. The ideal candidate will have a solid background in data warehousing, ETL, Databricks, Power BI, and enterprise data mastering.\n\n\n\n\n\n\nWe are all different, yet we all use our unique contributions to serve patients. The professional we seek is someone with these qualifications.",,,,"['Data Engineering', 'data lakes', 'data pipelines', 'ETL processes', 'AWS', 'data warehouses', 'BI solutions']",2025-06-13 05:19:19
Data Engineer - R&D Data Catalyst Team,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role, you will be responsible for the end-to-end development of an enterprise analytics and data mastering solution using Databricks and Power BI. This role requires expertise in both data architecture and analytics, with the ability to create scalable, reliable, and impactful enterprise solutions that research cohort-building and advanced research pipeline. The ideal candidate will have experience creating and surfacing large unified repositories of human data, based on integrations from multiple repositories and solutions, and be extraordinarily skilled with data analysis and profiling.",,,,"['Data Engineering', 'data management', 'Power BI', 'data governance', 'data warehousing', 'Databricks', 'ETL', 'AWS']",2025-06-13 05:19:21
MDM Associate Data Engineer,Amgen Inc,1 - 4 years,Not Disclosed,['Hyderabad'],"We are seeking an MDM Associate Data Engineerwith 25 years of experience to support and enhance our enterprise MDM (Master Data Management) platforms using Informatica/Reltio. This role is critical in delivering high-quality master data solutions across the organization, utilizing modern tools like Databricks and AWS to drive insights and ensure data reliability. The ideal candidate will have strong SQL, data profiling, and experience working with cross-functional teams in a pharma environment.To succeed in this role, the candidate must have strong data engineering experience along with MDM knowledge, hence the candidates having only MDM experience are not eligible for this role. Candidate must have data engineering experience on technologies like (SQL, Python,",,,,"['MDM', 'PySpark', 'AWS architecture', 'Jira', 'Reltio', 'SQL', 'Informatica MDM', 'data modeling', 'Confluence', 'IDQ', 'Databricks', 'data stewardship processes', 'Python']",2025-06-13 05:19:23
Data Engineering Specialist,Sanofi,5 - 10 years,Not Disclosed,['Hyderabad'],"We are seeking an experienced Data Engineering Specialist interested in challenging the status quo to ensure the seamless creation and operation of the data pipelines that are needed by Sanofi s advanced analytic, AI and ML initiatives for the betterment of our global patients and customers.\nSanofi has recently embarked into a vast and ambitious digital transformation program. A cornerstone of this roadmap is the acceleration of its data transformation and of the adoption of artificial intelligence (AI) and machine learning (ML) solutions, to accelerate R&D, manufacturing and commercial performance and bring better drugs and vaccines to patients faster, to improve health and save lives\nMain Responsibilities:\nEstablish technical designs to meet Sanofi requirements aligned with the architectural and Data standards\nOwnership of the entire back end of the application, including the design, implementation, test, and troubleshooting of the core application logic, databases, data ingestion and transformation, data processing and orchestration of pipelines, APIs, CI/CD integration and other processes\nFine-tune and optimize queries using Snowflake platform and database techniques\nOptimize ETL/data pipelines to balance performance, functionality, and other operational requirements.\nAssess and resolve data pipeline issues to ensure performance and timeliness of execution\nAssist with technical solution discovery to ensure technical feasibility.\nAssist in setting up and managing CI/CD pipelines and development of automated tests\nDeveloping and managing microservices using python\nConduct peer reviews for quality, consistency, and rigor for production level solution\nDesign application architecture for efficient concurrent user handling, ensuring optimal performance during high usage periods\nOwn all areas of the product lifecycle: design, development, test, deployment, operation, and support\nQualifications:\n5+ years of relevant experience developing backend, integration, data pipelining, and infrastructure\nExpertise in database optimization and performance improvement\nExpertise in Python, PySpark, and Snowpark\nExperience data warehousing and object-relational database (Snowflake and PostgreSQL) and writing efficient SQL queries\nExperience in cloud-based data platforms (Snowflake, AWS)\nProficiency in developing robust, reliable APIs using Python and FastAPI Framework\nExpert in ELT and ETL & experience working with large data sets and performance and query optimization. IICS is a plus\nUnderstanding of data structures and algorithms\nUnderstanding of DBT is a plus\nExperience in modern testing framework (SonarQube, K6 is a plus)\nStrong collaboration skills, willingness to work with others to ensure seamless integration of the server-side and client-side\nKnowledge of DevOps best practices and associated tools is a plus, especially in the setup, configuration, maintenance, and troubleshooting of associated tools:\nContainers and containerization technologies (Kubernetes, Argo, Red Hat OpenShift)\nInfrastructure as code (Terraform)\nMonitoring and Logging (CloudWatch, Grafana)\nCI/CD Pipelines (JFrog Artifactory)\nScripting and automation (Python, GitHub, Github actions)\nExperience with JIRA & Confluence\nWorkflow orchestration (Airflow)\nMessage brokers (RabbitMQ)\nEducation: bachelors degree in computer science, engineering, or similar quantitative field of study\nWhy choose us\nBring the miracles of science to life alongside a supportive, future-focused team.\nDiscover endless opportunities to grow your talent and drive your career, whether it s through a promotion or lateral move, at home or internationally.\nEnjoy a thoughtful, we'll-crafted rewards package that recognizes your contribution and amplifies your impact.\nTake good care of yourself and your family, with a wide range of health and we'llbeing benefits including high-quality healthcare, prevention and we'llness programs and at least 14 weeks gender-neutral parental leave.\nOpportunity to work in an international environment, collaborating with diverse business teams and vendors, working in a dynamic team, and fully empowe'red to propose and implement innovative ideas.",Industry Type: Medical Services / Hospital,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'github', 'Postgresql', 'Data structures', 'Healthcare', 'Troubleshooting', 'JIRA', 'Monitoring', 'Python']",2025-06-13 05:19:24
Engineer II - Data Engg & AI,anblicks,5 - 10 years,Not Disclosed,['Ahmedabad'],"We are seeking a highly skilled and experienced Senior AI Engineer to lead the design, development, and deployment of advanced AI systems. You will work on cutting-edge machine learning models, natural language processing, computer vision, and AI infrastructure to solve real-world problems and drive innovation across our products and services.\nKey Responsibilities:\nDesign, develop, and deploy scalable AI/ML models for production environments.\nLead end-to-end AI project lifecycles from data collection and preprocessing to model training, evaluation, and deployment.\nCollaborate with cross-functional teams including data scientists, software engineers, and product managers.\nOptimize model performance and ensure robustness, fairness, and explainability.\nStay current with the latest research and advancements in AI and machine learning.\nMentor junior engineers and contribute to building a strong AI engineering culture.\nRequired Qualifications:\nBachelor s or Master s degree in Computer Science, Artificial Intelligence, Machine Learning, or a related field (PhD preferred).\n5+ years of experience in AI/ML engineering with a strong portfolio of deployed models.\nProficiency in Python and ML libraries such as TensorFlow, PyTorch, Scikit-learn, etc.\nExperience with cloud platforms (AWS, Azure) and MLOps tools.\nStrong understanding of data structures, algorithms, and software engineering principles.\nExcellent problem-solving and communication skills.\nPreferred Qualifications:\nExperience with LLMs, RAG, or agentic AI (Crew AI) systems.\nFamiliarity with vector databases, prompt engineering, and AI safety practices.\nContributions to open-source AI projects or published research papers.\nExperience with real-time inference systems and edge AI.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Computer vision', 'Artificial Intelligence', 'Machine learning', 'Data collection', 'Data structures', 'Natural language processing', 'Research', 'Open source', 'Python']",2025-06-13 05:19:26
"Data Engineer, AVP",NatWest Markets,16 - 18 years,Not Disclosed,['Gurugram'],"Join us as a Data Engineer\nWe re looking for someone to build effortless, digital first customer experiences to help simplify our organisation and keep our data safe and secure\nDay-to-day, you ll develop innovative, data-driven solutions through data pipelines, modelling and ETL design while inspiring to be commercially successful through insights\nIf you re ready for a new challenge, and want to bring a competitive edge to your career profile by delivering streaming data ingestions, this could be the role for you\nWere offering this role at assistant vice president level\nWhat you ll do\nYour daily responsibilities will include you developing a comprehensive knowledge of our data structures and metrics, advocating for change when needed for product development. You ll also provide transformation solutions and carry out complex data extractions.\nWe ll expect you to develop a clear understanding of data platform cost levels to build cost-effective and strategic solutions. You ll also source new data by using the most appropriate tooling before integrating it into the overall solution to deliver it to our customers.\nYou ll also be responsible for:\nDriving customer value by understanding complex business problems and requirements to correctly apply the most appropriate and reusable tools to build data solutions\nParticipating in the data engineering community to deliver opportunities to support our strategic direction\nCarrying out complex data engineering tasks to build a scalable data architecture and the transformation of data to make it usable to analysts and data scientists\nBuilding advanced automation of data engineering pipelines through the removal of manual stages\nLeading on the planning and design of complex products and providing guidance to colleagues and the wider team when required\nThe skills you ll need\nTo be successful in this role, you ll have an understanding of data usage and dependencies with wider teams and the end customer. You ll also have experience of extracting value and features from large scale data.\nWe ll expect you to have experience of ETL technical design, data quality testing, cleansing and monitoring, data sourcing, exploration and analysis, and data warehousing and data modelling capabilities.\nYou ll also need:\nExperience of using programming languages alongside knowledge of data and software engineering fundamentals\nGood knowledge of modern code development practices\nGreat communication skills with the ability to proactively engage with a range of stakeholders\nHours\n45\nJob Posting Closing Date:\n16/06/2025",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Usage', 'Technical design', 'Programming', 'Data structures', 'Data quality', 'Assistant Vice President', 'Data warehousing', 'Monitoring', 'Data architecture']",2025-06-13 05:19:28
Azure Data Engineer (Azure Databricks must),Fortune India 500 IT Services Firm,5 - 8 years,Not Disclosed,['Hyderabad'],"We are looking for an experienced Azure Data Engineer with strong expertise in Azure Databricks to join our data engineering team.\n\nMandatory skill- Azure Databricks\nExperience- 5 to 8 years\nLocation- Hyderabad\nKey Responsibilities:\nDesign and build data pipelines and ETL/ELT workflows using Azure Databricks and Azure Data Factory\nIngest, clean, transform, and process large datasets from diverse sources (structured and unstructured)\nImplement Delta Lake solutions and optimize Spark jobs for performance and reliability\nIntegrate Azure Databricks with other Azure services including Data Lake Storage, Synapse Analytics, and Event Hubs\n\n\n\nInterested candidates share your CV at himani.girnar@alikethoughts.com with below details\n\nCandidate's name-\nEmail and Alternate Email ID-\nContact and Alternate Contact no-\nTotal exp-\nRelevant experience-\nCurrent Org-\nNotice period-\nCCTC-\nECTC-\nCurrent Location-\nPreferred Location-\nPancard No-",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Azure Data Factory', 'Pyspark', 'Azure Data Lake', 'SQL']",2025-06-13 05:19:29
DBA DATA Engineer || 12 Lakhs CTC,Robotics Technologies,8 - 9 years,12 Lacs P.A.,['Hyderabad( Banjara hills )'],"Dear Candidate,\nWe are seeking a skilled and experienced DBA Data Engineer to join our growing data team. The ideal candidate will play a key role in designing, implementing, and maintaining our databases and data pipeline architecture. You will collaborate with software engineers, data analysts, and DevOps teams to ensure efficient data flow, data integrity, and optimal database performance across all systems. This role requires a strong foundation in database administration, SQL performance tuning, data modeling, and experience with both on-prem and cloud-based environments.\n\nRequirements:\nBachelors degree in Computer Science, Information Systems, or a related field.\n8+ years of experience in database administration and data engineering.\nProven expertise in RDBMS (Oracle, MySQL, and PostgreSQL) and NoSQL systems (MongoDB, Cassandra).\nExperience managing databases in cloud environments (AWS, Azure, or GCP).\nProficiency in ETL processes and tools (e.g., Apache NiFi, Talend, Informatica, AWS Glue).\nStrong experience with scripting languages such as Python, Bash, or PowerShell.\n\nDBA Data Engineer Roles & Responsibilities:\nDesign and maintain scalable and high-performance database architectures.\nMonitor and optimize database performance using tools like CloudWatch, Oracle Enterprise Manager, pgAdmin, Mongo Compass, or Dynatrace.\nDevelop and manage ETL/ELT pipelines to support business intelligence and analytics.\nEnsure data integrity and security through best practices in backup, recovery, and encryption.\nAutomate regular database maintenance tasks using scripting and scheduled jobs.\nImplement high availability, failover, and disaster recovery strategies.\nConduct performance tuning of queries, stored procedures, indexes, and table structures.\nCollaborate with DevOps to automate database deployments using CI/CD and IaC tools (e.g., Terraform, AWS CloudFormation).\nDesign and implement data models, including star/snowflake schemas for data warehousing.\nDocument data flows, data dictionaries, and database configurations.\nManage user access and security policies using IAM roles or database-native permissions.\nAnalyze existing data systems and propose modernization or migration plans (on-prem to cloud, SQL to NoSQL, etc.).\nUse AWS RDS, Amazon Redshift, Azure SQL Database, or Google BigQuery as needed.\nStay up-to-date with emerging database technologies and make recommendations.\n\nMust-Have Skills:\nDeep knowledge of SQL and database performance tuning.\nHands-on experience with database migrations and replication strategies.\nFamiliarity with data governance, data quality, and compliance frameworks (GDPR, HIPAA, etc.).\nStrong problem-solving and troubleshooting skills.\nExperience with data streaming platforms such as Apache Kafka, AWS Kinesis, or Apache Flink is a plus.\nExperience with data lake and data warehouse architectures.\nExcellent communication and documentation skills.\n\nSoft Skills:\nProblem-Solving: Ability to analyze complex problems and develop effective solutions.\nCommunication Skills: Strong verbal and written communication skills to effectively collaborate with cross-functional teams.\nAnalytical Thinking: Ability to think critically and analytically to solve technical challenges.\nTime Management: Capable of managing multiple tasks and deadlines in a fast-paced environment.\nAdaptability: Ability to quickly learn and adapt to new technologies and methodologies.\n\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database Administration', 'Database Migration', 'Dba Skills', 'High Availability', 'Db Administration', 'Hadr', 'Database Security', 'Disaster Recovery', 'Dr Testing', 'DR', 'Luw', 'Db Upgrade', 'Brtools', 'UDDI', 'Os Migration', 'Dbase', 'DBMS', 'IBM DB2', 'Db Migration', 'Disaster Recovery Planning', 'Db Dba', 'Backup And Recovery']",2025-06-13 05:19:31
Snowflake Data Engineer,Prudent Globaltech Solutions,7 - 12 years,16-27.5 Lacs P.A.,['Hyderabad( Madhapur )'],"Job Description Data Engineer\nWe are seeking a highly skilled Data Engineer with extensive experience in Snowflake, Data Build Tool (dbt), Snaplogic, SQL Server, PostgreSQL, Azure Data Factory, and other ETL tools. The ideal candidate will have a strong ability to optimize SQL queries and a good working knowledge of Python. A positive attitude and excellent teamwork skills are essential.\n\nRole & responsibilities\nData Pipeline Development: Design, develop, and maintain scalable data pipelines using Snowflake, DBT, Snaplogic, and ETL tools.",,,,"['Snowflake', 'Azure Data Factory', 'ADF', 'Data Engineer', 'ETL', 'SQL']",2025-06-13 05:19:33
Snowflake Data Engineer,Epam Systems,5 - 10 years,Not Disclosed,['Chennai'],"Key Skills:\nSnowflake (Snow SQL, Snow PLSQL and Snowpark)\nStrong Python\nAirflow/DBT\nAny DevOps tools\nAWS/Azure Cloud Skills\n\nRequirements:\nLooking for engineer for information warehouse\nWarehouse is based on AWS/Azure, DBT, Snowflake.\nStrong programming experience with Python.\nExperience with workflow management tools like Argo/Oozie/Airflow.\nExperience in Snowflake modelling - roles, schema, databases\nExperience in data Modeling (Data Vault).\nExperience in design and development of data transformation pipelines using the DBT framework.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Python', 'Azure Cloud', 'AWS', 'SQL']",2025-06-13 05:19:35
Azure Data Engineer,CODERZON Technologies Pvt Ltd,3 - 8 years,6-18 Lacs P.A.,['Kochi'],"Looking for a Data Engineer with 3+ yrs exp in Azure Data Factory, Synapse, Data Lake, Databricks, SQL, Python, Spark, CI/CD. Preferred: DP-203 cert, real-time data tools (Kafka, Stream Analytics), data governance (Purview), Power BI.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Synapse', 'Pyspark', 'Azure Databricks', 'Azure Data Lake', 'SQL Azure', 'Python']",2025-06-13 05:19:36
Hadoop Data Engineer,Envision Technology Solutions,3 - 8 years,5-15 Lacs P.A.,"['New Delhi', 'Hyderabad', 'Gurugram']","Primary Skill – Hadoop, Hive, Python, SQL, Pyspark/Spark.\nLocation –Hyderabad / Gurgaon;",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Hadoop', 'Hive', 'Spark', 'Python', 'SQL']",2025-06-13 05:19:38
Lead Infrastructure Engineer - Data Platforms,General Mills,6 - 10 years,Not Disclosed,['Mumbai'],"Position Title\nLead Infrastructure Engineer - Data Platforms\nFunction/Group\nDigital and Technology\nLocation\nMumbai\nShift Timing\nGeneral\nRole Reports to\nDT Manager Cloud Data Platforms\nRemote/Hybrid/in-Office\nHybrid\nABOUT GENERAL MILLS\nWe make food the world loves: 100 brands. In 100 countries. Across six continents. With iconic brands like Cheerios, Pillsbury, Betty Crocker, Nature Valley, and H agen-Dazs, we ve been serving up food the world loves for 155 years (and counting). Each of our brands has a unique story to tell.\nHow we make our food is as important as the food we make. Our values are baked into our legacy and continue to accelerate\nus into the future as an innovative force for good. General Mills was founded in 1866 when Cadwallader Washburn boldly bought the largest flour mill west of the Mississippi. That pioneering spirit lives on today through our leadership team who upholds a vision of relentless innovation while being a force for good. For more details check out http://www.generalmills.com\nGeneral Mills India Center (GIC) is our global capability center in Mumbai that works as an extension of our global organization delivering business value, service excellence and growth, while standing for good for our planet and people.\nWith our team of 1800+ professionals, we deliver superior value across the areas of Supply chain (SC) , Digital Technology (DT) Innovation, Technology Quality (ITQ), Consumer and Market Intelligence (CMI), Sales Strategy Intelligence (SSI) , Global Shared Services (GSS) , Finance Shared Services (FSS) and Human Resources Shared Services (HRSS).For more details check out https://www.generalmills.co.in\nWe advocate for advancing equity and inclusion to create more equitable workplaces and a better tomorrow.\nJOB OVERVIEW\nFunction Overview\nThe Digital and Technology team at General Mills stands as the largest and foremost unit, dedicated to exploring the latest trends and innovations in technology while leading the adoption of cutting-edge technologies across the organization. Collaborating closely with global business teams, the focus is on understanding business models and identifying opportunities to leverage technology for increased efficiency and disruption. The teams expertise spans a wide range of areas, including AI/ML, Data Science, IoT, NLP, Cloud, Infrastructure, RPA and Automation, Digital Transformation, Cyber Security, Blockchain, SAP S4 HANA and Enterprise Architecture. The MillsWorks initiative embodies an agile@scale delivery model, where business and technology teams operate cohesively in pods with a unified mission to deliver value for the company. Employees working on significant technology projects are recognized as Digital Transformation change agents.\nThe team places a strong emphasis on service partnerships and employee engagement with a commitment to advancing equity and supporting communities. In fostering an inclusive culture, the team values individuals passionate about learning and growing with technology, exemplified by the Work with Heartphilosophy, emphasizing results over facetime. Those intrigued by the prospect of contributing to the digital transformation journey of a Fortune 500 company are encouraged to explore more details about the function through the provided Link\nPurpose of the role\nThe Digital and Technology team of General Mills India Centre is looking for a MS SQL Server / Cloud database administrator (DBA) with Operations-oriented/DevOps skill set to support Cloud/On Prem databases. This opportunity is in a fast-paced environment as we migrate and refactor enterprise-scale databases from our on-premises datacenters to a cloud environment. The ideal candidate will have experience operating in a fast-paced, complex, and multi-platform environment and will contribute to the strategic direction of our database infrastructure.\nKEY ACCOUNTABILITIES\nProvide technical leadership for migrating existing Microsoft SQL Server and NoSQL Databases to Public Cloud, developing and owning migration processes, documentation, and tooling. This includes defining strategies and roadmaps for database migrations, ensuring alignment with overall IT strategy and leveraging automation wherever possible to streamline the process.\nExpertly administer and manage mission-critical, complex, and high-volume Database Platforms in a 24/7 environment, implementing and maintaining automated monitoring and alerting systems to proactively identify and address potential issues.\nProactively identify and address potential database performance bottlenecks, proposing and implementing automated solutions to optimize database efficiency and scalability. This includes developing and deploying automated scripts for performance tuning and optimization.\nLead the design and implementation of highly available and scalable database solutions in the cloud (GCP preferred), ensuring compliance with security and governance standards and utilizing Infrastructure as Code (IaC) for automated provisioning and management of database infrastructure.\nAdminister and troubleshoot SQL Server, PostgreSQL, MySQL, and NoSQL DBs (MongoDB), implementing best practices and resolving complex performance issues through automated scripting and tooling.\nDevelop and maintain comprehensive documentation for database systems, processes, and procedures.\nChampion the adoption of DevOps practices, including CI/CD, infrastructure as code (Terraform), and automation (Ansible, Python, PowerShell) to streamline database management and deployment. Actively participate in the development and improvement of automated deployment pipelines.\nCollaborate with application stakeholders to understand their database requirements and provide technical guidance on database design and optimization, emphasizing automation opportunities to improve development workflows.\nContribute to the development and implementation of database security policies and procedures, ensuring compliance with industry best practices and leveraging automation for security auditing and vulnerability management.\nActively participate in Agile development processes, contributing to sprint planning, daily stand-ups, and retrospectives, focusing on automation opportunities to improve team efficiency and reduce manual effort.\nMentor and guide junior team members, fostering a culture of knowledge sharing and continuous learning, particularly around automation and DevOps practices.\nStay abreast of emerging technologies and trends in database administration and cloud computing, recommending and implementing innovative solutions to improve database performance and reliability, with a focus on automation and efficiency gains.\nMINIMUM QUALIFICATIONS\n9+ years of hands-on experience with leading design, refactoring, or migration of databases in cloud infrastructures and services for at least one of Microsoft Azure, Amazon Web Services, and Google GCP (preferred). Experience with automating database migration processes is highly desirable.\nExperience maintaining and administering with CI/CD tools such as Ansible, GitHub, and Artifactory in a cloud environment and developing/writing scripts using advanced DevOps languages such as Python, PowerShell. Demonstrated ability to design and implement automated workflows.\nExperience working with infrastructure as code such as Terraform, or equivalent. Proven ability to automate infrastructure provisioning and management.\nExperience with concepts, processes tools required for cloud adoption including cloud security, governance, and integration. Experience with automating security and compliance tasks is a plus.\nExperience with SQL Server AlwaysOn and Windows clustering. Experience with automating the management of high-availability clusters is preferred.\nExperience with agile techniques and methods.\nFamiliarity with user expectations for cloud Databases (to be able to design user-centric engineering services e.g., provisioning self-service workflow). Experience with automating user provisioning and access management is a plus.\nWorking knowledge of DevOps, Agile development processes, exploration, and POCs.\nAbility to work collaboratively across functional team boundaries.\nPREFERRED QUALIFICATIONS\nGood understanding hands-on experience of Linux OS and Windows Server (2012+).\nExperience working with high availability, disaster recovery, backup strategies, and server tuning strategies including parameters, resources, contention, etc.\nExcellent interpersonal and communication skills.\nAbility to work in a fast-paced team environment.\nFlexibility, reliability, initiative, responsibility, and a can-domentality.",Industry Type: Food Processing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'MS SQL', 'SAP', 'Linux', 'MySQL', 'Infrastructure', 'Workflow', 'Windows', 'SQL', 'Python']",2025-06-13 05:19:40
Cloud Data Platform Engineer - C,Capgemini,3 - 7 years,Not Disclosed,['Mumbai'],"\nA Data Platform Engineer specialises in the design, build, and maintenance of cloud-based data infrastructure and platforms for data-intensive applications and services. They develop Infrastructure as Code and manage the foundational systems and tools for efficient data storage, processing, and management. This role involves architecting robust and scalable cloud data infrastructure, including selecting and implementing suitable storage solutions, data processing frameworks, and data orchestration tools. Additionally, a Data Platform Engineer ensures the continuous evolution of the data platform to meet changing data needs and leverage technological advancements, while maintaining high levels of data security, availability, and performance. They are also tasked with creating and managing processes and tools that enhance operational efficiency, including optimising data flow and ensuring seamless data integration, all of which are essential for enabling developers to build, deploy, and operate data-centric applications efficiently.\n\n - Grade Specific \nAn expert on the principles and practices associated with data platform engineering, particularly within cloud environments, and demonstrates proficiency in specific technical areas related to cloud-based data infrastructure, automation, and scalability.Key responsibilities encompass:Team Leadership and ManagementSupervising a team of platform engineers, with a focus on team dynamics and the efficient delivery of cloud platform solutions.Technical Guidance and Decision-MakingProviding technical leadership and making pivotal decisions concerning platform architecture, tools, and processes. Balancing hands-on involvement with strategic oversight.Mentorship and Skill DevelopmentGuiding team members through mentorship, enhancing their technical proficiencies, and nurturing a culture of continual learning and innovation in platform engineering practices.In-Depth Technical ProficiencyPossessing a comprehensive understanding of platform engineering principles and practices, and demonstrating expertise in crucial technical areas such as cloud services, automation, and system architecture.Community ContributionMaking significant contributions to the development of the platform engineering community, staying informed about emerging trends, and applying this knowledge to drive enhancements in capability.\n\n Skills (competencies)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cloud services', 'cloud platform', 'cloud data flow', 'system architecture', 'data flow', 'hive', 'snowflake', 'python', 'airflow', 'microsoft azure', 'data warehousing', 'data engineering', 'sql', 'docker', 'spark', 'gcp', 'kafka', 'hadoop', 'sqoop', 'bigquery', 'big data', 'aws', 'etl', 'data integration']",2025-06-13 05:19:42
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Hyderabad'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in shaping the data platform components.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead data platform blueprint and design- Implement data platform components effectively- Ensure seamless integration between systems and data models\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform- Strong understanding of data platform architecture- Experience in data integration and data modeling- Knowledge of cloud platforms like AWS or Azure- Hands-on experience with SQL and NoSQL databases\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform- This position is based at our Hyderabad office- An Engineering graduate preferably Computer Science graduate with 15 years of full-time education is required\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['sql', 'data modeling', 'data analytics', 'platform architecture', 'aws', 'kubernetes', 'continuous integration', 'openshift', 'docker', 'microservices', 'iot', 'java', 'git', 'gcp', 'devops', 'linux', 'jenkins', 'mysql', 'hadoop', 'big data', 'microsoft azure', 'cloud platforms', 'nosql', 'agile', 'data integration']",2025-06-13 05:19:44
Data Platform Engineer,Accenture,3 - 8 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in shaping the data platform components.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with Integration Architects and Data Architects to design and implement data platform components.- Ensure seamless integration between various systems and data models.- Develop and maintain data platform blueprints.- Provide technical expertise in data platform design and implementation.- Troubleshoot and resolve data platform related issues.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of data platform architecture and design principles.- Experience in implementing and optimizing data pipelines.- Knowledge of cloud-based data solutions.- Hands-on experience with data platform security and compliance measures.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'java', 'data modeling', 'platform architecture', 'design principles', 'hive', 'python', 'oracle', 'enterprise architecture', 'microsoft azure', 'data warehousing', 'sql', 'docker', 'infrastructure architecture', 'spark', 'gcp', 'design patterns', 'hadoop', 'agile', 'aws', 'big data']",2025-06-13 05:19:45
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\nRoles & Responsibilities:- Assist with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data pipelines and ETL processes using Databricks Unified Data Analytics Platform.- Design and implement data security and access controls for the data platform.- Troubleshoot and resolve issues related to the data platform and data pipelines.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nExperience with Databricks Unified Data Analytics Platform.- Must To Have\n\n\n\n\nSkills:\nStrong understanding of data modeling and database design principles.- Good To Have\n\n\n\n\nSkills:\nExperience with cloud-based data platforms such as AWS or Azure.- Good To Have\n\n\n\n\nSkills:\nExperience with data security and access controls.- Good To Have\n\n\n\n\nSkills:\nExperience with data visualization tools such as Tableau or Power BI.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions.- This position is based at our Bangalore, Hyderabad, Chennai and Pune Offices.- Mandatory office (RTO) for 2- 3 days and have to work on 2 shifts (Shift A- 10:00am to 8:00pm IST and Shift B - 12:30pm to 10:30 pm IST)\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['database design', 'data modeling', 'data analytics', 'microsoft azure', 'design principles', 'hive', 'sql', 'java', 'spark', 'design patterns', 'oops', 'mysql', 'hadoop', 'etl', 'big data', 'c#', 'rest', 'python', 'data security', 'power bi', 'javascript', 'sql server', 'data bricks', 'tableau', 'kafka', 'sqoop', 'aws']",2025-06-13 05:19:48
"AI/ML Engineer (Specializing in NLP/ML, Large Data Processing,",Synechron,8 - 10 years,Not Disclosed,"['Pune', 'Hinjewadi']","job requisition idJR1027361\n\nJob Summary\nSynechron seeks a highly skilled AI/ML Engineer specializing in Natural Language Processing (NLP), Large Language Models (LLMs), Foundation Models (FMs), and Generative AI (GenAI). The successful candidate will design, develop, and deploy advanced AI solutions, contributing to innovative projects that transform monolithic systems into scalable microservices integrated with leading cloud platforms such as Azure, Amazon Bedrock, and Google Gemini. This role plays a critical part in advancing Synechrons capabilities in cutting-edge AI technologies, enabling impactful business insights and product innovations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCandidates with extensive research or academic experience in AI/ML, especially in NLP and large-scale data processing, are eligible if they have practical deployment experience.",,,,"['python', 'data management', 'data processing', 'pipeline', 'big data', 'continuous integration', 'kubernetes', 'deploying models', 'natural language processing', 'ci/cd', 'fms', 'artificial intelligence', 'docker', 'sql', 'microservices', 'tensorflow', 'java', 'pytorch', 'jenkins', 'keras', 'aws']",2025-06-13 05:19:50
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Hyderabad'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the organization.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the organization.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Hyderabad office.- 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data bricks', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-13 05:19:51
Quality Engineer-Data,IBM,2 - 5 years,Not Disclosed,['Kochi'],The ability to be a team player\nThe ability and skill to train other people in procedural and technical topics\nStrong communication and collaboration skills\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nAble to write complex SQL queries ;\nHaving experience in Azure Databricks\n\n\nPreferred technical and professional experience\nExcellent communication and stakeholder management skills,Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure databricks', 'sql queries', 'sql', 'data quality', 'stakeholder management', 'hive', 'python', 'azure data lake', 'scala', 'pyspark', 'microsoft azure', 'data warehousing', 'power bi', 'azure data factory', 'sql server', 'tableau', 'sql azure', 'spark', 'hadoop', 'big data', 'etl', 'ssis']",2025-06-13 05:19:53
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Chennai'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\nRoles & Responsibilities:- Assist with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data pipelines using Databricks Unified Data Analytics Platform.- Design and implement data security and access controls using Databricks Unified Data Analytics Platform.- Troubleshoot and resolve issues related to data platform components using Databricks Unified Data Analytics Platform.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nExperience with Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with other big data technologies such as Hadoop, Spark, and Kafka.- Strong understanding of data modeling and database design principles.- Experience with data security and access controls.- Experience with data pipeline development and maintenance.- Experience with troubleshooting and resolving issues related to data platform components.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions.- This position is based at our Bangalore, Hyderabad, Chennai and Pune Offices.- Mandatory office (RTO) for 2- 3 days and have to work on 2 shifts (Shift A- 10:00am to 8:00pm IST and Shift B - 12:30pm to 10:30 pm IST)\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['database design', 'data modeling', 'spark', 'data analytics', 'design principles', 'hive', 'sql', 'java', 'design patterns', 'oops', 'mysql', 'hadoop', 'big data', 'etl', 'c#', 'rest', 'python', 'microsoft azure', 'javascript', 'sql server', 'data bricks', 'amazon ec2', 'kafka', 'troubleshooting', 'sqoop', 'aws']",2025-06-13 05:19:55
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Chennai'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\nRoles & Responsibilities:- Assist with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data pipelines using Databricks Unified Data Analytics Platform.- Design and implement data security and access controls using Databricks Unified Data Analytics Platform.- Troubleshoot and resolve issues related to data platform components using Databricks Unified Data Analytics Platform.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nExperience with Databricks Unified Data Analytics Platform.- Must To Have\n\n\n\n\nSkills:\nStrong understanding of data modeling and database design principles.- Good To Have\n\n\n\n\nSkills:\nExperience with cloud-based data platforms such as AWS or Azure.- Good To Have\n\n\n\n\nSkills:\nExperience with data security and access controls.- Good To Have\n\n\n\n\nSkills:\nExperience with data pipeline development and maintenance.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions.- This position is based at our Chennai office.\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['database design', 'data modeling', 'data analytics', 'microsoft azure', 'design principles', 'hive', 'data warehousing', 'sql', 'java', 'spark', 'design patterns', 'mysql', 'hadoop', 'big data', 'etl', 'rest', 'python', 'data security', 'javascript', 'sql server', 'data bricks', 'pipeline', 'kafka', 'sqoop', 'aws']",2025-06-13 05:19:57
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Chennai'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\nRoles & Responsibilities:- Assist with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data pipelines using Databricks Unified Data Analytics Platform.- Design and implement data security and access controls using Databricks Unified Data Analytics Platform.- Troubleshoot and resolve issues related to data platform components using Databricks Unified Data Analytics Platform.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nExperience with Databricks Unified Data Analytics Platform.- Must To Have\n\n\n\n\nSkills:\nStrong understanding of data platform components and architecture.- Good To Have\n\n\n\n\nSkills:\nExperience with cloud-based data platforms such as AWS or Azure.- Good To Have\n\n\n\n\nSkills:\nExperience with data security and access controls.- Good To Have\n\n\n\n\nSkills:\nExperience with data pipeline development and maintenance.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions.-This position is based at our Bangalore, Hyderabad, Chennai and Pune Offices.- Mandatory office (RTO) for 2- 3 days and have to work on 2 shifts (Shift A- 10:00am to 8:00pm IST and Shift B - 12:30pm to 10:30 pm IST)\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'data security', 'microsoft azure', 'data bricks', 'aws', 'hive', 'amazon redshift', 'pyspark', 'data warehousing', 'emr', 'sql', 'java', 'data modeling', 'spark', 'mysql', 'hadoop', 'big data', 'etl', 'python', 'machine learning', 'sql server', 'nosql', 'pipeline', 'amazon ec2', 'kafka', 'sqoop']",2025-06-13 05:19:59
Data Platform Engineer,Accenture,2 - 7 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 2 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'data analysis', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-13 05:20:01
Data Platform Engineer,Accenture,3 - 8 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'data analysis', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-13 05:20:03
"Level 2 Market Data Support Engineer (Windows, Linux, SQL, IM)",Synechron,3 - 7 years,Not Disclosed,['Pune'],"Job Summary\nSynechron is seeking a technically skilled and proactive Level 2 Market Data Support Engineer specializing in Market Data operations to join our dedicated back-office support team. This role is pivotal in maintaining the stability, security, and efficiency of our market data systems and related infrastructure. The ideal candidate will deliver advanced technical support, troubleshoot complex issues, and contribute to process improvements, ensuring seamless access and data accuracy in a highly regulated financial environment.\nThis position provides an opportunity to work with cross-functional teams, implement best practices, and support critical business functions. It requires a combination of technical expertise, analytical acumen, and effective communication skills to sustain the integrity of our market data platform and ensure high service standards.\nSoftware Requirements\nRequired Skills:\nProficiency with Windows Server management and troubleshooting (user accounts, system configuration)\nExperience managing and troubleshooting Unix/Linux environments\nStrong SQL skills for data retrieval, analysis, and troubleshooting (writing complex queries)\nHands-on experience with ITSM tools such as ServiceNow and JIRA for incident tracking\nFamiliarity with version control tools like Git for release and deployment management\nKnowledge of Batch Processing Systems to analyze historic system trends\nPreferred Skills:\nExposure to cloud platforms (AWS, Azure) is a plus\nKnowledge of monitoring tools and infrastructure automation techniques\nExperience with scripting languages such as PowerShell or Bash\nOverall Responsibilities\nProvide Level 2 support for market data systems, addressing incidents, service requests, and operational issues\nTroubleshoot and resolve technical issues related to Windows and Unix/Linux server environments, databases, and network/configuration discrepancies\nConduct root cause analysis to identify recurrence patterns and develop strategies for systemic resolution\nAssist with system upgrades, patches, and deployment procedures, ensuring minimal impact on business operations\nMonitor system health, review logs, and generate reports on system performance, data accuracy, and incident trends\nDocument procedures, incident resolutions, and system configurations to facilitate ongoing knowledge sharing\nCollaborate closely with cross-functional teams, including application support, infrastructure, and security teams to resolve technical problems effectively\nSupport incident progress tracking and facilitate resolution through ITSM tools\nWork in rotational shifts from 11:00 am IST to Midnight IST, ensuring 24/7 operational support\nStrategic objectives:\nEnsure high availability, data integrity, and security of market data systems\nEnhance incident response processes and reduce recurring issues\nDrive continuous process improvements and operational efficiencies\nPerformance outcomes:\nTimely resolution of incidents with minimal business impact\nAccurate documentation and effective communication with stakeholders\nSuccessful implementation of upgrades and systemic enhancements\nTechnical Skills (By Category)\nOperating Systems (Essential):\nWindows Server (administration, troubleshooting)\nUnix/Linux distributions (server management, scripting, troubleshooting)\nDatabases & Data Management (Essential):\nSQL query development and troubleshooting\nData integrity checks and analysis\nIncident & Change Management (Essential):\nUse of ServiceNow and JIRA platforms for incident management and tracking\nInfrastructure & Network (Essential):\nBasic understanding of networking and system configurations\nAbility to troubleshoot connectivity issues related to server and network\nScripting & Automation (Preferred):\nPowerShell, Bash scripting for routine automation and data analysis\nAdditional Skills (Preferred):\nMonitoring tools and dashboards (e.g., Nagios, LogicMonitor)\nCloud environments experience (AWS, Azure)\nExperience Requirements\n3 to 7 years of experience in market data support, IT operations, or application support roles within capital markets or financial sectors\nProven experience troubleshooting Windows and Unix/Linux server environments\nFamiliarity with database query formulation, analysis, and data reconciliation\nStrong incident management experience using ITSM tools (ServiceNow, JIRA)\nExperience supporting mission-critical financial systems is preferred\nAlternative pathways:\nCandidates with extensive technical support in related financial support roles demonstrating problem-solving and troubleshooting skills may be considered\nDay-to-Day Activities\nMonitor market data system logs and dashboards for anomalies\nTroubleshoot and resolve hardware, OS, database, and network issues\nAnalyze incident tickets, perform root cause analysis, and escalate as needed\nAssist in deploying patches, upgrades, and system changes with minimal disruption\nGenerate performance and incident trend reports\nMaintain detailed documentation, runbooks, and knowledge base articles\nCoordinate with infrastructure, application support, and security teams for issue resolution and process improvements\nSupport system audits, security compliance, and performance testing\nEngage in shift handovers, communicate incident status, and support team collaboration\nQualifications\nBachelor's degree in Computer Science, Information Technology, or related field; equivalent work experience accepted\nRelevant certifications such as ITIL Foundation, Certified Support Engineer, or industry-specific certifications are a plus\nWillingness to work rotational shifts from 11:00 am IST to Midnight IST\nDemonstrated technical expertise in Windows, Unix/Linux, databases, and incident management\nStrong analytical and troubleshooting skills with attention to detail\nEffective communicator capable of liaising with technical and non-technical stakeholders\nProfessional Competencies\nCritical thinking and advanced problem-solving capabilities\nExcellent verbal and written communication skills\nStakeholder-focused mindset with an emphasis on service quality\nStrong organizational skills for managing multiple incidents and tasks\nFlexibility and adaptability to changing priorities and shift schedules\nInitiative for continuous learning and process improvement\nCollaborative approach to team working and cross-departmental cooperation",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Market Data Support', 'Batch Processing Systems', 'ServiceNow', 'Git', 'Linux', 'Windows Server management', 'Unix/Linux', 'ITSM tools', 'Windows', 'Incident Management', 'SQL']",2025-06-13 05:20:05
Software Engineer Data Privacy,Bajaj Finserv Health,4 - 6 years,Not Disclosed,['Pune( Viman Nagar )'],"Job Description:\nKnowledgeable and proactive Data Privacy engineer to manage privacy and data protection initiatives in compliance with Indian data privacy regulations, including the Digital Personal Data Protection Act, 2023 (DPDP Act).\nThis role will work closely with legal, IT, security, HR, and business teams to implement privacy by design and foster a strong culture of privacy data protection\n\nKey Objectives/Responsibilities of this Role:\nImplement and oversee the data privacy program implementation in accordance with the Privacy Laws and requirements (DPDP Act, 2023 and other applicable Indian laws)\nDocument and maintain records of processing activities\nDraft, review, and update privacy notices, policies, consent mechanisms, and contractual clauses for data processing.\nConduct and review Data Protection Impact Assessments (DPIAs) and help identify and mitigate privacy risks.\nTrain and educate employees on Data privacy requirements and best practices.\nPerform regular compliance audits and assessments across internal departments and third-party vendors.\nDevelop and deliver training and awareness programs to ensure employees understand their responsibilities under the DPDP Act and internal privacy policies.\nHandle and ensure timely response to data principal rights requests (access, correction, erasure, grievance redressal) as per Privacy Law requirements\nCollaborate with legal and IT/security teams to ensure privacy by design and default in new processes, technologies, and products.\nStay updated with latest developments in data protection laws and regulations.\nSupport incident response and breach notification processes as per regulatory timelines and requirements.\nAssist the Data Privacy Officer in preparing reports for internal leadership or regulators, as required.\nServe as a subject matter expert on Indian data privacy regulations and provide guidance to cross-functional teams.\n\nMandatory Skillset & Experience:\nMinimum 4+ years of relevant experience in information security, compliance, or legal roles with focus on data privacy\nIn-depth knowledge of the Data Protection and Privacy Regulations, and awareness of related Indian IT regulations.\nExperienced in using Data protection management tools and softwares\nUnderstanding of global data privacy frameworks (GDPR, etc.) is a plus.\nStrong analytical and problem-solving skills with a practical approach to risk and compliance.\nAbility to communicate complex privacy topics in a clear and business-friendly manner.\nExperience working with privacy compliance tools and risk management systems.\n\nBehavioral Skills:\nSelf-driven and proactive.\nPlan and execute projects in a timely fashion meeting the project timelines\nExperienced in coordinating activities across different teams and stakeholders\nContinuous process improvement / quality assurance experience is a plus\nProactively identify potential data privacy issues and problems in business processes\n\nPreferred Qualification:\nBachelors or Masters degree in law, Technology, Compliance, Risk Management, or a related field.\n\nPreferred Certifications:\nDCPP (Data Protection Certified Professional - India)\nCIPP/A, CIPM (IAPP certifications)\nISO/IEC 27701, ISO 27001 (Good to have)",Industry Type: Medical Services / Hospital,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Protection Manager', 'Privacy Regulation', 'Data Privacy Law', 'Data Privacy', 'Gdpr']",2025-06-13 05:20:06
Lead Engineer - Data Science,Sasken Technologies,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Summary\nPerson at this position takes ownership of a module and associated quality and delivery. Person at this position provides instructions, guidance and advice to team members to ensure quality and on time delivery.\nPerson at this position is expected to be able to instruct and review the quality of work done by technical staff.\nPerson at this position should be able to identify key issues and challenges by themselves, prioritize the tasks and deliver results with minimal direction and supervision.\nPerson at this position has the ability to investigate the root cause of the problem and come up alternatives/ solutions based on sound technical foundation gained through in-depth knowledge of technology, standards, tools and processes.\nPerson has the ability to organize and draw connections among ideas and distinguish between those which are implementable.\nPerson demonstrates a degree of flexibility in resolving problems/ issues that atleast to in-depth command of all techniques, processes, tools and standards within the relevant field of specialisation.\n\n\nRoles & Responsibilities\nResponsible for requirement analysis and feasibility study including system level work estimation while considering risk identification and mitigation.\nResponsible for design, coding, testing, bug fixing, documentation and technical support in the assigned area. Responsible for on time delivery while adhering to quality and productivity goals.\nResponsible for traceability of the requirements from design to delivery Code optimization and coverage.\nResponsible for conducting reviews, identifying risks and ownership of quality of deliverables.\nResponsible for identifying training needs of the team.\nExpected to enhance technical capabilities by attending trainings, self-study and periodic technical assessments.\nExpected to participate in technical initiatives related to project and organization and deliver training as per plan and quality.\nExpected to be a technical mentor for junior members.\nPerson may be given additional responsibility of managing people based on discretion of Project Manager.\n\nEducation and Experience Required\nEngineering graduate, MCA, etc Experience: 5-8 years\n\n\nCompetencies Description\nData Science TCB is applicable to one who\n1) Analyses data to arrive at patterns/Insights/models\n2) Come up with models based on the data to provide recommendations, predictive analytics etc\n3) Provides implementation of the models in R, Matlab etc\n4) Can understand and apply machine learning/AI techniques\nPlatforms-\nUnix\nTools-\nR, Matlab, Spark Machine Learning, Python-ML, SPSS, SAS\nLanguages-\nR, Perl, Python, Scala\nSpecialization-\nCOGNITIVE ANALYTICS INCLUDING COMPUTER VISION, AI and ML, STATISTICS",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Unix', 'R', 'SAS', 'Scala', 'Perl', 'SPSS', 'Machine Learning', 'Python']",2025-06-13 05:20:08
"Sr. Staff Engineer, Data Frameworks",NetSkope Software,10 - 15 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","As a Sr. Staff Engineer on the Data Engineering Team you'll be working on some of the hardest problems in the field of Data, Cloud and Security with a mission to achieve the highest standards of customer success. You will be building blocks of technology that will define Netskope s future. You will leverage open source Technologies around OLAP, OLTP, Streaming, Big Data and ML models. You will help design, and build an end-to-end system to manage the data and infrastructure used to improve security insights for our global customer base.\nYou will be part of a growing team of renowned industry experts in the exciting space of Data and Cloud Analytics\nYour contributions will have a major impact on our global customer-base and across the industry through our market-leading products\nYou will solve complex, interesting challenges, and improve the depth and breadth of your technical and business skills.\nWhat you will be doing\nConceiving and building services used by Netskope products to validate, transform, load and perform analytics of large amounts of data using distributed systems with cloud scale and reliability.\nHelping other teams architect their applications using services from the Data team wile using best practices and sound designs.\nEvaluating many open source technologies to find the best fit for our needs, and contributing to some of them.\nWorking with the Application Development and Product Management teams to scale their underlying services\nProviding easy-to-use analytics of usage patterns, anticipating capacity issues and helping with long term planning\nLearning about and designing large-scale, reliable enterprise services.\nWorking with great people in a fun, collaborative environment.\nCreating scalable data mining and data analytics frameworks using cutting edge tools and techniques\nRequired skills and experience\n10+ years of industry experience building highly scalable distributed Data systems\nProgramming experience in Python, Java or Golang\nExcellent data structure and algorithm skills\nProven good development practices like automated testing, measuring code coverage.\nProven experience developing complex Data Platforms and Solutions using Technologies like Kafka, Kubernetes, MySql, Hadoop, Big Query and other open source databases\nExperience designing and implementing large, fault-tolerant and distributed systems around columnar data stores.\nExcellent written and verbal communication skills\nBonus points for contributions to the open source community\nEducation\nBSCS or equivalent required, MSCS or equivalent strongly preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'data security', 'MySQL', 'OLAP', 'Application development', 'Open source', 'Data mining', 'OLTP', 'Distribution system', 'Python']",2025-06-13 05:20:10
Data Science & AI Engineer,Blue Altair,5 - 8 years,Not Disclosed,['Pune'],"Greetings from Blue Altair!\nJob Overview:\nWe are seeking an experienced and highly skilled Data Science and AI Engineer to join our dynamic team. The ideal candidate will have 5+ years of experience working on cutting-edge data science and AI technologies across various cloud platforms with a strong focus to work on LLMs and SLMs. The role demands a professional capable of performing in a client-facing environment, as well as mentoring and guiding junior team members.\n\nTitle: Consultant/Sr. Consultant - Data Science Engineer\nExperience: 5-8 years\nLocation: Pune/Bangalore (Hybrid)\n\nRoles and responsibilities:\nDevelop, implement, and optimize machine learning models and AI algorithms to solve complex business problems.\nDesign, build, and fine-tune AI models, particularly focusing on LLMs and SLMs, using state-of-the-art techniques and architectures.\nApply advanced techniques in prompt engineering, model fine-tuning, and optimization to tailor models for specific business needs.\nDeploy and manage machine learning models and pipelines on cloud platforms (AWS, GCP, Azure, etc.).\nWork closely with clients to understand their data and AI needs and provide tailored solutions.\nCollaborate with cross-functional teams to integrate AI solutions into broader software architectures.\nMentor junior team members and provide guidance in implementing best practices in data science and AI development.\nStay up-to-date with the latest trends and advancements in data science, AI, and cloud technologies.\nPrepare technical documentation and present insights to both technical and non-technical stakeholders.\n\nRequirement:\n5+ years of experience in data science, machine learning, and AI technologies.\nProven experience working with cloud platforms such as Google Cloud, Microsoft Azure, or AWS.\nExpertise in programming languages such as Python, R, Julia, and AI frameworks like TensorFlow, PyTorch, Scikit-learn, Hugging face Transformers.\nKnowledge of data visualization tools (e.g., Matplotlib, Seaborn, Tableau)\nSolid understanding of data engineering concepts including ETL, data pipelines, and databases (SQL, NoSQL).\nExperience with MLOps practices and deployment of models in production environments.\nFamiliarity with NLP (Natural Language Processing) tasks and working with large-scale datasets.\nHands-on experience with generative AI models like GPT, Gemini, Claude, Mistral etc.\nClient-facing experience with strong communication skills to manage and engage stakeholders.\nStrong problem-solving skills and analytical mindset.\nAbility to work independently and as part of a team and mentor and provide technical leadership to junior team members.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['LLMs', 'Artificial Intelligence', 'MLOps', 'RAG', 'Natural Language Processing', 'Neural Networks', 'LLM', 'Machine Learning', 'AI Models', 'Data Science', 'PyTorch', 'SLM', 'AI Automation']",2025-06-13 05:20:12
Senior Engineering Manager - Data Operations,Amgen Inc,12 - 15 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nWe are seeking a seasoned Senior Engineering Manager(Data Engineering) to lead the end-to-end management of enterprise data assets and operational data workflows. This role is critical in ensuring the availability, quality, consistency, and timeliness of data across platforms and functions, supporting analytics, reporting, compliance, and digital transformation initiatives.As a senior leader in the data organization, you will oversee the day-to-day data operations, manage a team of data professionals, and drive process excellence in data intake, transformation, validation, and delivery. You will work closely with cross-functional teams including data engineering, analytics, IT, governance, and business stakeholders to align operational data capabilities with enterprise needs.",,,,"['Data Operations', 'Azure', 'Data Engineering', 'Neo4J', 'GCP', 'Engineering Management', 'troubleshooting', 'Stardog', 'Marklogic', 'AWS']",2025-06-13 05:20:13
"Sr Validation Engineer, R&D Data Catalyst Team",Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"Role Description:\nThe Validation and Testing Leadis responsible forleading the testing activities for software applications and solutions that meet business needs and ensuring the availability of critical systems and applications. This roleis for a lead tester with experience testing data management solutions and data analytics products, and with experience with designing and executing testing for GxP validated systems. The role involves working closely with product managers, designers, and other engineers to test high-quality, scalable software solutions.\nRoles & Responsibilities:",,,,"['Validation engineering', 'project management', 'data analytics', 'data validation', 'troubleshooting', 'agile', 'change management', 'sql', 'data profiling', 'jira']",2025-06-13 05:20:15
Senior Associate Data Security Engineer,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role the Senior Associate Data Security Engineer role will cover Data Loss Prevention (DLP) and Data Security Posture Management (DSPM) technologies. This role will report to the Manager, Data Security. This position will provide essential services that enable us to better pursue our mission.\nSr. Associate Data Security Engineers operate, manage, and improve Amgens DLP and DSPM solutions. In our Data Security team, they will operate data protection security technologies in a rapidly changing global security sector. They will work with other engineers and business units to help craft, build, coordinate, configure, and implement critical preventive and detective security controls related to the protection of Amgen data.",,,,"['Data Security', 'PowerShell', 'configuration management', 'Cloud Access Security', 'Linux', 'Incident management', 'ITIL', 'Python', 'SQL']",2025-06-13 05:20:17
Senior Staff Engineer - Data Science,The TJX Companies Inc,10 - 15 years,Not Disclosed,['Hyderabad'],"TJX Companies\nAt TJX Companies, every day brings new opportunities for growth, exploration, and achievement. You ll be part of our vibrant team that embraces diversity, fosters collaboration, and prioritizes your development. Whether you re working in our four global Home Offices, Distribution Centers or Retail Stores TJ Maxx, Marshalls, Homegoods, Homesense, Sierra, Winners, and TK Maxx, you ll find abundant opportunities to learn, thrive, and make an impact. Come join our TJX family a Fortune 100 company and the world s leading off-price retailer.\nJob Description:\nSr.Staff Engineer- Data Science\nWhat you ll discover\nInclusive culture and career growth opportunities\nA truly Global IT Organization that collaborates across North America, Europe, Asia and Australia, click here to learn more\nChallenging, collaborative, and team-based environment\nWhat you ll do\nThe Global Supply Chain - Logistics Team is responsible for managing various supply chain logistics related solutions within TJX IT. The organization delivers capabilities that enrich the customer experience and provide business value. We seek a motivated, talented Staff Engineer with good understanding of cloud base, database and BI concepts to help architect enterprise reporting solutions across global buying, planning and allocations.\nWhat you ll need\nThe Global Supply Chain - Logistics Team thrives on strong relationships with our business partners and working diligently to address their needs which supports TJX growth and operational stability. On this tightly knit and fast-paced solution delivery team you will be constantly challenged to stretch and think outside the box.\nYou will be working with product teams, architecture and business partners to strategically plan and deliver the product features by connecting the technical and business worlds. You will need to break down complex problems into steps that drive product development while keeping product quality and security as the priority. You will be responsible for most architecture, design and technical decisions within the assigned scope.\nKey Responsibilities:\nDesign, develop, test and deploy AI solutions using Azure AI services to meet business requirements.\nTrain, fine-tune, and evaluate AI models, including large language models (LLMs), ensuring they meet performance criteria and integrate seamlessly into new or existing solutions.\nDevelop and integrate APIs to enable smooth interaction between AI models and other applications, facilitating efficient model serving.\nCollaborate effectively with cross-functional teams, including data scientists, software engineers, and business stakeholders, to deliver comprehensive AI solutions.\nOptimize AI and ML model performance through techniques such as hyperparameter tuning and model compression to enhance efficiency and effectiveness.\nMonitor and maintain AI systems, providing technical support and troubleshooting to ensure continuous operation and reliability.\nCreate comprehensive documentation for AI solutions, including design documents, user guides, and operational procedures, to support development and maintenance.\nStay updated with the latest advancements in AI, machine learning, and cloud technologies, demonstrating a commitment to continuous learning and improvement.\nDesign, code, deploy, and support software components, working collaboratively with AI architects and engineers to build impactful systems and services.\nLead medium to large initiatives, prioritizing and assigning tasks, providing guidance, and resolving issues to ensure successful project delivery.\nMinimum Qualifications\nBachelors degree in computer science, engineering, or related field\n10+ years of experience in data/software engineering, design, implementation and architecture.\nAt least 5+ years of hands-on experience in developing AI/ML solutions, with a focus on deploying them in a cloud environment.\nDeep understanding of AI and ML algorithms with focus on Operations Research / Optimization knowledge (preferably Metaheuristics / Genetic Algorithms).\nStrong programming skills in Python with advanced OOPS concepts.\nGood understanding of structured, semi structured, and unstructured data, Data modelling, Data analysis, ETL and ELT.\nProficiency with Databricks & PySpark.\nExperience with MLOps practices including CI/CD for machine learning models.\nKnowledge of security best practices for deploying AI solutions, including data encryption and access control.\nKnowledge of ethical considerations in AI, including bias detection and mitigation strategies.\nThis role operates in an Agile/Scrum environment and requires a solid understanding of the full software lifecycle, including functional requirement gathering, design and development, testing of software applications, and documenting requirements and technical specifications.\nFully Owns Epic with limited or no guidance. Gives guidance and unblocks others; finds opportunities to mentor and grow teammates.\nDemonstrated leadership in the fields of Data/ Software Engineering or data science.\nStrong communication and influence skills. Solid team leadership with mentorship skills\nAbility to understand the work environment and competing priorities in conjunction with developing/meeting project goals\nShows a positive, open-minded and can-do attitude\nExperience in the following technologies:\nAdvanced Python programming (OOPS)\nOperations Research / Optimization knowledge (preferably Metaheuristics / Genetic Algorithms)\nDatabricks with Pyspark\nAzure / Cloud knowledge\nGithub / version control\nFunctional knowledge on Supply Chain / Logistics is preferred.\nIn addition to our open door policy and supportive work environment, we also strive to provide a competitive salary and benefits package. TJX considers all applicants for employment without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, gender identity and expression, marital or military status, or based on any individuals status in any group or class protected by applicable federal, state, or local law. TJX also provides reasonable accommodations to qualified individuals with disabilities in accordance with the Americans with Disabilities Act and applicable state and local law.\nAddress:\nSalarpuria Sattva Knowledge City, Inorbit Road\nLocation:\nAPAC Home Office Hyderabad IN",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Supply chain', 'Computer science', 'Data analysis', 'Version control', 'Machine learning', 'Troubleshooting', 'Project delivery', 'Technical support', 'Python', 'Logistics']",2025-06-13 05:20:18
Staff Software Engineer - Cloud Data Pipeline,Calix,12 - 15 years,Not Disclosed,['Bengaluru'],"This position is based in Bangalore, India.\n\nCalix is leading a service provider transformation to deliver a differentiated subscriber experience around the Smart Home and Business, while monetizing their network using Role based Cloud Services, Telemetry, Analytics, Automation, and the deployment of Software Driven Adaptive networks.\nAs part of a high performing global team, the right candidate will play a significant role as Calix Cloud Data Engineer involved in architecture design, implementation, technical leadership in data ingestion, extraction, and transformation domain.\nResponsibilities and Duties:\nTechnical leadership in all phases of software design and development in meeting requirements of service stability, reliability, scalability, and security.\nHiring, training, provide technical direction and lead discussions and coordinate deliverables across multiple engineering teams globally.\nWork closely with Cloud product owners to understand, analyze product requirements, provide feedback, coordinate resources and deliver a complete solution.\nDrive evaluation and selection of best fit, efficient, cost-effective solution stack for the Calix Cloud data platform.\nDrive development of scalable data pipeline infrastructure and services for enabling operationally efficient analytics solutions for Calix Cloud suite of products.\nCreate and extend data lake solution to enable data science workbenches and implement quality systems to ensure data quality, consistency, security, compliance, and lineage.\nDrive continuous optimization of the data pipelines with automation, and tools.\nHave a Test first mindset and use modern DevSecOps practices for Agile development.\nCollaborate with senior leadership to translate platform opportunities into an actionable roadmap, track progress, and deliver new platform capabilities on-time and on-budget.\nTriage and resolve customer escalations and technical issues.\nQualifications:\n12 + years of highly technical, hands-on software engineering experience with at least 7 years of Cloud based solution development\n3+ experience leading and mentoring engineering team with strong technical direction and delivering high quality software on schedule, including delivery for large, cross-functional projects and working with geographically distributed teams\nStrong, creative problem-solving skills and ability to abstract and share details to create meaningful articulation.\nPassionate about delivering high quality software solutions and enabling automation in all phases.\nGood understanding of big data engineering challenges and proven experience with data platform engineering (batch and streaming, ingestion, storage, processing, management, governance, integration, consumption patterns)\nExperience in designing and performance tuning batch-based, low latency real-time streaming and event-based data solutions (Kafka, Spark, Flink or similar frameworks).\nPractical experience of architecting with GCP Cloud platform and services and especially the Data ecosystem: BigQuery, Datastream, DataProc, Composer etc.\nDeep understanding of Data Cataloging, Data Governance, Data Privacy principles and frameworks to integrate into the Data engineering flows.\nAdvanced knowledge of Data Lake technologies, data storage formats (Parquet, ORC, Avro) and query engines and associated concepts for consumption layers.\nExperience implementing solutions that adhere to best practices and guidelines for different privacy and compliance practices around data (GDPR, CCPA).\nHands on expert level on one or more of the following programming languages - Python, Java\nOrganized and goal-focused, ability to deliver in a fast-paced environment.\nBS degree in Computer Science, Engineering, Mathematics, or relevant industry standard experience to match",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Performance tuning', 'Automation', 'Software design', 'Quality systems', 'data governance', 'Data quality', 'data privacy', 'Analytics', 'Python']",2025-06-13 05:20:20
"Software Engineer II, Backend - Data Platform",Abnormal Ai,3 - 8 years,Not Disclosed,['Bengaluru'],"We are seeking a highly skilled Software Engineer II to help shape the future of AI-powered application development. If youre passionate about cutting-edge technology, scalable systems, and solving real-world challenges, this is your opportunity.\n\nWhat Youll Do as a Software Engineer II\nAs part of our engineering team, you will:\n\nLeverage AI-powered Development Use Cursor, Copilot, and other AI tools to enhance productivity, optimize workflows, and automate repetitive tasks.\nDevelop Data-Driven Applications Build consumer-grade interfaces and APIs that power our advanced behavioral AI insights.\nCombat Modern Cyber Threats Design and deploy secure, scalable systems that detect and prevent sophisticated cyberattacks.\nCollaborate with Fortune 500 Enterprises Work with customers and security teams to rapidly iterate and deliver impactful solutions.\nBuild at Scale Design backend services and cloud architectures that support billions of security events across enterprises worldwide.\n\nTechnical Requirements\nWere looking for engineers who have:\n\n3+ years of professional experience working on data-intensive applications and distributed systems. This is not a Data Engineering role.\nBackend development experience with Python or Go.\nDepth in at least one key area of the data platform tech stack - batch processing, streaming systems, data orchestration, data infrastructure.\nFamiliarity with AI development tools such as Cursor, GitHub Copilot, or Claude.\nExperience / passion in building scalable, enterprise-grade applications.\nExperience with any big data technologies such as Spark or Databricks or Trino or Kafka/Hadoop etc.\nKnowledge of cloud platforms (AWS, GCP, or Azure) and containerization (Docker, Kubernetes).\nStrong fundamentals in computer science, data structures, and performance optimization.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Access control', 'Backend', 'Compliance', 'Software Engineer II', 'Manager Technology', 'Healthcare', 'Sales Development Representative', 'Genetics', 'Security operations', 'Recruitment']",2025-06-13 05:20:22
Remote Data Visualization Engineer 36Lakhs CTC|| Kandi Srinivasa Reddy,Integra Technologies,8 - 11 years,35-37.5 Lacs P.A.,"['Kolkata', 'Ahmedabad', 'Bengaluru']","Dear Candidate,\nWe are seeking a Data Visualization Engineer to turn complex data into clear, engaging visual insights that empower business decisions. This role involves working closely with analysts and stakeholders to build interactive dashboards and reports.\n\nKey Responsibilities:\nDesign and develop visualizations using tools like Power BI, Tableau, or Looker.\nTranslate data into compelling stories and business insights.\nOptimize dashboard performance and usability for end users.\nCollaborate with data engineering and analytics teams.\nImplement visualization standards and data governance practices.\nRequired Skills & Qualifications:\nProficiency in data visualization tools (Power BI, Tableau, D3.js).\nStrong knowledge of SQL and data modeling.\nUnderstanding of UX principles in data presentation.\nExperience working with large datasets and cloud-based data platforms.\nKnowledge of scripting languages (Python, R) is a plus.\nSoft Skills:\nStrong troubleshooting and problem-solving skills.\nAbility to work independently and in a team.\nExcellent communication and documentation skills.\n\nNote: If interested, please share your updated resume and preferred time for a discussion. If shortlisted, our HR team will contact you.\n\nKandi Srinivasa Reddy\nDelivery Manager\nIntegra Technologies",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Tableau', 'Quicksight', 'Data Visualization', 'Dashboard Development', 'Business Intelligence', 'Power Bi', 'Bi Tools', 'Dashboarding', 'Business Objects', 'Reporting Tools', 'SQL', 'Microstrategy', 'Dashboards', 'QlikView']",2025-06-13 05:20:24
Oracle BRM Data Migration Engineer,Techstar Group,5 - 10 years,Not Disclosed,"['Noida', 'Hyderabad', 'Bengaluru']","Work Location : Hyderabad, Bangalore, Noida, Pune\nQualifications and Skills :\n- Proven expertise in Oracle BRM (Mandatory skill) with a strong understanding of its architecture and modules to effectively manage data migration processes.\n\n- Hands-on experience in data migration activities, particularly with Oracle BRM, ensuring high efficiency and accuracy throughout migration projects.\n\n- Knowledge in SQL for querying and managing databases, crucial for data migration and integration tasks.\n\n- Strong knowledge of ETL tools and processes for efficient data extraction, transformation, and loading from various sources.\n\n- Ability to perform detailed data mapping, ensuring logical transformation and compatibility between source and target system data structures.\n\n- Experience in data cleansing techniques to ensure data integrity and consistency throughout the migration process.\n\n- Understanding of data quality principles and practices, essential to maintain high standards of data accuracy and dependability.\n\n- Proficiency in scripting for automation of data migration tasks, enhancing efficiency and reducing potential for errors.\n\n- Excellent analytical and problem-solving skills to identify and address data-related challenges and opportunities.\n\n- Handling the execution of the data migration and validations.\n\n- Handle the develop Migration strategy documents and techniques. Execute data integrity testing post migration.\n\n- Understanding BRM : Having a working knowledge of BRM data migration components, the BRM 12 schema, and the data model\n\n- Data migration strategy : Developing a migration strategy and implementation plan\n\n- Data loading : Being able to load data and integrate it with systems\n\n- Post-migration analysis : Performing post-migration analysis on events, invoices, open items, bills, and dunning\n\n- Data reconciliation : Developing scripts to reconcile migrated data\n\n- Working Knowledge of all the BRM Data migration components.\n\n- Must have hands-on in BRM to verify the sanity of the Data migration.\n\n- Advantage - Programming skills on Java technologies. Exp. in C/C++, Oracle 12c/19c, PL/SQL, PCM Java, BRM Webservice, Scripting language (perl/python)\n\nRoles and Responsibilities :\n\n- Analyze client data and formulating effective data migration plans tailored to Oracle BRM specifications.\n\n- Collaborate with cross-functional teams to gather and interpret data migration requirements accurately.\n\n- Develop and implement efficient data migration scripts and processes, ensuring minimal disruption to business operations.\n\n- Conduct thorough testing and validation of data migration outputs to guarantee data accuracy and conformity.\n\n- Monitor and troubleshoot migration activities to ensure seamless execution and rectify any issues promptly.\n\n- Document data migration processes, maps, and transformations for knowledge sharing and continuous improvement.\n\n- Liaise with stakeholders to present progress updates and discuss ongoing improvements to data migration practices.\n\n- Contribute to the development of data migration best practices and reusable frameworks within the organization.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Oracle BRM', 'Java', 'Data Quality', 'Oracle Apps', 'C++', 'PL/SQL', 'Data Migration', 'Data reconciliation', 'ETL', 'SQL']",2025-06-13 05:20:25
Staff Software Engineer - Cloud Infrastructure-Network Data Ingestion,Calix,3 - 7 years,Not Disclosed,['Bengaluru'],"Calix is leading a service provider transformation to deliver a differentiated subscriber experience around the Smart Home and Business, while monetizing their network using Role based Cloud Services, Telemetry, Analytics, Automation, and the deployment of Software Driven Adaptive networks.\n\nAs part of a high performing global engineering team, the right candidate will play a critical role in expanding the Calix Cloud Infrastructure capabilities and be part of a team that leads the effort defining and architecting a world class in-home eco-system. Calix Cloud empowers service providers with solutions for Insights and real time data to support delivery of the best customer experiences, improve operational efficiency and drive market penetration.\nResponsibilities\nDesign, develop and maintain backend infrastructure, workflows, and services for collection, processing, analysis, correlation, and monitoring in Calix Cloud\nDevelop solutions to support onboarding, partner integrations, managing, collecting, and analyzing data from large scale deployment of home networks and access network systems and make them available as insights for various BSP user roles.\nWork closely with Cloud product owners to understand, analyze product requirements, provide feedback, and deliver a complete solution.\nTechnical leadership of software design in meeting requirements of service stability, reliability, scalability, and security\nParticipate and drive technical discussions within engineering group in all phases of the SDLC: review requirements, produce design documents, participate in peer reviews, produce test plans, support QA team, provide internal training and support TAC team.\nSupport test strategy and automation in both end-to-end solution and functional testing.\nCustomer facing engineering role in debugging and resolving field issues.\nQualifications:\n10+ years of highly technical, hands-on software engineering experience delivering quality software releases.\nIndependent and Self driven and works in a Team.\nStrong, creative problem-solving skills and ability to abstract and share details to create meaningful articulation.\nAbility to drive technical discussions across x-functional teams.\nStrong Implementation background in distributed design, data consumption patterns, and pipelines and experience in designing real-time streaming and event-based data solutions\nProficient in design and implementation of microservices-based, API/Endpoint architectures\nStrong background in designing and developing event-based / pub-sub workflows & data ingestion solutions. Proficiency and hands on experience with Kafka at scale (or similar) desired.\nGood Experience with load balancers, WebSocket, MQTT and similar technologies at different layers for efficient data abstraction and transfer for large scale data connections / large flow of data\nGood understanding of implementation and deployment of Cloud based solutions (preferably GCP)\nStrong background in transactional databases and good understanding and experience with no-SQL datastores and working in defining optimal data models.\nGood understanding of Networking concepts.\nExpert in Go. Proficiency in other languages like Java, Python/JavaScript a plus.\nOrganized and goal-focused, ability to deliver in a fast-paced environment.\nEducation:\nBS degree in Computer Science, engineering, or mathematics or equivalent experience.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Software design', 'Functional testing', 'Debugging', 'Javascript', 'SDLC', 'Analytics', 'Monitoring', 'SQL', 'Python']",2025-06-13 05:20:27
Avaloq Software Engineer (Data),Luxoft,3 - 5 years,Not Disclosed,[],"Your expertise in Avaloq technologies, database management, and software development combined with a solid understanding of the Avaloq Enterprise-Wide Object Model will be critical in shaping and enhancing our data capabilities. This is a fantastic opportunity to bring your passion for innovation into a fast-paced, dynamic environment where your impact will be tangible.\nSkills\nMust have\nYou will have between 3-5 years experience working as an Avaloq Developer.\nMapping physical data from Avaloq and other platforms to the Enterprise Data Model, ensuring adherence to reference data standards and clear conceptual definitions.\nPerforming data quality assessments, developing strategies to enhance data integrity, and creating Avaloq system functionalities to mitigate the risk of poor data at source.\nSupporting option analysis, data profiling, and interfacing with external rule/result repositories.\nReviewing and optimising Avaloq APDM outcomes, defining treatment strategies, and improving system performance.\nIntegrating the Avaloq APDM with external data minimisation orchestration tools.\nDesigning and delivering data quality improvement solutions, including mass-data manipulation scripts and associated testing and reconciliation processes.\nAnalysing business requirements and developing tailored software solutions.\nSupporting technical analysis and enhancements for Avaloq change requests and incidents.\nCollaborating with stakeholders to build alignment around technology change initiatives.\nBuilding and maintaining synthetic data delivery routines for test environments.\nManaging market data ingestion into the Avaloq Core Platform (ACP) via third-party tools.\nNice to have\nACCP certification\nOther\nLanguages\nEnglish: C2 Proficient\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nData Modelling Expert (with Avaloq experience)\nAvaloq\nIndia\nBengaluru\nAvaloq Technical Lead\nAvaloq\nAustralia\nSydney\nSenior Avaloq Engineer\nAvaloq\nAustralia\nSydney\nRemote India, India\nReq. VR-114137\nAvaloq\nBCM Industry\n21/05/2025\nReq. VR-114137\nApply for Avaloq Software Engineer (Data) in Remote India\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Technical analysis', 'Quality improvement', 'orchestration', 'Data modeling', 'Reconciliation', 'Technical Lead', 'Data quality', 'market data', 'data integrity', 'Software solutions']",2025-06-13 05:20:29
ML Engineer/Data Scientist,Altimetrik,6 - 8 years,15-30 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities\nData Scientist /ML engineers : ML Engineer with Python, SQL, Machine Learning, Azure skills(Good to have)",Industry Type: IT Services & Consulting,,,"['Machine Learning', 'Python', 'SQL', 'Data Science', 'Ml', 'azure']",2025-06-13 05:20:31
Big Data Lead,IQVIA,8 - 13 years,25-40 Lacs P.A.,['Bengaluru'],"Job Title / Primary Skill: Big Data Developer (Lead/Associate Manager)\nManagement Level: G150\nYears of Experience: 8 to 13 years\nJob Location: Bangalore (Hybrid)\nMust Have Skills: Big data, Spark, Scala, SQL, Hadoop Ecosystem.\nEducational Qualification: BE/BTech/ MTech/ MCA, Bachelor or masters degree in Computer Science,\n\nJob Overview\nOverall Experience 8+ years in IT, Software Engineering or relevant discipline.\nDesigns, develops, implements, and updates software systems in accordance with the needs of the organization.\nEvaluates, schedules, and resources development projects; investigates user needs; and documents, tests, and maintains computer programs.\nJob Description:\nWe look for developers to have good knowledge of Scala programming skills and Knowledge of SQL\nTechnical Skills:\nScala, Python -> Scala is often used for Hadoop-based projects, while Python and Scala are choices for Apache Spark-based projects.\nSQL -> Knowledge of SQL (Structured Query Language) is important for querying and manipulating data\nShell Script -> Shell scripts are used for batch processing of data, it can be used for scheduling the jobs and shell scripts are often used for deploying applications\nSpark Scala -> Spark Scala allows you to write Spark applications using the Spark API in Scala\nSpark SQL -> It allows to work with structured data using SQL-like queries and Data Frame APIs.\nWe can execute SQL queries against Data Frames, enabling easy data exploration, transformation, and analysis.\n\nThe typical tasks and responsibilities of a Big Data Developer include:\n1. Data Ingestion: Collecting and importing data from various sources, such as databases, logs, APIs into the Big Data infrastructure.\n2. Data Processing: Designing data pipelines to clean, transform, and prepare raw data for analysis. This often involves using technologies like Apache Hadoop, Apache Spark.\n3. Data Storage: Selecting appropriate data storage technologies like Hadoop Distributed File System (HDFS), HIVE, IMPALA, or cloud-based storage solutions (Snowflake, Databricks).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SCALA', 'Big Data', 'Spark', 'Hive', 'Apache Pig', 'Hadoop', 'Hadoop Development', 'Mapreduce', 'Hdfs', 'Impala', 'YARN']",2025-06-13 05:20:33
Big Data Lead,Hexaware Technologies,8 - 13 years,18-25 Lacs P.A.,"['Chennai', 'Bengaluru', 'Mumbai (All Areas)']","As an Azure Data Engineer, we are looking for candidates who possess expertise in the following:\nDatabricks\nData Factory\nSQL\nPyspark/Spark\n\nRoles and Responsibilities:",,,,"['Databricks', 'Sql', 'Python']",2025-06-13 05:20:34
Big Data Developer - N,Infosys,5 - 10 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Job description\n\nHiring for Bigdata Developer with experience range 5 to 15 years.\n\nMandatory Skills: Bigdata, Scala, Spark, Hive, Kafka\n\nEducation: BE/B.Tech/MCA/M.Tech/MSc./MSts",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hive', 'SCALA', 'Big Data', 'Kafka', 'Spark', 'Bigdata Technologies']",2025-06-13 05:20:36
Cloud SRE - Big Data Platform,ANZ,6 - 11 years,Not Disclosed,['Bengaluru'],"As a Cloud Big Data SRE in our Cloud Infrastructure Team, you ll play a key role in hprimarily lead the engineering and operational activities on an enterprise big data platform (EBD) on GCP. The platform is vast with Petabytes of data mostly retail and commercial bank customers which is used for banks critical customer remediation program and is significant in regulatory compliance. Additionally, the role involves providing platform engineering leadership and mentorship within the EDAE team, significantly influencing the direction and success of ANZs data capabilities on a broad scale.\nBanking is changing and we re changing with it, giving our people great opportunities to try new things, learn and grow. Whatever your role at ANZ, you ll be building your future, while helping to build ours.\nRole Type:Permanent\nRole Location:Bengaluru\n\nWhat will your day look like?\nLead SRE principles adoption in squad through monitoring of reliability of platform and applications, automate operations.\nEnsure services meet defined uptime, performance and latency targets.\nSolving ambiguous and complex data platform engineering problems.\nWork collaboratively within and across teams, Tech Areas and Domains.\nUtilise tools and practices to maintain, verify and deploy solutions in the most efficient ways, we place a high emphasis on Software Fundamentals.\nImplements a culture within the Tribe and the Chapter, encouraging best practices around reviews, quality, and documentation.\nProvide ongoing support for platforms as required e.g. problem and incident management troubleshoot.\nWhat will you bring?\n\nTo grow and be successful in this role, you will ideally bring the following:\nExperienced in managing enterprise-scale big data lake platforms on cloud, with a strong focus on platform engineering, service hosting, and dependency management across distributed systems.\nSpecialize in SRE practices including reliability engineering, observability implementation, proactive monitoring, and incident response operations to ensure platform stability and operational excellence.\nProven experience in devops, automation, manage and version codebases , configurations and infrastructure automation using Terraform.\nProficiency in Python, with experience in a secondary language like Golang or Java.\n6+ years of experience as a Cloud SRE/Engineer (preferably GCP) with applications utilizing services such as:\nBig Data Ecosystem : Hadoop cluster and technologies like Spark, Hive, and HBase deployed on cloud platforms\nWorkflow Orchestration : Airflow (or equivalent managed services) for job scheduling and pipeline orchestration\nContainerization & Orchestration : Kubernetes for scalable deployment and orchestration of containerized workloads\nObservability: Centralized logging and metrics-based monitoring using cloud-native or open-source solutions\nCloud Object Storage: Scalable storage solutions such as GCS, S3, or Azure Blob Storage\nDistributed SQL Stores: Managed relational databases like Cloud SQL, Amazon RDS, or Azure SQL\nIdentity & Access Management: Role-based access control and policy enforcement via IAM across cloud environments\nYou re not expected to have 100% of these skills. At ANZ a growth mindset is at the heart of our culture, so if you have most of these things in your toolbox, we d love to hear from you.\nSo why join us?\nANZ is a place where big things happen as we work together to provide banking and financial services across more than 30 markets. With more than 7,500 people, our Bengaluru team is the banks largest technology, data and operations centre outside Australia. In operation for over 33 years, the centre is critical in delivering the banks strategy and making an impact for our millions of customers around the world. Our Bengaluru team not only drives the transformation initiatives of the bank, it also drives a culture that makes ANZ a great place to be. Were proud that people feel they can be themselves at ANZ and 90 percent of our people feel they belong.\nWe want to continue building a diverse workplace and welcome applications from everyone. Please talk to us about any adjustments you may require to our recruitment process or the role itself. If you are a candidate with a disability or access requirements, let us know how we can provide you with additional support.\nTo find out more about working at ANZ visit https://www.anz.com/careers/ . You can apply for this role by visiting ANZ Careers and searching for reference number 98658\n.\nJob Posting End Date\n18/06/2025 , 11.59pm, (Melbourne Australia)",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Access management', 'Workflow', 'Incident management', 'Open source', 'Distribution system', 'Monitoring', 'Financial services', 'SQL', 'Python']",2025-06-13 05:20:38
Big Data Developer/ Senior Big Data Developer,Grid Dynamics,5 - 10 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","About us\nGrid Dynamics (NASDAQ: GDYN) is a leading provider of technology consulting, platform and product engineering, AI, and advanced analytics services. Fusing technical vision with business acumen, we solve the most pressing technical challenges and enable positive business outcomes for enterprise companies undergoing business transformation. A key differentiator for Grid Dynamics is our 8 years of experience and leadership in enterprise AI, supported by profound expertise and ongoing investment in data, analytics, cloud & DevOps, application modernization and customer experience. Founded in 2006, Grid Dynamics is headquartered in Silicon Valley with offices across the Americas, Europe, and India.\n\nRole & responsibilities\nWe are looking for an enthusiastic and technology-proficient Big Data Engineer, who is eager to participate in the design and implementation of a top-notch Big Data solution to be deployed at massive scale.\nOur customer is one of the world's largest technology companies based in Silicon Valley with operations all over the world. On this project we are working on the bleeding-edge of Big Data technology to develop high performance data analytics platform, which handles petabytes datasets.\nEssential functions\nParticipate in design and development of Big Data analytical applications.\nDesign, support and continuously enhance the project code base, continuous integration pipeline, etc.\nWrite complex ETL processes and frameworks for analytics and data management.\nImplement large-scale near real-time streaming data processing pipelines.\nWork inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale.\nQualifications\nStrong coding experience with Scala, Spark,Hive, Hadoop.\nIn-depth knowledge of Hadoop and Spark, experience with data mining and stream processing technologies (Kafka, Spark Streaming, Akka Streams).\nUnderstanding of the best practices in data quality and quality engineering.\nExperience with version control systems, Git in particular.\nDesire and ability for quick learning of new tools and technologies.\nWould be a plus\nKnowledge of Unix-based operating systems (bash/ssh/ps/grep etc.).\nExperience with Github-based development processes.\nExperience with JVM build systems (SBT, Maven, Gradle).\nWe offer\nOpportunity to work on bleeding-edge projects\nWork with a highly motivated and dedicated team\nCompetitive salary\nFlexible schedule\nBenefits package - medical insurance, sports\nCorporate social events\nProfessional development opportunities\nWell-equipped office",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'SCALA', 'Hadoop', 'Spark']",2025-06-13 05:20:40
Big Data Developer-STG(P),Infosys,5 - 10 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities\n\nA day in the life of an Infoscion\nAs part of the Infosys delivery team, your primary role would be to ensure effective Design, Development, Validation and Support activities, to assure that our clients are satisfied with the high levels of service in the technology domain.\nYou will gather the requirements and specifications to understand the client requirements in a detailed manner and translate the same into system requirements.\nYou will play a key role in the overall estimation of work requirements to provide the right information on project estimations to Technology Leads and Project Managers.\nYou would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\nIf you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\n\nPrimary skills:Technology->Functional Programming->Scala\n\nAdditional information(Optional)\nKnowledge of design principles and fundamentals of architecture\nUnderstanding of performance engineering\nKnowledge of quality processes and estimation techniques\nBasic understanding of project domain\nAbility to translate functional / nonfunctional requirements to systems requirements\nAbility to design and code complex programs\nAbility to write test cases and scenarios based on the specifications\nGood understanding of SDLC and agile methodologies\nAwareness of latest technologies and trends\nLogical thinking and problem solving skills along with an ability to collaborate\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SCALA', 'Big Data', 'Spark', 'Hive', 'Kafka']",2025-06-13 05:20:42
Azure Data Factory (ADF) Engineer,Arieotech Solutions,3 - 5 years,Not Disclosed,['Pune'],"We are seeking an experienced Azure Data Factory Engineer to design, develop, and manage data pipelines using Azure Data Factory. The ideal candidate will possess hands-on expertise in ADF components and activities, and have practical knowledge of incremental data loading, file management, API integration, and cloud storage solutions. This role involves automating data workflows, optimizing performance, and ensuring the seamless flow of data within our cloud environment.\nKey Responsibilities:\nDesign and Develop Data Pipelines: Build and maintain scalable data pipelines using Azure Data Factory, ensuring efficient and reliable data movement and transformation.\nIncremental Data Loads: Implement and manage incremental data loading processes to ensure that only updated or new data is processed, optimizing data pipeline performance and reducing resource consumption.\nFile Management: Handle data ingestion and management from various file sources, including CSV, JSON, and Parquet formats, ensuring data accuracy and consistency.\nAPI Integration: Develop and configure data pipelines to interact with RESTful APIs for data extraction and integration, handling authentication and data retrieval processes effectively.\nCloud Storage Management: Work with Azure Blob Storage and Azure Data Lake Storage to manage and utilize cloud storage solutions, ensuring data is securely stored and easily accessible.\nADF Automation: Leverage Azure Data Factory s automation capabilities to schedule and monitor data workflows, ensuring timely execution and error-free operations.\nPerformance Optimization: Continuously monitor and optimize data pipeline performance, troubleshoot issues, and implement best practices to enhance efficiency.\nCollaboration: Work closely with data engineers, analysts, and other stakeholders to gather requirements, provide technical guidance, and ensure successful data integration solutions.\nQualifications:\nEducational Background: Bachelor s degree in Computer Science, Information Technology, or a related field (B. E, B.Tech, MCA, MCS). Advanced degrees or certifications are a plus.\nExperience: Minimum 3-5 years of hands-on experience with Azure Data Factory, including designing and implementing complex data pipelines.\nTechnical Skills:\nStrong knowledge of ADF components and activities, including datasets, pipelines, data flows, and triggers.\nProficiency in incremental data loading techniques and optimization strategies.\nExperience working with various file formats and handling large-scale data files.\nProven ability to integrate and interact with APIs for data retrieval and processing.\nHands-on experience with Azure Blob Storage and Azure Data Lake Storage.\nFamiliarity with ADF automation features and scheduling.\nSoft Skills:\nStrong problem-solving and analytical skills.\nExcellent communication and collaboration abilities.\nAbility to work independently and manage multiple tasks effectively.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Analytical skills', 'Automation', 'Storage management', 'cloud storage', 'JSON', 'Scheduling', 'Management', 'Information technology', 'Data extraction']",2025-06-13 05:20:43
"Associate Engineer, Digital Data Development",XL India Business Services Pvt. Ltd,1 - 4 years,Not Disclosed,['Gurugram'],"Engineer, Digital Data Development Gurgaon/ Bangalore, India AXA XL offers risk transfer and risk management solutions to clients globally\n\nWe offer worldwide capacity, flexible underwriting solutions, a wide variety of client-focused loss prevention services, and a team-based account management approach\n\nAXA XL recognizes data and information as critical business assets, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained dynamic advantage\n\nOur Innovation, Data, and Analytics (IDA) organization is focused on driving innovation by optimizing how we leverage data to drive strategy and create a new business model - disrupting the insurance market\n\nThis role is part of the Digital Data Dev Division within the Digital Transformation vertical of IDA\n\nIt will be responsible for different aspects of Data Product development lifecycle activities, including but not limited to Data Production Support, business stakeholders engagement for usage & problem resolutions, Product migrations, and platform/data product rollouts, performance stability & reliability\n\nWhat you ll be DOING What will your essential responsibilities include? Hands-on experience with CI/CD tools: Harness, Azure DevOps\n\nImplement and manage DevSecOps tools and CI/CD pipelines with security controls\n\nAutomate security scanning and compliance checks (SAST, DAST, container scanning, etc)\n\nCollaborate with development, operations, and security teams to embed security best practices\n\nConduct threat modeling, vulnerability assessments, and risk\n\nBuild, Release Management & DevSecOps support for various data solutions owned and managed by IDA organization\n\nExperience with cloud platforms like Azure is preferred\n\nProficiency in scripting languages: Python, Bash, PowerShell\n\nFamiliarity with containerization and orchestration: Docker, Kubernetes, OpenShift\n\nExperience using Tools like Git, JIRA, Confluence etc Knowledge of Artifactory like JFrog / X-Ray\n\nExperience of working with Agile methodologies\n\nGood knowledge of OOP concepts & Microservice-based architecture\n\nAnalyze and mitigate risks (technical or otherwise) about Data Solution build & release delivery timelines\n\nProvide top-class DevSecOps functionalities and support\n\nPartner with the Product & Production Support team(s) as a Data/DevSecOps/Technical SME for migration of re-architected Product/Product functionalities to the new Cloud Platform\n\nDemonstrate proactive communication with Business users, Development, Technology, Production Support, and Delivery Teams, and Senior Management\n\nProvide day-to-day management of the DevSecOps services and ensure smooth operation of the Release pipelines to various Environments\n\nWork in the Follow the Sun support model providing cross-team support coverage across Digital Data Dev division responsibilities\n\nBuild/Setup/Maintain various critical monitoring processes, alerts, and overall health reports (performance and functional) of production, and pre-production environments to be used by the Production Support Teams\n\nWork with Product Teams to build deployment pipelines for various Data Science Products used within IDA/Pricing & Analytics Teams\n\nOversee the development and maintenance of Build & Release Management processes and their documentation\n\nEnsure that all policies, standards, and best practices are followed and kept up to date\n\nTimely and accurate completion of emergency Release pipelines/processes in a manner that is auditable, testable, and maintainable\n\nEnsure any builds are consistent with Solution design, Security recommendations and business specifications\n\nAchieve & maintain the highest business customer confidence and net promoter score (NPS)\n\nGood grasp of Azure fundamentals (Microsoft AZ-900)\n\nRobust understanding of Designing and Implementing DevOps/DevSecOps Solutions (Microsoft AZ-400)\n\nKnowledge of Python or R Programming Language is a plus\n\nYou will report to Senior Delivery Lead\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Excellent understanding of DevOps principles with integrated security practices\n\nA minimum of an Undergraduate University Degree in Computer Science or related fields\n\nExtensive experience in data-focused roles (analytics, specialist, or engineer) and one or more areas of Build, Release & Data Management\n\nDistinctive problem-solving and analytical skills combined with robust business acumen\n\nExperience/knowledge of Microservices, Dot Net, R Programming Language, Python, Azure, and Kibana\n\nExperience with SQL, HIVE, ADLS, and Document Databases like Cosmos, SQL Databases & SQL DW Analytics\n\nExperience/Understanding of systems integration, and developer support tools Azure DevOps/DevSecOps, CI/CD pipelines, Release Management, Configuration Management, and Automation\n\nData Engineering background or working experience with ETL and big data platforms (HDInsight / ADLS / Data Bricks) a plus\n\nDesired Skills and Abilities: Demonstrates a level of experience/ability to influence and understand business problems in technical terminology and able to liaise with staff at all levels in the organization\n\nExcellent writing skills, with the ability to create clear requirements, specifications, and documentation for data systems\n\nExperience with multiple software delivery models (Waterfall, Agile, etc) is a plus\n\nPrevious experience leading small teams with a mix of onsite/offshore developers",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Production support', 'Configuration management', 'Agile', 'microsoft', 'Risk management', 'Release management', 'Analytics', 'SQL', 'Python']",2025-06-13 05:20:45
"Staff Engr - Data Science(Advanced OOPS,Python,PySpark,Databricks)",The TJX Companies Inc,5 - 10 years,Not Disclosed,['Hyderabad'],"TJX Companies\nAt TJX Companies, every day brings new opportunities for growth, exploration, and achievement. You ll be part of our vibrant team that embraces diversity, fosters collaboration, and prioritizes your development. Whether you re working in our four global Home Offices, Distribution Centers or Retail Stores TJ Maxx, Marshalls, Homegoods, Homesense, Sierra, Winners, and TK Maxx, you ll find abundant opportunities to learn, thrive, and make an impact. Come join our TJX family a Fortune 100 company and the world s leading off-price retailer.\nJob Description:\nAbout TJX:\nAt TJX, is a Fortune 100 company that operates off-price retailers of apparel and home fashions. TJX India - Hyderabad is the IT home office in the global technology organization of off-price apparel and home fashion retailer TJX, established to deliver innovative solutions that help transform operations globally. At TJX, we strive to build a workplace where our Associates contributions are welcomed and are embedded in our purpose to provide excellent value to our customers every day. At TJX India, we take a long-term view of your career. We have a high-performance culture that rewards Associates with career growth opportunities, preferred assignments, and upward career advancement. We take well-being very seriously and are committed to offering a great work-life balance for all our Associates.\nWhat you ll discover\nInclusive culture and career growth opportunities\nA truly Global IT Organization that collaborates across North America, Europe, Asia and Australia, click here to learn more\nChallenging, collaborative, and team-based environment\nWhat you ll do\nThe Global Supply Chain - Logistics Team is responsible for managing various supply chain logistics related solutions within TJX IT. The organization delivers capabilities that enrich the customer experience and provide business value. We seek a motivated, talented Staff E ngineer with good understanding of cloud base, database and BI concepts to help architect enterprise reporting solutions across global buying, planning and allocations .\nWhat you ll need\nThe Global Supply Chain - Logistics Team thrives on strong relationships with our business partners and working diligently to address their needs which supports TJX growth and operational stability. On this tightly knit and fast-paced solution delivery team you will be constantly challenged to stretch and think outside the box .\nYou will be working with product teams, architecture and business partners to strategically plan and deliver the product features by connecting the technical and business worlds. You will need to break down complex problems into steps that drive product development while keeping product quality and security as the priority. You will be responsible for most architecture, design and technical decisions within the assigned scope.\nKey Responsibilities:\nDesign, develop, test and deploy AI solutions using Azure AI services to meet business requirements , working collaboratively with architects and other engineers.\nTrain, fine-tune, and evaluate AI models, including large language models (LLMs), ensuring they meet performance criteria and integrate seamlessly into new or existing solutions.\nDevelop and integrate APIs to enable smooth interaction between AI models and other applications, facilitating efficient model serving.\nCollaborate effectively with cross-functional teams, including data scientists, software engineers, and business stakeholders, to deliver comprehensive AI solutions.\nOptimize AI and ML model performance through techniques such as hyperparameter tuning and model compression to enhance efficiency and effectiveness.\nMonitor and maintain AI systems, providing technical support and troubleshooting to ensure continuous operation and reliability.\nCreate comprehensive documentation for AI solutions, including design documents, user guides, and operational procedures, to support development and maintenance.\nStay updated with the latest advancements in AI, machine learning, and cloud technologies, demonstrating a commitment to continuous learning and improvement.\nDesign, code, deploy, and support software components, working collaboratively with AI architects and engineers to build impactful systems and services.\nLead medium complex initiatives, prioritizing and assigning tasks, providing guidance, and resolving issues to ensure successful project delivery.\nMinimum Qualifications\nBachelors degree in computer science, engineering, or related field\n8 + years of experience in data /software engineering, design, implementation and architecture.\nAt least 5+ years of hands-on experience in developing AI/ML solutions, with a focus on deploying them in a cloud environment .\nDeep understanding of AI and ML algorithms with focus on Operations Research / Optimization knowledge ( preferably M etaheuristics / Genetic Algorithms) .\nStrong programming skills in Python with advanced OOPS concepts.\nGood understanding of structured, semi structured, and unstructured data, Data modelling, Data analysis, ETL and ELT .\nProficiency with Databricks & PySpark .\nExperience with MLOps practices including CI/CD for machine learning models.\nKnowledge of security best practices for deploying AI solutions, including data encryption and access control.\nKnowledge of ethical considerations in AI, including bias detection and mitigation strategies.\nThis role operates in an Agile/Scrum environment and requires a solid understanding of the full software lifecycle, including functional requirement gathering, design and development, testing of software applications, and documenting requirements and technical specifications.\nFully Owns Epics with decreasing guidance. Takes initiative through identifying gaps and opportunities.\nStrong communication and influence skills. Solid team leadership with mentorship skills\nAbility to understand the work environment and competing priorities in conjunction with developing/meeting project goals .\nShows a positive, open-minded, and can-do attitude .\nExperience in the following technologies:\nAdvanced Python programming ( OOPS)\nOperations Research / Optimization knowledge ( preferably M etaheuristics / Genetic Algorithms)\nDatabricks with Pyspark\nAzure / Cloud knowledge\nGithub / version control\nFunctional knowledge on Supply Chain / Logistics is preferred.\nIn addition to our open door policy and supportive work environment, we also strive to provide a competitive salary and benefits package. TJX considers all applicants for employment without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, gender identity and expression, marital or military status, or based on any individuals status in any group or class protected by applicable federal, state, or local law. TJX also provides reasonable accommodations to qualified individuals with disabilities in accordance with the Americans with Disabilities Act and applicable state and local law.\nAddress:\nSalarpuria Sattva Knowledge City, Inorbit Road\nLocation:\nAPAC Home Office Hyderabad IN",Industry Type: Retail,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Supply chain', 'Data analysis', 'Operations research', 'Architecture', 'OOPS', 'Cloud', 'Machine learning', 'Technical support', 'Python', 'Logistics']",2025-06-13 05:20:47
AI Python Data Science Engineer,Probeseven,4 - 5 years,Not Disclosed,['Coimbatore'],"AI Python Data Science Engineer\nHot Openings\nAs a data science and analytics engineer, you will be involved in developing computer visions and data algorithms. Artificial intelligence development and deep machine learning implementations will be part of your development and deployment to the cloud.\n\nExperience for senior positions: 4 - 5+ years\nExperience for junior positions: 2 - 3 years\n\nRequired Tech Skills\nExperience in Python (must have) and/or R language.\nExperience with computer vision algorithms and data science.\nExperience in deep machine learning models.\nWell-versed in data visualization techniques.\nTroubleshoot and resolve code issues.\nCollaborate with data engineers to design and integrate the data sources.\nExperience in handling multiple priorities with Agile development.\nExperience with Git and working in a collaborative and distributive team environment.\nRequired Soft Skills\nExcellent listening, verbal, and written communication skills.\nStrong interpersonal & customer relationship skills.\nStrong analytical, problem solving, and decision-making skills.\nDocumentation skills.\nApply now\nHot Openings\nPHP + Node.js Developers\nFull Time\nTech Development\nExperience 4 - 6+ years",Industry Type: Film / Music / Entertainment,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer vision', 'GIT', 'data science', 'Analytical', 'Artificial Intelligence', 'Machine learning', 'PHP', 'Customer relationship', 'Analytics', 'Python']",2025-06-13 05:20:48
Data Visualization Engineer,Maimsd Technology,4 - 7 years,Not Disclosed,['Pune'],"Experience : 4 - 7 Yrs\n\nEmployment Type : Full Time, Permanent\n\nWorking mode : Regular\n\nNotice Period : Immediate - 15 Day\n\nAbout the Role :\n\nWe are seeking a skilled Data Visualization Engineer to join our team and transform raw data into actionable insights. You will play a crucial role in designing, developing, and maintaining interactive dashboards and reports using Power BI, Power Apps, Power Query, and Power Automate.\n\nResponsibilities :\n\n- Data Visualization : Create compelling and informative dashboards and reports using Power BI, effectively communicating complex data to stakeholders.\n\n- Data Integration : Import data from various sources (e.g., databases, spreadsheets, APIs) using Power Query and ensure data quality and consistency.\n\n- Data Modeling : Develop robust data models in Power BI to support complex analysis and reporting requirements.\n\n- Power Apps Development : Create custom applications using Power Apps to enable data-driven decision-making and automate workflows.\n\n- Power Automate Integration : Automate repetitive tasks and workflows using Power Automate to improve efficiency and reduce errors.\n\n- Cloud Data Analytics : Leverage cloud-based data analytics platforms to process and analyze large datasets.\n\n- Collaboration : Work closely with data analysts, data scientists, and business users to understand their requirements and deliver effective visualizations.\n\nQualifications :\n\nExperience : 4-7 years of experience in data visualization and business intelligence.\n\nTechnical Skills :\n\n- Proficiency in Power BI, Power Apps, Power Query, and Power Automate.\n\n- Strong understanding of data modeling and ETL processes.\n\n- Experience working with cloud-based data analytics platforms.\n\n- Familiarity with SQL and other data query languages.\n\nSoft Skills :\n\n- Excellent communication and interpersonal skills.\n\n- Strong problem-solving and analytical abilities.\n\n- Attention to detail and ability to deliver high-quality work",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Visualization', 'Reporting Analytics', 'Power BI', 'Dashboard Design', 'Power Automate', 'Data Modeling', 'ETL', 'Power Query M', 'Data Integration', 'SQL']",2025-06-13 05:20:50
Big Data Developer,Techstar Group,7 - 10 years,Not Disclosed,['Hyderabad'],"Responsibilities of the Candidate :\n\n- Be responsible for the design and development of big data solutions. Partner with domain experts, product managers, analysts, and data scientists to develop Big Data pipelines in Hadoop\n\n- Be responsible for moving all legacy workloads to a cloud platform\n\n- Work with data scientists to build Client pipelines using heterogeneous sources and provide engineering services for data PySpark science applications\n\n- Ensure automation through CI/CD across platforms both in cloud and on-premises\n\n- Define needs around maintainability, testability, performance, security, quality, and usability for the data platform\n\n- Drive implementation, consistent patterns, reusable components, and coding standards for data engineering processes\n\n- Convert SAS-based pipelines into languages like PySpark, and Scala to execute on Hadoop and non-Hadoop ecosystems\n\n- Tune Big data applications on Hadoop and non-Hadoop platforms for optimal performance\n\n- Apply an in-depth understanding of how data analytics collectively integrate within the sub-function as well as coordinate and contribute to the objectives of the entire function.\n\n- Produce a detailed analysis of issues where the best course of action is not evident from the information available, but actions must be recommended/taken.\n\n- Assess risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding Citigroup, its clients, and assets, by driving compliance with applicable laws, rules, and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct, and business practices, and escalating, managing and reporting control issues with transparency\n\nRequirements :\n\n- 6+ years of total IT experience\n\n- 3+ years of experience with Hadoop (Cloudera)/big data technologies\n\n- Knowledge of the Hadoop ecosystem and Big Data technologies Hands-on experience with the Hadoop eco-system (HDFS, MapReduce, Hive, Pig, Impala, Spark, Kafka, Kudu, Solr)\n\n- Experience in designing and developing Data Pipelines for Data Ingestion or Transformation using Java Scala or Python.\n\n- Experience with Spark programming (Pyspark, Scala, or Java)\n\n- Hands-on experience with Python/Pyspark/Scala and basic libraries for machine learning is required.\n\n- Proficient in programming in Java or Python with prior Apache Beam/Spark experience a plus.\n\n- Hand on experience in CI/CD, Scheduling and Scripting\n\n- Ensure automation through CI/CD across platforms both in cloud and on-premises\n\n- System level understanding - Data structures, algorithms, distributed storage & compute\n\n- Can-do attitude on solving complex business problems, good interpersonal and teamwork skills",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Hive', 'Data Engineering', 'Data Pipeline', 'PySpark', 'Hadoop', 'Kafka', 'HDFS', 'Spark', 'Python']",2025-06-13 05:20:52
Data Analytics Engineer,Automotive Industry,5 - 6 years,Not Disclosed,['Chennai'],"Position: Data Analytics Engineer\nExp: 5 -6 years\nNP: Immediate - 30 days\nQualification: B.tech\nLocation: Chennai Hybrid\nPrimary: Google Cloud Platform ,Python skills and Big Data pipeline\nSecondary: Big Query SQ, coding, testing, implementing, debugging workflows and apps\nKindly share your updated resume to aishwarya_s@onwardgroup.com\nKindly fill the below details\nTotal Exp:\nRelevant Exp:\nNotice Period: CTC:\nECTC:\nIf servicing NP, Last working Day, offered location & CTC:\nAvailable for Video modes interview on Weekdays (Y/N) :\nPAN Number:\nName as Per PAN Card:\nDate of Birth:\nAlternative Contact No:\nReason for Job Change:",Industry Type: Automobile,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['GCP', 'Bigquery', 'Google Cloud Platform', 'Query SQ', 'Python']",2025-06-13 05:20:54
Software Data Operations Engineer (BS+2),MAQ Software,2 - 5 years,Not Disclosed,['Noida'],"MAQ LLC d.b.a MAQ Software hasmultiple openings at Redmond, WA for:\nSoftware Data Operations Engineer (BS+2)\n\nResponsible for gathering & analyzing business requirements from customers. Implement,test and integrate software applications for use by customers. Develop &review cost effective data architecture to ensure appropriateness with currentindustry advances in data management, cloud & user experience. Automateuser test scenarios, debug & fix errors in cloud-based data infrastructure,reporting applications to meet customer needs. Must be able to traveltemporarily to client sites and or relocate throughout the United States.\n\nRequirements:Bachelors Degree or foreign equivalent in Computer Science, ComputerApplications, Computer Information Systems, Information Technology or relatedfield with two years of work experience in job offered, software engineer, systemsanalyst or related job.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Software Data Operations', 'software applications', 'data management', 'Data Operations', 'data architecture', 'data infrastructure']",2025-06-13 05:20:55
JavaScript Engineer For Training AI Data,G2i Inc,3 - 8 years,Not Disclosed,[],"Evaluating the quality of AI-generated code, including human-readable summaries of your rationale.\nBuilding and evaluating React components, hooks, and modern JavaScript solutions.\nSolving coding problems and writing functional and efficient JavaScript/React code.\nWriting robust test cases to confirm code works efficiently and effectively.\nCreating instructions to help others and reviewing code before it goes into the model.\nEngaging in a variety of projects, from evaluating code snippets to developing full mobile applications using chatbots.\nPay Rates\nCompensation rates average at $30/hr and can go up to $50+/hr. Expectations are 15+ hours per week; however, there is no upper limit. You can work as much as you want and will be paid weekly per hour of work done on the platform.\nContract Length\nThis is a long-term contract with no end date. We expect to have work for the next 2 years. You can end the contract at any time, but we hope you will commit to 12 months of work.\nFlexible Schedules\nDevelopers can set their own hours. Ideal candidates will be interested in spending 40 hours a week. You will be assigned to projects, so strong performers will adapt to the urgency of projects and stay engaged, but we are incredibly flexible on working hours. You can take a 3-hour lunch with no problem. Instead of tracking your hours, you are paid according to time spent on the platform, calculated in the coding exercises.\nInterview Process\nApply using this Ashby form.\nIf you seem like a good fit, well send an async RLHF code review that will take 35 minutes and must be finished within 72 hours of us sending it.\nYou ll receive credentials to the RLHF platform. We re doing regular calls to answer any further questions about onboarding, as well as providing a support team at your disposal.\nYou ll perform a simulated production-level task (RLHF task) on the platform. This will be the final stage, which will ultimately determine your leveling and which project you ll be assigned. Successful completion of this process provides you with an opportunity to work on projects as they become available.\nTech Stack Priorities\nThe current priority for this team is frontend engineers who are well versed in JavaScript, React, and modern web development frameworks and libraries.\nRequired Qualifications\n3+ years of experience in a software engineering/software development role.\nStrong proficiency with JavaScript/React and frontend development.\nComplete fluency in the English language.\nAbility to articulate complex technical concepts clearly and engagingly.\nExcellent attention to detail and ability to maintain consistency in writing. Solid understanding of grammar, punctuation, and style guidelines.\nNice To Haves:\nBachelors or Masters degree in Computer Science.\nExperience with modern JavaScript frameworks and libraries (Next.js, Vue, Angular).\nFamiliarity with frontend testing frameworks (Jest, React Testing Library, Cypress).\nKnowledge of state management solutions (Redux, Context API, MobX).\nExperience with TypeScript and modern frontend tooling.\nRecognized accomplishments or contributions to the coding community or in projects.\nProven analytical skills with an ability to approach problems creatively.\nAdept communication skills, especially when understanding and discussing project requirements.\nA commitment to continuous learning and staying updated with the latest coding advancements and best practices.\nEnthusiasm for teaching AI models and experience with technical writing!",Industry Type: Software Product,Department: Other,"Employment Type: Full Time, Permanent","['Computer science', 'Analytical skills', 'Front end', 'Technical writing', 'Coding', 'Web development', 'Javascript', 'Test cases', 'Mobile applications', 'Software engineering']",2025-06-13 05:20:57
Data Warehouse Engineer (SSIS),Tek Experts,9 - 10 years,Not Disclosed,['New Delhi'],"We re searching for a Data Warehouse Engineer with a proven track record of developing Business Intelligence solutions to drive key outcomes. The role is responsible for different layers of the data hierarchy - including database design, data collection and storage techniques, to a deep understanding of data transformation tools and methodologies, the provisioning and managing of analytical databases, and building infrastructures that bring machine learning capabilities into production. The role will operate within the Business Analytics unit under the Business Analytics Global Manager.\nAt TeKnowledge , your work makes an impact from day one. We partner with organizations to deliver AI-First Expert Technology Services that\ndrive meaningful impact in AI, Customer Experience, and Cybersecurity. We turn complexity into clarity and potential into progress in a place where people lead and tech empowers.\nYou ll be part of a diverse and inclusive team where trust, teamwork, and shared success fuel everything we do. We push boundaries, using advanced technologies to solve complex challenges for clients around the world.\nHere, your work drives real change, and your ideas help shape the future of technology. We invest in you with top-tier training, mentorship, and career development ensuring you stay ahead in an ever-evolving world.\nWhy You ll Enjoy It Here:\nBe Part of Something Big A growing company where your contributions matter.\nMake an Immediate Impact Support groundbreaking technologies with real-world results.\nWork on Cutting-Edge Tech AI, cybersecurity, and next-gen digital solutions.\nThrive in an Inclusive Team A culture built on trust, collaboration, and respect.\nWe Care Integrity, empathy, and purpose guide every decision.\nWe re looking for innovators, problem-solvers, and experts ready to drive change and grow with us.\nWe Are TeKnowledge. Where People Lead and Tech Empowers.\n\nResponsibilities\n\nEnd to end development of data models, including gathering the data sources, developing ETL processes, and developing front end data model solutions for our business to help our users make smarter decisions.\nFull responsibility on our DWH including infrastructure, data modeling, audit logging, etc.\nBuilding automated validation processes to ensure data integrity.\nOptimizing processes run-time and solving problems in a scalable manner.\nTaking full ownership of designing, building and deployment of data products.\nCollaborating with Backend, Data Science, Data Analysis, and Product teams.\n\nQualifications\n\nBachelor s degree in Information Systems, Industrial Engineering, or equivalent experience is required.\nProfessional fluency in English is essential, both written and spoken.\nThree or more years of experience in BI solutions development (DWH, ETL) as well as in SQL and ETL tools like SSIS is required.\nExperience with building automated validation processes to ensure data integrity is considered an advantage.\nExperience in data modeling, working ETL tools and methodology, as well as with BI reporting tools is required.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'Backend', 'Front end', 'Database design', 'Business analytics', 'Machine learning', 'HTML', 'SSIS', 'Business intelligence', 'SQL']",2025-06-13 05:20:58
Specialist Data Security Engineer,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role the Specialist Data Security Engineer covering Data Loss Prevention (DLP) and Cloud Access Security Broker (CASB) technologies. This role will report to the Manager, Data Security. This position will provide essential services that enable us to better pursue our mission.\nSpecialist Data Security Engineers operate, manage, and improve Amgens DLP and Cloud Access Security Broker (CASB) solutions. In our Data Security team, they will identify emerging risks related to changes in cloud technologies, advise management, and develop technical remediations to address those risks. Specialists lead the development of processes and procedures for multiple solutions which enable business units to remediate identify cloud data exposures. They run multiple projects simultaneously to implement and improve the cloud data security protection and use advanced analytics to demonstrate success.",,,,"['Data Security Engineering', 'SkyHigh', 'Data Protection', 'Cloud Access Security Platforms', 'Netskope', 'security frameworks', 'Agile methodology', 'IT operations', 'ITIL', 'Elastica']",2025-06-13 05:21:01
"Quant Data Specialist, Aladdin Financial Engineering - Associate",Primetrace Technologies,3 - 6 years,Not Disclosed,['Gurugram'],"About this role\nAbout Aladdin Financial Engineering (AFE):\nJoin a diverse and collaborative team of over 3 00 modelers and technologists in Aladdin Financial Engineering (AFE) within BlackRock Solutions, the business responsible for the research and development of Aladdin s financial models. This group is also accountable for analytics production, enhancing the infrastructure platform and delivering analytics content to portfolio and risk management professionals (both within BlackRock and across the Aladdin client community). The models developed and supported by AFE span a wide array of financial products covering equities, fixed income, commodities, derivatives, and private markets. AFE provides investment insights that range from an analysis of cash flows on a single bond, to the overall financial risk associated with an entire portfolio, balance sheet, or enterprise.\nRole Description:\nWe are looking for a person to join the Advanced Data Analytics team with AFE Single Security . Advanced Data Analytics is a team of Quantitative Data and Product Specialists, focused on delivering Single Security Data Content, Governance and Product Solutions and Research Platform. The team leverages data, cloud, and emerging technologies in building an innovative data platform, with the focus on business and research use cases in the S ingle S ecurity space. The team uses various statistical/mathematical methodologies to derive insights and generate content to help develop predictive models, clustering, and classification solutions and enable Governance . The team works on Mortgage, Structured & Credit Products.\nWe are looking for a person to help build and expand Data & Analytics Content in the Credit space . The person will be responsible for building, enhancing, and maintaining the Credit Content Suite . The person will work on the below -\nCredit Derived Data Content\nModel & Data Governance\nCredit Model & Analytics\nExperience\nExperience on Scala\nKnowledge of ETL, data curation and analytical jobs using distributed computing framework with Spark\nKnowledge and Experience of working with large enterprise databases like Snowflake, Cassandra & Cloud manged services like Dataproc , Databricks\nKnowledge of financial instruments like Corporate Bonds, Derivatives etc.\nKnowledge of regression methodologies\nAptitude for design and building tools for D ata Governance\nPython knowledge is a plus\nQualifications\nBachelors / masters in computer science with a major in Math, Econ, or related field\n3 - 6 years of relevant experience\nOur benefits\n\n.\nOur hybrid work model\n.\nAbout BlackRock\n.\nThis mission would not be possible without our smartest investment - the one we make in our employees. It s why we re dedicated to creating an environment where our colleagues feel welcomed, valued and supported with networks, benefits and development opportunities to help them thrive.\nFor additional information on BlackRock, please visit @blackrock | Twitter: @blackrock | LinkedIn: www.linkedin.com / company / blackrock\nBlackRock is proud to be an Equal Opportunity Employer. We evaluate qualified applicants without regard to age, disability, family status, gender identity, race, religion, sex, sexual orientation and other protected attributes at law.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Analytical', 'Fixed income', 'Financial risk', 'Finance', 'Healthcare', 'Data analytics', 'Risk management', 'Analytics', 'Balance Sheet', 'Financial engineering']",2025-06-13 05:21:02
Data Center Critical Facilities Engineer (Electrical),Equinix,3 - 4 years,Not Disclosed,['Mumbai'],"Who are we?\nEquinix is the world s digital infrastructure company , operating over 260 data centers across the globe. Digital leaders harness Equinixs trusted platform to bring together and interconnect foundational infrastructure at software speed. Equinix enables organizations to access all the right places, partners and possibilities to scale with agility, speed the launch of digital services, deliver world-class experiences and multiply their value, while supporting their sustainability goals.\nJoining our operations team means that you will be at the forefront of all we do, maintaining critical facilities infrastructure as part of a close-knit team delivering best-in-class service to our data center customers. We embrace diversity in thought and contribution and are committed to providing an equitable work environment that is foundational to our core values as a company and is vital to our success.",,,,"['Capacity management', 'Access control', 'Protection system', 'Compliance', 'Infrastructure', 'Corrective maintenance', 'Site administration', 'Vendor', 'Fire protection', 'Electricals']",2025-06-13 05:21:04
"Delivery Head - Infrastructure Engineering, Data Center",Bajaj Allianz General Insurance Company Limited,15 - 20 years,Not Disclosed,['Pune'],"The role requires strong leadership, strategic thinking, and the ability to drive innovation and efficiency within the technology department. It demands extensive experience in leading complex data center infrastructures, focusing on servers, SAN storage, high availability, disaster recovery, and hybrid environments, including data center operations and physical servers (blade and rack). Responsibilities include designing and testing backup strategies, maintaining documentation, ensuring compliance with regulations, and conducting product and vendor evaluations. Collaboration with various IT teams, the security team, and business stakeholders is essential\n",,,,"['process setting', 'document management', 'vmware', 'center', 'microsoft azure', 'itil service management', 'storage', 'shuttering', 'analysis', 'problem management', 'change management', 'cloud', 'data center', 'operations', 'service delivery', 'incident management', 'leadership', 'it infrastructure management', 'itil']",2025-06-13 05:21:06
Engineer- Data Support,Powerica,3 - 5 years,Not Disclosed,['Mumbai'],Job profile\na. Co-ordinate with project team\nb. Upload BVQ in oracle system\nc. BOQ revision and making entries.\nd. Data Accuracy\ne. Process management.\n\nDiploma in Engineering\nExp 3-5 Years’ experience.,Industry Type: Power,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Boq Preparation', 'Oracle', 'BVQ']",2025-06-13 05:21:08
Data Loss Prevention Engineer,Credent Infotech Solutions Llp,2 - 3 years,Not Disclosed,['Mumbai( kandivali )'],"Daily Monitoring and Investigation\n\nMonitor DLP alerts across email, endpoint, web, and cloud.\nPerform triage to determine false positives, true positives, and actual incidents.\nDocument findings and escalate critical violations per SOPs.\nIncident Response Support\n\nSupport incident response by providing evidence, logs, and context around DLP policy violations.\nCoordinate with IT, HR, and Legal teams for user engagement, awareness, and disciplinary action if necessary.\nParticipate in Root Cause Analysis (RCA) for recurring or high-severity incidents.\nPolicy Tuning and Optimization\n\nAnalyse alert trends and false positive patterns to suggest and implement policy refinements.\nWork with business and security teams to validate policy changes and test updated rulesets before production deployment.\nMaintain documentation of policy changes, rationales, and approvals.\nLifecycle Management\n\nSupport onboarding business units, or geographies into DLP coverage.\nMaintain and update DLP dashboards and reporting structures.\nStakeholder Communication\n\nProvide regular reports to CISO on DLP violations\nInterface with Data Owners, Business Units, and Compliance teams for policy alignment and exception management.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Endpoint Protection', 'Incident Response', 'Symantec Dlp', 'SIEM', 'Data Classification', 'Risk Assessment', 'Compliance', 'Powershell', 'Information Security', 'Dlp', 'Casb Netskope', 'Data Protection Manager', 'Microsoft Purview']",2025-06-13 05:21:09
S&C Global Network - AI - Life Sciences -Data Science Sr. Manager,Accenture,11 - 15 years,Not Disclosed,['Bengaluru'],"JR:\n\n\n\nR00229254\n\n\n\nExperience:\n\n\n\n11-15 Years\n\n\n\n\nEducational Qualification:\n\n\n\nBachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.\n\n\n\n---------------------------------------------------------------------\n\n\n\nJob Title -\n\n\n\nS&C Global Network - AI - Healthcare Analytics - Senior Manager\n\n\n\nManagement Level:\n\n\n\n6-Senior Manager\n\n\n\nLocation:\n\n\n\nBangalore/Gurgaon\n\n\n\nMust-have skills:R,Phython,SQL,Spark,Tableau ,Power BI\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\nAs part of our Data & AI practice, you will join a worldwide network of smart and driven colleagues experienced in leading AI/ML/Statistical tools, methods and applications. From data to analytics and insights to actions, our forward-thinking consultants provide analytically-informed, issue-based insights at scale to help our clients improve outcomes and achieve high performance.\n\n\n\nWHATS IN IT FOR YOU\nAn opportunity to work on high-visibility projects with top Pharma clients around the globe.\nPotential to Co-create with leaders in strategy, industry experts, enterprise function practitioners, and business intelligence professionals to shape and recommend innovative solutions that leverage emerging technologies.\nAbility to embed responsible business into everythingfrom how you service your clients to how you operate as a responsible professional.\nPersonalized training modules to develop your strategy & consulting acumen to grow your skills, industry knowledge, and capabilities.\nOpportunity to thrive in a culture that is committed to accelerating equality for all. Engage in boundaryless collaboration across the entire organization.\n\n\n\n\nWhat you would do in this role\nLead proposals, and business development efforts and coordinate with other colleagues to cross-sell/ up-sell Life Sciences offerings to existing as well as potential clients.\nLead client discussions, developing new industry Point of View (PoV), re-usable assets (tools)\nCollaborate closely with cross-functional teams including Data engineering, technology, and business stakeholders to identify opportunities for leveraging data to drive business solutions.\nLead and manage teams to deliver transformative and innovative client projects.\nGuide teams on analytical and AI methods and approaches\nManage client relationships to foster trust, deliver value, and build the Accenture brand\nDrive consulting practice innovation and thought leadership in your area of specialization\nSupport strategies and operating models focused on some business units and assess likely competitive responses. Also, assess implementation readiness and points of greatest impact.\nExecute a transformational change plan aligned with the clients business strategy and context for change. Engage stakeholders in the change journey and build commitment to change.\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\nProven experience in cross-sell/ up-sell\nLeverage ones hands-on experience of working across one or more of these areas such as real-world evidence data, R&D clinical data, and digital marketing data.\nExperience with handling Datasets like Komodo, RAVE, IQVIA, Truven, Optum, SHS, Specialty Pharmacy, PSP, etc.\nExperience in building and deployment of Statistical Models/Machine Learning including Segmentation & predictive modeling, hypothesis testing, multivariate statistical analysis, time series techniques, and optimization.\nExcellent analytical and problem-solving skills, with a data-driven mindset.\nAbility to solve complex business problems and deliver client delight.\nStrong writing skills to build points of view on current industry trends.\nGood Client handling skills; able to demonstrate thought leadership & problem-solving skills.\n\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | AccentureQualification\n\n\n\nExperience:\n\n\n\n11-15 Years\n\n\n\n\nEducational Qualification:\n\n\n\nBachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'power bi', 'sql', 'tableau', 'r', 'hypothesis testing', 'time series', 'business analytics', 'machine learning', 'data engineering', 'business intelligence', 'artificial intelligence', 'data science', 'computer science', 'spark', 'predictive modeling', 'segmentation', 'statistics', 'ml']",2025-06-13 05:21:11
Senior Data Scientist,Ericsson,3 - 8 years,Not Disclosed,['Bengaluru'],"About this Opportunity\nThe complexity of running and optimizing the next generation of wireless networks, such as 5G with distributed edge compute, will require Machine Learning (ML) and Artificial Intelligence (AI) technologies. Ericsson is setting up an AI Accelerator Hub in India to fast-track our strategy execution, using Machine Intelligence (MI) to drive thought leadership, automate, and transform Ericsson s offerings and operations. We collaborate with academia and industry to develop state-of-the-art solutions that simplify and automate processes, creating new value through data insights.\n\nAs a Senior Data Scientist, you will apply your knowledge of data science and ML tools backed with strong programming skills to solve real-world problems.\nResponsibilities:\n1. Lead AI/ML features/capabilities in product/business areas\n2. Define business metrics of success for AI/ML projects and translate them into model metrics\n3. Lead end-to-end development and deployment of Generative AI solutions for enterprise use cases\n4. Design and implement architectures for vector search, embedding models, and RAG systems\n5. Fine-tune and evaluate large language models (LLMs) for domain-specific tasks\n6. Collaborate with stakeholders to translate vague problems into concrete Generative AI use cases\n7. Develop and deploy generative AI solutions using AWS services such as SageMaker, Bedrock, and other AWS AI tools. Provide technical expertise and guidance on implementing GenAI models and best practices within the AWS ecosystem.\n8. Develop secure, scalable, and production-grade AI pipelines\n9. Ensure ethical and responsible AI practices\n10. Mentor junior team members in GenAI frameworks and best practices\n11. Stay current with research and industry trends in Generative AI and apply cutting-edge techniques\n12. Contribute to internal AI governance, tooling frameworks, and reusable components\n13. Work with large datasets including petabytes of 4G/5G networks and IoT data\n14. Propose/select/test predictive models and other ML systems\n15. Define visualization and dashboarding requirements with business stakeholders\n16. Build proof-of-concepts for business opportunities using AI/ML\n17. Lead functional and technical analysis to define AI/ML-driven business opportunities\n18. Work with multiple data sources and apply the right feature engineering to AI models\n19. Lead studies and creative usage of new/existing data sources",,,,"['Wireless', 'Computer science', 'Data analysis', 'cassandra', 'Neural networks', 'Artificial Intelligence', 'Machine learning', 'Telecommunication', 'data visualization', 'Python']",2025-06-13 05:21:13
"SENIOR, DATA SCIENTIST",Walmart,3 - 8 years,Not Disclosed,['Bengaluru'],"Position Summary...\nWhat youll do...\nAbout Team\nThe Catalog Data Science Team at Walmart Global Tech is focused on using the latest research in generative AI (GenAI), artificial intelligence (AI), machine learning (ML), statistics, deep learning, computer vision and optimization to implement solutions that ensure Walmart s product catalog is accurate, complete, and optimized for customer experience. Our team tackles complex data science and ML engineering challenges related to product classification, attribute extraction, trust & safety, and catalog optimization, empowering next-generation retail use cases.\nThe Data Science and ML Engineering community at Walmart Global Tech is active in most of the Hack events, utilizing the petabytes of data at our disposal, to build some of the coolest ideas. All the work we do at Walmart Global Tech will eventually benefit our operations & our associates, helping Customers Save Money to Live Better.\nWhat youll do:\nAs a Senior Data Scientist - ML Engineer, you ll have the opportunity to:\nDrive research initiatives and proof-of-concepts that push the state of the art in generative AI and large-scale machine learning.\nDesign and implement high-throughput, low-latency AI/ML pipelines and microservices that operate at global scale.\nOversee data ingestion, model training, evaluation, deployment and monitoring-ensuring performance, quality and reliability.\nCustomize and optimize LLMs for specific business use cases, balancing accuracy, latency and cost.\nPrototype novel generative AI solutions, integrate advancements into production, and collaborate with research partners.\nChampion best practices in data quality, lineage, governance and cost optimization across ML pipelines.\nMentor a team of ML engineers, establish coding standards, conduct design reviews, and foster a culture of continuous improvement.\nPresent your team s work at top-tier AI/ML conferences, publish scientific papers, and cultivate partnerships with universities and research labs.\nWhat youll bring\nPhD in Computer Science, Statistics, Applied Mathematics or related field with 3+ years experience in ML engineering-or Master s with 6+ years or Bachelor s with 8+ years.\nProven track record of leading and scaling AI/ML products in production environments.\nDeep expertise in generative AI, large-scale model deployment, and fine-tuning of transformer-based architectures.\nStrong programming skills in Python, or equivalent, and experience with big data frameworks (Spark, Hadoop) and ML platforms (TensorFlow, PyTorch).\nDemonstrated history of scientific publications or patents in AI/ML.\nExcellent communication skills, a growth mindset, and the ability to drive cross-functional collaboration.\nAbout Walmart Global Tech\n.\n.\nFlexible, hybrid work\n.\nBenefits\n.\nBelonging\n.\n.\nEqual Opportunity Employer\nWalmart, Inc., is an Equal Opportunities Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions - while being inclusive of all people.\nMinimum Qualifications...\nMinimum Qualifications:Option 1- Bachelors degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 3 years experience in an analytics related field. Option 2- Masters degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 1 years experience in an analytics related field. Option 3 - 5 years experience in an analytics or related field.\nPreferred Qualifications...\nPrimary Location...\n\n\n",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Prototype', 'Networking', 'Coding', 'Machine learning', 'Continuous improvement', 'Information technology', 'Monitoring', 'Analytics', 'Python']",2025-06-13 05:21:15
Data Governance Engineers,Meritus Management Service,4 - 9 years,14-17 Lacs P.A.,"['Pune', 'Gurugram']","Define, implement, & enforce data governance policies & standards to ensure data quality, consistency, & compliance across the organization\nCollaborate with data stewards, business users, & IT teams to maintain metadata, lineage, & data catalog tools",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Stewardship', 'Metadata', 'Data Governance', 'Metadata Management', 'Data Lineage', 'Data Modeling', 'SQL']",2025-06-13 05:21:17
Data Modeler,Synechron,0 - 2 years,Not Disclosed,['Bengaluru'],"Synechron is seeking a knowledgeable and proactive Data Modeler to guide the design and development of data structures that support our clients' business objectives. In this role, you will collaborate with cross-functional teams to translate business requirements into scalable and efficient data models, ensuring data accuracy, consistency, and integrity. You will contribute to creating sustainable and compliant data architectures that leverage emerging technologies such as cloud, IoT, mobile, and blockchain. Your work will be instrumental in enabling data-driven decision-making and operational excellence across projects.Software Required\n\nSkills:\nStrong understanding of data modeling concepts, methodologies, and tools Experience with data modeling for diverse technology platforms including cloud, mobile, IoT, and blockchain Familiarity with database management systems (e.g., relational, NoSQL) Knowledge of SDLC and Agile development practices Proficiency in modeling tools such as ERwin, PowerDesigner, or similar Preferred Skills:\nExperience with data integration tools and ETL processes Knowledge of data governance and compliance standards Familiarity with cloud platforms (AWS, Azure, GCP) and how they impact data architectureOverall Responsibilities Collaborate with business analysts, data engineers, and stakeholders to understand data requirements and translate them into robust data models Design logical and physical data models optimized for performance, scalability, and maintainability Develop and maintain documentation for data structures, including data dictionaries and metadata Conduct reviews of data models and code to ensure adherence to quality standards and best practices Assist in designing data security and privacy measures in alignment with organizational policies Stay informed about emerging data modeling trends and incorporate best practices into project delivery Support data migration, integration, and transformation activities as needed Provide technical guidance and mentorship related to data modeling standardsTechnical Skills (By Category) Data Modeling & Data Management: EssentialLogical/physical data modeling, ER diagrams, data dictionaries PreferredDimensional modeling, data warehousing, master data management Programming Languages: PreferredSQL (expertise in writing complex queries) OptionalPython, R for data analysis and scripting Databases & Data Storage Technologies: EssentialRelational databases (e.g., Oracle, SQL Server, MySQL) PreferredNoSQL (e.g., MongoDB, Cassandra), cloud-native data stores Cloud Technologies: PreferredBasic understanding of cloud data solutions (AWS, Azure, GCP) Frameworks & Libraries: Not typically required, but familiarity with data integration frameworks is advantageous Development Tools & Methodologies: EssentialData modeling tools (ERwin, PowerDesigner), version control (Git), Agile/Scrum workflows Security & Compliance: Knowledge of data security best practices, regulatory standards like GDPR, HIPAAExperience Minimum of 8+ years of direct experience in data modeling, data architecture, or related roles Proven experience designing data models for complex systems across multiple platforms (cloud, mobile, IoT, blockchain) Experience working in Agile environments using tools like JIRA, Confluence, Git Preference for candidates with experience supporting data governance and data quality initiativesNoteEquivalent demonstrated experience in relevant projects or certifications can qualify candidates.Day-to-Day Activities Participate in daily stand-ups and project planning sessions Collaborate with cross-functional teams to understand and analyze business requirements Create, review, and refine data models and associated documentation Develop data schemas, dictionaries, and standards to ensure consistency Support data migration, integration, and performance tuning activities Conduct peer reviews and provide feedback on data models and solutions Keep current with the latest industry developments in data architecture and modeling Troubleshoot and resolve data-related technical issuesQualifications Bachelors or Masters degree in Computer Science, Data Science, Information Technology, or related fields Demonstrated experience with data modeling tools and techniques in diverse technological environments Certifications related to data modeling, data management, or cloud platforms (preferred)Professional Competencies Strong analytical and critical thinking skills to develop optimal data solutions Effective communication skills for translating technical concepts to non-technical stakeholders Ability to work independently and in collaborative team environments Skilled problem solver able to handle complex data challenges Adaptability to rapidly evolving technologies and project requirements Excellent time management and prioritization skills to deliver quality outputs consistently",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data modeling', 'modeling tools', 'relational databases', 'scrum', 'agile', 'confluence', 'hipaa', 'data warehousing', 'data architecture', 'erwin', 'sql', 'git', 'gcp', 'mysql', 'etl', 'mongodb', 'jira', 'python', 'oracle', 'microsoft azure', 'sql server', 'nosql', 'gdpr', 'cassandra', 'aws', 'data integration', 'sdlc']",2025-06-13 05:21:19
Senior Data Analyst-Azure Data Factory,Lumen Technologies,8 - 12 years,Not Disclosed,['Bengaluru'],"Were looking for a Senior Data Analyst with a strong foundation in Azure-based data engineering and Machine Learning to design, develop, and optimize robust data pipelines, applications, and analytics infrastructure. This role demands deep technical expertise, cross-functional collaboration, and the ability to align data solutions with dynamic business needs.\nKey Responsibilities:\nData Pipeline Development:\nDesign and implement efficient data pipelines using Azure Databricks with PySpark to transform and process large datasets.\nOptimize data workflows for scalability, reliability, and performance.\nApplication Integration:\nCollaborate with cross-functional teams to develop APIs using the .NET Framework for Azure Web Application integration.\nEnsure smooth data exchange between applications and downstream systems.\nData Warehousing and Analytics:\nBuild and manage data warehousing solutions using Synapse Analytics and Azure Data Factory (ADF).\nDevelop and maintain reusable and scalable data models to support business intelligence needs.\nAutomation and Orchestration:\nUtilize Azure Logic Apps, Function Apps, and Azure DevOps to automate workflows and streamline deployments.\nImplement CI/CD pipelines for efficient code deployment and testing.\nInfrastructure Management:\nOversee Azure infrastructure management and maintenance, ensuring a secure and optimized environment.\nProvide support for performance tuning and capacity planning.\nBusiness Alignment:\nGain a deep understanding of AMO data sources and their business implications.\nWork closely with stakeholders to provide customized solutions aligning with business needs.\nBAU Support:\nMonitor and support data engineering workflows and application functionality in BAU mode.\nTroubleshoot and resolve production issues promptly to ensure business continuity.\nTechnical Expertise:\nProficiency in Microsoft SQL for complex data queries and database management.\nAdvanced knowledge of Azure Databricks and PySpark for data engineering and ETL processes.\nExperience with Azure Data Factory (ADF) for orchestrating data workflows.\nExpertise in Azure Synapse Analytics for data integration and analytics.\nProficiency in .NET Framework for API development and integration.\nCloud and DevOps Skills:\nStrong experience in Azure Infrastructure Management and optimization.\nHands-on knowledge of Azure Logic Apps, Function Apps, and Azure DevOps for CI/CD automation.\n""We are an equal opportunity employer committed to fair and ethical hiring practices. We do not charge any fees or accept any form of payment from candidates at any stage of the recruitment process. If anyone claims to offer employment opportunities in our company in exchange for money or any other benefit, please treat it as fraudulent and report it immediately.""\n#LI-BS1",Industry Type: Telecom / ISP,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Automation', 'orchestration', 'Infrastructure management', 'Machine learning', 'Business intelligence', 'Business continuity', 'Analytics', 'Downstream', 'Capacity planning']",2025-06-13 05:21:21
Big Data Engineer_ Info Edge _Noida,Info Edge,1 - 4 years,14-19 Lacs P.A.,['Noida'],"About Info Edge India Ltd.\nInfo Edge: Info Edge (India) Limited (NSE: NAUKRI) is among the leading internet companies in India. Info Edge, Indias premier online classifieds company is fundamentally in the matching business. With a network of 62 offices located in 43 cities throughout India, Info Edge has 5000 plus employees engaged in innovation, product development, integration with mobile and social media, technology and technology updation, research and development, quality assurance, sales, marketing and payment collection.\nThe umbrella brand has an online recruitment classifieds, www.naukri.com– India’s No. 1 Jobsite with over 75% traffic share, a matrimony classifieds, www.jeevansathi.com, a real estate classifieds, www.99acres.com– India’s largest property marketplace and an education classifieds, www.shiksha.com. Find out more about the Company at",,,,"['Data Modeling', 'Python', 'SCALA']",2025-06-13 05:21:22
"Senior Data Scientist (AI/ML, Data Analysis, Cloud (AWS), and Model",Synechron,8 - 13 years,Not Disclosed,['Pune'],"job requisition idJR1027352\n\nJob Summary\nSynechron is seeking an analytical and innovative Senior Data Scientist to support and advance our data-driven initiatives. The ideal candidate will have a solid understanding of data science principles, hands-on experience with AI/ML tools and techniques, and the ability to interpret complex data sets to deliver actionable insights. This role contributes to the organizations strategic decision-making and technology innovation by applying advanced analytics and machine learning models in a collaborative environment.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCandidates with extensive research or academic experience in AI/ML can be considered, provided they demonstrate practical application of skills.",,,,"['java', 'data science', 'python', 'deploying models', 'aws', 'continuous integration', 'kubernetes', 'scikit-learn', 'ci/cd', 'artificial intelligence', 'sql', 'docker', 'tensorflow', 'spark', 'pytorch', 'keras', 'hadoop', 'big data', 'mongodb', 'microsoft azure', 'nosql', 'pandas', 'amazon ec2', 'r', 'cassandra', 'agile']",2025-06-13 05:21:25
Data Techology Senior Associate,MSCI Services,4 - 7 years,Not Disclosed,['Pune'],"Overview\nThe Data Technology team at MSCI is responsible for meeting the data requirements across various business areas, including Index, Analytics, and Sustainability. Our team collates data from multiple sources such as vendors (e.g., Bloomberg, Reuters), website acquisitions, and web scraping (e.g., financial news sites, company websites, exchange websites, filings). This data can be in structured or semi-structured formats. We normalize the data, perform quality checks, assign internal identifiers, and release it to downstream applications.\nResponsibilities\nAs data engineers, we build scalable systems to process data in various formats and volumes, ranging from megabytes to terabytes. Our systems perform quality checks, match data across various sources, and release it in multiple formats. We leverage the latest technologies, sources, and tools to process the data. Some of the exciting technologies we work with include Snowflake, Databricks, and Apache Spark.\nQualifications\nCore Java, Spring Boot, Apache Spark, Spring Batch, Python. Exposure to sql databases like Oracle, Mysql, Microsoft Sql is a must. Any experience/knowledge/certification on Cloud technology preferrably Microsoft Azure or Google cloud platform is good to have. Exposures to non sql databases like Neo4j or Document database is again good to have.\n What we offer you\nTransparent compensation schemes and comprehensive employee benefits, tailored to your location, ensuring your financial security, health, and overall wellbeing.\nFlexible working arrangements, advanced technology, and collaborative workspaces.\nA culture of high performance and innovation where we experiment with new ideas and take responsibility for achieving results.\nA global network of talented colleagues, who inspire, support, and share their expertise to innovate and deliver for our clients.\nGlobal Orientation program to kickstart your journey, followed by access to our Learning@MSCI platform, LinkedIn Learning Pro and tailored learning opportunities for ongoing skills development.\nMulti-directional career paths that offer professional growth and development through new challenges, internal mobility and expanded roles.\nWe actively nurture an environment that builds a sense of inclusion belonging and connection, including eight Employee Resource Groups. All Abilities, Asian Support Network, Black Leadership Network, Climate Action Network, Hola! MSCI, Pride & Allies, Women in Tech, and Women’s Leadership Forum.\nAt MSCI we are passionate about what we do, and we are inspired by our purpose – to power better investment decisions. You’ll be part of an industry-leading network of creative, curious, and entrepreneurial pioneers. This is a space where you can challenge yourself, set new standards and perform beyond expectations for yourself, our clients, and our industry.\nMSCI is a leading provider of critical decision support tools and services for the global investment community. With over 50 years of expertise in research, data, and technology, we power better investment decisions by enabling clients to understand and analyze key drivers of risk and return and confidently build more effective portfolios. We create industry-leading research-enhanced solutions that clients use to gain insight into and improve transparency across the investment process.\nMSCI Inc. is an equal opportunity employer. It is the policy of the firm to ensure equal employment opportunity without discrimination or harassment on the basis of race, color, religion, creed, age, sex, gender, gender identity, sexual orientation, national origin, citizenship, disability, marital and civil partnership/union status, pregnancy (including unlawful discrimination on the basis of a legally protected parental leave), veteran status, or any other characteristic protected by law. MSCI is also committed to working with and providing reasonable accommodations to individuals with disabilities. If you are an individual with a disability and would like to request a reasonable accommodation for any part of the application process, please email Disability.Assistance@msci.com and indicate the specifics of the assistance needed. Please note, this e-mail is intended only for individuals who are requesting a reasonable workplace accommodation; it is not intended for other inquiries.\n To all recruitment agencies\nMSCI does not accept unsolicited CVs/Resumes. Please do not forward CVs/Resumes to any MSCI employee, location, or website. MSCI is not responsible for any fees related to unsolicited CVs/Resumes.\n Note on recruitment scams\nWe are aware of recruitment scams where fraudsters impersonating MSCI personnel may try and elicit personal information from job seekers. Read our full note on careers.msci.com",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'access', 'scala', 'pyspark', 'data warehousing', 'hibernate', 'research', 'sql', 'analytics', 'spring', 'java', 'spring batch', 'spark', 'gcp', 'mysql', 'html', 'hadoop', 'big data', 'etl', 'snowflake', 'python', 'oracle', 'data analysis', 'microsoft azure', 'power bi', 'sql server', 'javascript', 'data bricks', 'spring boot', 'tableau', 'neo4j', 'aws', 'sql database']",2025-06-13 05:21:26
"Senior Python Developer (Machine Learning,Data Analysis,Visualization)",Synechron,3 - 5 years,Not Disclosed,"['Pune', 'Hinjewadi']","Software Requirements\nRequired Skills:\nProficiency in Python (version 3.6+) with experience in data analysis, manipulation, and scripting\nKnowledge of SQL for data extraction, transformation, and database querying\nExperience with data visualization tools such as PowerBI, Tableau, or QlikView\nFamiliarity with AI and Machine Learning frameworks such as TensorFlow, Keras, PyTorch, or equivalent",,,,"['Python', 'PostgreSQL', 'MySQL', 'Data Analysis', 'Data Visualization', 'Oracle', 'ETL', 'Machine Learning']",2025-06-13 05:21:28
Data Techology Senior Associate,MSCI Services,4 - 8 years,Not Disclosed,['Pune'],"As data engineers, we build scalable systems to process data in various formats and volumes, ranging from megabytes to terabytes. Our systems perform quality checks, match data across various sources, and release it in multiple formats. We leverage the latest technologies, sources, and tools to process the data. Some of the exciting technologies we work with include Snowflake, Databricks, and Apache Spark.\nYour skills and experience that will help you excel\nCore Java, Spring Boot, Apache Spark, Spring Batch, Python. Exposure to sql databases like Oracle, Mysql, Microsoft Sql is a must. Any experience / knowledge / certification on Cloud technology preferrably Microsoft Azure or Google cloud platform is good to have. Exposures to non sql databases like Neo4j or Document database is again good to have.\n  What we offer you\nTransparent compensation schemes and comprehensive employee benefits, tailored to your location, ensuring your financial security, health, and overall we'llbeing.\nFlexible working arrangements, advanced technology, and collaborative workspaces.\nA culture of high performance and innovation where we experiment with new ideas and take responsibility for achieving results.\nA global network of talented colleagues, who inspire, support, and share their expertise to innovate and deliver for our clients.\nGlobal Orientation program to kickstart your journey, followe'd by access to our Learning@MSCI platform, LinkedIn Learning Pro and tailored learning opportunities for ongoing skills development.\nMulti-directional career paths that offer professional growth and development through new challenges, internal mobility and expanded roles.\nWe actively nurture an environment that builds a sense of inclusion belonging and connection, including eight Employee Resource Groups. All Abilities, Asian Support Network, Black Leadership Network, Climate Action Network, Hola! MSCI, Pride & Allies, Women in Tech, and Women s Leadership Forum.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['CVS', 'Core Java', 'Bloomberg', 'spring batch', 'MySQL', 'Oracle', 'Analytics', 'Downstream', 'Python', 'Recruitment']",2025-06-13 05:21:30
Senior Data Scientist,Capgemini,5 - 9 years,Not Disclosed,['Gurugram'],"At Capgemini Invent, we believe difference drives change. As inventive transformation consultants, we blend our strategic, creative and scientific capabilities,collaborating closely with clients to deliver cutting-edge solutions. Join us to drive transformation tailored to our client's challenges of today and tomorrow.Informed and validated by science and data. Superpowered by creativity and design. All underpinned by technology created with purpose.\n\n \n\nYour role \n\nAs a Senior Data Scientist, you are expected to develop and implement Artificial Intelligence based solutions across various disciplines for the Intelligent Industry vertical of Capgemini Invent. You are expected to work as an individual contributor or along with a team to help design and develop ML/NLP models as per the requirement. You will work closely with the Product Owner, Systems Architect and other key stakeholders right from conceptualization till the implementation of the project. You should take ownership while understanding the client requirement, the data to be used, security & privacy needs and the infrastructure to be used for the development and implementation.\n\nThe candidate will be responsible for executing data science projects independently to deliver business outcomes and is expected to demonstrate domain expertise, develop, and execute program plans and proactively solicit feedback from stakeholders to identify improvement actions. This role requires a strong technical background, excellent problem-solving skills, and the ability to work collaboratively with stakeholders from different functional and business teams.\nThe role also requires the candidate to collaborate on ML asset creation and eager to learn and impart trainings to fellow data science professionals. We expect thought leadership from the candidate, especially on proposing to build a ML/NLP asset based on expected industry requirements. Experience in building Industry specific (e.g. Manufacturing, R&D, Supply Chain, Life Sciences etc), production ready AI Models using microservices and web-services is a plus.\n\nProgramming Languages Python NumPy, SciPy, Pandas, MatPlotLib, Seaborne\nDatabases RDBMS (MySQL, Oracle etc.), NoSQL Stores (HBase, Cassandra etc.)\nML/DL Frameworks SciKitLearn, TensorFlow (Keras), PyTorch,\nBig data ML Frameworks - Spark (Spark-ML, Graph-X), H2O\nCloud Azure/AWS/GCP\n\n \n\nYour Profile \n\nPredictive and Prescriptive modelling using Statistical and Machine Learning algorithms including but not limited to Time Series, Regression, Trees, Ensembles, Neural-Nets (Deep & Shallow CNN, LSTM, Transformers etc.). Experience with open-source OCR engines like Tesseract, Speech recognition, Computer Vision, face recognition, emotion detection etc. is a plus.\nUnsupervised learning Market Basket Analysis, Collaborative Filtering, Dimensionality Reduction, good understanding of common matrix decomposition approaches like SVD. Various Clustering approaches Hierarchical, Centroid-based, Density-based, Distribution-based, Graph-based clustering like Spectral.\nNLP Information Extraction, Similarity Matching, Sentiment Analysis, Text Clustering, Semantic Analysis, Document Summarization, Context Mapping/Understanding, Intent Classification, Word Embeddings, Vector Space Models, experience with libraries like NLTK, Spacy, Stanford Core-NLP is a plus. Usage of Transformers for NLP and experience with LLMs like (ChatGPT, Llama) and usage of RAGs (vector stores like LangChain & LangGraps), building Agentic AI applications.\nModel Deployment ML pipeline formation, data security and scrutiny check and ML-Ops for productionizing a built model on-premises and on cloud.\n\nRequired Qualifications\nMasters degree in a quantitative field such as Mathematics, Statistics, Machine Learning, Computer Science or Engineering or a bachelors degree with relevant experience.\nGood experience in programming with languages such as Python/Java/Scala, SQL and experience with data visualization tools like Tableau or Power BI.\n\nPreferred Experience\nExperienced in Agile way of working, manage team effort and track through JIRA\nExperience in Proposal, RFP, RFQ and pitch creations and delivery to the big forum.\nExperience in POC, MVP, PoV and assets creations with innovative use cases\nExperience working in a consulting environment is highly desirable.\nPresupposition\n\nHigh Impact client communication\nThe job may also entail sitting as well as working at a computer for extended periods of time. Candidates should be able to effectively communicate by telephone, email, and face to face.\n\n \n\nWhat you will love about working here \nWe recognize the significance of flexible work arrangements to provide support. Be it remote work, or flexible work hours, you will get an environment to maintain healthy work life balance.\nAt the heart of our mission is your career growth. Our array of career growth programs and diverse professions are crafted to support you in exploring a world of opportunities.\nEquip yourself with valuable certifications in the latest technologies such as Generative AI.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['numpy', 'sql', 'java', 'python', 'pandas', 'scala', 'poc', 'nltk', 'dl', 'artificial intelligence', 'tensorflow', 'spacy', 'spark', 'gcp', 'pytorch', 'keras', 'mysql', 'hbase', 'ml', 'jira', 'scipy', 'rdbms', 'oracle', 'mvp', 'microsoft azure', 'power bi', 'nosql', 'tableau', 'cassandra', 'matplotlib', 'agile', 'aws']",2025-06-13 05:21:32
Data Scientist Sr. Analyst,Accenture,5 - 10 years,Not Disclosed,['Kochi'],"Job Title - + +\n\n\n\nManagement Level:\n\n\n\nLocation:Kochi, Coimbatore, Trivandrum\n\n\n\nMust have skills:Big Data, Python or R\n\n\n\n\nGood to have skills:Scala, SQL\n\n\n\nJob\n\n\nSummary\n\nA Data Scientist is expected to be hands-on to deliver end to end vis a vis projects undertaken in the Analytics space. They must have a proven ability to drive business results with their data-based insights. They must be comfortable working with a wide range of stakeholders and functional teams. The right candidate will have a passion for discovering solutions hidden in large data sets and working with stakeholders to improve business outcomes.\n\n\n\nRoles and Responsibilities\nIdentify valuable data sources and collection processes\nSupervise preprocessing of structured and unstructured data\nAnalyze large amounts of information to discover trends and patterns for insurance industry.\nBuild predictive models and machine-learning algorithms\nCombine models through ensemble modeling\nPresent information using data visualization techniques\nCollaborate with engineering and product development teams\nHands-on knowledge of implementing various AI algorithms and best-fit scenarios\nHas worked on Generative AI based implementations\n\n\n\nProfessional and Technical Skills\n3.5-5 years experience in Analytics systems/program delivery; at least 2 Big Data or Advanced Analytics project implementation experience\nExperience using statistical computer languages (R, Python, SQL, Pyspark, etc.) to manipulate data and draw insights from large data sets; familiarity with Scala, Java or C++\nKnowledge of a variety of machine learning techniques (clustering, decision tree learning, artificial neural networks, etc.) and their real-world advantages/drawbacks\nKnowledge of advanced statistical techniques and concepts (regression, properties of distributions, statistical tests and proper usage, etc.) and experience with applications\nHands on experience in Azure/AWS analytics platform (3+ years)\nExperience using variations of Databricks or similar analytical applications in AWS/Azure\nExperience using business intelligence tools (e.g. Tableau) and data frameworks (e.g. Hadoop)\nStrong mathematical skills (e.g. statistics, algebra)\nExcellent communication and presentation skills\nDeploying data pipelines in production based on Continuous Delivery practices.\n\n\n\n\nAdditional Information\nMulti Industry domain experience\nExpert in Python, Scala, SQL\nKnowledge of Tableau/Power BI or similar self-service visualization tools\nInterpersonal and Team skills should be top notch\nNice to have leadership experience in the past\nQualification\n\n\n\nExperience:3.5 -5 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'scala', 'sql', 'r', 'big data', 'advanced analytics', 'mathematics', 'data manipulation', 'presentation skills', 'microsoft azure', 'pyspark', 'power bi', 'machine learning', 'javascript', 'aws kinesis', 'tableau', 'decision tree', 'java', 'hadoop', 'data visualization', 'aws', 'statistics']",2025-06-13 05:21:34
Senior Analyst - Data Governance & Management,AMERICAN EXPRESS,3 - 7 years,Not Disclosed,['Gurugram'],"Here, your voice and ideas matter, your work makes an impact, and together, you will help us define the future of American Express.\nAt American Express, you ll be recognized for your contributions, leadership, and impact every colleague has the opportunity to share in the company s success. Together, we ll win as a team, striving to uphold our company values and powerful backing promise to provide the world s best customer experience every day. And we ll do it with the utmost integrity, and in an environment where everyone is seen, heard and feels like they belong.\nJoin Team Amex and lets lead the way together.",,,,"['Career development', 'metadata', 'Manager Quality Assurance', 'Data management', 'Finance', 'Shell scripting', 'Wellness', 'Data quality', 'Risk management', 'SQL']",2025-06-13 05:21:36
IN_Manager_Azure Data Engineer_Data & Analytics_Advisory_Bangalore,PwC Service Delivery Center,4 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\n& Summary A career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.\nResponsibilities\nDesign, develop, and optimize data pipelines and ETL processes using PySpark or Scala to extract, transform, and load large volumes of structured and unstructured data from diverse sources. Implement data ingestion, processing, and storage solutions on Azure cloud platform, leveraging services such as Azure Databricks, Azure Data Lake Storage, and Azure Synapse Analytics. Develop and maintain data models, schemas, and metadata to support efficient data access, query performance, and analytics requirements. Monitor pipeline performance, troubleshoot issues, and optimize data processing workflows for scalability, reliability, and costeffectiveness. Implement data security and compliance measures to protect sensitive information and ensure regulatory compliance. Requirement Proven experience as a Data Engineer, with expertise in building and optimizing data pipelines using PySpark, Scala, and Apache Spark. Handson experience with cloud platforms, particularly Azure, and proficiency in Azure services such as Azure Databricks, Azure Data Lake Storage, Azure Synapse Analytics, and Azure SQL Database. Strong programming skills in Python and Scala, with experience in software development, version control, and CI/CD practices. Familiarity with data warehousing concepts, dimensional modeling, and relational databases (e.g., SQL Server, PostgreSQL, MySQL).\nExperience with big data technologies and frameworks (e.g., Hadoop, Hive, HBase) is a plus.\nMandatory skill sets\nSpark, Pyspark, Azure\nPreferred skill sets\nSpark, Pyspark, Azure\nYears of experience required\n4 8\nEducation qualification\nB.Tech / M.Tech / MBA / MCA\nEducation\nDegrees/Field of Study required Bachelor of Engineering, Master of Business Administration, Master of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nPython (Programming Language)\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Coaching and Feedback, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling {+ 32 more}\nNo",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SAP', 'Data management', 'Data modeling', 'Postgresql', 'MySQL', 'Database administration', 'Agile', 'Apache', 'Business intelligence', 'Python']",2025-06-13 05:21:37
Data Scientist-Artificial Intelligence,IBM,5 - 7 years,Not Disclosed,['Bengaluru'],"Work with broader team to build, analyze and improve the AI solutions.\nYou will also work with our software developers in consuming different enterprise applications\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nResource should have 5-7 years of experience. Sound knowledge of Python and should know how to use the ML related services.\nProficient in Python with focus on Data Analytics Packages.\nStrategy Analyse large, complex data sets and provide actionable insights to inform business decisions.\nStrategy Design and implementing data models that help in identifying patterns and trends. Collaboration Work with data engineers to optimize and maintain data pipelines.\nPerform quantitative analyses that translate data into actionable insights and provide analytical, data-driven decision-making. Identify and recommend process improvements to enhance the efficiency of the data platform. Develop and maintain data models, algorithms, and statistical models\n\n\nPreferred technical and professional experience\nExperience with conversation analytics. Experience with cloud technologies\nExperience with data exploration tools such as Tableu",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['algorithms', 'python', 'data analytics', 'tableau', 'ml', 'hive', 'data analysis', 'natural language processing', 'pyspark', 'data warehousing', 'machine learning', 'artificial intelligence', 'sql', 'pandas', 'deep learning', 'java', 'data science', 'spark', 'kafka', 'hadoop', 'big data', 'aws', 'etl']",2025-06-13 05:21:39
S&C GN - Data&AI - CMT Eng - Consultant,Accenture,2 - 7 years,Not Disclosed,['Bengaluru'],"Job Title - S&C Global Network - AI - CMT DE- Consultant\n\n\n\nManagement Level:9- Consultant\n\n\n\nLocation:Open\n\n\n\nMust-have skills:Data Engineering\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nWe are looking for a passionate and results-driven\n\n\n\nData Engineerto join our growing data team. You will be responsible for designing, building, and maintaining scalable data pipelines and infrastructure that support data-driven decision-making across the organization.\n\n\n\n\nRoles & Responsibilities:\n\nDesign, build, and maintain robust, scalable, and efficient data pipelines (ETL/ELT).\nWork with structured and unstructured data across a wide variety of data sources.\nCollaborate with data analysts, data scientists, and business stakeholders to understand data requirements.\nOptimize data systems and architecture for performance, scalability, and reliability.\nMonitor data quality and support initiatives to ensure clean, accurate, and consistent data.\nDevelop and maintain data models and metadata.\nImplement and maintain best practices in data governance, security, and compliance.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n2+ years in data engineering or related fields\nProficiency in SQL and experience with relational databases (e.g., PostgreSQL, MySQL).\nStrong programming skills in Python, Scala, or Java.\nExperience with big data technologies such as Spark, Hadoop, or Hive.\nFamiliarity with cloud platforms like AWS, Azure, or GCP, especially services like S3, Redshift, BigQuery, or Azure Data Lake.\nExperience with orchestration tools like Airflow, Luigi, or similar.\nSolid understanding of data warehousing concepts and data modeling techniques.\nGood problem-solving skills and attention to detail.\nExperience with modern data stack tools like dbt, Snowflake, or Databricks.\nKnowledge of CI/CD pipelines and version control (e.g., Git).\nExposure to containerization (Docker, Kubernetes) and infrastructure as code (Terraform, CloudFormation).\n\n\n\n\nAdditional Information: - The ideal candidate will possess a strong educational background in quantitative discipline and experience in working with Hi-Tech clients\n\n- This position is based at our Bengaluru (preferred) and other AI Accenture locations.\n\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience:4+ years\n\n\n\n\nEducational Qualification:Btech/ BE",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'scala', 'data engineering', 'sql', 'java', 'hive', 'continuous integration', 'kubernetes', 'snowflake', 'amazon redshift', 'airflow', 'microsoft azure', 'ci/cd', 'aws cloudformation', 'docker', 'data bricks', 'data modeling', 'spark', 'gcp', 'data warehousing concepts', 'terraform', 'hadoop', 'aws']",2025-06-13 05:21:41
Data Science,Global Banking Organization,5 - 10 years,Not Disclosed,['Bengaluru'],"Key Skills: Machine Learning, Data Science, Azure, Python, Hadoop.\nRoles and Responsibilities:\nStrong understanding of Math, Statistics, and the theoretical foundations of Statistical & Machine Learning, including Parametric and Non-parametric models.\nApply advanced data mining techniques to curate, process, and transform raw data into reliable datasets.\nUse various statistical techniques and ML methods to perform predictive modeling/classification for problems related to clients, distribution, sales, client profiles, and segmentation, and provide actionable insights for business decision-making.\nDemonstrate expertise in the full Machine Learning lifecycle--feature engineering, training, validation, scaling, deployment, scoring, monitoring, and feedback loops.\nProficiency in Python visualization libraries such as matplotlib and seaborn.\nExperience with cloud computing infrastructure like Azure, including Machine Learning Studio, Azure Data Factory, Synapse, Python, and PySpark.\nAbility to develop, test, and deploy models on cloud/web platforms.\nExcellent knowledge of Deep Learning Architectures, including Convolutional Neural Networks and Transformer/LLM Foundation Models.\nStrong expertise in supervised and adversarial learning techniques.\nRobust working knowledge of deep learning frameworks such as TensorFlow, Keras, and PyTorch.\nExcellent Python coding skills.\nExperience with version control tools (Git, GitHub/GitLab) and data version control.\nExperience in end-to-end model deployment and productionization.\nDemonstrated proficiency in deploying, scaling, and optimizing ML models in production environments with low latency, high availability, and cost efficiency.\nSkilled in model interpretability and CI/CD for ML using tools like MLflow and Kubeflow, with the ability to implement automated monitoring, logging, and retraining strategies.\nExperience Requirement:\n5-12 years of experience in designing and deploying deep learning and machine learning solutions.\nProven track record of delivering AI/ML solutions in real-world business applications at scale.\nHands-on experience working in cross-functional teams including data engineers, product managers, and business stakeholders.\nExperience mentoring junior data scientists and providing technical leadership within a data science team.\nExperience working with big data tools and environments such as Hadoop, Spark, or Databricks is a plus.\nPrior experience in managing model lifecycle in enterprise production environments including drift detection and retraining pipelines.\nEducation: B.Tech.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Azure', 'Hadoop.', 'Machine Learning', 'Python']",2025-06-13 05:21:43
"Data Analyst, Staff",Qualcomm,4 - 7 years,Not Disclosed,['Bengaluru'],"Job Area: Miscellaneous Group, Miscellaneous Group > Data Analyst\n \n\nQualcomm Overview: \nQualcomm is a company of inventors that unlocked 5G ushering in an age of rapid acceleration in connectivity and new possibilities that will transform industries, create jobs, and enrich lives. But this is just the beginning. It takes inventive minds with diverse skills, backgrounds, and cultures to transform 5Gs potential into world-changing technologies and products. This is the Invention Age - and this is where you come in.\n\nGeneral Summary:\n\nAbout the Team\n\nQualcomm's People Analytics team plays a crucial role in transforming data into strategic workforce insights that drive HR and business decisions. As part of this lean but high-impact team, you will have the opportunity to analyze workforce trends, ensure data accuracy, and collaborate with key stakeholders to enhance our data ecosystem. This role is ideal for a generalist who thrives in a fast-paced, evolving environment""”someone who can independently conduct data analyses, communicate insights effectively, and work cross-functionally to enhance our People Analytics infrastructure.\n\nWhy Join Us\n\n\nEnd-to-End ImpactWork on the full analytics cycle""”from data extraction to insight generation""”driving meaningful HR and business decisions.\n\n\nCollaboration at ScalePartner with HR leaders, IT, and other analysts to ensure seamless data integration and analytics excellence.\n\n\nData-Driven CultureBe a key player in refining our data lake, ensuring data integrity, and influencing data governance efforts.\n\n\nProfessional GrowthGain exposure to multiple areas of people analytics, including analytics, storytelling, and stakeholder engagement.\n\n\nKey Responsibilities\n\n\nPeople Analytics & Insights\nAnalyze HR and workforce data to identify trends, generate insights, and provide recommendations to business and HR leaders.\nDevelop thoughtful insights to support ongoing HR and business decision-making.\nPresent findings in a clear and compelling way to stakeholders at various levels, including senior leadership.\n\n\nData Quality & Governance\nEnsure accuracy, consistency, and completeness of data when pulling from the data lake and other sources.\nIdentify and troubleshoot data inconsistencies, collaborating with IT and other teams to resolve issues.\nDocument and maintain data definitions, sources, and reporting standards to drive consistency across analytics initiatives.\n\n\nCollaboration & Stakeholder Management\nWork closely with other analysts on the team to align methodologies, share best practices, and enhance analytical capabilities.\nAct as a bridge between People Analytics, HR, and IT teams to define and communicate data requirements.\nPartner with IT and data engineering teams to improve data infrastructure and expand available datasets.\n\n\nQualifications\n\nRequired4-7 years experience in a People Analytics focused role\n\n\nAnalytical & Technical Skills\nStrong ability to analyze, interpret, and visualize HR and workforce data to drive insights.\nExperience working with large datasets and ensuring data integrity.\nProficiency in Excel and at least one data visualization tool (e.g., Tableau, Power BI).\n\n\nCommunication & Stakeholder Management\nAbility to communicate data insights effectively to both technical and non-technical audiences.\nStrong documentation skills to define and communicate data requirements clearly.\nExperience collaborating with cross-functional teams, including HR, IT, and business stakeholders.\n\n\nPreferred:\n\n\nTechnical Proficiency\nExperience with SQL, Python, or R for data manipulation and analysis.\nFamiliarity with HR systems (e.g., Workday) and cloud-based data platforms.\n\n\nPeople Analytics Expertise\nPrior experience in HR analytics, workforce planning, or related fields.\nUnderstanding of key HR metrics and workforce trends (e.g., turnover, engagement, diversity analytics).\n\n\nAdditional Information\nThis is an office-based position (4 days a week onsite) with possible locations that may include India and Mexico",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['sql', 'people analytics', 'documentation', 'tableau', 'data integration tools', 'hiring', 'data warehousing', 'data architecture', 'sourcing', 'jquery', 'staffing', 'plsql', 'oracle 10g', 'java', 'etl tool', 'html', 'etl', 'mongodb', 'python', 'oracle', 'power bi', 'hrsd', 'r', 'node.js', 'hr analytics', 'angularjs']",2025-06-13 05:21:44
IN_Manager_Azure Data Engineer_Data Analytics_Advisory,PwC Service Delivery Center,5 - 10 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nData, Analytics & AI\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\n& Summary A career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.\nResponsibilities\nMust have\nCandidates with minimum 5 years of relevant experience for 1012 years of total experience (Architect / Managerial level).\nDeep expertise with technologies such as Data factory, Data Bricks (Advanced), SQLDB (writing complex Stored Procedures), Synapse, Python scripting (mandatory), Pyspark scripting, Azure Analysis Services.\nMust be certified with DP 203 (Azure Data Engineer Associate), Databricks Certified Data Engineer Professional (Architect / Managerial level)\nStrong troubleshooting and debugging skills. Proven experience in working source control technologies (such as GITHUB, Azure DevOps), build and release pipelines.\nExperience in writing complex PySpark queries to perform data analysis.\nMandatory skill sets\nAzure Databricks, Pyspark, Datafactory\nPreferred skill sets\nAzure Databricks, Pyspark, Datafactory, Python, Azure Devops\nYears of experience required\n712yrs\nEducation qualification\nB.Tech / M.Tech / MBA / MCA\nEducation\nDegrees/Field of Study required Bachelor of Technology, Master of Business Administration\nDegrees/Field of Study preferred\nRequired Skills\nMicrosoft Azure\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Coaching and Feedback, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling {+ 32 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'Data modeling', 'Debugging', 'Database administration', 'Agile', 'Stored procedures', 'Apache', 'Business intelligence', 'Troubleshooting', 'Python']",2025-06-13 05:21:46
AWS Data Engineer_ Capgemini_ Pan India Location,Capgemini,5 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru', 'Mumbai (All Areas)']","Role & responsibilities\nAWS S3, Glue, API Gateway, Crawler, Athena, , Lambda, Dynamic DB, Redshift is an advantage\nExperience/knowledge with streaming technologies is must preferably Kafka\nShould have knowledge/experience with SQL\nGood analytical skills\nFamiliar working on Linux platforms\nHave good understanding on pros and cons and the cost impact of the AWS services being leveraged\nGood Communication skills.\n\nPrimary Skills\nAWS S3, Glue, Athena, Python or Pyspark\nShould have knowledge/experience with SQL\n\nSecondary Skills\nExcellent verbal and written communication and interpersonal skills\nAbility to work independently and within a team environment",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Lamda', 'AWS', 'Athena', 'SQL', 'Python', 'S', 'Amazon Redshift', 'Aws Glue']",2025-06-13 05:21:48
Data Bricks,PwC India,7 - 12 years,Not Disclosed,['Bengaluru'],"Job Summary:\n\nWe are seeking a talented Data Engineer with strong expertise in Databricks, specifically in Unity Catalog, PySpark, and SQL, to join our data team. Youll play a key role in building secure, scalable data pipelines and implementing robust data governance strategies using Unity Catalog.\n\nKey Responsibilities:",,,,"['DataBricks', 'Data Bricks', 'Pyspark', 'Delta Lake', 'Databricks Engineer', 'Unity Catalog', 'SQL']",2025-06-13 05:21:50
S&C GN - Data&AI - Life Sciences - Consultant,Accenture,4 - 9 years,Not Disclosed,['Bengaluru'],"Management Level:Ind&Func AI Decision Science Consultant\n\n\n\n\nJob Location:Bangalore / Gurgaon\n\n\n\nMust-have\n\n\n\n\nSkills:\nExcellent understanding of Pharma data sets commercial, clinical, Leverage ones hands on experience of working across one or more of these areas such as real-world evidence data, Statistical Models/Machine Learning including Segmentation & predictive modeling, hypothesis testing, multivariate statistical analysis, time series techniques, and optimization.\n\n\n\nGood-to-have\n\n\n\n\nSkills:\nProgramming languages such as R, Python, SQL, Spark, AWS, Azure, or Google Cloud for deploying and scaling language models, Data Visualization tools like Tableau, Power BI.\n\n\n\nExperience:Proven experience (4+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.\n\n\n\nJob\n\n\nSummary\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions. Provide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\nKey Responsibilities\nAn opportunity to work on high-visibility projects with top Pharma clients around the globe.\nPersonalized training modules to develop your strategy & consulting acumen to grow your skills, industry knowledge, and capabilities.\nProvide Subject matter expertise in various sub-segments of the LS industry.\nSupport delivery of small to medium-sized teams to deliver consulting projects for global clients.\nResponsibilities may include strategy, implementation, process design, and change management for specific modules.\nWork with the team or as an Individual contributor on the project assigned which includes a variety of skills to be utilized from Data Engineering to Data Science\nDevelop assets and methodologies, point-of-view, research, or white papers for use by the team and the larger community.\nAcquire new skills that have utility across industry groups.\nSupport strategies and operating models focused on some business units and assess likely competitive responses. Also, assess implementation readiness and points of greatest impact.\n\n\n\n\n\nAdditional Information\nProficient in Excel, MS Word, PowerPoint, etc.\nAbility to solve complex business problems and deliver client delight.\nStrong writing skills to build points of view on current industry trends.\nGood communication, interpersonal, and presentation skills\n\n\nAbout Our Company | Accenture (do not remove the hyperlink)\n\nQualification\n\n\n\nExperience:Proven experience (4+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['pharmaceutical', 'python', 'sql', 'life sciences', 'aws', 'microsoft azure', 'power bi', 'time series', 'machine learning', 'data engineering', 'artificial intelligence', 'tableau', 'r', 'data science', 'gcp', 'spark', 'predictive modeling', 'statistical modeling', 'data visualization', 'statistics']",2025-06-13 05:21:51
S&C Global Network - AI - CG&S - Consultant Data Science,Accenture,4 - 8 years,Not Disclosed,['Bengaluru'],"Job Title: Industry & Function AI Decision Science Consultant S&C Global Network\n\n\n\nManagement Level: 9 Consultant\n\n\n\nLocation: Primary - Bengaluru, Secondary - Gurugram\n\n\n\nMust-Have\n\n\n\n\nSkills:\nData Science, AI, ML, Experience with cloud platforms such as AWS, Azure, or Google Cloud, Hands-on experience in programming languages like Python, R, PySpark, and SQL\n\n\n\nGood-to-Have\n\n\n\n\nSkills:\nDeep Learning Techniques (e.g. RNN, CNN), Visualization tools like Power BI and Tableau, Exposure to tools like ChatGPT, Llama 2, Hugging Face, etc.\n\n\n\nJob\n\n\nSummary:\n\nAs an Industry & Function AI Decision Science Consultant, you will leverage your expertise in data science and Consumer Goods domain knowledge to design and deliver AI-driven solutions. Your role will include strategic analysis, project delivery, solution development, and technical execution to empower businesses with actionable insights and enable automated and augmented decision-making.\n\n\n\n\nRoles & Responsibilities:\nConduct strategic analysis of the AI, analytics, and data maturity landscape for clients in the Consumer Goods domain\nLead data science engagements, manage delivery teams, and build innovative AI capabilities\nDevelop and implement advanced analytics solutions tailored to client requirements\nUtilize languages like Python, PySpark, R, and SQL for data wrangling and machine learning model development\nLeverage cloud technologies (Azure, AWS, GCP) to integrate and implement AI solutions\nTranslate complex data into compelling narratives for effective data storytelling\nMentor junior team members and contribute to thought leadership\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\nProficiency in Python, R, PySpark, and SQL\nStrong knowledge of traditional statistical methods, machine learning techniques, and deep learning\nHands-on experience in Consumer Goods & Services domain\nCloud integration skills with platforms like AWS, Azure, or Google Cloud\nExperience with optimization techniques (exact and evolutionary)\nCertifications like AWS Certified Data Analytics Specialty or Google Professional Data Engineer\nFamiliarity with visualization tools like Tableau and Power BI\nExposure to large language models (e.g., ChatGPT, Llama 2)\nFamiliarity with version control systems like Git.\n\n\n\n\n\nAdditional Information:\nThe ideal candidate will have a strong educational background in data science, computer science, or a related field, along with a proven track record of delivering impactful AI-driven solutions in the Consumer Goods industry.\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience: Minimum 4-8 years of hands-on experience in data science with a focus on the Consumer Goods industry\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Economics, Mathematics, Computer Science, or equivalent degree with Data Science specialization (from a premier institute)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'artificial intelligence', 'deep learning', 'data science', 'ml', 'advanced analytics', 'cnn', 'data analytics', 'pyspark', 'microsoft azure', 'power bi', 'machine learning', 'sql', 'r', 'tableau', 'git', 'rnn', 'gcp', 'machine learning algorithms', 'aws', 'consumer goods', 'statistics']",2025-06-13 05:21:53
S&C GN - Data&AI - Life Sciences - Analyst,Accenture,2 - 7 years,Not Disclosed,['Bengaluru'],"Management Level:Ind & Func AI Decision Science Analyst\n\n\n\n\nJob Location:Bangalore / Gurgaon\n\n\n\nMust-have\n\n\n\n\nSkills:\nLife Sciences/Pharma/Healthcare projects and delivering successful outcomes, commercial, clinical, Statistical Models/Machine Learning including Segmentation & predictive modeling, hypothesis testing, multivariate statistical analysis, time series techniques, and optimization.\n\n\n\nGood-to-have\n\n\n\n\nSkills:\nProficiency in Programming languages such as R, Python, SQL, Spark, AWS, Azure, or Google Cloud for deploying and scaling language models, Data Visualization tools like Tableau, Power BI\n\n\n\nJob\n\n\nSummary\n\nWe are seeking an experienced and visionary - Accenture S&C Global Network - Data & AI practice help our clients grow their business in entirely new ways. Analytics enables our clients to achieve high performance through insights from data - insights that inform better decisions and strengthen customer relationships. From strategy to execution, Accenture works with organizations to develop analytic capabilities - from accessing and reporting on data to predictive modelling - to outperform the competition.\n\n\n\nKey Responsibilities\nSupport delivery of small to medium-sized teams to deliver consulting projects for global clients.\nAn opportunity to work on high-visibility projects with top Pharma clients around the globe.\nPersonalized training modules to develop your strategy & consulting acumen to grow your skills, industry knowledge, and capabilities.\nResponsibilities may include strategy, implementation, process design, and change management for specific modules.\nWork with the team or as an Individual contributor on the project assigned which includes a variety of skills to be utilized from Data Engineering to Data Science\nDevelop assets and methodologies, point-of-view, research, or white papers for use by the team and the larger community.\nWork on variety of projects in Data Modeling, Data Engineering, Data Visualization, Data Science etc.,\nAcquire new skills that have utility across industry groups.\n\n\n\n\n\nAdditional Information\nAbility to solve complex business problems and deliver client delight.\nStrong writing skills to build points of view on current industry trends.\nGood communication, interpersonal, and presentation skills\n\n\nAbout Our Company | Accenture (do not remove the hyperlink)\n\n\nQualification\n\n\n\nExperience:Proven experience (2+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['pharmaceutical', 'python', 'sql', 'life sciences', 'aws', 'presentation skills', 'microsoft azure', 'power bi', 'time series', 'machine learning', 'artificial intelligence', 'tableau', 'r', 'data science', 'gcp', 'spark', 'predictive modeling', 'statistical modeling', 'data visualization', 'statistics']",2025-06-13 05:21:55
S&C Global Network - AI - CG&S - Consultant Data Science,Accenture,4 - 8 years,Not Disclosed,['Bengaluru'],"Job Title: Industry & Function AI Decision Science Consultant S&C Global Network\n\n\n\nManagement Level: 9 Consultant\n\n\n\nLocation: Primary - Bengaluru, Secondary - Gurugram\n\n\n\nMust-Have\n\n\n\n\nSkills:\nData Science, AI, ML, Experience with cloud platforms such as AWS, Azure, or Google Cloud, Hands-on experience in programming languages like Python, R, PySpark, and SQL\n\n\n\nGood-to-Have\n\n\n\n\nSkills:\nDeep Learning Techniques (e.g. RNN, CNN), Visualization tools like Power BI and Tableau, Exposure to tools like ChatGPT, Llama 2, Hugging Face, etc.\n\n\n\nJob\n\n\nSummary:\n\nAs an Industry & Function AI Decision Science Consultant, you will leverage your expertise in data science and Consumer Goods domain knowledge to design and deliver AI-driven solutions. Your role will include strategic analysis, project delivery, solution development, and technical execution to empower businesses with actionable insights and enable automated and augmented decision-making.\n\n\n\n\nRoles & Responsibilities:\nConduct strategic analysis of the AI, analytics, and data maturity landscape for clients in the Consumer Goods domain\nLead data science engagements, manage delivery teams, and build innovative AI capabilities\nDevelop and implement advanced analytics solutions tailored to client requirements\nUtilize languages like Python, PySpark, R, and SQL for data wrangling and machine learning model development\nLeverage cloud technologies (Azure, AWS, GCP) to integrate and implement AI solutions\nTranslate complex data into compelling narratives for effective data storytelling\nMentor junior team members and contribute to thought leadership\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\nProficiency in Python, R, PySpark, and SQL\nStrong knowledge of traditional statistical methods, machine learning techniques, and deep learning\nHands-on experience in Consumer Goods & Services domain\nCloud integration skills with platforms like AWS, Azure, or Google Cloud\nExperience with optimization techniques (exact and evolutionary)\nCertifications like AWS Certified Data Analytics Specialty or Google Professional Data Engineer\nFamiliarity with visualization tools like Tableau and Power BI\nExposure to large language models (e.g., ChatGPT, Llama 2)\nFamiliarity with version control systems like Git.\n\n\n\n\n\nAdditional Information:\nThe ideal candidate will have a strong educational background in data science, computer science, or a related field, along with a proven track record of delivering impactful AI-driven solutions in the Consumer Goods industry.\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience: Minimum 4-8 years of hands-on experience in data science with a focus on the Consumer Goods industry\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Economics, Mathematics, Computer Science, or equivalent degree with Data Science specialization (from a premier institute)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'artificial intelligence', 'deep learning', 'data science', 'ml', 'advanced analytics', 'cnn', 'data analytics', 'pyspark', 'microsoft azure', 'power bi', 'machine learning', 'sql', 'r', 'tableau', 'git', 'rnn', 'gcp', 'machine learning algorithms', 'aws', 'consumer goods', 'statistics']",2025-06-13 05:21:57
S&C Global Network - AI - CG&S - Consultant Data Science,Accenture,4 - 8 years,Not Disclosed,['Bengaluru'],"Job Title: Industry & Function AI Decision Science Consultant S&C Global Network\n\n\n\nManagement Level: 9 Consultant\n\n\n\nLocation: Primary - Bengaluru, Secondary - Gurugram\n\n\n\nMust-Have\n\n\n\n\nSkills:\nData Science, AI, ML, Experience with cloud platforms such as AWS, Azure, or Google Cloud, Hands-on experience in programming languages like Python, R, PySpark, and SQL\n\n\n\nGood-to-Have\n\n\n\n\nSkills:\nDeep Learning Techniques (e.g. RNN, CNN), Visualization tools like Power BI and Tableau, Exposure to tools like ChatGPT, Llama 2, Hugging Face, etc.\n\n\n\nJob\n\n\nSummary:\n\nAs an Industry & Function AI Decision Science Consultant, you will leverage your expertise in data science and Consumer Goods domain knowledge to design and deliver AI-driven solutions. Your role will include strategic analysis, project delivery, solution development, and technical execution to empower businesses with actionable insights and enable automated and augmented decision-making.\n\n\n\n\nRoles & Responsibilities:\nConduct strategic analysis of the AI, analytics, and data maturity landscape for clients in the Consumer Goods domain\nLead data science engagements, manage delivery teams, and build innovative AI capabilities\nDevelop and implement advanced analytics solutions tailored to client requirements\nUtilize languages like Python, PySpark, R, and SQL for data wrangling and machine learning model development\nLeverage cloud technologies (Azure, AWS, GCP) to integrate and implement AI solutions\nTranslate complex data into compelling narratives for effective data storytelling\nMentor junior team members and contribute to thought leadership\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\nProficiency in Python, R, PySpark, and SQL\nStrong knowledge of traditional statistical methods, machine learning techniques, and deep learning\nHands-on experience in Consumer Goods & Services domain\nCloud integration skills with platforms like AWS, Azure, or Google Cloud\nExperience with optimization techniques (exact and evolutionary)\nCertifications like AWS Certified Data Analytics Specialty or Google Professional Data Engineer\nFamiliarity with visualization tools like Tableau and Power BI\nExposure to large language models (e.g., ChatGPT, Llama 2)\nFamiliarity with version control systems like Git.\n\n\n\n\n\nAdditional Information:\nThe ideal candidate will have a strong educational background in data science, computer science, or a related field, along with a proven track record of delivering impactful AI-driven solutions in the Consumer Goods industry.\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience: Minimum 4-8 years of hands-on experience in data science with a focus on the Consumer Goods industry\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Economics, Mathematics, Computer Science, or equivalent degree with Data Science specialization (from a premier institute)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'artificial intelligence', 'deep learning', 'data science', 'ml', 'advanced analytics', 'cnn', 'data analytics', 'pyspark', 'microsoft azure', 'power bi', 'machine learning', 'sql', 'r', 'tableau', 'git', 'rnn', 'gcp', 'machine learning algorithms', 'aws', 'consumer goods', 'statistics']",2025-06-13 05:21:59
Data Scientist-Artificial Intelligence,IBM,3 - 7 years,Not Disclosed,['Bengaluru'],"As an Associate Data Scientist at IBM, you will work to solve business problems using leading edge and open-source tools such as Python, R, and TensorFlow, combined with IBM tools and our AI application suites. You will prepare, analyze, and understand data to deliver insight, predict emerging trends, and provide recommendations to stakeholders.\n\nIn your role, you may be responsible for\nImplementing and validating predictive and prescriptive models and creating and maintaining statistical models with a focus on big data & incorporating machine learning. techniques in your projects\nWriting programs to cleanse and integrate data in an efficient and reusable manner\nWorking in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors\nCommunicating with internal and external clients to understand and define business needs and appropriate modelling techniques to provide analytical solutions.\nEvaluating modelling results and communicating the results to technical and non-technical audiences\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nProof of Concept (POC) DevelopmentDevelop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions.\nCollaborate with development teams to implement and iterate on POCs, ensuring alignment with customer requirements and expectations.\nHelp in showcasing the ability of Gen AI code assistant to refactor/rewrite and document code from one language to another, particularly COBOL to JAVA through rapid prototypes/ PoC\nDocument solution architectures, design decisions, implementation details, and lessons learned.\nCreate technical documentation, white papers, and best practice guides\n\n\nPreferred technical and professional experience\nStrong programming skills, with proficiency in Python and experience with AI frameworks such as TensorFlow, PyTorch, Keras or Hugging Face.\nUnderstanding in the usage of libraries such as SciKit Learn, Pandas, Matplotlib, etc. Familiarity with cloud platforms\nExperience and working knowledge in COBOL & JAVA would be preferred",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'scikit-learn', 'tensorflow', 'pytorch', 'keras', 'natural language processing', 'neural networks', 'predictive', 'huggingface', 'machine learning', 'prototype', 'artificial intelligence', 'sql', 'pandas', 'deep learning', 'r', 'java', 'cobol', 'data science', 'matplotlib', 'big data', 'statistics']",2025-06-13 05:22:01
S&C GN - Data&AI - Retail - Consultant,Accenture,4 - 9 years,Not Disclosed,['Bengaluru'],"Job Title - Retail Specialized Data Scientist Level 9 SnC GN Data & AI\n\n\n\nManagement Level:09 - Consultant\n\n\n\nLocation:Bangalore / Gurgaon / Mumbai / Chennai / Pune / Hyderabad / Kolkata\n\n\n\nMust have skills:\nA solid understanding of retail industry dynamics, including key performance indicators (KPIs) such as sales trends, customer segmentation, inventory turnover, and promotions.\nStrong ability to communicate complex data insights to non-technical stakeholders, including senior management, marketing, and operational teams.\nMeticulous in ensuring data quality, accuracy, and consistency when handling large, complex datasets.\nGather and clean data from various retail sources, such as sales transactions, customer interactions, inventory management, website traffic, and marketing campaigns.\nStrong proficiency in Python for data manipulation, statistical analysis, and machine learning (libraries like Pandas, NumPy, Scikit-learn).\nExpertise in supervised and unsupervised learning algorithms\nUse advanced analytics to optimize pricing strategies based on market demand, competitor pricing, and customer price sensitivity.\n\n\n\n\nGood to have skills:\nFamiliarity with big data processing platforms like Apache Spark, Hadoop, or cloud-based platforms such as AWS or Google Cloud for large-scale data processing.\nExperience with ETL (Extract, Transform, Load) processes and tools like Apache Airflow to automate data workflows.\nFamiliarity with designing scalable and efficient data pipelines and architecture.\nExperience with tools like Tableau, Power BI, Matplotlib, and Seaborn to create meaningful visualizations that present data insights clearly.\n\n\nJob\n\n\nSummary: The Retail Specialized Data Scientist will play a pivotal role in utilizing advanced analytics, machine learning, and statistical modeling techniques to help our retail business make data-driven decisions. This individual will work closely with teams across marketing, product management, supply chain, and customer insights to drive business strategies and innovations. The ideal candidate should have experience in retail analytics and the ability to translate data into actionable insights.\n\n\n\n\nRoles & Responsibilities:\nLeverage Retail Knowledge:Utilize your deep understanding of the retail industry (merchandising, customer behavior, product lifecycle) to design AI solutions that address critical retail business needs.\nGather and clean data from various retail sources, such as sales transactions, customer interactions, inventory management, website traffic, and marketing campaigns.\nApply machine learning algorithms, such as classification, clustering, regression, and deep learning, to enhance predictive models.\nUse AI-driven techniques for personalization, demand forecasting, and fraud detection.\nUse advanced statistical methods help optimize existing use cases and build new products to serve new challenges and use cases.\nStay updated on the latest trends in data science and retail technology.\nCollaborate with executives, product managers, and marketing teams to translate insights into business actions.\n\n\n\n\nProfessional & Technical Skills:\nStrong analytical and statistical skills.\nExpertise in machine learning and AI.\nExperience with retail-specific datasets and KPIs.\nProficiency in data visualization and reporting tools.\nAbility to work with large datasets and complex data structures.\nStrong communication skills to interact with both technical and non-technical stakeholders.\nA solid understanding of the retail business and consumer behavior.\nProgramming Languages:Python, R, SQL, Scala\nData Analysis Tools:Pandas, NumPy, Scikit-learn, TensorFlow, Keras\nVisualization Tools:Tableau, Power BI, Matplotlib, Seaborn\nBig Data Technologies:Hadoop, Spark, AWS, Google Cloud\nDatabases:SQL, NoSQL (MongoDB, Cassandra)\n\n\n\n\nAdditional Information: -\n\nQualification\n\n\n\nExperience:Minimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification:Bachelors or Master's degree in Data Science, Statistics, Computer Science, Mathematics, or a related field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'artificial intelligence', 'data visualization', 'statistics', 'algorithms', 'data manipulation', 'scikit-learn', 'scala', 'numpy', 'unsupervised learning', 'sql', 'pandas', 'tensorflow', 'spark', 'consumer behavior', 'keras', 'hadoop', 'aws', 'reporting tools', 'retail business']",2025-06-13 05:22:04
Data Modeler,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Data Modeler\n\n\n\n\n\nProject Role Description :Work with key business representatives, data owners, end users, application designers and data architects to model current and new data.\n\n\n\nMust have skills :Microsoft Power Business Intelligence (BI)\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Modeler, you will engage with key business representatives, data owners, end users, application designers, and data architects to model both current and new data. Your typical day will involve collaborating with various stakeholders to understand their data needs, analyzing existing data structures, and designing effective data models that support business objectives. You will also be responsible for ensuring that the data models are aligned with best practices and meet the requirements of the organization, facilitating seamless data integration and accessibility across different platforms.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Facilitate training sessions for junior team members to enhance their understanding of data modeling.- Continuously evaluate and improve data modeling processes to ensure efficiency and effectiveness.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Microsoft Power Business Intelligence (BI).- Strong understanding of data modeling concepts and best practices.- Experience with data integration techniques and tools.- Familiarity with database management systems and SQL.- Ability to communicate complex data concepts to non-technical stakeholders.\nAdditional Information:- The candidate should have minimum 5 years of experience in Microsoft Power Business Intelligence (BI).- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['business intelligence', 'sql', 'database management', 'data modeling', 'data integration', '3d modeling', 'tekla structures', '3ds max', 'python', '3d modeler', 'oracle', 'texturing', 'bi', 'data warehousing', 'sme', 'photoshop', 'autocad', 'sql server', 'maya', 'plsql', 'modeler', 'etl', 'tekla', 'informatica']",2025-06-13 05:22:06
IN_Director_Senior Data Architect_Data & Analytics_Advisory_ Bangalore,PwC Service Delivery Center,12 - 18 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nData, Analytics & AI\nManagement Level\nDirector\n& Summary\n\n\n.\n& Summary We are seeking an experienced Senior Data Architect to lead the design and development of our data architecture, leveraging cloudbased technologies, big data processing frameworks, and DevOps practices. The ideal candidate will have a strong background in data warehousing, data pipelines, performance optimization, and collaboration with DevOps teams.\nResponsibilities\n1. Design and implement endtoend data pipelines using cloudbased services (AWS/ GCP/Azure) and conventional data processing frameworks.\n2. Lead the development of data architecture, ensuring scalability, security, and performance.\n3. Collaborate with crossfunctional teams, including DevOps, to design and implement data lakes, data warehouses, and data ingestion/extraction processes. 4. Develop and optimize data processing workflows using PySpark, Kafka, and other big data processing frameworks.\n5. Ensure data quality, integrity, and security across all data pipelines and architectures.\n6. Provide technical leadership and guidance to junior team members.\n7. Design and implement data load strategies, data partitioning, and data storage solutions.\n8. Collaborate with stakeholders to understand business requirements and develop data solutions to meet those needs.\n9. Work closely with DevOps team to ensure seamless integration of data pipelines with overall system architecture.\n10. Participate in design and implementation of CI/CD pipelines for data workflows.\nDevOps Requirements\n1. Knowledge of DevOps practices and tools, such as Jenkins, GitLab CI/CD, or Apache Airflow.\n2. Experience with containerization using Docker.\n3. Understanding of infrastructure as code (IaC) concepts using tools like Terraform or AWS CloudFormation.\n4. Familiarity with monitoring and logging tools, such as Prometheus, Grafana, or ELK Stack.\nRequirements\n1. 1214 years of experience for Senior Data Architect in data architecture, data warehousing, and big data processing.\n2. Strong expertise in cloudbased technologies (AWS/ GCP/ Azure) and data processing frameworks (PySpark, Kafka, Flink , Beam etc.).\n3. Experience with data ingestion, data extraction, data warehousing, and data lakes.\n4. Strong understanding of performance optimization, data partitioning, and data storage solutions.\n5. Excellent leadership and communication skills.\n6. Experience with NoSQL databases is a plus.\nMandatory skill sets\n1. Experience with agile development methodologies.\n2. Certification in cloudbased technologies (AWS / GCP/ Azure) or data processing frameworks.\n3. Experience with data governance, data quality, and data security.\nPreferred skill sets\nKnowledge of AgenticAI and GenAI is added advantage\nYears of experience required\n12 to 18 years\nEducation qualification\nGraduate Engineer or Management Graduate\nEducation\nDegrees/Field of Study required Bachelor of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nAWS Devops\nAccepting Feedback, Accepting Feedback, Active Listening, Analytical Reasoning, Analytical Thinking, Application Software, Business Data Analytics, Business Management, Business Technology, Business Transformation, Coaching and Feedback, Communication, Creativity, Documentation Development, Embracing Change, Emotional Regulation, Empathy, Implementation Research, Implementation Support, Implementing Technology, Inclusion, Influence, Innovation, Intellectual Curiosity, Learning Agility {+ 28 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['System architecture', 'Business transformation', 'GCP', 'Analytical', 'Consulting', 'Data processing', 'Data quality', 'Operations', 'Monitoring', 'Data architecture']",2025-06-13 05:22:07
"STAFF, DATA SCIENTIST",Walmart,5 - 10 years,Not Disclosed,['Bengaluru'],"Position Summary...\nWhat youll do...\nAbout Team\nThe Catalog Data Science Team at Walmart Global Tech is focused on using the latest research in generative AI (GenAI), artificial intelligence (AI), machine learning (ML), statistics, deep learning, computer vision and optimization to implement solutions that ensure Walmart s product catalog is accurate, complete, and optimized for customer experience. Our team tackles complex data science and ML engineering challenges related to product classification, attribute extraction, trust & safety, and catalog optimization, empowering next-generation retail use cases.\nThe Data Science and ML Engineering community at Walmart Global Tech is active in most of the Hack events, utilizing the petabytes of data at our disposal, to build some of the coolest ideas. All the work we do at Walmart Global Tech will eventually benefit our operations & our associates, helping Customers Save Money to Live Better.\nWhat youll do:\nWe are looking for a Staff Machine Learning Engineer who can help build large scale AI/ML/Optimization products. Expected qualities include ability to build, deploy, maintain and troubleshoot large scale systems.\nAs a Staff ML Engineer, you ll have the opportunity to\nDrive research initiatives and proof-of-concepts that push the state of the art in generative AI and large-scale machine learning.\nDesign and implement high-throughput, low-latency AI/ML pipelines and microservices that operate at global scale.\nOversee data ingestion, model training, evaluation, deployment and monitoring-ensuring performance, quality and reliability.\nCustomize and optimize LLMs for specific business use cases, balancing accuracy, latency and cost.\nPrototype novel generative AI solutions, integrate advancements into production, and collaborate with research partners.\nChampion best practices in data quality, lineage, governance and cost optimization across ML pipelines.\nMentor a team of ML engineers, establish coding standards, conduct design reviews, and foster a culture of continuous improvement.\nPresent your team s work at top-tier AI/ML conferences, publish scientific papers, and cultivate partnerships with universities and research labs.\nWhat youll bring:\nPhD in Computer Science, Statistics, Applied Mathematics or related field with 5+ years experience in ML engineering-or Master s with 8+ years or Bachelor s with 10+ years.\nProven track record of leading and scaling AI/ML products in production environments.\nDeep expertise in generative AI, large-scale model deployment, and fine-tuning of transformer-based architectures.\nStrong programming skills in Python, or equivalent, and experience with big data frameworks (Spark, Hadoop) and ML platforms (TensorFlow, PyTorch).\nDemonstrated history of scientific publications or patents in AI/ML.\nExcellent communication skills, a growth mindset, and the ability to drive cross-functional collaboration.\nAbout Walmart Global Tech\n.\n.\nFlexible, hybrid work\n.\nBenefits\n.\nBelonging\n.\n.\nEqual Opportunity Employer\nWalmart, Inc., is an Equal Opportunities Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and\nMinimum Qualifications...\nMinimum Qualifications:Option 1: Bachelors degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 4 years experience in an analytics related field. Option 2: Masters degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 2 years experience in an analytics related field. Option 3: 6 years experience in an analytics or related field.\nPreferred Qualifications...\nPrimary Location...",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Prototype', 'Networking', 'Coding', 'Machine learning', 'Continuous improvement', 'Information technology', 'Monitoring', 'Analytics', 'Python']",2025-06-13 05:22:09
AVP Data Management Analyst,Wells Fargo,2 - 7 years,Not Disclosed,['Bengaluru'],"About this role:\nWells Fargo is seeking a Data Management Analyst\n\nIn this role, you will:\nParticipate in less complex analysis to identify and remediate data quality or integrity issues and to identify and remediate process or control gaps\nAdhere to data governance standards and procedures",,,,"['Data Management', 'Agile Methodology', 'Funds Transfer Pricing', 'Financial Data Mapping', 'Big Data Query Techniques', 'Lineage Tracing', 'Data warehousing', 'Data Governance', 'Jira', 'Market Risks', 'SQL']",2025-06-13 05:22:11
Advanced Data Science Associate,ZS,0 - 2 years,Not Disclosed,['Bengaluru'],"Develop advanced and efficient statistically effective algorithms that solve problems of high dimensionality .\nUtilize technical skills such as hypothesis testing, machine learning and retrieval processes to apply statistical and data mining techniques to identify trends, create figures, and analyze other relevant information.\nCollaborate with clients and other stakeholders at ZS to integrate and effectively communicate analysis findings.\nContribute to the assessment of emerging datasets and technologies that impact our analytical",,,,"['Text mining', 'Analytical', 'Management consulting', 'Financial planning', 'Machine learning', 'Hypothesis Testing', 'Predictive modeling', 'Data mining', 'big data']",2025-06-13 05:22:13
Data Analyst - Gurugram,Infosys,5 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru', 'PAN INDIA']","Responsibilities:\nUnderstand architecture requirements and ensure effective design, development, validation, and support activities.\nAnalyze user requirements, envisioning system features and functionality.\nIdentify bottlenecks and bugs, and recommend system solutions by comparing advantages and disadvantages of custom development.\nContribute to team meetings, troubleshooting development and production problems across multiple environments and operating platforms.\nEnsure effective design, development, validation, and support activities for Big Data solutions.\nTechnical and Professional Requirements:\nSkills:\nProficiency in Scala, Spark, Hive, and Kafka.\nIn-depth knowledge of design issues and best practices.\nSolid understanding of object-oriented programming.\nFamiliarity with various design, architectural patterns, and software development processes.\nExperience with both external and embedded databases.\nCreating database schemas that represent and support business processes.\nImplementing automated testing platforms and unit tests.\nPreferred Skills:\nTechnology -> Big Data -> Scala, Spark, Hive, Kafka\nAdditional Responsibilities:\nCompetencies:\nGood verbal and written communication skills.\nAbility to communicate with remote teams effectively.\nHigh flexibility to travel.\nEducational Requirements:Master of Computer Applications, Master of Technology, Master of Engineering, MSc, Bachelor of Technology, Bachelor of Computer Applications, Bachelor of Computer Science, Bachelor of Engineering",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Spark', 'Hive', 'Hadoop', 'Big Data', 'Kafka']",2025-06-13 05:22:15
Big data Developer,Diverse Lynx,3 - 5 years,Not Disclosed,['Bengaluru'],"Total Yrs. of Experience 8+ Yrs Relevant Yrs. of experience 4+ Yrs Detailed JD (Roles and Responsibilities)\nGather operational Client on business processes and policies from multiple sources\nPrepare periodical and ad-hoc reports using operational data\nDevelop semantic core to align data with business processes\nSupport operations teams work streams for data processing, analysis and reporting\nAnalyse data and Create dashboards for the senior management\nDesign and implement optimal processes\nRegression testing of the releases\nMandatory skills\nBig Data : Spark, Hive, DataBricks\nLanguage : SQL, JAVA /Python\nBI Analytics: Power BI (DAX), Tableau , Dataiku\nOperating System : Unix\nExperience with Data Migration, Data Engineering, Data Analysis\nDesired/ Secondary skills\nBig Data : SCALA, HADOOP\nTools: DB Visualizer, JIRA, GIT, Bitbucket, Control-M\nStrong problem-solving skills and the ability to work independently and in a team environment.\nExcellent communication skills and the ability to work effectively with cross-functional teams.\nDomain eCommerce / Retail Max Vendor Rate in Per Day (Currency in relevance to work location) 11000 INR/Day Delivery Anchor for tracking the sourcing statistics, technical evaluation, interviews and feedback etc. Prem_dason@infosys.com Work Location given in ECMS ID Bangalore (Hyd, Pune, Trivandrum locations also ok)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Data analysis', 'Data migration', 'Control-M', 'Data processing', 'Regression testing', 'Operations', 'Analytics', 'SQL', 'Python']",2025-06-13 05:22:17
"ETL Developer ( SSIS, Informatica ,Talend, Big Data)Group Manager",Vuram,5 - 10 years,Not Disclosed,['Bengaluru'],"Design, develop, and maintain relational and non-relational database systems.\nDefine analytical architecture including datalakes, lakehouse, data mesh and medallion patterns\nAbility to understand & analyze business requirements and translate them into analytical or relational database designAble to design for non-SQL datastores\nOptimize SQL queries, stored procedures, and database performance.Create and maintain ETL processes for data integration from various sources.\nWork closely with application teams to design database schemas and support integration.Monitor, troubleshoot, and resolve database issues related to performance, storage, and replication.Implement data security, backup, recovery, and disaster recovery procedures.\nEnsure data integrity and enforce best practices in database development.\nParticipate in code reviews and mentor junior developers.Collaborate with business and analytics teams for reporting and data warehousing needs.Must Have: Strong expertise in SQL and PL/SQL\nHands-on experience with at least one RDBMS: SQL Server, Oracle, PostgreSQL, or MySQL\nExperience with NoSQL databases: MongoDB, Cassandra, or Redis (at least one)\nETL Development Tools: SSIS, Informatica, Talend, or equivalent\nExperience in performance tuning and optimization\nDatabase design and modeling tools: Erwin, dbForge, or similar\nCloud platforms: Experience with AWS RDS, Azure SQL, or Google Cloud SQL\nBackup and Recovery Management, High Availability, and Disaster Recovery Planning\nUnderstanding of indexing, partitioning, replication, and sharding\nKnowledge of CI/CD pipelines and DevOps practices for database deployments\nExperience with Big Data technologies (Hadoop, Spark)Experience working in Agile/Scrum environments\n\n\nQualifications\nBachelor s or Master s degree in Computer Science, Information Technology, or a related field.8+ years of relevant experience in database design and development",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'RDBMS', 'Database design', 'MySQL', 'PLSQL', 'Informatica', 'Stored procedures', 'Oracle', 'SSIS', 'SQL']",2025-06-13 05:22:18
Analyst - Data Analytics,AMERICAN EXPRESS,0 - 4 years,Not Disclosed,['Gurugram'],The American Express Enterprise Digital Experimentation & Analytics (EDEA) leads the Enterprise Product Analytics and Experimentation charter for Brand & Performance Marketing and Digital Acquisition & Membership experiences as we'll as Enterprise Platforms. The focus of this collaborative team is to drive growth by enabling efficiencies in paid performance channels & evolve our digital experiences with actionable insights & analytics. The team specializes in using data around digital product usage to drive improvements in the acquisition customer experience to deliver higher satisfaction and business value.\n,,,,"['Mining', 'Career development', 'Finance', 'Analytical', 'Data processing', 'Analytics', 'SQL']",2025-06-13 05:22:20
Senior Data Scientist | Snowflakes | Tableau | AI/ML,Cisco,0 - 2 years,Not Disclosed,['Bengaluru'],"Job posting may be removed earlier if the position is filled or if a sufficient number of applications are received.\n\nMeet the Team\n\nWe are a dynamic and innovative team of Data Engineers, Data Architects, and Data Scientists based in Bangalore, India. Our mission is to harness the power of data to provide actionable insights that empower executives to make informed, data-driven decisions. By analyzing and interpreting complex datasets, we enable the organization to understand the health of the business and identify opportunities for growth and improvement.\n\nYour Impact\n\nWe are seeking a highly experienced and skilled Senior Data Scientist to join our dynamic team. The ideal candidate will possess deep expertise in machine learning models, artificial intelligence (AI), generative AI, and data visualization. Proficiency in Tableau and other visualization tools is essential. This role requires hands-on experience with databases such as Snowflake and Teradata, as well as advanced knowledge in various data science and AI techniques. The successful candidate will play a pivotal role in driving data-driven decision-making and innovation within our organization.\n\nKey Responsibilities\nDesign, develop, and implement advanced machine learning models to solve complex business problems.\nApply AI techniques and generative AI models to enhance data analysis and predictive capabilities.\nUtilize Tableau and other visualization tools to create insightful and actionable dashboards for stakeholders.\nManage and optimize large datasets using Snowflake and Teradata databases.\nCollaborate with cross-functional teams to understand business needs and translate them into analytical solutions.\nStay updated with the latest advancements in data science, machine learning, and AI technologies.\nMentor and guide junior data scientists, fostering a culture of continuous learning and development.\nCommunicate complex analytical concepts and results to non-technical stakeholders effectively.\nKey Technologies &\n\nSkills:\nMachine Learning ModelsSupervised learning, unsupervised learning, reinforcement learning, deep learning, neural networks, decision trees, random forests, support vector machines (SVM), clustering algorithms, etc.\nAI TechniquesNatural language processing (NLP), computer vision, generative adversarial networks (GANs), transfer learning, etc.\nVisualization ToolsTableau, Power BI, Matplotlib, Seaborn, Plotly, etc.\nDatabasesSnowflake, Teradata, SQL, NoSQL databases.\nProgramming LanguagesPython (essential), R, SQL.\nPython LibrariesTensorFlow, PyTorch, scikit-learn, pandas, NumPy, Keras, SciPy, etc.\nData ProcessingETL processes, data warehousing, data lakes.\nCloud PlatformsAWS, Azure, Google Cloud Platform.\nMinimum Qualifications\nBachelor's or Master's degree in Computer Science, Statistics, Mathematics, Data Science, or a related field.\nMinimum of [X] years of experience as a Data Scientist or in a similar role.\nProven track record in developing and deploying machine learning models and AI solutions.\nStrong expertise in data visualization tools, particularly Tableau.\nExtensive experience with Snowflake and Teradata databases.\nExcellent problem-solving skills and the ability to work independently and collaboratively.\nExceptional communication skills with the ability to convey complex information clearly.\nPreferred Qualifications (Provide up to five (5) bullet points these can include soft skills)\nExcellent communication and collaboration skills to work effectively in cross-functional teams.\nAbility to translate business requirements into technical solutions.\nStrong problem-solving skills and the ability to work with complex datasets.\nExperience in statistical analysis and machine learning techniques.\nUnderstanding of business domains such as sales, financials, marketing, and telemetry.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['machine learning', 'artificial intelligence', 'sql', 'tableau', 'data visualization', 'snowflake', 'scipy', 'python', 'scikit-learn', 'data warehousing', 'numpy', 'pandas', 'tensorflow', 'data integration tools', 'matplotlib', 'pytorch', 'keras', 'machine learning algorithms', 'etl', 'nosql databases']",2025-06-13 05:22:22
DE&A - AIML - Data Science - Data Science (Other) DE&A - AIML,Zensar,15 - 18 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Experience: 15+ years overall | Minimum 10 full-cycle AI/ML project implementations , including GenAI experience\nRole Summary:\nWe are seeking a AI Architect to lead strategic AI transformation initiatives. This role demands deep hands-on experience in AI, Machine Learning (ML), and Generative AI (GenAI) , along with the ability to engage directly with C-level stakeholders , align technical delivery with business objectives, and drive enterprise-wide adoption of advanced AI solutions.\nThe ideal candidate is a techno-strategic leader who can take AI/ML/GenAI projects from ideation to production building architectures, leading cross-functional teams, and ensuring regulatory and operational alignment in BFSI environments.\nKey\nConsulting & Business Alignment\nPartner with senior business and IT leadership , including CIOs, CDOs, and COOs , to identify high-impact use cases across retail banking, insurance, credit, and capital markets.\nTranslate complex BFSI challenges into technically feasible and scalable AI/ML/GenAI solutions.\nCreate strategic roadmaps, capability assessments, and PoV/PoC execution plans that align with business KPIs and regulatory needs.\nSolution Architecture & Delivery Leadership\nDesign and lead delivery of AI/ML/GenAI pipelines covering data ingestion, model training, validation, deployment, and monitoring.\nBuild and scale GenAI-based solutions like LLM-driven chatbots, intelligent document processing, RAG pipelines, summarization tools , and virtual assistants.\nArchitect cloud-native AI platforms using AWS (SageMaker, Bedrock) , Azure (ML, OpenAI) , or GCP (Vertex AI, BigQuery, LangChain) .\nDefine and implement MLOps and LLMOps frameworks for versioning, retraining, CI/CD, and production observability.\nEnsure adherence to Responsible AI principles , including explainability, bias mitigation, auditability, and regulatory compliance\nEngineering & Integration\nWork closely with data engineering teams to acquire, transform, and pipeline data from core banking systems, CRMs, claims systems, and real-time feeds.\nDesign architecture for data lakes, feature stores, and vector databases supporting AI and GenAI use cases.\nEnable seamless integration of AI capabilities into enterprise workflows, customer platforms, and decision engines via APIs and microservices.\nRequired Skills & Experience:\n15+ years of experience in AI/ML, data engineering, and cloud architecture.\nMinimum of 10 end-to-end AI/ML project implementations from use case discovery through to productionization.\nProven expertise in: (Any One)\nAI/ML frameworks : scikit-learn, XGBoost, TensorFlow, PyTorch\nGenAI/LLM platforms : OpenAI, Cohere, Mistral, LangChain, Hugging Face, vector DBs (Pinecone, FAISS, Chroma)\nCloud platforms : AWS, Azure, GCP - including AI/ML & GenAI native services\nMLOps/LLMOps tools : MLflow, Kubeflow, SageMaker Pipelines, Vertex AI Pipelines\nStrong experience with data security, governance, model risk management , and AI compliance frameworks relevant to BFSI.\nAbility to lead large cross-functional teams and engage both technical teams and senior stakeholders.\nExperience: 15+ years overall | Minimum 10 full-cycle AI/ML project implementations , including GenAI experience\nRole Summary:\nWe are seeking a AI Architect to lead strategic AI transformation initiatives. This role demands deep hands-on experience in AI, Machine Learning (ML), and Generative AI (GenAI) , along with the ability to engage directly with C-level stakeholders , align technical delivery with business objectives, and drive enterprise-wide adoption of advanced AI solutions.\nThe ideal candidate is a techno-strategic leader who can take AI/ML/GenAI projects from ideation to production building architectures, leading cross-functional teams, and ensuring regulatory and operational alignment in BFSI environments.\nKey\nConsulting & Business Alignment\nPartner with senior business and IT leadership , including CIOs, CDOs, and COOs , to identify high-impact use cases across retail banking, insurance, credit, and capital markets.\nTranslate complex BFSI challenges into technically feasible and scalable AI/ML/GenAI solutions.\nCreate strategic roadmaps, capability assessments, and PoV/PoC execution plans that align with business KPIs and regulatory needs.\nSolution Architecture & Delivery Leadership\nDesign and lead delivery of AI/ML/GenAI pipelines covering data ingestion, model training, validation, deployment, and monitoring.\nBuild and scale GenAI-based solutions like LLM-driven chatbots, intelligent document processing, RAG pipelines, summarization tools , and virtual assistants.\nArchitect cloud-native AI platforms using AWS (SageMaker, Bedrock) , Azure (ML, OpenAI) , or GCP (Vertex AI, BigQuery, LangChain) .\nDefine and implement MLOps and LLMOps frameworks for versioning, retraining, CI/CD, and production observability.\nEnsure adherence to Responsible AI principles , including explainability, bias mitigation, auditability, and regulatory compliance\nEngineering & Integration\nWork closely with data engineering teams to acquire, transform, and pipeline data from core banking systems, CRMs, claims systems, and real-time feeds.\nDesign architecture for data lakes, feature stores, and vector databases supporting AI and GenAI use cases.\nEnable seamless integration of AI capabilities into enterprise workflows, customer platforms, and decision engines via APIs and microservices.\nRequired Skills & Experience:\n15+ years of experience in AI/ML, data engineering, and cloud architecture.\nMinimum of 10 end-to-end AI/ML project implementations from use case discovery through to productionization.\nProven expertise in: (Any One)\nAI/ML frameworks : scikit-learn, XGBoost, TensorFlow, PyTorch\nGenAI/LLM platforms : OpenAI, Cohere, Mistral, LangChain, Hugging Face, vector DBs (Pinecone, FAISS, Chroma)\nCloud platforms : AWS, Azure, GCP - including AI/ML & GenAI native services\nMLOps/LLMOps tools : MLflow, Kubeflow, SageMaker Pipelines, Vertex AI Pipelines\nStrong experience with data security, governance, model risk management , and AI compliance frameworks relevant to BFSI.\nAbility to lead large cross-functional teams and engage both technical teams and senior stakeholders.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Solution architecture', 'Architect', 'Bfsi', 'data security', 'Consulting', 'Machine learning', 'Risk management', 'Operations', 'Monitoring', 'Core banking']",2025-06-13 05:22:24
Data Scientist,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nThe Data Scientist is responsible for developing and implementing AI-driven solutions to enhance cybersecurity measures within the organization. This role involves leveraging data science techniques to analyze security data, detect threats, and automate security processes. The Data Scientist will work closely with cybersecurity teams to identify data-driven automation opportunities, strengthening the organizations security posture.\nRoles & Responsibilities:",,,,"['data science', 'R', 'PyTorch', 'SAS', 'predictive analytics', 'Scikit-learn', 'SPSS', 'machine learning', 'data engineering', 'Python', 'TensorFlow']",2025-06-13 05:22:26
Data & Gen AI Specialist,Altimetrik,1 - 4 years,Not Disclosed,['Bengaluru'],"Job Title: Data & GenAI AWS Specialist\nExperience: 1-4 Years\nLocation: Bangalore\nMandatory Qualification: B.E./ B.Tech/ M.Tech/ MS from IIT or IISc ONLY\nJob Overview:\nWe are seeking a seasoned Data & GenAI Specialist with deep expertise in AWS Managed Services (PaaS) to join our innovative team. The ideal candidate will have extensive experience in designing sophisticated, scalable architectures for data pipelines and Generative AI (GenAI) solutions leveraging cloud services.",,,,"['Generative Ai', 'Cloud', 'Data Science', 'Open Source', 'Data Pipeline', 'GCP', 'Azure Cloud', 'Snowflake', 'Machine Learning', 'AWS']",2025-06-13 05:22:28
Data Scientist - Python / Machine Learning,Blueberry Unicorn Services,6 - 11 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","Working Hours : 2PM to 11PM IST\n\nMid-Level ML Engineers / Data Scientist Role : (4-5 years of experience )\n\n- Experience processing, filtering, and presenting large quantities (100K to Millions of rows) of data using Pandas and PySpark\n\n- Experience with statistical analysis, data modeling, machine learning, optimizations, regression modeling and forecasting, time series analysis, data mining, and demand modeling.\n\n- Experience applying various machine learning techniques and understanding the key parameters that affect their performance.\n\n- Experience with Predictive analytics (e.g., forecasting, time-series, neural networks) and Prescriptive analytics (e.g., stochastic optimization, bandits, reinforcement learning).\n\n- Experience with Python and Python packages like NumPy, Pandas and deep learning frameworks like TensorFlow, Pytorch and Keras\n\n- Experience in Big Data ecosystem with frameworks like Spark, PySpark , Unstructured DBs like Elasticsearch and MongoDB\n\n- Proficiency with TABLEAU or other web-based interfaces to create graphic-rich customizable plots, charts data maps etc.\n\n- Able to write SQL scripts for analysis and reporting (Redshift, SQL, MySQL).\n\n- Previous experience in ML, data scientist or optimization engineer role with a large technology company.\n\n- Experience in an operational environment developing, fast-prototyping, piloting, and launching analytic products.\n\n- Ability to develop experimental and analytic plans for data modeling processes, use of strong baselines, ability to accurately determine cause and effect relations.\n\n- Experience in creating data driven visualizations to describe an end-to-end system.\n\n- Excellent written and verbal communication skills. The role requires effective communication with colleagues from computer science, operations research, and business backgrounds.\n\n- Bachelors or Masters in Artificial Intelligence, Computer Science, Statistics, Applied Math, or a related field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Data Science', 'Data Scientist', 'Artificial Intelligence', 'Data Management', 'Big Data', 'Data Modeling', 'Spark', 'Numpy', 'Python', 'Predictive Analytics']",2025-06-13 05:22:29
Data Science Manager,ZS,10 - 15 years,Not Disclosed,"['Pune', 'Bengaluru']","A key enabler of our services is leveraging data in delivering client solutions. The data available about customers is getting richer and the problems that our customers are trying to answer continue to evolve. In our endeavor to stay ahead in providing solutions to these evolving complex problems, ZS has set up an Advanced Data Science which has three major focus areas:\nResearch the evolving datasets and advanced analytical techniques to develop new offerings/solutions\nDeliver client impact by collaboratively implementing these solutions",,,,"['Team management', 'data science', 'Pharma', 'Analytical', 'Management consulting', 'Financial planning', 'Healthcare', 'Project planning', 'Predictive modeling', 'Financial services']",2025-06-13 05:22:31
Enterprise Data Operations Manager,Pepsico,12 - 17 years,Not Disclosed,['Hyderabad'],"Overview\n\nDeputy Director - Data Engineering\n\nPepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT. The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCos global business scale to enable business insights, advanced analytics, and new product development. PepsiCos Data Management and Operations team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations, and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.",,,,"['Data Engineering', 'Pyspark', 'Azure', 'Power BI', 'Github', 'Azure Databricks', 'Tableau', 'ADO', 'Scala programming', 'SQL', 'Azure Data Factory', 'Azure Machine learning', 'Data Lakehouse', 'Azure Data Engineering', 'CI/CD', 'Data Warehousing', 'Data Analytics', 'AWS', 'Python']",2025-06-13 05:22:33
ETL Developer - Data & Analytics,Canpack India,3 - 5 years,Not Disclosed,['Pune'],"Giorgi Global Holdings, Inc. ( GGH ) is a privately held, diversified consumer products/packaging company with approximately 11,000 employees and operations in 20 countries. GGH consists of four US based companies ( The Giorgi Companies ) and one global packaging company ( CANPACK ).\nGGH has embarked on a transformation journey to become a digital, technology enabled, customer-centric, data and insights-driven organization. This transformation is evolving our business, strategy, core operations and IT solutions.\nAs an ETL Developer, you will be an integral part of our Data and Analytics team, working closely with the ETL Architect and other developers to design, develop, and maintain efficient data integration and transformation solutions. We are looking for a highly skilled ETL Developer with a deep understanding of ETL processes and data warehousing. The ideal candidate is passionate about optimizing data extraction, transformation, and loading workflows, ensuring high performance, accuracy, and scalability to support business intelligence initiatives.\nWhat you will do:\n1. Design, develop, test and maintain ETL processes and data pipelines to support data integration and transformation needs.\n2. Continuously improve ETL performance and reliability through best practices and optimization techniques.\n3. Develop and implement data validation and quality checks to ensure the integrity and consistency of data.\n4. Collaborate with ETL Architect, Data Engineers, and Business Intelligence teams to understand business requirements and translate them into technical solutions.\n5. Monitor, troubleshoot, and resolve ETL job failures, performance bottlenecks, and data discrepancies.\n6. Proactively identify and resolve ETL-related issues, minimizing impact on business operations.\n7. Contribute to documentation, training, and knowledge sharing to enhance team capabilities.\n8. Communicate progress and challenges clearly to both technical and non-technical teams\nEssential Requirements:\nBachelor s or master s degree in information technology, Computer Science, or a related field.\n3-5 years of relevant experience.\nPower-BI, Tabular Editor/Dax Studio, ALM/Github/Azure Devops skills\nExposure to SAP Systems/Modules like SD, MD, etc. to understand functional data.,\nExposure to MS Fbric, MS Azure Synapse Analytics\nCompetencies needed:\n- Hands-on experience with ETL development and data integration for large-scale systems\n- Experience with platforms such as Synapse Analytics, Azure Data Factory, Fabric, Redshift or Databricks\n- A solid understanding of data warehousing and ETL processes\n- Advanced SQL and PL/SQL skills such as query optimization, complex joins, window functions\n- Expertise in Python (pySpark) programming with a focus on data manipulation and analysis\n- Experience with Azure DevOps and CI/CD process\n- Excellent problem-solving and analytical skills\n- Experience in creating post-implementation documentation\n- Strong team collaboration skills\n- Attention to detail and a commitment to quality\nStrong interpersonal skills including analytical thinking, creativity, organizational abilities, high commitment, initiative in task execution, and a fast-learning capability for understanding IT concepts\n\nIf you are a current CANPACK employee, please apply through your Workday account .\nCANPACK Group is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, colour, religion, age, sex, sexual orientation, gender identity, national origin, disability, or any other characteristic protected by law or not related to job requirements, unless such distinction is required by law.",Industry Type: Packaging & Containers,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'SAP', 'Analytical', 'Packaging', 'PLSQL', 'Business intelligence', 'Information technology', 'Analytics', 'Python', 'Data extraction']",2025-06-13 05:22:34
Data Product Owner,Capgemini,9 - 14 years,Not Disclosed,['Hyderabad'],"\n\nThe Product Owner III will be responsible for defining and prioritizing features and user stories, outlining acceptance criteria, and collaborating with cross-functional teams to ensure successful delivery of product increments. This role requires strong communication skills to effectively engage with stakeholders, gather requirements, and facilitate product demos.\n\nThe ideal candidate should have a deep understanding of agile methodologies, experience in the insurance sector, and possess the ability to translate complex needs into actionable tasks for the development team.\n\n Key Responsibilities: \nDefine and communicate the  vision, roadmap, and backlog  for data products.\nManages team backlog items and prioritizes based on business value.\nPartners with the business owner to understand needs, manage scope and add/eliminate user stories while contributing heavy influence to build an effective strategy.\nTranslate business requirements into scalable data product features.\nCollaborate with data engineers, analysts, and business stakeholders to prioritize and deliver impactful solutions.\nChampion  data governance , privacy, and compliance best practices.\nAct as the voice of the customer to ensure usability and adoption of data products.\nLead Agile ceremonies (e.g., backlog grooming, sprint planning, demos) and maintain a clear product backlog.\nMonitor data product performance and continuously identify areas for improvement.\nSupport the integration of AI/ML solutions and advanced analytics into product offerings.\n\n\n  \n\n Required Skills & Experience: \nProven experience as a Product Owner, ideally in data or analytics domains.\nStrong understanding of  data engineering ,  data architecture , and  cloud platforms  (AWS, Azure, GCP).\nFamiliarity with  SQL , data modeling, and modern data stack tools (e.g., Snowflake, dbt, Airflow).\nExcellent  stakeholder management  and communication skills across technical and non-technical teams.\nStrong  business acumen  and ability to align data products with strategic goals.\nExperience with  Agile/Scrum methodologies  and working in cross-functional teams.\nAbility to  translate data insights into compelling stories and recommendations .\n\n\n",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data architecture', 'business acumen', 'data engineering', 'stakeholder management', 'agile methodology', 'snowflake', 'advanced analytics', 'microsoft azure', 'cloud platforms', 'user stories', 'demo', 'sql', 'data modeling', 'gcp', 'sprint planning', 'scrum', 'product performance', 'agile', 'aws', 'ml']",2025-06-13 05:22:36
S&C Global Network - AI - Life Sciences -Data Science Consultant,Accenture,4 - 9 years,Not Disclosed,['Gurugram'],"Job Title -\n\n\n\nS&C Global Network - AI - Healthcare Analytics - Consultant\n\n\n\nManagement Level:\n\n\n\n9-Team Lead/Consultant\n\n\n\nLocation:\n\n\n\nBangalore/Gurgaon\n\n\n\nMust-have skills:R,Phython,SQL,Spark,Tableau ,Power BI\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\nWHATS IN IT FOR YOU\nAn opportunity to work on high-visibility projects with top Pharma clients around the globe.\nPotential to Co-create with leaders in strategy, industry experts, enterprise function practitioners, and business intelligence professionals to shape and recommend innovative solutions that leverage emerging technologies.\nAbility to embed responsible business into everythingfrom how you service your clients to how you operate as a responsible professional.\nPersonalized training modules to develop your strategy & consulting acumen to grow your skills, industry knowledge, and capabilities.\nOpportunity to thrive in a culture that is committed to accelerating equality for all. Engage in boundaryless collaboration across the entire organization.\n\n\n\n\nWhat you would do in this role\nSupport delivery of small to medium-sized teams to deliver consulting projects for global clients.\nResponsibilities may include strategy, implementation, process design, and change management for specific modules.\nWork with the team or as an Individual contributor on the project assigned which includes a variety of skills to be utilized from Data Engineering to Data Science\nProvide Subject matter expertise in various sub-segments of the LS industry.\nDevelop assets and methodologies, point-of-view, research, or white papers for use by the team and the larger community.\nAcquire new skills that have utility across industry groups.\nSupport strategies and operating models focused on some business units and assess likely competitive responses. Also, assess implementation readiness and points of greatest impact.\nCo-lead proposals, and business development efforts and coordinate with other colleagues to create consensus-driven deliverables.\nExecute a transformational change plan aligned with the clients business strategy and context for change. Engage stakeholders in the change journey and build commitment to change.\nMake presentations wherever required to a known audience or client on functional aspects of his or her domain.\n\n\n\nWho are we looking for\nBachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.\nProven experience (4+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\nExcellent understanding of Pharma data sets commercial, clinical, RWE (Real World Evidence) & EMR (Electronic medical records)\nLeverage ones hands on experience of working across one or more of these areas such as real-world evidence data, R&D clinical data, digital marketing data.\nHands-on experience with handling Datasets like Komodo, RAVE, IQVIA, Truven, Optum etc.\nHands-on experience in building and deployment of Statistical Models/Machine Learning including Segmentation & predictive modeling, hypothesis testing, multivariate statistical analysis, time series techniques, and optimization.\nProficiency in Programming languages such as R, Python, SQL, Spark, etc.\nAbility to work with large data sets and present findings/insights to key stakeholders; Data management using databases like SQL.\nExperience with any of the cloud platforms like AWS, Azure, or Google Cloud for deploying and scaling language models.\nExperience with any of the Data Visualization tools like Tableau, Power BI, Qlikview, Spotfire is good to have.\nExcellent analytical and problem-solving skills, with a data-driven mindset.\nProficient in Excel, MS Word, PowerPoint, etc.\nAbility to solve complex business problems and deliver client delight.\nStrong writing skills to build points of view on current industry trends.\nGood communication, interpersonal, and presentation skills\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | Accenture\nQualification\n\n\n\nExperience:\n\n\n\n4-8 Years\n\n\n\n\nEducational Qualification:\n\n\n\nBachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'sql', 'tableau', 'r', 'spark', 'spotfire', 'power bi', 'microsoft azure', 'time series', 'emr', 'machine learning', 'data engineering', 'artificial intelligence', 'qlikview', 'data science', 'gcp', 'predictive modeling', 'segmentation', 'life sciences', 'data visualization', 'aws', 'statistics']",2025-06-13 05:22:38
S&C GN - Data&AI - Life Sciences - Consultant,Accenture,4 - 9 years,Not Disclosed,['Gurugram'],"Management Level:Ind&Func AI Decision Science Consultant\n\n\n\n\nJob Location:Bangalore / Gurgaon\n\n\n\nMust-have\n\n\n\n\nSkills:\nExcellent understanding of Pharma data sets commercial, clinical, Leverage ones hands on experience of working across one or more of these areas such as real-world evidence data, Statistical Models/Machine Learning including Segmentation & predictive modeling, hypothesis testing, multivariate statistical analysis, time series techniques, and optimization.\n\n\n\nGood-to-have\n\n\n\n\nSkills:\nProgramming languages such as R, Python, SQL, Spark, AWS, Azure, or Google Cloud for deploying and scaling language models, Data Visualization tools like Tableau, Power BI.\n\n\n\n\nJob\n\n\nSummary\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions. Provide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\nKey Responsibilities\nAn opportunity to work on high-visibility projects with top Pharma clients around the globe.\nPersonalized training modules to develop your strategy & consulting acumen to grow your skills, industry knowledge, and capabilities.\nProvide Subject matter expertise in various sub-segments of the LS industry.\nSupport delivery of small to medium-sized teams to deliver consulting projects for global clients.\nResponsibilities may include strategy, implementation, process design, and change management for specific modules.\nWork with the team or as an Individual contributor on the project assigned which includes a variety of skills to be utilized from Data Engineering to Data Science\nDevelop assets and methodologies, point-of-view, research, or white papers for use by the team and the larger community.\nAcquire new skills that have utility across industry groups.\nSupport strategies and operating models focused on some business units and assess likely competitive responses. Also, assess implementation readiness and points of greatest impact.\n\n\n\n\n\nAdditional Information\nProficient in Excel, MS Word, PowerPoint, etc.\nAbility to solve complex business problems and deliver client delight.\nStrong writing skills to build points of view on current industry trends.\nGood communication, interpersonal, and presentation skills\n\n\nAbout Our Company | Accenture (do not remove the hyperlink)\n\nQualification\n\n\n\nExperience:Proven experience (4+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['pharmaceutical', 'python', 'sql', 'life sciences', 'aws', 'microsoft azure', 'power bi', 'time series', 'machine learning', 'data engineering', 'artificial intelligence', 'tableau', 'r', 'data science', 'gcp', 'spark', 'predictive modeling', 'statistical modeling', 'data visualization', 'statistics']",2025-06-13 05:22:40
S&C GN - Data&AI - Life Sciences - Analyst,Accenture,2 - 7 years,Not Disclosed,['Gurugram'],"Management Level:Ind & Func AI Decision Science Analyst\n\n\n\n\nJob Location:Bangalore / Gurgaon\n\n\n\nMust-have\n\n\n\n\nSkills:\nLife Sciences/Pharma/Healthcare projects and delivering successful outcomes, commercial, clinical, Statistical Models/Machine Learning including Segmentation & predictive modeling, hypothesis testing, multivariate statistical analysis, time series techniques, and optimization.\n\n\n\nGood-to-have\n\n\n\n\nSkills:\nProficiency in Programming languages such as R, Python, SQL, Spark, AWS, Azure, or Google Cloud for deploying and scaling language models, Data Visualization tools like Tableau, Power BI\n\n\n\nExperience:Proven experience (2+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.\n\n\n\nJob\n\n\nSummary\n\nWe are seeking an experienced and visionary - Accenture S&C Global Network - Data & AI practice help our clients grow their business in entirely new ways. Analytics enables our clients to achieve high performance through insights from data - insights that inform better decisions and strengthen customer relationships. From strategy to execution, Accenture works with organizations to develop analytic capabilities - from accessing and reporting on data to predictive modelling - to outperform the competition.\n\n\n\nKey Responsibilities\nSupport delivery of small to medium-sized teams to deliver consulting projects for global clients.\nAn opportunity to work on high-visibility projects with top Pharma clients around the globe.\nPersonalized training modules to develop your strategy & consulting acumen to grow your skills, industry knowledge, and capabilities.\nResponsibilities may include strategy, implementation, process design, and change management for specific modules.\nWork with the team or as an Individual contributor on the project assigned which includes a variety of skills to be utilized from Data Engineering to Data Science\nDevelop assets and methodologies, point-of-view, research, or white papers for use by the team and the larger community.\nWork on variety of projects in Data Modeling, Data Engineering, Data Visualization, Data Science etc.,\nAcquire new skills that have utility across industry groups.\n\n\n\n\n\nAdditional Information\nAbility to solve complex business problems and deliver client delight.\nStrong writing skills to build points of view on current industry trends.\nGood communication, interpersonal, and presentation skills\n\n\nAbout Our Company | Accenture (do not remove the hyperlink)\n\n\nQualification\n\n\n\nExperience:Proven experience (2+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['pharmaceutical', 'python', 'sql', 'life sciences', 'aws', 'presentation skills', 'microsoft azure', 'power bi', 'time series', 'machine learning', 'artificial intelligence', 'tableau', 'r', 'data science', 'gcp', 'spark', 'predictive modeling', 'statistical modeling', 'data visualization', 'statistics']",2025-06-13 05:22:42
Data Catalogue - Analyst,AstraZeneca India Pvt. Ltd,1 - 8 years,Not Disclosed,['Chennai'],"Job Title: Data Catalogue Analyst\nCareer Leve : C3\nIntroduction to role:\nAre you ready to make a significant impact in the world of data management? As a Data Catalogue Analyst, youll play a crucial role in ensuring that data is findable, accessible, and fit for use across various business units. Youll be responsible for capturing metadata and developing our data catalogue, supporting the Commercial and Enabling Units business areas. This is your chance to contribute to meaningful work that drives excellence and breakthroughs.\nAccountabilities:\nSupport the Data Catalogue Principal to define Information Asset Registers across business areas to help profile information risk/value\nParticipate in projects to mitigate and control identified priority risk areas\nTake responsibility for nominated markets/business areas, develop domain knowledge and leverage internal customer relationships to respond to localised use cases\nAct as point of contact for nominated business areas or markets\nSupport initiatives to enhance the reusability and transparency of our data by making it available in our global data catalogue\nSupport the capture of user requirements for functionality and usability, and document technical requirements\nWork with IT partners to capture metadata for relevant data sets and lineage, and populate the catalogue\nWork with data stewards and business users to enrich catalogue entries with business data dictionary, business rules, glossaries\nComplete monitoring controls to assure metadata quality remains at a high level\nSupport catalogue principles and data governance leads for tool evaluation and UAT\nEssential Skills/Experience:\nDemonstrable experience of working in a data management, data governance or data engineering domain\nStrong business and system analysis skills\nDemonstrable experience with Data Catalogue, Search and Automation software (Collibra, Informatica, Talend etc)\nAbility to interpret and communicate technical information into business language and in alignment with AZ business\nSolid grasp of metadata harvesting methodologies and ability to create business and technical metadata sets.\nStrong engagement, communication and collaborator management skills, including excellent organizational, presentation and influencing skills\nHigh level of proficiency with common business applications (Excel, Visio, Word, PowerPoint & SAP business user)\nDesirable Skills/Experience:\nDemonstrable experience of working with Commercial or Finance data and systems (Veeva, Reltio, SAP) and consumption\nDomain knowledge of life sciences/pharmaceuticals; manufacturing; corporate finance; or sales & marketing\nExperience with data quality and profiling software\nExperience of working in a complex, diverse global organization\nAstraZeneca offers an environment where you can apply your skills to genuinely impact patients lives. With a focus on innovation and growth, youll be part of a team that challenges norms and embraces intelligent risks. Our collaborative community thrives on sharing knowledge and celebrating successes together. Here, youll find opportunities to learn from diverse perspectives, drive change, and contribute to our digital transformation journey.\nReady to take the next step in your career? Apply now and become a key player in shaping the future at AstraZeneca!\n11-Jun-2025\n11-Jun-2025",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'SAP', 'Data management', 'Senior Analyst', 'Life sciences', 'Corporate finance', 'Data quality', 'Visio', 'Monitoring', 'Recruitment']",2025-06-13 05:22:44
Associate Specialist Data Science,Merck Sharp & Dohme (MSD),2 - 7 years,Not Disclosed,['Pune'],"Primary Responsibilities\nSupport in establishing frameworks to standardize, productize and scale existing and new capabilities / analytical solutions\nImplement the vision, roadmap, and best practices for the Data Science Center of Excellence ( CoE ) to align with business goals\nSupport establishing governance frameworks to measure the value of products, standardize data science methodologies, coding practices, and project workflows\nWork with senior CoE members in development and maintenance of best practices for model and algorithm development and design, deployment, and monitoring across the enterprise functions\nCollaborate with product team on product development incorporating Agile framework and latest industry best practices and norms\nSupport in development of MLOps and ModelOps frameworks to streamline the development-to-deployment product pipeline\nDrive innovation by identifying, evaluating, and implementing cutting-edge data science methodologies based on latest published literature\n\nQualifications\nEducation & Work Experience Requiremen ts:\nMaster s degree (relevant field like Economics, Statistics, Mathematics, Operational Research) with 2+ years work experience.\nBachelor s degree (in Engineering or related field, such as Computer Science, Data Science, Statistics, Business, etc.) with at least 3 + years relevant experience\nPrior experience in research publications in reputed journal is a plus\nSkillset:\nCandidates must have -\nStrong programming skills in languages such as Python or R, and SQL with experience in data manipulation and analysis libraries (e.g., pandas, NumPy, scikit-learn, stats models)\nExperience with data science principles, machine learning (supervised and unsupervised) and GenAI algorithms, test-control analysis, propensity score matching etc.\nExposure to product roadmaps, Agile methodologies and backlog management, ensuring iterative and incremental product improvements\nStrong problem solving, business analysis and quantitative skills\nAbility to effectively communicate proposals to key stakeholders\nCandidates are desired but not mandatory to have -\nExperience and familiarity with underlying concepts such as Patient analytics, MMx etc.\nUnderstanding of Pharma commercial landscape will be a plus\nExperience working with healthcare, financial, or enterprise SaaS products\n  Search Firm Representatives Please Read Carefully\nEmployee Status:\nRegular\nRelocation:\nVISA Sponsorship:\nTravel Requirements:\nFlexible Work Arrangements:\nNot Applicable\nShift:\nValid Driving License:\nHazardous Material(s):\n\nRequired Skills:\nBusiness Intelligence (BI), Database Design, Data Engineering, Data Modeling, Data Science, Data Visualization, Machine Learning, Software Development, Stakeholder Relationship Management, Waterfall Model",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Relationship management', 'Business analysis', 'Coding', 'Pharma', 'Analytical', 'Healthcare', 'Business intelligence', 'Analytics', 'Monitoring', 'SQL']",2025-06-13 05:22:45
Data & Analytics Specialist,Roche Diagnostics,5 - 10 years,Not Disclosed,['Pune'],"At Roche you can show up as yourself, embraced for the unique qualities you bring. Our culture encourages personal expression, open dialogue, and genuine connections, where you are valued, accepted and respected for who you are, allowing you to thrive both personally and professionally. This is how we aim to prevent, stop and cure diseases and ensure everyone has access to healthcare today and for generations to come. Join Roche, where every voice matters.\nThe Position The Position\nWe are looking for a Data & Analytics Specialist/ Business Analyst who will join us in the newly setup Integrated Informatics for a journey to drive transformation with data and foster automated and efficient decision making throughout the organisation\nThe Data and Analytics Specialist must be the big-picture thinker who understands the value of data to the organisation, has a strong focus on delivering high value, connecting the dots, investing in right initiatives with reusability at the heart of it.\nIn this position you will be acting as squad lead, have end to end ownership of Product delivery with setting up teams from multiple teams/areas with focus on Lifecycle management of the product\nResponsibilities\nYou will work on various aspects of Analytics Solution Development, Data Management, Governance and Information Architecture including but not limited to:\nCollaborate with business stakeholders to understand their data and analytics needs and develop a product roadmap that aligns with business goals.\nDefine and prioritise product requirements, user stories, and acceptance criteria for data and analytics products and ensure what was agreed gets delivered.\nWork with data engineers and data scientists to develop data pipelines, analytical models, and visualisations that meet business requirements.\nCollaborate with Infrastructure Teams and software developers to ensure that data and analytics products are integrated into existing systems and platforms in a sustainable way that still meets the needs of business to generate the insights necessary to drive their decisions.\nMonitor data and analytics product performance and identify opportunities for improvement.\nStay up-to-date with industry trends and emerging technologies related to data and analytics in the pharmaceutical industry.\nAct as a subject matter expert for data and analytics products and provide guidance to business stakeholders on how to effectively use these products.\nAccountable to Develop and maintain documentation, training materials, and user guides for data and analytics products.\nThe ideal candidate\nBachelors or Masters degree in computer science, information systems, or a related field.\n5+ years of experience in roles such as Senior Data & Analytics Specialist, Data Solutions Lead, Data Architect, or Data Consultant, with a focus on solution design and implementation. Alternatively, 3-4 years of experience in data streams (e.g., Data Science, Data Engineering, Data Governance) combined with a couple of years in Strategic Data Consultancy / Data Product Ownership. Experience in the pharmaceutical or healthcare industry is highly desirable.\nHigh Level understanding of data engineering, data science, Data governance and analytics concepts and technologies.\nExperience working with cross-functional teams, including data engineers, data scientists, and software developers.\nExcellent communication and interpersonal skills.\nStrong analytical and problem-solving skills.\nExperience with agile development methodologies.\nKnowledge of regulatory requirements related to data and analytics in the pharmaceutical industry.\nKnowledge of working with vendor and customer master data for different divisions - Pharmaceuticals, Diagnostic, & Diabetes care.\nUnderstanding of the transparency reporting landscape.\nHands-on experience of working on applications such as Jira, SQL, Postman, SAP GUI, Monday.com, Trello\nProficient in the knowledge of different CRM/Master Data Management systems such as SFDC, Reltio MDM\nUnderstanding data protection laws and consent processes applicable to healthcare professionals and organizations before transparency disclosure.\nWho we are\nAt Roche, more than 100,000 people across 100 countries are pushing back the frontiers of healthcare. Working together, we ve become one of the world s leading research-focused healthcare groups. Our success is built on innovation, curiosity and diversity.\nBasel is the headquarters of the Roche Group and one of its most important centres of pharmaceutical research. Over 10,700 employees from over 100 countries come together at our Basel/Kaiseraugst site, which is one of Roche`s largest sites. Read more.\nBesides extensive development and training opportunities, we offer flexible working options, 18 weeks of maternity leave and 10 weeks of gender independent partnership leave. Our employees also benefit from multiple services on site such as child-care facilities, medical services, restaurants and cafeterias, as well as various employee events.\nWe believe in the power of diversity and inclusion, and strive to identify and create opportunities that enable all people to bring their unique selves to Roche.\nRoche is an Equal Opportunity Employer.\nWho we are\n.\n\nLet s build a healthier future, together.\nRoche is an Equal Opportunity Employer.\n""",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'SAP', 'Diagnostics', 'HP data protector', 'Analytical', 'Healthcare', 'JIRA', 'Analytics', 'SQL', 'CRM']",2025-06-13 05:22:47
Data & Analytics Specialist,Hoffmann La Roche,5 - 10 years,Not Disclosed,['Pune'],"At Roche you can show up as yourself, embraced for the unique qualities you bring. Our culture encourages personal expression, open dialogue, and genuine connections, where you are valued, accepted and respected for who you are, allowing you to thrive both personally and professionally. This is how we aim to prevent, stop and cure diseases and ensure everyone has access to healthcare today and for generations to come. Join Roche, where every voice matters.\nThe Position The Position\nWe are looking for a Data & Analytics Specialist/ Business Analyst who will join us in the newly setup Integrated Informatics for a journey to drive transformation with data and foster automated and efficient decision making throughout the organisation\nThe Data and Analytics Specialist must be the big-picture thinker who understands the value of data to the organisation, has a strong focus on delivering high value, connecting the dots, investing in right initiatives with reusability at the heart of it.\nIn this position you will be acting as squad lead, have end to end ownership of Product delivery with setting up teams from multiple teams/areas with focus on Lifecycle management of the product\nResponsibilities\nYou will work on various aspects of Analytics Solution Development, Data Management, Governance and Information Architecture including but not limited to:\nCollaborate with business stakeholders to understand their data and analytics needs and develop a product roadmap that aligns with business goals.\nDefine and prioritise product requirements, user stories, and acceptance criteria for data and analytics products and ensure what was agreed gets delivered.\nWork with data engineers and data scientists to develop data pipelines, analytical models, and visualisations that meet business requirements.\nCollaborate with Infrastructure Teams and software developers to ensure that data and analytics products are integrated into existing systems and platforms in a sustainable way that still meets the needs of business to generate the insights necessary to drive their decisions.\nMonitor data and analytics product performance and identify opportunities for improvement.\nStay up-to-date with industry trends and emerging technologies related to data and analytics in the pharmaceutical industry.\nAct as a subject matter expert for data and analytics products and provide guidance to business stakeholders on how to effectively use these products.\nAccountable to Develop and maintain documentation, training materials, and user guides for data and analytics products.\nThe ideal candidate\nBachelors or Masters degree in computer science, information systems, or a related field.\n5+ years of experience in roles such as Senior Data & Analytics Specialist, Data Solutions Lead, Data Architect, or Data Consultant, with a focus on solution design and implementation. Alternatively, 3-4 years of experience in data streams (e.g., Data Science, Data Engineering, Data Governance) combined with a couple of years in Strategic Data Consultancy / Data Product Ownership. Experience in the pharmaceutical or healthcare industry is highly desirable.\nHigh Level understanding of data engineering, data science, Data governance and analytics concepts and technologies.\nExperience working with cross-functional teams, including data engineers, data scientists, and software developers.\nExcellent communication and interpersonal skills.\nStrong analytical and problem-solving skills.\nExperience with agile development methodologies.\nKnowledge of regulatory requirements related to data and analytics in the pharmaceutical industry.\nKnowledge of working with vendor and customer master data for different divisions - Pharmaceuticals, Diagnostic, & Diabetes care.\nUnderstanding of the transparency reporting landscape.\nHands-on experience of working on applications such as Jira, SQL, Postman, SAP GUI, Monday.com, Trello\nProficient in the knowledge of different CRM/Master Data Management systems such as SFDC, Reltio MDM\nUnderstanding data protection laws and consent processes applicable to healthcare professionals and organizations before transparency disclosure.\nWho we are\n.\nBasel is the headquarters of the Roche Group and one of its most important centres of pharmaceutical research. Over 10,700 employees from over 100 countries come together at our Basel/Kaiseraugst site, which is one of Roche`s largest sites. Read more.\nBesides extensive development and training opportunities, we offer flexible working options, 18 weeks of maternity leave and 10 weeks of gender independent partnership leave. Our employees also benefit from multiple services on site such as child-care facilities, medical services, restaurants and cafeterias, as well as various employee events.\nWe believe in the power of diversity and inclusion, and strive to identify and create opportunities that enable all people to bring their unique selves to Roche.\nRoche is an Equal Opportunity Employer.\nWho we are\nA healthier future drives us to innovate. Together, more than 100 000 employees across the globe are dedicated to advance science, ensuring everyone has access to healthcare today and for generations to come. Our efforts result in more than 26 million people treated with our medicines and over 30 billion tests conducted using our Diagnostics products. We empower each other to explore new possibilities, foster creativity, and keep our ambitions high, so we can deliver life-changing healthcare solutions that make a global impact.\n\nLet s build a healthier future, together.\nRoche is an Equal Opportunity Employer.\n""",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'SAP', 'Diagnostics', 'HP data protector', 'Analytical', 'Healthcare', 'JIRA', 'Analytics', 'SQL', 'CRM']",2025-06-13 05:22:49
Data Scientist-Advanced Analytics,IBM,3 - 7 years,Not Disclosed,['Kochi'],"We are seeking a highly skilled Advanced Analytics Specialist to join our dynamic team. The successful candidate will be responsible for leveraging advanced analytics techniques to derive actionable insights, inform business decisions, and drive strategic initiatives. This role requires a deep understanding of data analysis, statistical modeling, machine learning, and data visualization.\nIn this role, you will be responsible for architecting and delivering AI solutions using cutting-edge technologies, with a strong focus on foundation models and large language models. You will work closely with customers, product managers, and development teams to understand business requirements and design custom AI solutions that address complex challenges. Experience with tools like Github Copilot, Amazon Code Whisperer etc. is desirable.\nSuccess is our passion, and your accomplishments will reflect this, driving your career forward, propelling your team to success, and helping our clients to thrive.\nDay-to-Day Duties:\nProof of Concept (POC) DevelopmentDevelop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions. Collaborate with development teams to implement and iterate on POCs, ensuring alignment with customer requirements and expectations.\nHelp in showcasing the ability of Gen AI code assistant to refactor/rewrite and document code from one language to another, particularly COBOL to JAVA through rapid prototypes/ PoC\nDocumentation and Knowledge SharingDocument solution architectures, design decisions, implementation details, and lessons learned. Create technical documentation, white papers, and best practice guides. Contribute to internal knowledge sharing initiatives and mentor new team members.\nIndustry Trends and InnovationStay up to date with the latest trends and advancements in AI, foundation models, and large language models. Evaluate emerging technologies, tools, and frameworks to assess their potential impact on solution design and implementation\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nDevelop and implement advanced analytical models and algorithms to solve complex business problems, analyze large datasets to uncover trends, patterns, and insights that drive business performance.\nCollaborate with cross-functional teams to identify key business challenges and opportunities, Create and maintain data pipelines and workflows to ensure the accuracy and integrity of data, Design and deliver insightful reports and dashboards to communicate findings to stakeholders.\nStay up to date with the latest advancements in analytics, machine learning, and data science. Provide technical expertise and mentorship to junior team members.\nQualificationsBachelor’s or master’s degree in data science, Statistics, Mathematics, Computer Science, or a related field. Proven experience in advanced analytics, data science, or a similar role. Proficiency in programming languages such as Python, R, or SQL. Experience with data visualization tools like Tableau, Power BI, or similar.\nStrong understanding of statistical modelling and machine learning algorithms. Excellent analytical, problem-solving, and critical thinking skills. Ability to communicate complex analytical concepts to non-technical stakeholders. Experience with big data technologies (e.g., Hadoop, Spark) is a plus\n\n\nPreferred technical and professional experience\nFamiliarity with cloud-based analytics platforms (e.g., AWS, Azure).\nKnowledge of natural language processing (NLP) and deep learning techniques.\nExperience with project management and agile methodologies",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analysis', 'machine learning', 'statistical modeling', 'data visualization', 'machine learning algorithms', 'advanced analytics', 'python', 'github', 'natural language processing', 'power bi', 'microsoft azure', 'sql', 'r', 'tableau', 'java', 'data science', 'spark', 'hadoop', 'aws']",2025-06-13 05:22:51
Data Modeler,Accenture,15 - 20 years,Not Disclosed,['Mumbai'],"Project Role :Data Modeler\n\n\n\n\n\nProject Role Description :Work with key business representatives, data owners, end users, application designers and data architects to model current and new data.\n\n\n\nMust have skills :Data Modeling Techniques and Methodologies\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\nProject Role :Data Architect & Modeler\n\nProject Role Description Data Model, Design, build and lead the complex ETL data integration pipelines to meet business process and application requirements. Management Level :9Work Experience :6+ yearsWork Location :AnyMust have skills :Data Architecture Principles\nGood to have skills :Data Modeling, Data Architect, Informatica PowerCenter, Informatica Data Quality, SAP BusinessObjects Data Services, SQL, PL/SQL, SAP HANA DB, MS Azure, Python, ErWin, SAP Power Designer Job :Data Architect, Modeler, and data Integration LeadKey Responsibilities:1) Working on building Data models, Forward and Reverse Engineering.2) Working on Data and design analysis and working with data analysts team on data model design.3) Working on presentations on design, end to end flow and data models.4) Work on new and existing data models using Power designer tools and other designing tools like Visio5) Work with functional SMEs, BAs to review requirements, mapping documents\nTechnical Experience:1) Should have good understanding of ETL design concepts like CDC, SCD, Transpose/ pivot, Updates, Validation2) Should have strong understanding of SQL concepts, Data warehouse concepts and can easily understand data technically and functionally.3) Good understanding of various file formats like xml, delimited, fixed width etc.4) Understand the concepts of data quality, data cleansing, data profiling5) Good to have Python and other new data technologies and cloud exposure.6) Having Insurance background is a plus.\nEducational Qualification :15 years of fulltime education with BE/B Tech or equivalent\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Modeling Techniques and Methodologies.- Strong understanding of relational and non-relational database design principles.- Experience with data integration and ETL processes.- Familiarity with data governance and data quality frameworks.- Ability to translate business requirements into technical specifications.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'data architecture principles', 'data modeling', 'data warehousing concepts', 'sql joins', 'python', 'ms azure', 'sap', 'informatica powercenter', 'informatica data quality', 'data warehousing', 'erwin', 'data architecture', 'plsql', 'modeler', 'hana db', 'etl', 'sap hana', 'data integration']",2025-06-13 05:22:53
Data Modeler,Accenture,15 - 20 years,Not Disclosed,['Mumbai'],"Project Role :Data Modeler\n\n\n\n\n\nProject Role Description :Work with key business representatives, data owners, end users, application designers and data architects to model current and new data.\n\n\n\nMust have skills :Data Building Tool\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Modeler, you will engage with key business representatives, data owners, end users, application designers, and data architects to model both current and new data. Your typical day will involve collaborating with various stakeholders to understand their data needs, analyzing existing data structures, and designing effective data models that support business objectives. You will also be responsible for ensuring that the data models are aligned with best practices and organizational standards, facilitating smooth data integration and accessibility across different systems. This role requires a proactive approach to problem-solving and a commitment to delivering high-quality data solutions that enhance decision-making processes within the organization.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Facilitate training sessions and workshops to enhance team capabilities.- Continuously evaluate and improve data modeling processes to ensure efficiency.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Building Tool.- Strong understanding of data modeling techniques and methodologies.- Experience with data integration and ETL processes.- Familiarity with database management systems and SQL.- Ability to translate business requirements into technical specifications.\nAdditional Information:- The candidate should have minimum 7.5 years of experience in Data Building Tool.- This position is based in Mumbai.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['sql', 'database management', 'data modeling', 'etl', 'data integration', 'python', 'oracle', 'data analysis', 'data warehousing', 'sme', 'data architecture', 'business intelligence', 'sql server', 'plsql', 'unix shell scripting', 'etl tool', 'modeler', 'informatica', 'unix', 'etl process']",2025-06-13 05:22:55
Data Architect,Accenture,15 - 20 years,Not Disclosed,['Bhubaneswar'],"Project Role :Data Architect\n\n\n\n\n\nProject Role Description :Define the data requirements and structure for the application. Model and design the application data structure, storage and integration.\n\n\n\nMust have skills :Data Architecture Principles\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n18 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Architect, you will define the data requirements and structure for the application. Your typical day involves modeling and designing the application data structure, storage, and integration, ensuring that the data architecture aligns with business objectives and supports efficient data management practices. You will collaborate with various stakeholders to gather requirements and translate them into effective data solutions, while also addressing any challenges that arise in the data architecture process.\nRoles & Responsibilities:- Expected to be a Subject Matter Expert with deep knowledge and experience.- Should have influencing and advisory skills.- Engage with multiple teams and responsible for team decisions.- Expected to provide solutions to problems that apply across multiple teams, and provide solutions to business area problems.- Facilitate workshops and discussions to gather data requirements and ensure alignment with business goals.- Develop and maintain documentation related to data architecture, including data models and integration processes.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Architecture Principles.- Strong understanding of data modeling techniques and best practices.- Experience with data integration tools and methodologies.- Knowledge of database management systems and data storage solutions.- Familiarity with data governance and data quality frameworks.\nAdditional Information:- The candidate should have minimum 18 years of experience in Data Architecture Principles.- This position is based at our Mumbai office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data architecture', 'database management', 'data architecture principles', 'data modeling', 'data integration', 'python', 'data management', 'data warehousing', 'power bi', 'sql server', 'sql', 'plsql', 'data quality', 'tableau', 'dwbi', 'data governance', 'etl', 'ssis', 'informatica']",2025-06-13 05:22:57
Data Modeler,Accenture,12 - 15 years,Not Disclosed,['Kolkata'],"Project Role :Data Modeler\n\n\n\n\n\nProject Role Description :Work with key business representatives, data owners, end users, application designers and data architects to model current and new data.\n\n\n\nMust have skills :Data Building Tool\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Modeler, you will engage with key business representatives, data owners, end users, application designers, and data architects to model both current and new data. Your typical day will involve collaborating with various stakeholders to understand their data needs, analyzing existing data structures, and designing effective data models that support business objectives. You will also be responsible for ensuring that the data models are aligned with best practices and organizational standards, facilitating smooth data integration and accessibility across different systems. This role requires a proactive approach to problem-solving and a commitment to delivering high-quality data solutions that enhance decision-making processes within the organization.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Facilitate workshops and meetings to gather requirements and feedback from stakeholders.- Develop and maintain comprehensive documentation of data models and architecture.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Building Tool.- Strong understanding of data modeling techniques and methodologies.- Experience with data integration and ETL processes.- Familiarity with database management systems and SQL.- Ability to translate business requirements into technical specifications.\nAdditional Information:- The candidate should have minimum 12 years of experience in Data Building Tool.- This position is based at our Kolkata office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'database management', 'data modeling', 'etl', 'data integration', 'python', 'oracle', 'data analysis', 'data warehousing', 'sme', 'data architecture', 'business intelligence', 'sql server', 'plsql', 'unix shell scripting', 'etl tool', 'modeler', 'informatica', 'unix', 'etl process']",2025-06-13 05:22:59
Data Platform Architect,Accenture,15 - 25 years,Not Disclosed,['Bhubaneswar'],"Project Role :Data Platform Architect\n\n\n\n\n\nProject Role Description :Architects the data platform blueprint and implements the design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Data Architecture Principles\n\n\n\n\nGood to have skills :Amazon Web Services (AWS), Teradata Vantage, Data GovernanceMinimum\n\n\n\n15 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Architect, you will architect the data platform blueprint and implement the design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\nRoles & Responsibilities:- Expected to be a SME with deep knowledge and experience.- Should have Influencing and Advisory skills.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Lead the design and implementation of the data platform architecture.- Collaborate with cross-functional teams to ensure data platform alignment with business objectives.- Provide technical guidance and mentorship to junior team members.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Architecture Principles.- Good To Have\n\n\n\n\nSkills:\nExperience with Amazon Web Services (AWS), Teradata Vantage, Data Governance.- Strong understanding of data architecture principles and best practices.- Experience in designing and implementing scalable data solutions.- Knowledge of cloud platforms and data governance frameworks.\nAdditional Information:- The candidate should have a minimum of 15 years of experience in Data Architecture Principles.- This position is based at our Pune office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['vantage', 'data architecture', 'data architecture principles', 'data governance', 'aws', 'mainframes', 'oracle', 'data warehousing', 'microsoft azure', 'dbms', 'sql server', 'sql', 'jcl', 'java', 'data modeling', 'cobol', 'gcp', 'platform architecture', 'hadoop', 'etl', 'big data', 'informatica', 'vsam']",2025-06-13 05:23:00
Data Analyst - L3,Wipro,3 - 5 years,Not Disclosed,['Hyderabad'],"Role Purpose\nThe purpose of this role is to interpret data and turn into information (reports, dashboards, interactive visualizations etc) which can offer ways to improve a business, thus affecting business decisions.\nDo\n1. Managing the technical scope of the project in line with the requirements at all stages\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\n\n\n\n\nMandatory Skills: Business Analyst/ Data Analyst(Media). Experience: 3-5 Years.",,,,"['Data analysis', 'data validation', 'data mining', 'business analysis', 'data warehousing', 'business analytics', 'dbms', 'dashboards', 'sales', 'analytics reporting', 'reporting tools', 'data integration', 'digital transformation']",2025-06-13 05:23:02
Data Scientist,"Sourced Group, an Amdocs Company",4 - 9 years,Not Disclosed,['Gurugram'],"0px> Who are we?\nIn one sentence\nThis is a hands-on position for a motivated and talented innovator. The Data Scientist performs data mining and develops algorithms that provide insight from data.\nWhat will your job look like?\nYou will be responsible for and perform end-top-end data-based research.\nYou will craft data mining solutions to be implemented and executed with alignment to the planned scope and design coverage and needs/uses, demonstrating knowledge and a broad understanding of E2E business processes and requirements.\nYou will define the data analytics research plan, scope and resources required to meet the objectives of his/her area of ownership.\nYou will identify and analyze new data analytic directions and their potential business impact to determine the accurate prioritization of data analytics activities based on business needs and analytics value.\nYou will identify data sources, supervises the data collection process and crafts the data structure in collaboration with data experts (BI or big-data) and subject matter and business experts. Ensures that data used in the data analysis activities are of the highest quality.\nYou will construct data models (algorithms and formulas) for required business needs and predictions.\nYou will present results, including the preparation of patents and white papers and facilitating presentations during conferences.\nAll you need is...\nPh.D. in Computer Science, Mathematics or Statistics\n4 years experience in tasks related to data analytics\nKnowledge of telecommunications and of the subject area being investigated - advantage\nKnowledge in the product (ACC or other) application knowledge and configuration knowledge\nKnowledge in BSS, billing, Telco and the business processes\nFamiliarity in the Telco Networking - mobile, landline, cable TV, Internet\nknowledge in Oracle SQL\nWhy you will love this job:\nYou will ensure timely resolution or critical issue within the agreed SLA. This includes creating a positive customer support experience and build strong relationships through problem understanding, presenting promptly on progress, and handling customers with a professional demeanour.\nYou will be able to demonstrates an understanding of key business drivers and ensures strategic directions are followed and the organization succeeds\nWe are a dynamic, multi-cultural organization that constantly innovates and empowers our employees to grow. Our people our passionate, daring, and phenomenal teammates that stand by each other with a dedication to creating a diverse, inclusive workplace!\nWe offer a wide range of stellar benefits including health, dental, vision, and life insurance as well as paid time off, sick time, and parental leave!\n",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Bss', 'Networking', 'Billing', 'Data collection', 'Customer handling', 'Customer support', 'Data mining', 'Amdocs']",2025-06-13 05:23:04
Advanced Data Science Associate,ZS,0 - 2 years,Not Disclosed,['Pune'],"ZSs Insights & Analytics group partners with clients to design and deliver solutions to help them tackle a broad range of business challenges. Our teams work on multiple projects simultaneously, leveraging advanced data analytics and problem-solving techniques. Our recommendations and solutions are based on rigorous research and analysis underpinned by deep expertise and thought leadership.\nWhat you'll Do\nDevelop advanced and efficient statistically effective algorithms that solve problems of high",,,,"['Text mining', 'Analytical', 'Management consulting', 'Financial planning', 'Machine learning', 'Hypothesis Testing', 'Predictive modeling', 'Data mining', 'big data']",2025-06-13 05:23:06
Advanced Data Science Associate,ZS,0 - 2 years,Not Disclosed,"['Noida', 'Gurugram']","Develop advanced and efficient statistically effective algorithms that solve problems of high dimensionality .\nUtilize technical skills such as hypothesis testing, machine learning and retrieval processes to apply statistical and data mining techniques to identify trends, create figures, and analyze other relevant information.\nCollaborate with clients and other stakeholders at ZS to integrate and effectively communicate analysis findings.\nContribute to the assessment of emerging datasets and technologies that impact our analytical",,,,"['Text mining', 'Analytical', 'Management consulting', 'Financial planning', 'Machine learning', 'Hypothesis Testing', 'Predictive modeling', 'Data mining', 'big data']",2025-06-13 05:23:08
Sr SW Engineer - ML,Visa,2 - 7 years,Not Disclosed,['Bengaluru'],"This position is for a Senior Data Engineer with solid development experience who will focus on creating new capabilities for AI as a Service while maturing our code base and development processes. In this position, you are first a passionate and talented developer that can work in a dynamic environment as a member of Agile Scrum teams. Your strong technical leadership, problem-solving abilities, coding, testing and debugging skills is just a start. You must be dedicated to filling product backlog and delivering production-ready code. You must be willing to go beyond the routine and prepared to do a little bit of everything.\nEssential Functions\nCollaborate with project teams, data science teams and development teams to drive the technical roadmap and guide development and implementation of new data driven business solutions.\nDrive technical standard and best practices, and continuously improve AI Platform engineering scalability.\nArchitecture and design of AI Platform services including Machine Learning Engines, In Memory Computing Systems, Streaming Computing Systems, Distributed Data Systems and etc, in Golang, Java, and Python.\nCoordinate the implementation among development teams to ensure system performance, security, scalability and availability.\nCoaching and mentoring junior team members and evolving team talent pipeline.\n\n\nBasic Qualifications\n2+ years of relevant work experience and a Bachelors degree, OR 5+ years of relevant work experience\n\nPreferred Qualifications\n3 or more years of work experience with a Bachelor s Degree or more than 2 years of work experience with an Advanced Degree (e.g. Masters, MBA, JD, MD)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Basic', 'data science', 'Agile scrum', 'Architecture', 'Coding', 'Debugging', 'Machine learning', 'Technical leadership', 'Business solutions', 'Python']",2025-06-13 05:23:09
IN Senior Associate AWS DataOps Engineer,PwC Service Delivery Center,4 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\n& Summary We are looking for a seasoned AWS DataOps Engineer\nResponsibilities\nDesign, implement, and manage scalable data pipelines and ETL processes on AWS. Collaborate with data teams to understand requirements and translate them into robust data solutions. Proven experience with AWS data services such as S3, Redshift, RDS, Glue, and Lambda. Strong understanding of data architecture, data modeling, and data warehousing concepts. Strong programming and scripting skills in languages like Python, SQL, or Shell scripting. Experience with data pipeline and ETL tools, such as Apache Airflow or AWS Data Pipeline. Ensure data quality, integrity, and security through automated testing, monitoring, and alerting systems. Optimize data storage and retrieval using AWS services such as S3, Redshift, RDS, and DynamoDB. Implement data governance and compliance standards to ensure data privacy and security. Automate data integration and deployment processes using tools like AWS Data Pipeline, Glue, and Step Functions. Monitor and troubleshoot data workflows to ensure reliability and performance. Provide technical support and guidance to data teams on best practices for data management and operations.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience\nrequired\n48 Years\nEducation qualification BE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Master of Business Administration, Master of Engineering, Bachelor of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nDevOps\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'Data management', 'Data modeling', 'Shell scripting', 'Database administration', 'Agile', 'Apache', 'Technical support', 'SQL', 'Python']",2025-06-13 05:23:11
IN Senior Associate AWS DataOps Engineer,PwC Service Delivery Center,4 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\n& Summary We are looking for a seasoned AWS DataOps Engineer\nResponsibilities\nDesign, implement, and manage scalable data pipelines and ETL processes on AWS. Collaborate with data teams to understand requirements and translate them into robust data solutions. Proven experience with AWS data services such as S3, Redshift, RDS, Glue, and Lambda. Strong understanding of data architecture, data modeling, and data warehousing concepts. Strong programming and scripting skills in languages like Python, SQL, or Shell scripting. Experience with data pipeline and ETL tools, such as Apache Airflow or AWS Data Pipeline. Ensure data quality, integrity, and security through automated testing, monitoring, and alerting systems. Optimize data storage and retrieval using AWS services such as S3, Redshift, RDS, and DynamoDB. Implement data governance and compliance standards to ensure data privacy and security. Automate data integration and deployment processes using tools like AWS Data Pipeline, Glue, and Step Functions. Monitor and troubleshoot data workflows to ensure reliability and performance. Provide technical support and guidance to data teams on best practices for data management and operations.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience\nrequired\n48 Years\nEducation qualification BE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Master of Business Administration, Master of Engineering, Bachelor of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nDevOps\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'Data management', 'Data modeling', 'Shell scripting', 'Database administration', 'Agile', 'Apache', 'Technical support', 'SQL', 'Python']",2025-06-13 05:23:13
Senior Engineer II,AMERICAN EXPRESS,8 - 13 years,Not Disclosed,['Bengaluru'],"Join Team Amex and lets lead the way together.\nAmerican Express is looking for Senior Engineers to contribute to the company s focus on building products, like @Work, to support our large and global corporate clients. @Work helps our clients manage their Corporate Card and Corporate Purchasing Card programs more efficiently online. From performing everyday administrative tasks and account maintenance, to accessing reports and utilizing reconciliation solutions, @Work enables fast, efficient and effective program management resulting in time and cost savings for our clients.",,,,"['Computer science', 'Administration', 'Career development', 'Maven', 'Finance', 'Reconciliation', 'MySQL', 'Workflow', 'Monitoring', 'SQL']",2025-06-13 05:23:15
Senior Murex Front Office & Risk Support Engineer,Synechron,5 - 10 years,Not Disclosed,"['Pune', 'Bengaluru', 'Hinjewadi']","Job Summary\nSynechron is seeking an experienced Murex FO & Risk Support Specialist to join our dynamic team. This role is central to maintaining and supporting Murex platform functionalities related to front-office operations and risk management, with a focus on production support.\nThe individual will collaborate closely with business users and IT teams to resolve complex issues, optimize configurations, and ensure the stability of critical trading and risk systems. By providing expert-level support, this role contributes directly to the organizations ability to manage market and credit risks effectively, deliver timely business insights, and uphold operational resilience.\nSoftware Requirements\nOverall Responsibilities\nTechnical Skills (By Category)\nExperience Requirements\nDay-to-Day Activities\nQualifications\nProfessional Competencies",,,,"['Murex support', 'risk management', 'Risk Support', 'credit risk', 'market risk', 'pricing']",2025-06-13 05:23:16
Sr Manager of Software Engineering,JPMorgan Chase Bank,14 - 20 years,Not Disclosed,['Bengaluru'],"When you mentor and advise multiple technical teams and move financial technologies forward, it s a big challenge with big impact. You were made for this.\n\n\nAs a Senior Manager of Software Engineering at JPMorgan Chase within the Consumer and Community Banking Technology Team, you serve in a leadership role by providing technical coaching and advisory for multiple technical teams, as well as anticipate the needs and potential dependencies of other functions within the firm. As an expert in your field, your insights influence budget and technical considerations to advance operational efficiencies and functionalities.\n\nJob responsibilities\n\n\n\nProvide direction, oversight, and coaching for a team of entry-level to mid-level software engineers working on basic to moderately complex tasks.\n\nBe accountable for decisions affecting team resources, budget, tactical operations, and the execution and implementation of processes and procedures.\n\nLead the design, development, testing, and implementation of data visualization projects to support business objectives.\n\nCollaborate with data analysts, data scientists, and business stakeholders to understand data requirements and translate them into effective visual solutions.\n\nWork in an Agile development environment with team members, including Product Managers, UX Designers, QA Engineers, and other Software Engineers.\n\nValidate the technical feasibility of UI/UX designs and provide regular technical guidance to support business and technical teams, contractors, and vendors.\n\nDevelop secure, high-quality production code, review and debug code written by others, and drive decisions influencing product design, application functionality, and technical operations.\n\nServe as a subject matter expert in one or more areas of focus and actively contribute to the engineering community as an advocate of firmwide frameworks, tools, and practices of the Software Development Life Cycle.\n\nInfluence peers and project decision-makers to consider the use and application of leading-edge technologies.\n\nDevelop and maintain dashboards, reports, and interactive visualizations using tools such as Tableau, ensuring data accuracy and integrity by implementing best practices in data visualization and management.\n\nStay current with industry trends and emerging technologies in data visualization and analytics, communicate complex data insights clearly to various audiences, including senior management, and manage a team of data visualization associates, providing guidance and mentorship to junior team members.\n\n\n\nRequired qualifications, capabilities, and skills\n\n\n\nFormal training or certification in software engineering concepts and 5+ years of applied experience.\n\n5+ Years of experience as a Web/UI Lead Architect\n\nProficiency in Javascript, Typescript, HTML, CSS\n\nExpert knowledge in ReactJs, Redux, React hooks.\n\nStrong understanding of front-end coding and development technologies\n\nHands-on practical experience delivering system design, application development, testing, and operational stability\n\nAdvanced knowledge of software applications and technical processes with considerable in-depth knowledge in UI and Web Technologies\n\nAbility to tackle design and functionality problems independently with little to no oversight\n\nPractical cloud native experience\n\nExperience in Computer Science, Computer Engineering, Mathematics, or a related technical field\n\n\n\nPreferred qualifications, capabilities, and skills\n\n\n\nFull stack development with Node/. NET/Java\n\nFamiliarity with working in event driven environments\n\nA good understanding of cross-browser compatibility issues and their solutions along with Typescript\n\nExperience working with Databases and ability to write SQL queries along with experience with messaging platforms\n\nBachelor s degree in data science, Computer Science, Information Systems, Statistics, or a related field.\n\nProblem solver and solution oriented. Strong written and verbal communication skills. Jira and Agile practices\n\nExperience with big data technologies and machine learning is a plus.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Front end', 'Coding', 'Javascript', 'Agile', 'System design', 'HTML', 'Application development', 'JIRA', 'Analytics']",2025-06-13 05:23:18
Senior Backend Engineer - Bangalore - Hybrid (Product Based),Trigent Software Solutions,5 - 10 years,25-27.5 Lacs P.A.,['Bengaluru'],"Job Description:\nWe are looking for an experienced Backend Engineer to join our engineering team and contribute to building highly scalable, low-latency, and high-concurrency SaaS applications. The ideal candidate should have deep expertise in Java, cloud technologies (preferably AWS), and experience working with both RDBMS and NoSQL databases. You will be involved in the full software development lifecycle, from design to deployment, ensuring performance, security, and maintainability.\nKey Responsibilities:\nDesign, develop, and maintain high-performance, scalable, and secure backend services\nBuild and maintain RESTful APIs to support client-side applications and services\nWork on transactional and concurrent systems that serve large-scale SaaS platforms\nCollaborate with frontend engineers, architects, and DevOps to build robust cloud-based systems\nEnsure code quality and performance by writing unit/integration tests and performing code reviews\nTroubleshoot, debug, and optimize existing systems for reliability and efficiency\nFollow Agile development practices and participate in daily stand-ups and sprint planning\nMandatory Skills:\n58 years of backend development experience in building SaaS or transactional web applications\nStrong hands-on experience with Java and web application frameworks like Spring, Spring Boot\nExperience working on high-concurrency, low-latency, and high-availability systems\nSolid experience with at least one RDBMS (e.g., PostgreSQL, MySQL) and one NoSQL DB (e.g., MongoDB, Cassandra)\nExpertise in cloud platforms – preferably AWS (e.g., EC2, S3, RDS, Lambda)\nFamiliarity with application servers like Tomcat\nStrong knowledge of system design, data structures, and multithreading\nExperience with RESTful APIs and microservice architectures\nNice to Have:\nKnowledge of Big Data technologies (e.g., Kafka, Hadoop, Spark)\nFamiliarity with containerization tools like Docker and Kubernetes\nExperience with CI/CD tools and cloud deployment pipelines\nExposure to other cloud platforms like GCP or Azure",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['java', 'RDBMS', 'Saas Product Development', 'Nosql Databases', 'AWS', 'spring', 'tomcat']",2025-06-13 05:23:20
Senior Data Scientist,Epsilon,6 - 9 years,Not Disclosed,['Bengaluru'],"Responsibilities: -\nContribute and build an internal product library that is focused on solving business problems related to prediction & recommendation.\nResearch unfamiliar methodologies, techniques to fine tune existing models in the product suite and, recommend better solutions and/or technologies.\nImprove features of the product to include newer machine learning algorithms in the likes of product recommendation, real time predictions, fraud detection, offer personalization etc\nCollaborate with client teams to on-board data, build models and score predictions.\nParticipate in building automations and standalone applications around machine learning algorithms to enable a One Click solution to getting predictions and recommendations.\nAnalyze large datasets, perform data wrangling operations, apply statistical treatments to filter and fine tune input data, engineer new features and eventually aid the process of building machine learning models.\nRun test cases to tune existing models for performance, check criteria and define thresholds for success by scaling the input data to multifold.\nDemonstrate a basic understanding of different machine learning concepts such as Regression, Classification, Matrix Factorization, K-fold Validations and different algorithms such as Decision Trees, Random Forrest, K-means clustering.\nDemonstrate working knowledge and contribute to building models using deep learning techniques, ensuring robust, scalable and high-performance solutions\nMinimum Qualifications:\nEducation: Master's or PhD in a quantitative discipline (Statistics, Economics, Mathematics, Computer Science) is highly preferred.\nDeep Learning Mastery: Extensive experience with deep learning frameworks (TensorFlow, PyTorch, or Keras) and advanced deep learning projects across various domains, with a focus on multimodal data applications.\nGenerative AI Expertise: Proven experience with generative AI models and techniques, such as RAG, VAEs, Transformers, and applications at scale in content creation or data augmentation.\nProgramming and Big Data: Expert-level proficiency in Python and big data/cloud technologies (Databricks and Spark) with a minimum of 4-5 years of experience.\nRecommender Systems and Real-time Predictions: Expertise in developing sophisticated recommender systems, including the application of real-time prediction frameworks.\nMachine Learning Algorithms: In-depth experience with complex algorithms such as logistic regression, random forest, XGBoost, advanced neural networks, and ensemble methods.\nExperienced with machine learning algorithms such as logistic regression, random forest, XG boost, KNN, SVM, neural network, linear regression, lasso regression and k-means.\nDesirable Qualifications:\nGenerative AI Tools Knowledge: Proficiency with tools and platforms for generative AI (such as OpenAI, Hugging Face Transformers).\nDatabricks and Unity Catalog: Experience leveraging Databricks and Unity Catalog for robust data management, model deployment, and tracking.\nWorking experience in CI/CD tools such as GIT & BitBucket",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Data Engineering', 'Pyspark', 'Azure Aws', 'Generative AI', 'Big Data', 'AWS', 'Data Bricks', 'Deep Learning', 'Python', 'SQL']",2025-06-13 05:23:21
Senior Manager Marketing Data Analytics,Factspan Analytics,9 - 14 years,Not Disclosed,['Bengaluru'],"Position: Senior Manager marketing Analytics\nBengaluru, Karnataka\n\nFactspan Overview: Factspan is a pure play data and analytics services organization. We partner with fortune 500 enterprises to build an analytics center of excellence, generating insights and solutions from raw data to solve business challenges, make strategic recommendations and implement new processes that help them succeed. With offices in Seattle, Washington and Bengaluru, India; we use a global delivery model to service our customers. Our customers include industry leaders from Retail, Financial Services, Hospitality, and technology sectors.",,,,"['Market Mix Modelling', 'People Management Skills', 'Marketing Analytics', 'Mmm', 'Stakeholder Management', 'Delivery Management']",2025-06-13 05:23:23
"Sr. Data Analyst – Tableau, SQL, Snowflake",Int9 Solutions,5 - 7 years,Not Disclosed,['Bengaluru'],"We are looking for a skilled Data Analyst with excellent communication skills and deep expertise in SQL, Tableau, and modern data warehousing technologies. This role involves designing data models, building insightful dashboards, ensuring data quality, and extracting meaningful insights from large datasets to support strategic business decisions.\n\nKey Responsibilities:\nWrite advanced SQL queries to retrieve and manipulate data from cloud data warehouses such as Snowflake, Redshift, or BigQuery.\nDesign and develop data models that support analytics and reporting needs.\nBuild dynamic, interactive dashboards and reports using tools like Tableau, Looker, or Domo.\nPerform advanced analytics techniques including cohort analysis, time series analysis, scenario analysis, and predictive analytics.\nValidate data accuracy and perform thorough data QA to ensure high-quality output.\nInvestigate and troubleshoot data issues; perform root cause analysis in collaboration with BI or data engineering teams.\nCommunicate analytical insights clearly and effectively to stakeholders.\n\nRequired Skills & Qualifications:\nExcellent communication skills are mandatory for this role.\n5+ years of experience in data analytics, BI analytics, or BI engineering roles.\nExpert-level skills in SQL, with experience writing complex queries and building views.\nProven experience using data visualization tools like Tableau, Looker, or Domo.\nStrong understanding of data modeling principles and best practices.\nHands-on experience working with cloud data warehouses such as Snowflake, Redshift, BigQuery, SQL Server, or Oracle.\nIntermediate-level proficiency with spreadsheet tools like Excel, Google Sheets, or Power BI, including functions, pivots, and lookups.\nBachelor's or advanced degree in a relevant field such as Data Science, Computer Science, Statistics, Mathematics, or Information Systems.\nAbility to collaborate with cross-functional teams, including BI engineers, to optimize reporting solutions.\nExperience in handling large-scale enterprise data environments.\nFamiliarity with data governance, data cataloging, and metadata management tools (a plus but not required).",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Tableau', 'Data Warehousing', 'Data Analytics', 'SQL', 'Scenario Analysis', 'Cohort Analysis', 'Data Modeling', 'Predictive Analysis', 'Redshift']",2025-06-13 05:23:25
"Sr. Data Analyst – Tableau, SQL, Snowflake",Int9 Solutions,5 - 10 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","We are looking for a skilled Data Analyst with excellent communication skills and deep expertise in SQL, Tableau, and modern data warehousing technologies. This role involves designing data models, building insightful dashboards, ensuring data quality, and extracting meaningful insights from large datasets to support strategic business decisions.\n\nKey Responsibilities:\nWrite advanced SQL queries to retrieve and manipulate data from cloud data warehouses such as Snowflake, Redshift, or BigQuery.\nDesign and develop data models that support analytics and reporting needs.\nBuild dynamic, interactive dashboards and reports using tools like Tableau, Looker, or Domo.\nPerform advanced analytics techniques including cohort analysis, time series analysis, scenario analysis, and predictive analytics.\nValidate data accuracy and perform thorough data QA to ensure high-quality output.\nInvestigate and troubleshoot data issues; perform root cause analysis in collaboration with BI or data engineering teams.\nCommunicate analytical insights clearly and effectively to stakeholders.\n\nRequired Skills & Qualifications:\nExcellent communication skills are mandatory for this role.\n5+ years of experience in data analytics, BI analytics, or BI engineering roles.\nExpert-level skills in SQL, with experience writing complex queries and building views.\nProven experience using data visualization tools like Tableau, Looker, or Domo.\nStrong understanding of data modeling principles and best practices.\nHands-on experience working with cloud data warehouses such as Snowflake, Redshift, BigQuery, SQL Server, or Oracle.\nIntermediate-level proficiency with spreadsheet tools like Excel, Google Sheets, or Power BI, including functions, pivots, and lookups.\nBachelor's or advanced degree in a relevant field such as Data Science, Computer Science, Statistics, Mathematics, or Information Systems.\nAbility to collaborate with cross-functional teams, including BI engineers, to optimize reporting solutions.\nExperience in handling large-scale enterprise data environments.\nFamiliarity with data governance, data cataloging, and metadata management tools (a plus but not required).\nLocation : - Mumbai, Delhi / NCR, Bengaluru , Kolkata, Chennai, Hyderabad, Ahmedabad, Pune, Remote",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Tableau', 'SQL', 'BI Tools', 'Scenario Analysis', 'Cohort Analysis', 'Data Warehousing', 'SQL Server', 'Data Modeling', 'Data Analytics', 'Predictive Analysis', 'Redshift']",2025-06-13 05:23:27
"Senior Manager- Middle and Back Office Data Analyst- ISS,",Fidelity International,10 - 15 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Title: Middle and Back Office Data Analyst - ISS Data (Senior Manager)\nDepartment: Technology\nLocation: Bangalore & Gurgaon (hybrid / flexible working permitted)\nReports To: Middle and Back Office Data Product Owner\nLevel: Senior Manager\nWe re proud to have been helping our clients build better financial futures for over 50 years. How have we achieved this? By working together - and supporting each other - all over the world. So, join our [insert name of team/ business area] team and feel like you re part of something bigger.\nAbout your team\nThe Technology function provides IT services that are integral to running an efficient run-the business operating model and providing change-driven solutions to meet outcomes that deliver on our business strategy. These include the development and support of business applications that underpin our revenue, operational, compliance, finance, legal, marketing and customer service functions. The broader organisation incorporates Infrastructure services that the firm relies on to operate on a day-to-day basis including data centre, networks, proximity services, security, voice, incident management and remediation.\nThe ISS Technology group is responsible for providing Technology solutions to the Investment Solutions & Services (ISS) business (which covers Investment Management, Asset Management Operations & Distribution business units globally)\n\nThe ISS Technology team supports and enhances existing applications as well as designs, builds and procures new solutions to meet requirements and enable the evolving business strategy.\nAs part of this group, a dedicated ISS Data Programme team has been mobilised as a key foundational programme to support the execution of the overarching ISS strategy.\nAbout your role\nThe Middle and Back Office Data Analyst role is instrumental in the creation and execution of a future state design for Fund Servicing & Oversight data across Fidelity s key business areas. The successful candidate will have an in- depth knowledge of data domains that represent Middle and Back-office operations and technology.\nThe role will sit within the ISS Delivery Data Analysis chapter and fully aligned to deliver Fidelity s cross functional ISS Data Programme in Technology, and the candidate will leverage their extensive industry knowledge to build a future state platform in collaboration with Business Architecture, Data Architecture, and business stakeholders.\nThe role is to maintain strong relationships with the various business contacts to ensure a superior service to our clients.\nData Product - Requirements Definition and Delivery of Data Outcomes\nAnalysis of data product requirements to enable business outcomes, contributing to the data product roadmap\nCapture both functional and non-functional data requirements considering the data product and consumers perspectives.\nConduct workshops with both the business and tech stakeholders for requirements gathering, elicitation and walk throughs.\nResponsible for the definition of data requirements, epics and stories within the product backlog and providing analysis support throughout the SDLC.\nResponsible for supporting the UAT cycles, attaining business sign off on outcomes being delivered\nData Quality and Integrity:\nDefine data quality use cases for all the required data sets and contribute to the technical frameworks of data quality.\nAlign the functional solution with the best practice data architecture & engineering principles.\nCoordination and Communication:\nExcellent communication skills to influence technology and business stakeholders globally, attaining alignment and sign off on the requirements.\nCoordinate with internal and external stakeholders to communicate data product deliveries and the change impact to the operating model.\nAn advocate for the ISS Data Programme.\nCollaborate closely with Data Governance, Business Architecture, and Data owners etc.\nConduct workshops within the scrum teams and across business teams, effectively document the minutes and drive the actions.\nAbout you\nAt least 10 years of proven experience as a business/technical/data analyst within technology and/or business changes within the financial services /asset management industry.\nMinimum 5 years as a senior business/technical/data analyst adhering to agile methodology, delivering data solutions using industry leading data platforms such as Snowflake, State Street Alpha Data, Refinitiv Eikon, SimCorp Dimension, BlackRock Aladdin, FactSet etc.\nProven experience. of delivering data driven business outcomes using industry leading data platforms such as Snowflake.\nExcellent knowledge of data life cycle that drives Middle and Back Office capabilities such as trade execution, matching, confirmation, trade settlement, record keeping, accounting, fund & cash positions, custody, collaterals/margin movements, corporate actions , derivations and calculations such as holiday handling, portfolio turnover rates, funds of funds look through .\nIn Depth expertise in data and calculations across the investment industry covering the below.\nAsset-specific data: This includes data related to financial instruments reference data like asset specifications, maintenance records, usage history, and depreciation schedules.\nMarket data: This includes data like security prices, exchange rates, index constituents and licensing restrictions on them.\nABOR & IBOR data: This includes calculation engines covering input data sets, calculations and treatment of various instruments for ABOR and IBOR data leveraging platforms such as Simcorp, Neoxam, Invest1, Charles River, Aladdin etc. Knowledge of TPAs, how data can be structured in a unified way from heterogenous structures.\nShould possess Problem Solving, Attention to detail, Critical thinking.\nTechnical Skills: Excellent hands-on SQL, Advanced Excel, Python, ML (optional) and proven experience and knowledge of data solutions.\nKnowledge of data management, data governance, and data engineering practices\nHands on experience on data modelling techniques such as dimensional, data vault etc.\nWillingness to own and drive things, collaboration across business and tech stakeholders.\nFeel rewarded",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['IT services', 'Data analysis', 'Data management', 'Incident management', 'Scrum', 'Customer service', 'Asset management', 'SDLC', 'SQL', 'Python']",2025-06-13 05:23:29
Senior Flexera Data Analyst,Luxoft,6 - 11 years,Not Disclosed,['Gurugram'],"Internal Data Structures & Modeling\nDesign, maintain, and optimize internal data models and structures within the Flexera environment.\nMap business asset data to Flexeras normalized software models with precision and accuracy.\nEnsure accurate data classification, enrichment, and normalization to support software lifecycle tracking.\nPartner with infrastructure, operations, and IT teams to ingest and reconcile data from various internal systems.\nReporting & Analytics\nDesign and maintain reports and dashboards in Flexera or via external BI tools such as Power BI or Tableau.\nProvide analytical insights on software usage, compliance, licensing, optimization, and risk exposure.\nAutomate recurring reporting processes and ensure timely delivery to business stakeholders.\nWork closely with business users to gather requirements and translate them into meaningful reports and visualizations.\nAutomated Data Feeds & API Integrations\nDevelop and support automated data feeds using Flexera REST/SOAP APIs.\nIntegrate Flexera with enterprise tools (e.g., CMDB, SCCM, ServiceNow, ERP) to ensure reliable and consistent data flow.\nMonitor, troubleshoot, and resolve issues related to data extracts and API communication.\nImplement robust logging, alerting, and exception handling for integration pipelines.\nSkills\nMust have\nMinimum 6+ years of working with Flexera or similar software.\nFlexera Expertise: Strong hands-on experience with Flexera One, FlexNet Manager Suite, or similar tools.\nTechnical Skills:\nProficient in REST/SOAP API development and integration.\nStrong SQL skills and familiarity with data transformation/normalization concepts.\nExperience using reporting tools like Power BI, Tableau, or Excel for data visualization.\nFamiliarity with enterprise systems such as SCCM, ServiceNow, ERP, CMDBs, etc.\nProcess & Problem Solving:\nStrong analytical and troubleshooting skills for data inconsistencies and API failures.\nUnderstanding of license models, software contracts, and compliance requirements.\nNice to have\nSoft Skills: Excellent communication skills to translate technical data into business insights.\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nData Engineer with Neo4j\nData Science\nIndia\nChennai\nData Engineer with Neo4j\nData Science\nIndia\nBengaluru\nData Scientist\nData Science\nIndia\nBengaluru\nGurugram, India\nReq. VR-114544\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114544\nApply for Senior Flexera Data Analyst in Gurugram\n*",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ERP', 'neo4j', 'Analytical', 'Data structures', 'data visualization', 'SCCM', 'Licensing', 'Analytics', 'Reporting tools', 'SQL']",2025-06-13 05:23:30
Senior Flexera Data Analyst,Luxoft,6 - 11 years,Not Disclosed,['Bengaluru'],"Internal Data Structures & Modeling\nDesign, maintain, and optimize internal data models and structures within the Flexera environment.\nMap business asset data to Flexeras normalized software models with precision and accuracy.\nEnsure accurate data classification, enrichment, and normalization to support software lifecycle tracking.\nPartner with infrastructure, operations, and IT teams to ingest and reconcile data from various internal systems.\nReporting & Analytics\nDesign and maintain reports and dashboards in Flexera or via external BI tools such as Power BI or Tableau.\nProvide analytical insights on software usage, compliance, licensing, optimization, and risk exposure.\nAutomate recurring reporting processes and ensure timely delivery to business stakeholders.\nWork closely with business users to gather requirements and translate them into meaningful reports and visualizations.\nAutomated Data Feeds & API Integrations\nDevelop and support automated data feeds using Flexera REST/SOAP APIs.\nIntegrate Flexera with enterprise tools (e.g., CMDB, SCCM, ServiceNow, ERP) to ensure reliable and consistent data flow.\nMonitor, troubleshoot, and resolve issues related to data extracts and API communication.\nImplement robust logging, alerting, and exception handling for integration pipelines.\nSkills\nMust have\nMinimum 6+ years of working with Flexera or similar software.\nFlexera Expertise: Strong hands-on experience with Flexera One, FlexNet Manager Suite, or similar tools.\nTechnical Skills:\nProficient in REST/SOAP API development and integration.\nStrong SQL skills and familiarity with data transformation/normalization concepts.\nExperience using reporting tools like Power BI, Tableau, or Excel for data visualization.\nFamiliarity with enterprise systems such as SCCM, ServiceNow, ERP, CMDBs, etc.\nProcess & Problem Solving:\nStrong analytical and troubleshooting skills for data inconsistencies and API failures.\nUnderstanding of license models, software contracts, and compliance requirements.\nNice to have\nSoft Skills: Excellent communication skills to translate technical data into business insights.\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nData Engineer with Neo4j\nData Science\nIndia\nChennai\nData Engineer with Neo4j\nData Science\nIndia\nGurugram\nBusiness Analyst\nData Science\nPoland\nRemote Poland\nBengaluru, India\nReq. VR-114544\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114544\nApply for Senior Flexera Data Analyst in Bengaluru\n*",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ERP', 'neo4j', 'Analytical', 'Data structures', 'data visualization', 'SCCM', 'Licensing', 'Analytics', 'Reporting tools', 'SQL']",2025-06-13 05:23:32
Senior Flexera Data Analyst,Luxoft,6 - 11 years,Not Disclosed,['Chennai'],"Internal Data Structures & Modeling\nDesign, maintain, and optimize internal data models and structures within the Flexera environment.\nMap business asset data to Flexeras normalized software models with precision and accuracy.\nEnsure accurate data classification, enrichment, and normalization to support software lifecycle tracking.\nPartner with infrastructure, operations, and IT teams to ingest and reconcile data from various internal systems.\nReporting & Analytics\nDesign and maintain reports and dashboards in Flexera or via external BI tools such as Power BI or Tableau.\nProvide analytical insights on software usage, compliance, licensing, optimization, and risk exposure.\nAutomate recurring reporting processes and ensure timely delivery to business stakeholders.\nWork closely with business users to gather requirements and translate them into meaningful reports and visualizations.\nAutomated Data Feeds & API Integrations\nDevelop and support automated data feeds using Flexera REST/SOAP APIs.\nIntegrate Flexera with enterprise tools (e.g., CMDB, SCCM, ServiceNow, ERP) to ensure reliable and consistent data flow.\nMonitor, troubleshoot, and resolve issues related to data extracts and API communication.\nImplement robust logging, alerting, and exception handling for integration pipelines.\nSkills\nMust have\nMinimum 6+ years of working with Flexera or similar software.\nFlexera Expertise: Strong hands-on experience with Flexera One, FlexNet Manager Suite, or similar tools.\nTechnical Skills:\nProficient in REST/SOAP API development and integration.\nStrong SQL skills and familiarity with data transformation/normalization concepts.\nExperience using reporting tools like Power BI, Tableau, or Excel for data visualization.\nFamiliarity with enterprise systems such as SCCM, ServiceNow, ERP, CMDBs, etc.\nProcess & Problem Solving:\nStrong analytical and troubleshooting skills for data inconsistencies and API failures.\nUnderstanding of license models, software contracts, and compliance requirements.\nNice to have\nSoft Skills: Excellent communication skills to translate technical data into business insights.\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nData Engineer with Neo4j\nData Science\nIndia\nGurugram\nData Engineer with Neo4j\nData Science\nIndia\nBengaluru\nData Scientist\nData Science\nIndia\nBengaluru\nChennai, India\nReq. VR-114544\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114544\nApply for Senior Flexera Data Analyst in Chennai\n*",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ERP', 'neo4j', 'Analytical', 'Data structures', 'data visualization', 'SCCM', 'Licensing', 'Analytics', 'Reporting tools', 'SQL']",2025-06-13 05:23:34
Sr. Data Scientist-Stratup-Mid-Size companies Exp.@ Bangalore_Urgent,"A leader in this space, we deliver world...",8 - 13 years,Not Disclosed,['Bengaluru'],"Senior Data Scientist\n\nLocation: Onsite Bangalore\nExperience: 8+ years\n\nRole Overview\n\nWe are seeking a Senior Data Scientist with a strong foundation in machine learning, deep learning, and statistical modeling, with the ability to translate complex operational problems into scalable AI/ML solutions. In addition to core data science responsibilities, the role involves building production-ready backends in Python and contributing to end-to-end model lifecycle management. Exposure to computer vision is a plus, especially for industrial use cases like identification, intrusion detection, and anomaly detection.\n\nKey Responsibilities\n\nDevelop, validate, and deploy machine learning and deep learning models for forecasting, classification, anomaly detection, and operational optimization\nBuild backend APIs using Python (FastAPI, Flask) to serve ML/DL models in production environments\nApply advanced computer vision models (e.g., YOLO, Faster R-CNN) to object detection, intrusion detection, and visual monitoring tasks\nTranslate business problems into analytical frameworks and data science solutions\nWork with data engineering and DevOps teams to operationalize and monitor models at scale\nCollaborate with product, domain experts, and engineering teams to iterate on solution design\nContribute to technical documentation, model explainability, and reproducibility practices\n\n\nRequired Skills\n\nStrong proficiency in Python for data science and backend development\nExperience with ML/DL libraries such as scikit-learn, TensorFlow, or PyTorch\nSolid knowledge of time-series modeling, forecasting techniques, and anomaly detection\nExperience building and deploying APIs for model serving (FastAPI, Flask)\nFamiliarity with real-time data pipelines using Kafka, Spark, or similar tools\nStrong understanding of model validation, feature engineering, and performance tuning\nAbility to work with SQL and NoSQL databases, and large-scale datasets\nGood communication skills and stakeholder engagement experience\n\n\nGood to Have\n\nExperience with ML model deployment tools (MLflow, Docker, Airflow)\nUnderstanding of MLOps and continuous model delivery practices\nBackground in aviation, logistics, manufacturing, or other industrial domains\nFamiliarity with edge deployment and optimization of vision models\n\n\nQualifications\n\nMasters or PhD in Data Science, Computer Science, Applied Mathematics, or related field\n7+ years of experience in machine learning and data science, including end-to-end deployment of models in production",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scikit-learn', 'time-series modeling', 'ML/DL libraries', 'data science', 'Python', 'Airflow', 'Kafka', 'MLflow', 'logistics', 'anomaly detection', 'aviation', 'SQL', 'PyTorch', 'NoSQL', 'MLOps', 'forecasting techniques', 'Docker', 'manufacturing', 'FastAPI', 'Spark', 'TensorFlow', 'Flask']",2025-06-13 05:23:36
Senior Data Scientist,Cradlepoint,3 - 8 years,Not Disclosed,['Bengaluru'],"Join our Team\nAbout this Opportunity\nThe complexity of running and optimizing the next generation of wireless networks, such as 5G with distributed edge compute, will require Machine Learning (ML) and Artificial Intelligence (AI) technologies. Ericsson is setting up an AI Accelerator Hub in India to fast-track our strategy execution, using Machine Intelligence (MI) to drive thought leadership, automate, and transform Ericsson s offerings and operations. We collaborate with academia and industry to develop state-of-the-art solutions that simplify and automate processes, creating new value through data insights.\nWhat you will do\nAs a Senior Data Scientist, you will apply your knowledge of data science and ML tools backed with strong programming skills to solve real-world problems.\nResponsibilities:\n1. Lead AI/ML features/capabilities in product/business areas\n2. Define business metrics of success for AI/ML projects and translate them into model metrics\n3. Lead end-to-end development and deployment of Generative AI solutions for enterprise use cases\n4. Design and implement architectures for vector search, embedding models, and RAG systems\n5. Fine-tune and evaluate large language models (LLMs) for domain-specific tasks\n6. Collaborate with stakeholders to translate vague problems into concrete Generative AI use cases\n7. Develop and deploy generative AI solutions using AWS services such as SageMaker, Bedrock, and other AWS AI tools. Provide technical expertise and guidance on implementing GenAI models and best practices within the AWS ecosystem.\n8. Develop secure, scalable, and production-grade AI pipelines\n9. Ensure ethical and responsible AI practices\n10. Mentor junior team members in GenAI frameworks and best practices\n11. Stay current with research and industry trends in Generative AI and apply cutting-edge techniques\n12. Contribute to internal AI governance, tooling frameworks, and reusable components\n13. Work with large datasets including petabytes of 4G/5G networks and IoT data\n14. Propose/select/test predictive models and other ML systems\n15. Define visualization and dashboarding requirements with business stakeholders\n16. Build proof-of-concepts for business opportunities using AI/ML\n17. Lead functional and technical analysis to define AI/ML-driven business opportunities\n18. Work with multiple data sources and apply the right feature engineering to AI models\n19. Lead studies and creative usage of new/existing data sources\nWhat you will bring\n1. Bachelors/Masters/Ph.D. in Computer Science, Data Science, AI, ML, Electrical Engineering, or related disciplines from reputed institutes\n2. 3+ years of applied ML/AI production-level experience\n3. Strong programming skills (R/Python)\n4. Proven ability to lead AI/ML projects end-to-end\n5. Strong grounding in mathematics, probability, and statistics\n6. Hands-on experience with data analysis, visualization techniques, and ML frameworks (Python, R, H2O, Keras, TensorFlow, Spark ML)\n7. Experience with semi-structured/unstructured data for AI/ML models\n8. Strong understanding of building AI models using Deep Neural Networks\n9. Experience with Big Data technologies (Hadoop, Cassandra)\n10. Ability to source and combine data from multiple sources for ML models\nPreferred Qualifications:\n1. Good communication skills in English\n2. Certifying MI MOOCs, a plus\n3. Domain knowledge in Telecommunication/IoT, a plus\n4. Experience with data visualization and dashboard creation, a plus\n5. Knowledge of Cognitive models, a plus\n6. Experience in partnering and collaborative co-creation in a global matrix organization.\nWhy join Ericsson\n\n\nWhat happens once you apply\nPrimary country and city: India (IN) || Bangalore\nReq ID: 766481",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Wireless', 'Computer science', 'Data analysis', 'cassandra', 'Neural networks', 'Artificial Intelligence', 'Machine learning', 'Telecommunication', 'data visualization', 'Python']",2025-06-13 05:23:38
Data Analyst - Senior,FedEx,4 - 7 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Role & responsibilities :\n\nAct as a technical expert on complex and specialist subject(s).\nSupport management with the analysis, interpretation and application of complex information, contributing to the achievement of divisional and corporate goals. Supports or leads projects by applying area of expertise.\nLead and implement advanced analytical processes through data/text mining, model development, and prediction to enable informed business decisions.\nApply sound analytical expertise to examine structured and unstructured data from multiple disparate sources to provide insights and recommend high-quality solutions to leadership across levels.\nPlan initiatives from concept to execution with minimal supervision and communicate results to a broad range of audiences. Develops a superior understanding of pricing and revenue management through internal and external sources to creatively solve business problems and lead the team from concept to execution of projects.\nTypically uses data, statistical and quantitative analysis, modeling, and fact-based management to drive decision-making. Provides regular expert consultative advice to senior leadership.\nEffectively shares best practices and fosters knowledge sharing across teams. Provides crossteam and cross-org consultation and supports communities of practice excellence.\n\n\n\nPreferred candidate profile\n\nRelevant experience in analytics/consulting/informatics and statistics\nKey Skills - Data and Business Analytics, Advanced Statistics and Predictive Modelling,\nStakeholder Management, Project Management\nExperience in pricing and revenue management yield management, customer segmentation analytics, revenue impact analytics, etc. is a plus\nExposure to predictive analytics, ML/ AI techniques is an added advantage\nTools - Oracle, SQL Server, Teradata, SAS, Python, Tableau/PowerBI/Spotfire\nGood to have cloud computing, big data, Azure",Industry Type: Courier / Logistics (Logistics Tech),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'Business Insights', 'Python', 'SQL', 'Power Bi', 'Business Acumen', 'Tableau']",2025-06-13 05:23:40
Data Scientist,Big Oh Tech,4 - 6 years,Not Disclosed,['Noida'],"Key Responsibilities:\n\nDesign, build, and maintain robust and scalable data pipelines to support analytics and reporting needs.\nManage and optimize data lake architectures, with a focus on Apache Atlas for metadata management, data lineage, and governance.\nIntegrate and curate data from multiple structured and unstructured sources to enable advanced analytics.\nCollaborate with data scientists and business analysts to ensure availability of clean, well-structured data.\nImplement data quality, validation, and monitoring processes across data pipelines.\nDevelop and manage Power BI datasets and data models, supporting dashboard and report creation.\nSupport data cataloging and classification using Apache Atlas for enterprise-wide discoverability and compliance.\nEnsure adherence to data security, privacy, and compliance policies.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['advanced analytics', 'metadata', 'Compliance', 'Business Analyst', 'data security', 'power bi', 'Data quality', 'Management', 'Apache', 'Monitoring']",2025-06-13 05:23:41
Python Engineer - ML/Big Query - Hyd/Chennai/Bangalore,People staffing Solutions,5 - 10 years,12-20 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Key Responsibilities:\nDesign, develop, and maintain scalable and optimized ETL pipelines using Python and SQL.\nWork with Google BigQuery and other cloud-based platforms to build data warehousing solutions.\nDevelop and deploy ML models; collaborate with Data Scientists for productionizing models.\nWrite efficient and optimized SQL queries for large-scale data processing.\nBuild APIs using Flask/Django for machine learning and data applications.\nWork with both SQL and NoSQL databases including Elasticsearch.\nImplement data ingestion using batch and streaming technologies.\nEnsure data quality, integrity, and governance across the data lifecycle.\nAutomate and optimize CI/CD pipelines for data solutions.\nCollaborate with cross-functional teams to gather data requirements and deliver solutions.\nTroubleshoot and monitor data pipelines for seamless operations.\nRequired Skills & Qualifications:\nBachelor's or Master's degree in Computer Science, Engineering, or related field.\n5+ years of experience with Python in a data engineering and/or ML context.\nStrong hands-on experience with SQL, BigQuery, and cloud data platforms (preferably GCP).\nPractical knowledge of ML concepts and experience developing ML models.\nProficiency in frameworks such as Flask and Django.\nExperience with NoSQL databases and data streaming technologies.\nSolid understanding of data modeling, warehousing, and ETL frameworks.\nFamiliarity with CI/CD tools and automation best practices.\nExcellent communication, problem-solving, and collaboration skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Django', 'Machine Learning', 'Python', 'SQL', 'Pandas', 'Numpy', 'Ml', 'Flask']",2025-06-13 05:23:43
"AI/ML TESTING-AI, ML, DEEP LEARNING, DATA MINING",Zensar,2 - 7 years,Not Disclosed,['Pune'],"Zensar Technologies is looking for AI/ML TESTING-AI, ML, DEEP LEARNING, DATA MINING, ANALYTICS AI/ML TESTING-AI, ML, DEEP LEARNING, DATA MINING, ANALYTICS to join our dynamic team and embark on a rewarding career journey\n\nDevelops and executes test plans for AI and machine learning models\n\nValidates model accuracy, fairness, performance, and edge-case behavior\n\nImplements automation tools and creates synthetic test datasets\n\nEnsures compliance with model validation protocols and documentation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Design engineering', 'deep learning', 'Technology consulting', 'Focus', 'Agile', 'Conceptualization', 'Management', 'Data mining', 'Analytics', 'Testing']",2025-06-13 05:23:45
Business Data Analyst,CGI,5 - 8 years,Not Disclosed,['Hyderabad'],"Business Data Analyst - HealthCare\n\nJob Summary\nWe are seeking an experienced and results-driven Business Data Analyst with 5+ years of hands-on experience in data analytics, visualization, and business insight generation. This role is ideal for someone who thrives at the intersection of business and datatranslating complex data sets into compelling insights, dashboards, and strategies that support decision-making across the organization.\nYou will collaborate closely with stakeholders across departments to identify business needs, design and build analytical solutions, and tell compelling data stories using advanced visualization tools.\nKey Responsibilities\nData Analytics & Insights Analyze large and complex data sets to identify trends, anomalies, and opportunities that help drive business strategy and operational efficiency.\n• Dashboard Development & Data Visualization Design, develop, and maintain interactive dashboards and visual reports using tools like Power BI, Tableau, or Looker to enable data-driven decisions.\n• Business Stakeholder Engagement Collaborate with cross-functional teams to understand business goals, define metrics, and convert ambiguous requirements into concrete analytical deliverables.\n• KPI Definition & Performance Monitoring Define, track, and report key performance indicators (KPIs), ensuring alignment with business objectives and consistent measurement across teams.\n• Data Modeling & Reporting Automation Work with data engineering and BI teams to create scalable, reusable data models and automate recurring reports and analysis processes.\n• Storytelling with Data Communicate findings through clear narratives supported by data visualizations and actionable recommendations to both technical and non-technical audiences.\n• Data Quality & Governance Ensure accuracy, consistency, and integrity of data through validation, testing, and documentation practices.\nRequired Qualifications\nBachelor’s or Master’s degree in Business, Economics, Statistics, Computer Science, Information Systems, or a related field.\n• 5+ years of professional experience in a data analyst or business analyst role with a focus on data visualization and analytics.\n• Proficiency in data visualization tools: Power BI, Tableau, Looker (at least one).\n• Strong experience in SQL and working with relational databases to extract, manipulate, and analyze data.\n• Deep understanding of business processes, KPIs, and analytical methods.\n• Excellent problem-solving skills with attention to detail and accuracy.\n• Strong communication and stakeholder management skills with the ability to explain technical concepts in a clear and business-friendly manner.\n• Experience working in Agile or fast-paced environments.\nPreferred Qualifications\nExperience working with cloud data platforms (e.g., Snowflake, BigQuery, Redshift).\n• Exposure to Python or R for data manipulation and statistical analysis.\n• Knowledge of data warehousing, dimensional modeling, or ELT/ETL processes.\n• Domain experience in Healthcare is a plus.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Bigquery', 'Snowflake', 'Data Warehousing', 'Redshift', 'Python', 'ETL']",2025-06-13 05:23:46
Data Scientist,Mastercard,4 - 8 years,Not Disclosed,['Gurugram'],"As consumer preference for digital payments continues to grow, ensuring a seamless and secure consumer experience is top of mind. Optimization Soltions team focuses on tracking of digital performance across all products and regions, understanding the factors influencing performance and the broader industry landscape. This includes delivering data-driven insights and business recommendations, engaging directly with key external stakeholders on implementing optimization solutions (new and existing), and partnering across the organization to drive alignment and ensure action is taken.\n\nThe Role:\n\nWork closely with global optimization solutions team to architect, develop, and maintain advanced reporting and data visualization capabilities on large volumes of data to support data insights and analytical needs across products, markets, and services\nThe candidate for this position will focus on Building solutions using Machine Learning and creating actionable insights to support product optimization and sales enablement.\nPrototype new algorithms, experiment, evaluate and deliver actionable insights.\nDrive the evolution of products with an impact focused on data science and engineering.\nDesigning machine learning systems and self-running artificial intelligence (AI) software to automate predictive models.\nPerform data ingestion, aggregation, and processing on high volume and high dimensionality data to drive and enable data unification and produce relevant insights.\nContinuously innovate and determine new approaches, tools, techniques & technologies to solve business problems and generate business insights & recommendations.\nApply knowledge of metrics, measurements, and benchmarking to complex and demanding solutions.\n\nAll about You\nA superior academic record at a leading university in Computer Science, Data Science, Technology, mathematics, statistics, or a related field or equivalent work experience\nExperience in data management, data mining, data analytics, data reporting, data product development and quantitative analysis\nStrong analytical skills with track record of translating data into compelling insights\nPrior experience working in a product development role.\nknowledge of ML frameworks, libraries, data structures, data modeling, and software architecture.\nproficiency in using Python/Spark, Hadoop platforms & tools (Hive, Impala, Airflow, NiFi), and SQL to build Big Data products & platforms\nExperience with Enterprise Business Intelligence Platform/Data platform ie Tableau, PowerBI is a plus.\nDemonstrated success interacting with stakeholders to understand technical needs and ensuring analyses and solutions meet their needs effectively.\nAbility to build a strong narrative on the business value of products and actively participate in sales enablement efforts.\nAble to work in a fast-paced, deadline-driven environment as part of a team and as an individual contributor.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data management', 'Data modeling', 'Information security', 'Machine learning', 'Data structures', 'Data mining', 'Business intelligence', 'SQL', 'Python']",2025-06-13 05:23:48
Data Scientist,Dynamic Yield,5 - 10 years,Not Disclosed,['Gurugram'],"Our Purpose\nMastercard powers economies and empowers people in 200+ countries and territories worldwide. Together with our customers, we re helping build a sustainable economy where everyone can prosper. We support a wide range of digital payments choices, making transactions secure, simple, smart and accessible. Our technology and innovation, partnerships and networks combine to deliver a unique set of products and services that help people, businesses and governments realize their greatest potential.\nTitle and Summary\nData Scientist\nWho is Mastercard?\nMastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships, and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.\nOur decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.\n\nOur Team:\nAs consumer preference for digital payments continues to grow, ensuring a seamless and secure consumer experience is top of mind. Optimization Soltions team focuses on tracking of digital performance across all products and regions, understanding the factors influencing performance and the broader industry landscape. This includes delivering data-driven insights and business recommendations, engaging directly with key external stakeholders on implementing optimization solutions (new and existing), and partnering across the organization to drive alignment and ensure action is taken.\nAre you excited about Data Assets and the value they bring to an organization?\nAre you an evangelist for data-driven decision-making?\nAre you motivated to be part of a team that builds large-scale Analytical Capabilities supporting end users across 6 continents?\nDo you want to be the go-to resource for data science & analytics in the company?\n\n\nThe Role:\n\nWork closely with global optimization solutions team to architect, develop, and maintain advanced reporting and data visualization capabilities on large volumes of data to support data insights and analytical needs across products, markets, and services\nThe candidate for this position will focus on Building solutions using Machine Learning and creating actionable insights to support product optimization and sales enablement.\nPrototype new algorithms, experiment, evaluate and deliver actionable insights.\nDrive the evolution of products with an impact focused on data science and engineering.\nDesigning machine learning systems and self-running artificial intelligence (AI) software to automate predictive models.\nPerform data ingestion, aggregation, and processing on high volume and high dimensionality data to drive and enable data unification and produce relevant insights.\nContinuously innovate and determine new approaches, tools, techniques & technologies to solve business problems and generate business insights & recommendations.\nApply knowledge of metrics, measurements, and benchmarking to complex and demanding solutions.\n\nAll about You\nA superior academic record at a leading university in Computer Science, Data Science, Technology, mathematics, statistics, or a related field or equivalent work experience\nExperience in data management, data mining, data analytics, data reporting, data product development and quantitative analysis\nStrong analytical skills with track record of translating data into compelling insights\nPrior experience working in a product development role.\nknowledge of ML frameworks, libraries, data structures, data modeling, and software architecture.\nproficiency in using Python/Spark, Hadoop platforms & tools (Hive, Impala, Airflow, NiFi), and SQL to build Big Data products & platforms\nExperience with Enterprise Business Intelligence Platform/Data platform i.e. Tableau, PowerBI is a plus.\nDemonstrated success interacting with stakeholders to understand technical needs and ensuring analyses and solutions meet their needs effectively.\nAbility to build a strong narrative on the business value of products and actively participate in sales enablement efforts.\nAble to work in a fast-paced, deadline-driven environment as part of a team and as an individual contributor.\nCorporate Security Responsibility\n\nAll activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:\nAbide by Mastercard s security policies and practices;\nEnsure the confidentiality and integrity of the information being accessed;\nReport any suspected information security violation or breach, and\nComplete all periodic mandatory security trainings in accordance with Mastercard s guidelines.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data management', 'Data modeling', 'Information security', 'Machine learning', 'Data structures', 'Data mining', 'Business intelligence', 'SQL', 'Python']",2025-06-13 05:23:50
Be Our Next Data Modeller!!,Zensar,5 - 10 years,Not Disclosed,"['Hyderabad', 'Delhi / NCR']","Proficiency in data modeling tools such as ER/Studio, ERwin or similar.\nDeep understanding of relational database design, normalization/denormalization, and data warehousing principles.\nExperience with SQL and working knowledge of database platforms like Oracle, SQL Server, PostgreSQL, or Snowflake.\nStrong knowledge of metadata management, data lineage, and data governance practices.\nUnderstanding of data integration, ETL processes, and data quality frameworks.\nAbility to interpret and translate complex business requirements into scalable data models.\nExcellent communication and documentation skills to collaborate with cross-functional teams.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Er Studio', 'ERwin', 'SQL', 'Snowflake.', 'Metadata Management', 'PostgreSQL', 'data lineage', 'data governance', 'data warehousing', 'SQL Server', 'Oracle']",2025-06-13 05:23:52
Associate- Referral - Decision Science / Data Science,Axtria,3 - 5 years,Not Disclosed,['Gurugram'],"Position Summary \n\nThis Requisition is for the Employee Referral Campaign.\n\nWe are seeking high-energy, driven, and innovative Data Scientists to join our Data Science Practice to develop new, specialized capabilities for Axtria, and to accelerate the company’s growth by supporting our clients’ commercial & clinical strategies.\n\n Job Responsibilities \n\nBe an Individual Contributor tothe Data Science team and solve real-world problems using cutting-edge capabilities and emerging technologies.\n\nHelp clients translate the business use cases they are trying to crack into data science solutions. Provide genuine assistance to users by advising them on how to leverage Dataiku DSS to implement data science projects, from design to production.\n\nData Source Configuration, Maintenance, Document and maintain work-instructions.\n\nDeep working onmachine learning frameworks such as TensorFlow, Caffe, Keras, SparkML\n\nExpert knowledge in Statistical and Probabilistic methods such as SVM, Decision-Trees, Clustering\n\nExpert knowledge of python data-science and math packages such as NumPy , Pandas, Sklearn\n\nProficiency in object-oriented languages (Java and/or Kotlin),Python and common machine learning frameworks(TensorFlow, NLTK, Stanford NLP, Ling Pipe etc\n\n\n Education \n\nBachelor Equivalent - Engineering\nMaster's Equivalent - Engineering\n\n Work Experience \n\nData Scientist 3-5 years of relevant experience in advanced statistical and mathematical models and predictive modeling using Python. Experience in the data science space prior relevant experience in Artificial intelligence and machine Learning algorithms for developing scalable models supervised and unsupervised techniques likeNLP and deep Learning Algorithms. Ability to build scalable models using Python, R-Studio, R Shiny, PySpark, Keras, and TensorFlow. Experience in delivering data science projects leveraging cloud infrastructure. Familiarity with cloud technology such as AWS / Azure and knowledge of AWS tools such as S3, EMR, EC2, Redshift, and Glue; viz tools like Tableau and Power BI. Relevant experience in Feature Engineering, Feature Selection, and Model Validation on Big Data. Knowledge of self-service analytics platforms such as Dataiku/ KNIME/ Alteryx will be an added advantage.\n\nML Ops Engineering 3-5 years of experience with MLOps Frameworks like Kubeflow, MLFlow, Data Robot, Airflow, etc., experience with Docker and Kubernetes, OpenShift. Prior experience in end-to-end automated ecosystems including, but not limited to, building data pipelines, developing & deploying scalable models, orchestration, scheduling, automation, and ML operations. Ability to design and implement cloud solutions and ability to build MLOps pipelines on cloud solutions (AWS, MS Azure, or GCP). Programming languages like Python, Go, Ruby, or Bash, a good understanding of Linux, knowledge of frameworks such as Keras, PyTorch, TensorFlow, etc. Ability to understand tools used by data scientists and experience with software development and test automation. Good understanding of advanced AI/ML algorithms & their applications.\n\nGen AI :Minimum of 4-6 years develop, test, and deploy Python based applications on Azure/AWS platforms.Must have basic knowledge on concepts of Generative AI / LLMs / GPT.Deep understanding of architecture and work experience on Web Technologies.Python, SQL hands-on experience.Expertise in any popular python web frameworks e.g. flask, Django etc. Familiarity with frontend technologies like HTML, JavaScript, REACT.Be an Individual Contributor in the Analytics and Development team and solve real-world problems using cutting-edge capabilities and emerging technologies based on LLM/GenAI/GPT.Can interact with client on GenAI related capabilities and use cases.",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'gpm', 'machine learning', 'python data', 'statistics', 'kubernetes', 'microsoft azure', 'numpy', 'javascript', 'sql', 'docker', 'pandas', 'tensorflow', 'java', 'django', 'predictive modeling', 'python web framework', 'mathematical modeling', 'pytorch', 'keras', 'aws', 'flask', 'advanced statistical']",2025-06-13 05:23:54
Big Data Developer,Binary Infoways,6 - 10 years,12-22 Lacs P.A.,['Hyderabad'],"AWS (EMR, S3, Glue, Airflow, RDS, Dynamodb, similar)\nCICD (Jenkins or another)\nRelational Databases experience (any)\nNo SQL databases experience (any)\nMicroservices or Domain services or API gateways or similar\nContainers (Docker, K8s, similar)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'AWS', 'Python', 'Airflow', 'Java', 'Big Data', 'EMR', 'SQL', 'Jenkins', 'Glue', 'SCALA', 'Big Data Technologies', 'Spark']",2025-06-13 05:23:55
Data Entry Job in Big Pharma Company at Borivali East in Mumbai,Big and Reputed Pharma Company of India ...,1 - 5 years,1.5-2.5 Lacs P.A.,['Mumbai (All Areas)'],"Only 1 Year+ experienced candidate in any field, who know basic Word, Excel and computer.\n\nThis is office job and you need to work on word, excel and email for the company\n\n2 Saturday and all Sunday are Holiday\n\nFor query call at 8000044060\n\nRequired Candidate profile\nOnly 1 Year+ experienced candidate in any field, who know basic Word, Excel and computer.\n\nThis is office job and you need to work on word, excel and email.\n\n2 Saturday and all Sunday are Holiday",Industry Type: Pharmaceutical & Life Sciences,Department: Administration & Facilities,"Employment Type: Full Time, Permanent","['Office Work', 'Back Office', 'Computer', 'Data Entry', 'operation', 'Backend', 'Typing', 'Excel', 'Word', 'Computer Operating', 'MS Office']",2025-06-13 05:23:57
Global IT Software Engineer Senior Manager & Chapter Lead,Boston Consulting Group,10 - 15 years,Not Disclosed,['Gurugram'],"To realize our digital transformation, we need to transform our products, experiences, processes, technology and how we operate. Delivering our clients unrivalled experience of exceptional service, value and flexibility is part of our DNA.\nWe are looking for people who are passionate about Digital Products Transformation based on modern technology stack and Gen AI with significant expertise in and Agile ways of working. To execute this transformation, we need people who take the lead in defining standards and guardrails of working and developing expertise further within the teams.\nOur transformation requires a series of efforts across HR space. Over the next few years, you will have the opportunity to lead our most important transformation programs. At the start of your journey, you will focus on talent management solutions related to Staffing and Mobility of our teams.",,,,"['IT services', 'Technical analysis', 'SOA', 'Testing tools', 'Consulting', 'Javascript', 'Performance testing', 'Release management', 'SDLC', 'Analytics']",2025-06-13 05:23:59
Senior Data Manager/ Lead,Codeforce 360,6 - 8 years,Not Disclosed,['Hyderabad'],"Job Description:\nWe are looking for a highly experienced and dynamic Senior Data Manager / Lead to oversee a team of Data Engineers and Data Scientists. This role demands a strong background in data platforms such as Snowflake and proficiency in Python, combined with excellent people management and project leadership skills. While hands-on experience in the technologies is beneficial, the primary focus of this role is on team leadership, strategic planning, and project delivery .\n\nJob Title : Senior Data Manager / Lead\nLocation: Hyderabad (Work From Office)\nShift Timing: 10AM-7PM\nKey Responsibilities:\nLead, mentor, and manage a team of Data Engineers and Data Scientists.\nOversee the design and implementation of data pipelines and analytics solutions using Snowflake and Python.\nCollaborate with cross-functional teams (product, business, engineering) to align data solutions with business goals.\nEnsure timely delivery of projects, with high quality and performance.\nConduct performance reviews, training plans, and support career development for the team.\nSet priorities, allocate resources, and manage workloads within the data team.\nDrive adoption of best practices in data management, governance, and documentation.\nEvaluate new tools and technologies relevant to data engineering and data science.\n\nRequired Skills & Qualifications:\n6+ years of experience in data-related roles, with at least 23 years in a leadership or management position.\nStrong understanding of Snowflake architecture, performance tuning, data sharing, security, etc.\nSolid knowledge of Python for data engineering or data science tasks.\nExperience in leading data migration, ETL/ELT, and analytics projects.\nAbility to translate business requirements into technical solutions.\nExcellent leadership, communication, and stakeholder management skills.\nExposure to tools like Databricks, Dataiku, Airflow, or similar platforms is a plus.\nBachelors or Master’s degree in Computer Science, Engineering, Mathematics, or a related field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Data Bricks', 'Python', 'Airflow', 'Data Migration', 'Dataiku', 'Data Warehousing', 'ETL', 'ELT', 'SQL']",2025-06-13 05:24:01
Data Science_ Lead,Rishabh Software,8 - 13 years,Not Disclosed,"['Ahmedabad', 'Bengaluru', 'Vadodara']","Job Description\n\nWith excellent analytical and problem-solving skills, you should understand business problems of the customers, translate them into scope of work and technical specifications for developing into Data Science projects. Efficiently utilize cutting edge technologies in AI, Generative AI areas and implement solutions for business problems. Good exposure technology platforms for Data Science, AI, Gen AI, cloud with implementation experience. Ability to provide end to end technical solutions leveraging latest AI, Gen AI tools, frameworks for the business problems. This Job requires the following:",,,,"['Data Science', 'gen ai', 'Computer Vision', 'Machine Learning', 'Deep Learning', 'Tensorflow', 'NLP', 'Artificial Intelligence', 'Dl', 'Python']",2025-06-13 05:24:03
Lead SDET | Open Source Data Platform (Onsite),Acceldata,5 - 10 years,Not Disclosed,['Bengaluru'],"About the Role:\nWe are looking for an experienced Lead SDET for our ODP, specializing in ensuring the quality and performance of large-scale data systems.\nIn this role, you will work closely with development and operations teams to design and execute comprehensive test strategies for Open Source Data Platform (ODP), including Hadoop, Spark, Hive, Kafka, and other related technologies. You will focus on test automation, performance tuning, and identifying bottlenecks in distributed data systems.\nYour key responsibilities will include writing test plans, creating automated test scripts, and conducting functional, regression, and performance testing. You will be responsible for identifying and resolving defects, ensuring data integrity, and improving testing processes. Strong collaboration skills are essential as you will be interacting with cross-functional teams and driving quality initiatives. Your work will directly contribute to maintaining high-quality standards for big data solutions and enhancing their reliability at scale.\n\nYou are a great fit for this role if you have\nProven expertise in Quality Engineering, with a strong background in test automation, performance testing, and defect management across multiple data platforms.\nA proactive mindset to define and implement comprehensive test strategies that ensure the highest quality standards are met.\nExperience in working with both functional and non-functional testing, with a particular focus on automated test development.\nA collaborative team player with the ability to effectively work cross-functionally with development teams to resolve issues and deliver timely fixes.\nStrong communication skills with the ability to mentor junior engineers and share knowledge to improve testing practices across the team.\nA commitment to continuous improvement, with the ability to analyze testing processes and recommend enhancements to align with industry best practices.\nAbility to quickly learn new technologies\n\nWhat we look for:\n6-10 years of hands-on experience in quality engineering and quality assurance, focusing on test automation, performance testing, and defect management across multiple data platforms\nProficiency in programming languages such as Java, Python, or Scala for writing test scripts and automating test cases with hands-on experience in developing automated tests using other test automation frameworks, ensuring robust and scalable test suites.\nProven ability to define and execute comprehensive test strategies, including writing test plans, test cases, and scripts for both functional and non-functional testing to ensure predictable delivery of high-quality products and solutions \nExperience with version control systems like Git and CI/CD tools such as Jenkins or GitLab CI to manage code changes and automate test execution within the development pipeline.\nExpertise in identifying, tracking, and resolving defects and issues, collaborating closely with developers and product teams to ensure timely fixes.\nStrong communication skills with the ability to work cross-functionally with development teams and mentor junior team members to improve testing practices and tools.\nAbility to analyze testing processes, recommend improvements and ensure the testing environment aligns with industry best practices, contributing to the overall quality of software.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Coding', 'API Testing', 'Java', 'Non Functional Testing', 'Ui Automation Testing', 'Database Testing', 'Ui Automation', 'Selenium', 'Functional Testing']",2025-06-13 05:24:05
Enterprise Data Architect - Azure / ETL,Leading Client,10 - 12 years,Not Disclosed,['Bengaluru'],"Key Responsibilities :\n\nAs an Enterprise Data Architect, you will :\n\n- Lead Data Architecture : Design, develop, and implement comprehensive enterprise data architectures, primarily leveraging Azure and Snowflake platforms.\n\n- Data Transformation & ETL : Oversee and guide complex data transformation and ETL processes for large and diverse datasets, ensuring data integrity, quality, and performance.\n\n- Customer-Centric Data Design : Specialize in designing and optimizing customer-centric datasets from various sources, including CRM, Call Center, Marketing, Offline, and Point of Sale systems.\n\n- Data Modeling : Drive the creation and maintenance of advanced data models, including Relational, Dimensional, Columnar, and Big Data models, to support analytical and operational needs.\n\n- Query Optimization : Develop, optimize, and troubleshoot complex SQL and NoSQL queries to ensure efficient data retrieval and manipulation.\n\n- Data Warehouse Management : Apply advanced data warehousing concepts to build and manage high-performing, scalable data warehouse solutions.\n\n- Tool Evaluation & Implementation : Evaluate, recommend, and implement industry-leading ETL tools such as Informatica and Unifi, ensuring best practices are followed.\n\n- Business Requirements & Analysis : Lead efforts in business requirements definition and management, structured analysis, process design, and use case documentation to translate business needs into technical specifications.\n\n- Reporting & Analytics Support : Collaborate with reporting teams, providing architectural guidance and support for reporting technologies like Tableau and PowerBI.\n\n- Software Development Practices : Apply professional software development principles and best practices to data solution delivery.\n\n- Stakeholder Collaboration : Interface effectively with sales teams and directly engage with customers to understand their data challenges and lead them to successful outcomes.\n\n- Project Management & Multi-tasking : Demonstrate exceptional organizational skills, with the ability to manage and prioritize multiple simultaneous customer projects effectively.\n\n- Strategic Thinking & Leadership : Act as a self-managed, proactive, and customer-focused leader, driving innovation and continuous improvement in data architecture.\n\nPosition Requirements :\n\nof strong experience with data transformation & ETL on large data sets.\n\n- Experience with designing customer-centric datasets (i.e., CRM, Call Center, Marketing, Offline, Point of Sale, etc.).\n\n- 5+ years of Data Modeling experience (i.e., Relational, Dimensional, Columnar, Big Data).\n\n- 5+ years of complex SQL or NoSQL experience.\n\n- Extensive experience in advanced Data Warehouse concepts.\n\n- Proven experience with industry ETL tools (i.e., Informatica, Unifi).\n\n- Solid experience with Business Requirements definition and management, structured analysis, process design, and use case documentation.\n\n- Experience with Reporting Technologies (i.e., Tableau, PowerBI).\n\n- Demonstrated experience in professional software development.\n\n- Exceptional organizational skills and ability to multi-task simultaneous different customer projects.\n\n- Strong verbal & written communication skills to interface with sales teams and lead customers to successful outcomes.\n\n- Must be self-managed, proactive, and customer-focused.\n\nTechnical Skills :\n\n- Cloud Platforms : Microsoft Azure\n\n- Data Warehousing : Snowflake\n\n- ETL Methodologies : Extensive experience in ETL processes and tools\n\n- Data Transformation : Large-scale data transformation\n\n- Data Modeling : Relational, Dimensional, Columnar, Big Data\n\n- Query Languages : Complex SQL, NoSQL\n\n- ETL Tools : Informatica, Unifi (or similar enterprise-grade tools)\n\n- Reporting & BI : Tableau, PowerBI",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'Enterprise Architect', 'Data Architect', 'Big Data', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'ETL', 'Reporting Tools', 'SQL']",2025-06-13 05:24:06
Lead Azure Data Factory (ADF),Quatrro,3 - 8 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","To handle all Data/BI responsibilities, including a major part of the work on ADF and team handling.\n  Key Responsibilities:\nData Warehouse Development:\nDesign and implement scalable and efficient data warehouse solutions.\nDevelop complex SQL Server-based solutions, including T-SQL queries, stored procedures, and performance tuning.\nOptimize SQL Server databases, develop T-SQL scripts, and improve query performance.\nETL Development and Maintenance:\nBuild and optimize ETL workflows using Azure Data Factory (ADF) for data integration from multiple sources.\nEnsure high-performance data pipelines for large-scale data processing.\nIntegrate and automate data processes using Azure Functions to extend ETL capabilities.\nCloud Integration:\nImplement cloud-native solutions leveraging Azure SQL Database, Azure Functions, and Synapse Analytics.\nSupport hybrid data integration scenarios combining on-premises and Azure services.\nData Governance and Quality:\nEstablish and maintain robust data quality frameworks and governance standards.\nEnsure consistency, accuracy, and security of data across all platforms.\nLeadership and Collaboration:\nLead a BI and data professionals team, providing mentorship and technical direction.\nPartner with stakeholders to understand business requirements and deliver data-driven solutions.\nDefine project goals, timelines, and resources for successful execution.\nShould be flexible to support multiple IT platforms\nManaging day-to-day activities Jira request, SQL execution, access request, resolving alerts, and updating Tickets",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Back office', 'Data processing', 'Support services', 'Data quality', 'Stored procedures', 'Outsourcing', 'JIRA', 'Analytics', 'SQL']",2025-06-13 05:24:08
MDM Data Analyst / Steward Lead,Gallagher Service Center (GSC),3 - 7 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\n\nThe MDM Analyst / Data Steward works closely with business stakeholders to understand and gather data requirements, develop data models and database designs, and define and implement data standards, policies, and procedures. This role also implements any rules inside of the MDM tool to improve the data, performs deduplication projects to develop golden records, and overall works towards improving the quality of data in the domain assigned.\n\nRequired skills :\nTechnical Skills: Proficiency in MDM tools and technologies such as Informatica MDM, CluedIn, or similar platforms is essential. Familiarity with data modeling, data integration, and data quality control techniques is also important. Experience with data governance platforms like Collibra and Alation can be beneficial1.\nAnalytical Skills: Strong analytical and problem-solving skills are crucial for interpreting and working with large volumes of data. The ability to translate complex business requirements into practical MDM solutions is also necessary.\nData Management: Experience in designing, implementing, and maintaining master data management systems and solutions. This includes conducting data cleansing, data auditing, and data validation activities.\nCommunication and Collaboration: Excellent communication and interpersonal skills to effectively collaborate with business stakeholders, IT teams, and other departments.\nData Governance: In-depth knowledge of data governance, data quality, and data integration principles. The ability to develop and implement data management processes and policies is essential.\nEducational Background: A Bachelor's or Master's degree in Computer Science, Information Systems, Data Science, or a related field is typically required1.\nCertifications: Certification in the MDM domain (e.g., Certified MDM Professional) can be a plus\n\nKey Skills:\nBecome the expert at the assigned domain of data\nUnderstand all source systems feeding into the MDM\nWrite documentation of stewardship for the domain\nDevelop rules and standards for the domain of data\nGenerate measures of improvement to demonstrate to the business the quality of the data\n\nWe are seeking candidates who can join immediately or within a maximum of 30 days' notice.\nMinimum of 3+ years of relevant experience is required.\nCandidates who are willing to relocate to Bangalore or are already based in Bangalore.\nCandidates should be flexible with working UK/US shifts.",Industry Type: Analytics / KPO / Research,Department: Other,"Employment Type: Full Time, Permanent","['Informatica Mdm', 'Data Modeling', 'Data Integration']",2025-06-13 05:24:09
Senior ETL Engineer/Consultant Specialist,Hsbc,3 - 6 years,Not Disclosed,['Hyderabad'],"Some careers shine brighter than others.\nIf you re looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further.\nHSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions.\nWe are currently seeking an experienced professional to join our team in the role of Consultant Specialist\nIn this role you will be\nDesign and Develop ETL Processes: Lead the design and implementation of ETL processes using all kinds of batch/streaming tools to extract, transform, and load data from various sources into GCP.\nCollaborate with stakeholders to gather requirements and ensure that ETL solutions meet business needs.\nData Pipeline Optimization: Optimize data pipelines for performance, scalability, and reliability, ensuring efficient data processing workflows.\nMonitor and troubleshoot ETL processes, proactively addressing issues and bottlenecks.\nData Integration and Management: I ntegrate data from diverse sources, including databases, APIs, and flat files, ensuring data quality and consistency.\nManage and maintain data storage solutions in GCP (e. g. , BigQuery, Cloud Storage) to support analytics and reporting.\nGCP Dataflow Development: Write Apache Beam based Dataflow Job for data extraction, transformation, and analysis, ensuring optimal performance and accuracy.\nCollaborate with data analysts and data scientists to prepare data for analysis and reporting.\nAutomation and Monitoring: Implement automation for ETL workflows using tools like Apache Airflow or Cloud Composer, enhancing efficiency and reducing manual intervention.\nSet up monitoring and alerting mechanisms to ensure the health of data pipelines and compliance with SLAs.\nData Governance and Security: Apply best practices for data governance, ensuring compliance with industry regulations (e. g. , GDPR, HIPAA) and internal policies.\nCollaborate with security teams to implement data protection measures and address vulnerabilities.\nDocumentation and Knowledge Sharing: Document ETL processes, data models, and architecture to facilitate knowledge sharing and onboarding of new team members.\nConduct training sessions and workshops to share expertise and promote best practices within the team.\n\n\n\n\n\n\n\n\n\n\n\nRequirements\n\n\n\nTo be successful in this role, you should meet the following requirements:\nEducation: Bachelor s degree in Computer Science, Information Systems, or a related field.\nExperience: Minimum of 5 years of industry experience in data engineering or ETL development, with a strong focus on Data Stage and GCP.\nProven experience in designing and managing ETL solutions, including data modeling, data warehousing, and SQL development.\nTechnical Skills: Strong knowledge of GCP services (e. g. , BigQuery, Dataflow, Cloud Storage, Pub/Sub) and their application in data engineering.\nExperience of cloud-based solutions, especially in GCP, cloud certified candidate is preferred.\nExperience and knowledge of Bigdata data processing in batch mode and streaming mode, proficient in Bigdata eco systems, e. g. Hadoop, HBase, Hive, MapReduce, Kafka, Flink, Spark, etc.\nFamiliarity with Java Python for data manipulation on Cloud/Bigdata platform.\nAnalytical Skills: Strong problem-solving skills with a keen attention to detail.\nAbility to analyze complex data sets and derive meaningful insights.\nBenefits: Competitive salary and comprehensive benefits package.\nOpportunity to work in a dynamic and collaborative environment on cutting-edge data projects.\nProfessional development opportunities to enhance your skills and advance your career.\nIf you are a passionate data engineer with expertise in ETL processes and a desire to make a significant impact within our organization, we encourage you to apply for this exciting opportunity!",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Data modeling', 'HIPAA', 'Data quality', 'Apache', 'Monitoring', 'Analytics', 'Financial services', 'Python']",2025-06-13 05:24:11
"Senior Staff Engineer, Mobile Flutter",Nagarro,10 - 15 years,Not Disclosed,[],"Total Experience 10+ years.\nStrong working experience in Dart and Flutter.\nSolid understanding of Mobile App Architecture (MVVM, BLoC, Provider, GetX).\nExperience with local databases like Sqflite or alternatives.\nHands-on experience in unit testing and test automation for Flutter apps.\nProven experience in building and deploying apps to the App Store and Google Play Store.\nFamiliarity with Git, GitHub/GitLab, CI/CD tools (eg, Jenkins, Bitrise, GitHub Actions).\nDeep knowledge of mobile app lifecycle, design principles, and clean architecture patterns (MVVM, MVC, etc)\nExpertise in mobile app performance optimization and security best practices.\nExperience in API integration , RESTful services, and handling JSON data.\nProficient in Version Control Systems like Git.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them into technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc, and ensure that all relevant best practices are followe'd.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements\n\n\nBachelor s or master s degree in computer science, Information Technology, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'github', 'Version control', 'GIT', 'MVVM', 'JSON', 'MVC', 'Unit testing', 'High level design', 'Information technology']",2025-06-13 05:24:13
