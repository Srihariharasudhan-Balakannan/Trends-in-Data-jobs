job_title,company,experience,salary,locations,description,industry,department,employment_type,skills,scraped_at
Data Engineer - Python/SQL,Wipro,3 - 5 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\n\n\n\nDo\n1. Instrumental in understanding the requirements and design of the product/ software\nDevelop software solutions by studying information needs, studying systems flow, data usage and work processes\nInvestigating problem areas followed by the software development life cycle\nFacilitate root cause analysis of the system issues and problem statement\nIdentify ideas to improve system performance and impact availability\nAnalyze client requirements and convert requirements to feasible design\nCollaborate with functional teams or systems analysts who carry out the detailed investigation into software requirements\nConferring with project managers to obtain information on software capabilities\n\n\n\n2. Perform coding and ensure optimal software/ module development\nDetermine operational feasibility by evaluating analysis, problem definition, requirements, software development and proposed software\nDevelop and automate processes for software validation by setting up and designing test cases/scenarios/usage cases, and executing these cases\nModifying software to fix errors, adapt it to new hardware, improve its performance, or upgrade interfaces.\nAnalyzing information to recommend and plan the installation of new systems or modifications of an existing system\nEnsuring that code is error free or has no bugs and test failure\nPreparing reports on programming project specifications, activities and status\nEnsure all the codes are raised as per the norm defined for project / program / account with clear description and replication patterns\nCompile timely, comprehensive and accurate documentation and reports as requested\nCoordinating with the team on daily project status and progress and documenting it\nProviding feedback on usability and serviceability, trace the result to quality risk and report it to concerned stakeholders\n\n\n\n3. Status Reporting and Customer Focus on an ongoing basis with respect to project and its execution\nCapturing all the requirements and clarifications from the client for better quality work\nTaking feedback on the regular basis to ensure smooth and on time delivery\nParticipating in continuing education and training to remain current on best practices, learn new programming languages, and better assist other team members.\nConsulting with engineering staff to evaluate software-hardware interfaces and develop specifications and performance requirements\nDocument and demonstrate solutions by developing documentation, flowcharts, layouts, diagrams, charts, code comments and clear code\nDocumenting very necessary details and reports in a formal way for proper understanding of software from client proposal to implementation\nEnsure good quality of interaction with customer w.r.t. e-mail content, fault report tracking, voice calls, business etiquette etc\nTimely Response to customer requests and no instances of complaints either internally or externally\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure\n1.Continuous Integration, Deployment & Monitoring of Software100% error free on boarding & implementation, throughput %, Adherence to the schedule/ release plan\n2.Quality & CSATOn-Time Delivery, Manage software, Troubleshoot queries,Customer experience, completion of assigned certifications for skill upgradation\n3.MIS & Reporting100% on time MIS & report generation\nMandatory Skills: Python for Insights.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'module development', 'Data Engineering', 'software development', 'software programs', 'SQL']",2025-06-12 05:22:51
Data Engineer,AMERICAN EXPRESS,2 - 4 years,13-17 Lacs P.A.,"['Gurugram', 'Delhi / NCR']","Role & responsibilities\nUnderstanding business use cases and be able to convert to technical design\nPart of a cross-disciplinary team, working closely with other data engineers, software engineers, data scientists, data managers and business partners.\nYou will be designing scalable, testable and maintainable data pipelines\nIdentify areas for data governance improvements and help to resolve data quality problems through the appropriate choice of error detection and correction, process control and improvement, or process design changes",,,,"['Spark', 'SQL', 'Python', 'Hadoop', 'Big Data']",2025-06-12 05:22:54
Data Engineer,Visa,10 - 13 years,Not Disclosed,['Bengaluru'],"Translate business requirements and source system understanding into technical solutions using Opensource Tech Stack, Big Data, Java.\nWork with business partners directly to seek clarity on requirements.\nDefine solutions in terms of components, modules, and algorithms.\nDesign, develop, document, and implement new programs and subprograms, as we'll as enhancements, modifications and corrections to existing software.\nCreate technical documentation and procedures for installation and maintenance.\nWrite Unit Tests covering known use cases using appropriate tools.\nIntegrate test frameworks in the development process.\nWork with operations to get the solutions deployed.\nTake ownership of production deployment of code.\nCome up with Coding and Design best practices.\nThrive in a self-motivated, internal-innovation driven environment.\nAdapt quickly to new application knowledge and changes.\nThis is a hybrid position. Expectation of days in office will be confirmed by your Hiring Manager.\n\n\nBachelor degree in Computer Science, Electrical Engineering, Information Systems or other technical discipline.\nMinimum of 1 plus years of software development experience in Hadoop using Spark, Scala, Hive.\nExpertise in Object Oriented Programming Language Java, Python.\nExperience using CI CD Process, version control and bug tracking tools.\nResult-oriented with strong analytical and problem-solving skills.\nExperience with automation of job execution, validation and comparison of data files on Hadoop Environment at the field level.\nExperience in working with a small team and being a team player.\nStrong communication skills with proven ability to present complex ideas and document them in a clear and concise way.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Electrical engineering', 'Automation', 'Version control', 'Coding', 'Analytical', 'SCALA', 'Object oriented programming', 'Python', 'Technical documentation']",2025-06-12 05:22:56
Data Engineer,PwC India,4 - 8 years,Not Disclosed,['Bengaluru'],"If Interested please fill the below application link : https://forms.office.com/r/Zc8wDfEGEH\n\n\nResponsibilities:\nDeliver projects integrating data flows within and across technology systems.\nLead data modeling sessions with end user groups, project stakeholders, and technology teams to produce logical and physical data models.",,,,"['Pyspark', 'Aws Cloud', 'Azure Cloud', 'Python']",2025-06-12 05:22:58
Data Engineer,AMERICAN EXPRESS,3 - 8 years,Not Disclosed,['Chennai'],"You Lead the Way. We've Got Your Back.\n\nWith the right backing, people and businesses have the power to progress in incredible ways. When you join Team Amex, you become part of a global and diverse community of colleagues with an unwavering commitment to back our customers, communities and each other. Here, youll learn and grow as we help you create a career journey thats unique and meaningful to you with benefits, programs, and flexibility that support you personally and professionally.\nAt American Express, you’ll be recognized for your contributions, leadership, and impact—every colleague has the opportunity to share in the company’s success. Together, we’ll win as a team, striving to uphold our company values and powerful backing promise to provide the world’s best customer experience every day. And we’ll do it with the utmost integrity, and in an environment where everyone is seen, heard and feels like they belong. As part of our diverse tech team, you can architect, code and ship software that makes us an essential part of our customers’ digital lives. Here, you can work alongside talented engineers in an open, supportive, inclusive environment where your voice is valued, and you make your own decisions on what tech to use to solve challenging problems. Amex offers a range of opportunities to work with the latest technologies and encourages you to back the broader engineering community through open source. And because we understand the importance of keeping your skills fresh and relevant, we give you dedicated time to invest in your professional development. Find your place in technology on #TeamAmex.",,,,"['Data Engineering', 'GCP', 'Airflow', 'Pyspark', 'Bigquery', 'Hadoop', 'Big Data', 'SQL', 'Python', 'Backend Development']",2025-06-12 05:23:01
Data Engineer,HCLTech,11 - 14 years,16-27.5 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Roles and Responsibilities\nDesign, develop, and maintain large-scale data pipelines using Azure Data Factory (ADF) to extract, transform, and load data from various sources into Snowflake Data Warehouse.\nDevelop complex SQL queries to optimize database performance and troubleshoot issues in Snowflake tables.\nCollaborate with cross-functional teams to gather requirements for reporting needs and design scalable solutions using Power BI.\nEnsure high-quality data modeling by creating logical and physical models for large datasets.\nTroubleshoot technical issues related to ETL processes, data quality, and performance tuning.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Power Bi', 'Snowflake', 'Azure Data Lake', 'SQL', 'Data Modeling', 'Data Warehousing']",2025-06-12 05:23:03
Data Engineer - Hadoop,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Wipro Limited (NYSEWIT, BSE507685, NSEWIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients’ most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.\n About The Role  _x000D_\n\n \n\nRole Purpose  \nThe purpose of the role is to support process delivery by ensuring daily performance of the Production Specialists, resolve technical escalations and develop technical capability within the Production Specialists.\n\n\n ? _x000D_\n\n \n\nDo  \n\n\nOversee and support process by reviewing daily transactions on performance parameters\nReview performance dashboard and the scores for the team\nSupport the team in improving performance parameters by providing technical support and process guidance\nRecord, track, and document all queries received, problem-solving steps taken and total successful and unsuccessful resolutions\nEnsure standard processes and procedures are followed to resolve all client queries\nResolve client queries as per the SLA’s defined in the contract\nDevelop understanding of process/ product for the team members to facilitate better client interaction and troubleshooting\nDocument and analyze call logs to spot most occurring trends to prevent future problems\nIdentify red flags and escalate serious client issues to Team leader in cases of untimely resolution\nEnsure all product information and disclosures are given to clients before and after the call/email requests\nAvoids legal challenges by monitoring compliance with service agreements\n\n\n ? _x000D_\n\n\nHandle technical escalations through effective diagnosis and troubleshooting of client queries\nManage and resolve technical roadblocks/ escalations as per SLA and quality requirements\nIf unable to resolve the issues, timely escalate the issues to TA & SES\nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers’ and clients’ business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract SLA’s\n\n\n ? _x000D_\n\n\nBuild people capability to ensure operational excellence and maintain superior customer service levels of the existing account/client\nMentor and guide Production Specialists on improving technical knowledge\nCollate trainings to be conducted as triage to bridge the skill gaps identified through interviews with the Production Specialist\nDevelop and conduct trainings (Triages) within products for production specialist as per target\nInform client about the triages being conducted\nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nIdentify and document most common problems and recommend appropriate resolutions to the team\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\n\n ? _x000D_\n\nDeliver\nNoPerformance ParameterMeasure1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback, NSAT/ ESAT2Team ManagementProductivity, efficiency, absenteeism3Capability developmentTriages completed, Technical Test performance\n\nMandatory\n\nSkills:\nHadoop_x000D_.\n\nExperience5-8 Years_x000D_.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'data engineering', 'spark', 'troubleshooting', 'hadoop', 'cloudera', 'python', 'scala', 'big data analytics', 'oozie', 'airflow', 'pyspark', 'data warehousing', 'apache pig', 'machine learning', 'sql', 'mapreduce', 'sqoop', 'big data', 'aws', 'etl', 'hbase']",2025-06-12 05:23:05
Data Engineer,"NTT DATA, Inc.",1 - 4 years,Not Disclosed,['Bengaluru'],"Req ID: 321498\n\nWe are currently seeking a Data Engineer to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\nJob Duties""¢ Work closely with Lead Data Engineer to understand business requirements, analyse and translate these requirements into technical specifications and solution design.\n""¢ Work closely with Data modeller to ensure data models support the solution design\n""¢ Develop , test and fix ETL code using Snowflake, Fivetran, SQL, Stored proc.\n""¢ Analysis of the data and ETL for defects/service tickets (for solution in production ) raised and service tickets.\n""¢ Develop documentation and artefacts to support projects\n\nMinimum Skills Required""¢ ADF\n""¢ Fivetran (orchestration & integration)\n""¢ SQL\n""¢ Snowflake DWH",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'data engineering', 'sql', 'oracle adf', 'etl', 'hive', 'python', 'data analysis', 'oracle', 'informatica powercenter', 'amazon redshift', 'talend', 'data warehousing', 'power bi', 'plsql', 'tableau', 'data modeling', 'spark', 'etl tool', 'technical specifications', 'hadoop', 'informatica', 'unix']",2025-06-12 05:23:08
Data Engineer - Marketing,Kimberly-Clark Corporation,7 - 12 years,Not Disclosed,['Bengaluru'],"Our Data Engineers play a crucial role in designing and operationalizing transformational enterprise data solutions on Cloud Platforms, integrating Azure services, Snowflake technology, and other third-party data technologies.\nCloud Data Engineers will work closely with a multidisciplinary agile team to build high-quality data pipelines that drive analytic solutions. These solutions will generate insights from our connected data, enabling Kimberly-Clark to advance its data-driven decision-making capabilities.\nThe ideal candidate will have a deep understanding of data architecture, data engineering, data warehousing, data analysis, reporting, and data science techniques and workflows. They should be skilled in creating data products that support analytic solutions and possess proficiency in working with APIs and understanding data structures to serve them.",,,,"['Performance tuning', 'Data modeling', 'RDBMS', 'Coding', 'Flex', 'Agile', 'Data structures', 'Troubleshooting', 'SQL', 'Python']",2025-06-12 05:23:10
Data Engineer,IBM,2 - 4 years,Not Disclosed,['Bengaluru'],"Ingest new data from relational and non-relational source database systems into our warehouse. Connect data from various sources.\n\nIntegrate data from external sources to warehouse by building facts and dimensions based on the EPM data model requirements.\n\n\n\nAutomate data exchange and processing through serverless data pipelines.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience in data analysis and integration.\nExperience in data building and consuming fact and dimension tables.\nExperience in automating data integration through data pipelines.\nExperience with object-oriented programing languages such as Python.\nExperience with structured data processing languages such as SQL and Spark SQL.\nExperience with REST APIs and JSON\nExperience in IBM Cloud data processing services such as IBM Code Engine, IBM Event Streams (Apache Kafka).\nStrong understanding of Datawarehouse concepts and various data warehouse architectures\n\n\nPreferred technical and professional experience\nExperience with IBM Cloud architecture\nExperience with DevOps.\nKnowledge of Agile development methodologies\nExperience with building containerized applications and running them in serverless environments on the Cloud such as IBM Code Engine, Kubernetes, or Satellite.\nExperience with IBM Cognitive Enterprise Data Platform and CodeHub.\nExperience with data integration tools such as IBM DataStage or Informatica",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data warehousing', 'sql', 'spark', 'data warehousing concepts', 'hive', 'kubernetes', 'rest', 'data analysis', 'ibm cloud', 'datastage', 'data processing', 'data engineering', 'apache', 'cloud architecture', 'cognitive', 'devops', 'ibm datastage', 'kafka', 'json', 'agile', 'hadoop', 'informatica']",2025-06-12 05:23:12
Data Engineer,DXC Technology,3 - 5 years,Not Disclosed,['Bengaluru'],"Job Description:\nSr. Snowflake, DBT, SQL Developer:\nWe are looking for an experienced senior Backend developer (Snowflake, DBT and SQL) The person should have proven hands-on experience in Snowflake, DBT and SQL. Azure and DBT is always an added advantage as Nestle is using DBT for Transformation.\nThe overall experience of 7+ years in SQL and with minimum 3-5 years of experience in Snowflake.\nExperience with SnowFlake data warehouse\nExperience with Data Ingestion into SnowFlake such as Snowpipe and DBT\nDBT Development: Design, develop, and maintain DBT models, transformations, and SQL code to build efficient data pipelines for analytics and reporting.\nGood understanding of SnowFlake architecture and processing\nHands-on experience in Snowflake Cloud Development\nExperience in writing complicated SQLs, analyzing query performance, query tuning, database indexes partitions, and stored procedure development.\nImplementing ETL pipelines within and outside of a data warehouse using Python and Snowflakes Snow SQL\nQuerying Snowflake using SQL\ngood knowledge of SQL language and data warehousing concepts,\no Experience with technologies such as SQL Server 2008, as well as with newer ones like SSIS and stored procedures\no Designing database tables and structures.\no Creating views, functions, and stored procedures.\no Writing optimized SQL queries for integration with other applications.\no Creating database triggers for use in automation.\no Maintaining data quality and overseeing database security.\no Exceptional experience developing codes, testing for quality assurance, administering RDBMS, and monitoring of database\no Strong knowledge and experience with relational databases and database administration (indexes, optimization, etc. )\no Querying Snowflake using SQL, Experience with SQL and PLSQL, SQL query tuning, database performance tuning and data warehousing concepts, Knowledge of DBT and Azure (ADF) is desirable.\nRecruitment fraud is a scheme in which fictitious job opportunities are offered to job seekers typically through online services, such as false websites, or through unsolicited emails claiming to be from the company. These emails may request recipients to provide personal information or to make payments as part of their illegitimate recruiting process. DXC does not make offers of employment via social media networks and DXC never asks for any money or payments from applicants at any point in the recruitment process, nor ask a job seeker to purchase IT or other equipment on our behalf. More information on employment scams is available here .",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Automation', 'Manager Quality Assurance', 'RDBMS', 'Database administration', 'Stored procedures', 'SSIS', 'Analytics', 'Monitoring', 'Python']",2025-06-12 05:23:15
Data Engineer-Hive/Impala/Kudo,IBM,15 - 20 years,Not Disclosed,['Mumbai'],"Location Mumbai\n\n Role Overview :\nAs a Big Data Engineer, you'll design and build robust data pipelines on Cloudera using Spark (Scala/PySpark) for ingestion, transformation, and processing of high-volume data from banking systems.\n\n Key Responsibilities :\nBuild scalable batch and real-time ETL pipelines using Spark and Hive\nIntegrate structured and unstructured data sources\nPerform performance tuning and code optimization\nSupport orchestration and job scheduling (NiFi, Airflow)\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience3–15 years\nProficiency in PySpark/Scala with Hive/Impala\nExperience with data partitioning, bucketing, and optimization\nFamiliarity with Kafka, Iceberg, NiFi is a must\nKnowledge of banking or financial datasets is a plus",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'scala', 'impala', 'apache nifi', 'kafka', 'cloudera', 'amazon redshift', 'pyspark', 'data warehousing', 'sql', 'etl pipelines', 'apache', 'spark', 'linux', 'hadoop', 'big data', 'etl', 'hbase', 'python', 'performance tuning', 'oozie', 'airflow', 'data engineering', 'nosql', 'mapreduce', 'sqoop', 'aws', 'unix']",2025-06-12 05:23:18
Big Data Developer/Data Engineer,Grid Dynamics,5 - 10 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\nExperience: 5 - 8 years\nEmployment Type: Full-Time\n\nJob Summary:\nWe are looking for a highly skilled Scala and Spark Developer to join our data engineering team. The ideal candidate will have strong experience in building scalable data processing solutions using Apache Spark and writing robust, high-performance applications in Scala. You will work closely with data scientists, data analysts, and product teams to design, develop, and optimize large-scale data pipelines and ETL workflows.\n\nKey Responsibilities:\nDevelop and maintain scalable data processing pipelines using Apache Spark and Scala.\nWork on batch and real-time data processing using Spark (RDD/DataFrame/Dataset).\nWrite efficient and maintainable code following best practices and coding standards.\nCollaborate with cross-functional teams to understand data requirements and implement solutions.\nOptimize performance of Spark jobs and troubleshoot data-related issues.\nIntegrate data from multiple sources and ensure data quality and consistency.\nParticipate in design reviews, code reviews, and provide technical leadership when needed.\nContribute to data modeling, schema design, and architecture discussions.\nRequired Skills:\nStrong programming skills in Scala.\nExpertise in Apache Spark (Core, SQL, Streaming).\nHands-on experience with distributed computing and large-scale data processing.\nExperience with data formats like Parquet, Avro, ORC, and JSON.\nGood understanding of functional programming concepts.\nFamiliarity with data ingestion tools (Kafka, Flume, Sqoop, etc.).\nExperience working with Hadoop ecosystem (HDFS, Hive, YARN, etc.) is a plus.\nStrong SQL skills and experience working with relational and NoSQL databases.\nExperience with version control tools like Git.\nPreferred Qualifications:\nBachelor's or Masters degree in Computer Science, Engineering, or related field.\nExperience with cloud platforms like AWS, Azure, or GCP (especially EMR, Databricks, etc.).\nKnowledge of containerization (Docker, Kubernetes) is a plus.\nFamiliarity with CI/CD tools and DevOps practices.ndidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Scala', 'Pyspark', 'Spark']",2025-06-12 05:23:20
Data Engineer,"NTT DATA, Inc.",4 - 9 years,Not Disclosed,['Chennai'],"Req ID: 324631\n\nWe are currently seeking a Data Engineer to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\n\nKey Responsibilities:\n\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\n\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\n\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\n\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\n\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\n\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\n\nAdhere to Agile practices throughout the solution development process.\n\nDesign, build, and deploy databases and data stores to support organizational requirements.\n\nBasic Qualifications:\n\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\n\nExperience with Informatica, Python, Databricks, Azure Data Engineer\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\n\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\n\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\n\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\n\nUndergraduate or Graduate degree preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'nosql', 'elastic search', 'cassandra', 'solr', 'core data', 'cloudera', 'python', 'data analytics', 'scala', 'kudu', 'microsoft azure', 'data engineering', 'data bricks', 'gen', 'java', 'apache nifi', 'spark', 'gcp', 'kafka', 'software engineering', 'hadoop', 'aws', 'data integration']",2025-06-12 05:23:22
Starburst Data Engineer/ Architect,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Starburst Data Engineer/Architect \nExpertise in Starburst and policy management like Ranger or equivalent.\nIn-depth knowledge of data modelling principles and techniques, including relational and dimensional.\nExcellent problem solving skills and the ability to troubleshoot and debug complex data related issues.\nStrong awareness of data tools and platforms like: Starburst, Snowflakes, Databricks and programming languages like SQL.\nIn-depth knowledge of data management principles, methodologies, and best practices with excellent analytical, problem-solving and decision making skills.\nDevelop, implement and maintain database systems using SQL.\nWrite complex SQL queries for integration with applications.\nDevelop and maintain data models (Conceptual, physical and logical) to meet organisational needs.\n\n\n ? \n\nDo\n\n1. Managing the technical scope of the project in line with the requirements at all stages\n\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\n\nb. Develop record management process and policies\n\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\n\nd. Providing sales data, proposals, data insights and account reviews to the client base\n\ne. Identify areas to increase efficiency and automation of processes\n\nf. Set up and maintain automated data processes\n\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\n\nh. Produce and track key performance indicators\n\n\n\n2. Analyze the data sets and provide adequate information\n\na. Liaise with internal and external clients to fully understand data content\n\nb. Design and carry out surveys and analyze survey data as per the customer requirement\n\nc. Analyze and interpret complex data sets relating to customer??s business and prepare reports for internal and external audiences using business analytics reporting tools\n\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\n\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\n\nf. Develop predictive models and share insights with the clients as per their requirement\n\n ? \n\nDeliver\nNoPerformance ParameterMeasure1.Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\n\n ? \n\n ? \nMandatory\n\nSkills:\nStartburst.\n\nExperience5-8 Years.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data management', 'sql', 'data bricks', 'data modeling', 'policy management', 'hive', 'snowflake', 'python', 'data mining', 'data warehousing', 'power bi', 'dbms', 'data architecture', 'sql server', 'plsql', 'tableau', 'unix shell scripting', 'spark', 'hadoop', 'etl', 'ssis', 'data integration', 'informatica']",2025-06-12 05:23:25
Data Engineer,"NTT DATA, Inc.",4 - 9 years,Not Disclosed,['Pune'],"Req ID: 324653\n\nWe are currently seeking a Data Engineer to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nKey Responsibilities:\n\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\n\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\n\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\n\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\n\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\n\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\n\nAdhere to Agile practices throughout the solution development process.\n\nDesign, build, and deploy databases and data stores to support organizational requirements.\n\nBasic Qualifications:\n\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\n\nExperience with Informatica, Python, Databricks, Azure Data Engineer\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\n\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\n\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\n\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\n\nUndergraduate or Graduate degree preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'nosql', 'elastic search', 'cassandra', 'solr', 'core data', 'cloudera', 'python', 'data analytics', 'scala', 'kudu', 'microsoft azure', 'data engineering', 'data bricks', 'gen', 'java', 'apache nifi', 'spark', 'gcp', 'kafka', 'software engineering', 'hadoop', 'aws', 'data integration']",2025-06-12 05:23:28
Data Engineer,"NTT DATA, Inc.",4 - 9 years,Not Disclosed,['Pune'],"Req ID: 324609\n\nWe are currently seeking a Data Engineer to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nKey Responsibilities:\n\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\n\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\n\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\n\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\n\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\n\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\n\nAdhere to Agile practices throughout the solution development process.\n\nDesign, build, and deploy databases and data stores to support organizational requirements.\n\nBasic Qualifications:\n\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\n\nExperience with Informatica, Python, Databricks, Azure Data Engineer\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\n\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\n\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\n\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\n\nUndergraduate or Graduate degree preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'nosql', 'elastic search', 'cassandra', 'solr', 'core data', 'cloudera', 'python', 'data analytics', 'scala', 'kudu', 'microsoft azure', 'data engineering', 'data bricks', 'gen', 'java', 'apache nifi', 'spark', 'gcp', 'kafka', 'software engineering', 'hadoop', 'aws', 'data integration']",2025-06-12 05:23:30
Data Engineer,Accenture,5 - 10 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\nMust have skills :SAP HCM On Premise ABAP\n\n\nGood to have skills :SAP HCM Organizational Management\nMinimum\n\n5 year(s) of experience is required\n\n\nEducational Qualification :15 years full time education\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems. Be involved in the end-to-end data management process.\nRoles & Responsibilities:\nExpected to be an SME.\nCollaborate and manage the team to perform.\nResponsible for team decisions.\nEngage with multiple teams and contribute on key decisions.\nProvide solutions to problems for their immediate team and across multiple teams.\nDevelop and maintain data pipelines.\nEnsure data quality and integrity.\nImplement ETL processes for data migration and deployment.\nProfessional & Technical Skills:\nMust To Have Skills:Proficiency in SAP HCM On Premise ABAP.\nGood To Have Skills:Experience with SAP HCM Organizational Management.\nStrong understanding of data management principles.\nExperience in designing and implementing data solutions.\nProficient in ETL processes and data migration techniques.\nAdditional Information:\nThe candidate should have a minimum of 5 years of experience in SAP HCM On Premise ABAP.\nThis position is based at our Bengaluru office.\nA 15 years full-time education is required.\n\n\nQualifications\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data management', 'data migration', 'sap hcm', 'abap', 'etl process', 'hive', 'python', 'data analysis', 'oracle', 'data warehousing', 'data engineering', 'business intelligence', 'sql server', 'sql', 'plsql', 'data quality', 'data modeling', 'spark', 'hadoop', 'etl', 'aws', 'big data', 'informatica', 'unix']",2025-06-12 05:23:32
Data Engineer,Accenture,5 - 10 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\nMust have skills :SAP HCM On Premise ABAP\n\n\nGood to have skills :SAP HCM Organizational Management\nMinimum\n5 year(s) of experience is required\n\n\nEducational Qualification :15 years full time education\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across systems. You will play a crucial role in managing and analyzing data to support business decisions and drive data-driven insights.\nRoles & Responsibilities:\nExpected to be an SME, collaborate and manage the team to perform.\nResponsible for team decisions.\nEngage with multiple teams and contribute on key decisions.\nProvide solutions to problems for their immediate team and across multiple teams.\nDesign, develop, and maintain data solutions for data generation, collection, and processing.\nCreate data pipelines to extract, transform, and load data across systems.\nEnsure data quality and integrity throughout the data lifecycle.\nImplement ETL processes to migrate and deploy data across systems.\nProfessional & Technical Skills:\nMust To Have Skills:Proficiency in SAP HCM On Premise ABAP.\nGood To Have Skills:Experience with SAP HCM Organizational Management.\nStrong understanding of data engineering principles and best practices.\nExperience with data modeling and database design.\nHands-on experience with ETL tools and processes.\nProficient in programming languages such as ABAP, SQL, and Python.\nAdditional Information:\nThe candidate should have a minimum of 5 years of experience in SAP HCM On Premise ABAP.\nA 15 years full-time education is required.\n\n Qualifications \n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'database design', 'data quality', 'etl tool', 'etl', 'hive', 'oracle', 'data analysis', 'data warehousing', 'pyspark', 'business intelligence', 'sql server', 'plsql', 'tableau', 'spark', 'hadoop', 'informatica', 'unix', 'etl process']",2025-06-12 05:23:35
Data Engineer,Careerzgraph,2 - 4 years,4.8-7.2 Lacs P.A.,['Bengaluru( Electronic City )'],"Hands On* Experience Required (Code Level) *\nSQL Query Writing - Query Optimization -\nPython Scripting - Programming\n* Design, develop & maintain data pipelines using Python, Azure & Power BI\n* Collaborate with cross-functional teams on ETL projects\n\n\nAssistive technologies\nProvident fund\nHealth insurance",Industry Type: Recruitment / Staffing,"Department: Production, Manufacturing & Engineering","Employment Type: Full Time, Permanent","['SQL Queries', 'Optimization', 'Python', 'Azure', 'Power Bi', 'Stored Procedures', 'Optimization Techniques', 'Programming', 'ETL', 'SQL Scripting']",2025-06-12 05:23:37
Data/ML Ops Engineer,"NTT DATA, Inc.",2 - 5 years,Not Disclosed,['Bengaluru'],"Additional Career Level Description:\n\n\nKnowledge and application:\nSeasoned, experienced professional; has complete knowledge and understanding of area of specialization.\nUses evaluation, judgment, and interpretation to select right course of action.\n\n\n\nProblem solving:\nWorks on problems of diverse scope where analysis of information requires evaluation of identifiable factors.\nResolves and assesses a wide range of issues in creative ways and suggests variations in approach.\n\n\n\nInteraction:\nEnhances relationships and networks with senior internal/external partners who are not familiar with the subject matter often requiring persuasion.\nWorks with others outside of own area of expertise, with the ability to adapt style to differing audiences and often advises others on difficult matters.\n\n\n\nImpact:\nImpacts short to medium term goals through personal effort or influence over team members.\n\n\n\nAccountability:\nAccountable for own targets with work reviewed at critical points.\nWork is done independently and is reviewed at critical points.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ML Ops', 'python', 'spark', 'big data', 'data engineering', 'artificial intelligence', 'ml', 'sql']",2025-06-12 05:23:39
Data Engineer KL-BL,PureSoftware Pvt Ltd,7 - 12 years,Not Disclosed,"['Bengaluru', 'Malaysia']","Core Competences Required and Desired Attributes:\n  Bachelor's degree in computer science, Information Technology, or a related field.\nProficiency in Azure Data Factory, Azure Databricks and Unity Catalog, Azure SQL Database, and other Azure data services.\nStrong programming skills in SQL, Python and PySpark languages.\nExperience in the Asset Management domain would be preferable.\nStrong proficiency in data analysis and data modelling, with the ability to extract insights from complex data sets.\nHands-on experience in Power BI, including creating custom visuals, DAX expressions, and data modelling.\nFamiliarity with Azure Analysis Services, data modelling techniques, and optimization.\nExperience with data quality and data governance frameworks with an ability to debug, fine tune and optimise large scale data processing jobs.\nStrong analytical and problem-solving skills, with a keen eye for detail.\nExcellent communication and interpersonal skills, with the ability to work collaboratively in a team environment.\nProactive and self-motivated, with the ability to manage multiple tasks and deliver high-quality results within deadlines.\n\nRoles and Responsibilities\nCore Competences Required and Desired Attributes:\n  Bachelor's degree in computer science, Information Technology, or a related field.\nProficiency in Azure Data Factory, Azure Databricks and Unity Catalog, Azure SQL Database, and other Azure data services.\nStrong programming skills in SQL, Python and PySpark languages.\nExperience in the Asset Management domain would be preferable.\nStrong proficiency in data analysis and data modelling, with the ability to extract insights from complex data sets.\nHands-on experience in Power BI, including creating custom visuals, DAX expressions, and data modelling.\nFamiliarity with Azure Analysis Services, data modelling techniques, and optimization.\nExperience with data quality and data governance frameworks with an ability to debug, fine tune and optimise large scale data processing jobs.\nStrong analytical and problem-solving skills, with a keen eye for detail.\nExcellent communication and interpersonal skills, with the ability to work collaboratively in a team environment.\nProactive and self-motivated, with the ability to manage multiple tasks and deliver high-quality results within deadlines.",Industry Type: Not mentioned,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['azure databricks', 'python', 'data services', 'data analysis', 'modeling', 'analytical', 'languages', 'catalog', 'pyspark', 'datafactory', 'interpersonal skills', 'microsoft azure', 'power bi', 'azure data factory', 'data engineering', 'sql', 'sql azure', 'data modeling', 'azure analysis', 'programming', 'communication skills']",2025-06-12 05:23:42
Data Engineer,Lenskart,1 - 4 years,Not Disclosed,['Bengaluru'],"Key Responsibilities\nBuild and maintain scalable ETL/ELT data pipelines using Python and cloud-native tools.\nDesign and optimize data models and queries on Google BigQuery for analytical workloads.\nDevelop, schedule, and monitor workflows using orchestration tools like Apache Airflow or Cloud Composer.\nIngest and integrate data from multiple structured and semi-structured sources, including MySQL, MongoDB, APIs, and cloud storage.",,,,"['GCP', 'Bigquery', 'MySQL', 'MongoDB', 'Python']",2025-06-12 05:23:44
Lead Data Engineer,Acuity Knowledge Partners,8 - 13 years,20-25 Lacs P.A.,"['Pune', 'Bangalore Rural', 'Gurugram']","Desired Skills and experience\n9+ years of experience in software development with a focus on data projects using Python, PySpark, and associated frameworks.\nProven experience as a Data Engineer with experience in Azure cloud.\nExperience implementing solutions using Azure cloud services, Azure Data Factory, Azure Lake Gen 2, Azure Databases, Azure Data Fabric, API Gateway management, Azure Functions.",,,,"['Data Engineering', 'Python', 'Pyspark', 'ETL', 'SQL']",2025-06-12 05:23:46
Data Engineer _Technology Lead,Broadridge,6 - 10 years,Not Disclosed,['Bengaluru'],"Key Responsibilities:\nAnalyzes and solve problems using technical experience, judgment and precedents\nProvides informal guidance to new team members\nExplains complex information to others in straightforward situations\n1. Data Engineering and Modelling:\nDesign & Develop Scalable Data Pipelines: Leverage AWS technologies to design, develop, and manage end-to-end data pipelines with services like .",,,,"['Star Schema', 'Snowflake', 'AWS', 'Apache Airflow']",2025-06-12 05:23:49
Data Engineer,7dxperts,5 - 8 years,15-20 Lacs P.A.,['Bengaluru'],"Role & responsibilities\n3+ years of experience in Spark, Databricks, Hadoop, Data and ML Engineering.\n3+ Years on experience in designing architectures using AWS cloud services & Databricks.\nArchitecture, design and build Big Data Platform (Data Lake / Data Warehouse / Lake house) using Databricks services and integrating with wider AWS cloud services.\nKnowledge & experience in infrastructure as code and CI/CD pipeline to build and deploy data platform tech stack and solution.\nHands-on spark experience in supporting and developing Data Engineering (ETL/ELT) and Machine learning (ML) solutions using Python, Spark, Scala or R languages.\nDistributed system fundamentals and optimising Spark distributed computing.\nExperience in setting up batch and streams data pipeline using Databricks DLT, jobs and streams.\nUnderstand the concepts and principles of data modelling, Database, tables and can produce, maintain, and update relevant data models across multiple subject areas.\nDesign, build and test medium to complex or large-scale data pipelines (ETL/ELT) based on feeds from multiple systems using a range of different storage technologies and/or access methods, implement data quality validation and to create repeatable and reusable pipelines\nExperience in designing metadata repositories, understanding range of metadata tools and technologies to implement metadata repositories and working with metadata.\nUnderstand the concepts of build automation, implementing automation pipelines to build, test and deploy changes to higher environments.\nDefine and execute test cases, scripts and understand the role of testing and how it works.\n\nPreferred candidate profile\nBig Data technologies Databricks, Spark, Hadoop, EMR or Hortonworks.\nSolid hands-on experience in programming languages Python, Spark, SQL, Spark SQL, Spark Streaming, Hive and Presto\nExperience in different Databricks components and API like notebooks, jobs, DLT, interactive and jobs cluster, SQL warehouse, policies, secrets, dbfs, Hive Metastore, Glue Metastore, Unity Catalog and ML Flow.\nKnowledge and experience in AWS Lambda, VPC, S3, EC2, API Gateway, IAM users, roles & policies, Cognito, Application Load Balancer, Glue, Redshift, Spectrum, Athena and Kinesis.\nExperience in using source control tools like git, bit bucket or AWS code commit and automation tools like Jenkins, AWS Code build and Code deploy.\nHands-on experience in terraform and Databricks API to automate infrastructure stack.\nExperience in implementing CI/CD pipeline and ML Ops pipeline using Git, Git actions or Jenkins.\nExperience in delivering project artifacts like design documents, test cases, traceability matrix and low-level design documents.\nBuild references architectures, how-tos, and demo applications for customers.\nReady to complete certifications",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'Data Bricks', 'Python', 'ML', 'ML Engineering', 'Pyspark', 'MLops', 'Ci Cd Pipeline', 'GIT', 'Machine Learning', 'SQL']",2025-06-12 05:23:51
Data Engineer,Prohr Strategies,9 - 11 years,Not Disclosed,['Bengaluru'],"Hands-on Data Engineer with strong Databricks expertise in Git/DevOps integration, Unity Catalog governance, and performance tuning of data transformation workloads. Skilled in optimizing pipelines and ensuring secure, efficient data operations.",Industry Type: Emerging Technologies (AI/ML),Department: Data Science & Analytics,"Employment Type: Full Time, Temporary/Contractual","['Data Transformation', 'GIT', 'Azure Databricks', 'Databricks', 'Devops', 'Data Engineering', 'Governance', 'Catalog', 'Code Versioning Tools']",2025-06-12 05:23:53
Data Engineer,Bebo Technologies,4 - 9 years,Not Disclosed,['Chandigarh'],"Design, build, and maintain scalable and reliable data pipelines on Databricks, Snowflake, or equivalent cloud platforms.\nIngest and process structured, semi-structured, and unstructured data from a variety of sources including APIs, RDBMS, and file systems.\nPerform data wrangling, cleansing, transformation, and enrichment using PySpark, Pandas, NumPy, or similar libraries.\nOptimize and manage large-scale data workflows for performance, scalability, and cost-efficiency.\nWrite and optimize complex SQL queries for transformation, extraction, and reporting.\nDesign and implement efficient data models and database schemas with appropriate partitioning and indexing strategies for Data Warehouse or Data Mart.\nLeverage cloud services (e.g., AWS S3, Glue, Kinesis, Lambda) for storage, processing, and orchestration.\nUse orchestration tools like Airflow, Temporal, or AWS Step Functions to manage end-to-end workflows.\nBuild containerized solutions using Docker and manage deployment pipelines via CI/CD tools such as Azure DevOps, GitHub Actions, or Jenkins.\nCollaborate closely with data scientists, analysts, and business stakeholders to understand requirements and deliver data solutions.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'python', 'Snowflake', 'Data Bricks', 'sql']",2025-06-12 05:23:55
Data Engineer,Aqilea Softech,5 - 9 years,13-20 Lacs P.A.,"['Bangalore Rural', 'Bengaluru']","Job Title: Data Engineer\nCompany : Aqilea India(Client : H&M India)\nEmployment Type: Full Time\nLocation: Bangalore(Hybrid)\nExperience: 4.5 to 9 years\nClient : H&M India\n\nAt H&M, we welcome you to be yourself and feel like you truly belong. Help us reimagine the future of an entire industry by making everyone look, feel, and do good. We take pride in our history of making fashion accessible to everyone and led by our values we strive to build a more welcoming, inclusive, and sustainable industry. We are privileged to have more than 120,000 colleagues, in over 75 countries across the world. Thats 120 000 individuals with unique experiences, skills, and passions. At H&M, we believe everyone can make an impact, we believe in giving people responsibility and a strong sense of ownership. Our business is your business, and when you grow, we grow.\nWebsite : https://career.hm.com/\n\nWe are seeking a skilled and forward-thinking Data Engineer to join our Emerging Tech team. This role is designed for someone passionate about working with cutting-edge technologies such as AI, machine learning, IoT, and big data to turn complex data sets into actionable insights.\nAs the Data Engineer in Emerging Tech, you will be responsible for designing, implementing, and optimizing data architectures and processes that support the integration of next-generation technologies. Your role will involve working with large-scale datasets, building predictive models, and utilizing emerging tools to enable data-driven decision-making across the business. You ll collaborate with technical and business teams to uncover insights, streamline data pipelines, and ensure the best use of advanced analytics technologies.\n\nKey Responsibilities:\nDesign and build scalable data architectures and pipelines that support machine learning, analytics, and IoT initiatives.\nDevelop and optimize data models and algorithms to process and analyse large-scale, complex data sets.\nImplement data governance, security, and compliance measures to ensure high-quality\nCollaborate with cross-functional teams (engineering, product, and business) to translate business requirements into data-driven solutions.\nEvaluate, integrate, and optimize new data technologies to enhance analytics capabilities and drive business outcomes.\nApply statistical methods, machine learning models, and data visualization techniques to deliver actionable insights.\nEstablish best practices for data management, including data quality, consistency, and scalability.\nConduct analysis to identify trends, patterns, and correlations within data to support strategic business initiatives.\nStay updated on the latest trends and innovations in data technologies and emerging data management practices.\n\nSkills Required :\nBachelors or masters degree in data science, Computer Science, Engineering, Statistics, or a related field.\n4.5-9 years of experience in data engineering, data science, or a similar analytical role, with a focus on emerging technologies.\nProficiency with big data frameworks (e.g., Hadoop, Spark, Kafka) and experience with modern cloud platforms (AWS, Azure, or GCP).\nSolid skills in Python, SQL, and optionally R, along with experience using machine learning libraries such as Scikit-learn, TensorFlow, or PyTorch.\nExperience with data visualization tools (e.g., Tableau or Power BI or D3.js) to communicate insights effectively.\nFamiliarity with IoT and edge computing data architectures is a plus.\nUnderstanding of data governance, compliance, and privacy standards.\nAbility to work with both structured and unstructured data.\nExcellent problem-solving, communication, and collaboration skills, with the ability to work in a fast-paced, cross-functional team environment.\nA passion for emerging technologies and a continuous desire to learn and innovate.\nInterested Candidates can share your Resumes to mail id karthik.prakadish@aqilea.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Powerbi', 'Hadoop', 'Kafka', 'Tableau', 'Azure', 'GCP', 'Data Engineer', 'Spark', 'AWS', 'Python', 'SQL']",2025-06-12 05:23:58
Consultant - Data Engineer,Affine Analytics,5 - 8 years,Not Disclosed,['Bengaluru'],"We are looking for a highly skilled API & Pixel Tracking Integration Engineer to lead the development and deployment of server-side tracking and attribution solutions across multiple platforms. The ideal candidate brings deep expertise in CAPI integrations (Meta, Google, and other platforms), secure data handling using cryptographic techniques, and experience working within privacy-first environments like Azure Clean Rooms.\nThis role requires strong hands-on experience in Azure cloud services, OCI (Oracle Cloud Infrastructure), and marketing technology stacks including Adobe Tag Management and Pixel Management. You will work closely with engineering, analytics, and marketing teams to deliver scalable, compliant, and secure data tracking solutions that drive business insights and performance.",,,,"['Python', 'Azure Cloud technologies', 'Azure Data Factory', 'Adobe Tag Management', 'Azure security', 'ADF', 'ADLS', 'Azure Functions', 'Key Vault']",2025-06-12 05:24:00
Big Data Engineer - Hadoop,Info Origin Technologies Pvt Ltd,3 - 7 years,Not Disclosed,"['Hyderabad', 'Gurugram']","Role: Hadoop Data Engineer\nLocation: Gurgaon / Hyderabad\nWork Mode: Hybrid\nEmployment Type: Full-Time\nInterview Mode: First Video then In Person\nJob Description\nJob Overview:\nWe are looking for experienced Data Engineers proficient in Hadoop, Hive, Python, SQL, and Pyspark/Spark to join our dynamic team. Candidates will be responsible for designing, developing, and maintaining scalable big data solutions.\nKey Responsibilities:\nDevelop and optimize data pipelines for large-scale data processing.\nWork with structured and unstructured datasets to derive actionable insights.\nCollaborate with cross-functional teams to enhance data-driven decision-making.\nEnsure the performance, scalability, and reliability of data architectures.\nImplement best practices for data security and governance.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hive', 'Hadoop', 'Pyspark', 'Big Data', 'Python', 'SQL']",2025-06-12 05:24:02
Data Engineer,Talent Aspire,2 - 7 years,Not Disclosed,"['Chandigarh', 'Bengaluru', 'Remote']","As the Data Engineer, you will play a pivotal role in shaping our data infrastructure and\nexecuting against our strategy. You will ideate alongside engineering, data and our clients to\ndeploy data products with an innovative and meaningful impact to clients. You will design, build,\nand maintain scalable data pipelines and workflows on AWS. Additionally, your expertise in AI\nand machine learning will enhance our ability to deliver smarter, more predictive solutions.\nKey Responsibilities\nCollaborate with other engineers, customers to brainstorm and develop impactful data\nproducts tailored to our clients.\nLeverage AI and machine learning techniques to integrate intelligent features into our\nofferings.\nDevelop, and optimize end-to-end data pipelines on AWS\nFollow best practices in software architecture and development.\nImplement effective cost management and performance optimization strategies.\nDevelop and maintain systems using Python, SQL, PySpark, and Django for front-end\ndevelopment.\nWork directly with clients and end-users and address their data needs\nUtilize databases and tools including and not limited to, Postgres, Redshift, Airflow, and\nMongoDB to support our data ecosystem.\nLeverage AI frameworks and libraries to integrate advanced analytics into our solutions.\nQualifications\n\nExperience:\nMinimum of 3 years of experience in data engineering, software development, or\nrelated roles.\nProven track record in designing and deploying AWS cloud infrastructure\nsolutions\nAt least 2 years in data analysis and mining techniques to aid in descriptive and\ndiagnostic insights\nExtensive hands-on experience with Postgres, Redshift, Airflow, MongoDB, and\nreal-time data workflows.\n\nTechnical Skills:\nExpertise in Python, SQL, and PySpark\nStrong background in software architecture and scalable development practices.\nTableau, Metabase or similar viz tools experience\nWorking knowledge of AI frameworks and libraries is a plus.\nLeadership & Communication:\nDemonstrates ownership and accountability for delivery with a strong\ncommitment to quality.\nExcellent communication skills with a history of effective client and end-user\nengagement.\nStartup & Fintech Mindset:\nAdaptability and agility to thrive in a fast-paced, early-stage startup environment.\nPassion for fintech innovation and a strong desire to make a meaningful impact\non the future of finance.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'and PySpark', 'Django', 'AI frameworks', 'Python', 'SQL']",2025-06-12 05:24:05
Data Engineer,Wissen Infotech,5 - 10 years,8-18 Lacs P.A.,"['Bengaluru', 'Mumbai (All Areas)']","Key Responsibilities\n• Design, develop, and optimize data pipelines using Python and AWS services such as Glue, Lambda, S3, EMR, Redshift, Athena, and Kinesis.\n• Implement ETL/ELT processes to extract, transform, and load data from various sources into centralized repositories (e.g., data lakes or data warehouses).\n• Collaborate with cross-functional teams to understand business requirements and translate them into scalable data solutions.\n• Monitor, troubleshoot, and enhance data workflows for performance and cost optimization.\n• Ensure data quality and consistency by implementing validation and governance practices.\n• Work on data security best practices in compliance with organizational policies and regulations.\n• Automate repetitive data engineering tasks using Python scripts and frameworks.\n• Leverage CI/CD pipelines for deployment of data workflows on AWS.\n\nRequired Skills and Qualifications\n\n• Professional Experience: 5+ years of experience in data engineering or a related field.\n• Programming: Strong proficiency in Python, with experience in libraries like pandas, pySpark, or boto3.\n• AWS Expertise: Hands-on experience with core AWS services for data engineering, such as AWS Glue for ETL/ELT, S3 for storage.\n• Redshift or Athena for data warehousing and querying.\n• Lambda for serverless compute.\n• Kinesis or SNS/SQS for data streaming.\n• IAM Roles for security. • Databases: Proficiency in SQL and experience with relational (e.g., PostgreSQL, MySQL) and NoSQL (e.g., DynamoDB) databases. • Data Processing: Knowledge of big data frameworks (e.g., Hadoop, Spark) is a plus. • DevOps: Familiarity with CI/CD pipelines and tools like Jenkins, Git, and CodePipeline. • Version Control: Proficient with Git-based workflows. • Problem Solving: Excellent analytical and debugging skills. Optional Skills • Knowledge of data modeling and data warehouse design principles. • Experience with data visualization tools (e.g., Tableau, Power BI). • Familiarity with containerization (e.g., Docker) and orchestration (e.g., Kubernetes).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'ETL', 'AWS', 'Python', 'SQL', 'Snowflake', 'Hadoop', 'SCALA', 'Big Data', 'Spark', 'Aws Glue']",2025-06-12 05:24:07
Data Engineer,LTIMindtree,5 - 8 years,Not Disclosed,"['Noida', 'Chennai', 'Bengaluru']","1.Big Data Engineer:\n\nCompany: LTIMINDTREE\nMandotory skills - Python,Pyspark & AWS\nLocation : Noida & Pan India\nExperience : 5-8Years\nSalary: 19LPA\n\nShare your cv at Muktai.S@alphacom.in",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'AWS', 'Python']",2025-06-12 05:24:09
Data Engineer,Nemetschek,5 - 10 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']","Role & responsibilities\n5+ years in software development, with a focus on data-intensive applications, cloud solutions, and scalable data architectures.\nDevelopment experience in GoLang for building scalable and efficient data applications.\nExperience with Snowflake, Redshift, or similar data platforms including architecture, data modeling, performance optimization, and integrations.\nExperience designing and building data lakes and data warehouses, ensuring data integrity, scalability, and performance.\nProficient in developing and managing ETL pipelines, using modern tools and techniques to transform, load, and integrate data efficiently.\nExperience with high-volume event streams (such as Kafka, Kinesis) and near real-time data processing solutions for fast and accurate reporting.\nHands-on experience with Terraform for automating infrastructure deployment and configuration management in cloud environments.\nExperience with containerization technologies (Docker, Kubernetes) and orchestration.\nSolid grasp of database fundamentals (SQL, NoSQL, data modeling, performance tuning)\nExperience with CI/CD pipelines and automation tools for testing, deployment, and continuous improvement.\nExperience working in AWS cloud environments, specifically with big data solutions and serverless architectures\nAbility to mentor and guide junior engineers, fostering a culture of learning and innovation\nStrong communication skills to articulate technical concepts clearly to non-technical stakeholders.\nWHAT WE OFFER\nA young, dynamic, and innovation-oriented environment\nA wide variety of projects within different industries\nA very open and informal culture where knowledge sharing, and employee development are key.\nRoom for personal initiative, development, and growth\nRealistic career opportunities\nCompetitive package and fringe benefits.\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Golang', 'Snowflake', 'Javascript', 'ETL', 'AWS']",2025-06-12 05:24:12
Lead Data Engineer,"NTT DATA, Inc.",5 - 10 years,Not Disclosed,['Bengaluru'],"Req ID: 306669\n\nWe are currently seeking a Lead Data Engineer to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\n Position Overview  We are seeking a highly skilled and experienced Lead Data/Product Engineer to join our dynamic team. The ideal candidate will have a strong background in streaming services and AWS cloud technology, leading teams and directing engineering workloads. This is an opportunity to work on the core systems supporting multiple secondary teams, so a history in software engineering and interface design would be an advantage.\n\n\n\n Key Responsibilities  \n\nLead and direct a small team of engineers engaged in\n\n- Engineering reuseable assets for the later build of data products\n\n- Building foundational integrations with Kafka, Confluent Cloud and AWS\n\n- Integrating with a large number of upstream and downstream technologies\n\n- Providing best in class documentation for downstream teams to develop, test and run data products built using our tools\n\n- Testing our tooling, and providing a framework for downstream teams to test their utilisation of our products\n\n- Helping to deliver CI, CD and IaC for both our own tooling, and as templates for downstream teams\n\n\n\n Required Skills and Qualifications  \n\n\n\n- Bachelor's degree in Computer Science, Engineering, or related field\n\n- 5+ years of experience in data engineering\n\n- 3+ years of experience with real time (or near real time) streaming systems\n\n- 2+ years of experience leading a team of data engineers\n\n- A willingness to independently learn a high number of new technologies and to lead a team in learning new technologies\n\n- Experience in AWS cloud services, particularly Lambda, SNS, S3, and EKS, API Gateway\n\n- Strong experience with Python\n\n- Strong experience in Kafka\n\n- Excellent understanding of data streaming architectures and best practices\n\n- Strong problem-solving skills and ability to think critically\n\n- Excellent communication skills to convey complex technical concepts both directly and through documentation\n\n- Strong use of version control and proven ability to govern a team in the best practice use of version control\n\n- Strong understanding of Agile and proven ability to govern a team in the best practice use of Agile methodologies\n\n\n\n Preferred Skills and Qualifications  \n\n   \n\n- An understanding of cloud networking patterns and practises\n\n- Experience with working on a library or other long term product\n\n- Knowledge of the Flink ecosystem\n\n- Experience with terraform\n\n- Experience with CI pipelines\n\n- Ability to code in a JVM language\n\n- Understanding of GDPR and the correct handling of PII\n\n- Knowledge of technical interface design\n\n- Basic use of Docker",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'version control', 'kafka', 'cloud formation aws', 'agile', 'jvm', 'cloud services', 'api gateway', 'eks', 'data engineering', 'docker', 'sql', 'java', 'lambda expressions', 'aws cloud', 'devops', 'real time pcr', 'sns', 'terraform', 'software engineering', 'aws', 'agile methodology', 'interface design']",2025-06-12 05:24:14
Big Data Engineer,Grid Dynamics,6 - 11 years,Not Disclosed,['Bengaluru'],"NOTE: We are only looking for candidates who can join Immediately to available to join in 15 days\nExperience level- 6+ years\nLocation: Bangalore (Candidates who are currently in Bangalore can apply)\n\nQualifications we are looking for\nMaster/Bachelor degree in Computer Science, Electrical Engineering, Information Systems or other technical discipline; advanced degree preferred.\nMinimum of 7+ years of software development experience (with a concentration in data centric initiatives), with demonstrated expertise in leveraging standard development best practice methodologies.\nMinimum 4+ years of experience in Hadoop using Core Java Programming, Spark, Scala, Hive and Go lang\nExpertise in Object Oriented Programming Language Java\nExperience using CI/CD Process, version control and bug tracking tools.\nExperience in handling very large data volume in Real Time and batch mode.\nExperience with automation of job execution and validation\nStrong knowledge of Database concepts\nStrong team player.\nStrong communication skills with proven ability to present complex ideas and document in a clear and concise way.\nQuick learner; self-starter, detailed and in-depth.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Scala', 'Big data engineer', 'Spark', 'Java', 'Core Java', 'Hive', 'CI', 'Hadoop', 'Big Data', 'Java Development', 'Big Data Technologies', 'Ci/Cd']",2025-06-12 05:24:17
Data Engineer,Kanini Software Solutions,12 - 20 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","We are looking for a skilled Data Engineer to join our growing data team. The ideal candidate will be responsible for designing, building, and maintaining data pipelines and infrastructure to support data analytics and business intelligence needs. A strong foundation in cloud data platforms, data transformation tools, and programming is essential.\nKey Responsibilities:\nDesign and implement scalable data pipelines using Azure Data Lake and dbt.\nIngest and transform data from various sources including databases, APIs, flat files, JSON, and XML.",,,,"['Pyspark', 'Azure', 'Snowflake']",2025-06-12 05:24:19
Consultant - Data Engineer (with Fabric),Affine Analytics,5 - 8 years,Not Disclosed,['Bengaluru'],"We are looking for a highly skilled API & Pixel Tracking Integration Engineer to lead the development and deployment of server-side tracking and attribution solutions across multiple platforms. The ideal candidate brings deep expertise in CAPI integrations (Meta, Google, and other platforms), secure data handling using cryptographic techniques, and experience working within privacy-first environments like Azure Clean Rooms.\nThis role requires strong hands-on experience in C# development, Azure cloud services, OCI (Oracle Cloud Infrastructure), and marketing technology stacks including Adobe Tag Management and Pixel Management. You will work closely with engineering, analytics, and marketing teams to deliver scalable, compliant, and secure data tracking solutions that drive business insights and performance.",,,,"['Adobe Tag Management', 'Data Engineering', 'Azure Data Factory', 'Azure security', 'ADF', 'ADLS', 'Azure Functions', 'Meta CAPI', 'Google Enhanced Conversions', 'Key Vault', 'Cosmos DB']",2025-06-12 05:24:21
Data Engineer,IT Services Company,2 - 3 years,6-7 Lacs P.A.,['Pune'],"Data Engineer\nJob Description :\nJash Data Sciences: Letting Data Speak!\nDo you love solving real-world data problems with the latest and best techniques? And having fun while solving them in a team! Then come and join our high-energy team of passionate data people. Jash Data Sciences is the right place for you.\nWe are a cutting-edge Data Sciences and Data Engineering startup based in Pune, India. We believe in continuous learning and evolving together. And we let the data speak!\nWhat will you be doing?\nYou will be discovering trends in the data sets and developing algorithms to transform\nraw data for further analytics\nCreate Data Pipelines to bring in data from various sources, with different formats,\ntransform it, and finally load it to the target database.\nImplement ETL/ ELT processes in the cloud using tools like AirFlow, Glue, Stitch, Cloud\nData Fusion, and DataFlow.\nDesign and implement Data Lake, Data Warehouse, and Data Marts in AWS, GCP, or\nAzure using Redshift, BigQuery, PostgreSQL, etc.\nCreating efficient SQL queries and understanding query execution plans for tuning\nqueries on engines like PostgreSQL.\nPerformance tuning of OLAP/ OLTP databases by creating indices, tables, and views.\nWrite Python scripts for the orchestration of data pipelines\nHave thoughtful discussions with customers to understand their data engineering\nrequirements. Break complex requirements into smaller tasks for execution.\nWhat do we need from you?\nStrong Python coding skills with basic knowledge of algorithms/data structures and\ntheir application.\nStrong understanding of Data Engineering concepts including ETL, ELT, Data Lake, Data\nWarehousing, and Data Pipelines.\nExperience designing and implementing Data Lakes, Data Warehouses, and Data Marts\nthat support terabytes of scale data.\nA track record of implementing Data Pipelines on public cloud environments\n(AWS/GCP/Azure) is highly desirable\nA clear understanding of Database concepts like indexing, query performance\noptimization, views, and various types of schemas.\nHands-on SQL programming experience with knowledge of windowing functions,\nsubqueries, and various types of joins.\nExperience working with Big Data technologies like PySpark/ Hadoop\nA good team player with the ability to communicate with clarity\nShow us your git repo/ blog!\nQualification\n1-2 years of experience working on Data Engineering projects for Data Engineer I\n2-5 years of experience working on Data Engineering projects for Data Engineer II\n1-5 years of Hands-on Python programming experience\nBachelors/Masters' degree in Computer Science is good to have\nCourses or Certifications in the area of Data Engineering will be given a higher preference.\nCandidates who have demonstrated a drive for learning and keeping up to date with technology by continuing to do various courses/self-learning will be given high preference.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Airflow', 'Elt', 'Data Mart', 'Data Pipeline', 'ETL', 'Pyspark', 'Hadoop', 'Data Bricks', 'SQL', 'Data Fusion', 'Glue', 'GCP', 'Data Flow', 'Data Warehousing', 'Azzure', 'AWS']",2025-06-12 05:24:23
Cloud Data Engineer,PwC India,3 - 8 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Job Description:\n\nWe are seeking skilled and dynamic Cloud Data Engineers specializing in AWS, Azure, Databricks. The ideal candidate will have a strong background in data engineering, with a focus on data ingestion, transformation, and warehousing. They should also possess excellent knowledge of PySpark or Spark, and a proven ability to optimize performance in Spark job executions.\n\nKey Responsibilities:",,,,"['AWS OR Azure', 'Azure Data Engineer OR AWS Data Engineer', 'Azure', 'AWS']",2025-06-12 05:24:25
Data Engineer,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"Role Description:\nAs part of the cybersecurity organization, In this vital role you will be responsible for designing, building, and maintaining data infrastructure to support data-driven decision-making. This role involves working with large datasets, developing reports, executing data governance initiatives, and ensuring data is accessible, reliable, and efficiently managed. The role sits at the intersection of data infrastructure and business insight delivery, requiring the Data Engineer to design and build robust data pipelines while also translating data into meaningful visualizations for stakeholders across the organization. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture, ETL processes, and cybersecurity data frameworks.\nRoles & Responsibilities:\nDesign, develop, and maintain data solutions for data generation, collection, and processing.\nBe a key team member that assists in design and development of the data pipeline.\nBuild data pipelines and ensure data quality by implementing ETL processes to migrate and deploy data across systems.\nDevelop and maintain interactive dashboards and reports using tools like Tableau, ensuring data accuracy and usability\nSchedule and manage workflows the ensure pipelines run on schedule and are monitored for failures.\nCollaborate with multi-functional teams to understand data requirements and design solutions that meet business needs.\nDevelop and maintain data models, data dictionaries, and other documentation to ensure data accuracy and consistency.\nImplement data security and privacy measures to protect sensitive data.\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nCollaborate and communicate effectively with product teams.\nCollaborate with data scientists to develop pipelines that meet dynamic business needs.\nShare and discuss findings with team members practicing SAFe Agile delivery model.\n\n\nBasic Qualifications:\nMasters degree and 1 to 3 years of experience of Computer Science, IT or related field experience OR\nBachelors degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience\nPreferred Qualifications:\nHands on experience with data practices, technologies, and platforms, such as Databricks, Python, GitLab, LucidChart, etc.\nHands-on experience with data visualization and dashboarding toolsTableau, Power BI, or similar is a plus\nProficiency in data analysis tools (e.g. SQL) and experience with data sourcing tools\nExcellent problem-solving skills and the ability to work with large, complex datasets\nUnderstanding of data governance frameworks, tools, and best practices\nKnowledge of and experience with data standards (FAIR) and protection regulations and compliance requirements (e.g., GDPR, CCPA)\n\nGood-to-Have Skills:\nExperience with ETL tools and various Python packages related to data processing, machine learning model development\nStrong understanding of data modeling, data warehousing, and data integration concepts\nKnowledge of Python/R, Databricks, cloud data platforms\nExperience working in Product team's environment\nExperience working in an Agile environment\n\nProfessional Certifications:\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\n\nSoft Skills:\nInitiative to explore alternate technology and approaches to solving problems\nSkilled in breaking down problems, documenting problem statements, and estimating efforts\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to handle multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data engineering', 'data analysis', 'data modeling', 'analysis tools', 'data warehousing', 'troubleshooting', 'data architecture', 'data integration', 'etl process']",2025-06-12 05:24:28
Data Engineer,Emiza Supply Chain Services,2 - 6 years,Not Disclosed,['Mumbai (All Areas)( Vidya Vihar West )'],"Role & responsibilities\n\nKey Responsibilities\n\nDesign, build, and maintain scalable data pipelines and ETL/ELT processes.\nIntegrate data from various internal and external sources (e.g., ERP, WMS, APIs).\nOptimize and monitor data flows for performance and reliability.\nCollaborate with data analysts, software developers, and business teams to understand data requirements.\nEnsure data quality, consistency, and security across the data lifecycle.\nSupport reporting, dashboarding, and data science initiatives with clean and structured data.\nMaintain data documentation and metadata repositories.\n\nPreferred candidate profile\n\nBachelor's or Masters degree in Computer Science, Engineering, or a related field.\n2+ years of experience in a data engineering or similar role.\nStrong proficiency in SQL and working with relational databases (e.g., PostgreSQL, MySQL).\nExperience with big data technologies like Spark, Hadoop, or similar is a plus.\nHands-on experience with ETL tools (e.g., Apache Airflow, Talend, DBT).\nProficiency in Python or Scala for data processing.\nFamiliarity with cloud platforms (AWS, GCP, or Azure), especially with data services like S3, Redshift, BigQuery, etc.\nKnowledge of APIs and data integration concepts.",Industry Type: Courier / Logistics,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Etl Pipelines', 'Data Engineering', 'Hadoop', 'Cloud Platform', 'Elt', 'Data Pipeline', 'Spark', 'AWS', 'Python', 'SQL']",2025-06-12 05:24:30
Data Engineer,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will responsible for designing, building, maintaining, analyzing, and interpreting data to provide actionable insights that drive business decisions. This role involves working with large datasets, developing reports, supporting and executing data governance initiatives, and visualizing data to ensure data is accessible, reliable, and efficiently managed. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture and ETL processes.\nRoles & Responsibilities:\nDesign, develop, and maintain data solutions for data generation, collection, and processing.\nBe a key team member that assists in the design and development of the data pipeline.\nCreate data pipelines and ensure data quality by implementing ETL processes to migrate and deploy data across systems.\nContribute to the design, development, and implementation of data pipelines, ETL/ELT processes, and data integration solutions.\nTake ownership of data pipeline projects from inception to deployment, manage scope, timelines, and risks.\nCollaborate with cross-functional teams to understand data requirements and design solutions that meet business needs.\nDevelop and maintain data models, data dictionaries, and other documentation to ensure data accuracy and consistency.\nImplement data security and privacy measures to protect sensitive data.\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nCollaborate and communicate effectively with product teams.\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients.\nBasic Qualifications and Experience\nMasters degree and 1 to 3 years of experience in Computer Science, IT, or related field OR\nBachelors degree and 3 to 5 years of experience in Computer Science, IT, or related field OR\nDiploma and 7 to 9 years of experience in Computer Science, IT, or related field\nMust-Have Skills:\nHands-on experience with big data technologies and platforms, such as Databricks, Apache Spark (PySpark, SparkSQL), workflow orchestration, performance tuning on big data processing.\nProficiency in data analysis tools (e.g., SQL) and experience with data visualization tools.\nExcellent problem-solving skills and the ability to work with large, complex datasets.\nPreferred Qualifications:\nGood-to-Have Skills:\nExperience with ETL tools such as Apache Spark, and various Python packages related to data processing, machine learning model development.\nStrong understanding of data modeling, data warehousing, and data integration concepts.\nKnowledge of Python/R, Databricks, SageMaker, cloud data platforms.\nProfessional Certifications:\nCertified Data Engineer / Data Analyst (preferred on Databricks or cloud environments).\nCertified Data Scientist (preferred on Databricks or Cloud environments).\nMachine Learning Certification (preferred on Databricks or Cloud environments).\nSoft Skills:\nExcellent critical-thinking and problem-solving skills.\nStrong communication and collaboration skills.\nDemonstrated awareness of how to function in a team setting.\nDemonstrated presentation skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'SageMaker', 'R', 'data modeling', 'data warehousing', 'cloud data platforms', 'Databricks', 'ETL', 'data integration', 'Python']",2025-06-12 05:24:33
Sr. Executive Data Engineering Analytics,IndiGo,5 - 10 years,Not Disclosed,['Gurugram'],"Role & responsibilities\nDevelop in Python, create responsive dashboards, and manage large datasets.\nDesign and deploy Power BI reports based on business needs.\nApply machine learning, deep learning, and statistical analysis (e.g., classification, regression, sentiment analysis, time series).\nTranslate technical concepts for non-technical stakeholders.\nDesign and implement Big Data platform components (batch/stream processing, memory cache, SQL query layer, rule engine).\nBuild scalable data solutions.\nConduct root cause analysis, troubleshoot applications, and support configurations.\nAutomate processes and reporting within the Operations Control Center (OCC).\nCollaborate with leadership to solve business problems and define objectives.\nRecommend and implement automated solutions.\nPerform data analysis and apply statistical methods for decision-making.\n\nPreferred candidate profile\n\nEducation: Bachelors or Master’s in Computer Science, Engineering, or related field. (Mathematics/Statistics)\nExperience: 5–10 years in data engineering & analytics.\nSkills:\nPython (hands-on)\nPower BI (dashboarding)\nSQL/SSMS (data storage and extraction)\nPower Automate / Power Apps (nice to have)",Industry Type: Travel & Tourism,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Python', 'SQL', 'SSMS', 'Power Bi', 'Power Automate']",2025-06-12 05:24:35
Data Engineer,Xenonstack,2 - 5 years,Not Disclosed,['Mohali( Phase 8B Mohali )'],"At XenonStack, We committed to become the Most Value Driven Cloud Native, Platform Engineering and Decision Driven Analytics Company. Our Consulting Services and Solutions towards the Neural Company and its Key Drivers.\nXenonStacks DataOps team is looking for a Data Engineer who will be responsible for employing techniques to create and sustain structures that allow for the analysis of data while remaining familiar with dominant programming and deployment strategies in the field.\nYou should demonstrate flexibility, creativity, and the capacity to receive and utilize constructive criticism. The ideal candidate should be highly skilled in all aspects of Python, Java/Scala, SQL and analytical skills.\nJob Responsibilities:\nDevelop, construct, test and maintain Data Platform Architectures\nAlign Data Architecture with business requirements\nLiaising with co-workers and clients to elucidate the requirements for each task.\nScalable and High Performant Data Platform Infrastructure that allows big data to be accessed and analysed quickly by BI & AI Teams.\nReformulating existing frameworks to optimize their functioning.\nTransforming Raw Data into InSights for manipulation by Data Scientists.\nEnsuring that your work remains backed up and readily accessible to relevant co-workers.\nRemaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.\nRequirements:\nTechnical Requirements\nExperience of Python, Java/Scala\nGreat Statistical / SQL based Analytical Skills\nExperience of Data Analytics Architectural Design Patterns for Batch, Event Driven and Real-Time Analytics Use Cases\nUnderstanding of Data warehousing, ETL tools, machine learning, Data EPIs\nExcellent in Algorithms and Data Systems\nUnderstanding of Distributed System for Data Processing and Analytics\nFamiliarity with Popular Data Analytics Framework like Hadoop , Spark , Delta Lake , Time Series / Analytical Stores Stores.\nProfessional Attributes:\nExcellent communication skills & Attention to detail.\nAnalytical mind and problem-solving Aptitude with Strong Organizational skills & Visual Thinking.\nBenefits:\nDiscover the benefits of joining our team:\nDynamic and purposeful work culture in a people-oriented organization contributing to multi-million-dollar projects with guaranteed job security.\nOpen, authentic, and transparent communication fostering a warm work environment.\nRegular constructive feedback and exposure to diverse technologies.\nRecognition and rewards for exceptional performance achievements.\nAccess to certification courses & Skill Sessions to develop continually and refine your skills.\nAdditional allowances for team members assigned to specific projects.\nSpecial skill allowances to acknowledge and compensate for unique expertise.\nComprehensive medical insurance policy for your health and well-being.\nTo Learn more about the company -\nWebsite - http://www.xenonstack.com/",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hadoop', 'Spark', 'ETL', 'Python', 'SQL', 'Java', 'Data Processing', 'Machine Learning']",2025-06-12 05:24:38
Associate Data Engineer,"NTT DATA, Inc.",1 - 3 years,Not Disclosed,"['New Delhi', 'Chennai', 'Bengaluru']","Your day at NTT DATA\nWe are seeking an experienced Data Engineer to join our team in delivering cutting-edge Generative AI (GenAI) solutions to clients. The successful candidate will be responsible for designing, developing, and deploying data pipelines and architectures that support the training, fine-tuning, and deployment of LLMs for various industries. This role requires strong technical expertise in data engineering, problem-solving skills, and the ability to work effectively with clients and internal teams.\n\nWhat youll be doing\n\nKey Responsibilities:\nDesign, develop, and manage data pipelines and architectures to support GenAI model training, fine-tuning, and deployment\nData Ingestion and Integration: Develop data ingestion frameworks to collect data from various sources, transform, and integrate it into a unified data platform for GenAI model training and deployment.\nGenAI Model Integration: Collaborate with data scientists to integrate GenAI models into production-ready applications, ensuring seamless model deployment, monitoring, and maintenance.\nCloud Infrastructure Management: Design, implement, and manage cloud-based data infrastructure (e.g., AWS, GCP, Azure) to support large-scale GenAI workloads, ensuring cost-effectiveness, security, and compliance.\nWrite scalable, readable, and maintainable code using object-oriented programming concepts in languages like Python, and utilize libraries like Hugging Face Transformers, PyTorch, or TensorFlow\nPerformance Optimization: Optimize data pipelines, GenAI model performance, and infrastructure for scalability, efficiency, and cost-effectiveness.\nData Security and Compliance: Ensure data security, privacy, and compliance with regulatory requirements (e.g., GDPR, HIPAA) across data pipelines and GenAI applications.\nClient Collaboration: Collaborate with clients to understand their GenAI needs, design solutions, and deliver high-quality data engineering services.\nInnovation and R&D: Stay up to date with the latest GenAI trends, technologies, and innovations, applying research and development skills to improve data engineering services.\nKnowledge Sharing: Share knowledge, best practices, and expertise with team members, contributing to the growth and development of the team.\n\nBachelors degree in computer science, Engineering, or related fields (Masters recommended)\nExperience with vector databases (e.g., Pinecone, Weaviate, Faiss, Annoy) for efficient similarity search and storage of dense vectors in GenAI applications\n5+ years of experience in data engineering, with a strong emphasis on cloud environments (AWS, GCP, Azure, or Cloud Native platforms)\nProficiency in programming languages like SQL, Python, and PySpark\nStrong data architecture, data modeling, and data governance skills\nExperience with Big Data Platforms (Hadoop, Databricks, Hive, Kafka, Apache Iceberg), Data Warehouses (Teradata, Snowflake, BigQuery), and lakehouses (Delta Lake, Apache Hudi)\nKnowledge of DevOps practices, including Git workflows and CI/CD pipelines (Azure DevOps, Jenkins, GitHub Actions)\nExperience with GenAI frameworks and tools (e.g., TensorFlow, PyTorch, Keras)\nNice to have:\nExperience with containerization and orchestration tools like Docker and Kubernetes\nIntegrate vector databases and implement similarity search techniques, with a focus on GraphRAG is a plus\nFamiliarity with API gateway and service mesh architectures\nExperience with low latency/streaming, batch, and micro-batch processing\nFamiliarity with Linux-based operating systems and REST APIs",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Apache Iceberg', 'Faiss', 'PySpark', 'Kafka', 'Pinecone', 'GitHub Actions', 'Snowflake', 'Apache Hudi', 'AWS', 'Azure DevOps', 'Python', 'Azure', 'BigQuery', 'Hadoop', 'Annoy', 'Teradata', 'SQL', 'Jenkins', 'Hive', 'Cloud Native platforms', 'GCP', 'Delta Lake', 'Databricks', 'Weaviate']",2025-06-12 05:24:40
Data Engineer,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nAs part of the cybersecurity organization, the Data Engineer is responsible for designing, building, and maintaining data infrastructure to support data-driven decision-making. This role involves working with large datasets, developing reports, executing data governance initiatives, and ensuring data is accessible, reliable, and efficiently managed. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture, ETL processes, and cybersecurity data frameworks.\nRoles & Responsibilities:\nDesign, develop, and maintain data solutions for data generation, collection, and processing.\nBe a key team member that assists in design and development of the data pipeline.\nCreate data pipelines and ensure data quality by implementing ETL processes to migrate and deploy data across systems.\nSchedule and manage workflows the ensure pipelines run on schedule and are monitored for failures.\nCollaborate with cross-functional teams to understand data requirements and design solutions that meet business needs.\nDevelop and maintain data models, data dictionaries, and other documentation to ensure data accuracy and consistency.\nImplement data security and privacy measures to protect sensitive data.\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nCollaborate and communicate effectively with product teams.\nCollaborate with data scientists to develop pipelines that meet dynamic business needs.\nShare and discuss findings with team members practicing SAFe Agile delivery model.\nFunctional Skills:\nBasic Qualifications:\nMasters degree and 1 to 3 years of Computer Science, IT or related field experience OR\nBachelors degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience\nPreferred Qualifications:\nHands on experience with data practices, technologies, and platforms, such as Databricks, Python, Gitlab, LucidChart,etc.\nProficiency in data analysis tools (e.g. SQL) and experience with data sourcing tools\nExcellent problem-solving skills and the ability to work with large, complex datasets\nUnderstanding of data governance frameworks, tools, and best practices\nKnowledge of and experience with data standards (FAIR) and protection regulations and compliance requirements (e.g., GDPR, CCPA)\nGood-to-Have Skills:\nExperience with ETL tools and various Python packages related to data processing, machine learning model development\nStrong understanding of data modeling, data warehousing, and data integration concepts\nKnowledge of Python/R, Databricks, cloud data platforms\nExperience working in Product team's environment\nExperience working in an Agile environment\nProfessional Certifications:\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nSoft Skills:\nInitiative to explore alternate technology and approaches to solving problems\nSkilled in breaking down problems, documenting problem statements, and estimating efforts\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data engineering', 'data security', 'Agile', 'cloud data platforms', 'Databricks', 'data governance frameworks', 'ETL', 'AWS', 'SQL', 'Python']",2025-06-12 05:24:43
Consultant-Data Engineer (Only from Pharma/Lifescience/Biotech domain),Chryselys,6 - 11 years,Not Disclosed,['Hyderabad'],"Job Description for Consultant - Data Engineer\nAbout Us:\nChryselys is a Pharma Analytics & Business consulting company that delivers data-driven insights leveraging AI-powered, cloud-native platforms to achieve high-impact transformations.\nWe specialize in digital technologies and advanced data science techniques that provide strategic and operational insights.\nWho we are:\nPeople - Our team of industry veterans, advisors and senior strategists have diverse backgrounds and have worked at top tier companies.\nQuality - Our goal is to deliver the value of a big five consulting company without the big five cost.\nTechnology - Our solutions are Business centric built on cloud native technologies.\nKey Responsibilities and Core Competencies:\n•      You will be responsible for managing and delivering multiple Pharma projects.\n•      Leading a team of atleast 8 members, resolving their technical and business related problems and other queries.\n•      Responsible for client interaction; requirements gathering, creating required documents, development, quality assurance of the deliverables.\n•      Good collaboration with onshore and Senior folks.\n•      Should have fair understanding of Data Capabilities (Data Management, Data Quality, Master and Reference Data).\n•      Exposure to Project management methodologies including Agile and Waterfall.\n•      Experience working in RFPs would be a plus.\nRequired Technical Skills:\n•      Proficient in Python, Pyspark, SQL\n•      Extensive hands-on experience in big data processing and cloud technologies like AWS and Azure services, Databricks etc.\n•      Strong experience working with cloud data warehouses like Snowflake, Redshift, Azure etc.\n•      Good experience in ETL, Data Modelling, building ETL Pipelines.\n•      Conceptual knowledge of Relational database technologies, Data Lake, Lake Houses etc.\n•      Sound knowledge in Data operations, quality and data governance.\nPreferred Qualifications:\n•      Bachelors or master’s Engineering/ MCA or equivalent degree.\n•      6-13 years of experience as Data Engineer, with atleast 2 years in managing medium to large scale programs.\n•      Minimum 5 years of Pharma and Life Science domain exposure in IQVIA, Veeva, Symphony, IMS etc.\n•      High motivation, good work ethic, maturity, self-organized and personal initiative.\n•      Ability to work collaboratively and providing the support to the team.\n•      Excellent written and verbal communication skills.\n•      Strong analytical and problem-solving skills.\nLocation\n•      Preferably Hyderabad, India",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'AWS', 'Data Bricks', 'Python', 'SQL', 'Data Engineering', 'Microsoft Azure', 'Data Lake', 'Data Warehousing']",2025-06-12 05:24:46
Data Engineer,Trantor,5 - 10 years,Not Disclosed,[],"We are looking for a skilled and motivated Data Engineer with deep expertise in GCP,\nBigQuery, Apache Airflow to join our data platform team. The ideal candidate should have hands-on experience building scalable data pipelines, automating workflows, migrating large-scale datasets, and optimizing distributed systems. The candidate should have experience with building Web APIs using Python. This role will play a key part in designing and maintaining robust data engineering solutions across cloud and on-prem environments.\nKey Responsibilities\nBigQuery & Cloud Data Pipelines:\nDesign and implement scalable ETL pipelines for ingesting large-scale datasets.\nBuild solutions for efficient querying of tables in BigQuery.\nAutomated scheduled data ingestion using Google Cloud services and scheduled\nApache Airflow DAGs",,,,"['Airflow', 'Etl Pipelines', 'GCP', 'Bigquery', 'Python', 'SFTP', 'ETL', 'SQL']",2025-06-12 05:24:49
Data Engineer,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nDesign, develop, and maintain complex ETL/ELT data pipelines in Databricks using PySpark, Scala, and SQL to process large-scale datasets\nUnderstand the biotech/pharma or related domains & build highly efficient data pipelines to migrate and deploy complex data across systems\nDesign and Implement solutions to enable unified data access, governance, and interoperability across hybrid cloud environments\nIngest and transform structured and unstructured data from databases (PostgreSQL, MySQL, SQL Server, MongoDB etc.), APIs, logs, event streams, images, pdf, and third-party platforms\nEnsuring data integrity, accuracy, and consistency through rigorous quality checks and monitoring\nExpert in data quality, data validation and verification frameworks\nInnovate, explore and implement new tools and technologies to enhance efficient data processing\nProactively identify and implement opportunities to automate tasks and develop reusable frameworks\nWork in an Agile and Scaled Agile (SAFe) environment, collaborating with cross-functional teams, product owners, and Scrum Masters to deliver incremental value\nUse JIRA, Confluence, and Agile DevOps tools to manage sprints, backlogs, and user stories\nSupport continuous improvement, test automation, and DevOps practices in the data engineering lifecycle\nCollaborate and communicate effectively with the product teams, with cross-functional teams to understand business requirements and translate them into technical solutions\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. We are looking for highly motivated expert Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks.\nBasic Qualifications:\nMasters degree and 1 to 3 years of Computer Science, IT or related field experience OR\nBachelors degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies\nProficiency in workflow orchestration, performance tuning on big data processing\nStrong understanding of AWS services\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices\nPreferred Qualifications:\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nData Engineering experience in Biotechnology or pharma industry\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and DevOps\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data engineering', 'Maven', 'data validation', 'PySpark', 'Scala', 'APIs', 'SQL Server', 'SQL', 'Jenkins', 'Git', 'MySQL', 'troubleshooting', 'MongoDB', 'ETL']",2025-06-12 05:24:52
Data Engineer,Cloud Angles Digital Transformation,3 - 5 years,Not Disclosed,['Noida'],"Essential Functions/Responsibilities/Duties\n•       Work closely with Senior Business Intelligence engineer and BI architect to understand the schema objects and build BI reports and Dashboards\n•       Participation in sprint refinement, planning, and kick-off to understand the Agile process and Sprint priorities\n•       Develop necessary transformations and aggregate tables required for the reporting\\Dashboard needs\n•       Understand the Schema layer in MicroStrategy and business requirements\n•       Develop complex reports and Dashboards in MicroStrategy\n•       Investigate and troubleshoot issues with Dashboard and reports\n•       Proactively researching new technologies and proposing improvements to processes and tech stack\n•       Create test cases and scenarios to validate the dashboards and maintain data accuracy\nEducation and Experience\n•       3 years of experience in Business Intelligence and Data warehousing\n•       3+ years of experience in MicroStrategy Reports and Dashboard development\n•       2 years of experience in SQL\n•       Bachelors or masters degree in IT or Computer Science or ECE.\n•       Nice to have – Any MicroStrategy certifications\nRequired Knowledge, Skills, and Abilities\n•       Good in writing complex SQL, including aggregate functions, subqueries and complex date calculations and able to teach these concepts to others.\n•       Detail oriented and able to examine data and code for quality and accuracy.\n•       Self-Starter – taking initiative when inefficiencies or opportunities are seen.\n•       Good understanding of modern relational and non-relational models and differences between them\n•       Good understanding of Datawarehouse concepts, snowflake & star schema architecture and SCD concepts\n•       Good understanding of MicroStrategy Schema objects\n•       Develop Public objects such as metrics, filters, prompts, derived objects, custom groups and consolidations in MicroStrategy\n•       Develop complex reports and dashboards using OLAP and MTDI cubes\n•       Create complex dashboards with data blending\n•       Understand VLDB settings and report optimization\n•       Understand security filters and connection mappings in MSTR\nWork Environment\nAt Personify Health, we value and celebrate diversity and are committed to creating an inclusive environment for all employees. We believe in creating teams made up of individuals with various backgrounds, experiences, and perspectives. Diversity inspires innovation and collaboration and challenges us to produce better solutions. But more than this, diversity is our strength and a catalyst in our ability to change lives for the good. \nPhysical Requirements\n•       Constantly operates a computer and other office productivity machinery, such as copy machine, computer printer, calculator, etc.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Microstrategy', 'SQL', 'Dashboards']",2025-06-12 05:24:54
Data Engineer,Databeat,3 - 7 years,Not Disclosed,['Hyderabad( Rai Durg )'],"Experience Required: 3+ years\n\nTechnical knowledge: AWS, Python, SQL, S3, EC2, Glue, Athena, Lambda, DynamoDB, RedShift, Step Functions, Cloud Formation, CI/CD Pipelines, Github, EMR, RDS,AWS Lake Formation, GitLab, Jenkins and AWS CodePipeline.\n\n\n\nRole Summary: As a Senior Data Engineer,with over 3 years of expertise in Python, PySpark, SQL to design, develop and optimize complex data pipelines, support data modeling, and contribute to the architecture that supports big data processing and analytics to cutting-edge cloud solutions that drive business growth. You will lead the design and implementation of scalable, high-performance data solutions on AWS and mentor junior team members.This role demands a deep understanding of AWS services, big data tools, and complex architectures to support large-scale data processing and advanced analytics.\nKey Responsibilities:\nDesign and develop robust, scalable data pipelines using AWS services, Python, PySpark, and SQL that integrate seamlessly with the broader data and product ecosystem.\nLead the migration of legacy data warehouses and data marts to AWS cloud-based data lake and data warehouse solutions.\nOptimize data processing and storage for performance and cost.\nImplement data security and compliance best practices, in collaboration with the IT security team.\nBuild flexible and scalable systems to handle the growing demands of real-time analytics and big data processing.\nWork closely with data scientists and analysts to support their data needs and assist in building complex queries and data analysis pipelines.\nCollaborate with cross-functional teams to understand their data needs and translate them into technical requirements.\nContinuously evaluate new technologies and AWS services to enhance data capabilities and performance.\nCreate and maintain comprehensive documentation of data pipelines, architectures, and workflows.\nParticipate in code reviews and ensure that all solutions are aligned to pre-defined architectural specifications.\nPresent findings to executive leadership and recommend data-driven strategies for business growth.\nCommunicate effectively with different levels of management to gather use cases/requirements and provide designs that cater to those stakeholders.\nHandle clients in multiple industries at the same time, balancing their unique needs.\nProvide mentoring and guidance to junior data engineers and team members.\n\n\n\nRequirements:\n3+ years of experience in a data engineering role with a strong focus on AWS, Python, PySpark, Hive, and SQL.\nProven experience in designing and delivering large-scale data warehousing and data processing solutions.\nLead the design and implementation of complex, scalable data pipelines using AWS services such as S3, EC2, EMR, RDS, Redshift, Glue, Lambda, Athena, and AWS Lake Formation.\nBachelor's or Masters degree in Computer Science, Engineering, or a related technical field.\nDeep knowledge of big data technologies and ETL tools, such as Apache Spark, PySpark, Hadoop, Kafka, and Spark Streaming.\nImplement data architecture patterns, including event-driven pipelines, Lambda architectures, and data lakes.\nIncorporate modern tools like Databricks, Airflow, and Terraform for orchestration and infrastructure as code.\nImplement CI/CD using GitLab, Jenkins, and AWS CodePipeline.\nEnsure data security, governance, and compliance by leveraging tools such as IAM, KMS, and AWS CloudTrail.\nMentor junior engineers, fostering a culture of continuous learning and improvement.\nExcellent problem-solving and analytical skills, with a strategic mindset.\nStrong communication and leadership skills, with the ability to influence stakeholders at all levels.\nAbility to work independently as well as part of a team in a fast-paced environment.\nAdvanced data visualization skills and the ability to present complex data in a clear and concise manner.\nExcellent communication skills, both written and verbal, to collaborate effectively across teams and levels.\n\nPreferred Skills:\nExperience with Databricks, Snowflake, and machine learning pipelines.\nExposure to real-time data streaming technologies and architectures.\nFamiliarity with containerization and serverless computing (Docker, Kubernetes, AWS Lambda).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Aws Glue', 'SQL', 'Data Pipeline', 'Python', 'Amazon Ec2', 'Data Engineering', 'Data Bricks', 'Aws Lambda', 'Amazon Redshift', 'Azure Cloud', 'Data Lake', 'Data Modeling', 'Athena']",2025-06-12 05:24:56
Data Engineer,DATA ENGINEER,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Title: Data Engineer\nExperience: 5+ Years\nLocation: Hyderabad (Onsite)\nAvailability: Immediate Joiners Preferred\nJob Description:\nWe are seeking an experienced Data Engineer with a strong background in Java, Spark, and Scala to join our dynamic team in Hyderabad. The ideal candidate will be responsible for building scalable data pipelines, optimizing data processing workflows, and supporting data-driven solutions for enterprise-grade applications. This is a full-time onsite role.\nKey Responsibilities:\nDesign, develop, and maintain robust and scalable data processing pipelines.\nWork with large-scale data using distributed computing technologies like Apache Spark.\nDevelop applications and data integration workflows using Java and Scala.\nCollaborate with cross-functional teams including Data Scientists, Analysts, and Product Managers.\nEnsure data quality, integrity, and security in all data engineering solutions.\nMonitor and troubleshoot performance and data issues in production systems.\nMust-Have Skills:\nStrong hands-on experience with Java, Apache Spark, and Scala.\nProven experience working on large-scale data processing systems.\nSolid understanding of distributed systems and performance tuning.\nGood-to-Have Skills:\nExperience with Hadoop, Hive, and HDFS.\nFamiliarity with data warehousing concepts and ETL processes.\nExposure to cloud data platforms is a plus.\nDesired Candidate Profile:\n5+ years of relevant experience in data engineering or big data technologies.\nStrong problem-solving and analytical skills.\nExcellent communication and collaboration skills.\nAbility to work independently in a fast-paced environment.\nAdditional Details:\nWork Mode: Onsite (Hyderabad)\nEmployment Type: Full-time\nNotice Period: Immediate joiners highly preferred, candidates serving notice period.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'SCALA', 'Spark', 'Hive', 'Hadoop', 'Kafka']",2025-06-12 05:24:58
Data Engineer,Forbes Global 2000 IT Services Firm,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Title: Big Data Engineer Java & Spark\nLocation: Hyderabad\nWork Mode: Onsite (5 days a week)\nExperience: 5 to 10 Years\nJob Summary:\nWe are hiring an experienced Big Data Engineer with strong expertise in Java, Apache Spark, and Big Data technologies. You will be responsible for designing and implementing scalable data pipelines that support real-time and batch processing for data-driven applications.\nKey Responsibilities:\nDevelop and maintain scalable batch and streaming data pipelines using Java and Apache Spark\nWork with Hadoop, Hive, Kafka, and HDFS to manage and process large datasets\nCollaborate with data analysts, scientists, and other engineering teams to understand data requirements\nOptimize Spark jobs and ensure performance and reliability in production\nMaintain data quality, governance, and security best practices\nRequired Skills:\n510 years of hands-on experience in data engineering or related roles\nStrong programming skills in both Java\nExpertise in Apache Spark for data processing and transformation\nGood understanding of Big Data frameworks: Hadoop, Hive, Kafka, HDFS\nExperience with distributed systems and large-scale data processing\nFamiliarity with cloud platforms such as AWS, GCP, or Azure\nGood to Have:\nExperience with workflow orchestration tools like Airflow or NiFi\nKnowledge of containerization (Docker, Kubernetes)\nExposure to CI/CD pipelines and version control (e.g., Git)\nEducation:\nBachelors or Masters degree in Computer Science, Engineering, or related field\nWhy Join Us:\nBe part of a high-impact data engineering team\nWork on modern data platforms with the latest open-source tools\nStrong tech culture with career growth opportunities",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Spark', 'Hive', 'Hadoop']",2025-06-12 05:25:01
Sr. AWS Databricks Data Engineer,Tata Consultancy Services,6 - 11 years,Not Disclosed,"['Kolkata', 'Pune', 'Chennai']","Role & responsibilities\n\nData Engineer, Expertise in AWS, Databricks and Pyspark",,,,"['AWS', 'Data Bricks', 'Pyspark', 'Data engineer', 'Aws Databricks']",2025-06-12 05:25:04
Senior Data Engineer,Qualcomm,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Area: Information Technology Group, Information Technology Group > IT Data Engineer\n\nGeneral Summary:\n\nDeveloper will play an integral role in the PTEIT Machine Learning Data Engineering team. Design, develop and support data pipelines in a hybrid cloud environment to enable advanced analytics. Design, develop and support CI/CD of data pipelines and services. - 5+ years of experience with Python or equivalent programming using OOPS, Data Structures and Algorithms - Develop new services in AWS using server-less and container-based services. - 3+ years of hands-on experience with AWS Suite of services (EC2, IAM, S3, CDK, Glue, Athena, Lambda, RedShift, Snowflake, RDS) - 3+ years of expertise in scheduling data flows using Apache Airflow - 3+ years of strong data modelling (Functional, Logical and Physical) and data architecture experience in Data Lake and/or Data Warehouse - 3+ years of experience with SQL databases - 3+ years of experience with CI/CD and DevOps using Jenkins - 3+ years of experience with Event driven architecture specially on Change Data Capture - 3+ years of Experience in Apache Spark, SQL, Redshift (or) Big Query (or) Snowflake, Databricks - Deep understanding building the efficient data pipelines with data observability, data quality, schema drift, alerting and monitoring. - Good understanding of the Data Catalogs, Data Governance, Compliance, Security, Data sharing - Experience in building the reusable services across the data processing systems. - Should have the ability to work and contribute beyond defined responsibilities - Excellent communication and inter-personal skills with deep problem-solving skills.\n\nMinimum Qualifications:\n3+ years of IT-related work experience with a Bachelor's degree in Computer Engineering, Computer Science, Information Systems or a related field.\nOR\n5+ years of IT-related work experience without a Bachelors degree.\n\n2+ years of any combination of academic or work experience with programming (e.g., Java, Python).\n1+ year of any combination of academic or work experience with SQL or NoSQL Databases.\n1+ year of any combination of academic or work experience with Data Structures and algorithms.\n5 years of Industry experience and minimum 3 years experience in Data Engineering development with highly reputed organizations- Proficiency in Python and AWS- Excellent problem-solving skills- Deep understanding of data structures and algorithms- Proven experience in building cloud native software preferably with AWS suit of services- Proven experience in design and develop data models using RDBMS (Oracle, MySQL, etc.)\n\nDesirable - Exposure or experience in other cloud platforms (Azure and GCP) - Experience working on internals of large-scale distributed systems and databases such as Hadoop, Spark - Working experience on Data Lakehouse platforms (One House, Databricks Lakehouse) - Working experience on Data Lakehouse File Formats (Delta Lake, Iceberg, Hudi)\n\nBachelor's or Master's degree in Computer Science, Software Engineering, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['algorithms', 'python', 'data quality', 'data structures', 'aws', 'schema', 'continuous integration', 'glue', 'amazon redshift', 'event driven architecture', 'ci/cd', 'data engineering', 'sql', 'alerts', 'java', 'data modeling', 'spark', 'devops', 'data flow', 'nosql databases', 'sql database']",2025-06-12 05:25:06
Data Engineer Advisor,"NTT DATA, Inc.",8 - 13 years,Not Disclosed,['Bengaluru'],"Req ID: 327859\n\nWe are currently seeking a Data Engineer Advisor to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\nJob DutiesResponsibilitiesLead the development of backend systems using Django. Design and implement scalable and secure APIs. Integrate Azure Cloud services for application deployment and management. Utilize Azure Databricks for big data processing and analytics. Implement data processing pipelines using PySpark. Collaborate with front-end developers, product managers, and other stakeholders to deliver comprehensive solutions. Conduct code reviews and ensure adherence to best practices. Mentor and guide junior developers. Optimize database performance and manage data storage solutions. Ensure high performance and security standards for applications. Participate in architecture design and technical decision-making. Minimum Skills RequiredQualificationsBachelor's degree in Computer Science, Information Technology, or a related field. 8+ years of experience in backend development. 8+ years of experience with Django. Proven experience with Azure Cloud services. Experience with Azure Databricks and PySpark. Strong understanding of RESTful APIs and web services. Excellent communication and problem-solving skills. Familiarity with Agile methodologies. Experience with database management (SQL and NoSQL).\n\nSkills:\nDjango, Python, Azure Cloud, Azure Databricks, Delta Lake and Delta tables, PySpark, SQL/NoSQL databases, RESTful APIs, Git, and Agile methodologies",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['web services', 'rest', 'python', 'backend development', 'django', 'azure cloud services', 'css', 'pyspark', 'jquery', 'sql', 'git', 'asp.net', 'html', 'web api', 'mvc', 'wcf', 'agile methodology', 'azure databricks', 'c#', 'entity framework', 'azure cloud', 'javascript', 'sql server', 'nosql', 'angular', 'linq', '.net']",2025-06-12 05:25:09
Data Engineer- MS Fabric,InfoCepts,5 - 9 years,Not Disclosed,['India'],"Position: Data Engineer – MS Fabric\n  Purpose of the Position: As an MS Fabric Data engineer you will be responsible for designing, implementing, and managing scalable data pipelines. Strong experience in implementation and management of lake House using MS Fabric Azure Tech stack (ADLS Gen2, ADF, Azure SQL) .\nProficiency in data integration techniques, ETL processes and data pipeline architectures. Well versed in Data Quality rules, principles and implementation.\n",,,,"['components', 'data', 'scala', 'delta', 'pyspark', 'data warehousing', 'rules', 'azure data factory', 'sql', 'parquet', 'analytics', 'sql azure', 'spark', 'oracle adf', 'data pipeline architecture', 'etl', 'python', 'azure synapse', 'microsoft azure', 'power bi', 'data bricks', 'data quality', 'system', 't', 'fabric', 'data integration', 'etl process']",2025-06-12 05:25:11
Cloud Data Engineer,PwC India,5 - 8 years,10-20 Lacs P.A.,['Bengaluru'],"Role & responsibilities\nStrong hands-on experience with multi cloud (AWS, Azure, GCP)  services such as GCP BigQuery, Dataform AWS Redshift, \nProficient in PySpark and SQL for building scalable data processing pipelines\nKnowledge of utilizing serverless technologies like AWS Lambda, and Google Cloud Functions \nExperience in orchestration frameworks like Apache Airflow, Kubernetes, and Jenkins to manage and orchestrate data pipelines",,,,"['Pyspark', 'Python', 'SQL', 'Datafactory', 'GCP', 'Microsoft Azure', 'AWS', 'Data Bricks']",2025-06-12 05:25:14
Data Engineer,Sorice Solutions,4 - 8 years,6-12 Lacs P.A.,['Chennai'],"Location: Chennai\n\n\nRole & responsibilities\n\n\nBackend data model and data architecture, data migration, using python/java to build data ingestion pipelines and data validation processes, develop task schedulers for different data sources",Industry Type: Miscellaneous,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Migration', 'Data Modeling', 'Python', 'SQL']",2025-06-12 05:25:16
"Data Engineer Openings at Advantum Health, Hyderabad",Advantum Health,3 - 5 years,Not Disclosed,['Hyderabad'],"Data Engineer openings at Advantum Health Pvt Ltd, Hyderabad.\nOverview:\nWe are looking for a Data Engineer to build and optimize robust data pipelines that support AI and RCM analytics. This role involves integrating structured and unstructured data from diverse healthcare systems into scalable, AI-ready datasets.\nKey Responsibilities:\nDesign, implement, and optimize data pipelines for ingesting and transforming healthcare and RCM data.\nBuild data marts and warehouses to support analytics and machine learning.\nEnsure data quality, lineage, and governance across AI use cases.\nIntegrate data from EMRs, billing platforms, claims databases, and third-party APIs.\nSupport data infrastructure in a HIPAA-compliant cloud environment.\nQualifications:\nBachelors in Computer Science, Data Engineering, or related field.\n3+ years of experience with ETL/ELT pipelines using tools like Apache Airflow, dbt, or Azure Data Factory.\nStrong SQL and Python skills.\nExperience with healthcare data standards (HL7, FHIR, X12) preferred.\nFamiliarity with data lake house architectures and AI integration best practices\nPh: 9177078628\nEmail id: jobs@advantumhealth.com\nAddress: Advantum Health Private Limited, Cyber gateway, Block C, 4th floor Hitech City, Hyderabad.\nDo follow us on LinkedIn, Facebook, Instagram, YouTube and Threads\nAdvantum Health LinkedIn Page:\nhttps://lnkd.in/gVcQAXK3\n\nAdvantum Health Facebook Page:\nhttps://lnkd.in/g7ARQ378\n\nAdvantum Health Instagram Page:\nhttps://lnkd.in/gtQnB_Gc\n\nAdvantum Health India YouTube link:\nhttps://lnkd.in/g_AxPaPp\n\nAdvantum Health Threads link:\nhttps://lnkd.in/gyq73iQ6",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'SQL', 'Python', 'Airflow', 'ETL', 'Elt']",2025-06-12 05:25:18
Data Engineer,Conversehr Business Solutions,4 - 7 years,15-30 Lacs P.A.,['Hyderabad'],"What are the ongoing responsibilities of Data Engineer responsible for?\nWe are building a growing Data and AI team. You will play a critical role in the efforts to centralize structured and unstructured data for the firm. We seek a candidate with skills in data modeling, data management and data governance, and can contribute first-hand towards firms data strategy. The ideal candidate is a self-starter with a strong technical foundation, a collaborative mindset, and the ability to navigate complex data challenges #ASSOCIATE\nWhat ideal qualifications, skills & experience would help someone to be successful?\nBachelors degree in computer science or computer applications; or equivalent experience in lieu of degree with 3 years of industry experience.\nStrong expertise in data modeling and data management concepts. Experience in implementing master data management is preferred.\nSound knowledge on Snowflake and data warehousing techniques.\nExperience in building, optimizing, and maintaining data pipelines and data management frameworks to support business needs.\nProficiency in at least one programming language, preferably python.\nCollaborate with cross-functional teams to translate business needs into scalable data and AI-driven solutions.\nTake ownership of projects from ideation to production, operating in a startup-like culture within an enterprise environment. Excellent communication, collaboration, and ownership mindset.\nFoundational Knowledge of API development and integration.\nKnowledge of Tableau, Alteryx is good-to-have.\nWork Shift Timings - 2:00 PM - 11:00 PM IST",Industry Type: Financial Services (Asset Management),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Master Data Management', 'Python', 'Etl Pipelines', 'Alteryx', 'ai', 'Data Modeling', 'Tableau', 'ETL']",2025-06-12 05:25:20
Azure Data Engineer,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of the role is to support process delivery by ensuring daily performance of the Production Specialists, resolve technical escalations and develop technical capability within the Production Specialists.\n\nDo\nOversee and support process by reviewing daily transactions on performance parameters\nReview performance dashboard and the scores for the team\nSupport the team in improving performance parameters by providing technical support and process guidance\nRecord, track, and document all queries received, problem-solving steps taken and total successful and unsuccessful resolutions\nEnsure standard processes and procedures are followed to resolve all client queries\nResolve client queries as per the SLAs defined in the contract\nDevelop understanding of process/ product for the team members to facilitate better client interaction and troubleshooting\nDocument and analyze call logs to spot most occurring trends to prevent future problems\nIdentify red flags and escalate serious client issues to Team leader in cases of untimely resolution\nEnsure all product information and disclosures are given to clients before and after the call/email requests\nAvoids legal challenges by monitoring compliance with service agreements\n\nHandle technical escalations through effective diagnosis and troubleshooting of client queries\nManage and resolve technical roadblocks/ escalations as per SLA and quality requirements\nIf unable to resolve the issues, timely escalate the issues to TA & SES\nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers and clients business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract SLAs\nBuild people capability to ensure operational excellence and maintain superior customer service levels of the existing account/client\nMentor and guide Production Specialists on improving technical knowledge\nCollate trainings to be conducted as triage to bridge the skill gaps identified through interviews with the Production Specialist\nDevelop and conduct trainings (Triages) within products for production specialist as per target\nInform client about the triages being conducted\nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nIdentify and document most common problems and recommend appropriate resolutions to the team\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\nDeliver\n\nNoPerformance ParameterMeasure1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback, NSAT/ ESAT2Team ManagementProductivity, efficiency, absenteeism3Capability developmentTriages completed, Technical Test performance\nMandatory Skills: Azure Data Factory. Experience: 5-8 Years.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'azure databricks', 'azure data lake', 'ssas', 'ssrs', 'microsoft azure', 'azure data factory', 'ssis', 'msbi', 'sql server', 'sql']",2025-06-12 05:25:22
"Tcs is hiring For Azure Data Engineer (ADF, Python, Pyspark,DataBricks",Tata Consultancy Services,5 - 10 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Role & responsibilities\nStrong understanding of Azure environment (PaaS, IaaS) and experience in working with Hybrid model\nAt least 1 project experience in Azure Data Stack that involves components like Azure Data Lake, Azure Synapse Analytics, Azure Data Factory, Azure Data Bricks, Azure Analysis Service, Azure SQL DWH\nStrong hands-on SQL/T-SQL/Spark SQL and database concepts",,,,"['Azure Databricks', 'Azure Data Factory', 'Pyspark', 'Python Data', 'Microsoft Azure', 'Devops', 'Python', 'SQL']",2025-06-12 05:25:24
Data Engineer,Tekskills India pvt ltd,7 - 9 years,8-15 Lacs P.A.,['Hyderabad'],"Role & Responsibilities Role Overview: We are seeking a talented and forward-thinking Data Engineer for one of the large financial services GCC based in Hyderabad with responsibilities that include designing and constructing data pipelines, integrating data from multiple sources, developing scalable data solutions, optimizing data workflows, collaborating with cross-functional teams, implementing data governance practices, and ensuring data security and compliance.\n\nTechnical Requirements: • Proficiency in ETL, Batch, and Streaming Process • Experience with BigQuery, Cloud Storage, and CloudSQL • Strong programming skills in Python, SQL, and Apache Beam for data processing • Understanding of data modeling and schema design for analytics • Knowledge of data governance, security, and compliance in GCP • Familiarity with machine learning workflows and integration with GCP ML tools • Ability to optimize performance within data pipelines\n\nFunctional Requirements: • Ability to collaborate with Data Operations, Software Engineers, Data Scientists, and Business SMEs to develop Data Product Features • Experience in leading and mentoring peers within an existing development team • Strong communication skills to craft and communicate robust solutions • Proficient in working with Engineering Leads, Enterprise and Data Architects, and Business Architects to build appropriate data foundations • Willingness to work on contemporary data architecture in Public and Private Cloud environments This role offers a compelling opportunity for a seasoned Data Engineering to drive transformative cloud initiatives within the financial sector, leveraging unparalleled experience and expertise to deliver innovative cloud solutions that align with business imperatives and regulatory requirements. Qualification o Engineering Grad / Postgraduate CRITERIA o Proficient in ETL, Python, and Apache Beam for data processing efficiency. o Demonstrated expertise in BigQuery, Cloud Storage, and CloudSQL utilization. o Strong collaboration skills with cross-functional teams for data product development. o Comprehensive knowledge of data governance, security, and compliance in GCP. o Experienced in optimizing performance within data pipelines for efficiency.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ETL', 'GCP', 'Apache beam', 'Bigquery', 'Cloud sql', 'Cloudstorage', 'Python']",2025-06-12 05:25:26
AWS Data Engineer,Infosys,5 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Significant 5 to 9 years of experience in designing and implementing scalable data engineering solutions on AWS.\nStrong proficiency in Python programming language.\nExpertise in serverless architecture and AWS services such as Lambda, Glue, Redshift, Kinesis, SNS, SQS, and CloudFormation.\nExperience with Infrastructure as Code (IaC) using AWS CDK for defining and provisioning AWS resources.\nProven leadership skills with the ability to mentor and guide junior team members.\nExcellent understanding of data modeling concepts and experience with tools like ERStudio.\nStrong communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.\nExperience with Apache Airflow for orchestrating data pipelines is a plus.\nKnowledge of Data Lakehouse, dbt, or Apache Hudi data format is a plus.\nRoles and Responsibilities\nDesign, develop, test, deploy, and maintain large-scale data pipelines using AWS services such as S3, Glue, Lambda, Redshift.\nCollaborate with cross-functional teams to gather requirements and design solutions that meet business needs.\nDesired Candidate Profile\n5-9 years of experience in an IT industry setting with expertise in Python programming language (Pyspark).\nStrong understanding of AWS ecosystem including S3, Glue, Lambda, Redshift.\nBachelor's degree in Any Specialization (B.Tech/B.E.).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Aws Glue', 'Pyspark', 'Aws Lambda', 'Amazon Redshift', 'Athena', 'Python']",2025-06-12 05:25:28
AWS Data Engineer,Infosys,5 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","AWS Data Engineer\n\nTo Apply, use the below link:\nhttps://career.infosys.com/jobdesc?jobReferenceCode=INFSYS-EXTERNAL-210775&rc=0\n\nJOB Profile:\nSignificant 5 to 9 years of experience in designing and implementing scalable data engineering solutions on AWS.\nStrong proficiency in Python programming language.\nExpertise in serverless architecture and AWS services such as Lambda, Glue, Redshift, Kinesis, SNS, SQS, and CloudFormation.\nExperience with Infrastructure as Code (IaC) using AWS CDK for defining and provisioning AWS resources.\nProven leadership skills with the ability to mentor and guide junior team members.\nExcellent understanding of data modeling concepts and experience with tools like ERStudio.\nStrong communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.\nExperience with Apache Airflow for orchestrating data pipelines is a plus.\nKnowledge of Data Lakehouse, dbt, or Apache Hudi data format is a plus.\n\n\nRoles and Responsibilities\nDesign, develop, test, deploy, and maintain large-scale data pipelines using AWS services such as S3, Glue, Lambda, Redshift.\nCollaborate with cross-functional teams to gather requirements and design solutions that meet business needs.\nDesired Candidate Profile\n5-9 years of experience in an IT industry setting with expertise in Python programming language (Pyspark).\nStrong understanding of AWS ecosystem including S3, Glue, Lambda, Redshift.\nBachelor's degree in Any Specialization (B.Tech/B.E.).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Aws Glue', 'AWS Data Engineer', 'Pyspark', 'Aws Lambda', 'Redshift Aws', 'Python']",2025-06-12 05:25:30
NLP Data Engineer - Risk Insights & Monitoring,MNC IT,10 - 12 years,25-30 Lacs P.A.,"['Pune', 'Mumbai (All Areas)']","Design and implement state-of-the-art NLP models, including but not limited to text classification, semantic search, sentiment analysis, named entity recognition, and summary generation.\nconduct data preprocessing, and feature engineering to improve model accuracy and performance.\nStay updated with the latest developments in NLP and ML, and integrate cutting-edge techniques into our solutions.\ncollaborate with Cross-Functional Teams: Work closely with data scientists, software engineers, and product managers to align NLP projects with business objectives.\ndeploy models into production environments and monitor their performance to ensure robustness and reliability.\nmaintain comprehensive documentation of processes, models, and experiments, and report findings to stakeholders.\nimplement and deliver high quality software solutions / components for the Credit Risk monitoring platform.\nleverage his/her expertise to mentor developers; review code and ensure adherence to standards.\napply a broad range of software engineering practices, from analyzing user needs and developing new features to automated testing and deployment\nensure the quality, security, reliability, and compliance of our solutions by applying our digital principles and implementing both functional and non-functional requirements\nbuild observability into our solutions, monitor production health, help to resolve incidents, and remediate the root cause of risks and issues\nunderstand, represent, and advocate for client needs\nshare knowledge and expertise with colleagues , help with hiring, and contribute regularly to our engineering culture and internal communities.\nExpertise -\nBachelor of Engineering or equivalent.\nIdeally 8-10Yrs years of experience in NLP based applications focused on Banking / Finance sector.\nPreference for experience in financial data extraction and classification.\nInterested in learning new technologies and practices, reuse strategic platforms and standards, evaluate options, and make decisions with long-term sustainability in mind.\nProficiency in programming languages such as Python & Java. Experience with frameworks like TensorFlow, PyTorch, or Keras.\nIn-depth knowledge of NLP techniques and tools, including spaCy, NLTK, and Hugging Face.\nExperience with data handling and processing tools like Pandas, NumPy, and SQL.\nPrior experience in agentic AI, LLMs ,prompt engineering and generative AI is a plus.\nBackend development and microservices using Java Spring Boot, J2EE, REST for implementing projects with high SLA of data availability and data quality.\nExperience of building cloud ready and migrating applications using Azure and understanding of the Azure Native Cloud services, software design and enterprise integration patterns.\nKnowledge of SQL and PL/SQL (Oracle) and UNIX, writing queries, packages, working with joins, partitions, looking at execution plans, and tuning queries.\nA real passion for and experience of Agile working practices, with a strong desire to work with baked in quality subject areas such as TDD, BDD, test automation and DevOps principles\nExperience in Azure development including Databricks , Azure Services , ADLS etc.\nExperience using DevOps toolsets like GitLab, Jenkins",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['data engineer', 'Natural Language Processing', 'Python', 'Java']",2025-06-12 05:25:32
Azure Data Engineer,Infosys,5 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Job description\nWe are looking for Azure Data Engineer's resources having minimum 5 to 9 years of Experience.\n\nRole & responsibilities\nBlend of technical expertise with 5 to 9 year of experience, analytical problem-solving, and collaboration with cross-functional teams. Design and implement Azure data engineering solutions (Ingestion & Curation)\nCreate and maintain Azure data solutions including Azure SQL Database, Azure Data Lake, and Azure Blob Storage.\nDesign, implement, and maintain data pipelines for data ingestion, processing, and transformation in Azure.\nUtilizing Azure Data Factory or comparable technologies, create and maintain ETL (Extract, Transform, Load) operations\nUse Azure Data Factory and Databricks to assemble large, complex data sets\nImplementing data validation and cleansing procedures will ensure the quality, integrity, and dependability of the data.\nEnsure data quality / security and compliance.\nOptimize Azure SQL databases for efficient query performance.\nCollaborate with data engineers, and other stakeholders to understand requirements and translate them into scalable and reliable data platform architectures.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Azure Data Factory', 'Azure Synapse', 'Pyspark', 'Azure Data Lake']",2025-06-12 05:25:35
Data Engineer,Reputed Client,5 - 10 years,18-25 Lacs P.A.,[],"Data Engineer\n(Python, PySpark, SQL and Spark SQL)\n\nExperience - 5-10 Years\nMandate Skills: Python, PySpark, SQL and SparkSQL\nWorking Hours: 11:00 am to 8 pm\n\n(Candidate has to be flexible. 4-hour overlap with US business hours)\n\nSalary : 1.50 LPM to 2 LPM + Tax (Rate is not fixed, negotiable depending upon the candidate feedback)\n\nRemote / Hybrid (3 Days in a week WFO) (Pune, Bangalore, Noida, Mumbai, Hyderabad)\n\nNOTE: Need candidates within these cities, they have to collect assets from the office / need to be available for meetings - if they are working remotely)\n\nIt's a 6 months (C2H role).",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Python', 'PySpark', 'Spark', 'SQL']",2025-06-12 05:25:37
Data Engineer,Revature,1 - 2 years,Not Disclosed,['Chennai'],"The ideal candidate should have a strong background in SQL, BigQuery, and Google Cloud Platform (GCP), with hands-on experience in developing reports and dashboards using Looker Studio, Looker Standard, and LookML. Excellent communication skills and the ability to work collaboratively with cross-functional teams are essential for success in this role.\nKey Responsibilities:\nDesign, develop, and maintain dashboards and reports using Looker Studio and Looker Standard.\nDevelop and maintain LookML models, explores, and views to support business reporting requirements.\nOptimize and write advanced SQL queries for data extraction, transformation, and analysis.\nWork with BigQuery as the primary data warehouse for managing and analyzing large datasets.\nCollaborate with business stakeholders to understand data requirements and translate them into scalable reporting solutions.\nImplement data governance, access controls, and performance optimizations within the Looker environment.\nPerform root-cause analysis and troubleshooting for reporting and data issues.\nMaintain documentation for Looker projects, data models, and data dictionaries.\nStay updated with the latest Looker and GCP features and best practices.",,,,"['SQL', 'Python', 'Bigquery', 'Gcp Cloud']",2025-06-12 05:25:39
AWS Databricks Data Engineer,Tata Consultancy Services,6 - 11 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']","Role & responsibilities\n\nData Engineer, Expertise in AWS, Databricks and Pyspark",,,,"['Pyspark', 'databricks', 'AWS', 'data engineer', 'Aws Databricks']",2025-06-12 05:25:41
Azure Data Engineer,Hexaware Technologies,6 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","Hi\n\nWork Location : Chennai AND Bangalore\nWork location : Imm - 30 days\n\nPrimary: Azure Databricks,ADF, Pyspark SQL",,,,"['Pyspark', 'Azure Databricks', 'SQL', 'Azure Data Factory']",2025-06-12 05:25:43
Data Engineer,Society Managers,3 - 5 years,Not Disclosed,['Mumbai (All Areas)'],"We are seeking a skilled and driven SDE-II (Data Engineering) to join our dynamic team. In this role, you will design, develop, and maintain scalable data pipelines, working with large, complex datasets. Youll collaborate closely with cross-functional teams to gather data requirements and contribute to the architecture of our data systems, leveraging your expertise in tools like Databricks, Spark, and SQL.\n\nRoles and responsibilities\nData Pipeline Development: Design,build, and maintain scalable data pipelines using Databricks, Python,and Spark.\nData Processing & Transformation: Handle large, complex datasets to ensure efficient data processing and transformations.\nCollaboration: Work with cross-functional teams to gather, understand, and implement data requirements.\nSQL & ETL: Write and optimize SQL queries for data extraction, transformation, and loading (ETL) processes.\nData Quality & Security: Ensure data accuracy, integrity, and security across all stages of the data lifecycle.\nSystem Design & Architecture: Contribute to the design and architecture of scalable data systems and solutions.\nRequired Skills and Qualification\nExperience: 3+ years of experience in data engineering or a related field.\nDatabricks & Spark: Strong expertise in Databricks and distributed data processing with Spark.\nProgramming: Proficiency in Python for data engineering tasks.\nSQL Optimization: Solid experience in writing and optimizing complex SQL queries.\nData Systems Knowledge: Hands-on experience with large-scale data systems and tools.\nDomain Knowledge: Familiarity with Capital Market/Private Equity is a plus (relaxation may apply).\nData Visualization: Experience with Tableau for creating insightful data visualizations and reports.\nPreferred skills\nCloud Platforms: Familiarity with cloud services like AWS, Azure, or GCP.\nData Warehousing & ETL:Experience with data warehousing concepts and ETL processes.\nAnalytical Skills: Strong problem-solving and analytical capabilities Analytics Tools: Hands-on experience with tools like Amplitude, PostHog, Google Analytics, or Mixpanel.\nAdditional Tools: Knowledge of Python for web scraping and frameworks like Django (good to have).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Microsoft Azure', 'Data Bricks', 'Spark', 'ETL', 'Python', 'SQL']",2025-06-12 05:25:46
"Data engineer with Gen AI- Balewadi, pune- hybrid",Indian MNC,5 - 10 years,15-30 Lacs P.A.,['Pune( Balewadi )'],"Role & responsibilities\nWe are seeking a skilled Data Engineer with advanced expertise in Python, PySpark, Databricks, and Machine Learning, along with a working knowledge of Generative and Agentic AI. This role is critical in ensuring data integrity and driving innovation across enterprise systems. You will design and implement ML-driven solutions to enhance Data Governance & Data Privacy initiatives through automation, self-service capabilities, and scalable, AI-enabled innovation.\nKey Responsibilities:\nImplement ML and Generative/Agentic AI solutions to optimize Data Governance processes.\nDesign, develop, and maintain scalable data pipelines using Python, PySpark, and Databricks.\nDevelop automation frameworks to support data quality, lineage, classification, and access control.\nDevelop and deploy machine learning models to uncover data patterns, detect anomalies, and enhance data governance and privacy compliance\nCollaborate with data stewards, analysts, and governance teams to build self-service data capabilities.\nWork with Databricks, Azure Data Lake, AWS, and other cloud-based data platforms for data engineering.\nBuild, configure, and integrate APIs for seamless system interoperability.\nEnsure data integrity, consistency, and compliance across systems and workflows.\nIntegrate AI models to support data discovery, metadata enrichment, and intelligent recommendations.\nOptimize data architecture to support analytics, reporting, and governance use cases.\nMonitor and improve the performance of ML/AI components in production environments.\nStay updated with emerging AI and data engineering technologies to drive continuous innovation.\nTechnical Skills:\nStrong programming skills in Python, PySpark, SQL for data processing and automation.\nExperience with Databricks and Snowflake (preferred) for building and maintaining data pipelines.\nExperience with Machine Learning model development and Generative/Agentic AI frameworks (e.g. LLMs, Transformers, LangChain) especially in the Data Management space\nExperience working with REST APIs & JSON for service integration\nExperience working with cloud-based platforms such as Azure, AWS, or GCP\nPower BI dashboard development experience is a plus.\nSoft Skills:\nStrong problem-solving skills and attention to detail.\nExcellent communication and collaboration abilities, with experience working across technical and business teams",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Generative Ai', 'Azure Databricks', 'Ml']",2025-06-12 05:25:48
Data Engineer,MNC,4 - 8 years,Not Disclosed,['Hyderabad'],"Job Title: Software Engineer -Data Engineer\nPosition: Software Engineer\nExperience: 4-6 years (Less YOE will be Rejected)\nCategory: Software Development/ Engineering\nShift Timings: 1:00 pm to 10:00 pm\nMain location: Hyderabad\nWork Type: Work from office\nNotice Period: 0-30 Days\nSkill: Python, Pyspark, Data Bricks\nEmployment Type: Full Time\n\n• Bachelor's in Computer Science, Computer Engineering or related field\nRequired qualifications to be successful in this role\nMust have Skills:\n• 3+ yrs. Development experience with Spark (PySpark), Python and SQL.\n• Extensive knowledge building data pipelines\n• Hands on experience with Databricks Devlopment\n• Strong experience with\n• Strong experience developing on Linux OS.\n• Experience with scheduling and orchestration (e.g. Databricks Workflows,airflow, prefect, control-m).\n\nGood to have skills:\n• Solid understanding of distributed systems, data structures, design principles.\n• Agile Development Methodologies (e.g. SAFe, Kanban, Scrum).\n• Comfortable communicating with teams via showcases/demos.\n• Play key role in establishing and implementing migration patterns for the Data Lake Modernization project.\n• Actively migrate use cases from our on premises Data Lake to Databricks on GCP.\n• Collaborate with Product Management and business partners to understand use case requirements and reporting.\n• Adhere to internal development best practices/lifecycle (e.g. Testing, Code Reviews, CI/CD, Documentation) .\n• Document and showcase feature designs/workflows.\n• Participate in team meetings and discussions around product development.\n• Stay up to date on industry latest industry trends and design patterns.\n• 3+ years experience with GIT.\n• 3+ years experience with CI/CD (e.g. Azure Pipelines).\n• Experience with streaming technologies, such as Kafka, Spark.\n• Experience building applications on Docker and Kubernetes.\n• Cloud experience (e.g. Azure, Google).\n\nInterested Candidates can drop your Resume on Mail id :- "" tarun.k@talent21.in """,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Bricks', 'Python', 'Pyspark', 'SQL', 'ETL', 'Airflow', 'Azure Pipelines', 'Kafka', 'Design', 'Docker', 'Azure Cloud', 'Control-M', 'Cicd Pipeline', 'Modernization', 'testing', 'Documentation', 'Workflow', 'Code Review', 'Gcp Cloud', 'Product Development', 'Agile Methodology', 'GIT', 'Linux', 'GCP', 'Data Lake', 'Kubernetes']",2025-06-12 05:25:50
Data Engineer,MNC,4 - 9 years,Not Disclosed,['Hyderabad'],"Job Title: Software Engineer -Data Engineer\nPosition: Software Engineer\nExperience: 4-9 years\nCategory: Software Development/ Engineering\nShift Timings: 1:00 pm to 10:00 pm\nMain location: Hyderabad\nWork Type: Work from office\nNotice Period: 0-30 Days\nSkill: Python, Pyspark, Data Bricks\nEmployment Type: Full Time\n\n• Bachelor's in Computer Science, Computer Engineering or related field\nRequired qualifications to be successful in this role\nMust have Skills:\n• 3+ yrs. Development experience with Spark (PySpark), Python and SQL.\n• Extensive knowledge building data pipelines\n• Hands on experience with Databricks Devlopment\n• Strong experience with\n• Strong experience developing on Linux OS.\n• Experience with scheduling and orchestration (e.g. Databricks Workflows,airflow, prefect, control-m).\n\nGood to have skills:\n• Solid understanding of distributed systems, data structures, design principles.\n• Agile Development Methodologies (e.g. SAFe, Kanban, Scrum).\n• Comfortable communicating with teams via showcases/demos.\n• Play key role in establishing and implementing migration patterns for the Data Lake Modernization project.\n• Actively migrate use cases from our on premises Data Lake to Databricks on GCP.\n• Collaborate with Product Management and business partners to understand use case requirements and reporting.\n• Adhere to internal development best practices/lifecycle (e.g. Testing, Code Reviews, CI/CD, Documentation) .\n• Document and showcase feature designs/workflows.\n• Participate in team meetings and discussions around product development.\n• Stay up to date on industry latest industry trends and design patterns.\n• 3+ years experience with GIT.\n• 3+ years experience with CI/CD (e.g. Azure Pipelines).\n• Experience with streaming technologies, such as Kafka, Spark.\n• Experience building applications on Docker and Kubernetes.\n• Cloud experience (e.g. Azure, Google).\n\nInterested Candidates can drop your Resume on Mail id :- "" kalyan.v@talent21.in """,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Bricks', 'Python', 'Pyspark', 'SQL', 'ETL', 'Airflow', 'Azure Pipelines', 'Kafka', 'Design', 'Docker', 'Azure Cloud', 'Control-M', 'Cicd Pipeline', 'Modernization', 'testing', 'Documentation', 'Workflow', 'Code Review', 'Gcp Cloud', 'Product Development', 'Agile Methodology', 'GIT', 'Linux', 'GCP', 'Data Lake', 'Kubernetes']",2025-06-12 05:25:53
Data Engineering Specialist,Accenture,4 - 8 years,Not Disclosed,"['Pune', 'Gurugram', 'Bengaluru']","Job Summary:\nWe are:\nSales Excellence. Sales Excellence at Accenture empowers our people to compete, win and grow. We provide everything they need to grow their client portfolios, optimize their deals and enable their sales talent, all driven by sales intelligence.\nThe team will be aligned to the Client Success, which is a new function to support Accenture’s approach to putting client value and client experience at the heart of everything we do to foster client love. Our ambition is that every client loves working with Accenture and believes we’re the ideal partner to help them create and realize their vision for the future – beyond their expectations.\nYou are:\nA builder at heart – curious about new tools and their usefulness, eager to create prototypes, and adaptable to changing paths. You enjoy sharing your experiments with a small team and are responsive to the needs of your clients.\nThe work:\nThe Center of Excellence (COE) enables Sales Excellence to deliver best-in-class service offerings to Accenture leaders, practitioners, and sales teams.\nAs a member of the COE Analytics Tools & Reporting team, you will help in building and enhancing data foundation for reporting tools and Analytics tool to provide insights on underlying trends and key drivers of the business.\n\nRoles & Responsibilities:\nCollaborate with the Client Success, Analytics COE, CIO Engineering/DevOps team, and stakeholders to build and enhance Client success data lake.\nWrite complex SQL scripts to transform data for the creation of dashboards or reports and validate the accuracy and completeness of the data.\nBuild automated solutions to support any business operation or data transfer.\nDocument and build efficient data model for reporting and analytics use case.\nAssure the Data Lake data accuracy, consistency, and timeliness while ensuring user acceptance and satisfaction.\nWork with the Client Success, Sales Excellence COE members, CIO Engineering/DevOps team and Analytics Leads to standardize Data in data lake.\nProfessional & Technical Skills:\nBachelor’s degree or equivalent experience in Data Engineering, analytics, or similar field.\nAt least 4 years of professional experience in developing and managing ETL pipelines.\nA minimum of 2 years of GCP experience.\nAbility to write complex SQL and prepare data for dashboarding.\nExperience in managing and documenting data models.\nUnderstanding of Data governance and policies.\nProficiency in Python and SQL scripting language.\nAbility to translate business requirements into technical specification for engineering team.\nCuriosity, creativity, a collaborative attitude, and attention to detail.\nAbility to explain technical information to technical as well as non-technical users.\nAbility to work remotely with minimal supervision in a global environment.\nProficiency with Microsoft office tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['GCP', 'Python', 'SQL', 'Data Engineering', 'ETL']",2025-06-12 05:25:55
Data Engineer - ETL/ Python,Meritus Management Service,5 - 7 years,10-14 Lacs P.A.,['Indore'],"Focus on Python, you'll play a crucial role in designing, developing, and maintaining data pipelines and ETL processes. Python to manage large datasets, automate data workflows, and ensure data accuracy and efficiency across our organization.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pandas', 'MySQL', 'Sqlalchemy', 'Numpy', 'Python', 'Azure Synapse', 'Postgresql', 'Etl Process', 'SQL']",2025-06-12 05:25:57
DataBricks - Data Engineering,Wipro,5 - 8 years,Not Disclosed,['Pune'],"Role Purpose\nThe purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\n\n\n\nDo\n1. Instrumental in understanding the requirements and design of the product/ software\nDevelop software solutions by studying information needs, studying systems flow, data usage and work processes\nInvestigating problem areas followed by the software development life cycle\nFacilitate root cause analysis of the system issues and problem statement\nIdentify ideas to improve system performance and impact availability\nAnalyze client requirements and convert requirements to feasible design\nCollaborate with functional teams or systems analysts who carry out the detailed investigation into software requirements\nConferring with project managers to obtain information on software capabilities\n\n\n\n2. Perform coding and ensure optimal software/ module development\nDetermine operational feasibility by evaluating analysis, problem definition, requirements, software development and proposed software\nDevelop and automate processes for software validation by setting up and designing test cases/scenarios/usage cases, and executing these cases\nModifying software to fix errors, adapt it to new hardware, improve its performance, or upgrade interfaces.\nAnalyzing information to recommend and plan the installation of new systems or modifications of an existing system\nEnsuring that code is error free or has no bugs and test failure\nPreparing reports on programming project specifications, activities and status\nEnsure all the codes are raised as per the norm defined for project / program / account with clear description and replication patterns\nCompile timely, comprehensive and accurate documentation and reports as requested\nCoordinating with the team on daily project status and progress and documenting it\nProviding feedback on usability and serviceability, trace the result to quality risk and report it to concerned stakeholders\n\n\n\n3. Status Reporting and Customer Focus on an ongoing basis with respect to project and its execution\nCapturing all the requirements and clarifications from the client for better quality work\nTaking feedback on the regular basis to ensure smooth and on time delivery\nParticipating in continuing education and training to remain current on best practices, learn new programming languages, and better assist other team members.\nConsulting with engineering staff to evaluate software-hardware interfaces and develop specifications and performance requirements\nDocument and demonstrate solutions by developing documentation, flowcharts, layouts, diagrams, charts, code comments and clear code\nDocumenting very necessary details and reports in a formal way for proper understanding of software from client proposal to implementation\nEnsure good quality of interaction with customer w.r.t. e-mail content, fault report tracking, voice calls, business etiquette etc\nTimely Response to customer requests and no instances of complaints either internally or externally\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure\n1.Continuous Integration, Deployment & Monitoring of Software100% error free on boarding & implementation, throughput %, Adherence to the schedule/ release plan\n2.Quality & CSATOn-Time Delivery, Manage software, Troubleshoot queries,Customer experience, completion of assigned certifications for skill upgradation\n3.MIS & Reporting100% on time MIS & report generation\nMandatory Skills: DataBricks - Data Engineering.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['DataBricks', 'module development', 'software development life cycle', 'Data Engineering', 'software development', 'quality assurance']",2025-06-12 05:26:00
Data Engineer,Get Your Job,3 - 8 years,9.5-18 Lacs P.A.,"['Hyderabad', 'Gurugram']","3-8 Years exp.\nJob Location – Gurgaon and Hyderabad\nWork Mode – Hybrid (3-4 Days work from office)\nNotice Period – Immediate to 30 Days Official NP, OR 45 days serving NP candidates, ONLY.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Hive', 'hadoop', 'Python', 'Spark', 'SQL']",2025-06-12 05:26:02
Data Ingest Engineer,"NTT DATA, Inc.",6 - 10 years,Not Disclosed,['Pune'],"Req ID: 323909\n\nWe are currently seeking a Data Ingest Engineer to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nJob DutiesThe Applications Development Technology Lead Analyst is a senior level position responsible for establishing and implementing new or revised application systems and programs in coordination with the Technology team.\nThis is a position within the Ingestion team of the DRIFT data ecosystem. The focus is on ingesting data in a timely , complete, and comprehensive fashion while using the latest technology available to Citi. The ability to leverage new and creative methods for repeatable data ingestion from a variety of data sources while always questioning ""is this the best way to solve this problem"" and ""am I providing the highest quality data to my downstream partners"" are the questions we are trying to solve.\n\nResponsibilities:\n""¢ Partner with multiple management teams to ensure appropriate integration of functions to meet goals as well as identify and define necessary system enhancements to deploy new products and process improvements\n""¢ Resolve variety of high impact problems/projects through in-depth evaluation of complex business processes, system processes, and industry standards\n""¢ Provide expertise in area and advanced knowledge of applications programming and ensure application design adheres to the overall architecture blueprint\n""¢ Utilize advanced knowledge of system flow and develop standards for coding, testing, debugging, and implementation\n""¢ Develop comprehensive knowledge of how areas of business, such as architecture and infrastructure, integrate to accomplish business goals\n""¢ Provide in-depth analysis with interpretive thinking to define issues and develop innovative solutions\n""¢ Serve as advisor or coach to mid-level developers and analysts, allocating work as necessary\n""¢ Appropriately assess risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding Citigroup, its clients and assets, by driving compliance with applicable laws, rules and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct and business practices, and escalating, managing and reporting control issues with transparency.\n\nMinimum Skills Required""¢ 6-10 years of relevant experience in Apps Development or systems analysis role\n""¢ Extensive experience system analysis and in programming of software applications\n""¢ Application Development using JAVA, Scala, Spark\n""¢ Familiarity with event driven applications and streaming data\n""¢ Experience with Confluent Kafka, HDFS, HIVE, structured and unstructured database systems (SQL and NoSQL)\n""¢ Experience with various schema and data types -> JSON, AVRO, Parquet, etc.\n""¢ Experience with various ELT methodologies and formats -> JDBC, ODBC, API, Web hook, SFTP, etc.\n""¢ Experience working within the Agile and version control tool sets (JIRA, Bitbucket, Git, etc.)\n""¢ Ability to adjust priorities quickly as circumstances dictate\n""¢ Demonstrated leadership and project management skills\n""¢ Consistently demonstrates clear and concise written and verbal communication",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['application software', 'system analysis', 'java', 'project management', 'applications programming', 'hive', 'scala', 'jdbc', 'bitbucket', 'sql', 'parquet', 'git', 'spark', 'data ingestion', 'json', 'debugging', 'api', 'jira', 'avro', 'odbc', 'elt', 'data engineering', 'nosql', 'app development', 'kafka', 'sftp', 'agile']",2025-06-12 05:26:04
Big Data Engineer,Rarr Technologies,6 - 8 years,Not Disclosed,['Bengaluru'],Job description\n\nProven experience working with data pipelines ETL BI regardless of the technology\nProven experience working with AWS including at least 3 of RedShift S3 EMR Cloud Formation DynamoDB RDS lambda\nBig Data technologies and distributed systems one of Spark Presto or Hive\nPython language scripting and object oriented\nFluency in SQL for data warehousing RedShift in particular is a plus\nGood understanding on data warehousing and Data modelling concepts\nFamiliar with GIT Linux CICD pipelines is a plus\nStrong systems process orientation with demonstrated analytical thinking organization skills and problem solving skills\nAbility to self manage prioritize and execute tasks in a demanding environment\nStrong consultancy orientation and experience with the ability to form collaborative\nproductive working relationships across diverse teams and cultures is a must\nWillingness and ability to train and teach others\nAbility to facilitate meetings and follow up with resulting action items,Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['python scripting', 'Big Data Technologies', 'ETL', 'AWS']",2025-06-12 05:26:07
Data Engineer,Meritus Management Service,5 - 10 years,10-20 Lacs P.A.,"['Nagpur', 'Pune', 'Gurugram']","We are looking for a skilled Data Engineer to design, build, and manage scalable data pipelines and ensure high-quality, secure, and reliable data infrastructure across our cloud and on-prem platforms.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Synapse Analytics', 'SQL', 'Azure Data Factory', 'Python', 'API Integration', 'Postgresql', 'Data Bricks', 'Scripting', 'SCALA', 'Data Lake', 'MongoDB', 'Data Warehousing', 'Data Modeling', 'ETL', 'Azure Devops']",2025-06-12 05:26:09
Data Engineer,Meritus Management Service,4 - 9 years,9-18 Lacs P.A.,"['Pune', 'Gurugram']","The first Data Engineer specializes in traditional ETL with SAS DI and Big Data (Hadoop, Hive). The second is more versatile, skilled in modern data engineering with Python, MongoDB, and real-time processing.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Etl Pipelines', 'Big Data', 'Informatica', 'SAS DI', 'SQL', 'Hive', 'Hadoop', 'Talend', 'ETL Tool', 'Python']",2025-06-12 05:26:12
Architect (Data Engineering),Amgen Inc,9 - 12 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\n\nRole Description:\n\nWe are seeking a Data Solutions Architect with deep expertise in Biotech/Pharma to design, implement, and optimize scalable and high-performance data solutions that support enterprise analytics, AI-driven insights, and digital transformation initiatives. This role will focus on data strategy, architecture, governance, security, and operational efficiency, ensuring seamless data integration across modern cloud platforms. The ideal candidate will work closely with engineering teams, business stakeholders, and leadership to establish a future-ready data ecosystem, balancing performance, cost-efficiency, security, and usability. This position requires expertise in modern cloud-based data architectures, data engineering best practices, and Scaled Agile methodologies.\n\nRoles & Responsibilities:\nDesign and implement scalable, modular, and future-proof data architectures that initiatives in enterprise.\nDevelop enterprise-wide data frameworks that enable governed, secure, and accessible data across various business domains.\nDefine data modeling strategies to support structured and unstructured data, ensuring efficiency, consistency, and usability across analytical platforms.\nLead the development of high-performance data pipelines for batch and real-time data processing, integrating APIs, streaming sources, transactional systems, and external data platforms.\nOptimize query performance, indexing, caching, and storage strategies to enhance scalability, cost efficiency, and analytical capabilities.\nEstablish data interoperability frameworks that enable seamless integration across multiple data sources and platforms.\nDrive data governance strategies, ensuring security, compliance, access controls, and lineage tracking are embedded into enterprise data solutions.\nImplement DataOps best practices, including CI/CD for data pipelines, automated monitoring, and proactive issue resolution, to improve operational efficiency.\nLead Scaled Agile (SAFe) practices, facilitating Program Increment (PI) Planning, Sprint Planning, and Agile ceremonies, ensuring iterative delivery of enterprise data capabilities.\nCollaborate with business stakeholders, product teams, and technology leaders to align data architecture strategies with organizational goals.\nAct as a trusted advisor on emerging data technologies and trends, ensuring that the enterprise adopts cutting-edge data solutions that provide competitive advantage and long-term scalability.\nMust-Have\n\nSkills:\nExperience in data architecture, enterprise data management, and cloud-based analytics solutions.\nWell versed in domain of Biotech/Pharma industry and has been instrumental in solving complex problems for them using data strategy.\nExpertise in Databricks, cloud-native data platforms, and distributed computing frameworks.\nStrong proficiency in modern data modeling techniques, including dimensional modeling, NoSQL, and data virtualization.\nExperience designing high-performance ETL/ELT pipelines and real-time data processing solutions.\nDeep understanding of data governance, security, metadata management, and access control frameworks.\nHands-on experience with CI/CD for data solutions, DataOps automation, and infrastructure as code (IaC).\nProven ability to collaborate with cross-functional teams, including business executives, data engineers, and analytics teams, to drive successful data initiatives.\nStrong problem-solving, strategic thinking, and technical leadership skills.\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with Apache Spark, Apache Airflow\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nGood-to-Have\n\nSkills:\nExperience with Data Mesh architectures and federated data governance models.\nCertification in cloud data platforms or enterprise architecture frameworks.\nKnowledge of AI/ML pipeline integration within enterprise data architectures.\nFamiliarity with BI & analytics platforms for enabling self-service analytics and enterprise reporting.\nEducation and Professional Certifications\n9 to 12 years of experience in Computer Science, IT or related field\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nSoft\n\nSkills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'continuous integration', 'technical leadership', 'metadata management', 'presentation skills', 'ci/cd', 'distributed computing', 'sql', 'data bricks', 'git', 'data modeling', 'spark', 'devops', 'data governance', 'jenkins', 'troubleshooting', 'access control', 'etl']",2025-06-12 05:26:14
Senior Data Engineer,Impetus Technologies,5 - 10 years,Not Disclosed,['United Arab Emirates'],"The Opportunity:We are seeking a highly motivated and technically strong Module Lead Software Engineer with significant expertise in Python, PySpark, and Palantir Foundry. In this role, you will be responsible for the end-to-end technical ownership, design, and delivery of a specific module or component within our enterprise data platform. You will combine hands-on development with technical leadership, ensuring the highest standards of code quality, performance, and reliability.\n\nKey Responsibilities:\n\nModule Technical Leadership & Ownership: Take full technical ownership of a specific module or component within the data platform on Palantir Foundry. This includes defining its technical roadmap, architecture, design patterns, and ensuring its integration into the broader data ecosystem.\nHands-on Development and Complex Problem Solving: Act as a lead individual contributor, developing sophisticated data pipelines, transformations, and applications using Python and PySpark within Palantir Foundry's various tools (e.g., Code Workbook, Pipeline Builder). Tackle the most challenging technical problems and implement core functionalities for the module.\nQuality Assurance and Best Practices Advocacy: Drive and enforce high standards for code quality, test coverage, documentation, and operational excellence within your module. Conduct rigorous code reviews, provide constructive feedback, and mentor engineers within your immediate scope to elevate their technical skills.\nCross-Functional Collaboration and Module Integration: Collaborate extensively with other module leads, architects, data scientists, and business stakeholders to ensure seamless integration of your module's deliverables. Proactively identify and manage technical dependencies and ensure the module aligns with overall project goals and architectural vision.\n• Performance Optimization and Troubleshooting: Continuously monitor and optimize the performance of your module's data pipelines and applications. Efficiently troubleshoot and resolve complex technical issues, data quality concerns, and system failures specific to your module.\n\nRequired Qualifications:\n\nExperience: 6-8 years of progressive experience in software development with a strong focus on data engineering.\nPython Proficiency: Expert-level proficiency in Python, including advanced programming concepts, data structures, and performance optimization techniques.\nPySpark Expertise: Strong experience with PySpark for large-scale distributed data processing, transformations, and analytics.\nPalantir Foundry: Proven, hands-on experience designing, developing, and deploying solutions within Palantir Foundry is essential.\nDeep familiarity with Foundry's data integration capabilities, Code Workbook, Pipeline Builder, Data Health checks, and Ontology modeling.\nExperience with Foundry's approach to data governance and versioning.\nSQL Skills: Excellent SQL skills for complex data querying, manipulation, and optimization.\nData Warehousing/Lakes: Solid understanding of data warehousing concepts, data lake architectures, and ETL/ELT principles.\nCloud Platforms: Experience with at least one major cloud platform (AWS, Azure, GCP), particularly with data-related services.\nVersion Control: Strong experience with Git and collaborative development workflows.\n\nPreferred Qualifications (Nice-to-Have):\n\nExperience mentoring or leading small technical teams/pods.\nFamiliarity with containerization technologies (Docker, Kubernetes).\nExperience with streaming data technologies (e.g., Kafka, Kinesis).\nUnderstanding of CI/CD pipelines for data solutions.\nKnowledge of data governance, data quality, and metadata management best practices.\nExperience in [specific industry, e.g., Financial Services, Manufacturing, Healthcare].",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Palantir Foundry', 'Python', 'Spark', 'Data Warehousing', 'SQL']",2025-06-12 05:26:16
Data Engineer,reycruit,7 - 12 years,35-40 Lacs P.A.,['Hyderabad'],"Looking for 8+ years\nPython+Azure/Aws cloud is mandatory\n1st round- virtual\n2nd round- F2F\nMust have 7+ years of relevant experience- should have hands-on experience with ETL/ELT processes, cloud-based data solutions, and big data technologies.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Pipeline', 'Big Data', 'Elt', 'ETL']",2025-06-12 05:26:18
Senior Data Engineer,S&P Global Market Intelligence,6 - 11 years,Not Disclosed,['Gurugram'],"\n\nAbout the Role: \n\nGrade Level (for internal use):\n10\n \n\nPosition summary \n\n Our proprietary software-as-a-service helps automotive dealerships and sales teams better understand and predict exactly which customers are ready to buy, the reasons why, and the key offers and incentives most likely to close the sale. Its micro-marketing engine then delivers the right message at the right time to those customers, ensuring higher conversion rates and a stronger ROI. \n\n \n\nWhat You'll Do \n\n You will be part of our Data Platform & Product Insights data engineering team. As part of this agile team, you will work in our cloud native environment to \n\n Build & support data ingestion and processing pipelines in cloud. This will entail extraction, load and transformation of big data from a wide variety of sources, both batch & streaming, using latest data frameworks and technologies \n\n Partner with product team to assemble large, complex data sets that meet functional and non-functional business requirements, ensure build out of Data Dictionaries/Data Catalogue and detailed documentation and knowledge around these data assets, metrics and KPIs. \n\n Warehouse this data, build data marts, data aggregations, metrics, KPIs, business logic that leads to actionable insights into our product efficacy, marketing platform, customer behaviour, retention etc. \n\n Build real-time monitoring dashboards and alerting systems. \n\n Coach and mentor other team members. \n\n\n \n\nWho you are \n\n 6+ years of experience in Big Data and Data Engineering. \n\n Strong knowledge of advanced SQL, data warehousing concepts and DataMart designing. \n\n Have strong programming skills in SQL, Python/ PySpark etc. \n\n Experience in design and development of data pipeline, ETL/ELT process on-premises/cloud. \n\n Experience in one of the Cloud providers GCP, Azure, AWS. \n\n Experience with relational SQL and NoSQL databases, including Postgres and MongoDB. \n\n Experience workflow management toolsAirflow, AWS data pipeline, Google Cloud Composer etc. \n\n Experience with Distributed Versioning Control environments such as GIT, Azure DevOps \n\n Building Docker images and fetch/promote and deploy to Production. Integrate Docker container orchestration framework using Kubernetes by creating pods, config Maps, deployments using terraform. \n\n Should be able to convert business queries into technical documentation. \n\n Strong problem solving and communication skills. \n\n Bachelors or an advanced degree in Computer Science or related engineering discipline. \n\n\n \n\nGood to have some exposure to \n\n Exposure to any Business Intelligence (BI) tools like Tableau, Dundas, Power BI etc. \n\n Agile software development methodologies. \n\n Working in multi-functional, multi-location teams \n\n  \n \n\nGrade10  \n \n\nLocationGurugram \n \n\nHybrid Modeltwice a week work from office  \n \n\nShift Time12 pm to 9 pm IST  \n What You'll Love About Us Do ask us about these! \n\n \n\nTotal Rewards. Monetary, beneficial and developmental rewards! \n\n \n\nWork Life Balance. You can't do a good job if your job is all you do! \n\n \n\nPrepare for the Future.Academy we are all learners; we are all teachers! \n\n \n\nEmployee Assistance Program. Confidential and Professional Counselling and Consulting. \n\n \n\nDiversity & Inclusion. HeForShe! \n\n \n\nInternal Mobility.\n\nGrow with us! \n\n  \n  \n  \n  \n\nAbout automotiveMastermind\n\nWho we are:\n\nFounded in 2012, automotiveMastermind is a leading provider of predictive analytics and marketing automation solutions for the automotive industry and believes that technology can transform data, revealing key customer insights to accurately predict automotive sales. Through its proprietary automated sales and marketing platform, Mastermind, the company empowers dealers to close more deals by predicting future buyers and consistently marketing to them. automotiveMastermind is headquartered in New York City. For more information, visit automotivemastermind.com.\n\nAt automotiveMastermind, we thrive on high energy at high speed. Were an organization in hyper-growth mode and have a fast-paced culture to match. Our highly engaged teams feel passionately about both our product and our people. This passion is what continues to motivate and challenge our teams to be best-in-class. Our cultural values of Drive and Help have been at the core of what we do, and how we have built our culture through the years. This cultural framework inspires a passion for success while collaborating to win.\n\nWhat we do:\n\nThrough our proprietary automated sales and marketing platform, Mastermind, we empower dealers to close more deals by predicting future buyers and consistently marketing to them. In short, we help automotive dealerships generate success in their loyalty, service, and conquest portfolios through a combination of turnkey predictive analytics, proactive marketing, and dedicated consultative services.\n\nWhats In It For\n\nYou\n\nOur Purpose:\n\nProgress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technologythe right combination can unlock possibility and change the world.Our world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence, pinpointing risks and opening possibilities. We Accelerate Progress.\n\nOur People:\n\nOur Values:\n\nIntegrity, Discovery, Partnership\n\nAt S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of\n\nintegrity in all we do, bring a spirit of\n\ndiscovery to our work, and collaborate in close\n\npartnership with each other and our customers to achieve shared goals.\n\nBenefits:\n\nWe take care of you, so you cantake care of business. We care about our people. Thats why we provide everything youand your careerneed to thrive at S&P Global.\n\nHealth & WellnessHealth care coverage designed for the mind and body.\n\n\n\nContinuous LearningAccess a wealth of resources to grow your career and learn valuable new skills.\n\nInvest in Your FutureSecure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\n\nFamily Friendly PerksIts not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\n\nBeyond the BasicsFrom retail discounts to referral incentive awardssmall perks can make a big difference.\n\nFor more information on benefits by country visithttps://spgbenefits.com/benefit-summaries",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['advance sql', 'python', 'pyspark', 'sql', 'data warehousing concepts', 'kubernetes', 'microsoft azure', 'data warehousing', 'power bi', 'relational sql', 'elt', 'data engineering', 'business intelligence', 'azure devops', 'docker', 'nosql', 'tableau', 'git', 'postgresql', 'gcp', 'agile', 'big data', 'aws', 'mongodb']",2025-06-12 05:26:21
Senior Big Data Engineer,Meritus Management Service,6 - 8 years,20-30 Lacs P.A.,"['Nagpur', 'Pune']","Build and maintain scalable Big Data pipelines using Hadoop, PySpark, and SQL for batch and real-time processing.\nCollaborate with cross-functional teams to transform, optimize, and secure large datasets while ensuring data quality and performance.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hadoop', 'Python', 'Big Data']",2025-06-12 05:26:23
Azure Data Engineer,HTC Global Services,4 - 8 years,Not Disclosed,['Bengaluru( Murugeshpalya )'],"Job Summary:\nWe are looking for a highly skilled Azure Data Engineer with experience in building and managing scalable data pipelines using Azure Data Factory, Synapse, and Databricks. The ideal candidate should be proficient in big data tools and Azure services, with strong programming knowledge and a solid understanding of data architecture and cloud platforms.\n\nKey Responsibilities:",,,,"['Power Bi', 'Azure Databricks', 'Azure Data Factory', 'Synapse', 'Python', 'Java', 'Scala', 'Kafka', 'big data tools', 'SQL', 'EventHub', 'Azure cloud services', 'Spark']",2025-06-12 05:26:25
Lead Data Engineer,Conduent,8 - 13 years,Not Disclosed,['Noida'],"Job Overview \n\nWe are looking for a Data Engineer who will be part of our Analytics Practice and will be expected to actively work in a multi-disciplinary fast paced environment. This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project; its primary responsibility is the acquisition, transformation, loading and processing of data from a multitude of disparate data sources, including structured and unstructured data for advanced analytics and machine learning in a big data environment.\n\n\n Responsibilities: \nEngineer a modern data pipeline to collect, organize, and process data from disparate sources.\nPerforms data management tasks, such as conduct data profiling, assess data quality, and write SQL queries to extract and integrate data\nDevelop efficient data collection systems and sound strategies for getting quality data from different sources\nConsume and analyze data from the data pool to support inference, prediction and recommendation of actionable insights to support business growth.\nDesign and develop ETL processes using tools and scripting. Troubleshoot and debug ETL processes. Performance tuning and opitimization of the ETL processes.\nProvide support to new of existing applications while recommending best practices and leading projects to implement new functionality.\nCollaborate in design reviews and code reviews to ensure standards are met. Recommend new standards for visualizations.\nLearn and develop new ETL techniques as required to keep up with the contemporary technologies.\nReviews the solution requirements and architecture to ensure selection of appropriate technology, efficient use of resources and integration of multiple systems and technology.\nSupport presentations to Customers and Partners\nAdvising on new technology trends and possible adoption to maintain competitive advantage\n\n\n Experience Needed: \n8+ years of related experience is required.\nA BS or Masters degree in Computer Science or related technical discipline is required\nETL experience with data integration to support data marts, extracts and reporting\nExperience connecting to varied data sources\nExcellent SQL coding experience with performance optimization for data queries.\nUnderstands different data models like normalized, de-normalied, stars, and snowflake models. Worked with transactional, temporarl, time series, and structured and unstructured data.\nExperience on Azure Data Factory and Azure Synapse Analytics\nWorked in big data environments, cloud data stores, different RDBMS and OLAP solutions.\nExperience in cloud-based ETL development processes.\nExperience in deployment and maintenance of ETL Jobs.\nIs familiar with the principles and practices involved in development and maintenance of software solutions and architectures and in service delivery.\nHas strong technical background and remains evergreen with technology and industry developments.\nAt least 3 years of demonstrated success in software engineering, release engineering, and/or configuration management.\nHighly skilled in scripting languages like PowerShell.\nSubstantial experience in the implementation and exectuion fo CI/CD processes.\n\n\n Additional  \nDemonstrated ability to have successfully completed multiple, complex technical projects\nPrior experience with application delivery using an Onshore/Offshore model\nExperience with business processes across multiple Master data domains in a services based company\nDemonstrates a rational and organized approach to the tasks undertaken and an awareness of the need to achieve quality.\nDemonstrates high standards of professional behavior in dealings with clients, colleagues and staff.\nIs able to make sound and far reaching decisions alone on major issues and to take full responsibility for them on a technical basis.\nStrong written communication skills. Is effective and persuasive in both written and oral communication.\nExperience with gathering end user requirements and writing technical documentation\nTime management and multitasking skills to effectively meet deadlines under time-to-market pressure",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql coding', 'sql', 'configuration management', 'software engineering', 'release engineering', 'continuous integration', 'rdbms', 'sql queries', 'performance tuning', 'azure synapse', 'ci/cd', 'azure data factory', 'machine learning', 'data engineering', 'powershell', 'olap', 'etl', 'big data']",2025-06-12 05:26:28
Big Data Engineer,Client of Hiresquad Resources,5 - 8 years,22.5-30 Lacs P.A.,"['Noida', 'Hyderabad', 'Bengaluru']","Role: Data Engineer\nExp: 5 to 8 Years\nLocation: Bangalore, Noida, and Hyderabad (Hybrid, weekly 2 Days office must)\nNP: Immediate to 15 Days (Try to find only immediate joiners)\n\n\nNote:\nCandidate must have experience in Python, Kafka Streams, Pyspark, and Azure Databricks.\nNot looking for candidates who have only Exp in Pyspark and not in Python.\n\n\nJob Title: SSE Kafka, Python, and Azure Databricks (Healthcare Data Project)\nExperience:  5 to 8 years\n\nRole Overview:\nWe are looking for a highly skilled with expertise in Kafka, Python, and Azure Databricks (preferred) to drive our healthcare data engineering projects. The ideal candidate will have deep experience in real-time data streaming, cloud-based data platforms, and large-scale data processing. This role requires strong technical leadership, problem-solving abilities, and the ability to collaborate with cross-functional teams.\n\nKey Responsibilities:\nLead the design, development, and implementation of real-time data pipelines using Kafka, Python, and Azure Databricks.\nArchitect scalable data streaming and processing solutions to support healthcare data workflows.\nDevelop, optimize, and maintain ETL/ELT pipelines for structured and unstructured healthcare data.\nEnsure data integrity, security, and compliance with healthcare regulations (HIPAA, HITRUST, etc.).\nCollaborate with data engineers, analysts, and business stakeholders to understand requirements and translate them into technical solutions.\nTroubleshoot and optimize Kafka streaming applications, Python scripts, and Databricks workflows.\nMentor junior engineers, conduct code reviews, and ensure best practices in data engineering.\nStay updated with the latest cloud technologies, big data frameworks, and industry trends.\n\n\nRequired Skills & Qualifications:\n4+ years of experience in data engineering, with strong proficiency in Kafka and Python.\nExpertise in Kafka Streams, Kafka Connect, and Schema Registry for real-time data processing.\nExperience with Azure Databricks (or willingness to learn and adopt it quickly).\nHands-on experience with cloud platforms (Azure preferred, AWS or GCP is a plus).\nProficiency in SQL, NoSQL databases, and data modeling for big data processing.\nKnowledge of containerization (Docker, Kubernetes) and CI/CD pipelines for data applications.\nExperience working with healthcare data (EHR, claims, HL7, FHIR, etc.) is a plus.\nStrong analytical skills, problem-solving mindset, and ability to lead complex data projects.\nExcellent communication and stakeholder management skills.\n\n\n\nEmail: Sam@hiresquad.in",Industry Type: Medical Services / Hospital,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Kafka', 'Azure Databricks', 'Python', 'Etl Pipelines', 'Pyspark', 'spark architecture', 'Data Engineering', 'opps concepts', 'Data Streaming', 'Medallion Architecture', 'python scripts', 'schema registry', 'SQL Database', 'Nosql Databases', 'spark tuning', 'Kafka Streams', 'kafka connect']",2025-06-12 05:26:31
Collibra Data Governance Engineer,Allegis Group,6 - 11 years,Not Disclosed,[],"Collibra Data Governance Engineer\nJob Location : Hyderabad / Bangalore / Chennai / Noida/ Gurgaon / Pune / Indore / Mumbai/ Kolkata\nRequired Skills\n5+ years of experience in data governance and/or metadata management.\nHands-on experience with Collibra Data Governance Center (Collibra DGC), including workflow configuration, cataloging, and operating model customization.\nStrong knowledge of metadata management, data lineage, and data quality principles.\nHands-on experience with Snowflake\nFamiliarity with data integration tools and AWS cloud platform\nExperience with SQL and working knowledge of relational databases.\nUnderstanding of data privacy regulations (e.g., GDPR, CCPA) and compliance frameworks.\nPreferred Skills\nCertifications such as Collibra Certified Solution Architect.\nExperience integrating Collibra with tools like Snowflake, Tableau or other BI/analytics platforms.\nExposure to DataOps, MDM (Master Data Management), and data governance frameworks like DAMA-DMBOK.\nStrong communication and stakeholder management skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Collibra', 'Metadata', 'Data Governance']",2025-06-12 05:26:33
Data Engineer,Lance Labs,7 - 12 years,Not Disclosed,"['Noida', 'Chennai']","Deployment, configuration & maintenance of Databricks clusters & workspaces\nSecurity & Access Control\nAutomate administrative task using tools like Python, PowerShell &Terraform\nIntegrations with Azure Data Lake, Key Vault & implement CI/CD pipelines\n\nRequired Candidate profile\nAzure, AWS, or GCP; Azure experience is preferred\nStrong skills in Python, PySpark, PowerShell & SQL\nExperience with Terraform\nETL processes, data pipeline &big data technologies\nSecurity & Compliance",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Azure Databricks', 'SQL', 'Terraform', 'Python', 'Powershell', 'Ci/Cd', 'Data Pipeline', 'GCP', 'Azure Cloud', 'Azure Data Lake', 'ETL', 'AWS', 'Data Governance', 'Azure Devops']",2025-06-12 05:26:35
Senior Data Engineer,Talentien Global Solutions,4 - 8 years,12-18 Lacs P.A.,"['Hyderabad', 'Chennai', 'Coimbatore']","We are seeking a skilled and motivated Data Engineer to join our dynamic team. The ideal candidate will have experience in designing, developing, and maintaining scalable data pipelines and architectures using Hadoop, PySpark, ETL processes, and Cloud technologies.\n\nResponsibilities:\nDesign, develop, and maintain data pipelines for processing large-scale datasets.\nBuild efficient ETL workflows to transform and integrate data from multiple sources.\nDevelop and optimize Hadoop and PySpark applications for data processing.\nEnsure data quality, governance, and security standards are met across systems.\nImplement and manage Cloud-based data solutions (AWS, Azure, or GCP).\nCollaborate with data scientists and analysts to support business intelligence initiatives.\nTroubleshoot performance issues and optimize query executions in big data environments.\nStay updated with industry trends and advancements in big data and cloud technologies.\nRequired Skills:\nStrong programming skills in Python, Scala, or Java.\nHands-on experience with Hadoop ecosystem (HDFS, Hive, Spark, etc.).\nExpertise in PySpark for distributed data processing.\nProficiency in ETL tools and workflows (SSIS, Apache Nifi, or custom pipelines).\nExperience with Cloud platforms (AWS, Azure, GCP) and their data-related services.\nKnowledge of SQL and NoSQL databases.\nFamiliarity with data warehousing concepts and data modeling techniques.\nStrong analytical and problem-solving skills.\n\nInterested can reach us at +91 7305206696/ saranyadevib@talentien.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Hadoop', 'Spark', 'ETL', 'Airflow', 'Etl Pipelines', 'Big Data', 'EMR', 'Gcp Cloud', 'Data Bricks', 'Azure Cloud', 'Data Pipeline', 'SCALA', 'Snowflake', 'Data Lake', 'Data Warehousing', 'Data Modeling', 'AWS', 'Python']",2025-06-12 05:26:37
"Senior Data Engineer (Snowflake, DBT)",Allegis Global Solutions (AGS),5 - 10 years,Not Disclosed,[],"Senior Data Engineer (Snowflake, DBT, Azure)\nJob Location: Hyderabad / Bangalore / Chennai / Kolkata / Noida/ Gurgaon / Pune / Indore / Mumbai\n\nJob Details\nTechnical Expertise:\nStrong proficiency in Snowflake architecture, including data sharing, partitioning, clustering, and materialized views.\nAdvanced experience with DBT for data transformations and workflow management.\nExpertise in Azure services, including Azure Data Factory, Azure Data Lake, Azure Synapse, and Azure Functions.\nData Engineering:\nProficiency in SQL, Python, or other relevant programming languages.\nStrong understanding of data modeling concepts, including star schema and normalization.\nHands-on experience with ETL/ELT pipelines and data integration tools.\n\nPreferred Qualifications:\nCertifications in Snowflake, DBT, or Azure Data Engineering.\nFamiliar with data visualization tools like Power BI or Tableau.\nKnowledge of CI/CD pipelines and DevOps practices for data workflows.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Data Build Tool', 'Azure']",2025-06-12 05:26:39
"Senior Data Engineer - Airflow, PLSQL",Relanto Global,5 - 10 years,Not Disclosed,['Bengaluru'],"PositionSenior Data Engineer - Airflow, PLSQL \n\n Experience5+ Years \n\n LocationBangalore/Hyderabad/Pune \n\n\n\nSeeking a Senior Data Engineer with strong expertise in Apache Airflow and Oracle PL/SQL, along with working experience in Snowflake and Agile methodologies. The ideal candidate will also take up Scrum Master responsibilities and lead a data engineering scrum team to deliver robust, scalable data solutions.\n\n\n Key Responsibilities: \nDesign, develop, and maintain scalable data pipelines using Apache Airflow.\nWrite and optimize complex PL/SQL queries, procedures, and packages on Oracle databases.\nCollaborate with cross-functional teams to design efficient data models and integration workflows.\nWork with Snowflake for data warehousing and analytics use cases.\nOwn the delivery of sprint goals, backlog grooming, and facilitation of agile ceremonies as the Scrum Master.\nMonitor pipeline health and troubleshoot production data issues proactively.\nEnsure code quality, documentation, and best practices across the team.\nMentor junior data engineers and promote a culture of continuous improvement.\n\n\n Required Skills and Qualifications: \n5+ years of experience as a Data Engineer in enterprise environments.\nStrong expertise in  Apache Airflow  for orchestrating workflows.\nExpert in  Oracle PL/SQL  - stored procedures, performance tuning, debugging.\nHands-on experience with  Snowflake  - data modeling, SQL, optimization.\nWorking knowledge of version control (Git) and CI/CD practices.\nPrior experience or certification as a  Scrum Master  is highly desirable.\nStrong analytical and problem-solving skills with attention to detail.\nExcellent communication and leadership skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'plsql', 'stored procedures', 'oracle pl', 'performance tuning', 'hive', 'continuous integration', 'ci/cd', 'data warehousing', 'pyspark', 'git', 'apache', 'data modeling', 'spark', 'debugging', 'hadoop', 'big data', 'snowflake', 'python', 'oracle', 'sql queries', 'airflow', 'data engineering', 'agile', 'sqoop']",2025-06-12 05:26:42
Senior Azure Data Engineer (Only Immediate Join),Adecco,7 - 12 years,15-30 Lacs P.A.,['Bengaluru'],"Position : Senior Azure Data Engineer (Only Immediate Joiner)\nLocation : Bangalore\nMode of Work : Work from Office\nExperience : 7 years relevant experience\nJob Type : Full Time (On Roll)\n\nJob Description\nRoles and Responsibilities:\nThe Data Engineer will work on data engineering projects for various business units, focusing on delivery of complex data management solutions by leveraging industry best practices. They work with the project team to build the most efficient data pipelines and data management solutions that make data easily available for consuming applications and analytical solutions. A Data engineer is expected to possess strong technical skills.\nKey Characteristics\nTechnology champion who constantly pursues skill enhancement and has inherent curiosity to understand work from multiple dimensions.\nInterest and passion in Big Data technologies and appreciates the value that can be brought in with an effective data management solution.\nHas worked on real data challenges and handled high volume, velocity, and variety of data.\nExcellent analytical & problem-solving skills, willingness to take ownership and resolve technical challenges.\nContributes to community building initiatives like CoE, CoP.\nMandatory skills:\nAzure - Master\nELT - Skill\nData Modeling - Skill\nData Integration & Ingestion - Skill\nData Manipulation and Processing - Skill\nGITHUB, Action, Azure DevOps - Skill\nData factory, Databricks, SQL DB, Synapse, Stream Analytics, Glue, Airflow, Kinesis, Redshift, SonarQube, PyTest - Skill\nOptional skills:\nExperience in project management, running a scrum team.\nExperience working with BPC, Planning.\nExposure to working with external technical ecosystem.\nMKDocs documentation\n\nInterested candidates kindly share your CV and below details to usha.sundar@adecco.com\n1) Present CTC (Fixed + VP) -\n2) Expected CTC -\n3) No. of years experience -\n4) Notice Period -\n5) Offer-in hand -\n6) Reason of Change -\n7) Present Location -",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Pyspark', 'SCALA', 'Azure Data Lake', 'Databricks', 'Stream Analytics', 'Azure Data Engineering', 'Azure Databricks', 'Data factory', 'Streaming data', 'Data Bricks', 'SQL']",2025-06-12 05:26:44
Sr Data Engineer - Remote,Teamplus Staffing Solution,5 - 10 years,12-18 Lacs P.A.,"['Pune', 'Bengaluru', 'Delhi / NCR']","SQL, SNOWFLAKE, TABLEAU\nSQL, SNOWFLAKE,DBT, Datawarehousing\nSQL, SNOWFLAKE, Python, DBT, Datawarehousing\nSQL, SNOWFLAKE, Datawarehousing, any ETL tool(preffered is Matillion)\nSQL, SNOWFLAKE, TABLEAU",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Matillion', 'SNOWFLAKE', 'Data Warehousing', 'TABLEAU', 'SQL', 'dbt', 'ETL', 'Python']",2025-06-12 05:26:46
Data Lineage Engineers,Altimetrik,5 - 10 years,15-30 Lacs P.A.,"['Pune', 'Chennai', 'Bengaluru']","Role & responsibilities\nSkill need :  Data Lineage with Ab-initio - Metadata hub\n\nSkills & Experience:\nExpertise in mHub or similar tools, data pipelines, and cloud platforms.\nProficiency in Python, Oracle, SQL, Java, and ETL tools.\n5-10 years of experience in data engineering and governance.",,,,"['Ab-initio', 'Metadata hub', 'Python', 'mhub', 'ETL', 'Oracle', 'SQL']",2025-06-12 05:26:49
Senior Data Engineer,Epsilon,5 - 9 years,Not Disclosed,['Bengaluru'],"This position in the Engineering team under the Digital Experience organization. We drive the first mile of the customer experience through personalization of offers and content. We are currently on the lookout for a smart, highly driven engineer.\nYou will be part of a team that is focused on building & managing solutions, pipelines using marketing technology stacks. You will also be expected to Identify and implement improvements including for optimizing data delivery and automate processes/pipelines.\nThe incumbent is also expected to partner with various stakeholders, bring scientific rigor to design and develop high quality solutions.\nCandidate must have excellent verbal and written communication skills and be comfortable working in an entrepreneurial, startup environment within a larger company.\nClick here to view how Epsilon transforms marketing with 1 View, 1 Vision and 1 Voice.\n\nBrief Description of Role:\nExperience with both structured and unstructured data\nExperience working on AdTech or MarTech technologies.\nExperience in relational and non-relational databases and SQL (NoSQL is a plus).\nUnderstanding of Data Modeling, Data Catalog concepts and tools\nAbility to deal with data imperfections such as missing values, outliers, inconsistent formatting, etc.\nCollaborate with other members of the team to ensure high quality deliverables\nLearning and implementing the latest design patterns in data engineering\n\nData Management\nExperience with both structured and unstructured data\nExperience building Data and CI/CD pipelines\nExperience working on AdTech or MarTech technologies is added advantage\nExperience in relational and non-relational databases and SQL (NoSQL is a plus).\nHands on experience building ETL workflows/pipelines on large volumes of data\nGood understanding of Data Modeling, Data Warehouse, Data Catalog concepts and tools\nAble to identify, join, explore, and examine data from multiple disparate sources and formats\nAbility to reduce large quantities of unstructured or formless data and get it into a form in which it can be analyzed\nAbility to deal with data imperfections such as missing values, outliers, inconsistent formatting, etc.\nDevelopment\nAbility to write code in programming languages such as Python and shell script on Linux\nFamiliarity with development methodology such as Agile/Scrum\nLove to learn new technologies, keep abreast of the latest technologies within the cloud architecture, and drive your organization to adapt to emerging best practices\nGood knowledge of working in UNIX/LINUX systems\nQualifications\nBachelors degree in computer science with 5+ years of similar experience\nTech Stack: Python, SQL, Scripting language (preferably JavaScript)\nExperience or knowledge on Adobe Experience Platform (RT-CDP/AEP)\nExperience working in Cloud Platforms (GCP or AWS)\nFamiliarity with automated unit/integration test frameworks\nGood written and spoken communication skills, team player.\nStrong analytic thought process and ability to interpret findings",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Data Bricks', 'Python', 'SQL', 'Azure Aws', 'AWS']",2025-06-12 05:26:51
Senior Data Engineer,Wavicle Data Solutions,6 - 11 years,15-25 Lacs P.A.,"['Chennai', 'Coimbatore', 'Bengaluru']","Hi Professionals,\n\nWe are looking for Senior Data Engineer for Permanent Role\n\nWork Location: Hybrid Chennai, Coimbatore or Bangalore\n\nExperience: 6 to 12 Years\n\nNotice Period: 0 TO 15 Days or Immediate Joiner.\n\nSkills:\n1. Python\n2. Pyspark\n3. SQL\n4. AWS\n5. GCP\n6. MLOps\n\nInterested can send your resume to gowtham.veerasamy@wavicledata.com.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'GCP', 'AWS', 'Ml']",2025-06-12 05:26:54
"Senior Data Engineer (Exp into Azure Databricks,Pyspark, SQL)",Adecco India,7 - 12 years,22.5-30 Lacs P.A.,"['Pune', 'Bengaluru']","Job Role & responsibilities:-\n\nUnderstanding operational needs by collaborating with specialized teams\nSupporting key business operations. This involves architecture designing, building and deploying data systems, pipelines etc\nDesigning and implementing agile, scalable, and cost efficiency solution on cloud data services.\nLead a team of developers, implement Sprint planning and executions to ensure timely deliveries\n\nTechnical Skill, Qualification & experience required:-\n\n7-10 years of experience in Azure Cloud Data Engineering, Azure Databricks, datafactory , Pyspark, SQL,Python\nHands on experience in Data Engineer, Azure Databricks, Data factory, Pyspark, SQL\nProficient in Cloud Services Azure\nArchitect and implement ETL and data movement solutions.\nMigrate data from traditional database systems to Cloud environment\nStrong hands-on experience for working with Streaming dataset\nBuilding Complex Notebook in Databricks to achieve business Transformations.\nHands-on Expertise in Data Refinement using Pyspark and Spark SQL\nFamiliarity with building dataset using Scala.\nFamiliarity with tools such as Jira and GitHub\nExperience leading agile scrum, sprint planning and review sessions\nGood communication and interpersonal skills\nComfortable working in a multidisciplinary team within a fast-paced environment\n\n* Immediate Joiners will be preferred only",Industry Type: Insurance,Department: Other,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineer', 'Azure data engineer', 'Data Bricks', 'SQL', 'Data Engineering', 'Python']",2025-06-12 05:26:57
Senior Data Engineer,SPAN.IO,5 - 10 years,Not Disclosed,['Bengaluru( Indira Nagar )'],"Senior Data Engineer\n\nOur Mission\n\nSPAN is enabling electrification for all\nWe are a mission-driven company designing, building, and deploying products that electrify the built environment, reduce carbon emissions, and slow the effects of climate change.\nDecarbonization is the process to reduce or remove greenhouse gas emissions, especially carbon dioxide, from entering our atmosphere.\nElectrification is the process of replacing fossil fuel appliances that run on gas or oil with all-electric upgrades for a cleaner way to power our lives.\n\nAt SPAN, we believe in:\nEnabling homes and vehicles powered by clean energy\nMaking electrification upgrades possible\nBuilding more resilient homes with reliable backup\nDesigning a flexible and distributed electrical grid\n\nThe Role\nAs a Data Engineer you would be working to design, build, test and create infrastructure necessary for real time analytics and batch analytics pipelines. You will work with multiple teams within the org to provide analysis, insights on the data. You will also be involved in writing ETL processes that support data ingestion. You will also guide and enforce best practices for data management, governance and security. You will build infrastructure to monitor these data pipelines / ETL jobs / tasks and create tooling/infrastructure for providing visibility into these.\n\nResponsibilities\nWe are looking for a Data Engineer with passion for building data pipelines, working with product, data science and business intelligence teams and delivering great solutions. As a part of the team you:-\nAcquire deep business understanding on how SPAN data flows from IoT device to cloud through the system and build scalable and optimized data solutions that impact many stakeholders.\nBe an advocate for data quality and excellence of our platform.\nBuild tools that help streamline the management and operation of our data ecosystem.\nEnsure best practices and standards in our data ecosystem are shared across teams.\nWork with teams within the company to build close relationships with our partners to understand the value our platform can bring and how we can make it better.\nImprove data discovery by creating data exploration processes and promoting adoption of data sources across the company.\nHave a desire to write tools and applications to automate work rather than do everything by hand.\nAssist internal teams in building out data logging, alerting and monitoring for their applications\nAre passionate about CI/CD process.\nDesign, develop and establish KPIs to monitor analysis and provide strategic insights to drive growth and performance.\n\nAbout You\n\nRequired Qualifications\nBachelor's Degree in a quantitative discipline: computer science, statistics, operations research, informatics, engineering, applied mathematics, economics, etc.\n5+ years of relevant work experience in data engineering, business intelligence, research or related fields.\nExpert level production-grade, programming experience in at least one of these languages (Python, Kotlin, or other JVM based languages)\nExperience in writing clean, concise and well structured code in one of the above languages.\nExperience working with Infrastructure-as-code tools: Pulumi, Terraform, etc.\nExperience working with CI/CD systems: Circle-CI, Github Actions, Argo-CD, etc.\nExperience managing data engineering infrastructure through Docker and Kubernetes\nExperience working with latency data processing solutions like Flink, Prefect, AWS Kinesis, Kafka, Spark Stream processing etc.\nExperience with SQL/Relational databases, OLAP databases like Snowflake.\nExperience working in AWS: S3, Glue, Athena, MSK, EMR, ECR etc.\n\nBonus Qualifications\nExperience with the Energy industry\nExperience with building IoT and/or hardware products\nUnderstanding of electrical systems and residential loads\nExperience with data visualization using Tableau.\nExperience in Data loading tools like FiveTran as well as data debugging tools such as DataDog\n\nLife at SPAN\nOur Bengaluru team plays a pivotal role in SPANs continued growth and expansion. Together, were driving engineering, product development, and operational excellence to shape the future of home energy solutions.\nAs part of our team in India, youll have the opportunity to collaborate closely with our teams in the US and across the globe. This international collaboration fosters innovation, learning, and growth, while helping us achieve our bold mission of electrifying homes and advancing clean energy solutions worldwide.\nOur in-office culture offers the chance for dynamic interactions and hands-on teamwork, making SPAN a truly collaborative environment where every team members contribution matters.\nOur climate-focused culture is driven by a team of forward-thinkers, engineers, and problem-solvers who push boundaries every day.\nDo mission-driven work: Every role at SPAN directly advances clean energy adoption.\nBring powerful ideas to life: We encourage diverse ideas and perspectives to drive stronger products.\nNurture an innovation-first mindset: We encourage big thinking and bold action.\nDeliver exceptional customer value: We value hard work, and the ability to deliver exceptional customer value.\n\nBenefits at SPAN India\nGenerous paid leave\nComprehensive Insurance & Health Benefits\nCentrally located office in Bengaluru with easy access to public transit, dining, and city amenities\n\nInterested in joining our team? Apply today and well be in touch with the next steps!",Industry Type: Electronics Manufacturing,"Department: Production, Manufacturing & Engineering","Employment Type: Full Time, Permanent","['Terraform', 'Snowflake', 'AWS', 'Python', 'SQL', 'Java', 'Apache Flink', 'Kotlin']",2025-06-12 05:26:59
Technical Lead - Sr. Data Engineer,Edgematics Consulting,8 - 13 years,Not Disclosed,['Pune'],"About This Role :\n\nWe are looking for a talented and experienced Data Engineer with Tech Lead with hands-on expertise in any ETL Tool with full knowledge about CI/CD practices with leading a team technically more than 5 and client facing and create Data Engineering, Data Quality frameworks. As a tech lead must ensure to build ETL jobs, Data Quality Jobs, Big Data Jobs performed performance optimization by understanding the requirements, create re-usable assets and able to perform production deployment and preferably worked in DWH appliances Snowflake / redshift / Synapse\n\nResponsibilities\nWork with a team of engineers in designing, developing, and maintaining scalable and efficient data solutions using Any Data Integration (any ETL tool like Talend / Informatica) and any Big Data technologies.\nDesign, develop, and maintain end-to-end data pipelines using Any ETL Data Integration (any ETL tool like Talend / Informatica) to ingest, process, and transform large volumes of data from heterogeneous sources.\nHave good experience in designing cloud pipelines using Azure Data Factory or AWS Glues/Lambda.\nImplemented Data Integration end to end with any ETL technologies.\nImplement database solutions for storing, processing, and querying large volumes of structured and unstructured and semi-structured data\nImplement Job Migrations of ETL Jobs from Older versions to New versions.\nImplement and write advanced SQL scripts in SQL Database at medium to expert level. \nWork with technical team with client and provide guidance during technical challenges.\nIntegrate and optimize data flows between various databases, data warehouses, and Big Data platforms.\nCollaborate with cross-functional teams to gather data requirements and translate them into scalable and efficient data solutions.\nOptimize ETL, Data Load performance, scalability, and cost-effectiveness through optimization techniques.\nInteract with Client on a daily basis and provide technical progress and respond to technical questions.\nImplement best practices for data integration.\nImplement complex ETL data pipelines or similar frameworks to process and analyze massive datasets.\nEnsure data quality, reliability, and security across all stages of the data pipeline.\nTroubleshoot and debug data-related issues in production systems and provide timely resolution.\nStay current with emerging technologies and industry trends in data engineering technologies, CI/CD, and incorporate them into our data architecture and processes.\nOptimize data processing workflows and infrastructure for performance, scalability, and cost-effectiveness.\nProvide technical guidance and foster a culture of continuous learning and improvement.\nImplement and automate CI/CD pipelines for data engineering workflows, including testing, deployment, and monitoring.\nPerform migration to production deployment from lower environments, test & validate\n\nMust Have Skills\nMust be certified in any ETL tools, Database, Cloud.(Snowflake certified is more preferred)\nMust have implemented at least 3 end-to-end projects in Data Engineering.\nMust have worked on performance management optimization and tuning for data loads, data processes, data transformation in big data\nMust be flexible to write code using JAVA/Scala/Python etc. as required\nMust have implemented CI/CD pipelines using tools like Jenkins, GitLab CI, or AWS CodePipeline.\nMust have managed a team technically of min 5 members and guided the team technically.\nMust have the Technical Ownership capability of Data Engineering delivery.\nStrong communication capabilities with client facing.\nBachelor's or Master's degree in Computer Science, Engineering, or a related field.\n5 years of experience in software engineering or a related role, with a strong focus on Any ETL Tool, database, integration.\nProficiency in Any ETL tools like Talend , Informatica etc for Data Integration for building and orchestrating data pipelines.\nHands-on experience with relational databases such as MySQL, PostgreSQL, or Oracle, and NoSQL databases such as MongoDB, Cassandra, or Redis.\nSolid understanding of database design principles, data modeling, and SQL query optimization.\nExperience with data warehousing, Data Lake , Delta Lake concepts and technologies, data modeling, and relational databases.",Industry Type: IT Services & Consulting,"Department: Production, Manufacturing & Engineering","Employment Type: Full Time, Permanent","['Data Engineering', 'Snowflake', 'ETL', 'Azure Aws', 'Data Management', 'Big Data', 'Ci/Cd', 'Data Integration', 'Data Quality', 'Data Pipeline', 'Data Warehousing', 'Data Modeling', 'Data Governance']",2025-06-12 05:27:02
Lead AWS Glue Data Engineer,Allegis Group,8 - 13 years,Not Disclosed,[],"Lead AWS Glue Data Engineer\nJob Location : Hyderabad / Bangalore / Chennai / Noida/ Gurgaon / Pune / Indore / Mumbai/ Kolkata\n\nWe are seeking a skilled Lead AWS Data Engineer with 8+ years of strong programming and SQL skills to join our team. The ideal candidate will have hands-on experience with AWS Data Analytics services and a basic understanding of general AWS services. Additionally, prior experience with Oracle and Postgres databases and secondary skills in Python and Azure DevOps will be an advantage.\n\nKey Responsibilities:\nDesign, develop, and optimize data pipelines using AWS Data Analytics services such as RDS, DMS, Glue, Lambda, Redshift, and Athena.\nImplement data migration and transformation processes using AWS DMS and Glue.\nWork with SQL (Oracle & Postgres) to query, manipulate, and analyse large datasets.\nDevelop and maintain ETL/ELT workflows for data ingestion and transformation.\nUtilize AWS services like S3, IAM, CloudWatch, and VPC to ensure secure and efficient data operations.\nWrite clean and efficient Python scripts for automation and data processing.\nCollaborate with DevOps teams using Azure DevOps for CI/CD pipelines and infrastructure management.\nMonitor and troubleshoot data workflows to ensure high availability and performance.\n\nPreferred Qualifications:\nAWS certifications in Data Analytics, Solutions Architect, or DevOps.\nExperience with data warehousing concepts and data lake implementations.\nHands-on experience with Infrastructure as Code (IaC) tools like Terraform or CloudFormation.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['RDS', 'Glue', 'DMS', 'Lambda', 'Redshift', 'Athena']",2025-06-12 05:27:04
Data Engineer - SAS Migration,Crisil,2 - 4 years,Not Disclosed,['Mumbai'],"The SAS to Databricks Migration Developer will be responsible for migrating existing SAS code, data processes, and workflows to the Databricks platform. This role requires expertise in both SAS and Databricks, with a focus on converting SAS logic into scalable PySpark and Python code. The developer will design, implement, and optimize data pipelines, ensuring seamless integration and functionality within the Databricks environment. Collaboration with various teams is essential to understand data requirements and deliver solutions that meet business needs",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'scala', 'pyspark', 'data warehousing', 'data migration', 'sql', 'spark', 'gcp', 'mysql', 'hadoop', 'bigquery', 'big data', 'etl', 'python', 'sas', 'teradata', 'airflow', 'microsoft azure', 'data engineering', 'sql server', 'dataproc', 'data bricks', 'cloud data flow', 'kafka', 'migration', 'sqoop', 'data flow']",2025-06-12 05:27:07
Data Engineering - Senior Developer with Salesforce,Job Delights,5 - 10 years,25-27.5 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Data Engineering with SQL, Python, ETL & SalesForce Marketing Cloud (Must)",Industry Type: Software Product,Department: Other,"Employment Type: Full Time, Permanent","['Salesforce Marketing Cloud', 'SQL', 'Marketing Cloud', 'Salesforce', 'Python']",2025-06-12 05:27:09
"Data Engineering : Sr Software Engineer, Tech Lead & Sr Tech Lead",Reflion Tech,7 - 12 years,22.5-37.5 Lacs P.A.,"['Mumbai( Ghansoli )', 'Navi Mumbai', 'Mumbai (All Areas)']","Hiring: Data Engineering Senior Software Engineer / Tech Lead / Senior Tech Lead\n\n- Hybrid (3 Days from office) | Shift: 2 PM 11 PM IST\n- Experience: 5 to 12+ years (based on role & grade)\n\nOpen Grades/Roles:\nSenior Software Engineer: 58 Years\nTech Lead: 7–10 Years\nSenior Tech Lead: 10–12+ Years\n\nJob Description – Data Engineering Team\n\nCore Responsibilities (Common to All Levels):\n\nDesign, build and optimize ETL/ELT pipelines using tools like Pentaho, Talend, or similar\nWork on traditional databases (PostgreSQL, MSSQL, Oracle) and MPP/modern systems (Vertica, Redshift, BigQuery, MongoDB)\nCollaborate cross-functionally with BI, Finance, Sales, and Marketing teams to define data needs\nParticipate in data modeling (ER/DW/Star schema), data quality checks, and data integration\nImplement solutions involving messaging systems (Kafka), REST APIs, and scheduler tools (Airflow, Autosys, Control-M)\nEnsure code versioning and documentation standards are followed (Git/Bitbucket)\n\nAdditional Responsibilities by Grade\n\nSenior Software Engineer (5–8 Yrs):\nFocus on hands-on development of ETL pipelines, data models, and data inventory\nAssist in architecture discussions and POCs\nGood to have: Tableau/Cognos, Python/Perl scripting, GCP exposure\n\nTech Lead (7–10 Yrs):\nLead mid-sized data projects and small teams\nDecide on ETL strategy (Push Down/Push Up) and performance tuning\nStrong working knowledge of orchestration tools, resource management, and agile delivery\n\nSenior Tech Lead (10–12+ Yrs):\nDrive data architecture, infrastructure decisions, and internal framework enhancements\nOversee large-scale data ingestion, profiling, and reconciliation across systems\nMentoring junior leads and owning stakeholder delivery end-to-end\nAdvantageous: Experience with AdTech/Marketing data, Hadoop ecosystem (Hive, Spark, Sqoop)\n\n- Must-Have Skills (All Levels):\n\nETL Tools: Pentaho / Talend / SSIS / Informatica\nDatabases: PostgreSQL, Oracle, MSSQL, Vertica / Redshift / BigQuery\nOrchestration: Airflow / Autosys / Control-M / JAMS\nModeling: Dimensional Modeling, ER Diagrams\nScripting: Python or Perl (Preferred)\nAgile Environment, Git-based Version Control\nStrong Communication and Documentation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'SQL', 'ETL', 'Orchestration', 'Postgresql', 'Peri', 'Informatica', 'ETL Tool', 'SSIS', 'Elt', 'Modeling', 'MongoDB', 'Data Architecture', 'Talend', 'Pentaho', 'Python']",2025-06-12 05:27:12
Senior Data Engineer,The Main Stage Productions,4 - 6 years,Not Disclosed,['Bengaluru'],"Design and implement cloud-native data architectures on AWS, including data lakes, data warehouses, and streaming pipelines using services like S3, Glue, Redshift, Athena, EMR, Lake Formation, and Kinesis.\nDevelop and orchestrate ETL/ELT pipelines\n\nRequired Candidate profile\nParticipate in pre-sales and consulting activities such as:\nEngaging with clients to gather requirements and propose AWS-based data engineering solutions.\nSupporting RFPs/RFIs, technical proposals",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AWS Glue', 'GitHub Actions', 'PySpark', 'Scala', 'CodePipeline', 'Step Functions', 'data engineering']",2025-06-12 05:27:14
Linux Kernel Engineer Senior For Data Center SoC,Qualcomm,2 - 7 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nAs a Senior Software Engineer, you will play a pivotal role in designing, developing, optimizing, and commercializing software solutions for Qualcomms next-generation data center platforms. You will collaborate closely with cross-functional teams to advance critical technologies such as virtualization, memory management, scheduling, and the Linux Kernel.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\nCollaborate within the team and across teams to design, develop, and release our software, tooling, and practices to meet community standards and internal and external requirements.\nBring up platform solutions across the Qualcomm chipset portfolio.\nTriage software build, tooling, packaging, functional, or stability failures.\nGuide and support development teams inside and outside the Linux organization, focusing on Linux userspace software functionality, integration, and maintenance.\nWork with development and product teams as necessary for issue resolution.\n\n\nPreferred Qualifications:\nMaster's Degree in Engineering, Information Systems, Computer Science, or a related field.\nStrong background in Computer Science and software fundamentals.\nWorking knowledge of C, C++, and proficiency in scripting languages (Bash, Python, etc.).\nExperience using git/gerrit.\nStrong understanding of the Linux kernel, configuration techniques like ACPI and device tree, system services, and various components that make up a Linux distribution.\nExperience with Linux distributions such as Debian, Ubuntu, RedHat, Yocto, etc.\nFamiliarity with package managers and their workings is crucial.\nFamiliarity with CI/CD tools.\nProven ability and interest in debugging complex compute and data center systems.\nStrong ability to solve problems in a non-linear fashion.\nQuick learner; able to grasp concepts with only basic training and the initiative to ask questions and investigate new areas and concepts as needed.\nPrior experience with Qualcomm software platforms is a plus.\nMature interpersonal skills with an ability to collaboratively work within the team and with many varied teams to resolve problems spanning many disciplines.\nProven ability to work in a dynamic, multi-tasked environment.\nExcellent written and verbal communication skills are required.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['computer science', 'linux', 'software engineering', 'scripting languages', 'linux kernel', 'continuous integration', 'c++', 'redhat linux', 'ci/cd', 'gerrit', 'git', 'java', 'yocto', 'embedded systems', 'debian', 'html', 'mysql', 'python', 'c', 'ubuntu', 'javascript', 'data center', 'embedded c', 'bash', 'aws']",2025-06-12 05:27:16
Senior Data Engineer,Amgen Inc,3 - 8 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nDesign, develop, and maintain scalable ETL/ELT pipelines to support structured, semi-structured, and unstructured data processing across the Enterprise Data Fabric.\nImplement real-time and batch data processing solutions, integrating data from multiple sources into a unified, governed data fabric architecture.\nOptimize big data processing frameworks using Apache Spark, Hadoop, or similar distributed computing technologies to ensure high availability and cost efficiency.\nWork with metadata management and data lineage tracking tools to enable enterprise-wide data discovery and governance.\nEnsure data security, compliance, and role-based access control (RBAC) across data environments.\nOptimize query performance, indexing strategies, partitioning, and caching for large-scale data sets.\nDevelop CI/CD pipelines for automated data pipeline deployments, version control, and monitoring.\nImplement data virtualization techniques to provide seamless access to data across multiple storage systems.\nCollaborate with cross-functional teams, including data architects, business analysts, and DevOps teams, to align data engineering strategies with enterprise goals.\nStay up to date with emerging data technologies and best practices, ensuring continuous improvement of Enterprise Data Fabric architectures.\nMust-Have Skills:\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies.\nProficiency in workflow orchestration, performance tuning on big data processing.\nStrong understanding of AWS services\nExperience with Data Fabric, Data Mesh, or similar enterprise-wide data architectures.\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and DevOps\nEducation and Professional Certifications\nMasters degree and 3 to 4 + years of Computer Science, IT or related field experience\nOR\nBachelors degree and 5 to 8 + years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Git', 'PySpark', 'CI/CD', 'Databricks', 'ETL', 'NOSQL', 'AWS', 'data integration', 'SQL', 'Apache Spark', 'Python']",2025-06-12 05:27:18
Consultant - Lead Data Engineer,Affine Analytics,5 - 8 years,Not Disclosed,['Bengaluru'],"Strong experience with Python, SQL, pySpark, AWS Glue. Good to have - Shell Scripting, Kafka\nGood knowledge of DevOps pipeline usage (Jenkins, Bitbucket, EKS, Lightspeed)\nExperience of AWS tools (AWS S3, EC2, Athena, Redshift, Glue, EMR, Lambda, RDS, Kinesis, DynamoDB, QuickSight etc.).\nOrchestration using Airflow\nGood to have - Streaming technologies and processing engines, Kinesis, Kafka, Pub/Sub and Spark Streaming\nGood debugging skills",,,,"['Python', 'RDS', 'Shell Scripting', 'Kafka', 'AWS Glue', 'DynamoDB', 'Lightspeed', 'EMR', 'EKS', 'pySpark', 'Redshift', 'SQL', 'Jenkins', 'QuickSight', 'Glue', 'EC2', 'Kinesis', 'AWS S3', 'Bitbucket', 'Athena', 'Lambda']",2025-06-12 05:27:21
Senior AWS Data Engineer,Sightspectrum,4 - 7 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Must-Have Qualifications:\nAWS Expertise: Strong hands-on experience with AWS data services including Glue, Redshift, Athena, S3, Lake Formation, Kinesis, Lambda, Step Functions, EMR, and CloudWatch.\nETL/ELT Engineering: Deep proficiency in designing robust ETL/ELT pipelines with AWS Glue (PySpark/Scala), Python, dbt, or other automation frameworks.\nData Modeling: Advanced knowledge of dimensional (Star/Snowflake) and normalised data modeling, optimised for Redshift and S3-based lakehouses.\nProgramming Skills: Proficient in Python, SQL, and PySpark, with automation and scripting skills for data workflows.\nArchitecture Leadership: Demonstrated experience leading large-scale AWS data engineering projects across teams and domains.\nPre-sales & Consulting: Proven experience working with clients, responding to technical RFPs, and designing cloud-native data solutions.\nAdvanced PySpark Expertise: Deep hands-on experience in writing optimized PySpark code for distributed data processing, including transformation pipelines using DataFrames, RDDs, and Spark SQL, with a strong grasp of lazy evaluation, catalyst optimizer, and Tungsten execution engine.\nPerformance Tuning & Partitioning: Proven ability to debug and optimize Spark jobs through custom partitioning strategies, broadcast joins, caching, and checkpointing, with proficiency in tuning executor memory, shuffle configurations, and leveraging Spark UI for performance diagnostics in large-scale data workloads (>TB scale).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'AWS Data Engineer', 'SQL', 'ETL', 'Python', 'Airflow', 'Shell Scripting', 'Elt', 'S', 'Glue', 'Amazon Redshift', 'Redshift Aws', 'AWS', 'Athena']",2025-06-12 05:27:23
Senior Data Engineer,Amgen Inc,9 - 12 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nDesign, develop, and maintain scalable ETL/ELT pipelines to support structured, semi-structured, and unstructured data processing across the Enterprise Data Fabric.\nImplement real-time and batch data processing solutions, integrating data from multiple sources into a unified, governed data fabric architecture.\nOptimize big data processing frameworks using Apache Spark, Hadoop, or similar distributed computing technologies to ensure high availability and cost efficiency.\nWork with metadata management and data lineage tracking tools to enable enterprise-wide data discovery and governance.\nEnsure data security, compliance, and role-based access control (RBAC) across data environments.\nOptimize query performance, indexing strategies, partitioning, and caching for large-scale data sets.\nDevelop CI/CD pipelines for automated data pipeline deployments, version control, and monitoring.\nImplement data virtualization techniques to provide seamless access to data across multiple storage systems.\nCollaborate with cross-functional teams, including data architects, business analysts, and DevOps teams, to align data engineering strategies with enterprise goals.\nStay up to date with emerging data technologies and best practices, ensuring continuous improvement of Enterprise Data Fabric architectures.\nMust-Have Skills:\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies.\nProficiency in workflow orchestration, performance tuning on big data processing.\nStrong understanding of AWS services\nExperience with Data Fabric, Data Mesh, or similar enterprise-wide data architectures.\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\n9 to 12 years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data engineering', 'performance tuning', 'data security', 'data processing', 'Hadoop', 'Apache Spark', 'SQL', 'CI/CD', 'troubleshooting', 'big data', 'aws', 'ETL', 'Python']",2025-06-12 05:27:25
"Senior Data Engineer Databricks, ADF, PySpark",Suzva Software Technologies,6 - 11 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","Senior Data Engineer (Remote, Contract 6 Months) Databricks, ADF, and PySpark.\nWe are hiring a Senior Data Engineer for a 6-month remote contract position. The ideal candidate is highly skilled in building scalable data pipelines and working within the Azure cloud ecosystem, especially Databricks, ADF, and PySpark. You'll work closely with cross-functional teams to deliver enterprise-level data engineering solutions.\n\nKeyResponsibilities\nBuild scalable ETL pipelines and implement robust data solutions in Azure.\n\nManage and orchestrate workflows using ADF, Databricks, ADLS Gen2, and Key Vaults.\n\nDesign and maintain secure and efficient data lake architecture.\n\nWork with stakeholders to gather data requirements and translate them into technical specs.\n\nImplement CI/CD pipelines for seamless data deployment using Azure DevOps.\n\nMonitor data quality, performance bottlenecks, and scalability issues.\n\nWrite clean, organized, reusable PySpark code in an Agile environment.\n\nDocument pipelines, architectures, and best practices for reuse.\n\nMustHaveSkills\nExperience: 6+ years in Data Engineering\n\nTech Stack: SQL, Python, PySpark, Spark, Azure Databricks, ADF, ADLS Gen2, Azure DevOps, Key Vaults\n\nCore Expertise: Data Warehousing, ETL, Data Pipelines, Data Modelling, Data Governance\n\nAgile, SDLC, Containerization (Docker), Clean coding practices\n\nGoodToHaveSkills\nEvent Hubs, Logic Apps\n\nPower BI\n\nStrong logic building and competitive programming background\n\nLocation : - Remote,Mumbai, Delhi / NCR, Bengaluru , Kolkata, Chennai, Hyderabad, Ahmedabad, Pune",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Databricks', 'ADF', 'PySpark', 'ADLS Gen2', 'Azure Databricks', 'Key Vaults', 'Spark', 'Azure DevOps', 'SQL', 'Python']",2025-06-12 05:27:31
Data Engineer IV - Big Data / Spark,Sadup Soft,5 - 7 years,Not Disclosed,['Chennai'],"Must have skills :\n\n- Minimum of 5-7 years of experience in software development, with a focus on Java and infrastructure tools.\n\n- Min 6+ years of experience as a Data Engineer.\n\n- Good Experience in handling Big Data Spark, Hive SQL, BigQuery, SQL.\n\n- Candidate worked on cloud platforms and GCP would be an added advantage.\n\n- Good understanding of Hadoop based ecosystem including hard sequel, HDFS would be very essential.\n\n- Very good professional knowledge of PySpark or using Scala\n\nResponsibilities :\n\n- Collaborate with cross-functional teams such as Data Scientists, Product Partners and Partner Team Developers to identify opportunities for Big Data, Query ( Spark, Hive SQL, BigQuery, SQL ) tuning opportunities that can be solved using machine learning and generative AI.\n\n- Write clean, high-performance, high-quality, maintainable code.\n\n- Design and develop Big Data Engineering Solutions Applications for above ensuring scalability, efficiency, and maintainability of such solutions.\n\nRequirements :\n\n- A Bachelor or Master's degree in Computer Science or a related field.\n\n- Proven experience working as a Big Data & MLOps Engineer, with a focus on Spark, Scala Spark or PySpark, Spark SQL, BigQuery, Python, Google Cloud,.\n\n- Deep understanding and experience in tuning Dataproc, BigQuery, Spark Applications.\n\n- Solid knowledge of software engineering best practices, including version control systems (e.g Git), code reviews, and testing methodologies.\n\n- Strong communication skills to effectively collaborate and present findings to both technical and non-technical stakeholders.\n\n- Proven ability to adapt and learn new technologies and frameworks quickly.\n\n- A proactive mindset with a passion for continuous learning and research.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Data Engineering', 'BigQuery', 'GCP', 'Spark', 'Machine Learning', 'Python', 'SQL']",2025-06-12 05:27:33
Senior Data Engineer,Conversehr Business Solutions,7 - 12 years,30-45 Lacs P.A.,['Hyderabad'],"What is the Data team responsible for?\nAs a Senior Data Engineer, youll be a key member of the Data & AI team. This team is responsible for designing and delivering data engineering, analytics, and generative AI solutions that drive meaningful business impact. Were looking for a pragmatic, results-driven problem solver who thrives in a fast-paced environment and is passionate about building solutions on a scale #MID_SENIOR_LEVEL\nThe ideal candidate has a strong technical foundation, a collaborative mindset, and the ability to navigate complex challenges. You should be comfortable working in a fast-moving, startup-like environment within an established enterprise, and should bring strong skill sets to adapt to new solutions fast. You will play a pivotal role in optimizing data infrastructure, enabling data-driven decision-making and integrating AI across the organization.\nWhat is the Lead Software Engineer (Senior Data Engineer) responsible for?\nServe as a hands-on technical lead, driving project execution and delivery in our growing data team based in the Hyderabad office.\nCollaborate closely with the U.S.-based team and cross-functional stakeholders to understand business needs and deliver scalable solutions.\nLead the initiative to build firmwide data models and master data management solutions for structured data (in Snowflake) and manage unstructured data using vector embeddings.\nBuild, maintain, and optimize robust data pipelines and frameworks to support business intelligence and operational workflows.\nDevelop dashboards and data visualizations that support strategic business decisions.\nStay current with emerging trends in data engineering and help implement best practices within the team.\nMentor and support junior engineers, fostering a culture of learning and technical excellence.\nWhat ideal qualifications, skills & experience would help someone to be successful?\nBachelors or master’s degree in computer science, data science, engineering, or a related field.\n7+ years of experience in data engineering including 3+ years in a technical leadership role.\nStrong SQL skills and hands-on experience with modern data pipeline technologies (e.g., Spark, Flink).\nDeep expertise in the Snowflake ecosystem, including data modeling, data warehousing, and master data management.\nProficiency in at least one programming language - Python preferred.\nExperience with Tableau and Alteryx is a plus.\nSelf-starter with a passion for learning new tools and technologies.\nStrong communication skills and a collaborative, ownership-driven mindset.\nWork Shift Timings - 2:00 PM - 11:00 PM IST",Industry Type: Investment Banking / Venture Capital / Private Equity,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'Snowflake', 'Python', 'flink', 'Data Pipeline', 'Spark', 'SQL']",2025-06-12 05:27:35
Senior Data Engineer,Egon Zehnder,3 - 8 years,Not Disclosed,"['Noida', 'New Delhi', 'Gurugram']","Role & responsibilities\n\nAs a Data Engineer, you will be responsible for establishing and optimizing the flow of data throughout the organization while ensuring its security.\nExecuting end-to-end data pipeline, from designing the technical architecture, and developing the application to finally testing and implementing the proposed solution.\nThe Data Engineer is expected to help in driving the database architecture and design for Egon Zehnder large-scale, Intranet and Internet-based applications.\nThe Data Engineer should research new tools and technologies and come up with recommendations on how they can be used in Egon Zehnder applications.\nThe Data Engineer will also be expected to actively participate in design and implementation of projects bearing a high degree of technical complexity and/or scalability and performance significance.\nIdentifying data sources, both internal and external, and working out a plan for data management that is aligned with organizational data strategy.\nCollaborate cross-functionally with roles such as IT infrastructure, Digital/application development, and Legal to identify and highlight gaps and risks around Cybersecurity and Data Protection such as GDPR.\n\nExperience & Key Competencies\nEngineering Degree or equivalent.\n3+ years of experience of SQL writing and data modelling skills, with a solid understanding of data technologies including RDBMS, No-SQL databases.\nExperience with one or more of the following databases: SQL Server, MySQL, PostgreSQL, Oracle, Couch-base, Redis, Elastic Search or other NoSQL technologies.\nProven experience in building ETL/ELT pipelines, preferably from SQL to Azure services\nArchitecture experience of making ETL operations such as Medallion Architecture etc.\nFamiliarity with CI/CD practices for data pipelines and version control using Git\nExperience in integrating new/replacing vendor products in existing ecosystem.\nExperience in migrating data structured, unstructured data to Data lake\nExperience or understanding of performance engineering both at system level and database level.\nExperience or understanding of Data Governance, Data Quality, Data Issue Management.\nWork closely with the implementation teams of the various product lines to ensure that data architectural standards and best practices are being followed consistently across all Egon Zehnder applications\nEnsure compliance with all regulations, policies, and procedures.\nEscalate issues/risks pro-actively to appropriate stakeholders.\nRegularly communicate status and challenges to team members and management.\nSelf-driven with keenness to master, suggest and work with different technologies & toolsets.\nExcellent communication skills and interpersonal skills suitable for a diverse audience with ability to communicate in a positive friendly and effective manner with technical or non-technical users/customers\nExcellent and resourceful problem-solving skills, adaptable and willingness to learn.\nGood analysis skills - to be able to join the dots across multiple applications and interfaces between them.\n\n\nPreferred candidate profile\n\nExperience with one or more of the following databases: SQL Server, MySQL, PostgreSQL, Oracle, Couch-base, Redis, Elastic Search or other NoSQL technologies.\nExperience in Data factory and Data Lake technologies.\nRich Experience in data modelling techniques and creating various data models.\nExperience in Azure cloud services and architecture patterns.\nUnderstanding of RESTful APIs for data distribution.\nUnderstanding of deployment architecture and infrastructure in both on-prem and cloud hosting environments\nExcellent oral and written communication with an ability to articulate complex systems to multiple teams.\nSelf-motivation and the ability to work under minimal supervision\n\nBenefits\nBenefits which make us unique\nAt EZ, we know that great people are what makes a great firm. We value our people and offer employees a comprehensive benefits package. Learn more about what working at Egon Zehnder can mean for you!\nBenefits Highlights:\n•       5 Days working in a Fast-paced work environment\n•       Work directly with the senior management team\n•       Reward and Recognition\n•       Employee friendly policies\n•       Personal development and training\n•       Health Benefits, Accident Insurance\nPotential Growth for you!\nWe will nurture your talent in an inclusive culture that values diversity. You will be doing regular catchups with your Manager who will act as your career coach and guide you in your career goals and aspirations.\nLocation\nThe position is based at Egon Zehnders KCI office in Gurgaon, Plot no. 29, Institutional Area Sector 32.\nEZIRS Commitment to Diversity & Inclusion\nEgon Zehnder Information Research & Services (EZIRS) aims for a diverse workplace and strive to continuously lead with our firm values. We respect personal values of every individual irrespective of race, national or social origin, gender, religion, political or other opinion, disability, age and sexual orientation as warranted by basic rights enshrined in the UN Declaration of Human Rights. We believe diversity of our firm is central to the success and enables us to deliver better solutions for our clients. We are committed to creating an inclusive environment and supportive work environment, where everyone feels comfortable to be themselves and treated with dignity and respect and there is no unlawful discrimination related to employment, recruitment, training, promotion or remuneration.\nEgon Zehnder is an Equal Opportunity Employer\nEgon Zehnder provides equal employment opportunities to all applicants and employees without regard to race, color, creed, religion, sex, sexual orientation, gender identity, marital status, citizenship status, age, national origin, disability, or any other legally protected status and to affirmatively seek to advance the principles of equal employment opportunity.",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL Server', 'PostgreSQL', 'MySQL', 'Couch-base', 'Redis', 'Oracle', 'Elastic Search or other NoSQL technologies']",2025-06-12 05:27:38
Senior Data Engineer,Atidiv,5 - 8 years,10-17 Lacs P.A.,[],"Were Hiring! | Senior Data Engineer (Remote)\n\nLocation: Remote |\nShift: US - CST Time |\nDepartment: Data Engineering\n\nAre you a data powerhouse who thrives on solving complex data challenges? Do you love working with Python, AWS, and cutting-edge data tools? If yes, Atidiv wants YOU!",,,,"['SQL', 'Snowflake', 'Python', 'Airflow', 'Pyspark', 'Kafka', 'Lamda', 'Ci/Cd', 'EMR', 'Aws Glue', 'Lambda Aws', 'Cd Tools', 'Glue', 'Kinesis', 'Redshift Aws', 'DBT', 'aws']",2025-06-12 05:27:40
Azure Data Engineer,Big4,7 - 12 years,18-30 Lacs P.A.,['Bengaluru'],"Urgently Hiring for Senior Azure Data Engineer\n\nJob Location- Bangalore\nMinimum exp - Total 7+yrs with min 4 years relevant exp\n\nKeywords Databricks, Pyspark, SCALA, SQL, Live / Streaming data, batch processing data\n\nShare CV siddhi.pandey@adecco.com\nOR Call 6366783349\n\nRoles and Responsibilities:\nThe Data Engineer will work on data engineering projects for various business units, focusing on delivery of complex data management solutions by leveraging industry best practices. They work with the project team to build the most efficient data pipelines and data management solutions that make data easily available for consuming applications and analytical solutions. A Data engineer is expected to possess strong technical skills\n\nKey Characteristics\nTechnology champion who constantly pursues skill enhancement and has inherent curiosity to understand work from multiple dimensions\nInterest and passion in Big Data technologies and appreciates the value that can be brought in with an effective data management solution\nHas worked on real data challenges and handled high volume, velocity, and variety of data.\nExcellent analytical & problem-solving skills, willingness to take ownership and resolve technical challenges.\nContributes to community building initiatives like CoE, CoP.\nMandatory skills:\nAzure - Master\nELT - Skill\nData Modeling - Skill\nData Integration & Ingestion - Skill\nData Manipulation and Processing - Skill\nGITHUB, Action, Azure DevOps - Skill\nData factory, Databricks, SQL DB, Synapse, Stream Analytics, Glue, Airflow, Kinesis, Redshift, SonarQube, PyTest - Skill\nOptional skills:\nExperience in project management, running a scrum team.\nExperience working with BPC, Planning.\nExposure to working with external technical ecosystem.\nMKDocs documentation\n\nShare CV siddhi.pandey@adecco.com\nOR Call 6366783349",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['databricks', 'Azure Synapse', 'Pyspark', 'Stream Analytics', 'SCALA', 'SQL Azure', 'Data Bricks', 'SQL']",2025-06-12 05:27:43
Data Engineer 2,Uplers,3 - 8 years,Not Disclosed,['Bengaluru'],"About the Role:\nAs a Data Engineer, you will be part of the Data Engineering team with this role being inherently multi-functional, and the ideal candidate will work with Data Scientist, Analysts, Application teams across the company, as well as all other Data Engineering squads at Wayfair. We are looking for someone with a love for data, understanding requirements clearly and the ability to iterate quickly. Successful candidates will have strong engineering skills and communication and a belief that data-driven processes lead to phenomenal products.\n\nWhat you'll do:\nBuild and launch data pipelines, and data products focussed on SMART Org.\nHelping teams push the boundaries of insights, creating new product features using data, and powering machine learning models.\nBuild cross-functional relationships to understand data needs, build key metrics and standardize their usage across the organization.\nUtilize current and leading edge technologies in software engineering, big data, streaming, and cloud infrastructure\n\nWhat You'll Need:\nBachelor/Master degree in Computer Science or related technical subject area or equivalent combination of education and experience 3+ years relevant work experience in the Data Engineering field with web scale data sets.\nDemonstrated strength in data modeling, ETL development and data lake architecture.\nData Warehousing Experience with Big Data Technologies (Hadoop, Spark, Hive, Presto, Airflow etc.).\nCoding proficiency in at least one modern programming language (Python, Scala, etc)\nExperience building/operating highly available, distributed systems of data extraction, ingestion, and processing and query performance tuning skills of large data sets.\nIndustry experience as a Big Data Engineer and working along cross functional teams such as Software Engineering, Analytics, Data Science with a track record of manipulating, processing, and extracting value from large datasets.\nStrong business acumen. Experience leading large-scale data warehousing and analytics projects, including using GCP technologies Big Query, Dataproc, GCS, Cloud Composer, Dataflow or related big data technologies in other cloud platforms like AWS, Azure etc.\nBe a team player and introduce/follow the best practices on the data engineering space.\nAbility to effectively communicate (both written and verbally) technical information and the results of engineering design at all levels of the organization.\n\nGood to have :\nUnderstanding of NoSQL Database exposure and Pub-Sub architecture setup.\nFamiliarity with Bl tools like Looker, Tableau, AtScale, PowerBI, or any similar tools.\n\nPS: This role is with one of our clients who is a leading name in Retail Industry.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Data Engineering', 'Cloud Platform', 'Hive', 'GCP', 'Bigquery', 'Hadoop', 'SCALA', 'Big Data Technologies', 'Etl Development', 'Spark', 'Python']",2025-06-12 05:27:45
Azure Data Engineer,Our client is a multinational professio...,7 - 12 years,Not Disclosed,['Bengaluru'],"Urgently Hiring for Senior Azure Data Engineer\n\nJob Location- Bangalore\nMinimum exp - 7yrs- 11yrs\n\nKeywords Databricks, Pyspark, SCALA, SQL, Live / Streaming data, batch processing data\n\nShare CV Mohini.sharma@adecco.com\nOR Call 9740521948\n\nRoles and Responsibilities:\nThe Data Engineer will work on data engineering projects for various business units, focusing on delivery of complex data management solutions by leveraging industry best practices. They work with the project team to build the most efficient data pipelines and data management solutions that make data easily available for consuming applications and analytical solutions. A Data engineer is expected to possess strong technical skills.\nKey Characteristics\nTechnology champion who constantly pursues skill enhancement and has inherent curiosity to understand work from multiple dimensions.\nInterest and passion in Big Data technologies and appreciates the value that can be brought in with an effective data management solution.\nHas worked on real data challenges and handled high volume, velocity, and variety of data.\nExcellent analytical & problem-solving skills, willingness to take ownership and resolve technical challenges.\nContributes to community building initiatives like CoE, CoP.\nMandatory skills:\nAzure - Master\nELT - Skill\nData Modeling - Skill\nData Integration & Ingestion - Skill\nData Manipulation and Processing - Skill\nGITHUB, Action, Azure DevOps - Skill\nData factory, Databricks, SQL DB, Synapse, Stream Analytics, Glue, Airflow, Kinesis, Redshift, SonarQube, PyTest - Skill\nOptional skills:\nExperience in project management, running a scrum team.\nExperience working with BPC, Planning.\nExposure to working with external technical ecosystem.\nMKDocs documentation\n\nShare CV Mohini.sharma@adecco.com\nOR Call 9740521948",Industry Type: Financial Services (Asset Management),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Databricks', 'Pyspark', 'SCALA', 'SQL']",2025-06-12 05:27:48
Sr Data Engineering Manager,Amgen Inc,12 - 15 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nWe are seeking a Senior Data Engineering Manager with a strong background in Regulatory or Integrated Product Teams within the Biotech or Pharmaceutical domain. This role will lead the end-to-end data strategy and execution for regulatory product submissions, lifecycle management, and compliance reporting, ensuring timely and accurate delivery of regulatory data assets across global markets.You will be embedded in a cross-functional Regulatory Integrated Product Team (IPT) and serve as the data and technology lead, driving integration between scientific, regulatory, and engineering functions to support submission-ready data and regulatory intelligence solutions.\nRoles & Responsibilities:\nFunctional Skills:\nLead the engineering strategy and implementation for end-to-end regulatory operations, including data ingestion, transformation, integration, and delivery across regulatory systems.\nServe as the data engineering SME in the Integrated Product Team (IPT) to support regulatory submissions, agency interactions, and lifecycle updates.\nCollaborate with global regulatory affairs, clinical, CMC, quality, safety, and IT teams to gather submission data requirements and translate them into data engineering solutions.\nManage and oversee the development of data pipelines, data models, and metadata frameworks that support submission data standards (e.g., eCTD, IDMP, SPL, xEVMPD).\nEnable integration and reporting across regulatory information management systems (RIMS), EDMS, clinical trial systems, and lab data platforms.\nImplement data governance, lineage, validation, and audit trails for regulatory data workflows, ensuring GxP and regulatory compliance.\nGuide the development of automation solutions, dashboards, and analytics that improve visibility into submission timelines, data quality, and regulatory KPIs.\nEnsure interoperability between regulatory data platforms and enterprise data lakes or lakehouses for cross-functional reporting and insights.\nCollaborate with IT, data governance, and enterprise architecture teams to ensure alignment with overall data strategy and compliance frameworks.\nDrive innovation by evaluating emerging technologies in data engineering, graph data, knowledge management, and AI for regulatory intelligence.\nLead, mentor, and coach a small team of data engineers and analysts, fostering a culture of excellence, innovation, and delivery.\nDrive Agile and Scaled Agile (SAFe) methodologies, managing sprint backlogs, prioritization, and iterative improvements to enhance team velocity and project delivery.\nStay up-to-date with emerging data technologies, industry trends, and best practices, ensuring the organization leverages the latest innovations in data engineering and architecture.\nMust-Have Skills:\n812 years of experience in data engineering or data architecture, with 3+ years in a senior or managerial capacity, preferably within the biotech or pharmaceutical industry.\nProven experience supporting regulatory functions, including submissions, tracking, and reporting for FDA, EMA, and other global authorities.\nExperience with ETL/ELT tools, data pipelines, and cloud-based data platforms (e.g., Databricks, AWS, Azure, or GCP).\nFamiliarity with regulatory standards and data models such as eCTD, IDMP, HL7, CDISC, and xEVMPD.\nDeep understanding of GxP data compliance, audit requirements, and regulatory submission processes.\nExperience with tools like Power BI, Tableau, or Qlik for regulatory dashboarding and visualization is a plus.\nStrong project management, stakeholder communication, and leadership skills, especially in matrixed, cross-functional environments.\nAbility to translate technical capabilities into regulatory and business outcomes.Prepare team members for stakeholder discussions by helping assess data costs, access requirements, dependencies, and availability for business scenarios.\nGood-to-Have Skills:\nPrior experience working on integrated product teams or regulatory transformation programs.\nKnowledge of Regulatory Information Management Systems (RIMS), Veeva Vault RIM, or Master Data Management (MDM) in regulated environments.\nFamiliarity with Agile/SAFe methodologies and DevOps/DataOps best practices.\nEducation and Professional Certifications\n12 to 15 years of experience in Computer Science, IT or related field\nScaled Agile SAFe certification preferred\nProject Management certifications preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'engineering strategy', 'DevOps', 'Project Management', 'DataOps', 'Agile', 'data strategy']",2025-06-12 05:27:50
Senior Data Engineer,Jeavio,5 - 10 years,Not Disclosed,[],"We are seeking an experienced Senior Data Engineer to join our team. The ideal candidate will have a strong background in data engineering and AWS infrastructure, with hands-on experience in building and maintaining data pipelines and the necessary infrastructure components. The role will involve using a mix of data engineering tools and AWS services to design, build, and optimize data architecture.\n\nKey Responsibilities:\nDesign, develop, and maintain data pipelines using Airflow and AWS services.\nImplement and manage data warehousing solutions with Databricks and PostgreSQL.\nAutomate tasks using GIT / Jenkins.\nDevelop and optimize ETL processes, leveraging AWS services like S3, Lambda, AppFlow, and DMS.\nCreate and maintain visual dashboards and reports using Looker.\nCollaborate with cross-functional teams to ensure smooth integration of infrastructure components.\nEnsure the scalability, reliability, and performance of data platforms.\nWork with Jenkins for infrastructure automation.\n\nTechnical and functional areas of expertise:\nWorking as a senior individual contributor on a data intensive project\nStrong experience in building high performance, resilient & secure data processing pipelines preferably using Python based stack.\nExtensive experience in building data intensive applications with a deep understanding of querying and modeling with relational databases preferably on time-series data.\nIntermediate proficiency in AWS services (S3, Airflow)\nProficiency in Python and PySpark\nProficiency with ThoughtSpot or Databricks.\nIntermediate proficiency in database scripting (SQL)\nBasic experience with Jenkins for task automation\n\nNice to Have :\nIntermediate proficiency in data analytics tools (Power BI / Tableau / Looker / ThoughSpot)\nExperience working with AWS Lambda, Glue, AppFlow, and other AWS transfer services.\nExposure to PySpark and data automation tools like Jenkins or CircleCI.\nFamiliarity with Terraform for infrastructure-as-code.\nExperience in data quality testing to ensure the accuracy and reliability of data pipelines.\nProven experience working directly with U.S. client stakeholders.\nAbility to work independently and take the lead on tasks.\n\nEducation and experience:\nBachelors or masters in computer science or related fields.\n5+ years of experience\n\nStack/Skills needed:\nDatabricks\nPostgreSQL\nPython & Pyspark\nAWS Stack\nPower BI / Tableau / Looker / ThoughSpot\nFamiliarity with GIT and/or CI/CD tools",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'AWS', 'Data Bricks', 'Python', 'Etl Pipelines', 'Airflow', 'Database Scripting', 'Postgresql', 'Looker', 'SQL']",2025-06-12 05:27:53
Senior Data Engineer,A leading Bank of India,8 - 13 years,Not Disclosed,['Mumbai (All Areas)'],"Role & responsibilities:\nDesign, optimize, and manage complex SQL queries across large Oracle databases.\nBuild and maintain metadata layers (table schema, DDL, column descriptions, relationships).\nDevelop and fine-tune solutions for converting natural language to SQL using various python libraries or other solutions.\nBuild and integrate chatbot interfaces using Python (Streamlit, FastAPI, Flask). Preferred is Flask.\nHave handled feature building for creating dynamic query agents.\nImplement user query handling, ambiguity detection, and feedback loops.\nDesign and test proof-of-concept (PoC) solutions to solve business queries using Python.\nBuild modular components (e.g., SQL generators, data sanitizers, security layers).\nCollaborate with business and business analysts to convert ideas into working prototypes.\nImplement query safety checks to prevent injection and unauthorized data access.\nMaintain logs and audit trails for executed queries and system usage.\n\nPreferred candidate profile:\n\nStrong experience in data engineering, analytics, or backend systems.\nExpert-level SQL skills (especially Oracle SQL).\nStrong Python programming skills, including libraries like pandas, sqlalchemy,strea, flask, etc.\nExperience with one or more NLP libraries: OpenAI GPT (function calling), LangChain, HuggingFace Transformers.\nKnowledge of database metadata modeling and handling DDL for large-scale systems.\nExperience with REST APIs and building microservices in Python.\nExposure to modern LLM tools like DSPy, sqlcoder, Text2SQL, or LangChain SQL Agent.\nFamiliarity with RDBMS performance tuning and optimization strategies.\nKnowledge of visualization tools like Plotly, Dash, or Streamlit for result rendering.\nUnderstanding of RBAC and secure data access practices.",Industry Type: Banking,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Banking domain', 'Natural Language Processing', 'Large Language Model', 'Chatbot Development', 'Python', 'Tensorflow', 'Oracle SQL', 'Artificial Intelligence', 'FASTapi', 'Machine Learning', 'Deep Learning', 'Numpy', 'Scikit-Learn', 'Pytorch', 'Django', 'Pandas', 'Aiml', 'Flask']",2025-06-12 05:27:55
Senior Data Engineer (Data Architect),Adastra Corp,8 - 13 years,Not Disclosed,[],"Join our innovative team and architect the future of data solutions on Azure, Synapse, and Databricks!\nSenior Data Engineer (Data Architect)\nAdditional Details:\nNotice Period: 30 days (maximum)\nLocation: Remote\nAbout the Role\nDesign and implement scalable data pipelines, data warehouses, and data lakes that drive business growth. Collaborate with stakeholders to deliver data-driven insights and shape the data landscape.\nRequirements\n8+ years of experience in data engineering and data architecture\nStrong expertise in Azure services (Synapse Analytics, Databricks, Storage, Active Directory)\nProven experience in designing and implementing data pipelines, data warehouses, and data lakes\nStrong understanding of data governance, data quality, and data security\nExperience with infrastructure design and implementation, including DevOps practices and tools",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Azure', 'DESIGN', 'Architecture', 'Synapse Analytics']",2025-06-12 05:27:57
"Walk-In Senior Data Engineer - DataStage, Azure & Power BI",Net Connect,6 - 10 years,5-11 Lacs P.A.,['Hyderabad( Madhapur )'],"Greetings from NCG!\n\nWe have a opening for Snowflake Developer role in Hyderabad office!\nBelow JD for your reference\n\nJob Description:\n\nWe are hiring an experienced Senior Data Engineer with strong expertise in IBM DataStage, , and . The ideal candidate will be responsible for end-to-end data integration, transformation, and reporting solutions that drive business decisions.",,,,"['Azure Data Factory', 'Datastage', 'Etl Datastage']",2025-06-12 05:27:59
"Sr. Data Engineer, R&D Data Catalyst Team",Amgen Inc,7 - 9 years,Not Disclosed,['Hyderabad'],"The R&D Data Catalyst Team is responsible for buildingData Searching, Cohort Building, and Knowledge Management tools that provide the Amgen scientific community with visibility to Amgens wealth of human datasets, projects and study histories, and knowledge over various scientific findings. These solutions are pivotal tools in Amgens goal to accelerate the speed of discovery, and speed to market of advanced precision medications.\nThe Data Engineer will be responsible for the end-to-end development of an enterprise analytics and data mastering solution leveraging Databricks and Power BI. This role requiresexpertise in both data architecture and analytics, with the ability to create scalable, reliable, and high-performing enterprise solutions that research cohort-building and advanced research pipeline.The ideal candidate will have experience creating and surfacing large unifiedrepositories of human data, based on integrations from multiple repositories and solutions, and be exceptionally skilled with data analysis and profiling.\nYou will collaborate closely with stakeholders, product team members, and related IT teams, to design and implement data models, integrate data from various sources, and ensure best practices for data governance and security. The ideal candidate will have a strong background in data warehousing, ETL, Databricks, Power BI, and enterprise data mastering.\nRoles & Responsibilities:\nDesign and build scalable enterprise analytics solutions using Databricks, Power BI, and other modern data tools.\nLeverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation\nBreak down features into work that aligns with the architectural direction runway\nParticipate hands-on in pilots and proofs-of-concept for new patterns\nCreate robust documentation from data analysis and profiling, and proposed designs and data logic\nDevelop advanced sql queries to profile, and unify data\nDevelop data processing code in sql, along with semantic views to prepare data for reporting\nDevelop PowerBI Models and reporting packages\nDesign robust data models, and processing layers, that support both analytical processing and operational reporting needs.\nDesign and develop solutions based on best practices for data governance, security, and compliance within Databricks and Power BI environments.\nEnsure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.\nDevelop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.\nCollaborate with stakeholders to define data requirements, functional specifications, and project goals.\nContinuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions.\nBasic Qualifications and Experience:\nMasters degree with 1 to 3years of experience in Data Engineering OR\nBachelors degree with 4 to 5 years of experience in Data Engineering\nDiploma and 7 to 9 years of experience in Data Engineering.\nFunctional Skills:\nMust-Have Skills:\nMinimum of 3 years of hands-on experience with BI solutions (Preferrable Power BI or Business Objects) including report development, dashboard creation, and optimization.\nMinimum of 3years of hands-on experience building Change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management.\nHands-on experience with Databricks, including data engineering, optimization, and analytics workloads.\nDeep understanding of Power BI, including model design, DAX, and Power Query.\nProven experience designing and implementing data mastering solutions and data governance frameworks.\nExpertise in cloud platforms (AWS), data lakes, and data warehouses.\nStrong knowledge of ETL processes, data pipelines, and integration technologies.\nStrong communication and collaboration skills to work with cross-functional teams and senior leadership.\nAbility to assess business needs and design solutions that align with organizational goals.\nExceptional hands-on capabilities with data profiling, data transformation, data mastering\nSuccess in mentoring and training team members\nGood-to-Have Skills:\nExperience in developing differentiated and deliverable solutions\nExperience with human data, ideally human healthcare data\nFamiliarity with laboratory testing, patient data from clinical care, HL7, FHIR, and/or clinical trial data management\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nITIL Foundation or other relevant certifications (preferred)\nSAFe Agile Practitioner (6.0)\nMicrosoft Certified: Data Analyst Associate (Power BI) or related certification.\nDatabricks Certified Professional or similar certification.\nSoft Skills:\nExcellent analytical and troubleshooting skills\nDeep intellectual curiosity\nHighest degree of initiative and self-motivation\nStrong verbal and written communication skills, including presentation to varied audiences of complex technical/business topics\nConfidence technical leader\nAbility to work effectively with global, virtual teams, specifically including leveraging of tools and artifacts to assure clear and efficient collaboration across time zones\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong problem solving, analytical skills;\nAbility to learn quickly and retain and synthesize complex information from diverse sources",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'data analysis', 'ETL processes', 'DAX', 'Business Objects', 'data warehouse design', 'ETL', 'PowerBI Models', 'AWS', 'Power Query']",2025-06-12 05:28:02
Senior Data Engineer,Straive,6 - 10 years,Not Disclosed,['Mumbai (All Areas)'],Senior Data Engineer\nYou will have the following responsibilities:\nDesign\nAnalyse relevant internally and externally sourced data (raw data) to generate BI and Advanced\nAnalytics datasets based on your stakeholders requirements,,,,"['AWS', 'Postgresql', 'Snowflake', 'Data Warehousing', 'ETL', 'Aws Rds Oracle']",2025-06-12 05:28:04
"Data Engineer (Python, Kafka Stream, Pyspark, and Azure Databricks.)",Hire Squad,5 - 8 years,20-30 Lacs P.A.,"['Noida', 'Hyderabad', 'Bengaluru']","Looking for Data Engineers, immediate joiners only, for Hyderabad, Bengaluru and Noida Location.\n\n*Must have experience in Python, Kafka Stream, Pyspark, and Azure Databricks.*\n\nRole and responsibilities:\nLead the design, development, and implementation of real-time data pipelines using Kafka, Python, and Azure Databricks.\nArchitect scalable data streaming and processing solutions to support healthcare data workflows.\nDevelop, optimize, and maintain ETL/ELT pipelines for structured and unstructured healthcare data.\nEnsure data integrity, security, and compliance with healthcare regulations (HIPAA, HITRUST, etc.).\nCollaborate with data engineers, analysts, and business stakeholders to understand requirements and translate them into technical solutions.\nTroubleshoot and optimize Kafka streaming applications, Python scripts, and Databricks workflows.\nMentor junior engineers, conduct code reviews, and ensure best practices in data engineering.\nStay updated with the latest cloud technologies, big data frameworks, and industry trends.\n\n\nPreferred candidate profile :\n5+ years of experience in data engineering, with strong proficiency in Kafka and Python.\nExpertise in Kafka Streams, Kafka Connect, and Schema Registry for real-time data processing.\nExperience with Azure Databricks (or willingness to learn and adopt it quickly).\nHands-on experience with cloud platforms (Azure preferred, AWS or GCP is a plus).\nProficiency in SQL, NoSQL databases, and data modeling for big data processing.\nKnowledge of containerization (Docker, Kubernetes) and CI/CD pipelines for data applications.\nExperience working with healthcare data (EHR, claims, HL7, FHIR, etc.) is a plus.\nStrong analytical skills, problem-solving mindset, and ability to lead complex data projects.\nExcellent communication and stakeholder management skills.\n\n\n\nInterested, call:\nRose (9873538143 / WA : 8595800635)\nrose2hiresquad@gmail.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Azure Databricks', 'Kafka Streams', 'Python', 'Etl Pipelines', 'development', 'Data Engineering', 'cloud-based data platforms', 'implementation', 'Data Streaming', 'Lead the design', 'processing solutions', 'Hitrust', 'large-scale data processing', 'ELT pipelines', 'HIPAA', 'real-time data streaming']",2025-06-12 05:28:06
Senior Data Engineer,Tredence,4 - 8 years,Not Disclosed,['Pune'],"About Tredence:\nTredence is a global data science solutions provider founded in 2013 by Shub Bhowmick, Sumit Mehra, and Shashank Dubey focused on solving the last-mile problem in AI. Headquartered in San Jose, California, the company embraces a vertical-first approach and an outcome-driven mindset to help clients win and accelerate value realization from their analytics investments. The aim is to bridge the gap between insight delivery and value realization by providing customers with a differentiated approach to data and analytics through tailor-made solutions. Tredence is 1,800-plus employees strong with offices in San Jose, Foster City, Chicago, London, Toranto, and Bangalore, with the largest companies in retail, CPG, hi-tech, telecom, healthcare, travel, and industrials as clients.",,,,"['azure databricks', 'python', 'azure data lake', 'rdbms', 'data management', 'performance tuning', 'analytical', 'data', 'pyspark', 'data warehousing', 'azure data factory', 'data engineering', 'tools', 'artificial intelligence', 'sql', 'plsql', 'unix shell scripting', 'spark', 'etl', 'communication skills', 'agile methodology']",2025-06-12 05:28:09
Senior Data Engineer,TechAffinity,6 - 10 years,Not Disclosed,['Chennai'],"We are looking for a highly skilled Senior Data Engineer with strong expertise in Data Warehousing & Analytics to join our team. The ideal candidate will have extensive experience in designing and managing data solutions, advanced SQL proficiency, and hands-on expertise in Python.\nKey Responsibilities:\nDesign, develop, and maintain scalable data warehouse solutions.\nWrite and optimise complex SQL queries for data extraction, transformation, and Reporting.\nDevelop and automate data pipelines using Python.\nWork with AWS cloud services for data storage, processing, and analytics.\nCollaborate with cross-functional teams to provide data-driven insights and solutions.\nEnsure data integrity, security, and performance optimisation.\n\nRequired Skills & Experience:\n6-10 years of experience in Data Warehousing & Analytics.\nStrong proficiency in writing complex SQL queries with deep understanding of query optimization, stored procedures, and indexing.\nHands-on experience with Python for data processing and automation.\nExperience working with AWS cloud services.\nAbility to work independently and collaborate with teams across different time zones.\nGood to Have:\nExperience in the SAS domain and understanding of financial data structures.\nHands-on experience with reporting tools like Power BI or Tableau.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Power Bi', 'Tableau', 'SQL', 'Python', 'Data Warehousing Concepts', 'SAS', 'Azure Cloud', 'ETL', 'Aws Cloud Services', 'AWS', 'financial data structures.']",2025-06-12 05:28:11
Senior Data Engineer : 7+ Years,Jayam Solutions Pvt Ltd - CMMI Level III Company,5 - 9 years,Not Disclosed,['Hyderabad( Madhapur )'],"Job Description:\nPosition: Sr.Data Engineer\nExperience: Minimum 7 years\nLocation: Hyderabad\nJob Summary:\n\nWhat Youll Do\n\nDesign and build efficient, reusable, and reliable data architecture leveraging technologies like Apache Flink, Spark, Beam and Redis to support large-scale, real-time, and batch data processing.\nParticipate in architecture and system design discussions, ensuring alignment with business objectives and technology strategy, and advocating for best practices in distributed data systems.\nIndependently perform hands-on development and coding of data applications and pipelines using Java, Scala, and Python, including unit testing and code reviews.\nMonitor key product and data pipeline metrics, identify root causes of anomalies, and provide actionable insights to senior management on data and business health.\nMaintain and optimize existing datalake infrastructure, lead migrations to lakehouse architectures, and automate deployment of data pipelines and machine learning feature engineering requests.\nAcquire and integrate data from primary and secondary sources, maintaining robust databases and data systems to support operational and exploratory analytics.\nEngage with internal stakeholders (business teams, product owners, data scientists) to define priorities, refine processes, and act as a point of contact for resolving stakeholder issues.\nDrive continuous improvement by establishing and promoting technical standards, enhancing productivity, monitoring, tooling, and adopting industry best practices.\n\nWhat Youll Bring\n\nBachelors degree or higher in Computer Science, Engineering, or a quantitative discipline, or equivalent professional experience demonstrating exceptional ability.\n7+ years of work experience in data engineering and platform engineering, with a proven track record in designing and building scalable data architectures.\nExtensive hands-on experience with modern data stacks, including datalake, lakehouse, streaming data (Flink, Spark), and AWS or equivalent cloud platforms.\nCloud - AWS\nApache Flink/Spark , Redis\nDatabase platform- Databricks.\nProficiency in programming languages such as Java, Scala, and Python(Good to have) for data engineering and pipeline development.\nExpertise in distributed data processing and caching technologies, including Apache Flink, Spark, and Redis.\nExperience with workflow orchestration, automation, and DevOps tools (Kubernetes,git,Terraform, CI/CD).\nAbility to perform under pressure, managing competing demands and tight deadlines while maintaining high-quality deliverables.\nStrong passion and curiosity for data, with a commitment to data-driven decision making and continuous learning.\nExceptional attention to detail and professionalism in report and dashboard creation.\nExcellent team player, able to collaborate across diverse functional groups and communicate complex technical concepts clearly.\nOutstanding verbal and written communication skills to effectively manage and articulate the health and integrity of data and systems to stakeholders.\n\nPlease feel free to contact us: 9440806850\nEmail ID : careers@jayamsolutions.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Apache Flink', 'Redis', 'Spark', 'Python', 'SCALA', 'Ci/Cd', 'Devops', 'AWS']",2025-06-12 05:28:13
Senior Data Engineer,Parkar Global Technologies,6 - 10 years,Not Disclosed,['Pune'],"About Position:\nWe are looking for a Senior Data Engineer to play a key role in building, optimizing, and maintaining our Azure-based data platform, which supports IoT data processing, analytics, and AI/ML applications. As part of our Data Platform Team, you will design and develop scalable data pipelines, implement data governance frameworks, and ensure high-performance data processing to drive digital transformation across our business.\n\nResponsibilities:\nData Pipeline Development: Design, build, and maintain high-performance, scalable ETL/ELT pipelines using Azure Data Factory, Databricks, and ADLS.\nData Platform Enhancement: Contribute to the development and optimization of our Azure-based data platform, ensuring efficiency, reliability, and security.\nIoT & High-Volume Data Processing: Work with large-scale IoT and operational datasets, optimizing data ingestion, transformation, and storage.\nData Governance & Quality: Implement data governance best practices, ensuring data integrity, consistency, and compliance.\nPerformance Optimization: Improve query performance and storage efficiency for analytics and reporting use cases.\nCollaboration: Work closely with data scientists, architects, and business teams to ensure data availability and usability.\nInnovation & Automation: Identify opportunities for automation and process improvements, leveraging modern tools and technologies.\n\nRequirement:\n6+ years of experience in data engineering with a focus on Azure cloud technologies.\nStrong expertise in Azure Data Factory, Databricks, ADLS, and Power BI.\nProficiency in SQL, Python, and Spark for data processing and transformation.\nExperience with IoT data ingestion and processing, handling high-volume, real-time data streams.\nStrong understanding of data modeling, lakehouse architectures, and medallion frameworks.\nExperience in building and optimizing scalable ETL/ELT processes.\nKnowledge of data governance, security, and compliance frameworks.\nExperience with monitoring, logging, and performance tuning of data workflows.\nStrong problem-solving and analytical skills with a platform-first mindset.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Pyspark', 'Azure Databricks', 'Azure Data Lake', 'Azure Devops', 'SQL']",2025-06-12 05:28:16
Senior Data Engineer,Amgen Inc,3 - 7 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nDesign, develop, and maintain scalable ETL/ELT pipelines to support structured, semi-structured, and unstructured data processing across the Enterprise Data Fabric.\nImplement real-time and batch data processing solutions, integrating data from multiple sources into a unified, governed data fabric architecture.\nOptimize big data processing frameworks using Apache Spark, Hadoop, or similar distributed computing technologies to ensure high availability and cost efficiency.\nWork with metadata management and data lineage tracking tools to enable enterprise-wide data discovery and governance.\nEnsure data security, compliance, and role-based access control (RBAC) across data environments.\nOptimize query performance, indexing strategies, partitioning, and caching for large-scale data sets.\nDevelop CI/CD pipelines for automated data pipeline deployments, version control, and monitoring.\nImplement data virtualization techniques to provide seamless access to data across multiple storage systems.\nCollaborate with cross-functional teams, including data architects, business analysts, and DevOps teams, to align data engineering strategies with enterprise goals.\nStay up to date with emerging data technologies and best practices, ensuring continuous improvement of Enterprise Data Fabric architectures.\nMust-Have Skills:\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies.\nProficiency in workflow orchestration, performance tuning on big data processing.\nStrong understanding of AWS services\nExperience with Data Fabric, Data Mesh, or similar enterprise-wide data architectures.\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\n9 to 12 years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Maven', 'SparkSQL Apache Spark', 'PySpark', 'Subversion', 'OLAP', 'Scaled Agile methodologies', 'SQL', 'Scaled Agile Framework', 'Jenkins', 'NOSQL database', 'Git', 'Databricks', 'Data Fabric', 'Data Mesh', 'AWS', 'Python']",2025-06-12 05:28:19
Senior Data Engineer,Amgen Inc,3 - 7 years,Not Disclosed,['Hyderabad'],"What you will do\nRole Description:\nWe are seeking a Senior Data Engineer with expertise in Graph Data technologies to join our data engineering team and contribute to the development of scalable, high-performance data pipelines and advanced data models that power next-generation applications and analytics. This role combines core data engineering skills with specialized knowledge in graph data structures, graph databases, and relationship-centric data modeling, enabling the organization to leverage connected data for deep insights, pattern detection, and advanced analytics use cases. The ideal candidate will have a strong background in data architecture, big data processing, and Graph technologies and will work closely with data scientists, analysts, architects, and business stakeholders to design and deliver graph-based data engineering solutions.\nRoles & Responsibilities:\nDesign, build, and maintain robust data pipelines using Databricks (Spark, Delta Lake, PySpark) for complex graph data processing workflows.\nOwn the implementation of graph-based data models, capturing complex relationships and hierarchies across domains.\nBuild and optimize Graph Databases such as Stardog, Neo4j, Marklogic or similar to support query performance, scalability, and reliability.\nImplement graph query logic using SPARQL, Cypher, Gremlin, or GSQL, depending on platform requirements.\nCollaborate with data architects to integrate graph data with existing data lakes, warehouses, and lakehouse architectures.\nWork closely with data scientists and analysts to enable graph analytics, link analysis, recommendation systems, and fraud detection use cases.\nDevelop metadata-driven pipelines and lineage tracking for graph and relational data processing.\nEnsure data quality, governance, and security standards are met across all graph data initiatives.\nMentor junior engineers and contribute to data engineering best practices, especially around graph-centric patterns and technologies.\nStay up to date with the latest developments in graph technology, graph ML, and network analytics.\nWhat we expect of you\nMust-Have Skills:\nHands-on experience in Databricks, including PySpark, Delta Lake, and notebook-based development.\nHands-on experience with graph database platforms such as Stardog, Neo4j, Marklogic etc.\nStrong understanding of graph theory, graph modeling, and traversal algorithms\nProficiency in workflow orchestration, performance tuning on big data processing\nStrong understanding of AWS services\nAbility to quickly learn, adapt and apply new technologies with strong problem-solving and analytical skills\nExcellent collaboration and communication skills, with experience working with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\nMasters degree and 3 to 4 + years of Computer Science, IT or related field experience\nBachelors degree and 5 to 8 + years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'SPARQL', 'Maven', 'PySpark', 'GSQL', 'Subversion', 'AWS services', 'Stardog', 'Cypher', 'SAFe', 'Jenkins', 'DevOps', 'Git', 'Neo4j', 'Delta Lake', 'Graph Databases', 'Spark', 'Marklogic', 'Gremlin']",2025-06-12 05:28:21
Data Engineer - Associate,FedEx,1 - 2 years,Not Disclosed,"['Gurugram', 'Bengaluru', 'Mumbai (All Areas)']","Role & responsibilities :\n\nDevelop and maintain data workflows using Ab Initio tools.\nAnalyze data, troubleshoot issues, and resolve defects within data pipelines.\nParticipate in Agile ceremonies including daily stand-ups, sprint planning, and reviews.\nApply CI/CD practices using tools like Jenkins and work within Unix/Linux environments.\nMaintain documentation including metadata definitions, onboarding materials, and SOPs.\nContribute to modernization and cloud-readiness efforts, particularly leveraging Azure and Databricks.\n\n\n\nPreferred candidate profile\n\n1-2 years of hands-on experience in data engineering, ETL development, or data analytics.\nExperience with SQL and working knowledge of Unix/Linux systems.\nFamiliarity with ETL tools such as Ab Initio.\nAbility to analyze, debug, and optimize data workflows.\nExposure to CI/CD pipelines and version control practices.\nStrong analytical thinking, attention to detail, and documentation skills.\nComfortable working in Agile development environments",Industry Type: Courier / Logistics (Logistics Tech),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Ab Initio', 'ETL', 'Unix', 'Linux', 'Informatica', 'Data Modeling', 'Data Warehousing', 'SQL']",2025-06-12 05:28:23
Immediate Joiner- Data Engineer,Healthedge,1 - 4 years,Not Disclosed,['Bengaluru'],"Data Engineer\nYou will be working with agile cross functional software development teams developing cutting age software to solve a significant problem in the Provider Data Management space. This hire will have experience building large scale complex data systems involving multiple cross functional data sets and teams. The ideal candidate will be excited about working on new product development, is comfortable pushing the envelope and challenging the status quo, sets high standards for him/herself and the team, and works well with ambiguity.\nWhat you will do:\nBuild data pipelines to assemble large, complex sets of data that meet non-functional and functional business requirements.\nWork closely with data architect, SMEs and other technology partners to develop & execute data architecture and product roadmap.\nBuild analytical tools to utilize the data pipeline, providing actionable insight into key business performance including operational efficiency and business metrics.\nWork with stakeholders including the leadership, product, customer teams to support their data infrastructure needs while assisting with data-related technical issues.\nAct as a subject matter expert to other team members for technical guidance, solution design and best practices within the customer organization.\nKeep current on big data and data visualization technology trends, evaluate, work on proof-of-concept and make recommendations on cloud technologies.\nWhat you bring:\n2+ years of data engineering experience working in partnership with large data sets (preferably terabyte scale)\nExperience in building data pipelines using any of the ETL tools such as Glue, ADF, Notebooks, Stored Procedures, SQL/Python constructs or similar.\nDeep experience working with industry standard RDBMS such Postgres, SQL Server, Oracle, MySQL etc. and any of the analytical cloud databases such as Big Query, Redshift, Snowflake or similar\nAdvanced SQL expertise and solid programming experience with Python and/or Spark\nExperience working with orchestration tools such as Airflow and building complex dependency workflows.\nExperience, developing and implementing Data Warehouse or Data Lake Architectures, OLAP technologies, data modeling with star/snowflake-schemas to enable analytics & reporting.\nGreat problem-solving capabilities, troubleshooting data issues and experience in stabilizing big data systems.\nExcellent communication and presentation skills as youll be regularly interacting with stakeholders and engineering leadership.\nBachelors or master's in quantitative disciplines such as Computer Science, Computer Engineering, Analytics, Mathematics, Statistics, Information Systems, or other scientific fields.\nBonus points:\nHands-on deep experience with cloud data migration, and experience working with analytic platforms like Fabric, Databricks on the cloud.\nCertification in one of the cloud platforms (AWS/GCP/Azure)\nExperience or demonstrated understanding with real-time data streaming tools like Kafka, Kinesis or any similar tools.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ETL', 'SQL', 'Pyspark', 'Cloud', 'Python']",2025-06-12 05:28:25
Data Engineer - Analyst,FedEx,3 - 5 years,Not Disclosed,"['Gurugram', 'Bengaluru', 'Mumbai (All Areas)']","Role & responsibilities :\n\nDesign,develop, and maintain ETL workflows using Ab Initio.\nManage and support critical data pipelines and data sets across complex,high-volume environments.\nPerform data analysis and troubleshoot issues across Teradata and Oracle data sources.\nCollaborate with DevOps for CI/CD pipeline integration using Jenkins, and manage deployments in Unix/Linux environments.\nParticipate in Agile ceremonies including stand-ups, sprint planning, and roadmap discussions.\nSupport cloud migration efforts, including potential adoption of Azure,Databricks, and PySparkbased solutions.\nContribute to project documentation, metadata management (LDM, PDM), onboarding guides, and SOPs\n\n\n\nPreferred candidate profile\n\n3 years of experience in data engineering, with proven expertise in ETL development and maintenance.\nProficiency with Ab Initio tools (GDE, EME, Control Center).\nStrong SQL skills, particularly with Oracle or Teradata.\nSolid experience with Unix/Linux systems and scripting.\nFamiliarity with CI/CD pipelines using Jenkins or similar tools.\nStrong communication skills and ability to collaborate with cross-functional teams.",Industry Type: Courier / Logistics (Logistics Tech),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Ab Initio', 'Data Modeling', 'ETL', 'Cicd Pipeline', 'Informatica', 'Data Warehousing', 'Databricks', 'Teradata', 'Oracle', 'SQL']",2025-06-12 05:28:27
Staff Data Engineer - Machine Learning,Netradyne,5 - 8 years,22.5-35 Lacs P.A.,['Bengaluru'],"Role and Responsibilities:\n\nYou will be embedded within a team of machine learning engineers and data scientists; responsible for building and productizing generative AI and deep learning solutions. You will:\nDesign, develop and deploy production ready scalable solutions that utilizes GenAI, Traditional ML models, Data science and ETL pipelines\nCollaborate with cross-functional teams to integrate AI-driven solutions into business operations.\nBuild and enhance frameworks for automation, data processing, and model deployment.\nUtilize Gen-AI tools and workflows to improve the efficiency and effectiveness of AI solutions.\nConduct research and stay updated with the latest advancements in generative AI and related technologies.\nDeliver key product features within cloud analytics.\n\nRequirements:\n\nB. Tech, M. Tech or PhD in Computer Science, Data Science, Electrical Engineering, Statistics, Maths, Operations Research or related domain.\nStrong programming skills in Python, SQL and solid fundamentals in computer science, particularly in algorithms, data structures, and OOP.\nExperience with building end-to-end solutions on AWS cloud infra.\nGood understanding of internals and schema design for various data stores (RDBMS, Vector databases and NoSQL).\nExperience with Gen-AI tools and workflows, and large language models (LLMs).\nExperience with cloud platforms and deploying models at scale.\nStrong analytical and problem-solving skills with a keen attention to detail.\nStrong knowledge of statistics, probability, and estimation theory.\n\nDesired Skills:\n\nFamiliarity with frameworks such as PyTorch, TensorFlow and Hugging Face.\nExperience with data visualization tools like Tableau, Graphana, Plotly-Dash.\nExposure to AWS services like Kinesis, SQS, EKS, ASG, lambda etc.\nExpertise in at least one popular Python web-framework (like FastAPI, Django or Flask).\nExposure to quick prototyping using Streamlit, Gradio, Dash etc.\nExposure to Big Data processing (Snowflake, Redshift, HDFS, EMR)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'AWS', 'Generative Artificial Intelligence', 'Python', 'Big Data Technologies']",2025-06-12 05:28:30
Snowflake Data Engineer,Tredence,3 - 8 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\n\nDesign, build, and maintain scalable data pipelines using DBT and Airflow.\nDevelop and optimize SQL queries and data models in Snowflake.\nImplement ETL/ELT workflows, ensuring data quality, performance, and reliability.\nWork with Python for data processing, automation, and integration tasks.\nHandle JSON data structures for data ingestion, transformation, and APIs.\nLeverage AWS services (e.g., S3, Lambda, Glue, Redshift) for cloud-based data solutions. Collaborate with data analysts, engineers, and business teams to deliver high-quality data products.",,,,"['Snowflake', 'DBT', 'SQL']",2025-06-12 05:28:32
Urgent hiring For Cloud Data Engineer,Wowjobs,7 - 10 years,30-45 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities\n*  Design, Build, and Maintain ETL Pipelines: Develop robust, scalable, and efficient ETL workflows to ingest, transform, and load data into distributed data products within the Data Mesh architecture.\n*   Data Transformation with dbt: Use dbt to build modular, reusable transformation workflows that align with the principles of Data Products.\n*   Cloud Expertise: Leverage Google Cloud Platform (GCP) services such as BigQuery, Cloud Storage, Pub/Sub, and Dataflow to implement highly scalable data solutions.\n*   Data Quality & Governance: Enforce strict data quality standards by implementing validation checks, anomaly detection mechanisms, and monitoring frameworks.\n*   Performance Optimization: Continuously optimize ETL pipelines for speed, scalability, and cost efficiency.\n*   Collaboration & Ownership: Work closely with data product owners, BI developers, and stakeholders to understand requirements and deliver on expectations. Take full ownership of your deliverables.\n*   Documentation & Standards: Maintain detailed documentation of ETL workflows, enforce coding standards, and adhere to best practices in data engineering.\n*   Troubleshooting & Issue Resolution: Proactively identify bottlenecks or issues in pipelines and resolve them quickly with minimal disruption.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python coding', 'Cloud data engineer', 'ETL Workflow']",2025-06-12 05:28:34
Azure Data Engineer,Management Consulting,5 - 10 years,6-15 Lacs P.A.,['Bengaluru'],"Urgent Hiring _ Azure Data Engineer with a leading Management Consulting Company @ Bangalore Location.\n\nStrong expertise in Databricks & Pyspark while dealing with batch processing or live (streaming) data sources.\n4+ relevant years of experience in Databricks & Pyspark/Scala\n7+ total years of experience\nGood in data modelling and designing.\n\nCtc- Hike Shall be considered on Current/Last Drawn Pay\n\nApply - rohita.robert@adecco.com\n\nHas worked on real data challenges and handled high volume, velocity, and variety of data.\nExcellent analytical & problem-solving skills, willingness to take ownership and resolve technical challenges.\nContributes to community building initiatives like CoE, CoP.\nMandatory skills:\nAzure - Master\nELT - Skill\nData Modeling - Skill\nData Integration & Ingestion - Skill\nData Manipulation and Processing - Skill\nGITHUB, Action, Azure DevOps - Skill\nData factory, Databricks, SQL DB, Synapse, Stream Analytics, Glue, Airflow, Kinesis, Redshift, SonarQube, PyTest - Skill",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Azure Databricks', 'Airflow', 'Streaming Data', 'Elt', 'Data Bricks', 'Redshift', 'SQL', 'PyTest', 'Azure Data Engineer', 'Datafactory', 'Synapse', 'Glue', 'Stream Analytics', 'Kinesis', 'SCALA', 'SonarQube', 'Data Modeling']",2025-06-12 05:28:37
Sr Data Engineer - Fully Remote & Immediate Opportunity,Zealogics.com,10 - 15 years,Not Disclosed,[],"10 yrs of exp working in cloud-native data (Azure Preferred),Databricks, SQL,PySpark, migrating from Hive Metastore to Unity Catalog, Unity Catalog, implementing Row-Level Security (RLS), metadata-driven ETL design patterns,Databricks certifications",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Azure', 'Metadata', 'Data Bricks', 'Unity Catalog', 'ETL design', 'SQL']",2025-06-12 05:28:39
Data Engineer II - Marketplace (Experimentation Track),Booking Holdings,5 - 10 years,Not Disclosed,['Bengaluru'],"We are looking for a Data Engineer to join our team and help us to improve the platform that supports one of the best experimentation tools in the world.\nYou will work side by side with other data engineers and site reliability engineers to improve the reliability, scalability, maintenance and operations of all the data products that are part of the experimentation tool at Booking.com.\nYour day to day work includes but is not limited to: maintenance and operations of data pipelines and products that handles data at big scale; the development of capabilities for monitoring, alerting, testing and troubleshooting of the data ecosystem of the experiment platform; and the delivery of data products that produce metrics for experimentation at scale. You will collaborate with colleagues in Amsterdam to achieve results the right way. This will include engineering managers, product managers, engineers and data scientists.\nKey Responsibilities and Duties\nTake ownership of multiple data pipelines and products and provide innovative solutions to reduce the operational workload required to maintain them\nRapidly developing next-generation scalable, flexible, and high-performance data pipelines.\nContribute to the development of data platform capabilities such as testing, monitoring, debugging and alerting to improve the development environment of data products\nSolve issues with data and data pipelines, prioritizing based on customer impact.\nEnd-to-end ownership of data quality in complex datasets and data pipelines.\nExperiment with new tools and technologies, driving innovative engineering solutions to meet business requirements regarding performance, scaling, and data quality.\nProvide self-organizing tools that help the analytics community discover data, assess quality, explore usage, and find peers with relevant expertise.\nServe as the main point of contact for technical and business stakeholders regarding data engineering issues, such as pipeline failures and data quality concerns\nRole requirements\nMinimum 5 years of hands-on experience in data engineering as a Data Engineer or as a Software Engineer developing data pipelines and products.\nBachelors degree in Computer Science, Computer or Electrical Engineering, Mathematics, or a related field or 5 years of progressively responsible experience in the specialty as equivalent\nSolid experience in at least one programming language. We use Java and Python\nExperience building production data pipelines in the cloud, setting up data-lakes and server-less solutions\nHands-on experience with schema design and data modeling\nExperience designing systems E2E and knowledge of basic concepts (lb, db, caching, NoSQL, etc)\nKnowledge of Flink, CDC, Kafka, Airflow, Snowflake, DBT or equivalent tools\nPractical experience building data platform capabilities like testing, alerting, monitoring, debugging, security\nExperience working with big data.\nExperience working with teams located in different timezones is a plus\nExperience with experimentation, statistics and A/B testing is a plus",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Airflow', 'Java', 'CDC', 'NoSQL', 'Snowflake', 'DBT', 'Kafka', 'Python']",2025-06-12 05:28:41
Tech. PM - Data Engineering-Data Analytics@ Gurgaon/Blore_Urgent,A global leader in delivering innovative...,5 - 10 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Job Title - Technical Project Manager\n\nLocation - Gurgaon/ Bangalore\n\nNature of Job - Permanent\n\nDepartment - data analytics\n\nWhat you will be doing\n\n\nDemonstrated client servicing and business analytics skills with at least 5 - 9 years of experience as data engineer, BI developer, data analyst, technical project manager, program manager etc.\nTechnical project management- drive BRD, project scope, resource allocation, team\ncoordination, stakeholder communication, UAT, Prod fix, change requests, project governance\nSound knowledge of banking industry (payments, retail operations, fraud etc.)\nStrong ETL experience or experienced Teradata developer\nManaging team of business analysts, BI developers, ETL developers to ensure that projects are completed on time\nResponsible for providing thought leadership and technical advice on business issues\nDesign methodological frameworks and solutions.\n\n\nWhat were looking for\n\n\nBachelors/masters degree in computer science/data science/AI/statistics, Certification in Gen AI. Masters degree Preferred.\nManage multiple projects, at a time, from inception to delivery\nSuperior problem-solving, analytical, and quantitative skills\nEntrepreneurial mindset, coupled with a “can do” attitude\nDemonstrated ability to collaborate with cross-functional, cross-border teams and coach / mentor colleagues.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Technical Project Manager', 'Data Engineering', 'multiple projects', 'Technical project management', 'Data Analytics', 'project scope', 'ETL Pipeline', 'team coordination', 'resource allocation', 'Prod fix', 'drive BRD', 'program manager', 'Big data']",2025-06-12 05:28:43
Data Engineer-Having Stratup-Mid-Size company Exp.@ Bangalore_Urgent,"As a leader in this space, we deliver wo...",8 - 13 years,Not Disclosed,['Bengaluru'],"Data Engineer\n\nLocation: Bangalore - Onsite\nExperience: 8 - 15 years\nType: Full-time\n\nRole Overview\n\nWe are seeking an experienced Data Engineer to build and maintain scalable, high-performance data pipelines and infrastructure for our next-generation data platform. The platform ingests and processes real-time and historical data from diverse industrial sources such as airport systems, sensors, cameras, and APIs. You will work closely with AI/ML engineers, data scientists, and DevOps to enable reliable analytics, forecasting, and anomaly detection use cases.\nKey Responsibilities\nDesign and implement real-time (Kafka, Spark/Flink) and batch (Airflow, Spark) pipelines for high-throughput data ingestion, processing, and transformation.\nDevelop data models and manage data lakes and warehouses (Delta Lake, Iceberg, etc) to support both analytical and ML workloads.\nIntegrate data from diverse sources: IoT sensors, databases (SQL/NoSQL), REST APIs, and flat files.\nEnsure pipeline scalability, observability, and data quality through monitoring, alerting, validation, and lineage tracking.\nCollaborate with AI/ML teams to provision clean and ML-ready datasets for training and inference.\nDeploy, optimize, and manage pipelines and data infrastructure across on-premise and hybrid environments.\nParticipate in architectural decisions to ensure resilient, cost-effective, and secure data flows.\nContribute to infrastructure-as-code and automation for data deployment using Terraform, Ansible, or similar tools.\n\n\nQualifications & Required Skills\n\nBachelors or Master’s in Computer Science, Engineering, or related field.\n6+ years in data engineering roles, with at least 2 years handling real-time or streaming pipelines.\nStrong programming skills in Python/Java and SQL.\nExperience with Apache Kafka, Apache Spark, or Apache Flink for real-time and batch processing.\nHands-on with Airflow, dbt, or other orchestration tools.\nFamiliarity with data modeling (OLAP/OLTP), schema evolution, and format handling (Parquet, Avro, ORC).\nExperience with hybrid/on-prem and cloud platforms (AWS/GCP/Azure) deployments.\nProficient in working with data lakes/warehouses like Snowflake, BigQuery, Redshift, or Delta Lake.\nKnowledge of DevOps practices, Docker/Kubernetes, Terraform or Ansible.\nExposure to data observability, data cataloging, and quality tools (e.g., Great Expectations, OpenMetadata).\nGood-to-Have\nExperience with time-series databases (e.g., InfluxDB, TimescaleDB) and sensor data.\nPrior experience in domains such as aviation, manufacturing, or logistics is a plus.\n\nRole & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['aviation', 'Data Modeling', 'Python', 'OLAP', 'Cloud', 'ORC', 'logistics', 'Avro', 'Terraform', 'Snowflake', 'manufacturing', 'AWS', 'Parquet', 'Java', 'Azure', 'BigQuery', 'Data', 'Redshift', 'SQL', 'TimescaleDB', 'GCP', 'InfluxDB', 'dbt', 'Ansible', 'OLTP', 'Kubernetes']",2025-06-12 05:28:46
"Tech Lead- Data Engineer (SQL, Data Lake, Azure Data Factory)",Global Technology Company @ Pune,6 - 10 years,25-30 Lacs P.A.,"['Pune', 'Bengaluru', 'Mumbai (All Areas)']","Minimum of 6 yrs of Data Engineering Exp\nMust be an expert in SQL, Data Lake, Azure Data Factory, Azure Synapse, ETL, Databricks\nMust be an expert in data modeling, writing complex queries in SQL\nAbility to convert SQL code to PySpark\n\nRequired Candidate profile\nExp with SQL, Python, data modelling, data warehousing & dimensional modelling concept\nFamiliarity with data governance, data security & Production deployments using Azure DevOps CICD pipelines.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Synapse', 'Pyspark', 'Azure Data factory', 'Data Lake', 'Azure Services', 'Databricks', 'ETL', 'SQL']",2025-06-12 05:28:48
Azure Data Engineer/Lead/Architect (5 - 20 Years) (Pan India Location),Allegis Group,5 - 10 years,Not Disclosed,[],"Azure Data Engineer/Lead/Architect (5 - 20 Years) (Pan India Location)\nJob Location : Hyderabad / Bangalore / Chennai / Kolkata / Noida/ Gurgaon / Pune / Indore / Mumbai\n\n\n5 -20 years of relevant hands on development experience. And 4+ years as Azure Data Engineering role\nProficient in Azure technologies like ADB, ADF, SQL(capability of writing complex SQL queries), ADB, PySpark, Python, Synapse, Delta Tables, Unity Catalog\nHands on in Python, PySpark or Spark SQL\nHands on in Azure Analytics and DevOps\nTaking part in Proof of Concepts (POCs) and pilot solutions preparation\nAbility to conduct data profiling, cataloguing, and mapping for technical design and construction of technical data flows\nExperience in business processing mapping of data and analytics solutions",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Analytics', 'Azure Data Engineering', 'Azure Databricks', 'Devops', 'Python', 'Azure Data Factory', 'Pyspark', 'Azure', 'Adb']",2025-06-12 05:28:51
Cloud Data Engineer,Acesoft,7 - 9 years,19-20 Lacs P.A.,['Bengaluru'],"Hi all,\nWe are hiring for the role Cloud Data Engineer\nExperience: 7 - 9 years\nLocation: Bangalore\nNotice Period: Immediate - 15 Days\nSkills:\nOverall, 7 to 9 years of experience in cloud data and analytics platforms such as AWS, Azure, or GCP\n• Including 3+ years experience with Azure cloud Analytical tools is a must\n• Including 5+ years of experience working with data & analytics concepts such as SQL, ETL, ELT, reporting and report building, data visualization, data lineage, data importing & exporting, and data warehousing\n• Including 3+ years of experience working with general IT concepts such as integrations, encryption, authentication & authorization, batch processing, real-time processing, CI/CD, automation\n• Advanced knowledge of cloud technologies and services, specifically around Azure Data Analytics tools\no Azure Functions (Compute)\no Azure Blob Storage (Storage)\no Azure Cosmos DB (Databases)\no Azure Synapse Analytics (Databases)\no Azure Data Factory (Analytics)\no Azure Synapse Serverless SQL Pools (Analytics)\no Azure Event Hubs (Analytics- Realtime data)\n• Strong coding skills in languages such as\no SQL\no Python\no PySpark\n• Experience in data streaming technologies such as Kafka or Azure Event Hubs\n• Experience in handling unstructured streaming data is highly desired\n• Knowledge of Business Intelligence Dimensional Modelling, Star Schemas, Slowly Changing Dimensions\n• Broad understanding of data engineering methodologies and tools, including Data warehousing, DevOps/DataOps, Data ingestion, ELT/ETL and Data visualization tools\n• Knowledge of database management systems, data modelling, and data warehousing best practices\n• Experience in software development on a team using Agile methodology\n• Knowledge of data governance and security practices\n\nIf you are interested drop your resume at mojesh.p@acesoftlabs.com\nCall: 9701971793",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Cloud Analytics', 'Azure Cloud', 'Data Warehousing', 'AWS', 'Python', 'Azure Data Analytics tools', 'authentication & authorization', 'PySpark', 'batch processing', 'Elt', 'sql', 'automation', 'encryption', 'Azure Cosmos DB', 'real-time processing', 'CI/CD', 'etl', 'integrations']",2025-06-12 05:28:53
"Google Cloud Platform Data Engineer -GCP,BigQuery,SQL, Cloud Function",Tredence,5 - 10 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","Role - GCP Data Engineer\nExperience:4+ years\nPreferred - Data Engineering Background\nLocation - Bangalore, Chennai, Pune, Gurgaon, Kolkata\nRequired Skills - GCP DE Experience, Big query, SQL, Cloud compressor/Python, Cloud functions, Dataproc+pyspark, Python injection, Dataflow+PUB/SUB\n\nHere is the job description for the same -\nJob Requirement:\nHave Implemented and Architected solutions on Google Cloud Platform using the components of GCP\nExperience with Apache Beam/Google Dataflow/Apache Spark in creating end to end data pipelines.\nExperience in some of the following: Python, Hadoop, Spark, SQL, Big Query, Big Table Cloud Storage, Datastore, Spanner, Cloud SQL, Machine Learning.\nExperience programming in Java, Python, etc.\nExpertise in at least two of these technologies: Relational Databases, Analytical Databases, NoSQL databases.\nCertified in Google Professional Data Engineer/ Solution Architect is a major Advantage",,,,"['Pubsub', 'GCP', 'Bigquery', 'Google Cloud Platforms', 'SQL', 'Data Flow', 'Dataproc']",2025-06-12 05:28:56
Gcp Data Engineer,Royal Cyber,9 - 14 years,Not Disclosed,[],"Minimum 7+ years in data engineering with 5+ years of hands-on experience on GCP.\nProven track record with tools and services like BigQuery, Cloud Composer (Apache Airflow), Cloud Functions, Pub/Sub, Cloud Storage, Dataflow, and IAM/VPC.\nDemonstrated expertise in Apache Spark (batch and streaming), PySpark, and building scalable API integrations.\nAdvanced Airflow skills including custom operators, dynamic DAGs, and workflow performance tuning.\nCertifications\nGoogle Cloud Professional Data Engineer certification preferred.\nKey Skills\nMandatory Technical Skills\nAdvanced Python (PySpark, Pandas, pytest) for automation and data pipelines.\nStrong SQL with experience in window functions, CTEs, partitioning, and optimization.\nProficiency in GCP services including BigQuery, Dataflow, Cloud Composer, Cloud Functions, and Cloud Storage.\nHands-on with Apache Airflow, including dynamic DAGs, retries, and SLA enforcement.\nExpertise in API data ingestion, Postman collections, and REST/GraphQL integration workflows.\nFamiliarity with CI/CD workflows using Git, Jenkins, or Bitbucket.\nExperience with infrastructure security and governance using IAM and VPC.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Part Time, Temporary/Contractual","['GCP', 'Bigquery', 'Google Cloud Platforms', 'Cloud Storage', 'Data Flow']",2025-06-12 05:28:58
"Data Engineer( Python, AWS, Databricks, EKS, Airflow)",Banking,5 - 9 years,Not Disclosed,['Bengaluru'],"Exprence 5-8 Years\nLocation - Bangalore\nMode C2H\n\nHands on data engineering experience.\nHands on experience with Python programming\nHands-on Experience with AWS & EKS\nWorking knowledge of Unix, Databases, SQL\nWorking Knowledge on Databricks\nWorking Knowledge on Airflow and DBT",Industry Type: Investment Banking / Venture Capital / Private Equity,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Airflow', 'Data Engineering', 'AWS', 'Python', 'SQL', 'Databricks', 'Eks']",2025-06-12 05:29:00
Data Engineer - AWS,Happiest Minds Technologies,6 - 10 years,Not Disclosed,"['Pune', 'Bengaluru']","Role & responsibilities\nEssential Skills: Experience: 6 to 10 yrs\n- Technical Expertise: Proficiency in AWS services such as Amazon S3, Redshift, EMR, Glue, Lambda, and Kinesis. Strong skills in SQL and experience with scripting languages like Python or Java.\n- Data Engineering Experience: Hands on experience in building and maintaining data pipelines, data modeling, and working with big data technologies.\n- Problem-Solving Skills: Ability to analyze complex data issues and develop effective solutions to optimize data processing and storage.",,,,"['Data Engineering', 'Pyspark', 'Aws Lambda', 'Amazon Redshift', 'Aws Glue', 'Athena', 'AWS', 'Python', 'SQL']",2025-06-12 05:29:03
Aws Data Engineer,Hiring for Leading MNC Company!!,8 - 13 years,Not Disclosed,['Bengaluru'],"Warm Greetings from SP Staffing!!\n\nRole:AWS Data Engineer\nExperience Required :8 to 15 yrs\nWork Location :Bangalore\n\nRequired Skills,\n\nTechnical knowledge of data engineering solutions and practices. Implementation of data pipelines using tools like EMR, AWS Glue, AWS Lambda, AWS Step Functions, API Gateway, Athena\nProficient in Python and Spark, with a focus on ETL data processing and data engineering practices.\n\nInterested candidates can send resumes to nandhini.spstaffing@gmail.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ETL', 'AWS', 'Pyspark', 'python', 'EMR', 'Aws Glue', 'Aws Emr', 'AWS Data Engineer', 'Aws Lambda', 'Lakehouse', 'spark', 'Data Engineer', 'Athena', 'gateway']",2025-06-12 05:29:05
Azure Data Engineer,Tech Mahindra,5 - 8 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","Company Name: Tech Mahindra\nExperience: 5-8 Years\nLocation: Bangalore/Hyderabad (Hybrid Model)\nInterview Mode: Virtual\nInterview Rounds: 2 Rounds\nNotice Period: Immediate to 30 days\nGeneric Responsibilities :\nDesign, develop, and maintain large-scale data pipelines using Azure Data Factory (ADF) to extract, transform, and load data from various sources into Azure Databricks.\nCollaborate with cross-functional teams to gather requirements and design solutions for complex business problems.\nDevelop SQL queries and stored procedures to optimize database performance and troubleshoot issues in Azure Databricks.\nEnsure high availability, scalability, and security of the deployed solutions by monitoring logs, metrics, and alerts.\nGeneric Requirements :\n5-8 years of experience in designing and developing large-scale data engineering projects on Microsoft Azure platform.\nStrong expertise in Azure Data Factory (ADF), Azure Databricks, SQL Server Management Studio (T-SQL).\nExperience working with big data technologies such as Hadoop Distributed File System (HDFS), Spark Core/Scala programming languages.",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Azure Data Factory', 'Azure Data Lake', 'Azure Data Services', 'Data Bricks', 'SQL']",2025-06-12 05:29:07
Junior Data Engineer,Talent Corner Hr Services,1 - 5 years,7-10 Lacs P.A.,['Bengaluru( JP Nagar )'],"We are looking for a Junior Data Engineer with 1–3 years of experience, primarily focused on database management and data processing using MySQL. The candidate will support the data engineering team in maintaining reliable data pipelines",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MySQL', 'Query Optimization', 'Data Processing']",2025-06-12 05:29:10
Associate Data Engineer,Amgen Inc,0 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role We are seeking a Associate Data Engineer to design, build, and maintain scalable data solutions that drive business insights. You will work with large datasets, cloud platforms (AWS preferred), and big data technologies to develop ETL pipelines, ensure data quality, and support data governance initiatives.\nDevelop and maintain data pipelines, ETL/ELT processes, and data integration solutions.\nDesign and implement data models, data dictionaries, and documentation for accuracy and consistency.\nEnsure data security, privacy, and governance standard processes.\nUse Databricks, Apache Spark (PySpark, SparkSQL), AWS, Redshift, for scalable data processing.\nCollaborate with cross-functional teams to understand data needs and deliver actionable insights.\nOptimize data pipeline performance and explore new tools for efficiency.\nFollow best practices in coding, testing, and infrastructure-as-code (CI/CD, version control, automated testing).\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. Strong problem-solving, critical thinking, and communication skills.\nAbility to collaborate effectively in a team setting.\nProficiency in SQL, data analysis tools, and data visualization.\nHands-on experience with big data technologies (Databricks, Apache Spark, AWS, Redshift ).\nExperience with ETL tools, workflow orchestration, and performance tuning for big data.\nBasic Qualifications:\nBachelors degree and 0 to 3 years of experience OR Diploma and 4 to 7 years of experience in Computer science, IT or related field.\nPreferred Qualifications:\nKnowledge of data modeling, warehousing, and graph databases\nExperience with Python, SageMaker, and cloud data platforms.\nAWS Certified Data Engineer or Databricks certification preferred.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'data analysis', 'data modeling', 'data warehousing', 'data visualization', 'Databricks', 'ETL', 'AWS', 'SQL', 'Apache Spark', 'Python']",2025-06-12 05:29:12
Associate Data Engineer | Pricing | Full Time Contract 18 Months,Argus India Price Reporting Services,1 - 3 years,5-11 Lacs P.A.,['Mumbai (All Areas)'],"Associate Data Engineer - (Fixed Term Contract)\nMumbai\nJob Purpose:\nDue to the continued growth of the business and the importance of the data we use on a daily basis, we are currently looking for a for a junior data engineer to join our global Data team in Mumbai.\nYou will work closely with internal clients of the data team to support and maintain R (incl. R Shiny), Excel and database-based processes for gathering data, calculating prices and producing the reports and data feeds. The work also involves writing robust automated processes in R.\nThe team supports Argus key business processes every day, as such you will be required to work on a shift-based rota with other members of the team supporting the business until 8pm. Typically support hours run from 11pm to 8pm with each member of the team participating 2/3 times a week.\n\nKey Responsibilities:\nSupport and development of data processing systems\nClient support with queries relating to\nintegration of Argus data and metadata into client systems\ndata validation\nprovision of data\nData and systems support to Argus staff\nProject development\nMaintenance and development of existing systems\nmetadata modification\ndata cleansing\ndata checking\nRequired Skills and Experience:\nSignificant recent experience of developing tools using R (incl., R Shiny applications) in a commercial (work) environment.\nGood knowledge and experience of SQL, preferably in Oracle or MySQL, including stored procedures, functions, and triggers.\nExperience with version control systems (e.g., Git) and Unit Testing in R is required.\nAbility to work both as part of a team and autonomously\nExcellent communication skills.\n\nDesired Skills and Experience:\nBS degree in Computer Science, Mathematics, Business, Engineering or related field.\nExperience in visualisation techniques is desirable.\nAny experience working with energy markets and commodities data is highly desirable\nExperience developing production-grade scalable applications in R.\n\nPersonal Attributes:\nAbility to interact with non-technical people in plain language\nInnovative thinker with good problem-solving abilities and attention to detail\nNumerically proficient\nSelf-motivated with ability to work independently, prioritising tasks to meet deadlines\nCustomer service focused.\n\nBenefits:\nCompetitive salary\nFlexible Working Policy\nGroup healthcare scheme\n18 days annual leave\n8 days casual leave\nExtensive internal and external training",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['R', 'R Shiny', 'Unit Testing', 'Version Control Systems Svn', 'SQL']",2025-06-12 05:29:14
Associate Data Engineer,Amgen Inc,0 - 2 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for an Associate Data Engineer with deep expertise in writing data pipelines to build scalable, high-performance data solutions. The ideal candidate will be responsible for developing, optimizing and maintaining complex data pipelines, integration frameworks, and metadata-driven architectures that enable seamless access and analytics. This role prefers deep understanding of the big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nData Engineer who owns development of complex ETL/ELT data pipelines to process large-scale datasets\nContribute to the design, development, and implementation of data pipelines, ETL/ELT processes, and data integration solutions\nEnsuring data integrity, accuracy, and consistency through rigorous quality checks and monitoring\nExploring and implementing new tools and technologies to enhance ETL platform and performance of the pipelines\nProactively identify and implement opportunities to automate tasks and develop reusable frameworks\nEager to understand the biotech/pharma domains & build highly efficient data pipelines to migrate and deploy complex data across systems\nWork in an Agile and Scaled Agile (SAFe) environment, collaborating with cross-functional teams, product owners, and Scrum Masters to deliver incremental value\nUse JIRA, Confluence, and Agile DevOps tools to manage sprints, backlogs, and user stories.\nSupport continuous improvement, test automation, and DevOps practices in the data engineering lifecycle\nCollaborate and communicate effectively with the product teams, with cross-functional teams to understand business requirements and translate them into technical solutions\nMust-Have Skills:\nExperience in Data Engineering with a focus on Databricks, AWS, Python, SQL, and Scaled Agile methodologies\nProficiency & Strong understanding of data processing and transformation of big data frameworks (Databricks, Apache Spark, Delta Lake, and distributed computing concepts)\nStrong understanding of AWS services and can demonstrate the same\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery, and DevOps practices\nGood-to-Have Skills:\nData Engineering experience in Biotechnology or pharma industry\nExposure to APIs, full stack development\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\nBachelors degree and 2 to 5 + years of Computer Science, IT or related field experience\nOR\nMasters degree and 1 to 4 + years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Maven', 'test automation', 'data engineering lifecycle', 'Scaled Agile methodologies', 'JIRA', 'SQL', 'Apache Spark', 'Jenkins', 'Agile DevOps tools', 'ETL platform', 'Confluence', 'Scaled Agile', 'Delta Lake', 'Agile', 'Databricks', 'AWS', 'Python']",2025-06-12 05:29:17
Data Engineer Graph – Research Data and Analytics,Amgen Inc,2 - 4 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will be part Researchs Semantic Graph Team is seeking a qualified individual to design, build, and maintain solutions for scientific data that drive business decisions for Research. The successful candidate will construct scalable and high-performance data engineering solutions for extensive scientific datasets and collaborate with Research partners to address their data requirements. The ideal candidate should have experience in the pharmaceutical or biotech industry, leveraging their expertise in semantics, taxonomies, and linked data principles to ensure data harmonization and interoperability. Additionally, this individual should demonstrate robust technical skills, proficiency with data engineering technologies, and a thorough understanding of data architecture and ETL processes.\nRoles & Responsibilities:\nDesign, develop, and implement data pipelines, ETL/ELT processes, and data integration solutions\nTake ownership of data pipeline projects from inception to deployment, manage scope, timelines, and risks\nDevelop and maintain semantic data models for biopharma scientific data, data dictionaries, and other documentation to ensure data accuracy and consistency\nOptimize large datasets for query performance\nCollaborate with global multi-functional teams including research scientists to understand data requirements and design solutions that meet business needs\nImplement data security and privacy measures to protect sensitive data\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions\nCollaborate with Data Architects, Business SMEs, Software Engineers and Data Scientists to design and develop end-to-end data pipelines to meet fast paced business needs across geographic regions\nIdentify and resolve [complex] data-related challenges\nAdhere to standard processes for coding, testing, and designing reusable code/component\nExplore new tools and technologies that will help to improve ETL platform performance\nParticipate in sprint planning meetings and provide estimations on technical implementation\nMaintain comprehensive documentation of processes, systems, and solutions\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. T\nBasic Qualifications and Experience:\nDoctorate Degree OR Masters degree with 2- 4years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nBachelors degree with 4- 6years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nDiploma with 7- 9 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field\n\n\nPreferred Qualifications and Experience:\n4+ years of experience in designing and supporting biopharma scientific research data analytics (software platforms)\n\n\nFunctional Skills:\nMust-Have Skills:\nProficiency in SQL and Python for data engineering, test automation frameworks (pytest), and scripting tasks\nHands on experience with data technologies and platforms, such as Databricks, workflow orchestration, performance tuning on big data processing.\nExcellent problem-solving skills and the ability to work with large, complex datasets\n\n\nGood-to-Have Skills:\nA passion for tackling complex challenges in drug discovery with technology and data\nExperience with system administration skills, such as managing Linux and Windows servers, configuring network infrastructure, and automating tasks with shell scripting. Examples include setting up and maintaining virtual machines, troubleshooting server issues, and ensuring data security through regular updates and backups.\nSolid understanding of data modeling, data warehousing, and data integration concepts\nSolid experience using RDBMS (e.g. Oracle, MySQL, SQL server, PostgreSQL)\nKnowledge of cloud data platforms (AWS preferred)\nExperience with data visualization tools (e.g. Dash, Plotly, Spotfire)\nExperience with diagramming and collaboration tools such as Miro, Lucidchart or similar tools for process mapping and brainstorming\nExperience writing and maintaining user documentation in Confluence\nUnderstanding of data governance frameworks, tools, and standard processes\n\n\nProfessional Certifications:\nDatabricks Certified Data Engineer Professional preferred\n\n\nSoft Skills:\nExcellent critical-thinking and problem-solving skills\nGood communication and collaboration skills\nDemonstrated awareness of how to function in a team setting\nDemonstrated presentation skills",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analytics', 'PostgreSQL', 'MySQL', 'ETL', 'ELT', 'Oracle', 'SQL server', 'AWS']",2025-06-12 05:29:20
Hiring For Azure Data Engineer with MNC client-FTE-Hyd/Bangalore/Pune,The It Mind Services,4 - 6 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","JOB DESCRIPTION:\n\n• Strong experience in Azure Datafactory,Databricks, Eventhub, Python,PySpark ,Azure Synapse and SQL\n• Azure Devops experience to deploy the ADF pipelines.\n• Knowledge/Experience with Azure cloud stack.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Azure Synapse', 'Eventhub', 'Azure Datafactory', 'Databricks', 'Python', 'PySpark', 'Azure Devops']",2025-06-12 05:29:22
Data Engineering Lead,Yotta Techports,10 - 15 years,30-35 Lacs P.A.,['Hyderabad'],"Responsibilities:\nLead and manage an offshore team of data engineers, providing strategic guidance, mentorship, and support to ensure the successful delivery of projects and the development of team members.\nCollaborate closely with onshore stakeholders to understand project requirements, allocate resources efficiently, and ensure alignment with client expectations and project timelines.\nDrive the technical design, implementation, and optimization of data pipelines, ETL processes, and data warehouses, ensuring scalability, performance, and reliability.\nDefine and enforce engineering best practices, coding standards, and data quality standards to maintain high-quality deliverables and mitigate project risks.\nStay abreast of emerging technologies and industry trends in data engineering, and provide recommendations for tooling, process improvements, and skill development.\nAssume a data architect role as needed, leading the design and implementation of data architecture solutions, data modeling, and optimization strategies.\nDemonstrate proficiency in AWS services such as:\nExpertise in cloud data services, including AWS services like Amazon Redshift, Amazon EMR, and AWS Glue, to design and implement scalable data solutions.\nExperience with cloud infrastructure services such as AWS EC2, AWS S3, to optimize data processing and storage.\nKnowledge of cloud security best practices, IAM roles, and encryption mechanisms to ensure data privacy and compliance.\nProficiency in managing or implementing cloud data warehouse solutions, including data modeling, schema design, performance tuning, and optimization techniques.\nDemonstrate proficiency in modern data platforms such as Snowflake and Databricks, including:\nDeep understanding of Snowflake's architecture, capabilities, and best practices for designing and implementing data warehouse solutions.\nHands-on experience with Databricks for data engineering, data processing, and machine learning tasks, leveraging Spark clusters for scalable data processing.\nAbility to optimize Snowflake and Databricks configurations for performance, scalability, and cost-effectiveness.\nManage the offshore team's performance, including resource allocation, performance evaluations, and professional development, to maximize team productivity and morale.\n\nQualifications:\nBachelor's degree in Computer Science, Engineering, or a related field; advanced degree preferred.\n10+ years of experience in data engineering, with a proven track record of leadership and technical expertise in managing complex data projects.\nProficiency in programming languages such as Python, Java, or Scala, as well as expertise in SQL and relational databases (e.g., PostgreSQL, MySQL).\nStrong understanding of distributed computing, cloud technologies (e.g., AWS), and big data frameworks (e.g., Hadoop, Spark).\nExperience with data architecture design, data modeling, and optimization techniques.\nExcellent communication, collaboration, and leadership skills, with the ability to effectively manage remote teams and engage with onshore stakeholders.\nProven ability to adapt to evolving project requirements and effectively prioritize tasks in a fast-paced environment.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Team Handling', 'Snowflake', 'Data Services', 'Cloud Infrastructure', 'Data Bricks']",2025-06-12 05:29:24
Senior Engineer - Data Science,Sasken Technologies,2 - 5 years,Not Disclosed,['Bengaluru'],"Job Summary\nPerson at this position has gained significant work experience to be able to apply their knowledge effectively and deliver results. Person at this position is also able to demonstrate the ability to analyse and interpret complex problems and improve change or adapt existing methods to solve the problem.\nPerson at this position regularly interacts with interfacing groups / customer on technical issue clarification and resolves the issues. Also participates actively in important project/ work related activities and contributes towards identifying important issues and risks. Reaches out for guidance and advice to ensure high quality of deliverables.\nPerson at this position consistently seek opportunities to enhance their existing skills, acquire more complex skills and work towards enhancing their proficiency level in their field of specialisation.\nWorks under limited supervision of Team Lead/ Project Manager.\n\n\nRoles & Responsibilities\nResponsible for design, coding, testing, bug fixing, documentation and technical support in the assigned area. Responsible for on time delivery while adhering to quality and productivity goals. Responsible for adhering to guidelines and checklists for all deliverable reviews, sending status report to team lead and following relevant organizational processes. Responsible for customer collaboration and interactions and support to customer queries. Expected to enhance technical capabilities by attending trainings, self-study and periodic technical assessments. Expected to participate in technical initiatives related to project and organization and deliver training as per plan and quality.\n\n\nEducation and Experience Required\nEngineering graduate, MCA, etc Experience: 2-5 years\n\nCompetencies Description\nData Science TCB is applicable to one who\n1) Analyses data to arrive at patterns/Insights/models\n2) Come up with models based on the data to provide recommendations, predictive analytics etc\n3) Provides implementation of the models in R, Matlab etc\n4) Can understand and apply machine learning/AI techniques\nPlatforms-\nUnix\nTechnology Standard-\nNA\nTools-\nR, Matlab, Spark Machine Learning, Python-ML, SPSS, SAS\nLanguages-\nR, Perl, Python, Scala\nSpecialization-\nCOGNITIVE ANALYTICS INCLUDING COMPUTER VISION, AI and ML, STATISTICS.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'R', 'SAS', 'Scala', 'Perl', 'SPSS', 'Spark', 'machine learning', 'Python']",2025-06-12 05:29:26
Snowflake Data Engineer / Database Lead,TechStar Group,9 - 14 years,15-20 Lacs P.A.,['Hyderabad'],"Job Description:\nSQL & Database Management: Deep knowledge of relational databases (PostgreSQL), cloud-hosted data platforms (AWS, Azure, GCP), and data warehouses like Snowflake.\nETL/ELT Tools: Experience with SnapLogic, StreamSets, or DBT for building and maintaining data pipelines. / ETL Tools Extensive Experience on data Pipelines\nData Modeling & Optimization: Strong understanding of data modeling, OLAP systems, query optimization, and performance tuning.\nCloud & Security: Familiarity with cloud platforms and SQL security techniques (e.g., data encryption, TDE).\nData Warehousing: Experience managing large datasets, data marts, and optimizing databases for performance.\nAgile & CI/CD: Knowledge of Agile methodologies and CI/CD automation tools.\n\n\nRole & responsibilities\nBuild the data pipeline for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and cloud database technologies.\nWork with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data needs.\nWork with data and analytics experts to strive for greater functionality in our data systems.\nAssemble large, complex data sets that meet functional / non-functional business requirements.\n– Ability to quickly analyze existing SQL code and make improvements to enhance performance, take advantage of new SQL features, close security gaps, and increase robustness and maintainability of the code.\n– Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery for greater scalability, etc.\n– Unit Test databases and perform bug fixes.\n– Develop best practices for database design and development activities.\n– Take on technical leadership responsibilities of database projects across various scrum teams.\nManage exploratory data analysis to support dashboard development (desirable)\n\n\nRequired Skills:\n– Strong experience in SQL with expertise in relational database(PostgreSQL preferrable cloud hosted in AWS/Azure/GCP) or any cloud-based Data Warehouse (like Snowflake, Azure Synapse).\n– Competence in data preparation and/or ETL/ELT tools like SnapLogic, StreamSets, DBT, etc. (preferably strong working experience in one or more) to build and maintain complex data pipelines and flows to handle large volume of data.\n– Understanding of data modelling techniques and working knowledge with OLAP systems\n– Deep knowledge of databases, data marts, data warehouse enterprise systems and handling of large datasets.\n– In-depth knowledge of ingestion techniques, data cleaning, de-dupe, etc.\n– Ability to fine tune report generating queries.\n– Solid understanding of normalization and denormalization of data, database exception handling, profiling queries, performance counters, debugging, database & query optimization techniques.\n– Understanding of index design and performance-tuning techniques\n– Familiarity with SQL security techniques such as data encryption at the column level, Transparent Data Encryption(TDE), signed stored procedures, and assignment of user permissions\n– Experience in understanding the source data from various platforms and mapping them into Entity Relationship Models (ER) for data integration and reporting(desirable).\n– Adhere to standards for all database e.g., Data Models, Data Architecture and Naming Conventions\n– Exposure to Source control like GIT, Azure DevOps\n– Understanding of Agile methodologies (Scrum, Itanban)\n– experience with NoSQL database to migrate data into other type of databases with real time replication (desirable).\n– Experience with CI/CD automation tools (desirable)\n– Programming language experience in Golang, Python, any programming language, Visualization tools (Power\nBI/Tableau) (desirable).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ETL', 'Snowflake Developer', 'SQL', 'Snowflake Sql', 'Snowflake Data Engineer', 'Snowsql', 'Snowflake', 'Schema', 'Data Engineer', 'Data Sharing', 'Snowpipe', 'Streams']",2025-06-12 05:29:28
Data Engineering Manager,Amgen Inc,3 - 5 years,Not Disclosed,['Hyderabad'],"We are seeking a seasoned Engineering Manager (Data Engineering) to lead the end-to-end management of enterprise data assets and operational data workflows. This role is critical in ensuring the availability, quality, consistency, and timeliness of data across platforms and functions, supporting analytics, reporting, compliance, and digital transformation initiatives. You will be responsible for the day-to-day data operations, manage a team of data professionals, and drive process excellence in data intake, transformation, validation, and delivery. You will work closely with cross-functional teams including data engineering, analytics, IT, governance, and business stakeholders to align operational data capabilities with enterprise needs.\nRoles & Responsibilities:\nLead and manage the enterprise data operations team, responsible for data ingestion, processing, validation, quality control, and publishing to various downstream systems.\nDefine and implement standard operating procedures for data lifecycle management, ensuring accuracy, completeness, and integrity of critical data assets.\nOversee and continuously improve daily operational workflows, including scheduling, monitoring, and troubleshooting data jobs across cloud and on-premise environments.\nEstablish and track key data operations metrics (SLAs, throughput, latency, data quality, incident resolution) and drive continuous improvements.\nPartner with data engineering and platform teams to optimize pipelines, support new data integrations, and ensure scalability and resilience of operational data flows.\nCollaborate with data governance, compliance, and security teams to maintain regulatory compliance, data privacy, and access controls.\nServe as the primary escalation point for data incidents and outages, ensuring rapid response and root cause analysis.\nBuild strong relationships with business and analytics teams to understand data consumption patterns, prioritize operational needs, and align with business objectives.\nDrive adoption of best practices for documentation, metadata, lineage, and change management across data operations processes.\nMentor and develop a high-performing team of data operations analysts and leads.\nFunctional Skills:\nMust-Have Skills:\nExperience managing a team of data engineers in biotech/pharma domain companies.\nExperience in designing and maintaining data pipelines and analytics solutions that extract, transform, and load data from multiple source systems.\nDemonstrated hands-on experience with cloud platforms (AWS) and the ability to architect cost-effective and scalable data solutions.\nExperience managing data workflows in cloud environments such as AWS, Azure, or GCP.\nStrong problem-solving skills with the ability to analyze complex data flow issues and implement sustainable solutions.\nWorking knowledge of SQL, Python, or scripting languages for process monitoring and automation.\nExperience collaborating with data engineering, analytics, IT operations, and business teams in a matrixed organization.\nFamiliarity with data governance, metadata management, access control, and regulatory requirements (e.g., GDPR, HIPAA, SOX).\nExcellent leadership, communication, and stakeholder engagement skills.\nWell versed with full stack development & DataOps automation, logging frameworks, and pipeline orchestration tools.\nStrong analytical and problem-solving skills to address complex data challenges.\nEffective communication and interpersonal skills to collaborate with cross-functional teams.\nGood-to-Have Skills:\nData Engineering Management experience in Biotech/Life Sciences/Pharma\nExperience using graph databases such as Stardog or Marklogic or Neo4J or Allegrograph, etc.\nEducation and Professional Certifications\nDoctorate Degree with 3-5 + years of experience in Computer Science, IT or related field\nOR\nMasters degree with 6 - 8 + years of experience in Computer Science, IT or related field\nOR\nBachelors degree with 10 - 12 + years of experience in Computer Science, IT or related field\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Neo4J', 'HIPAA', 'Stardog', 'Databricks', 'Marklogic', 'AWS', 'SOX', 'GDPR']",2025-06-12 05:29:31
Data Engineering Lead Microsoft Power BI,Client of Techs To Suit,8 - 12 years,25-32.5 Lacs P.A.,"['Indore', 'Hyderabad', 'Ahmedabad']","Poistion - Data Engineering Lead\nExp - 8 to 12 Years\nJob Location: Hyderabad, Ahmedabad, Indore, India.\n\nMust be Able to join in 30 days\nJob Summary:\nAs a Data Engineering Lead, your role will involve designing, developing, and implementing\ninteractive dashboards and reports using data engineering tools. You will work closely with\nstakeholders to gather requirements and translate them into effective data visualizations that\nprovide valuable insights. Additionally, you will be responsible for extracting, transforming, and\nloading data from multiple sources into Power BI, ensuring its accuracy and integrity. Your\nexpertise in Power BI and data analytics will contribute to informed decision-making and\nsupport the organization in driving data-centric strategies and initiatives.\nWe are looking for you!\nAs an ideal candidate for the Data Engineering Lead position, you embody the qualities of a\nteam player with a relentless get-it-done attitude. Your intellectual curiosity and customer\nfocus drive you to continuously seek new ways to add value to your job accomplishments. You\nthrive under pressure, maintaining a positive attitude and understanding that your career is a\njourney. You are willing to make the right choices to support your growth. In addition to your\nexcellent communication skills, both written and verbal, you have a proven ability to create\nvisually compelling designs using tools like Power BI and Tableau that effectively\ncommunicate our core values.\nYou build high-performing, scalable, enterprise-grade applications and teams. Your creativity\nand proactive nature enable you to think differently, find innovative solutions, deliver high-\nquality outputs, and ensure customers remain referenceable. With over eight years of\nexperience in data engineering, you possess a strong sense of self-motivation and take\nownership of your responsibilities. You prefer to work independently with little to no\nsupervision.\nYou are process-oriented, adopt a methodical approach, and demonstrate a quality-first\nmindset. You have led mid to large-size teams and accounts, consistently using constructive\nfeedback mechanisms to improve productivity, accountability, and performance within the\nteam. Your track record showcases your results-driven approach, as you have consistently\ndelivered successful projects with customer case studies published on public platforms.\nOverall, you possess a unique combination of skills, qualities, and experiences that make you\nan ideal fit to lead our data engineering team(s). You value inclusivity and want to join a culture\nthat empowers you to show up as your authentic self.\nYou know that success hinges on commitment, our differences make us stronger, and the\nfinish line is always sweeter when the whole team crosses together. In your role, you shouldbe driving the team using data, data, and more data. You will manage multiple teams, oversee\nagile stories and their statuses, handle escalations and mitigations, plan ahead, identify hiring\nneeds, collaborate with recruitment teams for hiring, enable sales with pre-sales teams, and\nwork closely with development managers/leads for solutioning and delivery statuses, as well\nas architects for technology research and solutions.\nWhat You Will Do:\nAnalyze Business Requirements.\nAnalyze the Data Model and do GAP analysis with Business Requirements and Power\nBI. Design and Model Power BI schema.\nTransformation of Data in Power BI/SQL/ETL Tool.\nCreate DAX Formula, Reports, and Dashboards. Able to write DAX formulas.\nExperience writing SQL Queries and stored procedures.\nDesign effective Power BI solutions based on business requirements.\nManage a team of Power BI developers and guide their work.\nIntegrate data from various sources into Power BI for analysis.\nOptimize performance of reports and dashboards for smooth usage.\nCollaborate with stakeholders to align Power BI projects with goals.\nKnowledge of Data Warehousing(must), Data Engineering is a plus",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power Bi Dashboards', 'Microsoft Power Bi', 'SQL Queries', 'Azure Databricks', 'Dax', 'Azure Data Factory']",2025-06-12 05:29:34
Data Engineering Manager,Amgen Inc,8 - 12 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\n\n\nRole Description:\n\nWe are seeking a seasoned Engineering Manager (Data Engineering) to lead the end-to-end management of enterprise data assets and operational data workflows. This role is critical in ensuring the availability, quality, consistency, and timeliness of data across platforms and functions, supporting analytics, reporting, compliance, and digital transformation initiatives. You will be responsible for the day-to-day data operations, manage a team of data professionals, and drive process excellence in data intake, transformation, validation, and delivery. You will work closely with cross-functional teams including data engineering, analytics, IT, governance, and business stakeholders to align operational data capabilities with enterprise needs.\n\nRoles & Responsibilities:\nLead and manage the enterprise data operations team, responsible for data ingestion, processing, validation, quality control, and publishing to various downstream systems.\nDefine and implement standard operating procedures for data lifecycle management, ensuring accuracy, completeness, and integrity of critical data assets.\nOversee and continuously improve daily operational workflows, including scheduling, monitoring, and troubleshooting data jobs across cloud and on-premise environments.\nEstablish and track key data operations metrics (SLAs, throughput, latency, data quality, incident resolution) and drive continuous improvements.\nPartner with data engineering and platform teams to optimize pipelines, support new data integrations, and ensure scalability and resilience of operational data flows.\nCollaborate with data governance, compliance, and security teams to maintain regulatory compliance, data privacy, and access controls.\nServe as the primary escalation point for data incidents and outages, ensuring rapid response and root cause analysis.\nBuild strong relationships with business and analytics teams to understand data consumption patterns, prioritize operational needs, and align with business objectives.\nDrive adoption of best practices for documentation, metadata, lineage, and change management across data operations processes.\nMentor and develop a high-performing team of data operations analysts and leads.\nFunctional\n\nSkills:\nMust-Have Skills:\nExperience managing a team of data engineers in biotech/pharma domain companies.\nExperience in designing and maintaining data pipelines and analytics solutions that extract, transform, and load data from multiple source systems.\nDemonstrated hands-on experience with cloud platforms (AWS) and the ability to architect cost-effective and scalable data solutions.\nExperience managing data workflows in cloud environments such as AWS, Azure, or GCP.\nStrong problem-solving skills with the ability to analyze complex data flow issues and implement sustainable solutions.\nWorking knowledge of SQL, Python, or scripting languages for process monitoring and automation.\nExperience collaborating with data engineering, analytics, IT operations, and business teams in a matrixed organization.\nFamiliarity with data governance, metadata management, access control, and regulatory requirements (e.g., GDPR, HIPAA, SOX).\nExcellent leadership, communication, and stakeholder engagement skills.\nWell versed with full stack development & DataOps automation, logging frameworks, and pipeline orchestration tools.\nStrong analytical and problem-solving skills to address complex data challenges.\nEffective communication and interpersonal skills to collaborate with cross-functional teams.\nGood-to-Have\n\nSkills:\nData Engineering Management experience in Biotech/Life Sciences/Pharma\nExperience using graph databases such as Stardog or Marklogic or Neo4J or Allegrograph, etc.\nEducation and Professional Certifications\nDoctorate Degree with 3-5 + years of experience in Computer Science, IT or related field\nOR\nMasters degree with 6 - 8 + years of experience in Computer Science, IT or related field\nOR\nBachelors degree with 10 - 12 + years of experience in Computer Science, IT or related field\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft\n\nSkills:\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'fullstack development', 'logging framework', 'stakeholder engagement', 'troubleshooting', 'cloud platforms']",2025-06-12 05:29:36
Manager Data Engineer – Research Data and Analytics,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will create and develop data lake solutions for scientific data that drive business decisions for Research. You will build scalable and high-performance data engineering solutions for large scientific datasets and collaborate with Research collaborators. You will also provide technical leadership to junior team members. The ideal candidate possesses experience in the pharmaceutical or biotech industry, demonstrates deep technical skills, is proficient with big data technologies, and has a deep understanding of data architecture and ETL processes.\nRoles & Responsibilities:\nLead, manage, and mentor a high-performing team of data engineers\nDesign, develop, and implement data pipelines, ETL processes, and data integration solutions\nTake ownership of data pipeline projects from inception to deployment, manage scope, timelines, and risks\nDevelop and maintain data models for biopharma scientific data, data dictionaries, and other documentation to ensure data accuracy and consistency\nOptimize large datasets for query performance\nCollaborate with global multi-functional teams including research scientists to understand data requirements and design solutions that meet business needs\nImplement data security and privacy measures to protect sensitive data\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions\nCollaborate with Data Architects, Business SMEs, Software Engineers and Data Scientists to design and develop end-to-end data pipelines to meet fast paced business needs across geographic regions\nIdentify and resolve data-related challenges\nAdhere to best practices for coding, testing, and designing reusable code/component\nExplore new tools and technologies that will help to improve ETL platform performance\nParticipate in sprint planning meetings and provide estimations on technical implementation\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The [vital attribute] professional we seek is a [type of person] with these qualifications.\nBasic Qualifications:\nDoctorate Degree OR\nMasters degree with 4 - 6 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nBachelors degree with 6 - 8 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nDiploma with 10 - 12 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field\nPreferred Qualifications:\n3+ years of experience in implementing and supporting biopharma scientific research data analytics (software platforms)\n\n\nFunctional Skills:\nMust-Have Skills:\nProficiency in SQL and Python for data engineering, test automation frameworks (pytest), and scripting tasks\nHands on experience with big data technologies and platforms, such as Databricks, Apache Spark (PySpark, SparkSQL), workflow orchestration, performance tuning on big data processing\nExcellent problem-solving skills and the ability to work with large, complex datasets\nAble to engage with business collaborators and mentor team to develop data pipelines and data models\n\n\nGood-to-Have Skills:\nA passion for tackling complex challenges in drug discovery with technology and data\nGood understanding of data modeling, data warehousing, and data integration concepts\nGood experience using RDBMS (e.g. Oracle, MySQL, SQL server, PostgreSQL)\nKnowledge of cloud data platforms (AWS preferred)\nExperience with data visualization tools (e.g. Dash, Plotly, Spotfire)\nExperience with diagramming and collaboration tools such as Miro, Lucidchart or similar tools for process mapping and brainstorming\nExperience writing and maintaining technical documentation in Confluence\nUnderstanding of data governance frameworks, tools, and best practices\n\n\nProfessional Certifications:\nDatabricks Certified Data Engineer Professional preferred\n\n\nSoft Skills:\nExcellent critical-thinking and problem-solving skills\nGood communication and collaboration skills\nDemonstrated awareness of how to function in a team setting\nDemonstrated presentation skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Spotfire', 'PySpark', 'PostgreSQL', 'Plotly', 'SparkSQL', 'SQL server', 'SQL', 'process mapping', 'Dash', 'MySQL', 'ETL', 'Oracle', 'data governance frameworks', 'Python']",2025-06-12 05:29:39
Senior Data Engineer,Conviction HR,8 - 10 years,Not Disclosed,"['Kolkata', 'Hyderabad', 'Pune( Malad )']","Must have -Azure Data Factory (Mandatory). Azure Databricks, Pyspark and Python and advance SQL Azure eco-system. 1) Advanced SQL Skills. 2)Data Analysis. 3) Data Models. 4) Python (Desired). 5) Automation - Experience required : 8 to 10 years.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Data Engineering', 'Python', 'Azure Databricks', 'Data Modeling', 'Data Bricks', 'SQL']",2025-06-12 05:29:41
Data Engineer with GCP,Egen (Formerly SpringML),4 - 6 years,Not Disclosed,['Hyderabad( Nanakramguda )'],"Job Overview:\n\nWe are looking for a skilled and motivated Data Engineer with strong experience in Python programming and Google Cloud Platform (GCP) to join our data engineering team. The ideal candidate will be responsible for designing, developing, and maintaining robust and scalable ETL (Extract, Transform, Load) data pipelines. The role involves working with various GCP services, implementing data ingestion and transformation logic, and ensuring data quality and consistency across systems.",,,,"['GCP', 'Python', 'Azure Data Factory', 'Cloud Functions', 'IAM', 'Bigquery', 'Snowflake', 'Google Cloud Storage', 'SQL Server', 'Oracle', 'Data Integration']",2025-06-12 05:29:44
Urgent Hiring For Data Engineer with Product based MNC Pune,Peoplefy,7 - 12 years,Not Disclosed,['Pune'],Greetings from Peoplefy Infosolutions !!!\n\nWe are hiring for one of our reputed MNC client based in Pune.\nWe are looking for candidates with 7 + years of experience in below skills -\n\nPrimary skills :\nUnderstanding of AI ML in DE\nPython\nData Engineers\nDatabase -Big query or Snowflake\n\n\nInterested candidates for above position kindly share your CVs on chitralekha.so@peoplefy.com with below details -\n\nExperience :\nCTC :\nExpected CTC :\nNotice Period :\nLocation :,Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Aiml', 'ETL', 'Python', 'Bigquery', 'AWS']",2025-06-12 05:29:46
"Senior Engineer, MS - Data Center Operations","NTT DATA, Inc.",2 - 5 years,Not Disclosed,['Mumbai'],"Additional Career Level Description:\nKnowledge and application:\nApplies learned techniques, as well as company policies and procedures to resolve a variety of issues.\nProblem solving:\nWorks on problems of moderate scope, often varied and nonroutine where analysis requires a review of a variety of factors.\nFocuses on providing standard professional advice and creating initial analysis for review.\nInteraction:\nBuilds productive internal/external working relationships to resolve mutual problems by collaborating on procedures or transactions.\nImpact:\nWork mainly impacts short term team performance and occasionally medium-term goals.\nSupports the achievement of goals through own personal effort, assessing own progress.\nAccountability:\nExercises some of own judgement and is responsible for meeting own targets, normally receiving little instruction on day-to-day work, general instructions on new assignments.\nManages own impact on cost and profitability.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Center Operations', 'system administration', 'VMware', 'technical support', 'active directory', 'windows administration', 'networking']",2025-06-12 05:29:49
"Mid Data Science & AIML, GenAI Lead/Engineer","NTT DATA, Inc.",3 - 6 years,Not Disclosed,['Bengaluru'],"Req ID: 312501\n\nWe are currently seeking a Mid Data Science & AIML, GenAI Lead/Engineer to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\nJob DutiesJob TitleData Science & AIML, GenAI Lead/Engineer\n\nKey Responsibilities:\n""¢ Develop and implement traditional machine learning algorithms.\n""¢ Deploy at least one model in a production environment.\n""¢ Write and maintain Python code for data science and machine learning projects.\n\nMinimum Skills RequiredPreferred Qualifications:\n""¢ Knowledge of Deep Learning (DL) techniques.\n""¢ Experience working with Generative AI (GenAI) and Large Language Models (LLM).\n""¢ Exposure to Langchain.\n\n""‹""‹""‹""‹""‹""‹""‹",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'artificial intelligence', 'deep learning', 'data science', 'algorithms', 'natural language processing', 'scikit-learn', 'dl', 'aiml', 'numpy', 'sql', 'tensorflow', 'r', 'predictive modeling', 'statistical modeling', 'machine learning algorithms', 'statistics']",2025-06-12 05:29:51
"Snowflake Data Engineer-Cortex AI, Aws / azure","Location - Kolkata, Hyderabad, Bangalore...",7 - 12 years,20-35 Lacs P.A.,"['Kolkata', 'Hyderabad', 'Bengaluru']","Skill set\nSnowflake, AWS, Cortex AI, Horizon Catalog\nor\nSnowflake, AWS, (Cortex AI or Horizon Catalog)\nor\nSnowflake, Azure, Cortex AI, Horizon Catalog\nOr\nSnowflake, Azure, (Cortex AI or Horizon Catalog)\nPreferred Qualifications:\nBachelors degree in Computer Science, Data Engineering, or a related field.\nExperience in data engineering, with at least 3 years of experience working with Snowflake.\nProven experience in Snowflake, Cortex AI/ Horizon Catalog focusing on data extraction, chatbot development, and Conversational AI.\nStrong proficiency in SQL, Python, and data modeling.\nExperience with data integration tools (e.g., Matillion, Talend, Informatica).\nKnowledge of cloud platforms such as AWS or Azure, or GCP.\nExcellent problem-solving skills, with a focus on data quality and performance optimization.\nStrong communication skills and the ability to work effectively in a cross-functional team.\nProficiency in using DBT's testing and documentation features to ensure the accuracy and reliability of data transformations.\nUnderstanding of data lineage and metadata management concepts, and ability to track and document data transformations using DBT's lineage capabilities.\nUnderstanding of software engineering best practices and ability to apply these principles to DBT development, including version control, code reviews, and automated testing.\nShould have experience building data ingestion pipeline.\nShould have experience with Snowflake utilities such as SnowSQL, SnowPipe, bulk copy, Snowpark, tables, Tasks, Streams, Time travel, Cloning, Optimizer, Metadata Manager, data sharing, stored procedures and UDFs, Snowsight.\nShould have good experience in implementing CDC or SCD type 2\nProficiency in working with Airflow or other workflow management tools for scheduling and managing ETL jobs.\nGood to have experience in repository tools like Github/Gitlab, Azure repo",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Kafka', 'horizon Catalog', 'cortex AI', 'Python', 'aws', 'azure']",2025-06-12 05:29:53
Software Engineer (Java + Big Data),Impetus Technologies,5 - 7 years,Not Disclosed,['Chennai'],"Job: Software Developer (Java + Big Data)\nLocation: Indore\nYears of experience: 5-7 years\n\nRequisition Description\n1. Problem solving and analytical skills\n2. Good verbal and written communication skills\n\nRoles and Responsibilities\n\n1. Design and develop high performance, scale-able applications with Java + Bigdata  as minimum required skill .\nJava, Microservices , Spring boot, API ,Bigdata-Hive, Spark, Pyspark\n2. Build and maintain efficient data pipelines to process large volumes of structured and unstructured data.\n3. Develop micro-services, API and distributed systems\n4. Worked on Spark, HDFS, CEPh, Solr/Elastic search, Kafka, Deltalake\n5. Mentor and Guide junior members",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Java', 'Hive', 'Big Data', 'Spring Boot', 'Microservices', 'Hdfs', 'Spark Streaming']",2025-06-12 05:29:56
Data Engineer (Australian Shift 5 AM To 1 PM IST),Fascave It Solutions,4 - 9 years,12-15 Lacs P.A.,['Pune'],Position: Data Engineer\nLocation: Remote\nDuration: 10-12 Months (Contract)\nExperience: 4-10 Years\nShift Time: Australian Shift (5AM TO 1 PM IST)\n\nKey Requirements\nStrong SQL skills\nSnowflake\nAzure Data Factory\nPower BI\nSSIS (Nice to have),Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Power Bi', 'Snowflake', 'SQL', 'English', 'SSIS']",2025-06-12 05:29:58
"AI/ML Engineer (Specializing in NLP/ML, Large Data Processing,",Synechron,8 - 10 years,Not Disclosed,"['Pune', 'Hinjewadi']","job requisition idJR1027361\n\nJob Summary\nSynechron seeks a highly skilled AI/ML Engineer specializing in Natural Language Processing (NLP), Large Language Models (LLMs), Foundation Models (FMs), and Generative AI (GenAI). The successful candidate will design, develop, and deploy advanced AI solutions, contributing to innovative projects that transform monolithic systems into scalable microservices integrated with leading cloud platforms such as Azure, Amazon Bedrock, and Google Gemini. This role plays a critical part in advancing Synechrons capabilities in cutting-edge AI technologies, enabling impactful business insights and product innovations.\n\nSoftware\n\nRequired Proficiency:\nPython (core librariesTensorFlow, PyTorch, Hugging Face transformers, etc.)\nCloud platformsAzure, AWS, Google Cloud (familiarity with AI/ML services)\nContainerizationDocker, Kubernetes\nVersion controlGit\nData management toolsSQL, NoSQL databases (e.g., MongoDB)\nModel deployment and MLOps toolsMLflow, CI/CD pipelines, monitoring tools\nPreferred\n\nSkills:\nExperience with cloud-native AI frameworks and SDKs\nFamiliarity with AutoML tools\nAdditional programming languages (e.g., Java, Scala)\nOverall Responsibilities\nDesign, develop, and optimize NLP models, including advanced LLMs and Foundation Models, for diverse business use cases.\nLead the development of large data pipelines for training, fine-tuning, and deploying models on big data platforms.\nArchitect, implement, and maintain scalable AI solutions in line with MLOps best practices.\nTransition legacy monolithic AI systems into modular, microservices-based architectures for scalability and maintainability.\nBuild end-to-end AI applications from scratch, including data ingestion, model training, deployment, and integration.\nImplement retrieval-augmented generation techniques for enhanced context understanding and response accuracy.\nConduct thorough testing, validation, and debugging of AI/ML models and pipelines.\nCollaborate with cross-functional teams to embed AI capabilities into customer-facing and enterprise products.\nSupport ongoing maintenance, monitoring, and scaling of deployed AI systems.\nDocument system designs, workflows, and deployment procedures for compliance and knowledge sharing.\nPerformance Outcomes:\nProduction-ready AI solutions delivering high accuracy and efficiency.\nRobust data pipelines supporting training and inference at scale.\nSeamless integration of AI models with cloud infrastructure.\nEffective collaboration leading to innovative AI product deployment.\nTechnical Skills (By Category)\n\nProgramming Languages:\nEssential: Python (TensorFlow, PyTorch, Hugging Face, etc.)\nPreferred: Java, Scala\nDatabases/Data Management:\nSQL (PostgreSQL, MySQL), NoSQL (MongoDB, DynamoDB)\nCloud Technologies:\nAzure AI, AWS SageMaker, Bedrock, Google Cloud Vertex AI, Gemini\nFrameworks and Libraries:\nTransformers, Keras, scikit-learn, XGBoost, Hugging Face engines\nDevelopment Tools & Methodologies:\nDocker, Kubernetes, Git, CI/CD pipelines (Jenkins, Azure DevOps)\nSecurity & Compliance:\nKnowledge of data security standards and privacy policies (GDPR, HIPAA as applicable)\nExperience\n8 to 10 years of hands-on experience in AI/ML development, especially NLP and Generative AI.\nDemonstrated expertise in designing, fine-tuning, and deploying LLMs, FMs, and GenAI solutions.\nProven ability to develop end-to-end AI applications within cloud environments.\nExperience transforming monolithic architectures into scalable microservices.\nStrong background with big data processing pipelines.\nPrior experience working with cloud-native AI tools and frameworks.\nIndustry experience in finance, healthcare, or technology sectors is advantageous.\nAlternative Experience:\nCandidates with extensive research or academic experience in AI/ML, especially in NLP and large-scale data processing, are eligible if they have practical deployment experience.\n\nDay-to-Day Activities\nDevelop and optimize sophisticated NLP/GenAI models fulfilling business requirements.\nLead data pipeline construction for training and inference workflows.\nCollaborate with data engineers, architects, and product teams to ensure scalable deployment.\nConduct model testing, validation, and performance tuning.\nImplement and monitor model deployment pipelines, troubleshoot issues, and improve system robustness.\nDocument models, pipelines, and deployment procedures for audit and knowledge sharing.\nStay updated with emerging AI/ML trends, integrating best practices into projects.\nPresent findings, progress updates, and technical guidance to stakeholders.\nQualifications\nBachelors degree in Computer Science, Data Science, or related field; Masters or PhD preferred.\nCertifications in AI/ML, Cloud (e.g., AWS, Azure, Google Cloud), or Data Engineering are a plus.\nProven professional experience with advanced NLP and Generative AI solutions.\nCommitment to continuous learning to keep pace with rapidly evolving AI technologies.\nProfessional Competencies\nStrong analytical and problem-solving capabilities.\nExcellent communication skills, capable of translating complex technical concepts.\nCollaborative team player with experience working across global teams.\nAdaptability to rapidly changing project scopes and emerging AI trends.\nInnovation-driven mindset with a focus on delivering impactful solutions.\nTime management skills to prioritize and manage multiple projects effectively.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'data management', 'data processing', 'pipeline', 'big data', 'continuous integration', 'kubernetes', 'deploying models', 'natural language processing', 'ci/cd', 'fms', 'artificial intelligence', 'docker', 'sql', 'microservices', 'tensorflow', 'java', 'pytorch', 'jenkins', 'keras', 'aws']",2025-06-12 05:30:01
Azure Data Engineer - Remote,Software Company,4 - 8 years,8-13 Lacs P.A.,[],"Azure Cloud Technologies, Azure Data Factory, Azure Databricks (Advance Knowledge), PySpark, CI/CD Pipeline (Jenkins, GitLab CVCD or Azure DevOps), Data Ingestion, SOL\ndesigning, developing, & optimizing scalable data solutions.\n\nRequired Candidate profile\nAzure Databricks, Azure Data Factory expertise, PySpark proficiency, Big Data CI/CD, Troubleshoot, Jenkins, Gitlab CI/ CD, Data Pipeline Development & Deployment",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['Azure Develops', 'Azure Data Engineer', 'Azure Data Factory', 'Jenkins', 'Azure Cloud Technologies', 'PySpark', 'Azure Databricks', 'GitLab CI/CD', 'Data Injection, SOL', 'GitLab CVCD']",2025-06-12 05:30:05
DBA DATA Engineer || 12 Lakhs CTC,Robotics Technologies,5 - 9 years,12 Lacs P.A.,['Hyderabad( Banjara hills )'],"Dear Candidate,\nWe are seeking a skilled and experienced DBA Data Engineer to join our growing data team. The ideal candidate will play a key role in designing, implementing, and maintaining our databases and data pipeline architecture. You will collaborate with software engineers, data analysts, and DevOps teams to ensure efficient data flow, data integrity, and optimal database performance across all systems. This role requires a strong foundation in database administration, SQL performance tuning, data modeling, and experience with both on-prem and cloud-based environments.\n\nRequirements:\nBachelors degree in Computer Science, Information Systems, or a related field.\n5+ years of experience in database administration and data engineering.\nProven expertise in RDBMS (Oracle, MySQL, and PostgreSQL) and NoSQL systems (MongoDB, Cassandra).\nExperience managing databases in cloud environments (AWS, Azure, or GCP).\nProficiency in ETL processes and tools (e.g., Apache NiFi, Talend, Informatica, AWS Glue).\nStrong experience with scripting languages such as Python, Bash, or PowerShell.\n\nDBA Data Engineer Roles & Responsibilities:\nDesign and maintain scalable and high-performance database architectures.\nMonitor and optimize database performance using tools like CloudWatch, Oracle Enterprise Manager, pgAdmin, Mongo Compass, or Dynatrace.\nDevelop and manage ETL/ELT pipelines to support business intelligence and analytics.\nEnsure data integrity and security through best practices in backup, recovery, and encryption.\nAutomate regular database maintenance tasks using scripting and scheduled jobs.\nImplement high availability, failover, and disaster recovery strategies.\nConduct performance tuning of queries, stored procedures, indexes, and table structures.\nCollaborate with DevOps to automate database deployments using CI/CD and IaC tools (e.g., Terraform, AWS CloudFormation).\nDesign and implement data models, including star/snowflake schemas for data warehousing.\nDocument data flows, data dictionaries, and database configurations.\nManage user access and security policies using IAM roles or database-native permissions.\nAnalyze existing data systems and propose modernization or migration plans (on-prem to cloud, SQL to NoSQL, etc.).\nUse AWS RDS, Amazon Redshift, Azure SQL Database, or Google BigQuery as needed.\nStay up-to-date with emerging database technologies and make recommendations.\n\nMust-Have Skills:\nDeep knowledge of SQL and database performance tuning.\nHands-on experience with database migrations and replication strategies.\nFamiliarity with data governance, data quality, and compliance frameworks (GDPR, HIPAA, etc.).\nStrong problem-solving and troubleshooting skills.\nExperience with data streaming platforms such as Apache Kafka, AWS Kinesis, or Apache Flink is a plus.\nExperience with data lake and data warehouse architectures.\nExcellent communication and documentation skills.\n\nSoft Skills:\nProblem-Solving: Ability to analyze complex problems and develop effective solutions.\nCommunication Skills: Strong verbal and written communication skills to effectively collaborate with cross-functional teams.\nAnalytical Thinking: Ability to think critically and analytically to solve technical challenges.\nTime Management: Capable of managing multiple tasks and deadlines in a fast-paced environment.\nAdaptability: Ability to quickly learn and adapt to new technologies and methodologies.\n\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database Administration', 'Database Migration', 'Dba Skills', 'High Availability', 'Db Administration', 'Hadr', 'Database Security', 'Disaster Recovery', 'Dr Testing', 'DR', 'Luw', 'Db Upgrade', 'Brtools', 'UDDI', 'Os Migration', 'Dbase', 'DBMS', 'IBM DB2', 'Db Migration', 'Disaster Recovery Planning', 'Db Dba', 'Backup And Recovery']",2025-06-12 05:30:08
Data Engineer - R&D Data Catalyst Team,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role, you will be responsible for the end-to-end development of an enterprise analytics and data mastering solution using Databricks and Power BI. This role requires expertise in both data architecture and analytics, with the ability to create scalable, reliable, and impactful enterprise solutions that research cohort-building and advanced research pipeline. The ideal candidate will have experience creating and surfacing large unified repositories of human data, based on integrations from multiple repositories and solutions, and be extraordinarily skilled with data analysis and profiling.\nYou will collaborate closely with key customers, product team members, and related IT teams, to design and implement data models, integrate data from various sources, and ensure best practices for data governance and security. The ideal candidate will have a good background in data warehousing, ETL, Databricks, Power BI, and enterprise data mastering.\nDesign and build scalable enterprise analytics solutions using Databricks, Power BI, and other modern data tools.\nLeverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation\nBreak down features into work that aligns with the architectural direction runway\nParticipate hands-on in pilots and proofs-of-concept for new patterns\nCreate robust documentation from data analysis and profiling, and proposed designs and data logic\nDevelop advanced sql queries to profile, and unify data\nDevelop data processing code in sql, along with semantic views to prepare data for reporting\nDevelop PowerBI Models and reporting packages\nDesign robust data models, and processing layers, that support both analytical processing and operational reporting needs.\nDesign and develop solutions based on best practices for data governance, security, and compliance within Databricks and Power BI environments.\nEnsure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.\nDevelop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.\nCollaborate with key customers to define data requirements, functional specifications, and project goals.\nContinuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions.\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The R&D Data Catalyst Team is responsible for building Data Searching, Cohort Building, and Knowledge Management tools that provide the Amgen scientific community with visibility to Amgens wealth of human datasets, projects and study histories, and knowledge over various scientific findings. These solutions are pivotal tools in Amgens goal to accelerate the speed of discovery, and speed to market of advanced precision medications.\nBasic Qualifications:\nMasters degree and 1 to 3 years of Data Engineering experience OR\nBachelors degree and 3 to 5 years of Data Engineering experience OR\nDiploma and 7 to 9 years of Data Engineering experience\nMust Have Skills:\nMinimum of 3 years of hands-on experience with BI solutions (Preferable Power BI or Business Objects) including report development, dashboard creation, and optimization.\nMinimum of 3 years of hands-on experience building Change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management.\nHands-on experience with Databricks, including data engineering, optimization, and analytics workloads.\nDeep understanding of Power BI, including model design, DAX, and Power Query.\nProven experience designing and implementing data mastering solutions and data governance frameworks.\nExpertise in cloud platforms (AWS), data lakes, and data warehouses.\nStrong knowledge of ETL processes, data pipelines, and integration technologies.\nGood communication and collaboration skills to work with cross-functional teams and senior leadership.\nAbility to assess business needs and design solutions that align with organizational goals.\nExceptional hands-on capabilities with data profiling, data transformation, data mastering\nSuccess in mentoring and training team members\nGood to Have Skills:\nITIL Foundation or other relevant certifications (preferred)\nSAFe Agile Practitioner (6.0)\nMicrosoft Certified: Data Analyst Associate (Power BI) or related certification.\nDatabricks Certified Professional or similar certification.\nSoft Skills:\nExcellent analytical and troubleshooting skills\nDeep intellectual curiosity\nThe highest degree of initiative and self-motivation\nStrong verbal and written communication skills, including presentation to varied audiences of complex technical/business topics\nConfidence technical leader\nAbility to work effectively with global, remote teams, specifically including using of tools and artifacts to assure clear and efficient collaboration across time zones\nAbility to handle multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong problem solving, analytical skills;\nAbility to learn quickly and retain and synthesize complex information from diverse sources.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'data management', 'Power BI', 'data governance', 'data warehousing', 'Databricks', 'ETL', 'AWS']",2025-06-12 05:30:11
Azure Data Engineers For Pune IT Companies urgent,International IT Companies,3 - 8 years,9-16 Lacs P.A.,['Pune'],"We are looking for a skilled Azure Data Engineer to design, develop, optimize data pipelines for following\n1, SQL+ETL+AZURE+Python+Pyspark+Databricks\n2, SQL+ADF+ Azure\n3, SQL+Python+Pyspark\n- Strong proficiency in SQL for data manipulation querying\n\nRequired Candidate profile\n- Python and PySpark for data engineering tasks.\n- Exp with Databricks for big data processing analytics.\n- Knowledge of data modeling, warehousing, governance.\n- CI/CD pipelines for data deployment.\n\nPerks and benefits\nPerks and Benefits",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure', 'PySpark', 'SQL', 'ETL', 'Python', 'Data Transformation', 'Business Intelligence', 'ADF', 'Data Engineering', 'Data Pipelines', 'Big Data', 'AI', 'Machine Learning', 'Analytics', 'Cloud Computing', 'Data Security', 'Databricks', 'Data Governance']",2025-06-12 05:30:14
DBA DATA Engineer || 12 Lakhs CTC,Robotics Technologies,8 - 9 years,12 Lacs P.A.,['Hyderabad( Banjara hills )'],"Dear Candidate,\nWe are seeking a skilled and experienced DBA Data Engineer to join our growing data team. The ideal candidate will play a key role in designing, implementing, and maintaining our databases and data pipeline architecture. You will collaborate with software engineers, data analysts, and DevOps teams to ensure efficient data flow, data integrity, and optimal database performance across all systems. This role requires a strong foundation in database administration, SQL performance tuning, data modeling, and experience with both on-prem and cloud-based environments.\n\nRequirements:\nBachelors degree in Computer Science, Information Systems, or a related field.\n8+ years of experience in database administration and data engineering.\nProven expertise in RDBMS (Oracle, MySQL, and PostgreSQL) and NoSQL systems (MongoDB, Cassandra).\nExperience managing databases in cloud environments (AWS, Azure, or GCP).\nProficiency in ETL processes and tools (e.g., Apache NiFi, Talend, Informatica, AWS Glue).\nStrong experience with scripting languages such as Python, Bash, or PowerShell.\n\nDBA Data Engineer Roles & Responsibilities:\nDesign and maintain scalable and high-performance database architectures.\nMonitor and optimize database performance using tools like CloudWatch, Oracle Enterprise Manager, pgAdmin, Mongo Compass, or Dynatrace.\nDevelop and manage ETL/ELT pipelines to support business intelligence and analytics.\nEnsure data integrity and security through best practices in backup, recovery, and encryption.\nAutomate regular database maintenance tasks using scripting and scheduled jobs.\nImplement high availability, failover, and disaster recovery strategies.\nConduct performance tuning of queries, stored procedures, indexes, and table structures.\nCollaborate with DevOps to automate database deployments using CI/CD and IaC tools (e.g., Terraform, AWS CloudFormation).\nDesign and implement data models, including star/snowflake schemas for data warehousing.\nDocument data flows, data dictionaries, and database configurations.\nManage user access and security policies using IAM roles or database-native permissions.\nAnalyze existing data systems and propose modernization or migration plans (on-prem to cloud, SQL to NoSQL, etc.).\nUse AWS RDS, Amazon Redshift, Azure SQL Database, or Google BigQuery as needed.\nStay up-to-date with emerging database technologies and make recommendations.\n\nMust-Have Skills:\nDeep knowledge of SQL and database performance tuning.\nHands-on experience with database migrations and replication strategies.\nFamiliarity with data governance, data quality, and compliance frameworks (GDPR, HIPAA, etc.).\nStrong problem-solving and troubleshooting skills.\nExperience with data streaming platforms such as Apache Kafka, AWS Kinesis, or Apache Flink is a plus.\nExperience with data lake and data warehouse architectures.\nExcellent communication and documentation skills.\n\nSoft Skills:\nProblem-Solving: Ability to analyze complex problems and develop effective solutions.\nCommunication Skills: Strong verbal and written communication skills to effectively collaborate with cross-functional teams.\nAnalytical Thinking: Ability to think critically and analytically to solve technical challenges.\nTime Management: Capable of managing multiple tasks and deadlines in a fast-paced environment.\nAdaptability: Ability to quickly learn and adapt to new technologies and methodologies.\n\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database Administration', 'Database Migration', 'Dba Skills', 'High Availability', 'Db Administration', 'Hadr', 'Database Security', 'Disaster Recovery', 'Dr Testing', 'DR', 'Luw', 'Db Upgrade', 'Brtools', 'UDDI', 'Os Migration', 'Dbase', 'DBMS', 'IBM DB2', 'Db Migration', 'Disaster Recovery Planning', 'Db Dba', 'Backup And Recovery']",2025-06-12 05:30:17
Senior Data Science Engineer - Computer Vision,Uplers,6 - 7 years,25-35 Lacs P.A.,['Bengaluru'],"Senior Data Science Engineer - Computer Vision\n\nExperience: 6 - 7 Years Exp\nSalary : INR 25-35 Lacs per annum\nPreferred Notice Period: Within 30 Days\nShift: 09:00AM to 6:00PM IST\nOpportunity Type: Onsite (Bengaluru)\nPlacement Type: Contractual\nContract Duration: Full-Time, Indefinite Period\n\n(*Note: This is a requirement for one of Uplers' Clients)\n\nMust have skills required :\nComputer Vision, Deep Learning, Large scale Visual Datasets, MLFlow or DVC, Prometheus Or Grafana Or Sentry, Pytorch, Cloud Server (Google / AWS), Python\nGood to have skills :\nCV/AI Research, Healthcare, or surveillance/IoT-based CV applications., Publications in CVPR/NeurIPS/ICCU, Stakeholder Communication, TensorRT, Work in domains such as retail analytics\n\nRadius AI (One of Uplers' Clients) is Looking for:\nSenior Data Science Engineer - Computer Vision who is passionate about their work, eager to learn and grow, and who is committed to delivering exceptional results. If you are a team player, with a positive attitude and a desire to make a difference, then we want to hear from you.\n\nRole Overview Description\nSenior Data Science Engineer Computer Vision\nJob Summary\nWe are seeking a highly skilled and forward-thinking Senior Data Science Engineer to lead the development of advanced computer vision models and systems. You will play a pivotal role in designing, training, and deploying deep learning models, with a strong focus on real-time inference and edge deployment.\nThe ideal candidate will bring hands-on experience with state-of-the-art architectures and have a deep understanding of the complete ML lifecycle from data acquisition to deployment at scale.\n\nKey Responsibilities\nLead the development and implementation of computer vision models for tasks such as object detection, tracking, image retrieval, and scene understanding.\nDesign and execute end-to-end pipelines for data preparation, model training, evaluation, and deployment.\n¢ Perform fine-tuning and transfer learning on large-scale vision-language models to meet application-specific needs.\n¢ Optimize deep learning models for edge inference (NVIDIA Jetson, TensorRT, OpenVINO) and real-time performance.\n¢ Develop scalable and maintainable ML pipelines using tools such as MLflow, DVC, and Kubeflow.\n¢ Automate experimentation and deployment processes using CI/CD workflows.\n¢ Collaborate cross-functionally with MLOps, backend, and product teams to align technical efforts with business needs.\n¢ Monitor, debug, and enhance model performance in production environments.\n¢ Stay up-to-date with the latest trends in CV/AI research and rapidly prototype new ideas for real-world use.\n\nRequired Qualifications\n¢ 67+ years of hands-on experience in data science and machine learning, with at least 4 years focused on computer vision.\n¢ Strong experience with deep learning frameworks: PyTorch (preferred), TensorFlow, Hugging Face Transformers.\n¢ In-depth understanding and practical experience with Class-incremental learning and lifelong learning systems\n¢ Proficient in Python, including data processing libraries like NumPy, Pandas, and OpenCV.\n¢ Strong command of version control and reproducibility tools (e.g., MLflow, DVC, Weights & Biases).\n¢ Experience with training and optimizing models for GPU inference and edge deployment (Jetson, Coral, etc.).\n¢ Familiarity with ONNX, TensorRT, and model quantization/conversion techniques.\n¢ Demonstrated ability to analyze and work with large-scale visual datasets in real-time or near-real-time systems.\n\nPreferred Qualifications\n¢ Experience working in fast-paced startup environments with ownership of production AI systems.\n¢ Exposure to cloud platforms such as AWS (SageMaker, Lambda), GCP, or Azure for ML workflows.\n¢ Experience with video analytics, real-time inference, and event-based vision systems. ¢ Familiarity with monitoring tools for ML systems (e.g., Prometheus, Grafana, Sentry).\n\nEasy 3-Step Process:\n1. Click On Apply! And Register or log in on our portal\n2. Upload updated Resume & Complete the Screening Form\n3. Increase your chances to get shortlisted & meet the client for the Interview!\n\nAbout Our Client:\nRadiusAI is a pioneering computer vision analytics company revolutionizing retail operations with advanced, human-centric AI solutions. We offer the world's most advanced VisionAI checkout and we provide real-time data to improve operational efficiency across the entire retail industry, focusing on enterprise-level customers and secure edge integration\n\nAbout Uplers:\nOur goal is to make hiring and getting hired reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant product and engineering job opportunities and progress in their career.\n\n(Note: There are many more opportunities apart from this on the portal.)\n\nSo, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you!",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer Vision', 'Deep Learning', 'Pytorch', 'Large scale Visual Datasets', 'MLFlow or DVC', 'Prometheus Or Grafana Or Sentry']",2025-06-12 05:30:19
MDM Associate Data Engineer,Amgen Inc,1 - 4 years,Not Disclosed,['Hyderabad'],"We are seeking an MDM Associate Data Engineerwith 25 years of experience to support and enhance our enterprise MDM (Master Data Management) platforms using Informatica/Reltio. This role is critical in delivering high-quality master data solutions across the organization, utilizing modern tools like Databricks and AWS to drive insights and ensure data reliability. The ideal candidate will have strong SQL, data profiling, and experience working with cross-functional teams in a pharma environment.To succeed in this role, the candidate must have strong data engineering experience along with MDM knowledge, hence the candidates having only MDM experience are not eligible for this role. Candidate must have data engineering experience on technologies like (SQL, Python, PySpark, Databricks, AWS etc), along with knowledge of MDM (Master Data Management)\nRoles & Responsibilities:\nAnalyze and manage customer master data using Reltio or Informatica MDM solutions.\nPerform advanced SQL queries and data analysis to validate and ensure master data integrity.\nLeverage Python, PySpark, and Databricks for scalable data processing and automation.\nCollaborate with business and data engineering teams for continuous improvement in MDM solutions.\nImplement data stewardship processes and workflows, including approval and DCR mechanisms.\nUtilize AWS cloud services for data storage and compute processes related to MDM.\nContribute to metadata and data modeling activities.\nTrack and manage data issues using tools such as JIRA and document processes in Confluence.\nApply Life Sciences/Pharma industry context to ensure data standards and compliance.\nBasic Qualifications and Experience:\nMasters degree with 1 - 3 years of experience in Business, Engineering, IT or related field OR\nBachelors degree with 2 - 5 years of experience in Business, Engineering, IT or related field OR\nDiploma with 6 - 8 years of experience in Business, Engineering, IT or related field\nFunctional Skills:\nMust-Have Skills:\nAdvanced SQL expertise and data wrangling.\nStrong experience in Python and PySpark for data transformation workflows.\nStrong experience with Databricks and AWS architecture.\nMust have knowledge of MDM, data governance, stewardship, and profiling practices.\nIn addition to above, candidates having experience with Informatica or Reltio MDM platforms will be preferred.\nGood-to-Have Skills:\nExperience with IDQ, data modeling and approval workflow/DCR.\nBackground in Life Sciences/Pharma industries.\nFamiliarity with project tools like JIRA and Confluence.\nStrong grip on data engineering concepts.\nProfessional Certifications:\nAny ETL certification (e.g. Informatica)\nAny Data Analysis certification (SQL, Python, Databricks)\nAny cloud certification (AWS or AZURE)\nSoft Skills:\nStrong analytical abilities to assess and improve master data processes and solutions.\nExcellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders.\nEffective problem-solving skills to address data-related issues and implement scalable solutions.\nAbility to work effectively with global, virtual teams",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MDM', 'PySpark', 'AWS architecture', 'Jira', 'Reltio', 'SQL', 'Informatica MDM', 'data modeling', 'Confluence', 'IDQ', 'Databricks', 'data stewardship processes', 'Python']",2025-06-12 05:30:22
Openings For Data Engineer,Creative Solutions,5 - 10 years,4-9 Lacs P.A.,['Hyderabad'],"Skills: ADF,ETL,Rest API\nMicrosoft Power Platform / Snowflake Certification is a Plus\nPower Bi / Talend and Integration\nPower Apps\nSAP S4 Hana Abap,Odata Rest and Soap\nJob LOcation : Hyderabad",Industry Type: Software Product,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['ADF', 'power APPS', 'Azure Data Factory', 'Power Bi', 'Sap S4 Hana', 'ETL DEveloper', 'Talend']",2025-06-12 05:30:24
Assoc. Data Engineer - R&D Precision Medicine Team,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\n\n\nThe R&D Precision Medicine team is responsible for Data Standardization, Data Searching, Cohort Building, and Knowledge Management tools that provide the Amgen scientific community with access to Amgens wealth of human datasets, projects and study histories, and knowledge over various scientific findings. These data include clinical data, omics, and images. These solutions are pivotal tools in Amgens goal to accelerate the speed of discovery, and speed to market of advanced precision medications.\n\nThe Data Engineer will be responsible for full stack development of enterprise analytics and data mastering solutions leveraging Databricks and Power BI. This role requires expertise in both data architecture and analytics, with the ability to create scalable, reliable, and high-performing enterprise solutions that support research cohort-building and advanced AI pipelines. The ideal candidate will have experience creating and surfacing large unified repositories of human data, based on integrations from multiple repositories and solutions, and be exceptionally skilled with data analysis and profiling.\n\nYou will collaborate closely with partners, product team members, and related IT teams, to design and implement data models, integrate data from various sources, and ensure best practices for data governance and security. The ideal candidate will have a solid background in data warehousing, ETL, Databricks, Power BI, and enterprise data mastering.\n\nRoles & Responsibilities\nDesign and build scalable enterprise analytics solutions using Databricks, Power BI, and other modern data management tools.\nLeverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation\nBreak down features into work that aligns with the architectural direction runway\nParticipate hands-on in pilots and proofs-of-concept for new patterns\nCreate robust documentation from data analysis and profiling, and proposed designs and data logic\nDevelop advanced sql queries to profile, and unify data\nDevelop data processing code in sql, along with semantic views to prepare data for reporting\nDevelop PowerBI Models and reporting packages\nDesign robust data models, and processing layers, that support both analytical processing and operational reporting needs.\nDesign and develop solutions based on best practices for data governance, security, and compliance within Databricks and Power BI environments.\nEnsure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.\nDevelop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.\nCollaborate with partners to define data requirements, functional specifications, and project goals.\nContinuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions.\n\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The professional we seek is someone with these qualifications.\n\nBasic Qualifications:\nMasters degree with 1 to 3 years of experience in Data Engineering OR\nBachelors degree with 1 to 3 years of experience in Data Engineering\nMust-Have\n\nSkills:\nMinimum of 1 year of hands-on experience with BI solutions (Preferrable Power BI or Business Objects) including report development, dashboard creation, and optimization.\nMinimum of 1 year of hands-on experience building Change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management.\nHands-on experience with Databricks, including data engineering, optimization, and analytics workloads.\nExperience using cloud platforms (AWS), data lakes, and data warehouses.\nWorking knowledge of ETL processes, data pipelines, and integration technologies.\nGood communication and collaboration skills to work with cross-functional teams and senior leadership.\nAbility to assess business needs and design solutions that align with organizational goals.\nExceptional hands-on capabilities with data profiling and data anlysis\nGood-to-Have\n\nSkills:\nExperience with human data, ideally human healthcare data\nFamiliarity with laboratory testing, patient data from clinical care, HL7, FHIR, and/or clinical trial data management\nProfessional Certifications:\nITIL Foundation or other relevant certifications (preferred)\nSAFe Agile Practitioner (6.0)\nMicrosoft CertifiedData Analyst Associate (Power BI) or related certification.\nDatabricks Certified Professional or similar certification.\nSoft\n\nSkills:\nExcellent analytical and troubleshooting skills\nDeep intellectual curiosity\nHighest degree of initiative and self-motivation\nStrong verbal and written communication skills, including presentation to varied audiences of complex technical/business topics\nConfidence technical leader\nAbility to work effectively with global, virtual teams, specifically including using of tools and artifacts to assure clear and efficient collaboration across time zones\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong problem solving, analytical skills;\nAbility to learn quickly and retain and synthesize complex information from diverse sources",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'data lakes', 'data pipelines', 'ETL processes', 'AWS', 'data warehouses', 'BI solutions']",2025-06-12 05:30:27
Python Developer/ Python Data Engineer (Python+ ETL+ Pandas),Atyeti,5 - 8 years,Not Disclosed,[],"Role & responsibilities\nB.Tech or M.Tech in Computer Science, or equivalent experience.\n5+ years of experience working professionally as a Python Software Developer.\nOrganized, self-directed, and resourceful.\nExcellent written and verbal communication skills.\nExpert in python & pandas.\nExperience in building data pipelines, ETL and ELT processes.",,,,"['Pandas', 'ETL', 'Python', 'SQL']",2025-06-12 05:30:30
CMMS Data Engineer,Idexcel,3 - 6 years,Not Disclosed,['Vadodara'],"Role & responsibilities\n\nProfessional Summary:\nExperience: Extensive experience in major oil & gas industry equipment including pumps, compressors, turbines, piping, storage tanks, and vessels. Proficient in understanding master data and Bill of Materials (BoM) structures.\nKnowledge: In-depth knowledge of equipment classification, codes, and characteristics. Skilled in reviewing materials classification, codes, and characteristics.",,,,"['SAP PM', 'Material Management', 'SAP MM', 'Data Management', 'CMMS', 'SAP', 'MDM', 'Master Data Management']",2025-06-12 05:30:32
Data Engineer (AWS Databricks),Fortune India 500 IT Services Firm,5 - 7 years,15-22.5 Lacs P.A.,['Chennai'],"Role & responsibilities :\n\nJob Description:\n\nPrimarily looking for a Data Engineer (AWS) with expertise in processing data pipelines using Data bricks, PySpark SQL on Cloud distributions like AWS\nMust have AWS Data bricks ,Good-to-have PySpark, Snowflake, Talend\n\nRequirements-\n\n• Candidate must be experienced working in projects involving\n• Other ideal qualifications include experiences in\n• Primarily looking for a data engineer with expertise in processing data pipelines using Databricks Spark SQL on Hadoop distributions like AWS EMR Data\nbricks Cloudera etc.\n• Should be very proficient in doing large scale data operations using Databricks and overall very comfortable using Python\n• Familiarity with AWS compute storage and IAM concepts\n• Experience in working with S3 Data Lake as the storage tier\n\n• Any ETL background Talend AWS Glue etc. is a plus but not required\n• Cloud Warehouse experience Snowflake etc. is a huge plus\n• Carefully evaluates alternative risks and solutions before taking action.\n• Optimizes the use of all available resources\n• Develops solutions to meet business needs that reflect a clear understanding of the objectives practices and procedures of the corporation department and business unit\n\n• Skills\n• Hands on experience on Databricks Spark SQL AWS Cloud platform especially S3 EMR Databricks Cloudera etc.\n• Experience on Shell scripting\n• Exceptionally strong analytical and problem-solving skills\n• Relevant experience with ETL methods and with retrieving data from dimensional data models and data warehouses\n• Strong experience with relational databases and data access methods especially SQL\n• Excellent collaboration and cross functional leadership skills\n• Excellent communication skills both written and verbal\n• Ability to manage multiple initiatives and priorities in a fast-paced collaborative environment\n• Ability to leverage data assets to respond to complex questions that require timely answers\n• has working knowledge on migrating relational and dimensional databases on AWS Cloud platform\nSkills\nMandatory Skills: Apache Spark, Databricks, Java, Python, Scala, Spark SQL.\n\n\nNote : Need only Immediate joiners/ Serving notice period.\n\nInterested candidates can apply.\n\nRegards,\nHR Manager",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineer', 'Aws Databricks', 'SQL', 'Data Engineering', 'Azure Databricks', 'ETL', 'Talend', 'Python']",2025-06-12 05:30:35
Azure Data Engineer,Meritus Management Service,4 - 7 years,10-20 Lacs P.A.,['Pune'],"Experience in designing, developing, implementing, and optimizing data solutions on Microsoft Azure. Proven expertise in leveraging Azure services for ETL processes, data warehousing and analytics, ensuring optimal performance and scalability.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Pipeline', 'Synapse Analytics', 'Azure Databricks', 'Spark', 'Python', 'Pyspark', 'Ci/Cd', 'azure']",2025-06-12 05:30:37
Snowflake Data Engineer,Epam Systems,5 - 10 years,Not Disclosed,['Chennai'],"Key Skills:\nSnowflake (Snow SQL, Snow PLSQL and Snowpark)\nStrong Python\nAirflow/DBT\nAny DevOps tools\nAWS/Azure Cloud Skills\n\nRequirements:\nLooking for engineer for information warehouse\nWarehouse is based on AWS/Azure, DBT, Snowflake.\nStrong programming experience with Python.\nExperience with workflow management tools like Argo/Oozie/Airflow.\nExperience in Snowflake modelling - roles, schema, databases\nExperience in data Modeling (Data Vault).\nExperience in design and development of data transformation pipelines using the DBT framework.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Python', 'Azure Cloud', 'AWS', 'SQL']",2025-06-12 05:30:40
Lead Data Engineer ( Exp: 6+ Years ),Atyeti,6 - 11 years,Not Disclosed,"['Hyderabad', 'Pune']",Job Description :\n\nStrong experience on Python programming.\nExperience on Databricks.\nExperience on Database like SQL\nPerform database performance tuning and optimization. Databricks Platform\nWork with Databricks platform for big data processing and analytics.,,,,"['Pyspark', 'ETL', 'Data Bricks', 'Python', 'SQL']",2025-06-12 05:30:42
Lead Engineer - Data Science,Sasken Technologies,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Summary\nPerson at this position takes ownership of a module and associated quality and delivery. Person at this position provides instructions, guidance and advice to team members to ensure quality and on time delivery.\nPerson at this position is expected to be able to instruct and review the quality of work done by technical staff.\nPerson at this position should be able to identify key issues and challenges by themselves, prioritize the tasks and deliver results with minimal direction and supervision.\nPerson at this position has the ability to investigate the root cause of the problem and come up alternatives/ solutions based on sound technical foundation gained through in-depth knowledge of technology, standards, tools and processes.\nPerson has the ability to organize and draw connections among ideas and distinguish between those which are implementable.\nPerson demonstrates a degree of flexibility in resolving problems/ issues that atleast to in-depth command of all techniques, processes, tools and standards within the relevant field of specialisation.\n\n\nRoles & Responsibilities\nResponsible for requirement analysis and feasibility study including system level work estimation while considering risk identification and mitigation.\nResponsible for design, coding, testing, bug fixing, documentation and technical support in the assigned area. Responsible for on time delivery while adhering to quality and productivity goals.\nResponsible for traceability of the requirements from design to delivery Code optimization and coverage.\nResponsible for conducting reviews, identifying risks and ownership of quality of deliverables.\nResponsible for identifying training needs of the team.\nExpected to enhance technical capabilities by attending trainings, self-study and periodic technical assessments.\nExpected to participate in technical initiatives related to project and organization and deliver training as per plan and quality.\nExpected to be a technical mentor for junior members.\nPerson may be given additional responsibility of managing people based on discretion of Project Manager.\n\nEducation and Experience Required\nEngineering graduate, MCA, etc Experience: 5-8 years\n\n\nCompetencies Description\nData Science TCB is applicable to one who\n1) Analyses data to arrive at patterns/Insights/models\n2) Come up with models based on the data to provide recommendations, predictive analytics etc\n3) Provides implementation of the models in R, Matlab etc\n4) Can understand and apply machine learning/AI techniques\nPlatforms-\nUnix\nTools-\nR, Matlab, Spark Machine Learning, Python-ML, SPSS, SAS\nLanguages-\nR, Perl, Python, Scala\nSpecialization-\nCOGNITIVE ANALYTICS INCLUDING COMPUTER VISION, AI and ML, STATISTICS",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Unix', 'R', 'SAS', 'Scala', 'Perl', 'SPSS', 'Machine Learning', 'Python']",2025-06-12 05:30:45
Gcp Data Engineer,One Of the MNC company,6 - 8 years,Not Disclosed,"['Hyderabad', 'Pune', 'Mumbai (All Areas)']","Role & responsibilities\n\nDevelop, implement, and optimize ETL/ELT pipelines for processing large datasets efficiently.\n• Work extensively with BigQuery for data processing, querying, and optimization.\n• Utilize Cloud Storage, Cloud Logging, Dataproc, and Pub/Sub for data ingestion, storage, and event-driven processing.\n• Perform performance tuning and testing of the ELT platform to ensure high efficiency and scalability.\n• Debug technical issues, perform root cause analysis, and provide solutions for production incidents.\n• Ensure data quality, accuracy, and integrity across data pipelines.\n• Collaborate with cross-functional teams to define technical requirements and deliver solutions.\n• Work independently on assigned tasks while maintaining high levels of productivity and efficiency.\nSkills Required:\n• Proficiency in SQL and PL/SQL for querying and manipulating data.\n• Experience in Python for data processing and automation.\n• Hands-on experience with Google Cloud Platform (GCP), particularly:\no BigQuery (must-have)\no Cloud Storage\no Cloud Logging\no Dataproc\no Pub/Sub\n• Experience with GitHub and CI/CD pipelines for automation and deployment.\n• Performance tuning and performance testing of ELT processes.\n• Strong analytical and debugging skills to resolve data and pipeline issues efficiently.\n• Self-motivated and able to work independently as an individual contributor.\n• Good understanding of data modeling, database design, and data warehousing concepts.\n\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GCP', 'Bigquery', 'ETL', 'PL/SQL', 'SQL']",2025-06-12 05:30:48
Azure Data Engineer (Azure Databricks must),Fortune India 500 IT Services Firm,5 - 8 years,Not Disclosed,['Hyderabad'],"We are looking for an experienced Azure Data Engineer with strong expertise in Azure Databricks to join our data engineering team.\n\nMandatory skill- Azure Databricks\nExperience- 5 to 8 years\nLocation- Hyderabad\nKey Responsibilities:\nDesign and build data pipelines and ETL/ELT workflows using Azure Databricks and Azure Data Factory\nIngest, clean, transform, and process large datasets from diverse sources (structured and unstructured)\nImplement Delta Lake solutions and optimize Spark jobs for performance and reliability\nIntegrate Azure Databricks with other Azure services including Data Lake Storage, Synapse Analytics, and Event Hubs\n\n\n\nInterested candidates share your CV at himani.girnar@alikethoughts.com with below details\n\nCandidate's name-\nEmail and Alternate Email ID-\nContact and Alternate Contact no-\nTotal exp-\nRelevant experience-\nCurrent Org-\nNotice period-\nCCTC-\nECTC-\nCurrent Location-\nPreferred Location-\nPancard No-",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Azure Data Factory', 'Pyspark', 'Azure Data Lake', 'SQL']",2025-06-12 05:30:51
ML Engineer/Data Scientist,Altimetrik,6 - 8 years,15-30 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities\nData Scientist /ML engineers : ML Engineer with Python, SQL, Machine Learning, Azure skills(Good to have)",Industry Type: IT Services & Consulting,,,"['Machine Learning', 'Python', 'SQL', 'Data Science', 'Ml', 'azure']",2025-06-12 05:30:53
Data Scientist - L3,Wipro,3 - 5 years,Not Disclosed,['Ramdurg'],"Role Purpose\nThe purpose of the role is to define, architect and lead delivery of machine learning and AI solutions.\nDo\n1. Demand generation through support in Solution development\na. Support Go-To-Market strategy\ni. Contribute to development solutions, proof of concepts aligned to key offerings to enable solution led sales\nb. Collaborate with different colleges and institutes for research initiatives and provide data science courses\n2. Revenue generation through Building & operationalizing Machine Learning, Deep Learning solutions\na. Develop Machine Learning / Deep learning models for decision augmentation or for automation solutions\nb. Collaborate with ML Engineers, Data engineers and IT to evaluate ML deployment options\n3. Team Management\na. Talent Management\ni. Support on boarding and training to enhance capability & effectiveness\nDeliver\n\nNo.Performance ParameterMeasure\n1.Demand generation# PoC supported\n2.Revenue generation through deliveryTimeliness, customer success stories, customer use cases\n3.Capability Building & Team Management# Skills acquired\n\n\nMandatory Skills: Data Analysis.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'python', 'team management', 'natural language processing', 'scikit-learn', 'ml deployment', 'machine learning', 'data engineering', 'artificial intelligence', 'sql', 'deep learning', 'tensorflow', 'data science', 'predictive modeling', 'statistical modeling', 'ml']",2025-06-12 05:30:56
QA Engineer ( Data and Functional Testing),worxogo,2 - 5 years,Not Disclosed,[],"Responsibilities:\nPerform manual testing of web and mobile applications to ensure quality and performance.\nConduct various types of testing such as Functional, Usability, Integration, and GUI testing.\nDesign, develop, and execute comprehensive test cases and test scenarios based on product requirements.\nValidate data accuracy and consistency through extensive MySQL queries and checks.\nWork with large datasets and perform data validation using MS Excel.Collaborate with developers, product managers, and other QA team members to ensure timely delivery of high-quality features.\nUnderstand and follow the complete software testing lifecycle, including test planning, test execution, defect tracking, and reporting.\nUtilize tools like JIRA and Confluence for test management and documentation.\nContribute to automation testing efforts using Python, including scripting and executing automated test cases.\nParticipate in team discussions to improve product quality and testing processes.\nMaintain clear and effective communication across technical and non-technical teams.\nRequirements:\nMinimum 2 years of experience in Data Testing and Manual Testing.\nStrong knowledge of Manual Testing methodologies, techniques, and best practices.\nHands-on experience with MySQL and writing complex queries.\nProficiency in MS Excel for data validation and analysis.\nGood understanding of testing documentation like test plans, test cases, and test scenarios.\nExperience working with JIRA and Confluence is a plus.\nAbility to work independently and in a team-oriented, collaborative environment.\nWorking knowledge of Python and experience in developing automation test scripts.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Testing', 'Functional Testing', 'Automation', 'GUI Testing', 'Usability Testing', 'MySQL', 'Integration Testing', 'Manual Testing', 'Test Cases', 'Python']",2025-06-12 05:30:58
Data Analyst,"NTT DATA, Inc.",3 - 8 years,Not Disclosed,['Pune'],"Req ID: 324676\n\nWe are currently seeking a Data Analyst to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nKey Responsibilities:\n\nExtract, transform, and load (ETL) data from various sources, ensuring data quality, integrity, and accuracy.\n\nPerform data cleansing, validation, and preprocessing to prepare structured and unstructured data for analysis.\n\nDevelop and execute queries, scripts, and data manipulation tasks using SQL, Python, or other relevant tools.\n\nAnalyze large datasets to identify trends, patterns, and correlations, drawing meaningful conclusions that inform business decisions.\n\nCreate clear and concise data visualizations, dashboards, and reports to communicate findings effectively to stakeholders.\n\nCollaborate with clients and cross-functional teams to gather and understand data requirements, translating them into actionable insights.\n\nWork closely with other departments to support their data needs.\n\nCollaborate with Data Scientists and other analysts to support predictive modeling, machine learning, and statistical analysis.\n\nContinuously monitor data quality and proactively identify anomalies or discrepancies, recommending corrective actions.\n\nStay up-to-date with industry trends, emerging technologies, and best practices to enhance analytical techniques.\n\nAssist in the identification and implementation of process improvements to streamline data workflows and analysis.\n\nBasic Qualifications:\n\n3 + years of proficiency in data analysis tools such as [Tools - e.g., Excel, SQL, R, Python].\n\n3+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\n\nUndergraduate or Graduate degree preferred\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nStrong proficiency in data analysis tools such as Python, SQL, Talend (any ETL).\n\nExperience with data visualization tools like PowerBI.\n\nExperience with cloud data platforms .\n\nFamiliarity with ETL (Extract, Transform, Load) processes and tools.\n\nKnowledge of machine learning techniques and tools.\n\nExperience in a specific industry (e.g., financial services, healthcare, manufacturing) can be a plus.\n\nUnderstanding of data governance and data privacy regulations.\n\nAbility to query and manipulate databases and data warehouses.\n\nExcellent analytical and problem-solving skills.\n\nStrong communication skills with the ability to explain complex data insights to non-technical stakeholders.\n\nDetail-oriented with a commitment to accuracy.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analysis', 'data analytics', 'data engineering', 'analysis tools', 'software engineering', 'python', 'data manipulation', 'talend', 'power bi', 'data warehousing', 'machine learning', 'dashboards', 'sql', 'data cleansing', 'data quality', 'r', 'predictive modeling', 'data visualization', 'etl']",2025-06-12 05:31:00
"Delivery Head - Infrastructure Engineering, Data Center",Bajaj Allianz General Insurance Company Limited,15 - 20 years,Not Disclosed,['Pune'],"The role requires strong leadership, strategic thinking, and the ability to drive innovation and efficiency within the technology department. It demands extensive experience in leading complex data center infrastructures, focusing on servers, SAN storage, high availability, disaster recovery, and hybrid environments, including data center operations and physical servers (blade and rack). Responsibilities include designing and testing backup strategies, maintaining documentation, ensuring compliance with regulations, and conducting product and vendor evaluations. Collaboration with various IT teams, the security team, and business stakeholders is essential\n",,,,"['process setting', 'document management', 'vmware', 'center', 'microsoft azure', 'itil service management', 'storage', 'shuttering', 'analysis', 'problem management', 'change management', 'cloud', 'data center', 'operations', 'service delivery', 'incident management', 'leadership', 'it infrastructure management', 'itil']",2025-06-12 05:31:03
Big Data Lead,IQVIA,8 - 13 years,25-40 Lacs P.A.,['Bengaluru'],"Job Title / Primary Skill: Big Data Developer (Lead/Associate Manager)\nManagement Level: G150\nYears of Experience: 8 to 13 years\nJob Location: Bangalore (Hybrid)\nMust Have Skills: Big data, Spark, Scala, SQL, Hadoop Ecosystem.\nEducational Qualification: BE/BTech/ MTech/ MCA, Bachelor or masters degree in Computer Science,\n\nJob Overview\nOverall Experience 8+ years in IT, Software Engineering or relevant discipline.\nDesigns, develops, implements, and updates software systems in accordance with the needs of the organization.\nEvaluates, schedules, and resources development projects; investigates user needs; and documents, tests, and maintains computer programs.\nJob Description:\nWe look for developers to have good knowledge of Scala programming skills and Knowledge of SQL\nTechnical Skills:\nScala, Python -> Scala is often used for Hadoop-based projects, while Python and Scala are choices for Apache Spark-based projects.\nSQL -> Knowledge of SQL (Structured Query Language) is important for querying and manipulating data\nShell Script -> Shell scripts are used for batch processing of data, it can be used for scheduling the jobs and shell scripts are often used for deploying applications\nSpark Scala -> Spark Scala allows you to write Spark applications using the Spark API in Scala\nSpark SQL -> It allows to work with structured data using SQL-like queries and Data Frame APIs.\nWe can execute SQL queries against Data Frames, enabling easy data exploration, transformation, and analysis.\n\nThe typical tasks and responsibilities of a Big Data Developer include:\n1. Data Ingestion: Collecting and importing data from various sources, such as databases, logs, APIs into the Big Data infrastructure.\n2. Data Processing: Designing data pipelines to clean, transform, and prepare raw data for analysis. This often involves using technologies like Apache Hadoop, Apache Spark.\n3. Data Storage: Selecting appropriate data storage technologies like Hadoop Distributed File System (HDFS), HIVE, IMPALA, or cloud-based storage solutions (Snowflake, Databricks).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SCALA', 'Big Data', 'Spark', 'Hive', 'Apache Pig', 'Hadoop', 'Hadoop Development', 'Mapreduce', 'Hdfs', 'Impala', 'YARN']",2025-06-12 05:31:05
Big Data Developer-STG(P),Infosys,5 - 10 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities\n\nA day in the life of an Infoscion\nAs part of the Infosys delivery team, your primary role would be to ensure effective Design, Development, Validation and Support activities, to assure that our clients are satisfied with the high levels of service in the technology domain.\nYou will gather the requirements and specifications to understand the client requirements in a detailed manner and translate the same into system requirements.\nYou will play a key role in the overall estimation of work requirements to provide the right information on project estimations to Technology Leads and Project Managers.\nYou would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\nIf you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\n\nPrimary skills:Technology->Functional Programming->Scala\n\nAdditional information(Optional)\nKnowledge of design principles and fundamentals of architecture\nUnderstanding of performance engineering\nKnowledge of quality processes and estimation techniques\nBasic understanding of project domain\nAbility to translate functional / nonfunctional requirements to systems requirements\nAbility to design and code complex programs\nAbility to write test cases and scenarios based on the specifications\nGood understanding of SDLC and agile methodologies\nAwareness of latest technologies and trends\nLogical thinking and problem solving skills along with an ability to collaborate\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SCALA', 'Big Data', 'Spark', 'Hive', 'Kafka']",2025-06-12 05:31:08
Engineer - Investment Data Platform - 3+ Years - Pune,Crescendo Global,3 - 6 years,Not Disclosed,['Pune'],"Engineer - Investment Data Platform - 3+ Years - Pune\n\nWe are hiring a skilled Engineer to join the Investment Data Platform in Pune in Financial services. If you're passionate about data software and engineering and delivering high-quality software solutions using Azure and .Net technologies, this opportunity is for you.\n\nLocation: Pune\n\nYour Future Employer: Our client is a leading financial services firm with a global presence. They are committed to creating an inclusive and diverse workplace where all employees feel valued and have the opportunity to reach their full potential.\n\nResponsibilities:\nDeveloping and maintain software solutions aligned with business outcomes.\nCollaborating within agile teams to review user stories and implement features.\nMaintaining existing data platform artefacts and contribute to continuous improvement.\nBuilding scalable, robust software adhering to data engineering best practices.\nSupporting development of data ingestion, modeling, transformation, and deployment pipelines.\n\nRequirements:\n3+ years of experience in software engineering and 2+ years in data engineering.\nProficiency in C#, .Net Framework, SQL; exposure to Python, Java, PowerShell, or JavaScript.\nExperience with Azure Data Factory, CI/CD pipelines, and DevOps principles.\nStrong interpersonal and communication skills.\nBachelor's degree in computer science, engineering, finance or related field\n\nWhat is in it for you:\nJoin a high-performing team at a global investment leader\nExposure to cutting-edge Azure data platform technologies\nCompetitive compensation with hybrid work flexibility\n\nReach us: If you think this role is aligned with your career, kindly write to me along with your updated CV at aayushi.goyal@crescendogroup.in for a confidential discussion on the role.\n\nDisclaimer: Crescendo Global specializes in Senior to C-level niche recruitment. We are passionate about empowering job seekers and employers with an engaging and memorable job search and leadership hiring experience. Crescendo Global does not discriminate based on race, religion, color, origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\n\nNote: We receive a lot of applications daily, so it may not be possible to respond to each one individually. Please assume that your profile has not been shortlisted if you don't hear from us in a week. Thank you for your understanding.\n\nScammers can misuse Crescendo Globals name for fake job offers. We never ask for money, purchases, or system upgrades. Verify all opportunities at www.crescendo-global.com and report fraud immediately. Stay alert!\n\nProfile Keywords: Azure Data Engineering, C# Developer, .Net Engineer, SQL Data Engineer, DevOps Data, Databricks Jobs, Data Ingestion Engineer, Financial Services Tech Jobs, Asset Management IT, Financial Services",Industry Type: Financial Services (Asset Management),Department: Other,"Employment Type: Full Time, Permanent","['C#', 'dot net', 'SQL', 'Financial markets', 'asset management process', 'Azure Databricks', 'Azure DevOps']",2025-06-12 05:31:11
Big Data Developer - N,Infosys,5 - 10 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Job description\n\nHiring for Bigdata Developer with experience range 5 to 15 years.\n\nMandatory Skills: Bigdata, Scala, Spark, Hive, Kafka\n\nEducation: BE/B.Tech/MCA/M.Tech/MSc./MSts",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hive', 'SCALA', 'Big Data', 'Kafka', 'Spark', 'Bigdata Technologies']",2025-06-12 05:31:13
Cloud Data Support Streaming Engineer,Infogrowth,10 - 13 years,16-18 Lacs P.A.,['Bengaluru'],"Looking for a Cloud Data Support Streaming Engineer with 8+ years of experience in Azure Data Lake, Databricks, PySpark, and Python. Role includes monitoring, troubleshooting, and support for streaming data pipelines.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Pyspark', 'Azure Data Lake', 'Azure SQL', 'Data Bricks', 'Python', 'Azure Data Factory', 'Application Support', 'Data Streaming', 'API', 'SSIS', 'Monitoring', 'Batch Processing']",2025-06-12 05:31:15
"Sr Validation Engineer, R&D Data Catalyst Team",Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"Role Description:\nThe Validation and Testing Leadis responsible forleading the testing activities for software applications and solutions that meet business needs and ensuring the availability of critical systems and applications. This roleis for a lead tester with experience testing data management solutions and data analytics products, and with experience with designing and executing testing for GxP validated systems. The role involves working closely with product managers, designers, and other engineers to test high-quality, scalable software solutions.\nRoles & Responsibilities:\nParticipate inrequirementdiscussionsin order to create test scripts.\nBuild test scriptsper implementation project plan by working with various members of the product team and business partners.\nConduct informal and formal testing, consolidate all the findings and coordinate with developer(s) and business partners to resolve all the issues\nPerform regression testing to verify the changes do not negatively impact existing system functionality\nCommunicate potential risks and contingency plans with project management to ensure process compliance with all regulatory and Amgen procedural requirements\nIdentify and resolve technical challenges/bugs effectively\nWork closely with cross-functional teams, including product management, design, and QA, to deliver high-quality software on time\nSupport the creating and implementation of automated testing frameworks to improve efficiency and consistency\nBasic Qualifications and Experience:\nMasters degree with 4 - 6 years of experience in Computer Science, IT or related field OR\nBachelors degree with 6 - 8 years of experience in Computer Science, IT or related field\nDiploma and 10 to 12 years of experience in Computer Science, IT or related field\nFunctional Skills:\nMust-Have Skills:\nKnowledge of GxPsoftware validation processes\nExperience with test management in Jiraand test automation practices\nExpert sql skills for data profiling and creation of test data\nStrong experience testing analytics and data platforms\nGood Problem-solving skills - Identifying and fixing bugs, adapting to changes\nExcellent communication skills - Explaining design decisions, collaborating with teams\nExperienced in Agile methodology\nGood-to-Have Skills:\nExperience in Risk-based Approach to Change Management of Validated GxP Systems\nExperience with cloud-based technologies\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nSAFE for Teams certification (Preferred)\nSoft Skills:\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Validation engineering', 'project management', 'data analytics', 'data validation', 'troubleshooting', 'agile', 'change management', 'sql', 'data profiling', 'jira']",2025-06-12 05:31:18
Senior Engineering Manager - Data Operations,Amgen Inc,12 - 15 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nWe are seeking a seasoned Senior Engineering Manager(Data Engineering) to lead the end-to-end management of enterprise data assets and operational data workflows. This role is critical in ensuring the availability, quality, consistency, and timeliness of data across platforms and functions, supporting analytics, reporting, compliance, and digital transformation initiatives.As a senior leader in the data organization, you will oversee the day-to-day data operations, manage a team of data professionals, and drive process excellence in data intake, transformation, validation, and delivery. You will work closely with cross-functional teams including data engineering, analytics, IT, governance, and business stakeholders to align operational data capabilities with enterprise needs.\nRoles & Responsibilities:\nLead and manage the enterprise data operations team, responsible for data ingestion, processing, validation, quality control, and publishing to various downstream systems.\nDefine and implement standard operating procedures for data lifecycle management, ensuring availability, accuracy, completeness, and integrity of critical data assets.\nOversee and continuously improve daily operational workflows, including scheduling, monitoring, and troubleshooting data jobs across cloud and on-premise environments.\nEstablish and track key data operations metrics (SLAs, throughput, latency, data quality, incident resolution) and drive continuous improvements.\nPartner with data engineering and platform teams to optimize pipelines, support new data integrations, and ensure scalability and resilience of operational data flows.\nCollaborate with data governance, compliance, and security teams to maintain regulatory compliance, data privacy, and access controls.\nServe as the primary escalation point for data incidents and outages, ensuring rapid response and root cause analysis.\nBuild strong relationships with business and analytics teams to understand data consumption patterns, prioritize operational needs, and align with business objectives.\nDrive adoption of best practices for documentation, metadata, lineage, and change management across data operations processes.\nMentor and develop a high-performing team of data operations analysts and leads.\nFunctional Skills:\nMust-Have Skills:\nExperience managing a team of data engineers in biotech/pharmadomain companies.\nExperience in designing and maintainingdata pipelines and analytics solutions that extract, transform, and load data from multiple source systems.\nDemonstrated hands-on experience with cloud platforms (AWS) and the ability to architect cost-effective and scalable data solutions.\nExperience managing data workflows on Databricks in cloud environments such as AWS, Azure, or GCP.\nStrong problem-solving skills with the ability to analyze complex data flow issues and implement sustainable solutions.\nWorking knowledge of SQL, Python, PySparkor scripting languages for process monitoring and automation.\nExperience collaborating with data engineering, analytics, IT operations, and business teams in a matrixed organization.\nFamiliarity with data governance, metadata management, access control, and regulatory requirements (e.g., GDPR, HIPAA, SOX).\nExcellent leadership, communication, and stakeholder engagement skills.\nWell versed with full stack development& DataOps automation, logging & observability frameworks, and pipeline orchestration tools.\nStrong analytical and problem-solving skills to address complex data challenges.\nEffective communication and interpersonal skills to collaborate with cross-functional teams.\nGood-to-Have Skills:\nData Engineering Management experience in Biotech/Life Sciences/Pharma\nExperience using graph databases such as Stardog or Marklogic or Neo4J or Allegrograph, etc.\nEducation and Professional Certifications\n12 to 15 years of experience in Computer Science, IT or related field\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nExperience in life sciences, healthcare, or other regulated industries with large-scale operational data environments.\nFamiliarity with incident and change management processes (e.g., ITIL).\nSoft Skills:\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Operations', 'Azure', 'Data Engineering', 'Neo4J', 'GCP', 'Engineering Management', 'troubleshooting', 'Stardog', 'Marklogic', 'AWS']",2025-06-12 05:31:20
Software Engineering Manager - Research Data and Analytics,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nWe are seeking a Manager - Software Engineering to lead the design and development of software data and analytics applications to drive business decisions for Research. The ideal candidate possesses a deep understanding of software engineering principles, coupled with strong leadership and problem-solving skills. This position requires close collaboration with business analysts, scientists, and other engineers to create high-quality, scalable software solutions. You will continuously strive for innovation in the technologies and practices used for software engineering and develop a team of expert software engineers. You will collaborate with multi-functional teams, including, platform, functional IT, and business collaborators, to ensure that the solutions that are built align with business goals and are scalable, secure, and efficient.\nRoles & Responsibilities:\nTalent Growth & People Leadership: Lead, mentor, and manage a high-performing team of engineers, fostering an environment that encourages learning, collaboration, and innovation. Focus on nurturing future leaders and providing growth opportunities through coaching, training, and mentorship.\nPartner closely with product team owners, the business team including scientists, and other collaborators to lead the software engineering solutioning, ensuring deliverables are completed on time and within scope to deliver real value and meet business objectives\nDesign, develop, and implement applications and modules, including custom reports, interfaces, and enhancements\nAnalyze and understand the functional and technical requirements of applications, solutions and systems and translate them into software architecture and design specifications\nDevelop and implement unit tests, integration tests, and other testing strategies to ensure the quality of the software\nWork closely with product team, business team including scientists, and other collaborators\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients.\nBasic Qualifications:\nMasters degree with 4 - 6 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nBachelors degree with 6 - 8 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nDiploma with 10 - 12 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field\nExcellent leadership and project management skills, with the ability to manage multiple priorities simultaneously\nProficient in General Purpose High Level Language (e.g. NodeJS/Koa, Python or Java)\nProficient in Javascript UI Framework (e.g. React or ExtJs)\nProficient in SQL (e.g. Oracle, PostGres or Databricks)\nPreferred Qualifications:\n3+ years of experience in implementing and supporting custom software development for drug discovery\nStrong understanding of cloud platforms (e.g AWS) and containerization technologies (e.g., Docker, Kubernetes)\nExperience in automated testing tools and frameworks (e.g. Jest, Playwright, Cypress or Selenium)\nExperience with DevOps practices and CI/CD pipelines\nExperience with big data technologies (e.g., Spark, Databricks)\nExperienced with API integration, serverless, microservices architecture\nExperience with monitoring and logging tools (e.g., Prometheus, Grafana, Splunk)\nExperience of infrastructure as code (IaC) tools (Terraform, CloudFormation)\nExperience with version control systems like Git\nStrong understanding of software development methodologies, mainly Agile and Scrum\nStrong problem solving, analytical skills; Ability to learn quickly & work independently; Excellent communication and interpersonal skills\nProfessional Certifications\nAWS Certified Cloud Practitioner preferred\nSoft Skills:\nExcellent critical-thinking and problem-solving skills\nStrong communication and collaboration skills\nDemonstrated awareness of how to function in a team setting\nDemonstrated presentation skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Software Engineering', 'DevOps', 'microservices architecture', 'Spark', 'Databricks', 'API integration', 'AWS']",2025-06-12 05:31:22
Data Loss Prevention Engineer,Credent Infotech Solutions Llp,2 - 3 years,Not Disclosed,['Mumbai( kandivali )'],"Daily Monitoring and Investigation\n\nMonitor DLP alerts across email, endpoint, web, and cloud.\nPerform triage to determine false positives, true positives, and actual incidents.\nDocument findings and escalate critical violations per SOPs.\nIncident Response Support\n\nSupport incident response by providing evidence, logs, and context around DLP policy violations.\nCoordinate with IT, HR, and Legal teams for user engagement, awareness, and disciplinary action if necessary.\nParticipate in Root Cause Analysis (RCA) for recurring or high-severity incidents.\nPolicy Tuning and Optimization\n\nAnalyse alert trends and false positive patterns to suggest and implement policy refinements.\nWork with business and security teams to validate policy changes and test updated rulesets before production deployment.\nMaintain documentation of policy changes, rationales, and approvals.\nLifecycle Management\n\nSupport onboarding business units, or geographies into DLP coverage.\nMaintain and update DLP dashboards and reporting structures.\nStakeholder Communication\n\nProvide regular reports to CISO on DLP violations\nInterface with Data Owners, Business Units, and Compliance teams for policy alignment and exception management.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Endpoint Protection', 'Incident Response', 'Symantec Dlp', 'SIEM', 'Data Classification', 'Risk Assessment', 'Compliance', 'Powershell', 'Information Security', 'Dlp', 'Casb Netskope', 'Data Protection Manager', 'Microsoft Purview']",2025-06-12 05:31:25
Data Analytics Engineer,Automotive Industry,5 - 6 years,Not Disclosed,['Chennai'],"Position: Data Analytics Engineer\nExp: 5 -6 years\nNP: Immediate - 30 days\nQualification: B.tech\nLocation: Chennai Hybrid\nPrimary: Google Cloud Platform ,Python skills and Big Data pipeline\nSecondary: Big Query SQ, coding, testing, implementing, debugging workflows and apps\nKindly share your updated resume to aishwarya_s@onwardgroup.com\nKindly fill the below details\nTotal Exp:\nRelevant Exp:\nNotice Period: CTC:\nECTC:\nIf servicing NP, Last working Day, offered location & CTC:\nAvailable for Video modes interview on Weekdays (Y/N) :\nPAN Number:\nName as Per PAN Card:\nDate of Birth:\nAlternative Contact No:\nReason for Job Change:",Industry Type: Automobile,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['GCP', 'Bigquery', 'Google Cloud Platform', 'Query SQ', 'Python']",2025-06-12 05:31:27
Senior Associate Data Security Engineer,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role the Senior Associate Data Security Engineer role will cover Data Loss Prevention (DLP) and Data Security Posture Management (DSPM) technologies. This role will report to the Manager, Data Security. This position will provide essential services that enable us to better pursue our mission.\nSr. Associate Data Security Engineers operate, manage, and improve Amgens DLP and DSPM solutions. In our Data Security team, they will operate data protection security technologies in a rapidly changing global security sector. They will work with other engineers and business units to help craft, build, coordinate, configure, and implement critical preventive and detective security controls related to the protection of Amgen data.\nThis engineer will play a key role in designing, deploying, and maintaining solutions to build our rapidly growing operations.\nRoles & Responsibilities:\nMaintain the service delivery and working order of Amgen data security solutions across Amgens global enterprise\nExecute Amgen service management processes such as Incident Management, change processes, and service improvements for Amgens data security technologies\nAssist in the design and improvement of Amgens data security technologies and solutions. Build scripts for the configuration and the testing of the solution\nManage and perform analysis of escalated DLP events, engage with the business, fulfill legal hold requests, and provide executive reporting\nWork with business domain specialists to collect, analyze, build, tune and automate DLP policy sets\nAnalyze events and logs for suspicious activity and opportunities to improve posture, processes, procedures, and protections.\nConsult to the Incident Response team on investigations\nDevelop automation solutions in increase response times and reduce risk of identified incidents\nParticipate in regular meetings and conference calls with the client, IT, business partners and vendors to help ensure technical coverage for new or existing projects across the business\n\n\nFunctional Skills:\nMust-Have Skills:\nKnowledge of Cloud Access Security Platforms (Elastica, Netskope, SkyHigh, etc)\nUnderstanding of cloud and SAAS environments (AWS, O365, Box, Salesforce, etc)\nSolid experience with potential to grow knowledge in Linux/Windows OS and other infrastructure systems\nExperience with DLP and data protection technologies for a large global enterprise\nDemonstrated understanding on how emerging security technologies and data flows interoperate across complex, multi-cloud systems.\n\nBasic Qualifications:\nMasters degree and 1 to 3 years of experience OR\nBachelors degree and 3 to 5 years of experience OR\nDiploma and 7 to 9 years of experience.\nPreferred Qualifications:\nGood-to-Have Skills:\nComfort with scripting (PowerShell, Python, etc) and expression development (SQL, Regex)\nAbility to develop documentation for Infrastructure Security implementations\nBasic experience with ITIL processes such as Incident, Problem, and configuration management\nExperience in complex enterprise environments with competing business priorities\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nSystems Security Certified Practitioner (SSCP) or Security+\nSANS Certifications\nRelevant vendor-specific certifications\n\n\nSoft Skills:\nEstablished analytical and gap/fit assessment skills.\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals\nEffective presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Security', 'PowerShell', 'configuration management', 'Cloud Access Security', 'Linux', 'Incident management', 'ITIL', 'Python', 'SQL']",2025-06-12 05:31:29
Engineer- Data Support,Powerica,3 - 5 years,Not Disclosed,['Mumbai'],Job profile\na. Co-ordinate with project team\nb. Upload BVQ in oracle system\nc. BOQ revision and making entries.\nd. Data Accuracy\ne. Process management.\n\nDiploma in Engineering\nExp 3-5 Years’ experience.,Industry Type: Power,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Boq Preparation', 'Oracle', 'BVQ']",2025-06-12 05:31:31
Specialist Data Security Engineer,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role the Specialist Data Security Engineer covering Data Loss Prevention (DLP) and Cloud Access Security Broker (CASB) technologies. This role will report to the Manager, Data Security. This position will provide essential services that enable us to better pursue our mission.\nSpecialist Data Security Engineers operate, manage, and improve Amgens DLP and Cloud Access Security Broker (CASB) solutions. In our Data Security team, they will identify emerging risks related to changes in cloud technologies, advise management, and develop technical remediations to address those risks. Specialists lead the development of processes and procedures for multiple solutions which enable business units to remediate identify cloud data exposures. They run multiple projects simultaneously to implement and improve the cloud data security protection and use advanced analytics to demonstrate success.\nThis engineer will play a key role in educating and evangelizing to technologists and business leaders the security strategies that both protect and enable business processes related to cloud data handling.\nRoles & Responsibilities:\nDesigns, operates, maintains, and enhances capabilities for the technical systems that ensure protection of data for all Amgen global operations.\nIdentifies new risk areas for data and plans controls to mitigate those risks.\nResearches new technologies, processes, and approaches based on industry practices and recommends future plans for data protection.\nAuthors procedures and guidelines and advises on policies related to data protection requirements and remediation or investigation of violations.\nDevelops and conducts training on data protection technologies for operations staff. Educates business leadership about data risk.\nAdvises other technology groups on data protection strategies and recommends appropriate points of both technical and process integration.\nPartners with the Manager of Data Security to liaise to legal and human resources leadership on violation remediations.\nCollaborates with cloud strategy leaders and business unit leadership to ensure that cloud data protection is incorporated by design into new business projects.\nCollaborates with Cloud Security Engineers to integrate cloud data protection technology into the operations of traditional Data Loss Prevention operations.\n\nBasic Qualifications:\nMasters degree and 4 to 6 years of experience OR\nBachelors degree and 6 to 8 years of experience OR\nDiploma and 10 to 12 years of experience.\nFunctional Skills:\nMust-Have Skills\nFamiliarity with one or more security frameworks, especially in regulated environments.\nProficiency specifying requirements for technical systems, as well as designing, implementing, and operating those systems.\nExpertise in global IT operations, including an understanding of regulatory and cultural differences encountered when dealing with international peers and customers.\nDemonstrated competence maintaining applications on Windows and Linux based operating systems, and basic understanding of one or more programming or scripting languages.\nDemonstrated proficiency with one or more Cloud Access Security Platforms (Elastica, Netskope, SkyHigh,etc)\nTrack record of project management leadership, preferably using Agile methodology.\nDeep knowledge of the principles of Data Protection, including availability, integrity, and confidentiality of data.\n\n\nGood-to-Have Skills:\nProficiency with communications focused on both the development of written technical processes and the ability to convey complex ideas clearly in front of an audience.\nExperience with data analytics focused on building executive reports\nReputation of successfully navigating large enterprise environments, understanding both ITIL driven processes and business relationship building\nAbility to self-direct work on multiple priorities with little to no oversight, based on critical initiatives.\nProfessional Certifications (please mention if the certification is preferred or required for the role):\nSystems Security Certified Practitioner (SSCP) or Security+\nSANS Certifications\nCloud security certifications\nRelevant vendor-specific certifications\n\n\nSoft Skills:\nEstablished analytical and gap/fit assessment skills.\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals\nEffective presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Security Engineering', 'SkyHigh', 'Data Protection', 'Cloud Access Security Platforms', 'Netskope', 'security frameworks', 'Agile methodology', 'IT operations', 'ITIL', 'Elastica']",2025-06-12 05:31:34
MDM Data Platform Engineer,TechAffinity,5 - 8 years,Not Disclosed,['Chennai'],"Key Responsibilities:\nTechnical Skills:\nStrong proficiency in SQL for data manipulation and querying.\nKnowledge of Python scripting for data processing and automation.\nExperience in Reltio Integration Hub (RIH) and handling API-based integrations.\nFamiliarity with Data Modelling Matching, Survivorship concepts and methodologies.\nExperience with D&B, ZoomInfo, and Salesforce connectors for data enrichment.\nUnderstanding of MDM workflow configurations and role-based data governance\nExperience with AWS Databricks, Data Lake and Warehouse\nImplement and configure MDM solutions using Reltio while ensuring alignment with business requirements and best practices.\nDevelop and maintain data models, workflows, and business rules within the MDM platform.\nWork on Reltio Workflow (DCR Workflow & Custom Workflow) to manage data approvals and role-based assignments.\nSupport data integration efforts using Reltio Integration Hub (RIH) to facilitate data movement across multiple systems.\nDevelop ETL pipelines using SQL, Python, and integration tools to extract, transform, and load (ETL) data.\nWork with D&B, ZoomInfo, and Salesforce connectors for data enrichment and integration.\nPerform data analysis and profiling to identify data quality issues and recommend solutions for data cleansing and enrichment.\nCollaborate with stakeholders to define and document data governance policies, procedures, and standards.\nOptimize MDM workflows to enhance data stewardship and governance.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Master Data Management', 'Data Lake', 'Aws Databricks', 'SQL', 'Python', 'Reltio Integration Hub', 'MDM', 'Data Warehousing', 'Data Governance', 'Data Modelling Matching']",2025-06-12 05:31:37
Senior Web Crawler & Data Extraction Engineer,Netgraph Networking,5 - 10 years,Not Disclosed,[],"Summary\n\nTo enhance user profiling and risk assessment, we are building web crawlers to collect relevant user data from third-party sources, forums, and the dark web. We are seeking a Senior Web Crawler & Data Extraction Engineer to design and implement these data collection solutions.\n\nJob Responsibilities\n\nDesign, develop, and maintain web crawlers and scrapers to extract data from open web sources, forums, marketplaces, and the dark web.\nImplement data extraction pipelines that aggregate, clean, and structure data for fraud detection and risk profiling.\nUse Tor, VPNs, and other anonymization techniques to safely crawl the dark web while avoiding detection.\nDevelop real-time monitoring solutions for tracking fraudulent activities, data breaches, and cybercrime discussions.\nOptimize crawling speed and ensure compliance with website terms of service, ethical standards, and legal frameworks.\nIntegrate extracted data with fraud detection models, risk scoring algorithms, and cybersecurity intelligence tools.\nWork with data scientists and security analysts to develop threat intelligence dashboards from collected data.\nImplement anti-bot detection evasion techniques and handle CAPTCHAs using AI-driven solvers where necessary.\nStay updated on OSINT (Open-Source Intelligence) techniques, web scraping best practices, and cybersecurity trends.\n\nRequirements\n5+ years of experience in web crawling, data scraping, or cybersecurity data extraction.\nStrong proficiency in Python, Scrapy, Selenium, BeautifulSoup, Puppeteer, or similar frameworks.\nExperience working with Tor, proxies, and VPNs for anonymous web scraping.\nDeep understanding of HTTP protocols, web security, and bot detection mechanisms.\nExperience parsing structured and unstructured data from JSON, XML, and web pages.\nStrong knowledge of database management (SQL, NoSQL) for storing large-scale crawled data.\nFamiliarity with AI/ML-based fraud detection techniques and data classification methods.\nExperience working with cybersecurity intelligence sources, dark web monitoring, and OSINT tools.\nAbility to implement scalable, distributed web crawling architectures.\nKnowledge of data privacy regulations (GDPR, CCPA) and ethical data collection practices.\n\nNice to Have\nExperience in fintech, fraud detection, or threat intelligence.\nKnowledge of natural language processing (NLP) for analyzing cybercrime discussions.\nFamiliarity with machine learning-driven anomaly detection for fraud prevention.\nHands-on experience with cloud-based big data solutions (AWS, GCP, Azure, Elasticsearch, Kafka).",Industry Type: FinTech / Payments,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Scrapy', 'Web Crawling', 'Data Extraction', 'Web Scraping', 'Python', 'Automation', 'BeautifulSoup', 'Data Scraping', 'Selenium', 'Python Development']",2025-06-12 05:31:39
"Data Analyst, Staff",Qualcomm,4 - 7 years,Not Disclosed,['Bengaluru'],"Job Area: Miscellaneous Group, Miscellaneous Group > Data Analyst\n \n\nQualcomm Overview: \nQualcomm is a company of inventors that unlocked 5G ushering in an age of rapid acceleration in connectivity and new possibilities that will transform industries, create jobs, and enrich lives. But this is just the beginning. It takes inventive minds with diverse skills, backgrounds, and cultures to transform 5Gs potential into world-changing technologies and products. This is the Invention Age - and this is where you come in.\n\nGeneral Summary:\n\nAbout the Team\n\nQualcomm's People Analytics team plays a crucial role in transforming data into strategic workforce insights that drive HR and business decisions. As part of this lean but high-impact team, you will have the opportunity to analyze workforce trends, ensure data accuracy, and collaborate with key stakeholders to enhance our data ecosystem. This role is ideal for a generalist who thrives in a fast-paced, evolving environment""”someone who can independently conduct data analyses, communicate insights effectively, and work cross-functionally to enhance our People Analytics infrastructure.\n\nWhy Join Us\n\n\nEnd-to-End ImpactWork on the full analytics cycle""”from data extraction to insight generation""”driving meaningful HR and business decisions.\n\n\nCollaboration at ScalePartner with HR leaders, IT, and other analysts to ensure seamless data integration and analytics excellence.\n\n\nData-Driven CultureBe a key player in refining our data lake, ensuring data integrity, and influencing data governance efforts.\n\n\nProfessional GrowthGain exposure to multiple areas of people analytics, including analytics, storytelling, and stakeholder engagement.\n\n\nKey Responsibilities\n\n\nPeople Analytics & Insights\nAnalyze HR and workforce data to identify trends, generate insights, and provide recommendations to business and HR leaders.\nDevelop thoughtful insights to support ongoing HR and business decision-making.\nPresent findings in a clear and compelling way to stakeholders at various levels, including senior leadership.\n\n\nData Quality & Governance\nEnsure accuracy, consistency, and completeness of data when pulling from the data lake and other sources.\nIdentify and troubleshoot data inconsistencies, collaborating with IT and other teams to resolve issues.\nDocument and maintain data definitions, sources, and reporting standards to drive consistency across analytics initiatives.\n\n\nCollaboration & Stakeholder Management\nWork closely with other analysts on the team to align methodologies, share best practices, and enhance analytical capabilities.\nAct as a bridge between People Analytics, HR, and IT teams to define and communicate data requirements.\nPartner with IT and data engineering teams to improve data infrastructure and expand available datasets.\n\n\nQualifications\n\nRequired4-7 years experience in a People Analytics focused role\n\n\nAnalytical & Technical Skills\nStrong ability to analyze, interpret, and visualize HR and workforce data to drive insights.\nExperience working with large datasets and ensuring data integrity.\nProficiency in Excel and at least one data visualization tool (e.g., Tableau, Power BI).\n\n\nCommunication & Stakeholder Management\nAbility to communicate data insights effectively to both technical and non-technical audiences.\nStrong documentation skills to define and communicate data requirements clearly.\nExperience collaborating with cross-functional teams, including HR, IT, and business stakeholders.\n\n\nPreferred:\n\n\nTechnical Proficiency\nExperience with SQL, Python, or R for data manipulation and analysis.\nFamiliarity with HR systems (e.g., Workday) and cloud-based data platforms.\n\n\nPeople Analytics Expertise\nPrior experience in HR analytics, workforce planning, or related fields.\nUnderstanding of key HR metrics and workforce trends (e.g., turnover, engagement, diversity analytics).\n\n\nAdditional Information\nThis is an office-based position (4 days a week onsite) with possible locations that may include India and Mexico",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['sql', 'people analytics', 'documentation', 'tableau', 'data integration tools', 'hiring', 'data warehousing', 'data architecture', 'sourcing', 'jquery', 'staffing', 'plsql', 'oracle 10g', 'java', 'etl tool', 'html', 'etl', 'mongodb', 'python', 'oracle', 'power bi', 'hrsd', 'r', 'node.js', 'hr analytics', 'angularjs']",2025-06-12 05:31:42
Big Data Engineer_ Info Edge _Noida,Info Edge,1 - 4 years,14-19 Lacs P.A.,['Noida'],"About Info Edge India Ltd.\nInfo Edge: Info Edge (India) Limited (NSE: NAUKRI) is among the leading internet companies in India. Info Edge, Indias premier online classifieds company is fundamentally in the matching business. With a network of 62 offices located in 43 cities throughout India, Info Edge has 5000 plus employees engaged in innovation, product development, integration with mobile and social media, technology and technology updation, research and development, quality assurance, sales, marketing and payment collection.\nThe umbrella brand has an online recruitment classifieds, www.naukri.com– India’s No. 1 Jobsite with over 75% traffic share, a matrimony classifieds, www.jeevansathi.com, a real estate classifieds, www.99acres.com– India’s largest property marketplace and an education classifieds, www.shiksha.com. Find out more about the Company at",,,,"['Data Modeling', 'Python', 'SCALA']",2025-06-12 05:31:45
Data Analyst - L3,Wipro,3 - 5 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of this role is to interpret data and turn into information (reports, dashboards, interactive visualizations etc) which can offer ways to improve a business, thus affecting business decisions.\nDo\n1. Managing the technical scope of the project in line with the requirements at all stages\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\nb. Develop record management process and policies\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\nd. Providing sales data, proposals, data insights and account reviews to the client base\ne. Identify areas to increase efficiency and automation of processes\nf. Set up and maintain automated data processes\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\nh. Produce and track key performance indicators\n2. Analyze the data sets and provide adequate information\na. Liaise with internal and external clients to fully understand data content\nb. Design and carry out surveys and analyze survey data as per the customer requirement\nc. Analyze and interpret complex data sets relating to customers business and prepare reports for internal and external audiences using business analytics reporting tools\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\nf. Develop predictive models and share insights with the clients as per their requirement\n\nDeliver\n\nNo Performance Parameter Measure\n1. Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\nMandatory Skills: Tableau.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'Tableau', 'data warehouses', 'data integration', 'Data modelling']",2025-06-12 05:31:48
Data Scientist - L3,Wipro,3 - 5 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of the role is to define, architect and lead delivery of machine learning and AI solutions.\n\n\n\nDo\n1. Demand generation through support in Solution development\na. Support Go-To-Market strategy\ni. Contribute to development solutions, proof of concepts aligned to key offerings to enable solution led sales\nb. Collaborate with different colleges and institutes for research initiatives and provide data science courses\n2. Revenue generation through Building & operationalizing Machine Learning, Deep Learning solutions\na. Develop Machine Learning / Deep learning models for decision augmentation or for automation solutions\nb. Collaborate with ML Engineers, Data engineers and IT to evaluate ML deployment options\n3. Team Management\na. Talent Management\ni. Support on boarding and training to enhance capability & effectiveness\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure\n1.Demand generation# PoC supported\n2.Revenue generation through deliveryTimeliness, customer success stories, customer use cases\n3.Capability Building & Team Management# Skills acquired\n\n\n\n\n\n\nMandatory Skills: Data Analysis.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'ML deployment', 'Deep learning models', 'Solution development', 'Talent Management', 'Machine Learning']",2025-06-12 05:31:51
Data Bricks,PwC India,7 - 12 years,Not Disclosed,['Bengaluru'],"Job Summary:\n\nWe are seeking a talented Data Engineer with strong expertise in Databricks, specifically in Unity Catalog, PySpark, and SQL, to join our data team. Youll play a key role in building secure, scalable data pipelines and implementing robust data governance strategies using Unity Catalog.\n\nKey Responsibilities:",,,,"['DataBricks', 'Data Bricks', 'Pyspark', 'Delta Lake', 'Databricks Engineer', 'Unity Catalog', 'SQL']",2025-06-12 05:31:53
Junior Fullstack Engineer,Data Core Systems,1 - 2 years,Not Disclosed,['Kolkata'],Required\n• Degree in Computer Science or equivalent practical experience\n• 1+ year of experience developing with Python and React/TypeScript\n• Experience working in a team development environment (not just personal projects)\n• Basic understanding of web development\n\nPreferred\n• Experience working in a startup environment\n• Understanding of the full product development lifecycle\n• Ability to read technical documents in English\n• Contributions to open source projects,Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['React', 'Web Development', 'AWS', 'Python', 'Rest Web Api', 'HTML']",2025-06-12 05:31:55
Data Governance Engineers,Meritus Management Service,4 - 9 years,14-17 Lacs P.A.,"['Pune', 'Gurugram']","Define, implement, & enforce data governance policies & standards to ensure data quality, consistency, & compliance across the organization\nCollaborate with data stewards, business users, & IT teams to maintain metadata, lineage, & data catalog tools",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Stewardship', 'Metadata', 'Data Governance', 'Metadata Management', 'Data Lineage', 'Data Modeling', 'SQL']",2025-06-12 05:31:57
Data Analyst - Gurugram,Infosys,5 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru', 'PAN INDIA']","Responsibilities:\nUnderstand architecture requirements and ensure effective design, development, validation, and support activities.\nAnalyze user requirements, envisioning system features and functionality.\nIdentify bottlenecks and bugs, and recommend system solutions by comparing advantages and disadvantages of custom development.\nContribute to team meetings, troubleshooting development and production problems across multiple environments and operating platforms.\nEnsure effective design, development, validation, and support activities for Big Data solutions.\nTechnical and Professional Requirements:\nSkills:\nProficiency in Scala, Spark, Hive, and Kafka.\nIn-depth knowledge of design issues and best practices.\nSolid understanding of object-oriented programming.\nFamiliarity with various design, architectural patterns, and software development processes.\nExperience with both external and embedded databases.\nCreating database schemas that represent and support business processes.\nImplementing automated testing platforms and unit tests.\nPreferred Skills:\nTechnology -> Big Data -> Scala, Spark, Hive, Kafka\nAdditional Responsibilities:\nCompetencies:\nGood verbal and written communication skills.\nAbility to communicate with remote teams effectively.\nHigh flexibility to travel.\nEducational Requirements:Master of Computer Applications, Master of Technology, Master of Engineering, MSc, Bachelor of Technology, Bachelor of Computer Applications, Bachelor of Computer Science, Bachelor of Engineering",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Spark', 'Hive', 'Hadoop', 'Big Data', 'Kafka']",2025-06-12 05:31:59
BFSI Data and Analytics Project Lead - CITI Bank,"NTT DATA, Inc.",12 - 17 years,Not Disclosed,['Bengaluru'],"Req ID: 326459\n\nWe are currently seeking a BFSI Data and Analytics Project Lead - CITI Bank to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\n""Job DutiesData & Analytics Project Lead with over 12+ years of experience in BFSI Domain\nThe Data and Analytics Delivery Manager will oversee the successful delivery of the Client's data and analytics projects, ensuring our clients derive maximum value from their data assets. This leadership role involves setting strategy, managing delivery teams, collaborating across functions, and upholding data governance and quality standards. The ideal candidate brings strong technical and business acumen to build and execute data-driven strategies aligned with the Client's mission of transforming their business with data-driven insights.\nThe core responsibilities for the job include the following:\nProject and Program Oversight:\n""¢ Oversee end-to-end delivery of complex data and analytics projects, ensuring timely, high-quality, and cost-effective outcomes.\n""¢ Establish project governance, risk management, and quality assurance standards for effective project delivery.\n""¢ Monitor project portfolios and allocate resources to optimize productivity across multiple client engagements.\n\nStakeholder Collaboration and Engagement:\n""¢ Serve as the primary liaison between data delivery teams, sales, product, and client-facing teams to ensure client needs are met.\n""¢ Present data strategies, project status, and insights effectively to both technical and non-technical stakeholders, fostering alignment.\n""¢ Drive collaboration with product management and engineering teams to align on data needs and operational goals.\n\nInnovation and Technology Adoption:\n""¢ Stay abreast of the latest trends in GenAI, Agentic AI in data engineering, data science, machine learning, and AI to enhance the Client's data capabilities.\n""¢ Must have experience in Cloud Modernization, DWH, Datalake project execution\n""¢ Drive the adoption of advanced analytics tools and technologies to improve data delivery efficiency and solution impact.\n""¢ Assess and recommend new tools, platforms, and partners to continuously improve data solutions.\nTeam Development and Leadership:\n""¢ Recruit, mentor, and retain a high-performing data and analytics team, fostering a culture of collaboration and continuous improvement.\n""¢ Set performance goals, conduct regular evaluations, and provide ongoing feedback to support team growth.\n\nMinimum Skills Required:\n""¢ Educational BackgroundBachelor's or Master's degree in Data Science, Computer Science, Business Administration, or a related field.\n""¢ Experience15+ years of experience in data and analytics, including at least 5 years in a leadership role with a proven track record in delivery management.\n""¢ Technical ProficiencyDeep understanding of data warehousing, data visualization, data governance, data trust and big data tools (SQL, Python, R, Tableau, Power BI, and cloud platforms like AWS, Azure, or Google Cloud).\n""¢ Must have experience in Cloud Modernization, DWH, Datalake project execution\n""¢ BFSI KnowledgeMandatory to have worked in BFSI projects delivered Data & Analytics projects to BFSI clients\n""¢ Project Management ExpertiseStrong background in Agile, Scrum, or other project management methodologies.\n""¢ Leadership and CommunicationExcellent interpersonal and communication skills, with a demonstrated ability to lead, influence, and engage stakeholders at all levels.\n""¢ Analytical and Problem-Solving\n\nSkills:\nStrong analytical mindset with a track record of delivering actionable insights from complex data\nThe Data and Analytics Delivery Manager will oversee the successful delivery of the Client's data and analytics projects, ensuring our retail clients derive maximum value from their data assets. This leadership role involves setting strategy, managing delivery teams, collaborating across functions, and upholding data governance and quality standards. The ideal candidate brings strong technical and business acumen to build and execute data-driven strategies aligned with the Client's mission of transforming retail with data-driven insights.""",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data governance', 'scrum', 'agile', 'data visualization', 'big data', 'advanced analytics', 'python', 'data analytics', 'data warehousing', 'power bi', 'microsoft azure', 'project management process', 'machine learning', 'sql', 'tableau', 'r', 'bfsi', 'data science', 'gcp', 'project execution', 'aws']",2025-06-12 05:32:02
Senior Associate Data Scientist,"NTT DATA, Inc.",2 - 5 years,Not Disclosed,['Bengaluru'],"Your day at NTT DATA\nThe Data Scientist is a seasoned subject matter expert, tasked with participating in the adoption of data science and analytics within the organization.\n\nThe primary responsibility of this role is to participate in the creation and delivery of data-driven solutions that add business value using statistical models, machine learning algorithms, data mining, and visualization techniques.\n\nWhat youll be doing\n\nKey Responsibilities:\nDesigns, develops, and programs methods, processes, and systems to consolidate and analyze unstructured, diverse big data sources to generate actionable insights and solutions for client services and product enhancement.\nDesigns and enhances data collection procedures to include information that is relevant for building analytic systems.\nAccountable for ensuring that data used for analysis is processed, cleaned and, integrally verified and build algorithms necessary to find meaningful answers.\nDesigns and codes software programs, algorithms, and automated processes to cleanse, integrate and evaluate large datasets from multiple disparate sources.\nAccountable for providing meaningful insights from large data and metadata sources; interprets and communicates insights and findings from analysis and experiments to product, service, and business managers.\nAccountable for performing analysis using programming languages or statistical packages such as Python, pandas etc.\nDesigns scalable and highly available applications leveraging the latest tools and technologies.\nAccountable for creatively visualizing and effectively communicating results of data analysis, insights, and ideas in a variety of formats to key decision-makers within the business.\nCreates SQL queries for the analysis of data and visualize the output of the models.\nCreates documentation around processes and procedures and manages code reviews.\nAccountable for ensuring that industry standards best practices are applied to development activities.\nKnowledge and Attributes:\nSeasoned in data modelling, statistical methods and machine learning techniques.\nAbility to thrive in a dynamic, fast-paced environment.\nQuantitative and qualitative analysis skills.\nDesire to acquire more knowledge to keep up to speed with the ever-evolving field of data science.\nCuriosity to sift through data to find answers and more insights.\nGood understanding of the information technology industry within a matrixed organization and the typical business problems such organizations face.\nAbility to translate technical findings clearly and fluently to non-technical team business stakeholders to enable informed decision-making.\nAbility to create a storyline around the data to make it easy to interpret and understand.\nSelf-driven and able to work independently yet acts as a team player.\nAble to apply data science principles through a business lens.\nDesire to create strategies and solutions that challenge and expand the thinking of peers and business stakeholders.\nAcademic Qualifications and Certifications:\nBachelors degree or equivalent in Data Science, Business Analytics, Mathematics, Economics, Engineering, Computer Science or a related field.\nRelevant programming (Python) certification preferred.\nAgile certification preferred.\nRequired Experience:\nSeasoned experience in a data science position in a corporate environment and/or related industry.\nSeasoned experience in statistical modelling and data modelling, machine learning, data mining, unstructured data analytics, natural language processing.\nSeasoned experience in programming languages (Python, etc.).\nSeasoned experience working in databases (MySQL, Microsoft SQL Server, Azure Synapse, MongoDB)\nSeasoned experience working with and creating data architectures.\nSeasoned experience with extracting, cleaning, and transforming data and working with data owners to understand the data.\nSeasoned experience visualizing and/or presenting data for stakeholder use and reuse across the business.\nSeasoned experience on working with API (creating and using APIs)\nAutomation experience using Python scripting, UIPath, Selenium, PowerAutomate.\nSeasoned experience working on Linux operating system (Ubuntu)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Azure Synapse', 'Microsoft SQL Server', 'MySQL', 'Python scripting', 'PowerAutomate', 'Linux operating system', 'MongoDB', 'Selenium', 'Python', 'UIPath']",2025-06-12 05:32:04
Data Techology Senior Associate,MSCI Services,4 - 7 years,Not Disclosed,['Pune'],"Overview\nThe Data Technology team at MSCI is responsible for meeting the data requirements across various business areas, including Index, Analytics, and Sustainability. Our team collates data from multiple sources such as vendors (e.g., Bloomberg, Reuters), website acquisitions, and web scraping (e.g., financial news sites, company websites, exchange websites, filings). This data can be in structured or semi-structured formats. We normalize the data, perform quality checks, assign internal identifiers, and release it to downstream applications.\nResponsibilities\nAs data engineers, we build scalable systems to process data in various formats and volumes, ranging from megabytes to terabytes. Our systems perform quality checks, match data across various sources, and release it in multiple formats. We leverage the latest technologies, sources, and tools to process the data. Some of the exciting technologies we work with include Snowflake, Databricks, and Apache Spark.\nQualifications\nCore Java, Spring Boot, Apache Spark, Spring Batch, Python. Exposure to sql databases like Oracle, Mysql, Microsoft Sql is a must. Any experience/knowledge/certification on Cloud technology preferrably Microsoft Azure or Google cloud platform is good to have. Exposures to non sql databases like Neo4j or Document database is again good to have.\n What we offer you\nTransparent compensation schemes and comprehensive employee benefits, tailored to your location, ensuring your financial security, health, and overall wellbeing.\nFlexible working arrangements, advanced technology, and collaborative workspaces.\nA culture of high performance and innovation where we experiment with new ideas and take responsibility for achieving results.\nA global network of talented colleagues, who inspire, support, and share their expertise to innovate and deliver for our clients.\nGlobal Orientation program to kickstart your journey, followed by access to our Learning@MSCI platform, LinkedIn Learning Pro and tailored learning opportunities for ongoing skills development.\nMulti-directional career paths that offer professional growth and development through new challenges, internal mobility and expanded roles.\nWe actively nurture an environment that builds a sense of inclusion belonging and connection, including eight Employee Resource Groups. All Abilities, Asian Support Network, Black Leadership Network, Climate Action Network, Hola! MSCI, Pride & Allies, Women in Tech, and Women’s Leadership Forum.\nAt MSCI we are passionate about what we do, and we are inspired by our purpose – to power better investment decisions. You’ll be part of an industry-leading network of creative, curious, and entrepreneurial pioneers. This is a space where you can challenge yourself, set new standards and perform beyond expectations for yourself, our clients, and our industry.\nMSCI is a leading provider of critical decision support tools and services for the global investment community. With over 50 years of expertise in research, data, and technology, we power better investment decisions by enabling clients to understand and analyze key drivers of risk and return and confidently build more effective portfolios. We create industry-leading research-enhanced solutions that clients use to gain insight into and improve transparency across the investment process.\nMSCI Inc. is an equal opportunity employer. It is the policy of the firm to ensure equal employment opportunity without discrimination or harassment on the basis of race, color, religion, creed, age, sex, gender, gender identity, sexual orientation, national origin, citizenship, disability, marital and civil partnership/union status, pregnancy (including unlawful discrimination on the basis of a legally protected parental leave), veteran status, or any other characteristic protected by law. MSCI is also committed to working with and providing reasonable accommodations to individuals with disabilities. If you are an individual with a disability and would like to request a reasonable accommodation for any part of the application process, please email Disability.Assistance@msci.com and indicate the specifics of the assistance needed. Please note, this e-mail is intended only for individuals who are requesting a reasonable workplace accommodation; it is not intended for other inquiries.\n To all recruitment agencies\nMSCI does not accept unsolicited CVs/Resumes. Please do not forward CVs/Resumes to any MSCI employee, location, or website. MSCI is not responsible for any fees related to unsolicited CVs/Resumes.\n Note on recruitment scams\nWe are aware of recruitment scams where fraudsters impersonating MSCI personnel may try and elicit personal information from job seekers. Read our full note on careers.msci.com",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'access', 'scala', 'pyspark', 'data warehousing', 'hibernate', 'research', 'sql', 'analytics', 'spring', 'java', 'spring batch', 'spark', 'gcp', 'mysql', 'html', 'hadoop', 'big data', 'etl', 'snowflake', 'python', 'oracle', 'data analysis', 'microsoft azure', 'power bi', 'sql server', 'javascript', 'data bricks', 'spring boot', 'tableau', 'neo4j', 'aws', 'sql database']",2025-06-12 05:32:07
Data & AI Technical Solution Architects,"NTT DATA, Inc.",12 - 15 years,Not Disclosed,['Bengaluru'],"Req ID: 323775\n\nWe are currently seeking a Data & AI Technical Solution Architects to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\n""Job DutiesThe Data & AI Architect is a seasoned level expert who is responsible for participating in the delivery of multi-technology consulting services to clients by providing strategies and solutions on all aspects of infrastructure and related technology components.\n\nThis role collaborates with other stakeholders on the development of the architectural approach for one or more layer of a solution. This role has the primary objective is to work on strategic projects that ensure the optimal functioning of the client""™s technology infrastructure.\n""¢ Key Responsibilities:\n""¢ Ability and experience to have conversations with the CEO, Business owners and CTO/CDO\n""¢ Break down intricate business challenges, devise effective solutions, and focus on client needs.\n""¢ Craft high level innovative solution approach for complex business problems\n""¢ Utilize best practices and creativity to address challenges\n""¢ Leverage market research, formulate perspectives, and communicate insights to clients\n""¢ Establish strong client relationships\n""¢ Interact at appropriate levels to ensure client satisfaction\n""¢ Knowledge and Attributes:\n""¢ Ability to focus on detail with an understanding of how it impacts the business strategically.\n""¢ Excellent client service orientation.\n""¢ Ability to work in high-pressure situations.\n""¢ Ability to establish and manage processes and practices through collaboration and the understanding of business.\n""¢ Ability to create new and repeat business for the organization.\n""¢ Ability to contribute information on relevant vertical markets\n""¢ Ability to contribute to the improvement of internal effectiveness by contributing to the improvement of current methodologies, processes and tools.\n\nMinimum Skills RequiredAcademic Qualifications and Certifications:\n""¢ BE/BTech or equivalent in Information Technology and/or Business Management or a related field.\n""¢ Scaled Agile certification desirable.\n""¢ Relevant consulting and technical certifications preferred, for example TOGAF.\n\nRequired Experience12-15 years\n""¢ Seasoned demonstrable experience in a similar role within a large scale (preferably multi- national) technology services environment.\n""¢ Very good understanding of Data, AI, Gen AI and Agentic AI\n""¢ Must have Data Architecture and Solutioning experience. Capable of E2E Data Architecture and GenAI Solution design.\n""¢ Must be able to work on Data & AI RFP responses as Solution Architect\n""¢ 10+ years of experience in Solution Architecting of Data & Analytics, AI/ML & Gen AI Technical Architect\n""¢ Develop Cloud-native technical approach and proposal plans identifying the best practice solutions meeting the requirements for a successful proposal. Create, edit, and review documents, diagrams, and other artifacts in response to RPPs RFQs and Contribute to and participate in presentations to customers regarding proposed solutions.\n""¢ Proficient with Snowflake, Databricks, Azure, AWS, GCP cloud, Data Engineering & AI tools\n""¢ Experience with large scale consulting and program execution engagements in AI and data\n""¢ Seasoned multi-technology infrastructure design experience.\n""¢ Seasoned demonstrable level of expertise coupled with consulting and client engagement experience, demonstrating good experience in client needs assessment and change management.\n""¢ Additional\nAdditional\nAdditional Career Level Description:\nKnowledge and application:\n""¢ Seasoned, experienced professional; has complete knowledge and understanding of area of specialization.\n""¢ Uses evaluation, judgment, and interpretation to select right course of action.\nProblem solving:\n""¢ Works on problems of diverse scope where analysis of information requires evaluation of identifiable factors.\n""¢ Resolves and assesses a wide range of issues in creative ways and suggests variations in approach.\nInteraction:\n""¢ Enhances relationships and networks with senior internal/external partners who are not familiar with the subject matter often requiring persuasion.\n""¢ Works""",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'microsoft azure', 'data engineering', 'data bricks', 'aws', 'client engagement', 'ai solutions', 'togaf', 'aiml', 'data architecture', 'solution architecting', 'artificial intelligence', 'change management', 'gen', 'service orientation', 'solution design', 'gcp', 'gcp cloud', 'ml']",2025-06-12 05:32:10
BFSI Data and Analytics Delivery Manager,"NTT DATA, Inc.",18 - 23 years,Not Disclosed,['Bengaluru'],"Req ID: 326457\n\nWe are currently seeking a BFSI Data and Analytics Delivery Manager to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\n""Job DutiesData & Analytics Delivery Manager with over 18+ years of experience in BFSI Domain\nThe Data and Analytics Delivery Manager will oversee the successful delivery of the Client's data and analytics projects, ensuring our clients derive maximum value from their data assets. This leadership role involves setting strategy, managing delivery teams, collaborating across functions, and upholding data governance and quality standards. The ideal candidate brings strong technical and business acumen to build and execute data-driven strategies aligned with the Client's mission of transforming their business with data-driven insights.\nThe core responsibilities for the job include the following:\nProject and Program Oversight:\n""¢ Oversee end-to-end delivery of complex data and analytics projects, ensuring timely, high-quality, and cost-effective outcomes.\n""¢ Establish project governance, risk management, and quality assurance standards for effective project delivery.\n""¢ Monitor project portfolios and allocate resources to optimize productivity across multiple client engagements.\n\nStakeholder Collaboration and Engagement:\n""¢ Serve as the primary liaison between data delivery teams, sales, product, and client-facing teams to ensure client needs are met.\n""¢ Present data strategies, project status, and insights effectively to both technical and non-technical stakeholders, fostering alignment.\n""¢ Drive collaboration with product management and engineering teams to align on data needs and operational goals.\n\nInnovation and Technology Adoption:\n""¢ Stay abreast of the latest trends in GenAI, Agentic AI in data engineering, data science, machine learning, and AI to enhance the Client's data capabilities.\n""¢ Must have experience in Cloud Modernization, DWH, Datalake project execution\n""¢ Drive the adoption of advanced analytics tools and technologies to improve data delivery efficiency and solution impact.\n""¢ Assess and recommend new tools, platforms, and partners to continuously improve data solutions.\nTeam Development and Leadership:\n""¢ Recruit, mentor, and retain a high-performing data and analytics team, fostering a culture of collaboration and continuous improvement.\n""¢ Set performance goals, conduct regular evaluations, and provide ongoing feedback to support team growth.\n\nMinimum Skills Required:\n""¢ Educational BackgroundBachelor's or Master's degree in Data Science, Computer Science, Business Administration, or a related field.\n""¢ Experience15+ years of experience in data and analytics, including at least 5 years in a leadership role with a proven track record in delivery management.\n""¢ Technical ProficiencyDeep understanding of data warehousing, data visualization, data governance, data trust and big data tools (SQL, Python, R, Tableau, Power BI, and cloud platforms like AWS, Azure, or Google Cloud).\n""¢ Must have experience in Cloud Modernization, DWH, Datalake project execution\n""¢ BFSI KnowledgeMandatory to have worked in BFSI projects delivered Data & Analytics projects to BFSI clients\n""¢ Project Management ExpertiseStrong background in Agile, Scrum, or other project management methodologies.\n""¢ Leadership and CommunicationExcellent interpersonal and communication skills, with a demonstrated ability to lead, influence, and engage stakeholders at all levels.\n""¢ Analytical and Problem-Solving\n\nSkills:\nStrong analytical mindset with a track record of delivering actionable insights from complex data\nThe Data and Analytics Delivery Manager will oversee the successful delivery of the Client's data and analytics projects, ensuring our retail clients derive maximum value from their data assets. This leadership role involves setting strategy, managing delivery teams, collaborating across functions, and upholding data governance and quality standards. The ideal candidate brings strong technical and business acumen to build and execute data-driven strategies aligned with the Client's mission of transforming retail with data-driven insig""",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data governance', 'scrum', 'agile', 'data visualization', 'big data', 'advanced analytics', 'python', 'data analytics', 'data warehousing', 'power bi', 'microsoft azure', 'project management process', 'machine learning', 'sql', 'tableau', 'r', 'bfsi', 'data science', 'gcp', 'project execution', 'aws']",2025-06-12 05:32:12
"Senior Data Scientist (AI/ML, Data Analysis, Cloud (AWS), and Model",Synechron,8 - 13 years,Not Disclosed,['Pune'],"job requisition idJR1027352\n\nJob Summary\nSynechron is seeking an analytical and innovative Senior Data Scientist to support and advance our data-driven initiatives. The ideal candidate will have a solid understanding of data science principles, hands-on experience with AI/ML tools and techniques, and the ability to interpret complex data sets to deliver actionable insights. This role contributes to the organizations strategic decision-making and technology innovation by applying advanced analytics and machine learning models in a collaborative environment.\n\nSoftware\n\nRequired\n\nSkills:\nPython (including libraries such as pandas, scikit-learn, TensorFlow, PyTorch) proficiency in developing and deploying models\nR (optional, but preferred)\nData management tools (SQL, NoSQL databases)\nCloud platforms (preferably AWS or Azure) for data storage and ML deployment\nJupyter Notebooks or similar interactive development environments\nVersion control tools such as Git\nPreferred\n\nSkills:\nBig data technologies (Spark, Hadoop)\nModel deployment tools (MLflow, Docker, Kubernetes)\nData visualization tools (Tableau, Power BI)\nOverall Responsibilities\nAnalyze and interpret large and complex data sets to generate insights for business and technology initiatives.\nAssist in designing, developing, and implementing AI/ML models and algorithms to solve real-world problems.\nCollaborate with cross-functional teams including data engineers, software developers, and business analysts to integrate models into production systems.\nStay current with emerging trends, research, and best practices in AI/ML/Data Science and apply them to ongoing projects.\nDocument methodologies, modeling approaches, and insights clearly for technical and non-technical stakeholders.\nSupport model validation, testing, and performance monitoring to ensure accuracy and reliability.\nContribute to the development of data science workflows and standards within the organization.\nPerformance Outcomes:\nAccurate and reliable data models that support strategic decision-making.\nClear documentation and communication of findings and recommendations.\nEffective collaboration with technical teams to deploy scalable models.\nContinuous adoption of best practices in AI/ML and data management.\nTechnical Skills (By Category)\n\nProgramming Languages:\nEssential: Python (best practices in ML development), SQL\nPreferred: R, Java (for integration purposes)\nDatabases/Data Management:\nSQL databases, NoSQL (MongoDB, Cassandra)\nCloud data storage solutions (AWS S3, Azure Blob Storage)\nCloud Technologies:\nAWS (S3, EC2, SageMaker, Lambda)\nAzure Machine Learning (preferred)\nFrameworks & Libraries:\nTensorFlow, PyTorch, scikit-learn, Keras, XGBoost\nDevelopment Tools & Methodologies:\nJupyter Notebooks, Git, CI/CD pipelines\nAgile and Scrum processes\nSecurity Protocols:\nBest practices in data security and privacy, GDPR compliance\nExperience\n8+ years of professional experience in AI, ML, or Data Science roles.\nProven hands-on experience designing and deploying ML models in real-world scenarios.\nDemonstrated ability to analyze complex data sets and translate findings into business insights.\nPrevious experience working with cloud-based data science solutions is preferred.\nStrong portfolio showcasing data science projects, models developed, and practical impact.\nAlternative Pathways:\nCandidates with extensive research or academic experience in AI/ML can be considered, provided they demonstrate practical application of skills.\n\nDay-to-Day Activities\nConduct data exploration, cleaning, feature engineering, and model development.\nCollaborate with data engineers to prepare data pipelines for model training.\nBuild, validate, and refine machine learning models.\nPresent insights, models, and recommendations to technical and business stakeholders.\nSupport deployment of models into production environments.\nMonitor model performance and iterate to improve effectiveness.\nParticipate in team meetings, project planning, and reviewing progress.\nDocument methodologies and maintain version control of codebase.\nQualifications\nBachelors degree in Computer Science, Mathematics, Statistics, Data Science, or a related field; Masters or PhD highly desirable.\nEvidence of relevant coursework, certifications, or professional training in AI/ML.\nProfessional certifications (e.g., AWS Certified Machine Learning Specialty, Microsoft Certified Data Scientist) are a plus.\nCommitment to ongoing professional development in AI/ML methodologies.\nProfessional Competencies\nStrong analytical and critical thinking to solve complex problems.\nEffective communication skills for technical and non-technical audiences.\nDemonstrated ability to work collaboratively in diverse teams.\nAptitude for learning new tools, techniques, and technologies rapidly.\nInnovation mindset with a focus on applying emerging research.\nStrong organizational skills to manage multiple projects and priorities.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['java', 'data science', 'python', 'deploying models', 'aws', 'continuous integration', 'kubernetes', 'scikit-learn', 'ci/cd', 'artificial intelligence', 'sql', 'docker', 'tensorflow', 'spark', 'pytorch', 'keras', 'hadoop', 'big data', 'mongodb', 'microsoft azure', 'nosql', 'pandas', 'amazon ec2', 'r', 'cassandra', 'agile']",2025-06-12 05:32:15
Senior Data Scientist,"NTT DATA, Inc.",2 - 6 years,Not Disclosed,['Bengaluru'],"Your day at NTT DATA\nThe Senior Data Scientist is an advanced subject matter expert, tasked with taking accountability in the adoption of data science and analytics within the organization.\n\nThe primary responsibility of this role is to participate in the creation and delivery of data-driven solutions that add business value using statistical models, machine learning algorithms, data mining, and visualization techniques.\n\nKey responsibilities:\nDesigns, develops, and programs methods, processes, and systems to consolidate and analyze unstructured, diverse big data sources to generate actionable insights and solutions for client services and product enhancement.\nDesigns and enhances data collection procedures to include information that is relevant for building analytic systems.\nResponsible for ensuring that data used for analysis is processed, cleaned and, integrally verified and build algorithms necessary to find meaningful answers.\nDesigns and codes software programs, algorithms, and automated processes to cleanse, integrate and evaluate large datasets from multiple disparate sources\nProvides meaningful insights from large data and metadata sources; interprets and communicates insights and findings from analysis and experiments to product, service, and business managers.\nDirects scalable and highly available applications leveraging the latest tools and technologies.\nAccountable for creatively visualizing and effectively communicating results of data analysis, insights, and ideas in a variety of formats to key decision-makers within the business.\nCreates SQL queries for the analysis of data and visualizes the output of the models.\nResponsible for ensuring that industry standards best practices are applied to development activities.\n\nTo thrive in this role, you need to have:\nAdvanced understanding of data modelling, statistical methods and machine learning techniques.\nStrong ability to thrive in a dynamic, fast-paced environment.\nStrong quantitative and qualitative analysis skills.\nDesire to acquire more knowledge to keep up to speed with the ever-evolving field of data science.\nCuriosity to sift through data to find answers and more insights.\nAdvanced understanding of the information technology industry within a matrixed organization and the typical business problems such organizations face.\nStrong ability to translate technical findings clearly and fluently to non-technical team business stakeholders to enable informed decision-making.\nStrong ability to create a storyline around the data to make it easy to interpret and understand.\nSelf-driven and able to work independently yet acts as a team player.\n\nAcademic qualifications and certifications:\nBachelors degree or equivalent in Data Science, Business Analytics, Mathematics, Economics, Engineering, Computer Science or a related field.\nRelevant programming certification preferred.\nAgile certification preferred.\n\nRequired experience:\nAdvanced demonstrated experience in a data science position in a corporate environment and/or related industry.\nAdvanced demonstrated experience in statistical modelling and data modelling, machine learning, data mining, unstructured data analytics, natural language processing.\nAdvanced demonstrated experience in programming languages (R, Python, etc.).\nAdvanced demonstrated experience working with and creating data architectures.\nAdvanced demonstrated experience with extracting, cleaning, and transforming data and working with data owners to understand the data.\nAdvanced demonstrated experience visualizing and/or presenting data for stakeholder use and reuse across the business.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'data analytics', 'natural language processing', 'data modeling', 'data mining', 'statistical modeling', 'data architecture', 'machine learning']",2025-06-12 05:32:17
Data Architect Sr. Advisor,"NTT DATA, Inc.",12 - 15 years,Not Disclosed,['Bengaluru'],"Req ID: 323777\n\nWe are currently seeking a Data Architect Sr. Advisor to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\n""Job DutiesThe Data & AI Architect is a seasoned level expert who is responsible for participating in the delivery of multi-technology consulting services to clients by providing strategies and solutions on all aspects of infrastructure and related technology components.\n\nThis role collaborates with other stakeholders on the development of the architectural approach for one or more layer of a solution. This role has the primary objective is to work on strategic projects that ensure the optimal functioning of the client""™s technology infrastructure.\n""¢ Key Responsibilities:\n""¢ Ability and experience to have conversations with the CEO, Business owners and CTO/CDO\n""¢ Break down intricate business challenges, devise effective solutions, and focus on client needs.\n""¢ Craft high level innovative solution approach for complex business problems\n""¢ Utilize best practices and creativity to address challenges\n""¢ Leverage market research, formulate perspectives, and communicate insights to clients\n""¢ Establish strong client relationships\n""¢ Interact at appropriate levels to ensure client satisfaction\n""¢ Knowledge and Attributes:\n""¢ Ability to focus on detail with an understanding of how it impacts the business strategically.\n""¢ Excellent client service orientation.\n""¢ Ability to work in high-pressure situations.\n""¢ Ability to establish and manage processes and practices through collaboration and the understanding of business.\n""¢ Ability to create new and repeat business for the organization.\n""¢ Ability to contribute information on relevant vertical markets\n""¢ Ability to contribute to the improvement of internal effectiveness by contributing to the improvement of current methodologies, processes and tools.\n\nMinimum Skills RequiredAcademic Qualifications and Certifications:\n""¢ BE/BTech or equivalent in Information Technology and/or Business Management or a related field.\n""¢ Scaled Agile certification desirable.\n""¢ Relevant consulting and technical certifications preferred, for example TOGAF.\n\nRequired Experience12-15 years\n""¢ Seasoned demonstrable experience in a similar role within a large scale (preferably multi- national) technology services environment.\n""¢ Very good understanding of Data, AI, Gen AI and Agentic AI\n""¢ Must have Data Architecture and Solutioning experience. Capable of E2E Data Architecture and GenAI Solution design.\n""¢ Must be able to work on Data & AI RFP responses as Solution Architect\n""¢ 10+ years of experience in Solution Architecting of Data & Analytics, AI/ML & Gen AI Technical Architect\n""¢ Develop Cloud-native technical approach and proposal plans identifying the best practice solutions meeting the requirements for a successful proposal. Create, edit, and review documents, diagrams, and other artifacts in response to RPPs RFQs and Contribute to and participate in presentations to customers regarding proposed solutions.\n""¢ Proficient with Snowflake, Databricks, Azure, AWS, GCP cloud, Data Engineering & AI tools\n""¢ Experience with large scale consulting and program execution engagements in AI and data\n""¢ Seasoned multi-technology infrastructure design experience.\n""¢ Seasoned demonstrable level of expertise coupled with consulting and client engagement experience, demonstrating good experience in client needs assessment and change management.\n""¢ Additional\nAdditional\nAdditional Career Level Description:\nKnowledge and application:\n""¢ Seasoned, experienced professional; has complete knowledge and understanding of area of specialization.\n""¢ Uses evaluation, judgment, and interpretation to select right course of action.\nProblem solving:\n""¢ Works on problems of diverse scope where analysis of information requires evaluation of identifiable factors.\n""¢ Resolves and assesses a wide range of issues in creative ways and suggests variations in approach.\nInteraction:\n""¢ Enhances relationships and networks with senior internal/external partners who are not familiar with the subject matter often requiring persuasion.\n""¢ Works""",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['snowflake', 'microsoft azure', 'data engineering', 'data bricks', 'aws', 'python', 'client engagement', 'ai solutions', 'aiml', 'data architecture', 'machine learning', 'solution architecting', 'artificial intelligence', 'change management', 'gen', 'service orientation', 'gcp cloud', 'ml']",2025-06-12 05:32:20
Data Architect,.,7 - 12 years,20-35 Lacs P.A.,"['Hyderabad', 'Bengaluru']","Job Description\nWe are seeking a highly skilled Azure Data Engineer with strong expertise in Data Architecture, PySpark/Python, Azure Databricks, and data streaming solutions. The ideal candidate will have hands-on experience in designing and implementing large-scale data pipelines, along with solid knowledge of data governance and data modeling.\nKey Responsibilities\nDesign, develop, and optimize PySpark/Python-based data streaming jobs on Azure Databricks.\nBuild scalable and efficient data pipelines for batch and real-time processing.\nImplement data governance policies, ensuring data quality, security, and compliance.\nDevelop and maintain data models (dimensional, relational, NoSQL) to support analytics and reporting.\nCollaborate with cross-functional teams (data scientists, analysts, and business stakeholders) to deliver data solutions.\nTroubleshoot performance bottlenecks and optimize Spark jobs for efficiency.\nEnsure best practices in CI/CD, automation, and monitoring of data workflows.\nMentor junior engineers and lead technical discussions (for senior/managerial roles).\nMandatory Skills & Experience\n5+ years of relevant experience as a Data Engineer/Analyst/Architect (8+ years for Manager/Lead positions).\nExpert-level proficiency in PySpark/Python and Azure Databricks (must have worked on real production projects).\nStrong experience in building and optimizing streaming data pipelines (Kafka, Event Hubs, Delta Lake, etc.).\n4+ years of hands-on experience in data governance & data modeling (ER, star schema, data vault, etc.).\nIn-depth knowledge of Azure Data Factory, Synapse, ADLS, and SQL/NoSQL databases.\nExperience with Delta Lake, Databricks Workflows, and performance tuning.\nFamiliarity with data security, metadata management, and lineage tracking.\nExcellent communication skills (must be able to articulate technical concepts clearly).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Azure Databricks', 'Data Modeling', 'Data Governance', 'Python', 'ETL']",2025-06-12 05:32:22
Data Migration Engineer,A Global Engineering and Technology Solu...,4 - 6 years,Not Disclosed,['Pune'],"Roles and Responsibilities\nDesign, develop, test, and deploy data migration solutions using various tools such as PTC, WBM, CAD, PDM Link, Java Utilities, Loaders, SQL, and Oracle.\nCollaborate with cross-functional teams to identify business requirements and design data migration strategies that meet those needs.\nDevelop complex database queries to extract relevant data from legacy systems for migration into new platforms.\nConduct thorough testing of migrated data to ensure accuracy and integrity.\nProvide technical guidance on best practices for data management and governance.\nDesired Candidate Profile\n4-6 years of experience in Data Migration Engineering with expertise in one or more of the following areas: PTC/WBM/CAD/PDM Link/Java Utilities/Loaders/SQL/Oracle.\nBachelor's degree in Any Specialization (B.Tech/B.E.).\nStrong understanding of software development life cycle (SDLC) principles and methodologies.\nProficiency in writing efficient code using programming languages like Java or Python.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['WBM', 'Pdm Link', 'Java utilities', 'Oracle', 'Windchill', 'loaders', 'SQL']",2025-06-12 05:32:25
Data Integration - MSSQL + SSIS + SSMA,Hexaware Technologies,4 - 6 years,Not Disclosed,"['Chennai', 'Bengaluru']","Deep understanding of MSSQL, SSIS, and SSMA technologies.\nCollaborate with clients to understand project requirements and translate them into technical solutions.\nDevelop, test, and deploy MSSQL databases, SSIS packages, SSMS, and Python scripts to meet project objectives.\nDesign and implement project management artifacts, including project plans, timelines, status reports, and documentation.\nLead and participate in project meetings, providing updates on project status, risks, and issues.",,,,"['SSMA', 'MSSQL', 'SSIS', 'Python']",2025-06-12 05:32:28
Salesforce Data Cloud Architect,"NTT DATA, Inc.",4 - 8 years,Not Disclosed,"['Chennai', 'Gurugram', 'Bengaluru']","We are currently seeking a Salesforce Data Cloud Architect to join our team in ""‹""‹""‹""‹""‹""‹""‹Hyderabad, Telangana, India.\n\n\nSalesforce Data Cloud Expertise: Extensive knowledge of Salesforce Data Cloud features, capabilities, and best practices.\n\n\nData Modeling: Strong experience in designing and implementing data models.\n\n\nData Integration: Experience with data integration tools and techniques.\n\n\nData Quality: Understanding of data quality concepts and practices.\n\n\nData Governance: Knowledge of data governance principles and practices.\n\n\nSQL: Proficiency in SQL for data querying and manipulation.\n\n\nProblem-Solving: Strong analytical and problem-solving skills.\n\n\nCommunication: Excellent communication and collaboration skills.\n\n\nLocation - Bengaluru,Chennai,Gurugram,Hyderabad,Noida,Pune",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'salesforce', 'data modeling', 'data governance', 'data integration', 'c#', 'python', 'business analysis', 'sharepoint', 'user stories', 'news writing', 'javascript', 'editing', 'react.js', 'java', 'product management', 'content writing', 'scrum', 'html', 'agile', 'etl', 'jira']",2025-06-12 05:32:30
Data Analyst,PwC India,4 - 8 years,Not Disclosed,"['Bengaluru', 'Mumbai (All Areas)']","If Interested, please apply in the given link : https://forms.office.com/r/h5Qzqnb1Kr\n\nJob Title: Data Analyst (4 - 8 Years Experience)\nLocation: Bengaluru/ Mumbai\nType: Full-Time\nAbout the Role:\nWe are on the lookout for a sharp, self-driven Data Analyst with a strong command of SQL, Python, and relational databases. If solving complex data problems, building efficient data pipelines, and collaborating across teams excites you - youll thrive in this role.",,,,"['Python', 'SQL', 'Data analyst']",2025-06-12 05:32:32
Data Analyst - L4,Wipro,5 - 8 years,Not Disclosed,['Hyderabad'],"Role Purpose\nThe purpose of this role is to interpret data and turn into information (reports, dashboards, interactive visualizations etc) which can offer ways to improve a business, thus affecting business decisions.\nDo\n1. Managing the technical scope of the project in line with the requirements at all stages\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\nb. Develop record management process and policies\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\nd. Providing sales data, proposals, data insights and account reviews to the client base\ne. Identify areas to increase efficiency and automation of processes\nf. Set up and maintain automated data processes\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\nh. Produce and track key performance indicators\n2. Analyze the data sets and provide adequate information\na. Liaise with internal and external clients to fully understand data content\nb. Design and carry out surveys and analyze survey data as per the customer requirement\nc. Analyze and interpret complex data sets relating to customers business and prepare reports for internal and external audiences using business analytics reporting tools\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\nf. Develop predictive models and share insights with the clients as per their requirement\n\n\n\nDeliver\n\nNoPerformance ParameterMeasure\n1.Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\nMandatory Skills: Database Architecting.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'data validation', 'data mining', 'data warehousing', 'business analytics', 'dashboards', 'data integration']",2025-06-12 05:32:35
Data Analyst - L4,Wipro,5 - 8 years,Not Disclosed,['Hyderabad'],"Role Purpose\nThe purpose of this role is to interpret data and turn into information (reports, dashboards, interactive visualizations etc) which can offer ways to improve a business, thus affecting business decisions.\n\n\n\nDo\n1. Managing the technical scope of the project in line with the requirements at all stages\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\nb. Develop record management process and policies\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\nd. Providing sales data, proposals, data insights and account reviews to the client base\ne. Identify areas to increase efficiency and automation of processes\nf. Set up and maintain automated data processes\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\nh. Produce and track key performance indicators\n2. Analyze the data sets and provide adequate information\na. Liaise with internal and external clients to fully understand data content\nb. Design and carry out surveys and analyze survey data as per the customer requirement\nc. Analyze and interpret complex data sets relating to customers business and prepare reports for internal and external audiences using business analytics reporting tools\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\nf. Develop predictive models and share insights with the clients as per their requirement\n\n\n\nDeliver\n\nNoPerformance ParameterMeasure1.Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\n\n\n\n\n\nMandatory Skills: Database Architecting.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Database Architecting', 'data analysis', 'data mining', 'business analytics', 'reporting tools']",2025-06-12 05:32:37
Data Analyst - L3,Wipro,3 - 5 years,Not Disclosed,['Hyderabad'],"Role Purpose\nThe purpose of this role is to interpret data and turn into information (reports, dashboards, interactive visualizations etc) which can offer ways to improve a business, thus affecting business decisions.\nDo\n1. Managing the technical scope of the project in line with the requirements at all stages\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\nb. Develop record management process and policies\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\nd. Providing sales data, proposals, data insights and account reviews to the client base\ne. Identify areas to increase efficiency and automation of processes\nf. Set up and maintain automated data processes\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\nh. Produce and track key performance indicators\n2. Analyze the data sets and provide adequate information\na. Liaise with internal and external clients to fully understand data content\nb. Design and carry out surveys and analyze survey data as per the customer requirement\nc. Analyze and interpret complex data sets relating to customers business and prepare reports for internal and external audiences using business analytics reporting tools\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\nf. Develop predictive models and share insights with the clients as per their requirement\n\nDeliver\n\nNoPerformance ParameterMeasure1.Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\nMandatory Skills: Business Analyst/ Data Analyst(Media). Experience: 3-5 Years.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'data validation', 'data mining', 'business analysis', 'data warehousing', 'business analytics', 'dbms', 'dashboards', 'sales', 'analytics reporting', 'reporting tools', 'data integration', 'digital transformation']",2025-06-12 05:32:39
Data & Gen AI Specialist,Altimetrik,1 - 4 years,Not Disclosed,['Bengaluru'],"Job Title: Data & GenAI AWS Specialist\nExperience: 1-4 Years\nLocation: Bangalore\nMandatory Qualification: B.E./ B.Tech/ M.Tech/ MS from IIT or IISc ONLY\nJob Overview:\nWe are seeking a seasoned Data & GenAI Specialist with deep expertise in AWS Managed Services (PaaS) to join our innovative team. The ideal candidate will have extensive experience in designing sophisticated, scalable architectures for data pipelines and Generative AI (GenAI) solutions leveraging cloud services.",,,,"['Generative Ai', 'Cloud', 'Data Science', 'Open Source', 'Data Pipeline', 'GCP', 'Azure Cloud', 'Snowflake', 'Machine Learning', 'AWS']",2025-06-12 05:32:41
Data Analyst - L4,Wipro,5 - 8 years,Not Disclosed,['Chennai'],"Role Purpose\nThe purpose of this role is to interpret data and turn into information (reports, dashboards, interactive visualizations etc) which can offer ways to improve a business, thus affecting business decisions.\n\n\n\nDo\n1. Managing the technical scope of the project in line with the requirements at all stages\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\nb. Develop record management process and policies\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\nd. Providing sales data, proposals, data insights and account reviews to the client base\ne. Identify areas to increase efficiency and automation of processes\nf. Set up and maintain automated data processes\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\nh. Produce and track key performance indicators\n2. Analyze the data sets and provide adequate information\na. Liaise with internal and external clients to fully understand data content\nb. Design and carry out surveys and analyze survey data as per the customer requirement\nc. Analyze and interpret complex data sets relating to customers business and prepare reports for internal and external audiences using business analytics reporting tools\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\nf. Develop predictive models and share insights with the clients as per their requirement\n\n\n\nDeliver\n\nNoPerformance ParameterMeasure1.Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\n\n\n\n\n\nMandatory Skills: Google BigQuery.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Google BigQuery', 'analytics reporting', 'data analysis', 'data mining', 'business analytics', 'bigquery', 'data integration']",2025-06-12 05:32:44
Data Architect,Neo Aid,12 - 20 years,48-60 Lacs P.A.,['Bengaluru'],"Data Architect\nBangalore (Pune option).\nHybrid, 2-3 days WFO,\nUp to 60 LPA. Needs GCP, Data Engineering, Analytics, Visualization, Modeling. 1\n5-20 yrs exp, end-to-end data pipeline.\nNotice: 60 days.\nArchitecture - recent 1-2 years\n\n\nProvident fund",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Visualizing', 'Data Architecture', 'Data Modeling', 'Data Analytics', 'Gcp Cloud', 'ETL']",2025-06-12 05:32:46
Data and Analytics Architect - L1,Wipro,8 - 10 years,Not Disclosed,['Hyderabad'],"Role Purpose\nThe purpose of the role is to define and develop Enterprise Data Structure along with Data Warehouse, Master Data, Integration and transaction processing with maintaining and strengthening the modelling standards and business information.\n\n\n\nDo\n1. Define and Develop Data Architecture that aids organization and clients in new/ existing deals\na. Partnering with business leadership (adopting the rationalization of the data value chain) to provide strategic, information-based recommendations to maximize the value of data and information assets, and protect the organization from disruptions while also embracing innovation\nb. Assess the benefits and risks of data by using tools such as business capability models to create an data-centric view to quickly visualize what data matters most to the organization, based on the defined business strategy\nc. Create data strategy and road maps for the Reference Data Architecture as required by the clients\nd. Engage all the stakeholders to implement data governance models and ensure that the implementation is done based on every change request\ne. Ensure that the data storage and database technologies are supported by the data management and infrastructure of the enterprise\nf. Develop, communicate, support and monitor compliance with Data Modelling standards\ng. Oversee and monitor all frameworks to manage data across organization\nh. Provide insights for database storage and platform for ease of use and least manual work\ni. Collaborate with vendors to ensure integrity, objectives and system configuration\nj. Collaborate with functional & technical teams and clients to understand the implications of data architecture and maximize the value of information across the organization\nk. Presenting data repository, objects, source systems along with data scenarios for the front end and back end usage\nl. Define high-level data migration plans to transition the data from source to target system/ application addressing the gaps between the current and future state, typically in sync with the IT budgeting or other capital planning processes\nm. Knowledge of all the Data service provider platforms and ensure end to end view.\nn. Oversight all the data standards/ reference/ papers for proper governance\no. Promote, guard and guide the organization towards common semantics and the proper use of metadata\n\n\n\np. Collecting, aggregating, matching, consolidating, quality-assuring, persisting and distributing such data throughout an organization to ensure a common understanding, consistency, accuracy and control\nq. Provide solution of RFPs received from clients and ensure overall implementation assurance\ni. Develop a direction to manage the portfolio of all the databases including systems, shared infrastructure services in order to better match business outcome objectives\nii. Analyse technology environment, enterprise specifics, client requirements to set a collaboration solution for the big/small data\niii. Provide technical leadership to the implementation of custom solutions through thoughtful use of modern technology\niv. Define and understand current issues and problems and identify improvements\nv. Evaluate and recommend solutions to integrate with overall technology ecosystem keeping consistency throughout\nvi. Understand the root cause problem in integrating business and product units\nvii. Validate the solution/ prototype from technology, cost structure and customer differentiation point of view\nviii. Collaborating with sales and delivery leadership teams to identify future needs and requirements\nix. Tracks industry and application trends and relates these to planning current and future IT needs\n\n\n\n2. Building enterprise technology environment for data architecture management\na. Develop, maintain and implement standard patterns for data layers, data stores, data hub & lake and data management processes\nb. Evaluate all the implemented systems to determine their viability in terms of cost effectiveness\nc. Collect all the structural and non-structural data from different places integrate all the data in one database form\nd. Work through every stage of data processing: analysing, creating, physical data model designs, solutions and reports\ne. Build the enterprise conceptual and logical data models for analytics, operational and data mart structures in accordance with industry best practices\nf. Implement the best security practices across all the data bases based on the accessibility and technology\ng. Strong understanding of activities within primary discipline such as Master Data Management (MDM), Metadata Management and Data Governance (DG)\nh. Demonstrate strong experience in Conceptual, Logical and physical database architectures, design patterns, best practices and programming techniques around relational data modelling and data integration\n\n\n\n3. Enable Delivery Teams by providing optimal delivery solutions/ frameworks\na. Build and maintain relationships with delivery and practice leadership teams and other key stakeholders to become a trusted advisor\nb. Define database physical structure, functional capabilities, security, back-up and recovery specifications\nc. Develops and establishes relevant technical, business process and overall support metrics (KPI/SLA) to drive results\nd. Monitor system capabilities and performance by performing tests and configurations\ne. Integrate new solutions and troubleshoot previously occurred errors\nf. Manages multiple projects and accurately reports the status of all major assignments while adhering to all project management standards\ng. Identify technical, process, structural risks and prepare a risk mitigation plan for all the projects\nh. Ensure quality assurance of all the architecture or design decisions and provides technical mitigation support to the delivery teams\ni. Recommend tools for reuse, automation for improved productivity and reduced cycle times\nj. Help the support and integration team for better efficiency and client experience for ease of use by using AI methods.\nk. Develops trust and builds effective working relationships through respectful, collaborative engagement across individual product teams\nl. Ensures architecture principles and standards are consistently applied to all the projects\nm. Ensure optimal Client Engagement\ni. Support pre-sales team while presenting the entire solution design and its principles to the client\nii. Negotiate, manage and coordinate with the client teams to ensure all requirements are met\niii. Demonstrate thought leadership with strong technical capability in front of the client to win the confidence and act as a trusted advisor\nMandatory Skills: Prophecy.AI.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Prophecy.AI', 'metadata management', 'design patterns', 'data governance', 'data warehousing', 'AI methods', 'master data management']",2025-06-12 05:32:48
Data Scientist,"NTT DATA, Inc.",2 - 5 years,Not Disclosed,['Bengaluru'],"Your day at NTT DATA\nThe Senior Data Scientist is an advanced subject matter expert, tasked with taking accountability in the adoption of data science and analytics within the organization.\n\nThe primary responsibility of this role is to participate in the creation and delivery of data-driven solutions that add business value using statistical models, machine learning algorithms, data mining, and visualization techniques.\n\nWhat youll be doing\n\nKey Responsibilities:\nDesigns, develops, and programs methods, processes, and systems to consolidate and analyze unstructured, diverse big data sources to generate actionable insights and solutions for client services and product enhancement.\nDesigns and enhances data collection procedures to include information that is relevant for building analytic systems.\nResponsible for ensuring that data used for analysis is processed, cleaned and, integrally verified and build algorithms necessary to find meaningful answers.\nDesigns and codes software programs, algorithms, and automated processes to cleanse, integrate and evaluate large datasets from multiple disparate sources\nProvides meaningful insights from large data and metadata sources; interprets and communicates insights and findings from analysis and experiments to product, service, and business managers.\nDirects scalable and highly available applications leveraging the latest tools and technologies.\nAccountable for creatively visualizing and effectively communicating results of data analysis, insights, and ideas in a variety of formats to key decision-makers within the business.\nCreates SQL queries for the analysis of data and visualizes the output of the models.\nResponsible for ensuring that industry standards best practices are applied to development activities.\nKnowledge and Attributes:\nAdvanced understanding of data modelling, statistical methods and machine learning techniques.\nStrong ability to thrive in a dynamic, fast-paced environment.\nStrong quantitative and qualitative analysis skills.\nDesire to acquire more knowledge to keep up to speed with the ever-evolving field of data science.\nCuriosity to sift through data to find answers and more insights.\nAdvanced understanding of the information technology industry within a matrixed organization and the typical business problems such organizations face.\nStrong ability to translate technical findings clearly and fluently to non-technical team business stakeholders to enable informed decision-making.\nStrong ability to create a storyline around the data to make it easy to interpret and understand.\nSelf-driven and able to work independently yet acts as a team player.\nAcademic Qualifications and Certifications:\nBachelors degree or equivalent in Data Science, Business Analytics, Mathematics, Economics, Engineering, Computer Science or a related field.\nRelevant programming certification preferred.\nAgile certification preferred.\nRequired Experience:\nAdvanced demonstrated experience in a data science position in a corporate environment and/or related industry.\nAdvanced demonstrated experience in statistical modelling and data modelling, machine learning, data mining, unstructured data analytics, natural language processing.\nAdvanced demonstrated experience in programming languages (R, Python, etc.).\nAdvanced demonstrated experience working with and creating data architectures.\nAdvanced demonstrated experience with extracting, cleaning, and transforming data and working with data owners to understand the data.\nAdvanced demonstrated experience visualizing and/or presenting data for stakeholder use and reuse across the business.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data science', 'R', 'data modelling', 'data mining', 'statistical modelling', 'machine learning', 'Python', 'SQL']",2025-06-12 05:32:50
Data Visualization Expert - Quick sight,"NTT DATA, Inc.",2 - 5 years,Not Disclosed,['Chennai'],"We are currently seeking a Data Visualization Expert - Quick sight to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\n\n\n\n What awaits you/ Job Profile  \n\n   \nDesign and develop data visualizations using Amazon QuickSight to present complex data in clear and understandable Dashboards.\nCreate interactive dashboards and reports that allow end-users to explore data and draw meaningful conclusions.\nWork on Data preparation and ensure the good quality data is used in Visualization.\nCollaborate with data analysts and business stakeholders to understand data requirements, gather insights, and transform raw data into actionable visualizations.\nEnsure that the data visualizations are user-friendly, intuitive, and aesthetically pleasing. Optimize the user experience by incorporating best practices.\nIdentify and address performance bottlenecks in data queries and visualization.\nEnsure compliance with data security policies and governance guidelines when handling sensitive data within QuickSight.\nProvide training and support to end-users and stakeholders on how to interact with Dashboards.\nSelf-Managing and explore the latest technical development and incorporate in the project.\nExperience in analytics, reporting and business intelligence tools.\nUsing the Agile Methodology, attending daily standups and use of the Agile tools.\nLead Technical discussions with customers to find the best possible solutions.\n\n What should you bring along  \n\n   \n\n Must Have  \nOverall experience of 2-5 years in Data visualization development.\nMinimum of 2 years in QuickSight and 1-2 years in other BI Tools like Tableau, PowerBI, Qlik\nGood In writing complex SQL Scripting, Dataset Modeling.\nHands on in AWS -Athena, RDS, S3, IAM, permissions, Logging and monitoring Services.\nExperience working with various data sources and databases like Oracle, mySQL, S3, Athena.\nAbility to work with large datasets and design efficient data models for visualization.\nPrior experience in working in Agile, Scrum/Kanban working model.\n\n Nice to Have  \nKnowledge on Data ingestion and Data pipeline in AWS.\nKnowledge Amazon Q or AWS LLM Service to enable AI integration\n\n Must have skill  \n\nQuick sight, Tableau, SQL , AWS\n\n Good to have skills  \n\nQlikview ,Data Engineer, AWS LLM\n\n\n\n\n\n",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['sql', 'data visualization', 'quicksight', 'tableau', 'aws', 'hive', 'bi', 'data warehousing', 'dashboards', 'bi tools', 'iam', 'spark', 'kanban', 'ssrs', 'mysql', 'hadoop', 'etl', 'python', 'oracle', 'data analysis', 'power bi', 'amazon rds', 'sql server', 'qlikview', 'sql scripting', 'scrum', 'athena', 'agile', 'ssis']",2025-06-12 05:32:54
AVP Data Management Analyst,Wells Fargo,2 - 7 years,Not Disclosed,['Bengaluru'],"About this role:\nWells Fargo is seeking a Data Management Analyst\n\nIn this role, you will:\nParticipate in less complex analysis to identify and remediate data quality or integrity issues and to identify and remediate process or control gaps\nAdhere to data governance standards and procedures",,,,"['Data Management', 'Agile Methodology', 'Funds Transfer Pricing', 'Financial Data Mapping', 'Big Data Query Techniques', 'Lineage Tracing', 'Data warehousing', 'Data Governance', 'Jira', 'Market Risks', 'SQL']",2025-06-12 05:32:56
Data Visualization Expert - Quick sight,"NTT DATA, Inc.",4 - 5 years,Not Disclosed,['Chennai'],"We are currently seeking a Data Visualization Expert - Quick sight to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\n\n\n\n What awaits you/ Job Profile  \n\n\n\n Location Bangalore and Chennai, Hybrid mode,Immediate to 10 Days Notice period \nDevelop reports using Amazon Quicksight\nData Visualization DevelopmentDesign and develop data visualizations using Amazon Quicksight to present complex data in a clear and understandable format. Create interactive dashboards and reports that allow end-users to explore data and draw meaningful conclusions.\nData AnalysisCollaborate with data analysts and business stakeholders to understand data requirements, gather insights, and transform raw data into actionable visualizations.\nDashboard User Interface (UI) and User Experience (UX)Ensure that the data visualizations are user-friendly, intuitive, and aesthetically pleasing. Optimize the user experience by incorporating best practices in UI/UX design.\nData IntegrationWork closely with data engineers and data architects to ensure seamless integration of data sources into Quicksight, enabling real-time and up-to-date visualizations.\nPerformance OptimizationIdentify and address performance bottlenecks in data queries and visualization rendering to ensure quick and responsive dashboards.\nData Security and GovernanceEnsure compliance with data security policies and governance guidelines when handling sensitive data within Quicksight.\nTraining and DocumentationProvide training and support to end-users and stakeholders on how to interact with and interpret visualizations effectively. Create detailed documentation of the visualization development process.\nStay Updated with Industry TrendsKeep up to date with the latest data visualization trends, technologies, and best practices to continuously enhance the quality and impact of visualizations.\nUsing the Agile Methodology, attending daily standups and use of the Agile tools\nCollaborating with cross-functional teams and stakeholders to ensure data security, privacy, and compliance with regulations.\nusing Scrum/Kanban\nProficiency in Software Development best practices - Secure coding standards, Unit testing frameworks, Code coverage, Quality gates.\nAbility to lead and deliver change in a very productive way\nLead Technical discussions with customers to find the best possible solutions.\nW orking closely with the Project Manager, Solution Architect and managing client communication (as and when required)\n\n What should you bring along  \n\n\n\nMust Have\nPerson should have relevant work experience in analytics, reporting and business intelligence tools.\n4-5 years of hands-on experience in data visualization.\nRelatively 2-year Experience developing visualization using Amazon Quicksight.\nExperience working with various data sources and databases.\nAbility to work with large datasets and design efficient data models for visualization.\n\n\n\nNice to Have\nAI Project implementation and AI methods.\n\n Must have technical skill  \n\nQuick sight , SQL , AWS\n\n Good to have Technical skills  \n\nTableau, Data Engineer\n\n\n\n",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'data visualization', 'quicksight', 'software development', 'aws', 'hive', 'amazon redshift', 'unit testing', 'data warehousing', 'dashboards', 'business intelligence', 'spark', 'kanban', 'hadoop', 'etl', 'python', 'data analysis', 'ux', 'power bi', 'sql server', 'tableau', 'scrum', 'athena', 'agile', 'ssis']",2025-06-12 05:32:59
Data Scientist,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nThe Data Scientist is responsible for developing and implementing AI-driven solutions to enhance cybersecurity measures within the organization. This role involves leveraging data science techniques to analyze security data, detect threats, and automate security processes. The Data Scientist will work closely with cybersecurity teams to identify data-driven automation opportunities, strengthening the organizations security posture.\nRoles & Responsibilities:\nDevelop analytics to address security concerns, enhancements, and capabilities to improve the organization's security posture.\nCollaborate with Data Engineers to translate security-focused algorithms into effective solutions.\nWork in technical teams in development, deployment, and application of applied analytics, predictive analytics, and prescriptive analytics.\nPerform exploratory and targeted data analyses using descriptive statistics and other methods to identify security patterns and anomalies.\nDesign and implement security-focused analytics pipelines leveraging MLOps practices.\nCollaborate with data engineers on data quality assessment, data cleansing, and the development of security-related data pipelines.\nContribute to data engineering efforts to refine data infrastructure and ensure scalable, efficient security analytics.\nGenerate reports, annotated code, and other projects artifacts to document, archive, and communicate your work and outcomes.\nShare and discuss findings with team members practicing SAFe Agile delivery model.\nFunctional Skills:\nBasic Qualifications:\nMasters degree and 1 to 3 years of experience with one or more analytic software tools or languages (e.g., SAS, SPSS, R, Python) OR\nBachelors degree and 3 to 5 years of experience with one or more analytic software tools or languages (e.g., SAS, SPSS, R, Python) OR\nDiploma and 7 to 9 years of experience with one or more analytic software tools or languages (e.g., SAS, SPSS, R, Python)\nPreferred Qualifications:\nExperience with one or more analytic software tools or languages (e.g., SAS, SPSS, R, Python)\nDemonstrated skill in the use of applied analytics, descriptive statistics, feature extraction and predictive analytics on industrial datasets\nStrong foundation in machine learning algorithms and techniques\nExperience in statistical techniques and hypothesis testing, experience with regression analysis, clustering and classification\nGood-to-Have Skills:\nProficiency in Python and relevant ML libraries (e.g., TensorFlow, PyTorch, Scikit-learn)\nOutstanding analytical and problem-solving skills; Ability to learn quickly; Excellent communication and interpersonal skills\nExperience with data engineering and pipeline development\nExperience in analyzing time-series data for forecasting and trend analysis\nExperience with AWS, Azure, or Google Cloud\nExperience with Databricks platform for data analytics and MLOps\nExperience with Generative AI models (e.g., GPT, DALLE, Stable Diffusion) and their applications in cybersecurity and data analysis\nExperience working in Product team's environment\nExperience working in an Agile environment\nProfessional Certifications:\nAny AWS Developer certification (preferred)\nAny Python and ML certification (preferred)\nAny SAFe Agile certification (preferred)\nSoft Skills:\nInitiative to explore alternate technology and approaches to solving problems\nSkilled in breaking down problems, documenting problem statements, and estimating efforts\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data science', 'R', 'PyTorch', 'SAS', 'predictive analytics', 'Scikit-learn', 'SPSS', 'machine learning', 'data engineering', 'Python', 'TensorFlow']",2025-06-12 05:33:01
Enterprise Data Operations Manager,Pepsico,12 - 17 years,Not Disclosed,['Hyderabad'],"Overview\n\nDeputy Director - Data Engineering\n\nPepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT. The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCos global business scale to enable business insights, advanced analytics, and new product development. PepsiCos Data Management and Operations team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations, and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.\nIncrease awareness about available data and democratize access to it across the company.\nAs a data engineering lead, you will be the key technical expert overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be empowered to create & lead a strong team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premises data sources as well as cloud and remote systems.\nResponsibilities\n\nData engineering lead role for D&Ai data modernization (MDIP)\n\nIdeally Candidate must be flexible to work an alternative schedule either on tradition work week from Monday to Friday; or Tuesday to Saturday or Sunday to Thursday depending upon coverage requirements of the job. The candidate can work with immediate supervisor to change the work schedule on rotational basis depending on the product and project requirements.\nResponsibilities\nManage a team of data engineers and data analysts by delegating project responsibilities and managing their flow of work as well as empowering them to realize their full potential.\nDesign, structure and store data into unified data models and link them together to make the data reusable for downstream products.\nManage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.\nCreate reusable accelerators and solutions to migrate data from legacy data warehouse platforms such as Teradata to Azure Databricks and Azure SQL.\nEnable and accelerate standards-based development prioritizing reuse of code, adopt test-driven development, unit testing and test automation with end-to-end observability of data\nBuild and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality, performance and cost.\nCollaborate with internal clients (product teams, sector leads, data science teams) and external partners (SI partners/data providers) to drive solutioning and clarify solution requirements.\nEvolve the architectural capabilities and maturity of the data platform by engaging with enterprise architects to build and support the right domain architecture for each application following well-architected design standards.\nDefine and manage SLAs for data products and processes running in production.\nCreate documentation for learnings and knowledge transfer to internal associates.\nQualifications\n\n12+ years of engineering and data management experience\n\nQualifications\n12+ years of overall technology experience that includes at least 5+ years of hands-on software development, data engineering, and systems architecture.\n8+ years of experience with Data Lakehouse, Data Warehousing, and Data Analytics tools.\n6+ years of experience in SQL optimization and performance tuning on MS SQL Server, Azure SQL or any other popular RDBMS\n6+ years of experience in Python/Pyspark/Scala programming on big data platforms like Databricks\n4+ years in cloud data engineering experience in Azure or AWS.\nFluent with Azure cloud services. Azure Data Engineering certification is a plus.\nExperience with integration of multi cloud services with on-premises technologies.\nExperience with data modelling, data warehousing, and building high-volume ETL/ELT pipelines.\nExperience with data profiling and data quality tools like Great Expectations.\nExperience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.\nExperience with at least one business intelligence tool such as Power BI or Tableau\nExperience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.\nExperience with version control systems like ADO, Github and CI/CD tools for DevOps automation and deployments.\nExperience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.\nExperience with Statistical/ML techniques is a plus.\nExperience with building solutions in the retail or in the supply chain space is a plus.\nUnderstanding of metadata management, data lineage, and data glossaries is a plus.\nBA/BS in Computer Science, Math, Physics, or other technical fields.\nCandidate must be flexible to work an alternative work schedule either on tradition work week from Monday to Friday; or Tuesday to Saturday or Sunday to Thursday depending upon product and project coverage requirements of the job.\nCandidates are expected to be in the office at the assigned location at least 3 days a week and the days at work needs to be coordinated with immediate supervisor\nSkills, Abilities, Knowledge:\nExcellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.\nProven track record of leading, mentoring data teams.\nStrong change manager. Comfortable with change, especially that which arises through company growth.\nAbility to understand and translate business requirements into data and technical requirements.\nHigh degree of organization and ability to manage multiple, competing projects and priorities simultaneously.\nPositive and flexible attitude to enable adjusting to different needs in an ever-changing environment.\nStrong leadership, organizational and interpersonal skills; comfortable managing trade-offs.\nFoster a team culture of accountability, communication, and self-management.\nProactively drives impact and engagement while bringing others along.\nConsistently attain/exceed individual and team goals.\nAbility to lead others without direct authority in a matrixed environment.\nComfortable working in a hybrid environment with teams consisting of contractors as well as FTEs spread across multiple PepsiCo locations.\nDomain Knowledge in CPG industry with Supply chain/GTM background is preferred.",Industry Type: Beverage,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Pyspark', 'Azure', 'Power BI', 'Github', 'Azure Databricks', 'Tableau', 'ADO', 'Scala programming', 'SQL', 'Azure Data Factory', 'Azure Machine learning', 'Data Lakehouse', 'Azure Data Engineering', 'CI/CD', 'Data Warehousing', 'Data Analytics', 'AWS', 'Python']",2025-06-12 05:33:03
Data Science Manager job opening at GlobalData(Hyd),Globaldata,10 - 15 years,Not Disclosed,['Hyderabad( Kondapur )'],"Hello,\n\nUrgent job openings for Data Science Manager role @ GlobalData(Hyd).\n\nJob Description given below please go through to understand the requirement.\nif requirement is matching to your profile & interested to apply please share your updated resume @ mail id (m.salim@globaldata.com).",,,,"['Artificial Intelligence', 'Natural Language Processing', 'Machine Learning', 'Deep Learning', 'Data Science', 'Tensorflow', 'Predictive Modeling', 'Azure', 'Power Bi', 'Tableau', 'NLP', 'Hadoop Spark', 'AWS', 'Python']",2025-06-12 05:33:06
Data & AI Technical Solution ArchitectsData & AI Technical Solution,"NTT DATA, Inc.",12 - 15 years,Not Disclosed,['Pune'],"Title : Data & AI Technical Solution ArchitectsData & AI Technical Solution Architects\n\nReq ID: 323749\n\nWe are currently seeking a Data & AI Technical Solution ArchitectsData & AI Technical Solution Architects to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nJob DutiesThe Data & AI Architect is a seasoned level expert who is responsible for participating in the delivery of multi-technology consulting services to clients by providing strategies and solutions on all aspects of infrastructure and related technology components.\n\nThis role collaborates with other stakeholders on the development of the architectural approach for one or more layer of a solution. This role has the primary objective is to work on strategic projects that ensure the optimal functioning of the client""™s technology infrastructure.\n""¢ Key Responsibilities:\n""¢ Ability and experience to have conversations with the CEO, Business owners and CTO/CDO\n""¢ Break down intricate business challenges, devise effective solutions, and focus on client needs.\n""¢ Craft high level innovative solution approach for complex business problems\n""¢ Utilize best practices and creativity to address challenges\n""¢ Leverage market research, formulate perspectives, and communicate insights to clients\n""¢ Establish strong client relationships\n""¢ Interact at appropriate levels to ensure client satisfaction\n""¢ Knowledge and Attributes:\n""¢ Ability to focus on detail with an understanding of how it impacts the business strategically.\n""¢ Excellent client service orientation.\n""¢ Ability to work in high-pressure situations.\n""¢ Ability to establish and manage processes and practices through collaboration and the understanding of business.\n""¢ Ability to create new and repeat business for the organization.\n""¢ Ability to contribute information on relevant vertical markets\n""¢ Ability to contribute to the improvement of internal effectiveness by contributing to the improvement of current methodologies, processes and tools.\n\nMinimum Skills RequiredAcademic Qualifications and Certifications:\n""¢ BE/BTech or equivalent in Information Technology and/or Business Management or a related field.\n""¢ Scaled Agile certification desirable.\n""¢ Relevant consulting and technical certifications preferred, for example TOGAF.\n\nRequired Experience12-15 years\n""¢ Seasoned demonstrable experience in a similar role within a large scale (preferably multi- national) technology services environment.\n""¢ Very good understanding of Data, AI, Gen AI and Agentic AI\n""¢ Must have Data Architecture and Solutioning experience. Capable of E2E Data Architecture and GenAI Solution design.\n""¢ Must be able to work on Data & AI RFP responses as Solution Architect\n""¢ 10+ years of experience in Solution Architecting of Data & Analytics, AI/ML & Gen AI Technical Architect\n""¢ Develop Cloud-native technical approach and proposal plans identifying the best practice solutions meeting the requirements for a successful proposal. Create, edit, and review documents, diagrams, and other artifacts in response to RPPs RFQs and Contribute to and participate in presentations to customers regarding proposed solutions.\n""¢ Proficient with Snowflake, Databricks, Azure, AWS, GCP cloud, Data Engineering & AI tools\n""¢ Experience with large scale consulting and program execution engagements in AI and data\n""¢ Seasoned multi-technology infrastructure design experience.\n""¢ Seasoned demonstrable level of expertise coupled with consulting and client engagement experience, demonstrating good experience in client needs assessment and change management.\n""¢ Additional\nAdditional\nAdditional Career Level Description:\nKnowledge and application:\n""¢ Seasoned, experienced professional; has complete knowledge and understanding of area of specialization.\n""¢ Uses evaluation, judgment, and interpretation to select right course of action.\nProblem solving:\n""¢ Works on problems of diverse scope where analysis of information requires evaluation of identifiable factors.\n""¢ Resolves and assesses a wide range of issues in creative ways and suggests variations in approach.\nInteraction:\n""¢ Enhances relationships and networks with senior internal/external partners who are not familiar with the subject matter often requiring persuasion.\n""¢ Works",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'microsoft azure', 'data engineering', 'data bricks', 'aws', 'needs assessment', 'client engagement', 'ai solutions', 'togaf', 'aiml', 'data architecture', 'solution architecting', 'artificial intelligence', 'change management', 'gen', 'service orientation', 'gcp', 'gcp cloud', 'ml']",2025-06-12 05:33:08
Data & AI Technical Solution Architects,"NTT DATA, Inc.",12 - 15 years,Not Disclosed,['Pune'],"Req ID: 323754\n\nWe are currently seeking a Data & AI Technical Solution Architects to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nJob DutiesThe Data & AI Architect is a seasoned level expert who is responsible for participating in the delivery of multi-technology consulting services to clients by providing strategies and solutions on all aspects of infrastructure and related technology components.\n\nThis role collaborates with other stakeholders on the development of the architectural approach for one or more layer of a solution. This role has the primary objective is to work on strategic projects that ensure the optimal functioning of the client""™s technology infrastructure.\n""¢ Key Responsibilities:\n""¢ Ability and experience to have conversations with the CEO, Business owners and CTO/CDO\n""¢ Break down intricate business challenges, devise effective solutions, and focus on client needs.\n""¢ Craft high level innovative solution approach for complex business problems\n""¢ Utilize best practices and creativity to address challenges\n""¢ Leverage market research, formulate perspectives, and communicate insights to clients\n""¢ Establish strong client relationships\n""¢ Interact at appropriate levels to ensure client satisfaction\n""¢ Knowledge and Attributes:\n""¢ Ability to focus on detail with an understanding of how it impacts the business strategically.\n""¢ Excellent client service orientation.\n""¢ Ability to work in high-pressure situations.\n""¢ Ability to establish and manage processes and practices through collaboration and the understanding of business.\n""¢ Ability to create new and repeat business for the organization.\n""¢ Ability to contribute information on relevant vertical markets\n""¢ Ability to contribute to the improvement of internal effectiveness by contributing to the improvement of current methodologies, processes and tools.\n\nMinimum Skills RequiredAcademic Qualifications and Certifications:\n""¢ BE/BTech or equivalent in Information Technology and/or Business Management or a related field.\n""¢ Scaled Agile certification desirable.\n""¢ Relevant consulting and technical certifications preferred, for example TOGAF.\n\nRequired Experience12-15 years\n""¢ Seasoned demonstrable experience in a similar role within a large scale (preferably multi- national) technology services environment.\n""¢ Very good understanding of Data, AI, Gen AI and Agentic AI\n""¢ Must have Data Architecture and Solutioning experience. Capable of E2E Data Architecture and GenAI Solution design.\n""¢ Must be able to work on Data & AI RFP responses as Solution Architect\n""¢ 10+ years of experience in Solution Architecting of Data & Analytics, AI/ML & Gen AI Technical Architect\n""¢ Develop Cloud-native technical approach and proposal plans identifying the best practice solutions meeting the requirements for a successful proposal. Create, edit, and review documents, diagrams, and other artifacts in response to RPPs RFQs and Contribute to and participate in presentations to customers regarding proposed solutions.\n""¢ Proficient with Snowflake, Databricks, Azure, AWS, GCP cloud, Data Engineering & AI tools\n""¢ Experience with large scale consulting and program execution engagements in AI and data\n""¢ Seasoned multi-technology infrastructure design experience.\n""¢ Seasoned demonstrable level of expertise coupled with consulting and client engagement experience, demonstrating good experience in client needs assessment and change management.\n""¢ Additional\nAdditional\nAdditional Career Level Description:\nKnowledge and application:\n""¢ Seasoned, experienced professional; has complete knowledge and understanding of area of specialization.\n""¢ Uses evaluation, judgment, and interpretation to select right course of action.\nProblem solving:\n""¢ Works on problems of diverse scope where analysis of information requires evaluation of identifiable factors.\n""¢ Resolves and assesses a wide range of issues in creative ways and suggests variations in approach.\nInteraction:\n""¢ Enhances relationships and networks with senior internal/external partners who are not familiar with the subject matter often requiring persuasion.\n""¢ Works",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'microsoft azure', 'data engineering', 'data bricks', 'aws', 'client engagement', 'ai solutions', 'togaf', 'aiml', 'data architecture', 'solution architecting', 'artificial intelligence', 'change management', 'gen', 'service orientation', 'solution design', 'gcp', 'gcp cloud', 'ml']",2025-06-12 05:33:10
Data & AI Technical Solution Architects,"NTT DATA, Inc.",12 - 15 years,Not Disclosed,['Hyderabad'],"Req ID: 323774\n\nWe are currently seeking a Data & AI Technical Solution Architects to join our team in Hyderabad, Telangana (IN-TG), India (IN).\n\n""Job DutiesThe Data & AI Architect is a seasoned level expert who is responsible for participating in the delivery of multi-technology consulting services to clients by providing strategies and solutions on all aspects of infrastructure and related technology components.\n\nThis role collaborates with other stakeholders on the development of the architectural approach for one or more layer of a solution. This role has the primary objective is to work on strategic projects that ensure the optimal functioning of the client""™s technology infrastructure.\n""¢ Key Responsibilities:\n""¢ Ability and experience to have conversations with the CEO, Business owners and CTO/CDO\n""¢ Break down intricate business challenges, devise effective solutions, and focus on client needs.\n""¢ Craft high level innovative solution approach for complex business problems\n""¢ Utilize best practices and creativity to address challenges\n""¢ Leverage market research, formulate perspectives, and communicate insights to clients\n""¢ Establish strong client relationships\n""¢ Interact at appropriate levels to ensure client satisfaction\n""¢ Knowledge and Attributes:\n""¢ Ability to focus on detail with an understanding of how it impacts the business strategically.\n""¢ Excellent client service orientation.\n""¢ Ability to work in high-pressure situations.\n""¢ Ability to establish and manage processes and practices through collaboration and the understanding of business.\n""¢ Ability to create new and repeat business for the organization.\n""¢ Ability to contribute information on relevant vertical markets\n""¢ Ability to contribute to the improvement of internal effectiveness by contributing to the improvement of current methodologies, processes and tools.\n\nMinimum Skills RequiredAcademic Qualifications and Certifications:\n""¢ BE/BTech or equivalent in Information Technology and/or Business Management or a related field.\n""¢ Scaled Agile certification desirable.\n""¢ Relevant consulting and technical certifications preferred, for example TOGAF.\n\nRequired Experience12-15 years\n""¢ Seasoned demonstrable experience in a similar role within a large scale (preferably multi- national) technology services environment.\n""¢ Very good understanding of Data, AI, Gen AI and Agentic AI\n""¢ Must have Data Architecture and Solutioning experience. Capable of E2E Data Architecture and GenAI Solution design.\n""¢ Must be able to work on Data & AI RFP responses as Solution Architect\n""¢ 10+ years of experience in Solution Architecting of Data & Analytics, AI/ML & Gen AI Technical Architect\n""¢ Develop Cloud-native technical approach and proposal plans identifying the best practice solutions meeting the requirements for a successful proposal. Create, edit, and review documents, diagrams, and other artifacts in response to RPPs RFQs and Contribute to and participate in presentations to customers regarding proposed solutions.\n""¢ Proficient with Snowflake, Databricks, Azure, AWS, GCP cloud, Data Engineering & AI tools\n""¢ Experience with large scale consulting and program execution engagements in AI and data\n""¢ Seasoned multi-technology infrastructure design experience.\n""¢ Seasoned demonstrable level of expertise coupled with consulting and client engagement experience, demonstrating good experience in client needs assessment and change management.\n""¢ Additional\nAdditional\nAdditional Career Level Description:\nKnowledge and application:\n""¢ Seasoned, experienced professional; has complete knowledge and understanding of area of specialization.\n""¢ Uses evaluation, judgment, and interpretation to select right course of action.\nProblem solving:\n""¢ Works on problems of diverse scope where analysis of information requires evaluation of identifiable factors.\n""¢ Resolves and assesses a wide range of issues in creative ways and suggests variations in approach.\nInteraction:\n""¢ Enhances relationships and networks with senior internal/external partners who are not familiar with the subject matter often requiring persuasion.\n""¢ Works""",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'microsoft azure', 'data engineering', 'data bricks', 'aws', 'client engagement', 'ai solutions', 'togaf', 'aiml', 'data architecture', 'solution architecting', 'artificial intelligence', 'change management', 'gen', 'service orientation', 'solution design', 'gcp', 'gcp cloud', 'ml']",2025-06-12 05:33:13
"Big Data Lead (Subject Matter Expert- SME) | Lower Parel, Mumbai",Pracemo Global Solutions,8 - 13 years,35-50 Lacs P.A.,['Mumbai( Lower Parel )'],"Hiring Big Data Lead with 8+ years experience for US Shift time:\n\nMust Have:\n- Big Data: Spark, Hadoop, Kafka, Hive, Flink\n- Backend: Python, Scala\n- NoSQL: MongoDB, Cassandra\n- Cloud: AWS/AZURE/GCP, Snowflake, Databricks\n- Docker, Kubernetes, CI/CD\n\nRequired Candidate profile\n- Excellent in Mentoring/ Training in Big Data- HDFS, YARN, Airflow, Hive, Mapreduce, Hbase, Kafka & ETL/ELT, real-time streaming, data modeling\n- Immediate Joiner is plus\n- Excellent in Communication",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Hadoop', 'Big Data', 'Hive', 'SCALA', 'Spark', 'Cloudera', 'Cassandra', 'Kafka', 'Flume', 'Mapreduce', 'Hdfs', 'Impala', 'Spark Streaming', 'YARN', 'HBase', 'Apache Zookeeper', 'Apache Storm', 'NoSQL', 'Apache Pig', 'Sqoop', 'Kudu', 'MongoDB', 'Oozie']",2025-06-12 05:33:15
Senior Data Developer-MSBI,Intelliswift software,9 - 14 years,Not Disclosed,['Gurugram'],"Job Title: Senior Data Developer MSBI\nKey Responsibilities:\nDevelop new OLAP cubes using SSAS.\nDesign and implement robust ETL workflows using SSIS.\nBuild and maintain SSRS reports, including development of new static reports.\nLead new data integration projects and complex ETL flow implementations.\nPerform in-depth maintenance and optimization of existing cubes and data pipelines.\nCollaborate closely with Samsung Electronics and SDSE clients, particularly with the Samsung Nordics BI team.\nGather business requirements, provide solutions, and ensure timely delivery.\nUphold best coding practices, clean code principles, and performance optimization.\nRequired Skills:\nDeep expertise in MSBI Stack – SSIS, SSAS, SSRS.\nStrong command of MSSQL and relational database concepts.\nHands-on experience in data warehousing, OLAP cube development, and BI reporting.\nProven experience working in large enterprise environments with cross-functional teams.\nExcellent communication, problem-solving, and stakeholder management skills.",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['MS SQL', 'Tabular cube', 'SSAS', 'MSBI', 'SSIS', 'MSBI Developer']",2025-06-12 05:33:17
Data Analyst - L4,Wipro,5 - 8 years,Not Disclosed,['Hyderabad'],"Role Purpose\nThe purpose of this role is to interpret data and turn into information (reports, dashboards, interactive visualizations etc) which can offer ways to improve a business, thus affecting business decisions.\nDo\n1. Managing the technical scope of the project in line with the requirements at all stages\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\nb. Develop record management process and policies\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\nd. Providing sales data, proposals, data insights and account reviews to the client base\ne. Identify areas to increase efficiency and automation of processes\nf. Set up and maintain automated data processes\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\nh. Produce and track key performance indicators\n2. Analyze the data sets and provide adequate information\na. Liaise with internal and external clients to fully understand data content\nb. Design and carry out surveys and analyze survey data as per the customer requirement\nc. Analyze and interpret complex data sets relating to customers business and prepare reports for internal and external audiences using business analytics reporting tools\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\nf. Develop predictive models and share insights with the clients as per their requirement\n\nDeliver\n\nNo Performance Parameter Measure\n1. Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\nMandatory Skills: Tableau.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'Tableau', 'data warehouses', 'data integration', 'Data modelling']",2025-06-12 05:33:20
Data Science - Director job opening at GlobalData(Hyderabad),Globaldata,15 - 20 years,Not Disclosed,['Hyderabad( Kondapur )'],"R\nHello,\n\nUrgent job openings for Data Science - Director @ GlobalData(Hyderebad)\n\nJob Description given below please go through to understand the requirement.\n\nif requirement is matching to your profile & interested to apply please share your updated resume @ mail id (m.salim@globaldata.com).",,,,"['Data Science', 'Pytorch', 'Generative Ai Tools', 'Large Language Model', 'Python', 'Tensorflow', 'Natural Language Processing', 'Deep Learning']",2025-06-12 05:33:22
Field Engineer,Genpact,0 - 5 years,Not Disclosed,['Bengaluru'],"Genpact (NYSE: G) is a global professional services and solutions firm delivering outcomes that shape the future. Our 125,000+ people across 30+ countries are driven by our innate curiosity, entrepreneurial agility, and desire to create lasting value for clients. Powered by our purpose the relentless pursuit of a world that works better for people – we serve and transform leading enterprises, including the Fortune Global 500, with our deep business and industry knowledge, digital operations services, and expertise in data, technology, and AI.\nInviting applications for the role of Field Data Engineer!\nIn this role - The team developing industry leading X-ray generation subsystems (ie. tubes and generators) is looking for highly motivated engineers to join them in the efforts to monitor and maintain the large installed base of the imaging scanners that use these X-ray generation subsystem in the field.",,,,"['Field Engineering', 'Field Support']",2025-06-12 05:33:24
Specialist Data Scientist,NICE,8 - 11 years,Not Disclosed,['Pune'],"So, what’s the role all about?\nNICE provides state-of-the-art enterprise level AI and analytics for all forms of business communications between speech and digital.   We are a world class research team developing new algorithms and approaches to help companies with solving critical issues such as identifying their best performing agents, preventing fraud, categorizing customer issues, and determining overall customer satisfaction.  If you have interacted with a major contact center in the last decade, it is very likely we have processed your call. \nThe research group partners with all areas of NICE’s business to scale out the delivery of new technology and AI models to customers around the world that are tailored to their company, industry, and language needs.",,,,"['python', 'confluence', 'natural language processing', 'presentation skills', 'big data technologies', 'pyspark', 'microsoft azure', 'bert', 'machine learning', 'sql', 'tensorflow', 'data science', 'gcp', 'pytorch', 'machine learning algorithms', 'aws', 'big data', 'communication skills', 'statistics', 'jira']",2025-06-12 05:33:27
Business Data Analyst,CGI,5 - 8 years,Not Disclosed,['Hyderabad'],"Business Data Analyst - HealthCare\n\nJob Summary\nWe are seeking an experienced and results-driven Business Data Analyst with 5+ years of hands-on experience in data analytics, visualization, and business insight generation. This role is ideal for someone who thrives at the intersection of business and datatranslating complex data sets into compelling insights, dashboards, and strategies that support decision-making across the organization.\nYou will collaborate closely with stakeholders across departments to identify business needs, design and build analytical solutions, and tell compelling data stories using advanced visualization tools.\nKey Responsibilities\nData Analytics & Insights Analyze large and complex data sets to identify trends, anomalies, and opportunities that help drive business strategy and operational efficiency.\n• Dashboard Development & Data Visualization Design, develop, and maintain interactive dashboards and visual reports using tools like Power BI, Tableau, or Looker to enable data-driven decisions.\n• Business Stakeholder Engagement Collaborate with cross-functional teams to understand business goals, define metrics, and convert ambiguous requirements into concrete analytical deliverables.\n• KPI Definition & Performance Monitoring Define, track, and report key performance indicators (KPIs), ensuring alignment with business objectives and consistent measurement across teams.\n• Data Modeling & Reporting Automation Work with data engineering and BI teams to create scalable, reusable data models and automate recurring reports and analysis processes.\n• Storytelling with Data Communicate findings through clear narratives supported by data visualizations and actionable recommendations to both technical and non-technical audiences.\n• Data Quality & Governance Ensure accuracy, consistency, and integrity of data through validation, testing, and documentation practices.\nRequired Qualifications\nBachelor’s or Master’s degree in Business, Economics, Statistics, Computer Science, Information Systems, or a related field.\n• 5+ years of professional experience in a data analyst or business analyst role with a focus on data visualization and analytics.\n• Proficiency in data visualization tools: Power BI, Tableau, Looker (at least one).\n• Strong experience in SQL and working with relational databases to extract, manipulate, and analyze data.\n• Deep understanding of business processes, KPIs, and analytical methods.\n• Excellent problem-solving skills with attention to detail and accuracy.\n• Strong communication and stakeholder management skills with the ability to explain technical concepts in a clear and business-friendly manner.\n• Experience working in Agile or fast-paced environments.\nPreferred Qualifications\nExperience working with cloud data platforms (e.g., Snowflake, BigQuery, Redshift).\n• Exposure to Python or R for data manipulation and statistical analysis.\n• Knowledge of data warehousing, dimensional modeling, or ELT/ETL processes.\n• Domain experience in Healthcare is a plus.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Bigquery', 'Snowflake', 'Data Warehousing', 'Redshift', 'Python', 'ETL']",2025-06-12 05:33:30
Senior Data Manager/ Lead,Codeforce 360,6 - 8 years,Not Disclosed,['Hyderabad'],"Job Description:\nWe are looking for a highly experienced and dynamic Senior Data Manager / Lead to oversee a team of Data Engineers and Data Scientists. This role demands a strong background in data platforms such as Snowflake and proficiency in Python, combined with excellent people management and project leadership skills. While hands-on experience in the technologies is beneficial, the primary focus of this role is on team leadership, strategic planning, and project delivery .\n\nJob Title : Senior Data Manager / Lead\nLocation: Hyderabad (Work From Office)\nShift Timing: 10AM-7PM\nKey Responsibilities:\nLead, mentor, and manage a team of Data Engineers and Data Scientists.\nOversee the design and implementation of data pipelines and analytics solutions using Snowflake and Python.\nCollaborate with cross-functional teams (product, business, engineering) to align data solutions with business goals.\nEnsure timely delivery of projects, with high quality and performance.\nConduct performance reviews, training plans, and support career development for the team.\nSet priorities, allocate resources, and manage workloads within the data team.\nDrive adoption of best practices in data management, governance, and documentation.\nEvaluate new tools and technologies relevant to data engineering and data science.\n\nRequired Skills & Qualifications:\n6+ years of experience in data-related roles, with at least 23 years in a leadership or management position.\nStrong understanding of Snowflake architecture, performance tuning, data sharing, security, etc.\nSolid knowledge of Python for data engineering or data science tasks.\nExperience in leading data migration, ETL/ELT, and analytics projects.\nAbility to translate business requirements into technical solutions.\nExcellent leadership, communication, and stakeholder management skills.\nExposure to tools like Databricks, Dataiku, Airflow, or similar platforms is a plus.\nBachelors or Master’s degree in Computer Science, Engineering, Mathematics, or a related field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Data Bricks', 'Python', 'Airflow', 'Data Migration', 'Dataiku', 'Data Warehousing', 'ETL', 'ELT', 'SQL']",2025-06-12 05:33:33
Senior Data Scientist,Epsilon,6 - 9 years,Not Disclosed,['Bengaluru'],"Responsibilities: -\nContribute and build an internal product library that is focused on solving business problems related to prediction & recommendation.\nResearch unfamiliar methodologies, techniques to fine tune existing models in the product suite and, recommend better solutions and/or technologies.\nImprove features of the product to include newer machine learning algorithms in the likes of product recommendation, real time predictions, fraud detection, offer personalization etc\nCollaborate with client teams to on-board data, build models and score predictions.\nParticipate in building automations and standalone applications around machine learning algorithms to enable a One Click solution to getting predictions and recommendations.\nAnalyze large datasets, perform data wrangling operations, apply statistical treatments to filter and fine tune input data, engineer new features and eventually aid the process of building machine learning models.\nRun test cases to tune existing models for performance, check criteria and define thresholds for success by scaling the input data to multifold.\nDemonstrate a basic understanding of different machine learning concepts such as Regression, Classification, Matrix Factorization, K-fold Validations and different algorithms such as Decision Trees, Random Forrest, K-means clustering.\nDemonstrate working knowledge and contribute to building models using deep learning techniques, ensuring robust, scalable and high-performance solutions\nMinimum Qualifications:\nEducation: Master's or PhD in a quantitative discipline (Statistics, Economics, Mathematics, Computer Science) is highly preferred.\nDeep Learning Mastery: Extensive experience with deep learning frameworks (TensorFlow, PyTorch, or Keras) and advanced deep learning projects across various domains, with a focus on multimodal data applications.\nGenerative AI Expertise: Proven experience with generative AI models and techniques, such as RAG, VAEs, Transformers, and applications at scale in content creation or data augmentation.\nProgramming and Big Data: Expert-level proficiency in Python and big data/cloud technologies (Databricks and Spark) with a minimum of 4-5 years of experience.\nRecommender Systems and Real-time Predictions: Expertise in developing sophisticated recommender systems, including the application of real-time prediction frameworks.\nMachine Learning Algorithms: In-depth experience with complex algorithms such as logistic regression, random forest, XGBoost, advanced neural networks, and ensemble methods.\nExperienced with machine learning algorithms such as logistic regression, random forest, XG boost, KNN, SVM, neural network, linear regression, lasso regression and k-means.\nDesirable Qualifications:\nGenerative AI Tools Knowledge: Proficiency with tools and platforms for generative AI (such as OpenAI, Hugging Face Transformers).\nDatabricks and Unity Catalog: Experience leveraging Databricks and Unity Catalog for robust data management, model deployment, and tracking.\nWorking experience in CI/CD tools such as GIT & BitBucket",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Data Engineering', 'Pyspark', 'Azure Aws', 'Generative AI', 'Big Data', 'AWS', 'Data Bricks', 'Deep Learning', 'Python', 'SQL']",2025-06-12 05:33:36
Big Data Architect,Meritus Management Service,8 - 10 years,20-32.5 Lacs P.A.,"['Nagpur', 'Pune']","Design and implement scalable Big Data architecture and pipelines using tools like Hadoop, Spark, Kafka, and Hive.\nCollaborate with cross-functional teams to build real-time/batch systems, ensure data quality and governance.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Hadoop', 'SQL', 'architect level exposure', 'Pyspark']",2025-06-12 05:33:38
Associate- Referral - Decision Science / Data Science,Axtria,3 - 5 years,Not Disclosed,['Gurugram'],"Position Summary \n\nThis Requisition is for the Employee Referral Campaign.\n\nWe are seeking high-energy, driven, and innovative Data Scientists to join our Data Science Practice to develop new, specialized capabilities for Axtria, and to accelerate the company’s growth by supporting our clients’ commercial & clinical strategies.\n\n Job Responsibilities \n\nBe an Individual Contributor tothe Data Science team and solve real-world problems using cutting-edge capabilities and emerging technologies.\n\nHelp clients translate the business use cases they are trying to crack into data science solutions. Provide genuine assistance to users by advising them on how to leverage Dataiku DSS to implement data science projects, from design to production.\n\nData Source Configuration, Maintenance, Document and maintain work-instructions.\n\nDeep working onmachine learning frameworks such as TensorFlow, Caffe, Keras, SparkML\n\nExpert knowledge in Statistical and Probabilistic methods such as SVM, Decision-Trees, Clustering\n\nExpert knowledge of python data-science and math packages such as NumPy , Pandas, Sklearn\n\nProficiency in object-oriented languages (Java and/or Kotlin),Python and common machine learning frameworks(TensorFlow, NLTK, Stanford NLP, Ling Pipe etc\n\n\n Education \n\nBachelor Equivalent - Engineering\nMaster's Equivalent - Engineering\n\n Work Experience \n\nData Scientist 3-5 years of relevant experience in advanced statistical and mathematical models and predictive modeling using Python. Experience in the data science space prior relevant experience in Artificial intelligence and machine Learning algorithms for developing scalable models supervised and unsupervised techniques likeNLP and deep Learning Algorithms. Ability to build scalable models using Python, R-Studio, R Shiny, PySpark, Keras, and TensorFlow. Experience in delivering data science projects leveraging cloud infrastructure. Familiarity with cloud technology such as AWS / Azure and knowledge of AWS tools such as S3, EMR, EC2, Redshift, and Glue; viz tools like Tableau and Power BI. Relevant experience in Feature Engineering, Feature Selection, and Model Validation on Big Data. Knowledge of self-service analytics platforms such as Dataiku/ KNIME/ Alteryx will be an added advantage.\n\nML Ops Engineering 3-5 years of experience with MLOps Frameworks like Kubeflow, MLFlow, Data Robot, Airflow, etc., experience with Docker and Kubernetes, OpenShift. Prior experience in end-to-end automated ecosystems including, but not limited to, building data pipelines, developing & deploying scalable models, orchestration, scheduling, automation, and ML operations. Ability to design and implement cloud solutions and ability to build MLOps pipelines on cloud solutions (AWS, MS Azure, or GCP). Programming languages like Python, Go, Ruby, or Bash, a good understanding of Linux, knowledge of frameworks such as Keras, PyTorch, TensorFlow, etc. Ability to understand tools used by data scientists and experience with software development and test automation. Good understanding of advanced AI/ML algorithms & their applications.\n\nGen AI :Minimum of 4-6 years develop, test, and deploy Python based applications on Azure/AWS platforms.Must have basic knowledge on concepts of Generative AI / LLMs / GPT.Deep understanding of architecture and work experience on Web Technologies.Python, SQL hands-on experience.Expertise in any popular python web frameworks e.g. flask, Django etc. Familiarity with frontend technologies like HTML, JavaScript, REACT.Be an Individual Contributor in the Analytics and Development team and solve real-world problems using cutting-edge capabilities and emerging technologies based on LLM/GenAI/GPT.Can interact with client on GenAI related capabilities and use cases.",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'gpm', 'machine learning', 'python data', 'statistics', 'kubernetes', 'microsoft azure', 'numpy', 'javascript', 'sql', 'docker', 'pandas', 'tensorflow', 'java', 'django', 'predictive modeling', 'python web framework', 'mathematical modeling', 'pytorch', 'keras', 'aws', 'flask', 'advanced statistical']",2025-06-12 05:33:40
Lead Data Analyst-Business Intelligence,Tresvista Financial Services,6 - 10 years,Not Disclosed,['Bengaluru'],"Roles and Responsibilities\nArchitect and incorporate an effective Data framework enabling end to end Data Solution.\nUnderstand business needs, use cases and drivers for insights and translate them into detailed technical specifications.\nCreate epics, features and user stories with clear acceptance criteria for execution and delivery by the data engineering team.\nCreate scalable and robust data solution designs that incorporate governance, security and compliance aspects.\nDevelop and maintain logical and physical data models and work closely with data engineers, data analysts and data testers for successful implementation of them.\nAnalyze, assess and design data integration strategies across various sources and platforms.\nCreate project plans and timelines while monitoring and mitigating risks and controlling progress of the project.\nConduct daily scrum with the team with a clear focus on meeting sprint goals and timely resolution of impediments.\nAct as a liaison between technical teams and business stakeholders and ensure.\nGuide and mentor the team for best practices on Data solutions and delivery frameworks.\nActively work, facilitate and support the stakeholders/ clients to complete User Acceptance Testing ensure there is strong adoption of the data products after the launch.\nDefining and measuring KPIs/KRA for feature(s) and ensuring the Data roadmap is verified through measurable outcomes\n\nPrerequisites\n5 to 8 years of professional, hands on experience building end to end Data Solution on Cloud based Data Platforms including 2+ years working in a Data Architect role.\nProven hands on experience in building pipelines for Data Lakes, Data Lake Houses, Data Warehouses and Data Visualization solutions\nSound understanding of modern Data technologies like Databricks, Snowflake, Data Mesh and Data Fabric.\nExperience in managing Data Life Cycle in a fast-paced, Agile / Scrum environment.\nExcellent spoken and written communication, receptive listening skills, and ability to convey complex ideas in a clear, concise fashion to technical and non-technical audiences\nAbility to collaborate and work effectively with cross functional teams, project stakeholders and end users for quality deliverables withing stipulated timelines\nAbility to manage, coach and mentor a team of Data Engineers, Data Testers and Data Analysts. Strong process driver with expertise in Agile/Scrum framework on tools like Azure DevOps, Jira or Confluence\nExposure to Machine Learning, Gen AI and modern AI based solutions.\n\nExperience\nTechnical Lead Data Analytics with 6+ years of overall experience out of which 2+ years is on Data architecture.\n\nEducation\nEngineering degree from a Tier 1 institute preferred.\n\nCompensation\nThe compensation structure will be as per industry standards",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'Data Bricks', 'Data Lake', 'Data Warehousing', 'Python', 'Business Intelligence', 'Databricks Engineer', 'Machine Learning', 'Redshift Aws', 'Snowflake', 'Data Visualization', 'ETL', 'Data Mesh']",2025-06-12 05:33:42
Sr. Data Scientist-Stratup-Mid-Size companies Exp.@ Bangalore_Urgent,"A leader in this space, we deliver world...",8 - 13 years,Not Disclosed,['Bengaluru'],"Senior Data Scientist\n\nLocation: Onsite Bangalore\nExperience: 8+ years\n\nRole Overview\n\nWe are seeking a Senior Data Scientist with a strong foundation in machine learning, deep learning, and statistical modeling, with the ability to translate complex operational problems into scalable AI/ML solutions. In addition to core data science responsibilities, the role involves building production-ready backends in Python and contributing to end-to-end model lifecycle management. Exposure to computer vision is a plus, especially for industrial use cases like identification, intrusion detection, and anomaly detection.\n\nKey Responsibilities\n\nDevelop, validate, and deploy machine learning and deep learning models for forecasting, classification, anomaly detection, and operational optimization\nBuild backend APIs using Python (FastAPI, Flask) to serve ML/DL models in production environments\nApply advanced computer vision models (e.g., YOLO, Faster R-CNN) to object detection, intrusion detection, and visual monitoring tasks\nTranslate business problems into analytical frameworks and data science solutions\nWork with data engineering and DevOps teams to operationalize and monitor models at scale\nCollaborate with product, domain experts, and engineering teams to iterate on solution design\nContribute to technical documentation, model explainability, and reproducibility practices\n\n\nRequired Skills\n\nStrong proficiency in Python for data science and backend development\nExperience with ML/DL libraries such as scikit-learn, TensorFlow, or PyTorch\nSolid knowledge of time-series modeling, forecasting techniques, and anomaly detection\nExperience building and deploying APIs for model serving (FastAPI, Flask)\nFamiliarity with real-time data pipelines using Kafka, Spark, or similar tools\nStrong understanding of model validation, feature engineering, and performance tuning\nAbility to work with SQL and NoSQL databases, and large-scale datasets\nGood communication skills and stakeholder engagement experience\n\n\nGood to Have\n\nExperience with ML model deployment tools (MLflow, Docker, Airflow)\nUnderstanding of MLOps and continuous model delivery practices\nBackground in aviation, logistics, manufacturing, or other industrial domains\nFamiliarity with edge deployment and optimization of vision models\n\n\nQualifications\n\nMasters or PhD in Data Science, Computer Science, Applied Mathematics, or related field\n7+ years of experience in machine learning and data science, including end-to-end deployment of models in production",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scikit-learn', 'time-series modeling', 'ML/DL libraries', 'data science', 'Python', 'Airflow', 'Kafka', 'MLflow', 'logistics', 'anomaly detection', 'aviation', 'SQL', 'PyTorch', 'NoSQL', 'MLOps', 'forecasting techniques', 'Docker', 'manufacturing', 'FastAPI', 'Spark', 'TensorFlow', 'Flask']",2025-06-12 05:33:45
Senior Data Scientist,Fastenal,4 - 9 years,Not Disclosed,['Bengaluru'],"Job Description:\nWe are seeking a highly skilled and experienced Senior Data Scientist to join our team. The ideal candidate will have a strong background in data science, machine learning, and statistical analysis. As a Senior Data Scientist, you will be responsible for leading data-driven projects, developing predictive models, and providing actionable insights to drive business decisions.\n\nKey Responsibilities:",,,,"['Data Science', 'SQL', 'Pyspark', 'R', 'Python']",2025-06-12 05:33:47
Data Science_ Lead,Rishabh Software,8 - 13 years,Not Disclosed,"['Ahmedabad', 'Bengaluru', 'Vadodara']","Job Description\n\nWith excellent analytical and problem-solving skills, you should understand business problems of the customers, translate them into scope of work and technical specifications for developing into Data Science projects. Efficiently utilize cutting edge technologies in AI, Generative AI areas and implement solutions for business problems. Good exposure technology platforms for Data Science, AI, Gen AI, cloud with implementation experience. Ability to provide end to end technical solutions leveraging latest AI, Gen AI tools, frameworks for the business problems. This Job requires the following:",,,,"['Data Science', 'gen ai', 'Computer Vision', 'Machine Learning', 'Deep Learning', 'Tensorflow', 'NLP', 'Artificial Intelligence', 'Dl', 'Python']",2025-06-12 05:33:49
Python Engineer - ML/Big Query - Hyd/Chennai/Bangalore,People staffing Solutions,5 - 10 years,12-20 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Key Responsibilities:\nDesign, develop, and maintain scalable and optimized ETL pipelines using Python and SQL.\nWork with Google BigQuery and other cloud-based platforms to build data warehousing solutions.\nDevelop and deploy ML models; collaborate with Data Scientists for productionizing models.\nWrite efficient and optimized SQL queries for large-scale data processing.\nBuild APIs using Flask/Django for machine learning and data applications.\nWork with both SQL and NoSQL databases including Elasticsearch.\nImplement data ingestion using batch and streaming technologies.\nEnsure data quality, integrity, and governance across the data lifecycle.\nAutomate and optimize CI/CD pipelines for data solutions.\nCollaborate with cross-functional teams to gather data requirements and deliver solutions.\nTroubleshoot and monitor data pipelines for seamless operations.\nRequired Skills & Qualifications:\nBachelor's or Master's degree in Computer Science, Engineering, or related field.\n5+ years of experience with Python in a data engineering and/or ML context.\nStrong hands-on experience with SQL, BigQuery, and cloud data platforms (preferably GCP).\nPractical knowledge of ML concepts and experience developing ML models.\nProficiency in frameworks such as Flask and Django.\nExperience with NoSQL databases and data streaming technologies.\nSolid understanding of data modeling, warehousing, and ETL frameworks.\nFamiliarity with CI/CD tools and automation best practices.\nExcellent communication, problem-solving, and collaboration skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Django', 'Machine Learning', 'Python', 'SQL', 'Pandas', 'Numpy', 'Ml', 'Flask']",2025-06-12 05:33:52
"Sr. Data Analyst – Tableau, SQL, Snowflake",Int9 Solutions,5 - 7 years,Not Disclosed,['Bengaluru'],"We are looking for a skilled Data Analyst with excellent communication skills and deep expertise in SQL, Tableau, and modern data warehousing technologies. This role involves designing data models, building insightful dashboards, ensuring data quality, and extracting meaningful insights from large datasets to support strategic business decisions.\n\nKey Responsibilities:\nWrite advanced SQL queries to retrieve and manipulate data from cloud data warehouses such as Snowflake, Redshift, or BigQuery.\nDesign and develop data models that support analytics and reporting needs.\nBuild dynamic, interactive dashboards and reports using tools like Tableau, Looker, or Domo.\nPerform advanced analytics techniques including cohort analysis, time series analysis, scenario analysis, and predictive analytics.\nValidate data accuracy and perform thorough data QA to ensure high-quality output.\nInvestigate and troubleshoot data issues; perform root cause analysis in collaboration with BI or data engineering teams.\nCommunicate analytical insights clearly and effectively to stakeholders.\n\nRequired Skills & Qualifications:\nExcellent communication skills are mandatory for this role.\n5+ years of experience in data analytics, BI analytics, or BI engineering roles.\nExpert-level skills in SQL, with experience writing complex queries and building views.\nProven experience using data visualization tools like Tableau, Looker, or Domo.\nStrong understanding of data modeling principles and best practices.\nHands-on experience working with cloud data warehouses such as Snowflake, Redshift, BigQuery, SQL Server, or Oracle.\nIntermediate-level proficiency with spreadsheet tools like Excel, Google Sheets, or Power BI, including functions, pivots, and lookups.\nBachelor's or advanced degree in a relevant field such as Data Science, Computer Science, Statistics, Mathematics, or Information Systems.\nAbility to collaborate with cross-functional teams, including BI engineers, to optimize reporting solutions.\nExperience in handling large-scale enterprise data environments.\nFamiliarity with data governance, data cataloging, and metadata management tools (a plus but not required).",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Tableau', 'Data Warehousing', 'Data Analytics', 'SQL', 'Scenario Analysis', 'Cohort Analysis', 'Data Modeling', 'Predictive Analysis', 'Redshift']",2025-06-12 05:33:54
MDM Data Analyst / Steward Lead,Gallagher Service Center (GSC),3 - 7 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\n\nThe MDM Analyst / Data Steward works closely with business stakeholders to understand and gather data requirements, develop data models and database designs, and define and implement data standards, policies, and procedures. This role also implements any rules inside of the MDM tool to improve the data, performs deduplication projects to develop golden records, and overall works towards improving the quality of data in the domain assigned.\n\nRequired skills :\nTechnical Skills: Proficiency in MDM tools and technologies such as Informatica MDM, CluedIn, or similar platforms is essential. Familiarity with data modeling, data integration, and data quality control techniques is also important. Experience with data governance platforms like Collibra and Alation can be beneficial1.\nAnalytical Skills: Strong analytical and problem-solving skills are crucial for interpreting and working with large volumes of data. The ability to translate complex business requirements into practical MDM solutions is also necessary.\nData Management: Experience in designing, implementing, and maintaining master data management systems and solutions. This includes conducting data cleansing, data auditing, and data validation activities.\nCommunication and Collaboration: Excellent communication and interpersonal skills to effectively collaborate with business stakeholders, IT teams, and other departments.\nData Governance: In-depth knowledge of data governance, data quality, and data integration principles. The ability to develop and implement data management processes and policies is essential.\nEducational Background: A Bachelor's or Master's degree in Computer Science, Information Systems, Data Science, or a related field is typically required1.\nCertifications: Certification in the MDM domain (e.g., Certified MDM Professional) can be a plus\n\nKey Skills:\nBecome the expert at the assigned domain of data\nUnderstand all source systems feeding into the MDM\nWrite documentation of stewardship for the domain\nDevelop rules and standards for the domain of data\nGenerate measures of improvement to demonstrate to the business the quality of the data\n\nWe are seeking candidates who can join immediately or within a maximum of 30 days' notice.\nMinimum of 3+ years of relevant experience is required.\nCandidates who are willing to relocate to Bangalore or are already based in Bangalore.\nCandidates should be flexible with working UK/US shifts.",Industry Type: Analytics / KPO / Research,Department: Other,"Employment Type: Full Time, Permanent","['Informatica Mdm', 'Data Modeling', 'Data Integration']",2025-06-12 05:33:57
Data Entry Job in Big Pharma Company at Borivali East in Mumbai,Big and Reputed Pharma Company of India ...,1 - 5 years,1.5-2.5 Lacs P.A.,['Mumbai (All Areas)'],"Only 1 Year+ experienced candidate in any field, who know basic Word, Excel and computer.\n\nThis is office job and you need to work on word, excel and email for the company\n\n2 Saturday and all Sunday are Holiday\n\nFor query call at 8000044060\n\nRequired Candidate profile\nOnly 1 Year+ experienced candidate in any field, who know basic Word, Excel and computer.\n\nThis is office job and you need to work on word, excel and email.\n\n2 Saturday and all Sunday are Holiday",Industry Type: Pharmaceutical & Life Sciences,Department: Administration & Facilities,"Employment Type: Full Time, Permanent","['Office Work', 'Back Office', 'Computer', 'Data Entry', 'operation', 'Backend', 'Typing', 'Excel', 'Word', 'Computer Operating', 'MS Office']",2025-06-12 05:33:59
Senior Backend Engineer - Bangalore - Hybrid (Product Based),Trigent Software Solutions,5 - 10 years,25-27.5 Lacs P.A.,['Bengaluru'],"Job Description:\nWe are looking for an experienced Backend Engineer to join our engineering team and contribute to building highly scalable, low-latency, and high-concurrency SaaS applications. The ideal candidate should have deep expertise in Java, cloud technologies (preferably AWS), and experience working with both RDBMS and NoSQL databases. You will be involved in the full software development lifecycle, from design to deployment, ensuring performance, security, and maintainability.\nKey Responsibilities:\nDesign, develop, and maintain high-performance, scalable, and secure backend services\nBuild and maintain RESTful APIs to support client-side applications and services\nWork on transactional and concurrent systems that serve large-scale SaaS platforms\nCollaborate with frontend engineers, architects, and DevOps to build robust cloud-based systems\nEnsure code quality and performance by writing unit/integration tests and performing code reviews\nTroubleshoot, debug, and optimize existing systems for reliability and efficiency\nFollow Agile development practices and participate in daily stand-ups and sprint planning\nMandatory Skills:\n58 years of backend development experience in building SaaS or transactional web applications\nStrong hands-on experience with Java and web application frameworks like Spring, Spring Boot\nExperience working on high-concurrency, low-latency, and high-availability systems\nSolid experience with at least one RDBMS (e.g., PostgreSQL, MySQL) and one NoSQL DB (e.g., MongoDB, Cassandra)\nExpertise in cloud platforms – preferably AWS (e.g., EC2, S3, RDS, Lambda)\nFamiliarity with application servers like Tomcat\nStrong knowledge of system design, data structures, and multithreading\nExperience with RESTful APIs and microservice architectures\nNice to Have:\nKnowledge of Big Data technologies (e.g., Kafka, Hadoop, Spark)\nFamiliarity with containerization tools like Docker and Kubernetes\nExperience with CI/CD tools and cloud deployment pipelines\nExposure to other cloud platforms like GCP or Azure",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['java', 'RDBMS', 'Saas Product Development', 'Nosql Databases', 'AWS', 'spring', 'tomcat']",2025-06-12 05:34:02
"Sr. Data Analyst – Tableau, SQL, Snowflake",Int9 Solutions,5 - 10 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","We are looking for a skilled Data Analyst with excellent communication skills and deep expertise in SQL, Tableau, and modern data warehousing technologies. This role involves designing data models, building insightful dashboards, ensuring data quality, and extracting meaningful insights from large datasets to support strategic business decisions.\n\nKey Responsibilities:\nWrite advanced SQL queries to retrieve and manipulate data from cloud data warehouses such as Snowflake, Redshift, or BigQuery.\nDesign and develop data models that support analytics and reporting needs.\nBuild dynamic, interactive dashboards and reports using tools like Tableau, Looker, or Domo.\nPerform advanced analytics techniques including cohort analysis, time series analysis, scenario analysis, and predictive analytics.\nValidate data accuracy and perform thorough data QA to ensure high-quality output.\nInvestigate and troubleshoot data issues; perform root cause analysis in collaboration with BI or data engineering teams.\nCommunicate analytical insights clearly and effectively to stakeholders.\n\nRequired Skills & Qualifications:\nExcellent communication skills are mandatory for this role.\n5+ years of experience in data analytics, BI analytics, or BI engineering roles.\nExpert-level skills in SQL, with experience writing complex queries and building views.\nProven experience using data visualization tools like Tableau, Looker, or Domo.\nStrong understanding of data modeling principles and best practices.\nHands-on experience working with cloud data warehouses such as Snowflake, Redshift, BigQuery, SQL Server, or Oracle.\nIntermediate-level proficiency with spreadsheet tools like Excel, Google Sheets, or Power BI, including functions, pivots, and lookups.\nBachelor's or advanced degree in a relevant field such as Data Science, Computer Science, Statistics, Mathematics, or Information Systems.\nAbility to collaborate with cross-functional teams, including BI engineers, to optimize reporting solutions.\nExperience in handling large-scale enterprise data environments.\nFamiliarity with data governance, data cataloging, and metadata management tools (a plus but not required).\nLocation : - Mumbai, Delhi / NCR, Bengaluru , Kolkata, Chennai, Hyderabad, Ahmedabad, Pune, Remote",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Tableau', 'SQL', 'BI Tools', 'Scenario Analysis', 'Cohort Analysis', 'Data Warehousing', 'SQL Server', 'Data Modeling', 'Data Analytics', 'Predictive Analysis', 'Redshift']",2025-06-12 05:34:04
Big Data Scala developer (with Spark),Fortune India 500 IT Services Firm,5 - 8 years,Not Disclosed,"['Hyderabad', 'Pune']","We are looking for a highly skilled Big Data Scala Developer with solid experience in Apache Spark to join our data engineering team.\n\nExperience- 5 to 8yrs\nLocation- Pune, Hyderabad\nMandatory skills- Scala development, Spark\nKey Responsibilities:\nDesign, develop, and optimize batch and streaming data pipelines using Scala and Apache Spark.\nWrite efficient, reusable, and testable code following functional programming best practices.\nWork with large-scale datasets from a variety of sources (e.g., Kafka, Hive, S3, Parquet).\nCollaborate with data scientists, data analysts, and DevOps to ensure robust and scalable pipelines.\nTune Spark jobs for performance and resource efficiency.\nImplement data quality checks, logging, and error-handling mechanisms.\n\n\n\nInterested candidates share your CV at himani.girnar@alikethoughts.com with below details\n\nCandidate's name-\nEmail and Alternate Email ID-\nContact and Alternate Contact no-\nTotal exp-\nRelevant experience-\nCurrent Org-\nNotice period-\nCCTC-\nECTC-\nCurrent Location-\nPreferred Location-\nPancard No-",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Scala Programming', 'SCALA', 'Spark', 'Big Data', 'Big Data Technologies', 'SQL']",2025-06-12 05:34:06
Software Engineer Gen AI,Wells Fargo,2 - 7 years,Not Disclosed,['Bengaluru'],"locationsBengaluru, India\nposted onPosted 4 Days Ago\njob requisition idR-462134\nAbout this role:\nWells Fargo is seeking a Software Engineer.\n\nIn this role, you will:\nParticipate in low to moderately complex initiatives and projects associated with the technology domain, including installation, upgrades, and deployment efforts\nIdentify opportunities for service quality and availability improvements within the technology domain environment\nDesign, code, test, debug, and document for low to moderately complex projects and programs associated with technology domain, including upgrades and deployments\nReview and analyze technical assignments or challenges that are related to low to medium risk deliverables and that require research, evaluation, and selection of alternative technology domains\nPresent recommendations for resolving issues or may escalate issues as needed to meet established service level agreements\nExercise some independent judgment while also developing understanding of given technology domain in reference to security and compliance requirements\nProvide information to technology colleagues, internal partners, and stakeholders\n\nRequired Qualifications:\n2+ years of software engineering experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education\n\nDesired Qualifications:\nWork as a Generative AI engineer developing enterprise-scale AI applications\nDesign, implement, and optimize LLM-based solutions using state-of-the-art frameworks\nLead Gen AI initiatives focused on developing intelligent agents and conversational systems\nDesign and build robust LLM interfaces and orchestration pipelines\nDevelop evaluation frameworks to measure and improve model performance\nImplement prompt engineering techniques to optimize model outputs\nIntegrate Gen AI capabilities with existing enterprise applications\nBuild and maintain frontend interfaces for AI applications\nStrong proficiency in Python/Java and LLM orchestration frameworks (LangChain, LangGraph)\nBasic Knowledge of model context protocols, RAG architectures, and embedding techniques\nExperience with model evaluation frameworks and metrics for LLM performance\nProficiency in frontend development with React.js for AI applications\nExperience with UI/UX design patterns specific to AI interfaces\nExperience with vector databases and efficient retrieval methods\nKnowledge of prompt engineering techniques and best practices\nExperience with containerization and microservices architecture\nStrong understanding of semantic search and document retrieval systems\nWorking knowledge of both structured and unstructured data processing\nExperience with version control using GitHub and CI/CD pipelines\nExperience working with globally distributed teams in Agile scrums\n\nJob Expectations:\nUnderstanding of enterprise use cases for Generative AI\nKnowledge of responsible AI practices and ethical considerations\nAbility to optimize AI solutions for performance and cost\nWell versed in MLOps concepts for LLM applications\nStaying current with rapidly evolving Gen AI technologies and best practices\nExperience implementing security best practices for AI applications",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Gen AI', 'Java', 'UI design', 'UX design', 'LLM orchestration', 'React.js', 'Python', 'MLOps concepts']",2025-06-12 05:34:09
Sr. Data Analyst,Icims,4 - 9 years,Not Disclosed,['Hyderabad'],"Overview\nThe Senior Data Analyst is responsible for serving as a subject matter expert who can lead efforts to analyze data with the goal of delivering insights that will influence our products and customers. This position will report into the Data Analytics Manager, and will work closely with members of our product and marketing teams, data engineers, and members of our Customer Success organization supporting client outreach efforts. The chief functions of this role will be finding and sharing data-driven insights to deliver value to less technical audiences, and instilling best practices for analytics in the rest of the team.",,,,"['server', 'data', 'vlookup', 'market data', 'data mapping', 'dashboards', 'research', 'sql', 'analytics', 'tables', 'prep', 'pivot', 'data visualization', 'communication skills', 'python', 'data analytics', 'data analysis', 'insights', 'pivot table', 'data engineering', 'graph', 'excel', 'data quality', 'tableau', 'data governance', 'root cause']",2025-06-12 05:34:12
Senior Associate Data Scientist,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will identify trends, root causes, and potential improvements in our products and processes, ensuring that patient voices are heard and addressed with utmost precision.\nAs the Sr Associate Data Scientist at Amgen, you will be responsible for developing and deploying basic machine learning, operational research, semantic analysis, and statistical methods to uncover structure in large data sets. This role involves creating analytics solutions to address customer needs and opportunities.\nCollect, clean, and manage large datasets related to product performance and patient complaints.\nEnsure data integrity, accuracy, and accessibility for further analysis.\nDevelop and maintain databases and data systems for storing patient complaints and product feedback.\nAnalyze data to identify patterns, trends, and correlations in patient complaints and product issues.\nUse advanced statistical methods and machine learning techniques to uncover insights and root causes.\nDevelop analytics or predictive models to foresee potential product issues and patient concerns to address customer needs and opportunities.\nPrepare comprehensive reports and visualizations to communicate findings to key collaborators.\nPresent insights and recommendations to cross-functional teams, including product development, quality assurance, and customer service.\nCollaborate with regulatory and compliance teams to ensure adherence to healthcare standards and regulations.\nFind opportunities for product enhancements and process improvements based on data analysis.\nWork with product complaint teams to implement changes and monitor their impact.\nStay abreast of industry trends, emerging technologies, and standard methodologies in data science and healthcare analytics.\nEvaluate data to support product complaints.\nWork alongside software developers and software engineers to translate algorithms into commercially viable products and services.\nWork in technical teams in development, deployment, and application of applied analytics, predictive analytics, and prescriptive analytics.\nPerform exploratory and targeted data analyses using descriptive statistics and other methods.\nWork with data engineers on data quality assessment, data cleansing and data analytics\nGenerate reports, annotated code, and other projects artifacts to document, archive, and communicate your work and outcomes.\n\nBasic Qualifications:\nMasters degree and 1 to 3 years of Data Science and with one or more analytic software tools or languages, and data visualization tools experience OR\nBachelors degree and 3 to 5 years of Data Science and with one or more analytic software tools or languages, and data visualization tools experience OR\nDiploma and 7 to 9 years of Data Science and with one or more analytic software tools or languages, and data visualization tools experience\nPreferred Qualifications:\nDemonstrated skill in the use of applied analytics, descriptive statistics, feature extraction and predictive analytics on industrial datasets.\nExperience in statistical techniques and hypothesis testing, experience with regression analysis, clustering and classification.\nExperience in analyzing time-series data for forecasting and trend analysis.\nExperience with Data Bricks platform for data analytics.\nExperience working with healthcare data, including patient complaints, product feedback, and regulatory requirements.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'data bricks', 'hypothesis testing', 'predictive analytics', 'data visualization', 'machine learning', 'statistics']",2025-06-12 05:34:14
"Senior Engineer, Database",Nagarro,3 - 5 years,Not Disclosed,['Chennai'],"We're Nagarro.\n\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal experience 3+ years.\nHands-on working experience as a DBA managing large, mission-critical MySQL databases.\nStrong understanding of Relational Database Management Systems (RDBMS).\nProficiency in SQL (CRUD operations, Views, Materialized Views).\nDesign, develop, and optimize PL/SQL code including Packages, Procedures, Functions, Triggers, and advanced Exception Handling.\nExperience with PL/SQL code optimization techniques.\nExperience working with DB2 database engine.\nIntermediate knowledge of Unix and Shell Scripting.\nHands-on experience with IBM Data Studio.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\n\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them into technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers\nIdentifying different solutions and being able to narrow down the best option that meets the client’s requirements\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design documents explaining overall architecture, framework, and high-level design of the application for the developers\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken\nCarrying out POCs to make sure that suggested design/technologies meet the requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['RDBMS', 'Database', 'PLSQL', 'Db2 Engine']",2025-06-12 05:34:17
GEN AI Engineer,HCLTech,3 - 8 years,Not Disclosed,"['Noida', 'Chennai', 'Bengaluru']",Skill Needed\nJob:\nDefine the Agentic Function for Industrial Quality Inspection\nWorkflow design\nBring up the LLM and AI baseline framework -open source (Llava),,,,"['GEN AI', 'RAG', 'LLM', 'Open source', 'ML', 'Tensorflow']",2025-06-12 05:34:19
"Senior Data Scientist, Operations || Mumbai || 29 LPA",Argus India Price Reporting Services,5 - 10 years,20-25 Lacs P.A.,"['Mumbai Suburban', 'Navi Mumbai', 'Mumbai (All Areas)']","Senior Data Scientist, Operations\nMumbai, India\nAbout Argus:\n\nArgus is the leading independent provider of market intelligence to the global energy and commodity markets. We offer essential price assessments, news, analytics, consulting services, data science tools and industry conferences to illuminate complex and opaque commodity markets.\nHeadquartered in London with 1,500 staff, Argus is an independent media organisation with 30 offices in the worlds principal commodity trading hubs.\nCompanies, trading firms and governments in 160 countries around the world trust Argus data to make decisions, analyse situations, manage risk, facilitate trading and for long-term planning. Argus prices are used as trusted benchmarks around the world for pricing transportation, commodities and energy.\nFounded in 1970, Argus remains a privately held UK-registered company owned by employee shareholders and global growth equity firm General Atlantic.\n\nWhat were looking for:\nJoin our Generative AI team as a Senior Data Scientist, reporting directly to the Lead Data Scientist in India. You will play a crucial role in building, optimizing, and maintaining AI-ready data infrastructure for advanced Generative AI applications. Your focus will be on hands-on implementation of cutting-edge data extraction, curation, and metadata enhancement techniques for both text and numerical data. You will be a key contributor to the development of innovative solutions, ensuring rapid iteration and deployment, and supporting the Lead in achieving the team's strategic goals.\n\nWhat will you be doing:\nAI-Ready Data Development: Design, develop, and maintain high-quality AI-ready datasets, ensuring data integrity, usability, and scalability to support advanced Generative AI models.\nAdvanced Data Processing: Drive hands-on efforts in complex data extraction, cleansing, and curation for diverse text and numerical datasets. Implement sophisticated metadata enrichment strategies to enhance data utility and accessibility for AI systems.\nAlgorithm Implementation & Optimization: Implement and optimize state-of-the-art algorithms and pipelines for efficient data processing, feature engineering, and data transformation tailored for LLM and GenAI applications.\nGenAI Application Development: Apply and integrate frameworks like LangChain and Hugging Face Transformers to build modular, scalable, and robust Generative AI data pipelines and applications.\nPrompt Engineering Application: Apply advanced prompt engineering techniques to optimize LLM performance for specific data extraction, summarization, and generation tasks, working closely with the Lead's guidance.\nLLM Evaluation Support: Contribute to the systematic evaluation of Large Language Models (LLMs) outputs, analysing quality, relevance, and accuracy, and supporting the implementation of LLM-as-a-judge frameworks.\nRetrieval-Augmented Generation (RAG) Contribution: Actively contribute to the implementation and optimization of RAG systems, including working with embedding models, vector databases, and, where applicable, knowledge graphs, to enhance data retrieval for GenAI.\nTechnical Mentorship: Act as a technical mentor and subject matter expert for junior data scientists, providing guidance on best practices in coding and PR reviews, data handling, and GenAI methodologies.\nCross-Functional Collaboration: Collaborate effectively with global data science teams, engineering, and product stakeholders to integrate data solutions and ensure alignment with broader company objectives.\nOperational Excellence: Troubleshoot and resolve data-related issues promptly to minimize potential disruptions, ensuring high operational efficiency and responsiveness.\nDocumentation & Code Quality: Produce clean, well-documented, production-grade code, adhering to best practices for version control and software engineering.\n\nSkills and Experience:\nAcademic Background: Advanced degree in AI, statistics, mathematics, computer science, or a related field.\nProgramming and Frameworks: 2+ years of hands-on experience with Python, TensorFlow or PyTorch, and NLP libraries such as spaCy and Hugging Face.\nGenAI Tools: 1+ years Practical experience with LangChain, Hugging Face Transformers, and embedding models for building GenAI applications.\nPrompt Engineering: Deep expertise in prompt engineering, including prompt tuning, chaining, and optimization techniques.\nLLM Evaluation: Experience evaluating LLM outputs, including using LLM-as-a-judge methodologies to assess quality and alignment.\nRAG and Knowledge Graphs: Practical understanding and experience using vector databases. In addition, familiarity with graph-based RAG architectures and the use of knowledge graphs to enhance retrieval and reasoning would be a strong plus.\nCloud: 2+ years of experience with Gemini/OpenAI models and cloud platforms such as AWS, Google Cloud, or Azure. Proficient with Docker for containerization.\nData Engineering: Strong understanding of data extraction, curation, metadata enrichment, and AI-ready dataset creation.\nCollaboration and Communication: Excellent communication skills and a collaborative mindset, with experience working across global teams.\n\nWhats in it for you:\nCompetitive salary\nHybrid Working Policy (3 days in Mumbai office/ 2 days WFH once fully inducted)\nGroup healthcare scheme\n18 days annual leave\n8 days of casual leave\nExtensive internal and external training\n\nHours:\nThis is a full-time position operating under a hybrid model, with three days in the office and up to two days working remotely.\nThe team supports Argus key business processes every day, as such you will be required to work on a shift-based rota with other members of the team supporting the business until 8pm. Typically support hours run from 11am to 8pm with each member of the team participating up to 2/3 times a week.\n\nFor more details about the company and to apply please make sure you send your CV and cover letter via our website: www.argusmedia.com/en/careers/open-positions\nBy submitting your job application, you automatically acknowledge and consent to the collection, use and/or disclosure of your personal data to the Company. Argus is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, sexual orientation, gender identity, disability or veteran status.",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pytorch', 'Artificial Intelligence', 'LangChain', 'hugging face', 'Spacy', 'Tensorflow']",2025-06-12 05:34:21
Etl Testing Engineer,Infosys,3 - 8 years,Not Disclosed,"['Kolkata', 'Hyderabad', 'Bengaluru']","Job description,\n\nHiring for ETL testing with experience range 3-10 years\n\nMandatory Skills: ETL Testing\n\nLocation - Bangalore/Hyderabad/Pune/kolkata/Chennai/Bhubaneswar\n\nEducation: BE/B.Tech/MCA/M.Tech/MSc./MS\n\nResponsibilities\nA day in the life of an Infoscion\nAs part of the Infosys consulting team, your primary role would be to actively aid the consulting team in different phases of the project including problem definition, effort estimation, diagnosis, solution generation and design and deployment\nYou will explore the alternatives to the recommended solutions based on research that includes literature surveys, information available in public domains, vendor evaluation information, etc. and build POCs\nYou will create requirement specifications from the business needs, define the to-be-processes and detailed functional designs based on requirements.\nYou will support configuring solution requirements on the products; understand if any issues, diagnose the root-cause of such issues, seek clarifications, and then identify and shortlist solution alternatives\nYou will also contribute to unit-level and organizational initiatives with an objective of providing high quality value adding solutions to customers. If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ETL Testing', 'ETL', 'DWH Testing']",2025-06-12 05:34:24
Senior/Lead MLops Engineer,Tiger Analytics,7 - 10 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","JOB DESCRIPTION\n\nSenior MLE / Architect MLE (ML Ops) Chennai / Bangalore / Hyderabad (Hybrid)\n\nWho we are Tiger Analytics is a global leader in AI and analytics, helping Fortune 1000 companies solve their toughest challenges. We offer fullstack AI and analytics services & solutions to empower businesses to achieve real outcomes and value at scale. We are on a mission to push the boundaries of what AI and analytics can do to help enterprises navigate uncertainty and move forward decisively. Our purpose is to provide certainty to shape a better tomorrow. Our team of 4000+ technologists and consultants are based in the US, Canada, the UK, India, Singapore and Australia, working closely with clients across CPG, Retail, Insurance, BFS, Manufacturing, Life Sciences, and Healthcare. Many of our team leaders rank in Top 10 and 40 Under 40 lists, exemplifying our dedication to innovation and excellence. We are a Great Place to Work-Certified (2022-24), recognized by analyst firms such as Forrester, Gartner, HFS, Everest, ISG and others. We have been ranked among the Best and Fastest Growing analytics firms lists by Inc., Financial Times, Economic Times and Analytics India Magazine.",,,,"['MLops', 'Azure', 'Snowflake', 'Deployment', 'Ci/Cd', 'Machine Learning']",2025-06-12 05:34:27
"Senior Staff Engineer, QA Automation",Nagarro,10 - 13 years,Not Disclosed,['India'],"We're Nagarro.\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal Experience 10+ years.\nStrong working experience in QA with a strong background in both manual and automation testing.\nHands-on experience with Selenium WebDriver, Appium, and Postman.\nSolid understanding of REST API testing and automation using tools like RestAssured.\nProficient in testing frameworks such as TestNG, JUnit, or Cypress.\nStrong experience in automation of mobile and web applications.\nFamiliarity with CI/CD tools like Jenkins, GitLab CI, or equivalent.\nWorking knowledge of bug tracking and test management tools (e.g., JIRA, TestRail).\nExperience with BDD frameworks like Cucumber.\nGood command of scripting or programming in Java, Python, or similar languages is a plus.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\nRESPONSIBILITIES:\nUnderstanding the clients business use cases and technical requirements and be able to convert them in to technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['API Testing', 'Appium', 'QA Automation', 'Selenium', 'Postman']",2025-06-12 05:34:29
"Senior Staff Engineer, Frontend React",Nagarro,10 - 13 years,Not Disclosed,['India'],"We're Nagarro.\n\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal Experience 10+ years.\nHands on working experience in front-end or full-stack development experience, with building production apps in React.js and Next.js.\nHands-on expertise writing unit/integration tests for React components (Jest, React Testing Library, etc.)\nSolid grasp of state-management patterns and libraries (Redux, React Context, Zustand, etc.).\nStrong understanding of RESTful APIs, asynchronous programming (Promises, async/await), and modern build tools (Webpack, Vite, or Turbopack).\nPractical experience with Git, pull-request workflows, and collaborative development tools (GitHub, GitLab, Bitbucket).\nAdvanced proficiency in JavaScript (ES6+) and TypeScript.\nProblem-solving mindset with the ability to tackle complex data engineering challenges. \nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\n\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them in to technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Javascript', 'React.Js', 'Nextjs', 'Typescript']",2025-06-12 05:34:31
Sr Associate HR Data Analysis (Visier Admin),Amgen Inc,5 - 8 years,Not Disclosed,['Hyderabad'],"Role Description:\nAmgen is seeking a Sr Associate HR Data Analysis (Visier Admin). The Sr Associate HR Data Analysis (Visier Admin) will report to the Associate Director HR Technology.\nThe successful incumbent will have previous Visier reporting tool Admin experience.\nRoles & Responsibilities:\nHands on experience supporting Visier\nPrevious experience with Vee\nAdministrative tasks associated with Visier such as role assignments and creating roles\nVisier Security configuration, data integration, and data exports\nAbility to analyze, troubleshoot and resolve Visier data issues\nMust have previous experience handling large datasets and sensitive HR data\nBasic Qualifications and Experience:\n5 years minimum experience in human resources with hands on experience with Visier\nMasters degree, OR\nBachelors degree and 5 years of HR IS experience\nFunctional Skills:\nMust-Have Skills:\nStrong working knowledge of Visier\n5+ years experience in human resources and corporate service center supporting Workday\nSoft Skills:\nExcellent analytical and troubleshooting skills\nStrong quantitative, analytical (technical and business), problem solving skills, and attention to detail\nStrong verbal, written communication and presentation skills\nAbility to work effectively with global, virtual teams\nStrong technical acumen, logic, judgement and decision-making\nStrong initiative and desire to learn and grow\nAbility to manage multiple priorities successfully\nExemplary adherence to ethics, data privacy and compliance policies",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'Visier Admin', 'troubleshooting', 'data integration', 'HR Data Analysis']",2025-06-12 05:34:34
Cloud Engineer,"NTT DATA, Inc.",9 - 14 years,Not Disclosed,['Bengaluru'],"Req ID: 324901\n\nWe are currently seeking a Cloud Engineer to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\nAt NTT DATA, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company""™s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring, the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA and for the people who work here.\n\n\n\nPreferred Experience\nSolid understanding of cloud computing, networking, and storage principles with focus on Azure. Should have strong delivery knowledge and experience around cloud adoption and workload management in public cloud (IaaS and PaaS platforms)\nWilling to work on multiple cloud platforms. Understanding of the customer strategy, business, technical, security and compliance requirements.\nExpertise in Cloud Infrastructure Networking, Security, IAM, Data Security and leveraging the right solutions in these areas is required.\nSolid scripting and automation experience (DevOps & scripting). Implementation experience on Infrastructure as Code (IaC) using tools like ARM templates and/or Terraform.\nGood experience on emerging technologies, including Container bases orchestration, AKS, AI services etc.,\nInnovative self-starter, willing to learn, test, implement new technologies on existing and new public cloud providers.\nCollaborate with different teams ""“ Data Engineering, Data Analytics, InfoSec, Network/firewall, etc.\nClear understanding of automation.\nDefine requirements and evaluate existing architectures, Various technical solutions, products, Partners against our technical requirements ""“ technical, NFR such as operational resilience, scalability, reliability, performance.\nRegular interactions with senior management or executive levels on matters concerning several functional areas and/or customers are also part of the job\nAssist with the design and implementation of best governance practices for design, security, development, usability, cost optimization / control.\nKnowledge of best practices and market trends pertaining to Cloud and overall industry to provide thought leadership (seminars, whitepapers etc.,) and mentor team to build necessary competency\nCollaborating in the creation work/design documents with team assistance. Able to work independently in a project scenario and do POCs.\nShould be able to manage & mentor the junior team members from L2 / L1.\nAble to work on On-Call rotations.\nExperience handling P1/P2 calls and escalations\nAble to write quality KB articles, Problem Management articles, and SOPs/Runbooks.\nPassion for delivering timely and outstanding customer service\nGreat written and oral communication skills with internal and external customers\n\n\n\nBasic Qualifications\nAt least 9+ years of overall operational experience.\n6+ years of Azure experience\n3+ years of experience working in a diverse cloud support environment in a 24*7 production support model\n2+ years DevOps/scripting/APIs experience\n\n\nPreferred Certifications\nAzure Administrator Associate/Solution Architect Certifications.\nMicrosoft DevOps / Terraform certifications is an added advantage.\nAWS SysOps Administrator/Developer/Solutions Architect Associate Certifications\nFour Years degree on Information Technology degree or equivalent experience",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['data security', 'networking', 'iam', 'cloud infrastructure', 'cloud computing', 'container', 'public cloud', 'aks', 'microsoft azure', 'cloud platforms', 'cloud support', 'workload management', 'production support', 'firewall', 'devops', 'paas', 'terraform', 'api', 'iaas', 'arm templates', 'aws']",2025-06-12 05:34:37
"Director, Software Engineering/Architecture","NTT DATA, Inc.",10 - 15 years,Not Disclosed,['Bengaluru'],"Additional Career Level Description:\nKnowledge and application:\nUses extensive knowledge across functional areas to direct the application of existing policies and principles and guide the development of new policies and ideas across the function.\nLeads, integrates and directs work applying substantial practical expertise across function disciplines.\nProblem solving:\nSolutions are devised based on limited information and issues that are occasionally complex and fundamental principles and data may be in conflict.\nNew concepts and solutions consider multiple perspectives and future implications.\nInteraction:\nInteracts with senior management, executives, and/or major customers which frequently involves negotiating matters of significance to the organization.\nReconciles multiple stakeholder views to drive business results.\nImpact:\nWorks with senior management to establish strategic plans and translates business segment strategy into functional plans and guides execution.\nErroneous decisions will have a critical long term (typically up to five years) impact on the overall success of function or multi departments.\nAccountability:\nAccountable for results which impact function or multiple departments including budgets.\nDirect management of a team of professional managers and experienced individual contributors.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Software Engineering', 'python', 'c++', 'project management', 'java', 'software development', 'c', 'data structures', 'big data', 'aws', 'sql']",2025-06-12 05:34:39
Data Architect,Coforge,11 - 16 years,Not Disclosed,"['Noida', 'Greater Noida', 'Delhi / NCR']","-Data Architect Department:\nData & Analytics The Data Architect having more than 14 years of experience and should play a pivotal role in designing, developing, and governing scalable data architectures to support enterprise-wide data integration, analytics, and reporting.\nThis role will focus on creating unified data models, optimizing data pipelines, and ensuring compliance with regulatory standards (GDPR) using cloud-based platforms.\nThe ideal candidate is a strategic thinker with deep expertise in data modeling, cloud data platforms, and governance.",,,,"['Data Migration', 'Data Warehousing', 'Data Modeling', 'Informatica', 'SSIS', 'ETL Tool']",2025-06-12 05:34:41
"Senior Staff Engineer, Mobile -Flutter",Nagarro,10 - 13 years,Not Disclosed,['India'],"We're Nagarro.\n\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal Experience 10+ years.\nStrong working experience in Dart and Flutter.\nSolid understanding of Mobile App Architecture (MVVM, BLoC, Provider, GetX).\nExperience with local databases like Sqflite or alternatives.\nHands-on experience in unit testing and test automation for Flutter apps.\nProven experience in building and deploying apps to the App Store and Google Play Store.\nFamiliarity with Git, GitHub/GitLab, CI/CD tools (e.g., Jenkins, Bitrise, GitHub Actions).\nDeep knowledge of mobile app lifecycle, design principles, and clean architecture patterns (MVVM, MVC, etc.)\nExpertise in mobile app performance optimization and security best practices.\nExperience in API integration, RESTful services, and handling JSON data.\nProficient in Version Control Systems like Git.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them in to technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GIT', 'Flutter', 'Dart', 'Swift', 'IOS', 'Android']",2025-06-12 05:34:44
Digital Engineering Staff Engineer,"NTT DATA, Inc.",2 - 5 years,Not Disclosed,['Bengaluru'],"Req ID: 310007\n\nWe are currently seeking a Digital Engineering Staff Engineer to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\nData Modeler\n\n\n\nPosition Overview: The Data Modeler will be responsible for designing and implementing data models that support the organization's data management and analytics needs.\n\nThis role involves collaborating with various stakeholders to understand data sources, relationships, and business requirements, and translating them into effective data structures.\n\n\n\nKey Responsibilities:\n\n\nCollaborate with Business Analysts: Understand different data sources and their relationships.\n\n\nPrepare Conformed Dimension Matrix: Identify different grains of facts, finalize dimensions, and harmonize data across sources.\n\n\nCreate Data Models: Develop Source to Target Mapping (STMs) documentation and custom mappings (both technical and non-technical).\n\n\nInclude Transformation Rules: Ensure STMs include pseudo SQL queries for transformation rules.\n\n\nCoordinate Reviews: Work with Data Architects, Product Owners, and Enablement teams to review and approve models, STMs, and custom mappings.\n\n\nEngage with Data Engineers: Clarify any questions related to STMs and custom mappings.\n\n\n\nRequired Technical\n\nSkills:\n\n\n\nProficiency in SQL: Strong understanding of SQL and database management systems.\n\n\nData Modeling Tools: Familiarity with tools such as ERwin, IBM InfoSphere Data Architect, or similar.\n\n\nData Warehousing Concepts: Solid knowledge of data warehousing principles, ETL processes, and OLAP.\n\n\nData Governance and Compliance: Understanding of data governance frameworks and compliance requirements.\n\n\n\nKey Competencies:\n\n\nAnalytical\n\nSkills:\nAbility to analyze complex data sets and derive meaningful insights.\n\n\nAttention to Detail: Ensure accuracy and consistency in data models.\n\n\nCommunication\n\nSkills:\nEffectively collaborate with stakeholders and articulate technical concepts to non-technical team members.\n\n\nProject Management\n\nSkills:\nAbility to prioritize tasks, manage timelines, and coordinate with cross-functional teams.\n\n\nContinuous Learning and Adaptability: Commitment to ongoing professional development and adaptability to changing business needs and technologies.\n\n\n\nAdditional :\n\n\nProblem-Solving Abilities: Innovative solutions to data integration, quality, and performance challenges.\n\n\nKnowledge of Data Modeling Methodologies: Entity-relationship modeling, dimensional modeling, normalization techniques.\n\n\nFamiliarity with Business Intelligence Tools: Enhance ability to design data structures that facilitate data analysis and visualization.\n\n\n\nPreferred Qualifications:\n\n\nExperience in SDLC: Understanding of all phases of the Software Development Life Cycle.\n\n\nCertifications: Relevant certifications in data modeling, data warehousing, or related fields.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['database management system', 'data warehousing', 'sql', 'olap', 'etl process', 'c#', 'ibm infosphere', 'dimensional modeling', 'erwin', 'business intelligence', 'javascript', 'sql server', 'software development life cycle', 'linq', 'java', 'data modeling', 'asp.net', 'data structures', 'etl', 'sdlc']",2025-06-12 05:34:46
Senior .NET/GCP Engineer (REMOTE),"NTT DATA, Inc.",3 - 7 years,Not Disclosed,['Chennai'],"Req ID: 316318\n\nWe are currently seeking a Senior .NET/GCP Engineer (REMOTE) to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\n\n\n\n\n\nSenior .NET/GCP Engineer - Remote\n\n\n\n\n\nHow You""™ll Help Us:\n\nA Senior Application Developer is first and foremost a software developer who specializes in .NET C",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['dns', 'networking', 'technical support', 'active directory', 'dhcp', 'switching', 'medidata rave', 'routing', 'system administration', 'it recruitment', 'remote support', 'recruitment', 'application support', 'microsoft windows', 'desktop support', 'troubleshooting', '.net', 'hadoop', 'big data', 'ccna']",2025-06-12 05:34:49
Azure Data Bricks (4-15 Yrs) - Bangalore,Happiest Minds Technologies,4 - 9 years,Not Disclosed,['Bengaluru'],"Hi,\n\nGreetings from Happiest Minds Technologies\n\nCurrently we are hiring for below positions and looking for immediate joiners.\n1. Azure Databricks Bangalore 5 to 10 Yrs - Bangalore\nAs a Senior Azure Data Engineer, you will leverage Azure technologies to drive data transformation, analytics, and machine learning. You will design scalable Databricks data pipelines using PySpark, transforming raw data into actionable insights. Your role includes building, deploying, and maintaining machine learning models using MLlib or TensorFlow while optimizing cloud data integration from Azure Blob Storage, Data Lake, and SQL/NoSQL sources. You will execute large-scale data processing using Spark Pools, fine-tuning configurations for efficiency. The ideal candidate holds a Bachelors or Masters in Computer Science, Data Science, or a related field, with 7+ years in data engineering and 3+ years specializing in Azure Databricks, PySpark, and Spark Pools. Proficiency in Python PySpark, Pandas, NumPy, SciPy, Spark SQL, DataFrames, RDDs, Delta Lake, Databricks Notebooks, and MLflow is required, along with hands-on experience in Azure Data Lake, Blob Storage, and Synapse Analytics.",,,,"['Pyspark', 'Azure', 'Data Bricks', 'sql', 'ETL']",2025-06-12 05:34:51
Software Applications Development Engineer,"NTT DATA, Inc.",5 - 10 years,Not Disclosed,"['Mumbai', 'Hyderabad', 'Bengaluru']","Your day at NTT DATA\nThe Software Applications Development Engineer is a seasoned subject matter expert, responsible for developing new applications and improving upon existing applications based on the needs of the internal organization and or external clients.\nWhat you'll be doing\nYrs. Of Exp: 5 Yrs.\n\nData Engineer-\nWork closely with Lead Data Engineer to understand business requirements, analyse and translate these requirements into technical specifications and solution design.\nWork closely with Data modeller to ensure data models support the solution design\nDevelop , test and fix ETL code using Snowflake, Fivetran, SQL, Stored proc.\nAnalysis of the data and ETL for defects/service tickets (for solution in production ) raised and service tickets.\nDevelop documentation and artefacts to support projects.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Application Development', 'Data Engineering', 'Snowflake', 'ETL', 'Fivetran', 'SQL']",2025-06-12 05:34:53
App Dev & Support Engineer III,Conduent,6 - 11 years,Not Disclosed,['Bengaluru'],"Responsibilities\nDesign and develop highly scalable web-based applications based on business needs.\nDesign and customize software for client use with the aim of optimizing operational efficiency.\nA deep understanding of, and ability to use and explain all aspects of application integration in .NET and data integration with SQL Server and associated technologies and standards\nStrong background in building and operating SAAS platforms using the Microsoft technology stack with modern services-based architectures.\nAbility to recommend and configure Azure subscriptions and establish connectivity\nWork with IT teams to setup new application architecture requirements\nCoordinate releases with Quality Assurance Team and implement SDLC workflows and better source code integration.\nImplement build process and continuous build integration with Unit Testing framework.\nDevelop and maintain a thorough understanding of business needs from both technical and business perspectives\nAssist and mentor junior team members to enforce development guidelines.\nTake technical ownership of products and provide support with quick turnaround.\nEffectively prioritize and execute tasks in a high-pressure environment\n\n\nQualifications / Experience\nBachelor\\u2019s/master\\u2019s degree in computer science / computer engineering\nMinimum of 6+ years\\u2019 experience in building enterprise scale windows and web application using Microsoft .NET technologies.\n5+ years of experience in C#, ASP.NET MVC and.Net Core Web API\n1+ years of experience in Angular 2 or higher\nExperience in any of the following are also desirableBootstrap, Knockout, entity framework, nhibernate, Subversion, Linq, Asynchronous Module Definition (such as requirejs)\nIn depth knowledge on design patterns and unit testing frameworks.\nExperience with Agile application development.\nSQL Server development, performance tuning (SQL Server 2014/2016) and troubleshooting\nAbility to work with a sense of urgency and attention to detail\nExcellent oral and written communication skills.",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql server', 'application integration', 'design patterns', '.net', 'data integration', 'c#', 'unit testing framework', 'web application', 'performance tuning', 'unit testing', 'scale', 'knockoutjs', 'net mvc', 'angular', 'linq', 'saas applications', 'asp.net', 'web api', 'mvc', 'asp', 'microsoft net']",2025-06-12 05:34:56
Data Scientist,Ltimindtree,8 - 13 years,Not Disclosed,"['Chennai', 'Bengaluru', 'Mumbai (All Areas)']",We are looking for an experienced AI ML Developers experience in data science specializing in machine learning python statistical modelling and big data technologies pyspark sql.\n\nThe ideal candidate will have a strong background in developing and deploying machine learning models optimizing ML pipelines and handling largescale structured and unstructured data to drive business impact.\n\nDeep understanding of supervised and unsupervised learning including regression classification Multiclass classification clustering and NLP Proficiency in statistical analysis AB testing and causal inference techniques Experience with model deployment and MLOps in cloud environments AWS GCP \n\nKey Responsibilities\n\nDevelop and deploy machine learning models and predictive analytics solutions for business impact\nWork with largescale structured and unstructured data to extract insights and build scalable models\nDesign implement and optimize ML pipelines for realtime and batch processing\nCollaborate with engineering product and business stakeholders to translate business problems into data science solutions\nApply statistical modeling AB testing and causal inference techniques to evaluate business performance\nApply machine learning and statistical techniques for audience segmentation helping to identify patterns and optimise business strategies\nDrive research and innovation by staying updated with cuttingedge MLAI advancements and incorporating them into our solutions\nOptimize data science models for performance scalability and interpretability in production environments\nMentor junior data scientists and contribute to best practices in data science and engineering,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Aiml', 'Ab Testing']",2025-06-12 05:34:58
Data Scientist,Devon Software Services,3 - 7 years,12-19 Lacs P.A.,['Bengaluru'],"What you will do\nBuild end-to-end machine learning models to solve business problems in Marketing\nPerform feature engineering and support data engineering to build robust data pipelines on large marketing datasets from different sources\nCollaborate with ML Engineering to build ML Pipelines to Train, Test, Deploy, Serve and Monitor models, Tune Hyperparameters, detect model and data drift and resolve issues\nPresent machine learning models outcomes, and help interpret model predictions to various stakeholders using standard data visualization tools",,,,"['Tensorflow', 'Ai Algorithms', 'Ml Algorithms', 'Machine Learning', 'Python', 'Pytorch', 'Model Development']",2025-06-12 05:35:01
Data Scientist,Ltimindtree,6 - 11 years,Not Disclosed,['Bengaluru'],"F2F Weekend Drive - Bangalore- 14th June - DS Gen AI\n\nJob description\n\nWe are having a F2F weekend drive for the requirement of a Data Scientist + Gen AI at our LTIM Bangalore Whitefield office.\nDate - 14th June 2025\nExperience - 6+ Years\nMandatory Skills - Data Science, Gen AI, Python, RAG and Azure/AWS, AI/ML, NLPt\n\nLocation - LTIMindtree Bangalore Whitefield Office\n\nSecondary - (Any) Machine Learning, Deep Learning, ChatGPT, Langchain, Prompt, vector stores, RAG, llama, Computer vision, Deep learning, Machine learning, OCR, Transformer, regression, forecasting, classification, hyper parameter tunning, MLOps, Inference, Model training, Model Deployment\nGeneric JD-\nMore than 6 years of experience in Data Engineering, Data Science and AI / ML domain\nExcellent understanding of machine learning techniques and algorithms, such as GPTs, CNN, RNN, k-NN, Naive Bayes, SVM, Decision Forests, etc.\nExperience using business intelligence tools (e.g. Tableau, PowerBI) and data frameworks (e.g. Hadoop)\nExperience in Cloud native skills.\nKnowledge of SQL and Python; familiarity with Scala, Java or C++ is an asset\nAnalytical mind and business acumen and Strong math skills (e.g. statistics, algebra)\nExperience with common data science toolkits, such as TensorFlow, KERAs, PyTorch, PANDAs, Microsoft CNTK, NumPy etc. Deep expertise in at least one of these is highly desirable.\nExperience with NLP, NLG and Large Language Models like BERT, LLaMa, LaMDA, GPT, BLOOM, PaLM, DALL-E, etc.\nGreat communication and presentation skills. Should have experience in working in a fast-paced team culture.\nExperience with AIML and Big Data technologies like AWS SageMaker, Azure Cognitive Services, Google Colab, Jupyter Notebook, Hadoop, PySpark, HIVE, AWS EMR etc.\nExperience with NoSQL databases, such as MongoDB, Cassandra, HBase, Vector databases\nGood understanding of applied statistics skills, such as distributions, statistical testing, regression, etc.\nShould be a data-oriented person with analytical mind and business acumen.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Artificial Intelligence', 'Machine Learning', 'Python']",2025-06-12 05:35:03
Director Data Science,Astar Data,10 - 17 years,Not Disclosed,['Bengaluru'],"Sigmoid enables business transformation using data and analytics, leveraging real-time insights to make accurate and fast business decisions, by building modern data architectures using cloud and open source. Some of the worlds largest data producers engage with Sigmoid to solve complex business problems. Sigmoid brings deep expertise in data engineering, predictive analytics, artificial intelligence, and DataOps. Sigmoid has been recognized as one of the fastest growing technology companies in North America, 2021, by Financial Times, Inc. 5000, and Deloitte Technology Fast 500.\nOffices: New York | Dallas | San Francisco | Lima | Bengaluru\nThe below role is for our Bengaluru office.\n\nWhy Join Sigmoid?\n• Sigmoid provides the opportunity to push the boundaries of what is possible by seamlessly\ncombining technical expertise and creativity to tackle intrinsically complex business\nproblems and convert them into straight-forward data solutions.\n• Despite being continuously challenged, you are not alone. You will be part of a fast-paced\ndiverse environment as a member of a high-performing team that works together to\nenergize and inspire each other by challenging the status quo\n• Vibrant inclusive culture of mutual respect and fun through both work and play\nRoles and Responsibilities:\n• Convert broad vision and concepts into a structured data science roadmap, and guide a\nteam to successfully execute on it.\n• Handling end-to-end client AI & analytics programs in a fluid environment. Your role will be a\ncombination of hands-on contribution, technical team management, and client interaction.\n• Proven ability to discover solutions hidden in large datasets and to drive business results\nwith their data-based insights\n• Contribute to internal product development initiatives related to data science.\n• Drive excellent project management required to deliver complex projects, including\neffort/time estimation.\n• Be proactive, with full ownership of the engagement. Build scalable client engagement level\nprocesses for faster turnaround & higher accuracy\n• Define Technology/ Strategy and Roadmap for client accounts, and guides implementation\nof that strategy within projects\n• Manage the team-members, to ensure that the project plan is being adhered to over the\ncourse of the project\n• Build a trusted advisor relationship with the IT management at clients and internal accounts\nleadership.\nMandated Skills:\n• A B-Tech/M-Tech/MBA from a top tier Institutepreferably in a quantitativesubject\n• 10+ years of hands-onexperience in applied Machine Learning, AI and analytics\n• Experience of scientific programming in scripting languages like Python, R, SQL, NoSQL,\nSpark with ML tools & Cloud Technology (AWS, Azure, GCP)\n• Experience in Python libraries such as numpy, pandas, scikit-learn, tensor-flow, scrapy, BERT\netc. Strong grasp of depth and breadth of machine learning, deep learning, data mining, and\nstatistical concepts and experience in developing models and solutions in these areas\n• Expertise with client engagement, understanding complex problem statements, and offering\nsolutions in the domains of Supply Chain, Manufacturing, CPG, Marketing etc.\nDesired Skills:\nDeep understanding of ML algorithms for common use cases in both structured and\nunstructured data ecosystems.\nComfortable with large scale data processing and distributed computing\nProviding required inputs to sales, and pre-sales activities\nA self-starter who can work well with minimalguidance\nExcellent written and verbal communication skills",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Machine Learning', 'Algorithm Development', 'Pattern Recognition', 'Opencv', 'Image Processing', 'Artificial Intelligence', 'Natural Language Processing', 'Neural Networks', 'Computer Vision', 'Deep Learning']",2025-06-12 05:35:05
Principal Architect-Data & Cloud,Tothr,12 - 18 years,40-45 Lacs P.A.,"['Chennai', 'Bengaluru']","Experience in Hadoop, GCP/AWS/Azure Cloud\nETL technologies on Cloud like Spark, Pyspark/Scala, Dataflow\nETL tools like Informatica/DataStage/OWB/Talend\nExperience inS3, Cloud Storage, Athena, Glue, Sqoop, Flume, Hive, Kafka, Pub-Sub\n\nRequired Candidate profile\nMore than 10 years of experience in Technical, Solutioning, and Analytical roles.\n5+ years of experience in building and managing Data Lakes, Data Warehouse, Data Integration,",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Cloud', 'Technical', 'Analytical', 'MongoDB', 'data lake', 'Pyspark', 'Data Migration', 'Data Warehousing', 'Data Integration']",2025-06-12 05:35:08
Lead Data Scientist,Tothr,8 - 12 years,20-22.5 Lacs P.A.,"['Chennai', 'Bengaluru']","Experience working closely with other data scientists, data engineers' software engineers, data managers and business partners.\n7+ years in designing, planning, prototyping, productionizing, maintaining\nknowledge in Python, Go, Java,\nSQL knowledge",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Statistical Modeling', 'Python Development', 'Machine Learning', 'Deep Learning', 'Generative Ai', 'Advance Sql']",2025-06-12 05:35:10
TS - Data Architect & Database Expert,IT Company,7 - 12 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","DATA ARCHITECT- Data Architecture, Big Data, Data Modeling, or Database Administration, any DBMS,Oracle/SQL Server/PostgreSQL/MySQL\n\nDatabase Expert-Database Mgmt, SQL, Data Modeling, Data Warehousing, ETL, any DBMS Oracle/SQL Server/PostgreSQL/MySQL",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Database Administration', 'Data Architect', 'Database Management', 'Data Modeling', 'Data Architecture', 'Postgres Database', 'Postgresql', 'Big Data', 'Data Warehousing', 'Oracle', 'SQL', 'Data Warehousing Concepts']",2025-06-12 05:35:12
Opening For Senior Machine Learning Engineer with Fareportal,Fareportal,2 - 5 years,Not Disclosed,['Gurugram'],"Title: Senior Machine Learning Engineer\nLocation: Gurgaon, IN\nType: (Hybrid, In-Office)\nJob Description\n\nWho We Are:\nFareportal is a travel technology company powering a next-generation travel concierge service. Utilizing its innovative technology and company owned and operated global contact centres, Fareportal has built strong industry partnerships providing customers access to over 500 airlines, a million lodgings, and hundreds of car rental companies around the globe. With a portfolio of consumer travel brands including CheapOair and OneTravel, Fareportal enables consumers to book-online, on mobile apps for iOS and Android, by phone, or live chat. Fareportal provides its airline partners with access to a broad customer base that books high-yielding international travel and add-on ancillaries.",,,,"['Containerization', 'Machine Learning', 'Python', 'azure', 'sql', 'api', 'nosql', 'deployment']",2025-06-12 05:35:14
Data Scientist - ML,Client Of EDGE,8 - 13 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Delhi / NCR']","Our client is a India's marquee global technology company. They are an international flag-bearer of technical and managerial excellence. With offices around the globe, the company has a comprehensive presence across multiple segments of the IT product and service industries.\nWe are Seeking to identify a Data Scientist Engineer role, responsible for Leading the design, development, and deployment of advanced machine learning models and algorithms. Software engineering experience with Strong knowledge of machine learning frameworks (e.g., TensorFlow, PyTorch) and generative AI libraries., Understanding of C++ programming principles.\nLead the design, development, and deployment of advanced machine learning models and algorithms.\nTrain and fine-tune generative models on large datasets, optimizing model performance and efficiency.\nExperience of developing and deploying solutions on Nvidia or Intel software stacks will be an added advantage.\nExcellent problem-solving skills and the ability to work on complex, unstructured challenges.\nProficient Understanding of distributed Computing Principles. Working knowledge of frameworks such as accelerate, deepspeed, etc.\nExperience with hiring, mentoring and leading teams of ML engineers, data engineers, etc.\nConduct in-depth data analysis, feature engineering, and data pre-processing to extract meaningful insights.\nDevelop and execute data strategies to collect, process, and store data effectively.\nWork closely with data engineers and architects to ensure data availability and quality.\nCollaborate with cross-functional teams to develop AI-powered solutions that address business challenges and opportunities.\nEnsure the successful integration of AI models into production systems.\nStay up-to-date with the latest trends and advancements in data science and AI.\nDrive research initiatives to explore and implement innovative techniques and technologies.\nLead research initiatives to develop novel AI techniques and technologies.\nCommunicate findings, insights, and project progress to non-technical stakeholders in a clear and understandable manner.\nPublications or contributions to the field of CV, NLP or generative AI are a plus.\nYour Profile:\nAn Engineer with 5+ years of experience in Leading the design, development, and deployment of advanced machine learning models and algorithms.\nUnderstanding of C++ programming.\nStrong knowledge of machine learning frameworks (e.g., TensorFlow, PyTorch) and generative AI libraries.",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['C++', 'Machine Learning', 'Data Scientist', 'AI', 'Development', 'Deployment']",2025-06-12 05:35:17
Data Scientist(0217),A Reputed Organization,5 - 10 years,Not Disclosed,"['Kolkata', 'Pune', 'Bengaluru']","Lead the design, development, and deployment of advanced machine learning models and algorithms.\nTrain and fine-tune generative models on large datasets, optimizing model performance and efficiency.\nStrong knowledge of machine learning frameworks (e.g., TensorFlow, PyTorch) and generative AI libraries., Understanding of C++ programming principles.\nExperience of developing and deploying solutions on Nvidia or Intel software stacks will be an added advantage.\nExcellent problem-solving skills and the ability to work on complex, unstructured challenges.\nProficient Understanding of distributed Computing Principles. Working knowledge of frameworks such as accelerate, deepspeed, etc.\nExperience with hiring, mentoring and leading teams of ML engineers, data engineers, etc.\nConduct in-depth data analysis, feature engineering, and data preprocessing to extract meaningful insights.\nDevelop and execute data strategies to collect, process, and store data effectively.\nWork closely with data engineers and architects to ensure data availability and quality.\nCollaborate with cross-functional teams to develop AI-powered solutions that address business challenges and opportunities.\nEnsure the successful integration of AI models into production systems.\nStay up-to-date with the latest trends and advancements in data science and AI.\nDrive research initiatives to explore and implement innovative techniques and technologies.\nLead research initiatives to develop novel AI techniques and technologies.\nCommunicate findings, insights, and project progress to non-technical stakeholders in a clear and understandable manner.\nPublications or contributions to the field of CV, NLP or generative AI are a plus.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'C++', 'Data Scientist', 'AI']",2025-06-12 05:35:19
Data Solution Architect,Maveric,13 - 20 years,Not Disclosed,"['Chennai', 'Bengaluru']","Position Overview\nWe are looking for a highly experienced and versatile Solution Architect Data to lead the solution design and delivery of next-generation data solutions for our BFS clients. The ideal candidate will have a strong background in data architecture and engineering, deep domain expertise in financial services, and hands-on experience with cloud-native data platforms and modern data analytics tools. The role will require architecting solutions across Retail, Corporate, Wealth, and Capital Markets, as well as Payments, Lending, and Onboarding journeys. Possession of Data Analytics and Exposure to Data regulatory domain will be of distinct advantage. Hands on experience of AI & Gen AI enabling data related solution will be a distinct advantage for the position.",,,,"['Data Quality', 'Data Engineering', 'Data Governance', 'GenAI']",2025-06-12 05:35:21
Data Analyst-Having Stratup-Mid-Size companies Exp.@ Bangalore_Urgent,"A leader in this space, we deliver world...",8 - 13 years,Not Disclosed,['Bengaluru'],"Data Analyst\n\nLocation: Bangalore\nExperience: 8 - 15 Yrs\nType: Full-time\n\nRole Overview\n\nWe are seeking a skilled Data Analyst to support our platform powering operational intelligence across airports and similar sectors. The ideal candidate will have experience working with time-series datasets and operational information to uncover trends, anomalies, and actionable insights. This role will work closely with data engineers, ML teams, and domain experts to turn raw data into meaningful intelligence for business and operations stakeholders.\n\nKey Responsibilities\n\nAnalyze time-series and sensor data from various sources\nDevelop and maintain dashboards, reports, and visualizations to communicate key metrics and trends.\nCorrelate data from multiple systems (vision, weather, flight schedules, etc) to provide holistic insights.\nCollaborate with AI/ML teams to support model validation and interpret AI-driven alerts (e.g., anomalies, intrusion detection).\nPrepare and clean datasets for analysis and modeling; ensure data quality and consistency.\nWork with stakeholders to understand reporting needs and deliver business-oriented outputs.\n\n\nQualifications & Required Skills\n\nBachelors or Masters degree in Data Science, Statistics, Computer Science, Engineering, or a related field.\n5+ years of experience in a data analyst role, ideally in a technical/industrial domain.\nStrong SQL skills and proficiency with BI/reporting tools (e.g., Power BI, Tableau, Grafana).\nHands-on experience analyzing structured and semi-structured data (JSON, CSV, time-series).\nProficiency in Python or R for data manipulation and exploratory analysis.\nUnderstanding of time-series databases or streaming data (e.g., InfluxDB, Kafka, Kinesis).\nSolid grasp of statistical analysis and anomaly detection methods.\nExperience working with data from industrial systems or large-scale physical infrastructure.\n\n\nGood-to-Have Skills\n\nDomain experience in airports, smart infrastructure, transportation, or logistics.\nFamiliarity with data platforms (Snowflake, BigQuery, Custom-built using open-source).\nExposure to tools like Airflow, Jupyter Notebooks and data quality frameworks.\nBasic understanding of AI/ML workflows and data preparation requirements.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Kafka', 'SQL', 'airports', 'InfluxDB', 'Airflow', 'structured Data', 'time-series', 'JSON', 'Tableau', 'Grafana', 'R', 'AI/ML', 'Kinesis', 'Snowflake', 'time-series databases', 'Data Preparation', 'Python', 'smart infrastructure', 'BigQuery', 'streaming data', 'Power BI', 'CSV', 'transportation', 'logistic', 'reporting tools']",2025-06-12 05:35:24
"Sustainable, Client and Regulatory Reporting Data Product Owner",Capital Markets,15 - 20 years,Not Disclosed,['Bengaluru'],"Hiring, Sustainable, Client and Regulatory Reporting Data Product Owner - ISS Data (Associate Director)\nAbout your team\n\nThe Technology function provides IT services that are integral to running an efficient run-the business operating model and providing change-driven solutions to meet outcomes that deliver on our business strategy. These include the development and support of business applications that underpin our revenue, operational, compliance, finance, legal, marketing and customer service functions. The broader organisation incorporates Infrastructure services that the firm relies on to operate on a day-to-day basis including data centre, networks, proximity services, security, voice, incident management and remediation.\nThe Technology group is responsible for providing Technology solutions to the Investment Solutions & Services business (which covers Investment Management, Asset Management Operations & Distribution business units globally)\n\nThe Technology team supports and enhances existing applications as well as designs, builds and procures new solutions to meet requirements and enable the evolving business strategy.\nAs part of this group, a dedicated Data Programme team has been mobilised as a key foundational programme to support the execution of the overarching Investment Solutions and Service strategy.\n\nAbout your role\nThe Investment Reporting Data Product Owner role is instrumental in the creation and execution of a future state data reporting product to enable Regulatory, Client, Vendor, Internal & MI reporting and analytics. The successful candidate will have an in- depth knowledge of all data domains that represent institutional clients , the investment life cycle , regulatory and client reporting data requirements.\nThe role will sit within the ISS Delivery Data Analysis chapter and fully aligned with our cross functional ISS Data Programme in Technology, and the candidate will leverage their extensive industry knowledge to build a future state platform in collaboration with Business Architecture, Data Architecture, and business stakeholders.\nThe role is to maintain strong relationships with the various business contacts to ensure a superior service to our internal business stakeholders and our clients.\n\nKey Responsibilities\n\nLeadership and Management:\nLead the ISS distribution, Client Propositions, Sustainable Investing and Regulatory reporting data outcomes defining the data roadmap and capabilities and supporting the execution and delivery of the data solutions as a Data Product lead within the ISS Data Programme.\nLine management responsibilities for junior data analysts within the chapter, coaching, influencing and motivating them for high performance.\nDefine the data product vision and strategy with end-to-end thought leadership.\nLead and define the data product backlog , documentation, enable peer-reviews, analysis effort estimation, maintain backlog, and support end to end planning.\nBe a catalyst of change for driving efficiencies, scale and innovation.\n\nData Quality and Integrity:\nDefine data quality use cases for all the required data sets and contribute to the technical frameworks of data quality.\nAlign the functional solution with the best practice data architecture & engineering.\n\nCoordination and Communication:\nSenior management level communication to influence senior tech and business stakeholders globally, get alignment on the roadmaps.\nCoordinate with internal and external teams to communicate with those impacted by data flows.\nAn advocate for the ISS Data Programme.\nCollaborate closely with Data Governance, Business Architecture, and Data owners etc.\nConduct workshops within the scrum teams and across business teams, effectively document the minutes and drive the actions.\n\nAbout you\nThe Investment Reporting Data Product Owner role is instrumental in the creation and execution of a future state data reporting product to enable Regulatory, Client, Vendor, Internal & MI reporting and analytics. The successful candidate will have an in- depth knowledge of all data domains that represent institutional clients , the investment life cycle , regulatory and client reporting data requirements.\nThe role will sit within the ISS Delivery Data Analysis chapter and fully aligned with cross functional ISS Data Programme in Technology, and the candidate will leverage their extensive industry knowledge to build a future state platform in collaboration with Business Architecture, Data Architecture, and business stakeholders.\nThe role is to maintain strong relationships with the various business contacts to ensure a superior service to our internal business stakeholders and our clients.\n\nKey Responsibilities\n\nLeadership and Management:\nLead the ISS distribution, Client Propositions, Sustainable Investing and Regulatory reporting data outcomes defining the data roadmap and capabilities and supporting the execution and delivery of the data solutions as a Data Product lead within the ISS Data Programme.\nLine management responsibilities for junior data analysts within the chapter, coaching, influencing and motivating them for high performance.\nDefine the data product vision and strategy with end-to-end thought leadership.\nLead and define the data product backlog , documentation, enable peer-reviews, analysis effort estimation, maintain backlog, and support end to end planning.\nBe a catalyst of change for driving efficiencies, scale and innovation.\n\nData Quality and Integrity:\nDefine data quality use cases for all the required data sets and contribute to the technical frameworks of data quality.\nAlign the functional solution with the best practice data architecture & engineering.\n\nCoordination and Communication:\nSenior management level communication to influence senior tech and business stakeholders globally, get alignment on the roadmaps.\nCoordinate with internal and external teams to communicate with those impacted by data flows.\nAn advocate for the ISS Data Programme.\nCollaborate closely with Data Governance, Business Architecture, and Data owners etc.\nConduct workshops within the scrum teams and across business teams, effectively document the minutes and drive the actions.\n\nYour Skills and Experience\n\nStrong leadership and senior management level communication, internal and external client management and influencing skills.\nAt least 15 years of proven experience as a senior business/technical/data analyst within technology and/or business change delivering data led business outcomes within the financial services/asset management industry.\n5-10 years as a data product owner adhering to agile methodology, delivering data solutions using industry leading data platforms such as Snowflake, State Street Alpha Data, Refinitiv Eikon, SimCorp Dimension, BlackRock Aladdin, FactSet etc.\nOutstanding knowledge of Client life cycle covering institutional & wholesale with a focus on CRM data, Transfer agency data.\nVery good understanding of the data generated by investment management processes and how that is leveraged in Go-to market capabilities such as client reporting, Sales, Marketing.\nExcellent knowledge of regulatory environment with a focus on European regulations and ESG specific ones such as MIFID II, EMIR, SFDR.\nWork effortlessly in different operating models such as insourcing, outsourcing and hybrid models.\nAutomation mindset that can drive efficiencies and quality in the reporting landscape.\nKnowledge of industry standard data calcs for fund factsheets, Institutional admin and investment reports would be an added advantage.\nIn Depth expertise in data and calculations across the investment industry covering the below.\nClient Specific data: This includes institutional and wholesale client, account and channels data, client preferences and data sets needed for client analytics. Knowledge of Salesforce desirable.\nTransfer Agency & Platform data: This includes granular client holdings at various levels, client transactions and relevant ref data. Knowledge of role of TPAs as TA and integrating external feeds/products with strategic inhouse data platforms.\nInvestment data: This includes investment life cycle data covering data domains such as trading, ABOR, IBOR, Security and fund reference.\nShould possess Problem Solving, Attention to detail, Critical thinking.\nTechnical Skills: Hands on SQL, Advanced Excel, Python, ML (optional) and knowledge of end-to-end tech solutions involving data platforms.\nKnowledge of data management, data governance, and data engineering practices\nHands on experience with data modelling techniques such as dimensional, data vault.\nWillingness to own and drive things, collaboration across business and tech stakeholders.",Industry Type: Investment Banking / Venture Capital / Private Equity,Department: Product Management,"Employment Type: Full Time, Permanent","['Data Transformation', 'ESG Framework', 'Snowflake', 'Asset Management', 'Product Owner', 'Product Manager', 'MIFID II', 'alphastate street', 'SQL', 'EMIR', 'Data Quality', 'Data Analysis', 'charles river', 'Agile', 'UK Regulatory Reporting', 'data roadmap', 'Capital Market Operations', 'Aladdin', 'SFDR.', 'Python', 'ML']",2025-06-12 05:35:27
"Senior Engineer, Application Development",S&P Global Market Intelligence,5 - 8 years,Not Disclosed,['Hyderabad'],"Grade Level (for internal use):\n10\nMarket Intelligence\nThe Role: Senior Full Stack Developer\nGrade level :10\nThe Team: You will work with a team of intelligent, ambitious, and hard-working software professionals. The team is responsible for the architecture, design, development, quality, and maintenance of the next-generation financial data web platform. Other responsibilities include transforming product requirements into technical design and implementation. You will be expected to participate in the design review process, write high-quality code, and work with a dedicated team of QA Analysts, and Infrastructure Teams\nThe Impact: Market Intelligence is seeking a Software Developer to create software design, development, and maintenance for data processing applications. This person would be part of a development team that manages and supports the internal & external applications that is supporting the business portfolio. This role expects a candidate to handle any data processing, big data application development. We have teams made up of people that learn how to work effectively together while working with the larger group of developers on our\nplatform.\nWhats in it for you:\nOpportunity to contribute to the development of a world-class Platform Engineering team .\nEngage in a highly technical, hands-on role designed to elevate team capabilities and foster continuous skill enhancement.\nBe part of a fast-paced, agile environment that processes massive volumes of dataideal for advancing your software development and data engineering expertise while working with a modern tech stack.\nContribute to the development and support of Tier-1, business-critical applications that are central to operations.\nGain exposure to and work with cutting-edge technologies including AWS Cloud , EMR and Apache NiFi .\nGrow your career within a globally distributed team , with clear opportunities for advancement and skill development.\nResponsibilities:\nDesign and develop applications, components, and common services based on development models, languages and tools, including unit testing, performance testing and monitoring and implementation\nSupport business and technology teams as necessary during design, development and delivery to ensure scalable and robust solutions\nBuild data-intensive applications and services to support and enhance fundamental financials in appropriate technologies.( C#, .Net Core, Databricsk, Spark ,Python, Scala, NIFI , SQL)\nBuild data modeling, achieve performance tuning and apply data architecture concepts\nDevelop applications adhering to secure coding practices and industry-standard coding guidelines, ensuring compliance with security best practices (e.g., OWASP) and internal governance policies.\nImplement and maintain CI/CD pipelines to streamline build, test, and deployment processes; develop comprehensive unit test cases and ensure code quality\nProvide operations support to resolve issues proactively and with utmost urgency\nEffectively manage time and multiple tasks\nCommunicate effectively, especially written with the business and other technical groups\nWhat Were Looking For:\nBasic Qualifications:\nBachelorsMasters Degree in Computer Science, Information Systems or equivalent.\nMinimum 5 to 8 years of strong hand-development experience in C#, .Net Core, Cloud Native, MS SQL Server backend development. Proficiency with Object Oriented Programming.\nAdvance SQL programming skills\nPreferred experience or familiarity with tools and technologies such as Odata, Grafana, Kibana, Big Data platforms, Apache Kafka, GitHub, AWS EMR, Terraform, and emerging areas like AI/ML and GitHub Copilot.\nHighly recommended skillset in Databricks, SPARK, Scalatechnologies.\nUnderstanding of database performance tuning in large datasets\nAbility to manage multiple priorities efficiently and effectively within specific timeframes\nExcellent logical, analytical and communication skills are essential, with strong verbal and writing proficiencies\nKnowledge of Fundamentals, or financial industry highly preferred.\nExperience in conducting application design and code reviews\nProficiency with following technologies:\nObject-oriented programming\nPrograming Languages (C#, .Net Core)\nCloud Computing\nDatabase systems (SQL, MS SQL)\nNice to have: No-SQL (Databricks, Spark, Scala, python), Scripting (Bash, Scala, Perl, Powershell)\nPreferred Qualifications:\nHands-on experience with cloud computing platforms including AWS , Azure , or Google Cloud Platform (GCP) .\nProficient in working with Snowflake and Databricks for cloud-based data analytics and processing.\nBenefits:\nHealth & Wellness: Health care coverage designed for the mind and body.\nContinuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\nInvest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\nFamily Friendly Perks: Its not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\nBeyond the Basics: From retail discounts to referral incentive awardssmall perks can make a big difference.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GitHub Copilot', 'AI/ML', 'Kibana', 'python', 'GitHub', 'Scala', 'AWS EMR', 'Grafana', 'Odata', 'Big Data platforms', 'Terraform', 'Apache Kafka', 'Databricks', 'Spark']",2025-06-12 05:35:30
Gen AI- Sr. Engineer/Lead,Iris Software,7 - 12 years,Not Disclosed,"['Pune', 'Chennai', 'Delhi / NCR']","Programming Skills: Advanced proficiency in Python, with experience in AI/ML frameworks.\nAzure DevOps: Expertise in version control systems (e.g., Git) and Azure DevOps tools for automation and monitoring.\nCI/CD Pipelines: Proven ability to design and implement continuous integration and deployment workflows.\nInfrastructure as Code: Experience with ARM templates or Terraform for provisioning cloud infrastructure.\nContainerization: Strong knowledge of Docker and Kubernetes for application deployment and orchestration.\nExperience:\n8+ years of experience in software development, with 3+ years in AI/ML or Generative AI projects.\nDemonstrated experience in deploying and managing AI applications in production environments.\nProven track record of implementing DevOps best practices and automation strategies.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Gen AI', 'Azure', 'Devops', 'Python', 'Sql']",2025-06-12 05:35:32
MDM Associate Data Steward,Amgen Inc,0 - 3 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\n\nRole Description\n\nWe are seeking an MDM Associate Data Steward who will be responsible for ensuring the accuracy, completeness, and reliability of master data across critical business domains such as Customer, Product, Affiliations, and Payer. This role involves actively managing and curating master data through robust data stewardship processes, comprehensive data cataloging, and data governance frameworks utilizing Informatica or Reltio MDM platforms. Additionally, the incumbent will perform advanced data analysis, data validation, and data transformation tasks through SQL queries and Python scripts to enable informed, data-driven business decisions. The role emphasizes cross-functional collaboration with various teams, including Data Engineering, Commercial, Medical, Compliance, and IT, to align data management activities with organizational goals and compliance standards.\n\nRoles & Responsibilities\nResponsible for master data stewardship, ensuring data accuracy and integrity across key master data domains (e.g., Customer, Product, Affiliations).\nConduct advanced data profiling, cataloging, and reconciliation activities using Informatica or Reltio MDM platforms.\nManage the reconciliation of potential matches, ensuring accurate resolution of data discrepancies and preventing duplicate data entries.\nEffectively manage Data Change Request (DCR) processes, including reviewing, approving, and documenting data updates in compliance with established procedures and SLAs.\nExecute and optimize SQL queries for validation and analysis of master data.\nPerform basic Python for data transformation, quality checks, and automation.\nCollaborate effectively with cross-functional teams including Data Engineering, Commercial, Medical, Compliance, and IT to fulfill data requirements.\nSupport user acceptance testing (UAT) and system integration tests for MDM related system updates.\nImplement data governance processes ensuring compliance with enterprise standards, policies, and frameworks.\nDocument and maintain accurate SOPs, Data Catalogs, Playbooks, and SLAs.\nIdentify and implement process improvements to enhance data stewardship and analytic capabilities.\nPerform regular audits and monitoring to maintain high data quality and integrity.\nBasic Qualifications and Experience\nMasters degree with 1 - 3 years of experience in Business, Engineering, IT or related fieldOR\nBachelors degree with 2 - 5 years of experience in Business, Engineering, IT or related fieldOR\nDiploma with 6 - 8 years of experience in Business, Engineering, IT or related field\nFunctional\n\nSkills:\nMust-Have Skills:\nDirect experience in data stewardship, data profiling, and master data management.\nHands-on experience with Informatica or Reltio MDM platforms.\nProficiency in SQL for data analysis and querying.\nKnowledge of data cataloging techniques and tools.\nBasic proficiency in Python scripting for data processing.\nGood-to-Have\n\nSkills:\nExperience with PySpark and Databricks for large-scale data processing.\nBackground in the pharmaceutical, healthcare, or life sciences industries.\nFamiliarity with AWS or other cloud-based data solutions.\nStrong project management and agile workflow familiarity (e.g., using Jira, Confluence).\nUnderstanding of regulatory compliance related to data protection (GDPR, CCPA).\nProfessional Certifications\nAny ETL certification ( e.g. Informatica)\nAny Data Analysis certification (SQL)\nAny cloud certification (AWS or AZURE)\nSoft\n\nSkills:\nStrong analytical abilities to assess and improve master data processes and solutions.\nExcellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders.\nEffective problem-solving skills to address data-related issues and implement scalable solutions.\nAbility to work effectively with global, virtual teams",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data management', 'python', 'project management', 'data analysis', 'data stewardship', 'agile database', 'data processing', 'sql', 'data profiling']",2025-06-12 05:35:34
Data Architecture,Top B2B MNC in Management Consulting Dom...,5 - 8 years,Not Disclosed,['Bengaluru'],"About the Company\nGreetings from Teamware Solutions a division of Quantum Leap Consulting Pvt. Ltd\n\nAbout the Role\nWe are hiring a Data Architecture\n\nLocation: Bangalore\nWork Model: Hybrid\nExperience: 5-9 Years\nNotice Period: Immediate to 15 Days\n\nJob Description:\nData Architecture, Data Governance, Data Modeling\n\nAdditional Information:\nMandatory Skills: Data Architecture, Data Governance, Data Modeling\nNice to have skills Certification in Data Engineering\nInterview Mode Virtual Interview\nminimum 5 yrs relevant experience and maximum 9 yrs for this requirement. Someone with more experience in building PySpark data streaming jobs on Azure Databricks\nwho have done real projects, have expertise, and hands-on experience also\nAlso, Data governance and data modeling experience with a minimum of 4 years is mandatory\nCommunication should be excellent\n\n\nPlease let me know if you are interested in this position and send me your resumes to netra.s@twsol.com",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Architecture', 'Data Modeling', 'Data Governance', 'Data Engineering']",2025-06-12 05:35:36
Data Modelling and Data Visualization Specialist (Power BI),Krish Services Group,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Description Data Modelling and Data Visualization Specialist (Power BI)\n\n\nCompany Description - Krish is committed to enabling customers to achieve their technological goals by delivering solutions that combine the right technology, people, and costs. Our approach emphasizes building long-term relationships while ensuring customer success through tailored solutions, leveraging the expertise and integrity of our consultants and robust delivery processes.",,,,"['Power Bi', 'SSRS', 'SSIS', 'SQL Azure', 'Azure Data Factory', 'Azure Databricks', 'SQL Server', 'Tableau', 'Power Query', 'SQL', 'Onpremise', 'Azure Data Lake', 'Data Visualization', 'Dax', 'Data Modeling', 'ETL']",2025-06-12 05:35:39
Data Architect - AWS,Happiest Minds Technologies,10 - 15 years,Not Disclosed,"['Noida', 'Pune', 'Bengaluru']","Roles and responsibilities\nWork closely with the Product Owners and stake holders to design the Technical Architecture for data platform to meet the requirements of the proposed solution.\nWork with the leadership to set the standards for software engineering practices within the machine learning engineering team and support across other disciplines\nPlay an active role in leading team meetings and workshops with clients.\nChoose and use the right analytical libraries, programming languages, and frameworks for each task.",,,,"['SQL', 'data architect', 'Python', 'Pyspark', 'Apache Airflow', 'GLUE', 'Kinesis', 'Amazon Redshift', 'Data Architecture Principles', 'Data Modeling', 'Data Warehousing', 'Athena', 'Lambda', 'AWS']",2025-06-12 05:35:42
Master Data Management Architect,K-logix Partnering Solutions,8 - 13 years,Not Disclosed,[],"Bachelor'sMaster's\nOverview:\n\nWe are seeking a highly skilled and experienced Celonis MDM Data Architect to lead the design, implementation, and optimization of our Master Data Management (MDM) solutions in alignment with Celonis Process Mining and Execution Management System (EMS) capabilities.\nThe ideal candidate will play a key role in bridging data architecture and business process insights, ensuring data quality, consistency, and governance across the enterprise.\n\nKey Responsibilities:\nDesign and implement MDM architecture and data models aligned with enterprise standards and best practices.\n• Lead the integration of Celonis with MDM platforms to drive intelligent process automation, data governance, and operational efficiencies.\n• Collaborate with business stakeholders and data stewards to define MDM policies, rules, and processes.\n• Support data profiling, data cleansing, and data harmonization efforts to improve master data quality.\n• Work closely with Celonis analysts, data engineers, and process owners to deliver actionable insights based on MDM-aligned process data.\n• Develop and maintain scalable, secure, and high-performance data pipelines and integration architectures.\n• Translate business requirements into technical solutions, ensuring alignment with both MDM and Celonis data models.\n• Create and maintain data architecture documentation, data dictionaries, and metadata repositories.\n• Monitor and optimize the performance of MDM systems and Celonis EMS integrations.\nQualifications:\nBachelors or Masters degree in Computer Science, Information Systems, Data Engineering, or a related field.\n• 7+ years of experience in data architecture, MDM, or enterprise data management.\n• 2+ years of hands-on experience with Celonis and process mining tools.\n• Proficient in MDM platforms (e.g., Informatica MDM, SAP MDG, Oracle MDM, etc.).\n• Strong knowledge of data modeling, data governance, and metadata management.\n• Proficiency in SQL, data integration tools (e.g., ETL/ELT platforms), and APIs.\n• Deep understanding of business process management and data-driven transformation initiatives.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Celonis', 'MDM', 'Master Data Management', 'ETL', 'Elt']",2025-06-12 05:35:45
Ai Ml Engineer,Optum,5 - 10 years,Not Disclosed,['Noida'],"Optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health optimization on a global scale. Join us to start Caring. Connecting. Growing together.  \nAI Engineer is tasked with the design, development, and deployment of advanced generative AI models and systems. This position requires close collaboration with data scientists, product managers, and other stakeholders to integrate generative AI solutions into existing products and develop new innovative features. Proficiency in the Agentic AI framework is vital for coordinating multiple autonomous AI agents to accomplish complex tasks.\n\nPrimary Responsibilities:\nImplement Generative AI Models: Develop sophisticated generative AI algorithms and models to create new data samples, patterns, or content based on existing data or inputs\nData Processing: Collaborate with stakeholders to preprocess, analyze, and interpret extensive datasets\nModel Deployment: Deploy generative AI models into production environments, ensuring scalability and robustness\nOptimization: Conduct model testing, validation, and optimization to enhance performance\nIntegration: Work with cross-functional teams to seamlessly integrate generative AI solutions into products\nResearch: Stay current with the latest advancements in generative AI technologies and practices\nAgentic AI Framework: Utilize the Agentic AI framework to coordinate multiple AI agents for the completion of complex tasks\nMentorship: Provide mentorship to junior team members and offer technical guidance\nComply with the terms and conditions of the employment contract, company policies and procedures, and any and all directives (such as, but not limited to, transfer and/or re-assignment to different work locations, change in teams and/or work shifts, policies in regards to flexibility of work benefits and/or work environment, alternative work arrangements, and other decisions that may arise due to the changing business environment). The Company may adopt, vary or rescind these policies and directives in its absolute discretion and without any limitation (implied or otherwise) on its ability to do so\n\nRequired Qualifications:\nBachelor's or Master's degree in Computer Science, Engineering, or a related field\n5+ years of experience in software engineering with a focus on AI/ML\nExperience with data preprocessing and analysis\nKnowledge of the Agentic AI framework and its application in AI systems\nProficiency in machine learning frameworks such as TensorFlow and PyTorch\nSolid programming skills in Python, Java, or C++\nFamiliarity with cloud platforms (e.g., AWS, Google Cloud, Azure)\nProven excellent problem-solving abilities and algorithmic thinking\nProven solid communication and teamwork skills\n\nPreferred Qualifications:\nExperience with data processing\nKnowledge of version control systems like Git\nUnderstanding of Generative AI, associated technologies and frameworks like RAG, agents etc.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Agentic Ai', 'Gen AI', 'Cloud', 'RAG', 'LLM']",2025-06-12 05:35:47
"Lead Data Scientist, Operations || Mumbai || Max 38 LPA",Argus India Price Reporting Services,5 - 10 years,20-35 Lacs P.A.,"['Mumbai Suburban', 'Navi Mumbai', 'Mumbai (All Areas)']","Lead Data Scientist, Operations\nMumbai, India\n\nAbout Argus:\n\nArgus is the leading independent provider of market intelligence to the global energy and commodity markets. We offer essential price assessments, news, analytics, consulting services, data science tools and industry conferences to illuminate complex and opaque commodity markets.\nHeadquartered in London with 1,500 staff, Argus is an independent media organisation with 30 offices in the worlds principal commodity trading hubs.\nCompanies, trading firms and governments in 160 countries around the world trust Argus data to make decisions, analyse situations, manage risk, facilitate trading and for long-term planning. Argus prices are used as trusted benchmarks around the world for pricing transportation, commodities and energy.\nFounded in 1970, Argus remains a privately held UK-registered company owned by employee shareholders and global growth equity firm General Atlantic.\nWhat were looking for:\nJoin our Generative AI team to lead a new group in India, focused on creating and maintaining AI-ready data. As the point of contact in Mumbai, you will guide the local team and ensure seamless collaboration with our global counterparts. Your contributions will directly impact the development of innovative solutions used by industry leaders worldwide, supporting text and numerical data extraction, curation, and metadata enhancements to accelerate development and ensure rapid response times. You will play a pivotal role in transforming how our data are seamlessly integrated with AI systems, paving the way for the next generation of customer interactions.\n\nWhat will you be doing:\n\nLead and Develop the Team: Oversee a team of data scientists in Mumbai. Mentoring and guiding junior team members, fostering their professional growth and development.\nStrategic Planning: Develop and implement strategic plans for data science projects, ensuring alignment with the company's goals and objectives.\nAI-Ready Data Development: Design, develop, and maintain high-quality AI-ready datasets, ensuring data integrity, usability, and scalability to support advanced Generative AI models.\nAdvanced Data Processing: Drive hands-on efforts in complex data extraction, cleansing, and curation for diverse text and numerical datasets. Implement sophisticated metadata enrichment strategies to enhance data utility and accessibility for AI systems.\nAlgorithm Implementation & Optimization: Implement and optimize state-of-the-art algorithms and pipelines for efficient data processing, feature engineering, and data transformation tailored for LLM and GenAI applications.\nGenAI Application Development: Apply and integrate frameworks like LangChain and Hugging Face Transformers to build modular, scalable, and robust Generative AI data pipelines and applications.\nPrompt Engineering Application: Apply advanced prompt engineering techniques to optimize LLM performance for specific data extraction, summarization, and generation tasks, working closely with the Lead's guidance.\nLLM Evaluation Support: Contribute to the systematic evaluation of Large Language Models (LLMs) outputs, analysing quality, relevance, and accuracy, and supporting the implementation of LLM-as-a-judge frameworks.\nRetrieval-Augmented Generation (RAG) Contribution: Actively contribute to the implementation and optimization of RAG systems, including working with embedding models, vector databases, and, where applicable, knowledge graphs, to enhance data retrieval for GenAI.\nTechnical Leadership: Act as a technical leader and subject matter expert for junior data scientists, providing guidance on best practices in coding and PR reviews, data handling, and GenAI methodologies.\nCross-Functional Collaboration: Collaborate effectively with global data science teams, engineering, and product stakeholders to integrate data solutions and ensure alignment with broader company objectives.\nOperational Excellence: Troubleshoot and resolve data-related issues promptly to minimize potential disruptions, ensuring high operational efficiency and responsiveness.\nDocumentation & Code Quality: Produce clean, well-documented, production-grade code, adhering to best practices for version control and software engineering.\n\nSkills and Experience:\n\nLeadership Experience: Proven track record in leading and mentoring data science teams, with a focus on strategic planning and operational excellence.\nAcademic Background: Advanced degree in AI, statistics, mathematics, computer science, or a related field.\nProgramming and Frameworks: 5+ years of hands-on experience with Python, TensorFlow or PyTorch, and NLP libraries such as spaCy and Hugging Face.\nGenAI Tools: 2+ years of Practical experience with LangChain, Hugging Face Transformers, and embedding models for building GenAI applications.\nPrompt Engineering: Deep expertise in prompt engineering, including prompt tuning, chaining, and optimization techniques.\nLLM Evaluation: Experience evaluating LLM outputs, including using LLM-as-a-judge methodologies to assess quality and alignment.\nRAG and Knowledge Graphs: Practical understanding and experience using vector databases. In addition, familiarity with graph-based RAG architectures and the use of knowledge graphs to enhance retrieval and reasoning would be a strong plus.\nCloud: 2+ years of experience with Gemini/OpenAI models and cloud platforms such as AWS, Google Cloud, or Azure. Proficient with Docker for containerization.\nData Engineering: Strong understanding of data extraction, curation, metadata enrichment, and AI-ready dataset creation.\nCollaboration and Communication: Excellent communication skills and a collaborative mindset, with experience working across global teams.\n\nWhats in it for you:\n\nCompetitive salary\nHybrid Working Policy (3 days in Mumbai office/ 2 days WFH once fully inducted)\nGroup healthcare scheme\n18 days annual leave\n8 days of casual leave\nExtensive internal and external training\n\nHours:\n\nThis is a full-time position operating under a hybrid model, with three days in the office and up to two days working remotely.\nThe team supports Argus key business processes every day, as such you will be required to work on a shift-based rota with other members of the team supporting the business until 8pm. Typically support hours run from 11am to 8pm with each member of the team participating up to 2/3 times a week.\n\nFor more details about the company and to apply please make sure you send your CV and cover letter via our website: www.argusmedia.com/en/careers/open-positions\nBy submitting your job application, you automatically acknowledge and consent to the collection, use and/or disclosure of your personal data to the Company. Argus is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, sexual orientation, gender identity, disability or veteran status.",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Huggingface', 'Langchain', 'Spacy', 'Python', 'TensorFlow', 'Pytorch']",2025-06-12 05:35:49
Opening For DevOps Engineer,Randstad Digital,2 - 6 years,8-10 Lacs P.A.,['Chennai'],"Job Type - Third Party Payroll\nPayroll Company Name - Randstad Digital\nPosition - DevOps Engineer\nExperience - 3+Years\nLocation - Chennai only\nNotice Period - Immediate to 15days\nInterview Type - Virtual (Final Round Face2Face Interview)\nWork Mode - 5 days office\n\n\nIf you're interested, kindly share your updated resume and the following details to this below email ID or LinkedIn\n\n\nEmail Id: gayathri.nambi@randstaddigital.com\nLinkedIn: https://www.linkedin.com/in/gayathri-alagia-nambi-0827921a5/\n\n\nFull Name:\nPan Number:\nExperience:\nRelevant Experience:\nNotice period:\nCTC:\nExpected CTC:\nCurrent company:\nPayroll Company:\nLocation:\nPreferred location:\nOffer in hand : (Y/N)\nReason for Job Change:\n\nKey Responsibilities\nCollaborate closely with Product teams, Software, and Data engineers to maintain and develop tools and infrastructure, driving innovation and optimising existing and new product value streams.\nLead and advise on the modernisation of end-to-end ETL and real-time streaming pipelines, refining development processes, and introducing relevant technologies and tools.\nWork extensively with AWS cloud platforms and tools, applying Infrastructure as Code (IaC) practicesparticularly with Terraformand prioritising both cost optimisation and secure network configurations.\nEnsure robust security by implementing and managing cybersecurity frameworks and tools, including [specific security tools currently in use], to protect systems against threats.\nApply network knowledge to optimise and secure infrastructure, ensuring efficient communication and data flow across systems.\nImplement a proactive patch management strategy to keep all systems updated with the latest security patches.\nAdhere to change management protocols and version control standards, ensuring a secure code repository with reliable backup strategies for key intellectual property. (maybe we can rephrase it better)\nEffectively present and explain advanced technical designs to both technical and non-technical stakeholders.\n\nWhat we are looking for:\nEssential\nBachelor’s degree in software, network, or cybersecurity (or demonstrable equal experience)\nAt least 2 years’ experience in a DevOps Engineer role\nProven experience or certification in AWS\nProven experience in troubleshooting and debugging SQL, SNS, SQS, Kinesis, C# (.net Core), S3, FTP, AWS Lambda. HTTP level REST API experience (and previous API standards, e.g SOAP)\nProven experience in operating system administration (on both Wintel and Linux).\nProven experience in networking components and systems, including webservers, firewalls and network routing.\nFamiliarity with automation tooling (such as Jenkins) and runtime integrated analysis tooling (such as New Relic)\nFamiliarity with the design, development and maintenance of best-in-class analytical capabilities, including data warehousing (Redshift, OpenSearch, Athena, SQL, etc)\nFamiliarity of architectural design patterns for micro-services leveraging relational and big data technologies\nDisciplined, self-starter attitude driven to improve systems and processes and a willingness to learn\nExcellent documentation of processes\nProficiency in written and spoken English\nDesirable\nProvisioning new technical assets (e.g EC2 build), Kubernetes, Docker and associated virtualisation or containerisation technology.\nComplete familiarity with Agile development process\nExcellent documentation of processes",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['DevOps', 'Terraform', 'Docker', 'AWS', 'Kubernetes']",2025-06-12 05:35:51
ETL Developer,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\n\nResponsibilities:\nDesign and implement the data modeling, data ingestion and data processing for various datasets\nDesign, develop and maintain ETL Framework for various new data source\nDevelop data ingestion using AWS Glue/ EMR, data pipeline using PySpark, Python and Databricks.\nBuild orchestration workflow using Airflow & databricks Job workflow\nDevelop and execute adhoc data ingestion to support business analytics.\nProactively interact with vendors for any questions and report the status accordingly\nExplore and evaluate the tools/service to support business requirement\nAbility to learn to create a data-driven culture and impactful data strategies.\nAptitude towards learning new technologies and solving complex problem.\nQualifications:\nMinimum of bachelors degree. Preferably in Computer Science, Information system, Information technology.\nMinimum 5 years of experience on cloud platforms such as AWS, Azure, GCP.\nMinimum 5 year of experience in Amazon Web Services like VPC, S3, EC2, Redshift, RDS, EMR, Athena, IAM, Glue, DMS, Data pipeline & API, Lambda, etc.\nMinimum of 5 years of experience in ETL and data engineering using Python, AWS Glue, AWS EMR /PySpark and Airflow for orchestration.\nMinimum 2 years of experience in Databricks including unity catalog, data engineering Job workflow orchestration and dashboard generation based on business requirements\nMinimum 5 years of experience in SQL, Python, and source control such as Bitbucket, CICD for code deployment.\nExperience in PostgreSQL, SQL Server, MySQL & Oracle databases.\nExperience in MPP such as AWS Redshift, AWS EMR, Databricks SQL warehouse & compute cluster.\nExperience in distributed programming with Python, Unix Scripting, MPP, RDBMS databases for data integration\nExperience building distributed high-performance systems using Spark/PySpark, AWS Glue and developing applications for loading/streaming data into Databricks SQL warehouse & Redshift.\nExperience in Agile methodology\nProven skills to write technical specifications for data extraction and good quality code.\nExperience with big data processing techniques using Sqoop, Spark, hive is additional plus\nExperience in data visualization tools including PowerBI, Tableau.\nNice to have experience in UI using Python Flask framework anglular\n\n\nMandatory Skills: Python for Insights. Experience: 5-8 Years.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ETL', 'data bricks', 'aws glue', 'amazon ec2', 'python', 'spark', 'glue', 'amazon redshift', 'cloud platforms', 'aws', 'data engineering', 'sql']",2025-06-12 05:35:54
Openings For QA Engineer,Randstad Digital,3 - 6 years,6-10 Lacs P.A.,['Chennai'],"Job Type - Third Party Payroll\nPayroll Company Name - Randstad Digital\nPosition - QA Engineer\nExperience - 3+Years\nLocation - Chennai only\nNotice Period - Immediate to 15days\nInterview Type - Virtual (Final Round Face2Face Interview)\nWork Mode - 5 days office\nKey Skills - Automation Testing, Manual Testing, AWS, WebDriverIO, Cucumber/Gherkin\n\n\nIf you're interested, kindly share your updated resume and the following details to this below email ID or LinkedIn\n\n\nEmail Id: gayathri.nambi@randstaddigital.com\nLinkedIn: https://www.linkedin.com/in/gayathri-alagia-nambi-0827921a5/\n\n\nFull Name:\nPan Number:\nExperience:\nRelevant Experience:\nNotice period:\nCTC:\nExpected CTC:\nCurrent company:\nPayroll Company:\nLocation:\nPreferred location:\nOffer in hand : (Y/N)\nReason for Job Change:\n\nKey Responsibilities\nCreate, execute, and review test cases in TestRail while actively participating in the testing process.\nAdhere to the QA standards, metrics, and quality gates across all product develop-ment stages.\nDrive improvements to our WebDriverIO/Cucumber test automation framework, aim-ing for consistent scalability.\nWork with AWS-based environments to integrate QA processes into CI/CD pipelines.\nOperate within 2-week sprint cycles, partnering closely with Product, Engineering, and DevOps teams in an Agile environment.\nWork directly with product owners to understand acceptance criteria for upcoming features, particularly on data-heavy and API-driven functionality.\nParticipate in sprint planning, refinement, and retrospectives, acting as an advocate for product quality.\nCollaborate with Engineering teams to ensure big data ingestion and processing pipelines meet testing standards.\nPerform tests at various stages of the production process.\nDeveloping and executing test plans and test cases.\nConducting root cause analysis for defects and recommending corrective actions.\nIdentifying, documenting, and reporting bugs, errors, and inconsistencies.\nStaying updated with new testing tools and strategies.\n\nWhat we are looking for:\nEssential\nAt least 3 years of QA experience both automation and manual.\nProven expertise in JavaScript/TypeScript test automation using WebDriverIO and Cucumber/Gherkin\nFamiliarity with AWS services and integrating QA processes into AWS-based CI/CD pipelines.\nDemonstrable experience testing APIs (RESTful or microservices-based), including end-to-end workflows.\nSelfdiscipline and willingness to learn\nAbility to make connections with technical and non-technical stakeholders\nStrong problem-solving abilities in fast-paced Agile environments.\nProficiency in written and spoken English\nDesirable\nExperience in big data projects (e.g., data-intensive applications, data lakes, or ana-lytics pipelines).\nExperience in maritime, logistics, or risk intelligence industries.\nKnowledge of performance testing tools (e.g., JMeter) and security testing methodol-ogies.\nExperience with API testing frameworks and microservices architectures.\nBackground in implementing QA processes across multi-team agile engineering or-ganizations.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation Testing', 'Manual Testing', 'Cucumber', 'AWS', 'Gherkin', 'Typescript', 'Javascript']",2025-06-12 05:35:56
Software Engineer,Pine Labs,2 - 6 years,Not Disclosed,['Noida'],"Front End Development.\n\nThose who share our core belief of 'Every Day is Game Day'\nWe bring our best selves to work each day to realise our mission of enriching the world through the power of digital commerce and financial services.\n\nROLE PURPOSE\n\nWe are looking for a talented and experienced JavaScript Developer to join our team to build, enhance, and support enterprise-grade applications and automation workflows. The ideal candidate will have strong JavaScript development skills and a passion for building robust, scalable systems. Experience with ServiceNow is a plus, but not mandatory\n\nTHE RESPONSIBILTIES WE ENTRUST YOU WITH.\n\nProduct knowledge, Problem solving, critical thinking\nTechnical understanding, coding skills\nAnalytical ability, sound algorithms and Data structures knowledge\nWHAT MATTERS IN THIS ROLE\nExperience & Portfolio\n2-6 years of experience in JavaScript (ES6+).\nBachelor/master's degree from good institutes\nSolid understanding of web technologies (HTML, CSS, JSON, AJAX)\nExperience working with RESTful APIs and asynchronous programming.\nFamiliarity with Git, CI/CD workflows and agile development practices.\nStrong problem-solving and communication skills\n\nTechnical Skills & Efficiency\nDevelop and maintain web-based applications and automation scripts using JavaScript.\nCollaborate with cross-functional teams to understand business requirements and deliver effective technical solutions.\nWrite clean, maintainable code and follow best practices for development and testing.\nWork with APIs (REST/SOAP) for data integration between systems.\nBuild custom UI components and tools for internal automation.\nDebug and troubleshoot application issues efficiently.\nContribute to architecture discussions and help improve coding standards.\n\nTHINGS YOU SHOULD BE COMFORTABLE WITH\nWorking from office\nAs of now, we work out of our offices, 5 days a week.\nPushing the boundaries\nHave a big idea? See something that you feel we should do but havent done? We will hustle hard to make it happen. We encourage out of the box thinking, and if you bring that with you, we will make sure you get a bag that fits all the energy you bring along.\nWHAT WE VALUE IN OUR PEOPLE\nYou take the shot\nYou decide fast and deliver right.\nYou are the CEO of what you do\nYou show ownership and make things happen.\nYou sign your work like an artist\nYou seek to learn and take pride in the work you do.",Industry Type: FinTech / Payments,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Servicenow', 'Javascript', 'Java Script']",2025-06-12 05:35:59
AI Engineer,HCLTech,10 - 14 years,Not Disclosed,['Noida'],"Seniority: Senior\nDescription & Requirements\nPosition Summary\nThe Senior AI Engineer with GenAI expertise is responsible for developing advanced technical solutions, integrating cutting-edge generative AI technologies. This role requires a deep understanding of modern technical and cloud-native practices, AI, DevOps, and machine learning technologies, particularly in generative models. You will support a wide range of customers through the Ideation to MVP journey, showcasing leadership and decision-making abilities while tackling complex challenges.",,,,"['AI engineering', 'VMware', 'Java', 'Azure', 'Data engineering', 'AI models', 'Node.js', 'NLP', 'Azure AKS', 'Machine Learning Operations', 'AWS', 'Kubernetes', 'Python']",2025-06-12 05:36:02
"Staff Engineer, Nodejs",Nagarro,7 - 10 years,Not Disclosed,['India'],"We're Nagarro.\n\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal Experience 7+ years.\nExcellent knowledge developing scalable and highly available Restful APIs using NodeJS technologies.\nThorough understanding of React.js and its core principles and experience with popular React.js workflows (such as Flux or Redux or Context API or Data Structures).\nFamiliarity with common programming tools such as RESTful APIs, TypeScript, version control software, and remote deployment tools, CI/CD tools.\nUnderstanding of linter libraries (TSLINT, Prettier etc) and Unit testing using Jest, Enzyme, Jasmine or equivalent framework.\nStrong proficiency in JavaScript, including DOM manipulation and the JavaScript object model. Proficient with the latest versions of ECMAScript (JavaScript or TypeScript).\nUnderstanding of containerization, experienced in Dockers, Kubernetes.\nExposed to API gateway integrations like 3Scale.\nUnderstanding of Single-Sign-on or token-based authentication (Rest, JWT, OAuth).\nPossess expert knowledge of task/message queues include but not limited to: AWS, Microsoft Azure, Pushpin. and Kafka.\nPractical experience with GraphQL is good to have.\nWriting tested, idiomatic, and documented JavaScript, HTML and CSS.\nExperiencing in Developing responsive web-based UI.\nHave experience on Styled Components, Tailwind CSS, Material UI and other CSS-in-JS techniques.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them into technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'Typescript', 'Node.Js', 'Docker', 'Microservices', 'Kubernetes']",2025-06-12 05:36:04
SQL Server 2012/SQL Server Engineer,INTERNAL,5 - 10 years,Not Disclosed,['Bengaluru'],"Skill Name :SQL Server 2012/SQL Server Engineer (5 - 10 years)\nWork Location : Bangalore (preferred) / Any Deloitte USI location\n\nKey responsibilities:\nUnderstand customer use case, available data to prepare for automated & ongoing data\ningestion to meet customer requirements\n• Design technical solution with all data ingestion, transformation & scheduling to\nreview & sign-off with customer\n• Play role of the tech lead and responsible for end-to-end technical solutions\n• Identifying AEP enhancements, extending frameworks and incorporating new ideas\n• Closely collaborating with other AEP team members (sales teams, engineers,\nconsultants) and onshore teams for delivering projects\n• Enterprise level software development leveraging Big Data, Cloud technologies &\nPython\n• Building/operating highly scalable, fault tolerant, distributed systems for extraction,\ningestion to process large data sets\n• Experience with data analysis, modeling and mapping to coordinate closely with\nData Architect(s)\n• Build the necessary schemas, workflows to ingest customers data into AEP\nsuccessfully\n• Create necessary identity namespaces, privacy settings and merge poilicys required\nto build the solution\n• Build Audiences (segmentations) and create necessary pipeline for Destination\nactivation\n• Deploying the final solution to a production environment (or end-state\nenvironment)\n• Post-Deployment, provide ongoing maintenance & support of solution & knowledge\ntransfer to offshore support team",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Part Time, Temporary/Contractual","['AEP Architect', 'Data Architect', 'SQL Server', 'Data base', 'SQL']",2025-06-12 05:36:06
Snowflake Data Architect,Kasmo Digital,10 - 16 years,Not Disclosed,['Hyderabad'],"Required Skills & Qualifications:\n10-12 years of experience in data architecture, data warehousing, and cloud technologies.\nStrong expertise in Snowflake architecture, data modeling, and optimization.\nSolid hands-on experience with cloud platforms: AWS, Azure, and GCP.\nIn-depth knowledge of SQL, Python, PySpark, and related data engineering tools.\nExpertise in data modeling (both dimensional and normalized models).\nStrong experience with data integration, ETL processes, and pipeline development.\nCertification in Snowflake, AWS, Azure, or related cloud technologies.\nExperience working with large-scale data processing frameworks and platforms.\nExperience in data visualization tools and BI platforms (e.g., Tableau, Power BI).\nExperience in Agile methodologies and project management.\nStrong problem-solving skills with the ability to address complex technical challenges.\nExcellent communication skills and ability to work collaboratively with cross-functional teams.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Data Visualization', 'Data Modeling', 'Data Warehousing', 'SQL', 'Data Architecture', 'Python']",2025-06-12 05:36:09
ETL and BDX developer,"NTT DATA, Inc.",10 - 15 years,Not Disclosed,['Bengaluru'],"Req ID: 321918\n\nWe are currently seeking a ETL and BDX developer to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\n""¢ Develop and maintain Power BI dashboards, reports, and datasets.\n\n""¢ Collaborate with stakeholders to gather and analyse business requirements.\n\n""¢ Design robust and scalable data models using Power BI and underlying data sources.\n\n""¢ Write complex DAX expressions for calculated columns, measures, and KPIs.\n\n""¢ Optimize performance of Power BI reports and data models.\n\n""¢ Integrate Power BI with other data sources (SQL Server, Excel, Azure, SharePoint, etc.).\n\n""¢ Implement row-level security and data access control.\n\n""¢ Automate data refresh schedules and troubleshoot refresh failures.\n\n""¢ Mentor junior developers and conduct code reviews.\n\n""¢ Work closely with data engineering teams to ensure data accuracy and integrity.\n\n""¢ Exp working on Power Query and data flows.\n\n""¢ Strong in writing SQL queries\n\n\n\nTotal Exp7 ""“ 10 Yrs.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql queries', 'power bi', 'microsoft azure', 'sql server', 'power query', 'power bi reports', 'ssas', 'bi', 'sharepoint', 'data engineering', 'dashboards', 'sql', 'data modeling', 'power bi dashboards', 'ssrs', 'dax', 'code review', 'etl', 'ssis', 'etl development', 'data flow', 'msbi']",2025-06-12 05:36:12
Data Architect / Engagement Lead,Ignitho,7 - 10 years,Not Disclosed,['Chennai( Sholinganallur )'],"Job Title: Data Architect / Engagement Lead\nLocation: Chennai\nReports To: CEO\n\nAbout the Company:\nIgnitho Inc. is a leading AI and data engineering company with a global presence, including US, UK, India, and Costa Rica offices.\nVisit our website to learn more about our work and culture: www.ignitho.com.\nIgnitho is a portfolio company of Nuivio Ventures Inc., a venture builder dedicated to developing Enterprise AI product companies across various domains, including AI, Data Engineering, and IoT.\nLearn more about Nuivio at: www.nuivio.com.\n\nJob Summary:\nAs the Data Architect and Engagement Lead, you will define the data architecture strategy and lead client engagements, ensuring alignment between data solutions and business goals. This dual role blends technical leadership with client-facing responsibilities.\n\nKey Responsibilities:\nDesign scalable data architectures, including storage, processing, and integration layers.\nLead technical discovery and requirements gathering sessions with clients.\nProvide architectural oversight for data and AI solutions.\nAct as a liaison between technical teams and business stakeholders.\nDefine data governance, security, and compliance standards.\n\nRequired Qualifications:\nBachelors or Masters in computer science, Information Systems, or similar.\n7+ years of experience in data architecture, with client-facing experience.\nDeep knowledge of data modelling, cloud data platforms (Snowflake / BigQuery/ Redshift / Azure), and orchestration tools.\nExcellent communication, stakeholder management, and technical leadership skills.\nFamiliarity with AI/ML systems and their data requirements is a strong plus.",Industry Type: Emerging Technologies (AI/ML),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Aiml', 'Data Modeling', 'Azure Cloud', 'Bigquery', 'Redshift Aws', 'Artificial Intelligence', 'Snowflake', 'Machine Learning']",2025-06-12 05:36:14
Azure Data Architect,Syren Technologies,10 - 18 years,Not Disclosed,[],"About Syren Cloud\n\nSyren Cloud Technologies is a cutting-edge company specializing in supply chain solutions and data engineering. Their intelligent insights, powered by technologies like AI and NLP, empower organizations with real-time visibility and proactive decision-making. From control towers to agile inventory management, Syren unlocks unparalleled success in supply chain management.\n\nRole Summary",,,,"['Pyspark', 'Azure', 'Architecture', 'Data Bricks']",2025-06-12 05:36:16
LLMOps Engineer,HCLTech,8 - 10 years,Not Disclosed,['Noida'],"Position Summary\nLLMOps(Large language model operations) Engineer will play a pivotal role in building and maintaining the infrastructure and pipelines for our cutting-edge Generative AI applications, establishing efficient and scalable systems for LLM research, evaluation, training, and fine-tuning. Engineer will be responsible for managing and optimizing large language models (LLMs) across various platforms This position is uniquely tailored for those who excel in crafting pipelines, cloud infrastructure, environments, and workflows. Your expertise in automating and streamlining the ML lifecycle will be instrumental in ensuring the efficiency, scalability, and reliability of our Generative AI models and associated platform. LLMOps engineers expertise will ensure the smooth deployment, maintenance, and performance of these AI platforms and powerful large language models.\n\nYou will follow Site Reliability Engineering & MLOps principles and will be encouraged to contribute your own best practices and ideas to our ways of working.\nReporting to the Head of Cloud Native operations, you will be an experienced thought leader, and comfortable engaging senior managers and technologists. You will engage with clients, display technical leadership, and guide the creation of efficient and complex products/solutions.\nKey Responsibilities\nTechnical & Architectural Leadership\n\nContribute to the technical delivery of projects, ensuring a high quality of work that adheres to best practices, brings innovative approaches and meets client expectations. Project types include following (but not limited to):\nSolution architecture, Proof of concepts (PoCs), MVP, design, develop, and implementation of ML/LLM pipelines for generative AI models, data management & preparation for fine tuning, training, deployment, and monitoring.\nAutomate ML tasks across the model lifecycle.\nContribute to HCL thought leadership across the Cloud Native domain with an expert understanding of advanced AI solutions using Large Language Models (LLM) & Natural Language Processing (NLP) techniques and partner technologies.\nCollaborate with cross-functional teams to integrate LLM and NLP technologies into existing systems.\nEnsure the highest levels of governance and compliance are maintained in all ML and LLM operations.\nStay abreast of the latest developments in ML and LLM technologies and methodologies, integrating these innovations to enhance operational efficiency and model effectiveness.\nCollaborate with global peers from partner ecosystems on joint technical projects. This partner ecosystem includes Google, Microsoft, Nvidia, AWS, IBM, Red Hat, Intel, Cisco, and Dell VMware etc.\nService Delivery\n\nProvide a technical hands-on contribution. Create scalable infra to support enterprise loads (distributed GPU compute, foundation models, orchestrating across multiple cloud vendors, etc.)\nEnsuring the reliable and efficient platform operations.\nApply data science, machine learning, deep learning, and natural language processing methods to analyse, process, and improve the models data and performance.\nUnderstanding of Explainability & Biased Detection concepts.\nCreate and optimize prompts and queries for retrieval augmented generation and prompt engineering techniques to enhance the models capabilities and user experience w.r.t Operations & associated platforms.\nClient-facing influence and guidance, engaging in consultative client discussions and performing a Trusted Advisor role.\nProvide effective support to HCL Sales and Delivery teams.\nSupport sales pursuits and enable HCL revenue growth.\nDefine the modernization strategy for client platform and associated IT practices, create solution architecture and provide oversight of the client journey.\nInnovation & Initiative\n\nAlways maintain hands-on technical credibility, keep in front of the industry, and be prepared to show and lead the way forward to others.\nEngage in technical innovation and support HCLs position as an industry leader.\nActively contribute to HCL sponsorship of leading industry bodies such as the CNCF and Linux Foundation.\nContribute to thought leadership by writing Whitepapers, blogs, and speaking at industry events.\nBe a trusted, knowledgeable internal innovator driving success across our global workforce.\nClient Relationships\n\nAdvise on best practices related to platform & Operations engineering and cloud native operations, run client briefings and workshops, and engage technical leaders in a strategic dialogue.\nDevelop and maintain strong relationships with client stakeholders.\nPerform a Trusted Advisor role.\nContribute to technical projects with a strong focus on technical excellence and on-time delivery.\nMandatory Skills & Experience\nExpertise in designing and optimizing machine-learning operations, with a preference for LLMOps.\nProficient in Data Science, Machine Learning, Python, SQL, Linux/Unix shell scripting.\nExperience on Large Language Models and Natural Language Processing (NLP), and experience with researching, training, and fine-tuning LLMs. Contribute towards fine-tune Transformer models for optimal performance in NLP tasks, if required.\nImplement and maintain automated testing and deployment processes for machine learning models w.r.t LLMOps.\nImplement version control, CI/CD pipelines, and containerization techniques to streamline ML and LLM workflows.\nDevelop and maintain robust monitoring and alerting systems for generative AI models ensuring proactive identification and resolution of issues.\nResearch or engineering experience in deep learning with one or more of the following: generative models, segmentation, object detection, classification, model optimisations.\nExperience implementing RAG frameworks as part of available-ready products.\nExperience in setting up the infrastructure for the latest technology such as Kubernetes, Serverless, Containers, Microservices etc.\nExperience in scripting programming to automate deployments and testing, worked on tools like Terraform and Ansible. Scripting languages like Python, bash, YAML etc.\nExperience on CI/CD opensource and enterprise tool sets such as Argo CD, Jenkins.\nExperience with the GitHub/DevOps Lifecycle\nExperience in at least one of the Observability solutions (Prometheus, EFK stacks, ELK stacks, Grafana, Dynatrace, AppDynamics)\nExperience in at-least one of the clouds for example - Azure/AWS/GCP\nSignificant experience on microservices-based, container-based or similar modern approaches of applications and workloads.\nYou have exemplary verbal and written communication skills (English). Able to interact and influence at the highest level, you will be a confident presenter and speaker, able to command the respect of your audience.\nDesired Skills & Experience\nBachelor level technical degree or equivalent experience; Computer Science, Data Science, or Engineering background preferred; masters degree desired.\nExperience in LLMOps or related areas, such as DevOps, data engineering, or ML infrastructure.\nHands-on experience in deploying and managing machine learning and large language model pipelines in cloud platforms (e.g., AWS, Azure) for ML workloads.\nFamiliar with data science, machine learning, deep learning, and natural language processing concepts, tools, and libraries such as Python, TensorFlow, PyTorch, NLTK etc.\nExperience in using retrieval augmented generation and prompt engineering techniques to improve the models quality and diversity to improve operations efficiency. Proven experience in developing and fine-tuning Language Models (LLMs).\nStay up-to-date with the latest advancements in Generative AI, conduct research, and explore innovative techniques to improve model quality and efficiency.\nThe perfect candidate will already be working within a System Integrator, Consulting or Enterprise organisation with 8+ years of experience in a technical role within the Cloud domain.\nDeep understanding of core practices including SRE, Agile, Scrum, XP and Domain Driven Design. Familiarity with the CNCF open-source community.\nEnjoy working in a fast-paced environment using the latest technologies, love Labs dynamic and high-energy atmosphere, and want to build your career with an industry leader.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['LLMOps', 'Architectural Leadership', 'Machine Learning', 'Unix shell scripting', 'SQL', 'Data Science', 'NLP', 'Linux', 'Terraform', 'Ansible', 'CI/CD', 'technical delivery', 'AWS', 'Python']",2025-06-12 05:36:19
Principal - Data Architect,Affine Analytics,8 - 12 years,Not Disclosed,['Chennai'],"We are seeking a highly skilled Data Architect to design and implement robust, scalable, and secure data solutions on AWS Cloud. The ideal candidate should have expertise in AWS services, data modeling, ETL processes, and big data technologies, with hands-on experience in Glue, DMS, Python, PySpark, and MPP databases like Snowflake, Redshift, or Databricks.\nKey Responsibilities:\nArchitect and implement data solutions leveraging AWS services such as EC2, S3, IAM, Glue (Mandatory), and DMS for efficient data processing and storage.",,,,"['Python', 'S3', 'AWS Glue', 'DMS', 'SQL Server', 'Redshift', 'Glue', 'IAM', 'EC2', 'Snowflake', 'Databricks', 'Oracle', 'Lambda']",2025-06-12 05:36:21
Job opening For Data Warehouse + ADF + ETL,bct,3 - 6 years,Not Disclosed,['Pune'],"Greetings of the Day !!!\n\nWe have job opening for Data Warehouse + ADF + ETL with one of our Client .If you are interested for this role , kindly share update resume along with below details in this email id : shaswati.m@bct-consulting.com\n\nJob Description:\nSenior Data Engineer\nAs a Senior Data Engineer, you will support the European World Area using the Windows & Azure suite of Analytics & Data platforms. The focus of the role is on the technical aspects and implementation of data gathering, integration and database design.\nWe look forward to seeing your application!\nIn This Role, Your Responsibilities Will Be:\nData Ingestion and Integration: Collaborate with Product Owners and analysts to understand data requirements & design, develop, and maintain data pipelines for ingesting, transforming, and integrating data from various sources into Azure Data Services.\nMigration of existing ETL packages: Migrate existing SSIS packages to Synapse pipelines\nData Modelling: Assist in designing and implementing data models, data warehouses, and databases in Azure Synapse Analytics, Azure Data Lake Storage, and other Azure services.\nData Transformation: Develop ETL (Extract, Transform, Load) processes using SQL Server Integration Services (SSIS), Azure Synapse Pipelines, or other relevant tools to prepare data for analysis and reporting.\nData Quality and Governance: Implement data quality checks and data governance practices to ensure the accuracy, consistency, and security of data assets.\nMonitoring and Optimization: Monitor and optimize data pipelines and workflows for performance, scalability, and cost efficiency.\nDocumentation: Maintain comprehensive documentation of processes, including data lineage, data dictionaries, and pipeline schedules.\nCollaboration: Work closely with cross-functional teams, including data analysts, data scientists, and business stakeholders, to understand their data needs and deliver solutions accordingly.\nAzure Services: Stay updated on Azure data services and best practices to recommend and implement improvements in our data architecture and processes\nFor This Role, You Will Need:\n3-5 years of experience in Data Warehousing with On-Premises or Cloud technologies\nStrong practical experience of Synapse pipelines / ADF.\nStrong practical experience of developing ETL packages using SSIS.\nStrong practical experience with T-SQL or any variant from other RDBMS.\nGraduate degree educated in computer science or a relevant subject.\nStrong analytical and problem-solving skills.\nStrong communication skills in dealing with internal customers from a range of functional areas.\nWillingness to work flexible working hours according to project requirements.\nTechnical documentation skills.\nFluent in English.\nPreferred Qualifications that Set You Apart:\nOracle PL/SQL.\nExperience in working on Azure Services like Azure Synapse Analytics, Azure Data Lake.\nWorking experience with Azure DevOps paired with knowledge of Agile and/or Scrum methods of delivery.\nLanguages: French, Italian, or Spanish would be an advantage.\nAgile certification.\nThanks,\nShaswati",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ADF', 'ETL', 'SSIS', 'Data ware house']",2025-06-12 05:36:23
Security Managed Services Engineer (L2),"NTT DATA, Inc.",5 - 10 years,Not Disclosed,['Mumbai'],"Your day at NTT DATA\nThe Security Managed Services Engineer (L2) is a developing engineering role, responsible for providing a managed service to clients to ensure that their Security Infrastructures and systems remain operational.\n\nThrough the proactive monitoring, identifying, investigating, and resolving of technical incidents and problems, this role is able to restore service to clients.\n\nThe primary objective of this role is to proactively review client requests or tickets and apply technical/process knowledge to resolve them without breaching service level agreement (SLA) and focuses on second-line support for incidents and requests with a medium level of complexity.\n\nThe Security Managed Services Engineer (L2) may also contribute to support on project work as and when required.\nWhat you'll be doing\nKey Responsibilities:\nMin 5 Years exp\nCollaborate with Company to address challenging issues in cyber, analytics, machine learning, optimization, and computer networking to research solutions.\nPropose new research projects to tackle complex cyber, analytics, machine learning, optimization, and networking problems.\nPossess expertise in comprehending advanced persistent threats, emerging threats, and malware within a corporate environment.\nUnderstand attacks, attack vectors, and kill chain methodology.\nDemonstrate proficiency in working with big data and executing complex queries across multiple platforms.\nExhibit a strong grasp of malware analysis, threat taxonomy, and threat indicators.\nCompetently engage with various security technologies.\n\nAcademic Qualifications and Certifications:\nBachelor's degree or equivalent qualification in IT/Computing (or demonstrated equivalent work experience).\nCTIA/CEH/CSA certification in must.\nWorkplace type:\nOn-site Working",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Security engineering', 'malware analysis', 'networking', 'Security monitoring', 'Security analysis']",2025-06-12 05:36:26
Calibration Engineer,Motherson Technology Services Limited,5 - 10 years,Not Disclosed,['Pune'],"Role & responsibilities\n• Define and update the Application project plan for the assigned Projects.\n• Manage with pro-active approach the project plan changes during the development phase.\n• Adopt calibration methodologies, procedures and tools shared by Marelli HQ.\n• Promote effective solutions together with team Application and Functions Design Teams\n• Guarantee the compliance of calibration process workflow with the standards defined by Team.\n• Promote the use of statistical analysis and big data management, in cooperation with team, to validate strategies performance and diagnosis robustness.\n• Customer technical reference for all the issues related to calibration.\n• Support on Customer site for calibration activities development, when requested.\n• Confirm with the Car Maker for the process of calibration via label review.\nCoordinate the activities on the test development vehicles assigned to each project.\n• Take part to calibration design review - risk analyses with Team or with the Customers.\n• Analysis and resolution of vehicle fleets and vehicle market concerns\n\nPreferred candidate profile\nSkill\nGasoline/CNG/Flex fuel working experience.\nMPFI/GDI engines.\nBase calibration on test bed and vehicle.\nTransient calibration.\nDrivability and test trips experience.\nStart ability calibration.\nIdle calibration.\nEmission calibration.\nOBD-1 and OBD-2B calibration.",Industry Type: IT Services & Consulting,"Department: Production, Manufacturing & Engineering","Employment Type: Full Time, Permanent","['gasoline', 'calibration', 'Obd', 'INCA', 'Emission', 'Engine Calibration']",2025-06-12 05:36:28
Software Engineer,Intelliswift software,3 - 8 years,Not Disclosed,['Bengaluru'],"Job Description:\nWe are looking for a skilled Palantir Foundry Developer with strong hands-on experience in data engineering using PySpark and SQL. The ideal candidate should be proficient in designing, building, and maintaining scalable data pipelines and integrating with Palantir Foundry environments.\nKey Skills:\nPalantir Foundry (Mandatory)\nPySpark,\nAdvanced SQL and Data Modelling\nData Pipeline Development and Optimization\nETL Processes, Data Transformation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Palantir Foundry', 'SQL']",2025-06-12 05:36:30
Master Data Management Data Architect,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will be responsible for designing, building, maintaining, analyzing, and interpreting data deliver actionable insights that drive business decisions. This role involves working with large datasets, developing reports, supporting and driving data governance initiatives, and visualizing data to ensure data is accessible, reliable, and efficiently managed. The ideal candidate has deep technical skills and provides administration support for Master Data Management (MDM) and Data Quality platform, including solution architecture, inbound/outbound data integration (ETL), Data Quality (DQ), and maintenance/tuning of match rules.\nRoles & Responsibilities:\nDesign, develop, and maintain data solutions for data generation, collection, and processing\nCollaborate and communicate with MDM Developers, Data Architects, Product teams, Business SMEs, and Data Scientists to design and develop end-to-end data pipelines to meet fast paced business needs across geographic regions\nIdentify and resolve complex data-related challenges\nAdhere to standard processes for coding, testing, and designing reusable code/component\nParticipate in sprint planning meetings and provide estimations on technical implementation\nAs a SME, work with the team on MDM related product installation, configuration, customization and optimization\nResponsible for the understanding, documentation, maintenance, and additional creation of master data related data-models (conceptual, logical, and physical) and database structures\nReview technical model specifications and participate in data quality testing\nCollaborate with Data Quality & Governance Analyst and Data Governance Organization to monitor and preserve the master data quality\nCreate and maintain system specific master data data-dictionaries for domains in scope\nArchitect MDM Solutions, including data modeling and data source integrations from proof-of-concept through development and delivery\nDevelop the architectural design for Master Data Management domain development, base object integration to other systems and general solutions as related to Master Data Management\nDevelop and deliver solutions individually or as part of a development team\nApproves code reviews and technical work\nMaintains compliance with change control, SDLC and development standards\nContribute to the design, development, and implementation of data pipelines, ETL/ELT processes, and data integration solutions\nCollaborate with multi-functional teams to understand data requirements and design solutions that meet business needs\nImplement data security and privacy measures to protect sensitive data\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions\n\nBasic Qualifications:\nMasters degree and 1 to 3 years of Computer Science, IT or related field experience OR\nBachelors degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience.\nPreferred Qualifications:\nExpertise in architecting and designing Master Data Management (MDM) solutions.\nPractical experience with AWS Cloud, Databricks, Apache Spark, workflow orchestration, and optimizing big data processing performance.\nFamiliarity with enterprise source systems and consumer systems for master and reference data, such as CRM, ERP, and Data Warehouse/Business Intelligence.\nAt least 2 to 3 years of experience as an MDM developer using Informatica MDM or Reltio MDM, along with strong proficiency in SQL.\n\n\nGood-to-Have Skills:\nExperience with ETL tools such as Apache Spark, and various Python packages related to data processing, machine learning model development.\nGood understanding of data modeling, data warehousing, and data integration concepts.\nExperience with development using Python, React JS, cloud data platforms.\nCertified Data Engineer / Data Analyst (preferred on Databricks or cloud environments).\n\n\nSoft Skills:\nExcellent critical-thinking and problem-solving skills\nGood communication and collaboration skills\nDemonstrated awareness of how to function in a team setting\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'Business Intelligence', 'Data Warehouse', 'cloud data platforms', 'Databricks', 'ETL', 'React JS', 'Python']",2025-06-12 05:36:33
Data Scientist,Ltimindtree,7 - 12 years,Not Disclosed,['Hyderabad'],Data Scientist\n\nJob Description\n\nResponsibilities\n\nWork with team members across multiple disciplines to understand the data behind product features user behaviors the security landscape and our goals\nAnalyze data from several large sources then automate solutions using scheduled processes models and alerts\nWork with partners to design and improve metrics that guide our decisions for the product\nDetect patterns associated with fraudulent accounts and anomalous behavior\nSolve scientific problems and create new methods independently\nTranslate requirements and security questions into data insights\nSet up alerting mechanisms so our leadership is always aware of the security posture\n\nQualifications\n\nPostgraduate degree with specialization in machine learning artificial intelligence statistics or related fields or 2 years of equivalent work experience in applied machine learning and analytics\nExperience with SQL Snowflake and NoSQL databases\nProficiency in Python programming\nFamiliarity with statistics modeling and data visualization\n\nExperience\n\nExperience building statistical and machine learning models applying techniques such as regression classification clustering and anomaly detection Time series and Classical ML modeling\nFamiliarity with Snowflake SQL\nFamiliarity with cloud platforms such as AWS\nSome experience to software development or data engineering\nAnalyze business problems or research questions identify relevant data points and extract meaningful insights,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Snowflake Sql', 'AWS']",2025-06-12 05:36:35
Data Scientist For DMAI,Prodapt Solutions,2 - 5 years,Not Disclosed,['Chennai'],"Overview\n\nThe Senior Data Science Engineer will leverage advanced data science techniques to solve complex business problems, guide decision-making processes, and mentor junior team members. This role requires a combination of technical expertise in data analysis, machine learning, and project management skills.\n\nResponsibilities\n\n Data Analysis and Modeling Analyze large-scale telecom datasets to extract actionable insights and build predictive models for network optimization and customer retention.\n Conduct statistical analyses  to validate models and ensure their effectiveness.\n Machine Learning Development Design and implement machine learning algorithms for fraud detection, churn prediction, and network failure analysis.\n Telecom-Specific Analytics Apply domain knowledge to improve customer experience by analyzing usage patterns, optimizing services, and predicting customer lifetime value.\n ETL Processes Develop robust pipelines for extracting, transforming, and loading telecom data from diverse sources.\n Collaboration Work closely with data scientists, software engineers, and telecom experts to deploy solutions that enhance operational efficiency.\n Data Governance :  Ensure data integrity, privacy, security and compliance with industry standards\n\n\nAdvanced degree in Data Science, Statistics, Computer Science, or a related field.\nExtensive experience in data science roles with a strong focus on machine learning and statistical modeling.\nProficiency in programming languages such as Python or R and strong SQL skills.\nFamiliarity with big data technologies (e.g., Hadoop, Spark) is advantageous.\nExpertise in cloud platforms such as AWS or Azure.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['project management', 'data analysis', 'machine learning', 'sql', 'statistical modeling', 'algorithms', 'python', 'big data technologies', 'microsoft azure', 'cloud platforms', 'r', 'data science', 'spark', 'data governance', 'hadoop', 'aws', 'etl', 'machine learning algorithms', 'statistics']",2025-06-12 05:36:37
Sr. Database Engineer For US shift (Eastern Time),TEOCO,5 - 10 years,Not Disclosed,"['Kolkata', 'Bengaluru']","Position: Sr. Database Engineer US shift (Eastern Time)\nLocation: Kolkata or Bangalore\nFull time permanent position\n\nUS shift (Eastern Time) : 4.30PM to 12.30AM IST (complete work from home)\n\nExperience required: 6-8+ years\n\n\nMajor skills required: SQL, C# or Python, any ETL tool\n\n\nProduct Development:\n\nWork with the business analysts to understand the high level business need and requirements;\nWork with operation team to understand issues and provide step-by-step solutions;\nImplement business logic using SQL;\nMonitor the system for any issues and quickly respond to emergencies;\nDeep detailed understanding of internal ETL tool;\nPrepare product for the release and drive the process;\nProficiency in query optimization (understanding query plans, improving execution time etc.);\nWrite scripts using internal code language (C# based) in order to optimize the process;\nUnderstand the overall process and data flows;\nPerform detailed analysis of the code and do research to help analysts understand current situation and make a decision.\n\nExperience and required skills:\n\nStrong understanding of relational databases;\nAdvanced SQL knowledge is required;\nWorking experience with MPP (MPP Massively Parallel Processing) Databases, understanding database design (data distribution, partitioning etc.);\nMedium Linux knowledge is required;\nC# or Python mid-level;\nExperience with analytic reporting tools such as SAP Business Object is preferred;\nAbility to work in a multi-cultured team environment;\nStrong oral and written communication skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL', 'C#', 'python', 'ETL']",2025-06-12 05:36:40
Data Scientist,Celebal Technologies,3 - 6 years,Not Disclosed,"['Mumbai', 'Navi Mumbai']","About Us: Celebal Technologies is a leading Solution Service company that provide Services the field of Data Science, Big Data, Enterprise Cloud & Automation. We are at the forefront of leveraging cuttingedge technologies to drive innovation and enhance our business processes. As part of our commitment to staying ahead in the industry, we are seeking a talented and experienced Data & AI Engineer with strong Azure cloud competencies to join our dynamic team.\n\nJob Summary: We are looking for a highly skilled Data Scientist with deep expertise in time series forecasting, particularly in demand forecasting and customer lifecycle analytics (CLV). The ideal candidate will be proficient in Python or PySpark, have hands-on experience with tools like Prophet and ARIMA, and be comfortable working in Databricks environments. Familiarity with classic ML models and optimization techniques is a plus.\n\nKey Responsibilities\n• Develop, deploy, and maintain time series forecasting models (Prophet, ARIMA, etc.) for demand forecasting and customer behavior modeling.\n• Design and implement Customer Lifetime Value (CLV) models to drive customer retention and engagement strategies.\n• Process and analyze large datasets using PySpark or Python (Pandas).\n• Partner with cross-functional teams to identify business needs and translate them into data science solutions.\n• Leverage classic ML techniques (classification, regression) and boosting algorithms (e.g., XGBoost, LightGBM) to support broader analytics use cases.\n• Use Databricks for collaborative development, data pipelines, and model orchestration.\n• Apply optimization techniques where relevant to improve forecast accuracy and business decision-making.\n• Present actionable insights and communicate model results effectively to technical and non-technical stakeholders.\n\nRequired Qualifications\n• Strong experience in Time Series Forecasting, with hands-on knowledge of Prophet, ARIMA, or equivalent Mandatory.\n• Proven track record in Demand Forecasting Highly Preferred.\n• Experience in modeling Customer Lifecycle Value (CLV) or similar customer analytics use cases Highly Preferred.\n• Proficiency in Python (Pandas) or PySpark Mandatory.\n• Experience with Databricks Mandatory.\n• Solid foundation in statistics, predictive modeling, and machine learning",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning Operations', 'Demand Forecasting', 'Data Bricks', 'Pyspark', 'Large Language Model', 'Time Series', 'Spark', 'Machine Learning', 'Python']",2025-06-12 05:36:42
Devops AWS DATA Engineeer|| Technical Analyst || 12Lakhs CTC,Robotics Technologies,5 - 9 years,11-12 Lacs P.A.,['Hyderabad( Banjara hills )'],"We are seeking a highly skilled Devops Engineer to join our dynamic development team. In this role, you will be responsible for designing, developing, and maintaining both frontend and backend components of our applications using Devops and associated technologies.\nYou will collaborate with cross-functional teams to deliver robust, scalable, and high-performing software solutions that meet our business needs. The ideal candidate will have a strong background in devops, experience with modern frontend frameworks, and a passion for full-stack development.\n\nRequirements:\nBachelor's degree in Computer Science Engineering, or a related field.\n5 to 9+ years of experience in full-stack development, with a strong focus on DevOps.\n\nDevOps with AWS Data Engineer - Roles & Responsibilities:\nUse AWS services like EC2, VPC, S3, IAM, RDS, and Route 53.\nAutomate infrastructure using Infrastructure as Code (IaC) tools like Terraform or AWS CloudFormation.\nBuild and maintain CI/CD pipelines using tools AWS CodePipeline, Jenkins,GitLab CI/CD.\nCross-Functional Collaboration\nAutomate build, test, and deployment processes for Java applications.\nUse Ansible, Chef, or AWS Systems Manager for managing configurations across environments.\nContainerize Java apps using Docker.\nDeploy and manage containers using Amazon ECS, EKS (Kubernetes), or Fargate.\nMonitoring & Logging using Amazon CloudWatch,Prometheus + Grafana,E\nStack (Elasticsearch, Logstash, Kibana),AWS X-Ray for distributed tracing manage access with IAM roles/policies.\nUse AWS Secrets Manager / Parameter Store for managing credentials.\nEnforce security best practices, encryption, and audits.\nAutomate backups for databases and services using AWS Backup, RDS Snapshots, and S3 lifecycle rules.\nImplement Disaster Recovery (DR) strategies.\nWork closely with development teams to integrate DevOps practices.\nDocument pipelines, architecture, and troubleshooting runbooks.\nMonitor and optimize AWS resource usage.\nUse AWS Cost Explorer, Budgets, and Savings Plans.\n\nMust-Have Skills:\nExperience working on Linux-based infrastructure.\nExcellent understanding of Ruby, Python, Perl, and Java.\nConfiguration and managing databases such as MySQL, Mongo.\nExcellent troubleshooting.\nSelecting and deploying appropriate CI/CD tools\nWorking knowledge of various tools, open-source technologies, and cloud services.\nAwareness of critical concepts in DevOps and Agile principles.\nManaging stakeholders and external interfaces.\nSetting up tools and required infrastructure.\nDefining and setting development, testing, release, update, and support processes for DevOps operation.\nHave the technical skills to review, verify, and validate the software code developed in the project.\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Iac', 'Devops', 'Jenkins', 'AWS', 'Kubernetes', 'RDS', 'Aws Cloudformation', 'Amazon Cloudwatch', 'Prometheus', 'Ci/Cd', 'Grafana', 'DR', 'Cloud Trail', 'Docker', 'IAM', 'Ansible / Chef', 'fargate', 'Gitlab', 'Monitoring', 'Python']",2025-06-12 05:36:44
Devops AWS DATA Engineeer|| Technical Analyst || 12Lakhs CTC,Robotics Technologies,7 - 9 years,11-12 Lacs P.A.,['Hyderabad( Banjara hills )'],"We are seeking a highly skilled Devops Engineer to join our dynamic development team. In this role, you will be responsible for designing, developing, and maintaining both frontend and backend components of our applications using Devops and associated technologies.\nYou will collaborate with cross-functional teams to deliver robust, scalable, and high-performing software solutions that meet our business needs. The ideal candidate will have a strong background in devops, experience with modern frontend frameworks, and a passion for full-stack development.\n\nRequirements:\nBachelor's degree in Computer Science Engineering, or a related field.\n7 to 9+ years of experience in full-stack development, with a strong focus on DevOps.\n\nDevOps with AWS Data Engineer - Roles & Responsibilities:\nUse AWS services like EC2, VPC, S3, IAM, RDS, and Route 53.\nAutomate infrastructure using Infrastructure as Code (IaC) tools like Terraform or AWS CloudFormation.\nBuild and maintain CI/CD pipelines using tools AWS CodePipeline, Jenkins,GitLab CI/CD.\nCross-Functional Collaboration\nAutomate build, test, and deployment processes for Java applications.\nUse Ansible, Chef, or AWS Systems Manager for managing configurations across environments.\nContainerize Java apps using Docker.\nDeploy and manage containers using Amazon ECS, EKS (Kubernetes), or Fargate.\nMonitoring & Logging using Amazon CloudWatch,Prometheus + Grafana,E\nStack (Elasticsearch, Logstash, Kibana),AWS X-Ray for distributed tracing manage access with IAM roles/policies.\nUse AWS Secrets Manager / Parameter Store for managing credentials.\nEnforce security best practices, encryption, and audits.\nAutomate backups for databases and services using AWS Backup, RDS Snapshots, and S3 lifecycle rules.\nImplement Disaster Recovery (DR) strategies.\nWork closely with development teams to integrate DevOps practices.\nDocument pipelines, architecture, and troubleshooting runbooks.\nMonitor and optimize AWS resource usage.\nUse AWS Cost Explorer, Budgets, and Savings Plans.\n\nMust-Have Skills:\nExperience working on Linux-based infrastructure.\nExcellent understanding of Ruby, Python, Perl, and Java.\nConfiguration and managing databases such as MySQL, Mongo.\nExcellent troubleshooting.\nSelecting and deploying appropriate CI/CD tools\nWorking knowledge of various tools, open-source technologies, and cloud services.\nAwareness of critical concepts in DevOps and Agile principles.\nManaging stakeholders and external interfaces.\nSetting up tools and required infrastructure.\nDefining and setting development, testing, release, update, and support processes for DevOps operation.\nHave the technical skills to review, verify, and validate the software code developed in the project.\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Iac', 'Devops', 'Jenkins', 'AWS', 'Kubernetes', 'RDS', 'Aws Cloudformation', 'Amazon Cloudwatch', 'Prometheus', 'Ci/Cd', 'Grafana', 'DR', 'Cloud Trail', 'Docker', 'IAM', 'Ansible / Chef', 'fargate', 'Gitlab', 'Monitoring', 'Python']",2025-06-12 05:36:48
Data Engineer_ Immediate Joiner Required,Healthcare technology services,6 - 7 years,15-25 Lacs P.A.,"['Pune', 'Chennai']","Role: Data Engineer\nExperience: 6-9Years\nRelevant Experience in Data Engineer: 6+ Years\nNotice Period: Immediate Joiners Only\nJob Location: Pune and Chennai\n\nKey Responsibilities:\n\nMandatory Skill: Spark,SQL and Python\n\nMust Have:\nRelevant experience of 6-9 years as a Data Engineer\nExperience in programming language like Python\nGood Understanding of ETL (Extract, Transform, Load) concepts\nGood analytical and problem-solving skills\nKnowledge of a ticketing tool like JIRA/SNOW\nGood communication skills to interact with Customers on issues & requirements.\n\nReach us:If you are interested in this position and meet the above qualifications, please reach out to me directly at swati@cielhr.com and share your updated resume highlighting your relevant experience.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Spark', 'Python', 'SQL']",2025-06-12 05:36:50
Data Analyst,Talent Hire It Solutions,5 - 10 years,Not Disclosed,"['Kochi', 'Thiruvananthapuram']","Collaborate with business stakeholders to understand data needs and translate them into analytical\nrequirements.\nAnalyze large datasets to uncover trends, patterns, and actionable insights.\nDesign and build dashboards and reports using Power BI.\nPerform ad-hoc analysis and develop data-driven narratives to support decision-making.\nEnsure data accuracy, consistency, and integrity through data validation and quality checks.\nBuild and maintain SQL queries, views, and data models for reporting purposes.\nCommunicate findings clearly through presentations, visualizations, and written summaries.\nPartner with data engineers and architects to improve data pipelines and architecture.\nContribute to the definition of KPIs, metrics, and data governance standards.\n\nJob Specification / Skills and Competencies\nBachelors or Master’s degree in Statistics, Mathematics, Computer Science,\nEconomics, or a related field.\n5+ years of experience in a data analyst or business intelligence role.\nAdvanced proficiency in SQL and experience working with relational databases (e.g.,\nSQL Server, Redshift, Snowflake).\n\nJob Description\n\n2\n\nHands-on experience in Power BI.\nProficiency in Python, Excel and data storytelling.\nUnderstanding of data modelling, ETL concepts, and basic data architecture.\nStrong analytical thinking and problem-solving skills.\nExcellent communication and stakeholder management skills\nTo adhere to the Information Security Management policies and procedures.\n\nSoft Skills Required\nMust be a good team player with good communication skills\nMust have good presentation skills\nMust be a pro-active problem solver and a leader by self\nManage & nurture a team of data engineersRole & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SQL', 'Power BI', 'Amazon Athena', 'Python']",2025-06-12 05:36:53
"Data Modeller, Healthcare",Digital Convergence Technologies,4 - 9 years,20-35 Lacs P.A.,['Pune'],"Data/ETL Architect/Data Modeler:\nDevelop conceptual, logical, and physical data models to ensure accurate, scalable, and optimized data structures aligned with business requirements.\nCollaborate with business and technical teams to define data flow, transformation rules, and ensure alignment with data governance and quality standards.\nDesign end-to-end ETL architecture and data integration solutions.\nTechnologies - SQL, ETL, Big Data, ER Studio Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: Miscellaneous,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Modelling', 'Healthcare Domain', 'ERwin']",2025-06-12 05:36:55
Data Architect,Opus Technologies,10 - 16 years,35-50 Lacs P.A.,['Pune'],"Role & responsibilities\nDefine and evolve data engineering & analytics offerings aligned with payments domain needs (e.g., transaction analytics, fraud detection, customer insights).\nLead reference architecture creation for data modernization, real-time analytics, and cloud-native data platforms (e.g., Azure Synapse, GCP BigQuery).\nBuild reusable components, PoCs, and accelerators for ingestion, transformation, data quality, and governance.\nSupport pre-sales engagements with solution design, estimation, and client workshops.\nGuide delivery teams on data platform implementation, optimization, and security.\nMentor and upskill talent pool through learning paths and certifications.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Kafka', 'Spark', 'Python', 'Azure Data Factory']",2025-06-12 05:36:57
Data Stage Developer,Anblicks Solutions,6 - 11 years,Not Disclosed,['Chennai'],"Job Summary:\nWe are looking for a seasoned ETL Engineer with hands-on experience in Talend or IBM DataStage, preferably both, to lead data integration efforts in the mortgage domain. The ideal candidate will play a key role in designing, developing, and managing scalable ETL solutions that support critical mortgage data processing and analytics workloads.\n\nKey Responsibilities:\nEnd-to-end ETL solution development using Talend or DataStage.Design and implement robust data pipelines for mortgage origination, servicing, and compliance data.Collaborate with business stakeholders and data analysts to gather requirements and deliver optimized solutions.Perform code reviews, mentor junior team members, and ensure adherence to data quality and performance standards.Manage job orchestration, scheduling, and error handling mechanisms.Document ETL workflows, data dictionaries, and system processes.Ensure data privacy and compliance requirements are embedded in all solutions.\n\nRequired Skills:\nStrong experience in ETL tools Talend (preferred) or IBM DataStage.Solid understanding of mortgage lifecycle and related data domains.Proficiency in SQL and experience with relational databases (e.g., Oracle, SQL Server, Snowflake).Familiarity with job scheduling tools, version control, and CI/CD pipelines.Excellent problem-solving, leadership, and communication skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Datastage', 'Ibm Datastage', 'Etl Datastage']",2025-06-12 05:36:59
MDM Data Science Manager,Amgen Inc,10 - 14 years,Not Disclosed,['Hyderabad'],"We are seeking an accomplished and visionary Data Scientist/ GenAI Lead to join Amgens Enterprise Data Management team.\nAs MDM Data Science/Manager, you will lead the design, development, and deployment of Generative AI and ML models to power data-driven decisions across business domains.\nThis role is ideal for an AI practitioner who thrives in a collaborative environment and brings a strategic mindset to applying advanced AI techniques to solve real-world problems.To succeed in this role, the candidate must have strong AI/ML, Data Science, GenAI experience along with MDM knowledge, hence the candidates having only MDM experience are not eligible for this role. Candidate must have AI/ML, data science and GenAI experience on technologies like (PySpark/PyTorch, TensorFlow, LLM, Autogen, Hugging FaceVectorDB,Embeddings, RAGsetc), along with knowledge of MDM (Master Data Management)\nRoles & Responsibilities:\nDrive development of enterprise-level GenAI applications using LLM frameworks such as Langchain, Autogen, and Hugging Face.\nArchitect intelligent pipelines using PySpark, TensorFlow, and PyTorch within Databricks and AWS environments.\nImplement embedding models andmanage VectorStores for retrieval-augmented generation (RAG) solutions.\nIntegrate and leverage MDM platforms like Informatica and Reltio to supply high-quality structured data to ML systems.\nUtilize SQL and Python for data engineering, data wrangling, and pipeline automation.\nBuild scalable APIs and services to serve GenAI models in production.\nLead cross-functional collaboration with data scientists, engineers, and product teams to scope, design, and deploy AI-powered systems.\nEnsure model governance, version control, and auditability aligned with regulatory and compliance expectations.\nBasic Qualifications and Experience:\nMasters degree with 8 - 10 years of experience in Data Science, Artificial Intelligence, Computer Science, or related fields OR\nBachelors degree with 10 - 14 years of experience in Data Science, Artificial Intelligence, Computer Science, or related fields OR\nDiploma with 14 - 16 years of hands-on experience in Data Science, AI/ML technologies, or related technical domains\nFunctional Skills:\nMust-Have Skills:\n10+ years of experience working in AI/ML or Data Science roles, including designing and implementing GenAI solutions.\nExtensive hands-on experience with LLM frameworks and tools such as Langchain, Autogen, Hugging Face, OpenAI APIs, and embedding models.\nStrong programming background with Python, PySpark, and experience in building scalable solutions using TensorFlow, PyTorch, and SK-Learn.\nProven track record of building and deploying AI/ML applications in cloud environments such as AWS.\nExpertise in developing APIs, automation pipelines, and serving GenAI models using frameworks like Django, FastAPI, and DataBricks.\nSolid experience integrating and managing MDM tools (Informatica/Reltio) and applying data governance best practices.\nGuide the team on development activities and lead the solution discussions\nMust have core technical capabilities in GenAI, Data Science space\nGood-to-Have Skills:\nPrior experience in Data Modeling, ETL development, and data profiling to support AI/ML workflows.\nWorking knowledge of Life Sciences or Pharma industry standards and regulatory considerations.\nProficiency in tools like JIRA and Confluence for Agile delivery and project collaboration.\nFamiliarity with MongoDB, VectorStores, and modern architecture principles for scalable GenAI applications.\nProfessional Certifications:\nAny ETL certification (e.g. Informatica)\nAny Data Analysis certification (SQL)\nAny cloud certification (AWS or AZURE)\nData Science and ML Certification\nSoft Skills:\nStrong analytical abilities to assess and improve master data processes and solutions.\nExcellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders.\nEffective problem-solving skills to address data-related issues and implement scalable solutions.\nAbility to work effectively with global, virtual teams",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Science', 'AZURE', 'AI/ML', 'PyTorch', 'Data Analysis', 'ETL', 'AWS', 'SQL', 'TensorFlow']",2025-06-12 05:37:02
Data Analytics Mgr,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\n\n\nIn this vital role you will report to the Organizational Planning Analytics & Insights Procurement & Sourcing Lead, you will support Amgens Tech & Workforce Strategy by applying business analytics and change leadership skills to drive insights that impact resource allocation and sourcing strategy.\n\nYour responsibilities include dashboard development, ad-hoc reporting, business partnering & engagement, and financial baselining. This role supports organizational change and enables the development of an integrated approach to global sourcing and financial planning.\n\nReporting to the Organizational Planning Analytics & Insights Procurement & Sourcing Lead, you will support Amgens Tech & Workforce Strategy by applying business analytics and change management skills to drive insights that impact resource allocation and sourcing strategy.\n\nYour responsibilities include dashboard development, ad-hoc reporting, business partnering & engagement, and financial baselining. This role supports change management and enables the development of an integrated approach to global sourcing and financial planning.\n\nRoles & Responsibilities:\nAddressing business challenges through process evaluation and insight generation.\nDevelop insights with a strong focus on Tableau and Power BI dashboard creation, as well as PowerPoint presentations.\nGuide data analysts and data engineers on standard methodologies for building data pipelines to support dashboards and other business objectives.\nConduct ad hoc analyses of FP&A and sourcing/procurement data.\nAddressing business challenges through process evaluation and insight generation.\nDevelop insights with a strong focus on Tableau and Power BI dashboard creation, as well as PowerPoint presentations.\nGuide data analysts and data engineers on standard methodologies for building data pipelines to support dashboards and other business objectives.\nConduct ad hoc analyses of FP&A and sourcing/procurement data.\nWhat we expect of you\n\nWe are all different, yet we all use our unique contributions to serve patients.\n\nBasic Qualifications:\nMasters degree and 4 to 6 years of applicable experience in business analysis (finance analysis, data analysis, sourcing analysis, or similar experience OR\nBachelors degree and 6 to 8 years of applicable experience in business analysis (finance analysis, data analysis, sourcing analysis, or similar experience OR\nDiploma and 10 to 12 years of applicable experience in business analysis (finance analysis, data analysis, sourcing analysis, or similar experience\nPreferred Qualifications:\nMasters degree in data science, business, statistics, data mining, applied mathematics, business analytics, engineering, computer science, or a related field\n4 years of relevant experience in data science, data analytics, consulting, and/or financial planning & analysis.\nA keen eye for design, with the ability to craft engaging PowerPoint decks and develop compelling Power BI and Tableau dashboards.\nProven expertise in statistical/mathematical modeling and working with structured/unstructured data.\nExperience with procurement, sourcing, and/or financial planning data.\nSkilled in automating data workflows using tools like Tableau, Python, R, Alteryx, and PowerApps.\nKnowledge of global finance systems, Procurement, and sourcing operations.\nExperience with data analysis, budgeting, forecasting, and strategic planning in the Bio-Pharmaceutical or biotech industry.\nGrowing in a start-up environment, building a data-driven transformation capability.\nUnderstanding of the Bio-Pharmaceutical and biotech industry trends and operations.\nProven ability to engage with cross-functional business leaders to align data strategies with corporate objectives, redefining complex data insights into actionable strategies.\nFlexible work models, including remote work arrangements, where possible\n\nAs we work to develop treatments that deal with others, we also work to care for your professional and personal growth and well-being. From our competitive benefits to our collaborative culture, well support your journey every step of the way.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'data analytics', 'data science', 'mathematical modeling', 'financial planning', 'financial planning and analysis', 'statistics']",2025-06-12 05:37:04
MDM Data Scientist,Amgen Inc,3 - 8 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nWe are seeking an accomplished and visionary Data Scientist/ GenAIdeveloper to join Amgens Enterprise Data Management team. As part of MDM team, you will be responsible for designing, developing, and deploying Generative AI and ML models to power data-driven decisions across business domains. This role is ideal for an AI practitioner who thrives in a collaborative environment and brings a strategic mindset to applying advanced AI techniques to solve real-world problems.To succeed in this role, the candidate must have strong AI/ML, Data Science, GenAI experience along with MDM knowledge, hence the candidates having only MDM experience are not eligible for this role. Candidate must have AI/ML, data science and GenAI experience on technologies like (PySpark/PyTorch, TensorFlow, LLM, Autogen, Hugging FaceVectorDB,Embeddings, RAGsetc), along with knowledge of MDM (Master Data Management)\nRoles & Responsibilities:\nDevelop enterprise-level GenAI applications using LLM frameworks such as Langchain, Autogen, and Hugging Face.\nDesign and develop intelligent pipelines using PySpark, TensorFlow, and PyTorch within Databricks and AWS environments.\nImplement embedding models andmanage VectorStores for retrieval-augmented generation (RAG) solutions.\nIntegrate and leverage MDM platforms like Informatica and Reltio to supply high-quality structured data to ML systems.\nUtilize SQL and Python for data engineering, data wrangling, and pipeline automation.\nBuild scalable APIs and services to serve GenAI models in production.\nLead cross-functional collaboration with data scientists, engineers, and product teams to scope, design, and deploy AI-powered systems.\nEnsure model governance, version control, and auditability aligned with regulatory and compliance expectations.\nBasic Qualifications and Experience:\nMasters degree with 4 - 6 years of experience in Business, Engineering, IT or related field OR\nBachelors degree with 6 - 9 years of experience in Business, Engineering, IT or related field OR\nDiploma with 10 - 12 years of experience in Business, Engineering, IT or related field\nFunctional Skills:\nMust-Have Skills:\n6+ years of experience working in AI/ML or Data Science roles, including designing and implementing GenAI solutions.\nExtensive hands-on experience with LLM frameworks and tools such as Langchain, Autogen, Hugging Face, OpenAI APIs, and embedding models.\nStrong programming background with Python, PySpark, and experience in building scalable solutions using TensorFlow, PyTorch, and SK-Learn.\nProven track record of building and deploying AI/ML applications in cloud environments such as AWS.\nExpertise in developing APIs, automation pipelines, and serving GenAI models using frameworks like Django, FastAPI, and DataBricks.\nSolid experience integrating and managing MDM tools (Informatica/Reltio) and applying data governance best practices.\nGuide the team on development activities and lead the solution discussions\nMust have core technical capabilities in GenAI, Data Science space\nGood-to-Have Skills:\nPrior experience in Data Modeling, ETL development, and data profiling to support AI/ML workflows.\nWorking knowledge of Life Sciences or Pharma industry standards and regulatory considerations.\nProficiency in tools like JIRA and Confluence for Agile delivery and project collaboration.\nFamiliarity with MongoDB, VectorStores, and modern architecture principles for scalable GenAI applications.\nProfessional Certifications:\nAny ETL certification (e.g. Informatica)\nAny Data Analysis certification (SQL)\nAny cloud certification (AWS or AZURE)\nData Science and ML Certification\nSoft Skills:\nStrong analytical abilities to assess and improve master data processes and solutions.\nExcellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders.\nEffective problem-solving skills to address data-related issues and implement scalable solutions.\nAbility to work effectively with global, virtual teams\nWe will ensure that individuals with disabilities are provided with reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['MDM', 'GenAI', 'Langchain', 'PySpark', 'VectorStores', 'Hugging Face', 'LLM', 'Data Science', 'DataBricks', 'SK-Learn', 'AI/ML', 'Autogen', 'PyTorch', 'Django', 'OpenAI APIs', 'FastAPI', 'MongoDB', 'Data Modeling', 'PySpark/PyTorch', 'TensorFlow', 'Python']",2025-06-12 05:37:07
Azure Data lead,Infyjob Technology Services,7 - 12 years,Not Disclosed,['Chennai'],"candidate must have 7 year relevant experience in this following key skills\nazure data bricks\nAzure data factory\npython\nSQL\nonly we looking immediate joiners, even only give first priority for tier one companies\n\nonly chennai loaction available\n\nRequired Candidate profile\nAzure Data Engineer, Azure Solutions Architect , azure data factory",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Databricks', 'SQL', 'Python']",2025-06-12 05:37:09
MDM Data Scientist,Amgen Inc,4 - 9 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nWe are seeking an accomplished and visionary Data Scientist/ GenAIdeveloper to join Amgens Enterprise Data Management team. As part of MDM team, you will be responsible for designing, developing, and deploying Generative AI and ML models to power data-driven decisions across business domains. This role is ideal for an AI practitioner who thrives in a collaborative environment and brings a strategic mindset to applying advanced AI techniques to solve real-world problems.\nTo succeed in this role, the candidate must have strong AI/ML, Data Science, GenAI experience along with MDM knowledge, hence the candidates having only MDM experience are not eligible for this role. Candidate must have AI/ML, data science and GenAI experience on technologies like (PySpark/PyTorch, TensorFlow, LLM, Autogen, Hugging FaceVectorDB,Embeddings, RAGsetc), along with knowledge of MDM (Master Data Management)\nRoles & Responsibilities:\nDevelop enterprise-level GenAI applications using LLM frameworks such as Langchain, Autogen, and Hugging Face.\nDesign and develop intelligent pipelines using PySpark, TensorFlow, and PyTorch within Databricks and AWS environments.\nImplement embedding models andmanage VectorStores for retrieval-augmented generation (RAG) solutions.\nIntegrate and leverage MDM platforms like Informatica and Reltio to supply high-quality structured data to ML systems.\nUtilize SQL and Python for data engineering, data wrangling, and pipeline automation.\nBuild scalable APIs and services to serve GenAI models in production.\nLead cross-functional collaboration with data scientists, engineers, and product teams to scope, design, and deploy AI-powered systems.\nEnsure model governance, version control, and auditability aligned with regulatory and compliance expectations.\nBasic Qualifications and Experience:\nMasters degree with 4 - 6 years of experience in Business, Engineering, IT or related field OR\nBachelors degree with 6 - 9 years of experience in Business, Engineering, IT or related field OR\nDiploma with 10 - 12 years of experience in Business, Engineering, IT or related field\nFunctional Skills:\nMust-Have Skills:\n6+ years of experience working in AI/ML or Data Science roles, including designing and implementing GenAI solutions.\nExtensive hands-on experience with LLM frameworks and tools such as Langchain, Autogen, Hugging Face, OpenAI APIs, and embedding models.\nStrong programming background with Python, PySpark, and experience in building scalable solutions using TensorFlow, PyTorch, and SK-Learn.\nProven track record of building and deploying AI/ML applications in cloud environments such as AWS.\nExpertise in developing APIs, automation pipelines, and serving GenAI models using frameworks like Django, FastAPI, and DataBricks.\nSolid experience integrating and managing MDM tools (Informatica/Reltio) and applying data governance best practices.\nGuide the team on development activities and lead the solution discussions\nMust have core technical capabilities in GenAI, Data Science space\nGood-to-Have Skills:\nPrior experience in Data Modeling, ETL development, and data profiling to support AI/ML workflows.\nWorking knowledge of Life Sciences or Pharma industry standards and regulatory considerations.\nProficiency in tools like JIRA and Confluence for Agile delivery and project collaboration.\nFamiliarity with MongoDB, VectorStores, and modern architecture principles for scalable GenAI applications.\nProfessional Certifications:\nAny ETL certification (e.g. Informatica)\nAny Data Analysis certification (SQL)\nAny cloud certification (AWS or AZURE)\nData Science and ML Certification\nSoft Skills:\nStrong analytical abilities to assess and improve master data processes and solutions.\nExcellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders.\nEffective problem-solving skills to address data-related issues and implement scalable solutions.\nAbility to work effectively with global, virtual teams.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'GenAI', 'Langchain', 'PySpark', 'Hugging Face', 'OpenAI API', 'Autogen', 'PyTorch', 'Django', 'MDM', 'FastAPI', 'Data Modeling', 'ETL', 'TensorFlow', 'Python']",2025-06-12 05:37:12
Snowflake Developer with Azure Data Factory,Net Connect,6 - 10 years,6-11 Lacs P.A.,['Hyderabad'],Greetings from NCG!\n\nWe have a opening for Snowflake Developer role in Hyderabad office!\n\nBelow JD for your reference\n\nJob Description:,,,,"['Azure Data Factory', 'Snowflake', 'SQL']",2025-06-12 05:37:14
Data Architect (Data Bricks),Diacto Technologies Pvt Ltd,5 - 9 years,Not Disclosed,['Pune( Baner )'],"Job Overview:\nDiacto is seeking an experienced and highly skilled Data Architect to lead the design and development of scalable and efficient data solutions. The ideal candidate will have strong expertise in Azure Databricks, Snowflake (with DBT, GitHub, Airflow), and Google BigQuery. This is a full-time, on-site role based out of our Baner, Pune office.\n\nQualifications:\nB.E./B.Tech in Computer Science, IT, or related discipline\nMCS/MCA or equivalent preferred\n\nKey Responsibilities:\nDesign, build, and optimize robust data architecture frameworks for large-scale enterprise solutions\nArchitect and manage cloud-based data platforms using Azure Databricks, Snowflake, and BigQuery\nDefine and implement best practices for data modeling, integration, governance, and security\nCollaborate with engineering and analytics teams to ensure data solutions meet business needs\nLead development using tools such as DBT, Airflow, and GitHub for orchestration and version control\nTroubleshoot data issues and ensure system performance, reliability, and scalability\nGuide and mentor junior data engineers and developers\n\nExperience and Skills Required:\n5 to12 years of experience in data architecture, engineering, or analytics roles\nHands-on expertise in Databricks, especially Azure Databricks\nProficient in Snowflake, with working knowledge of DBT, Airflow, and GitHub\nExperience with Google BigQuery and cloud-native data processing workflows\nStrong knowledge of modern data architecture, data lakes, warehousing, and ETL pipelines\nExcellent problem-solving, communication, and analytical skills\n\nNice to Have:\nCertifications in Azure, Snowflake, or GCP\nExperience with containerization (Docker/Kubernetes)\nExposure to real-time data streaming and event-driven architecture\n\nWhy Join Diacto Technologies?\nCollaborate with experienced data professionals and work on high-impact projects\nExposure to a variety of industries and enterprise data ecosystems\nCompetitive compensation, learning opportunities, and an innovation-driven culture\nWork from our collaborative office space in Baner, Pune\nHow to Apply:\nOption 1 (Preferred)\n\nCopy and paste the following link on your browser and submit your application for the automated interview process: -\n\nhttps://app.candidhr.ai/app/candidate/gAAAAABoRrTQoMsfqaoNwTxsE_qwWYcpcRyYJk7NzSUmO3LKb6rM-8FcU58CUPYQKc65n66feHor-TGdCEfyouj0NmKdgYcNbA==/\n\nOption 2\n\n1. Please visit our website's career section at https://www.diacto.com/careers/\n2. Scroll down to the ""Who are we looking for?"" section\n3. Find the listing for "" Data Architect (Data Bricks)"" and\n4. Proceed with the virtual interview by clicking on ""Apply Now.""",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Azure Databricks', 'Airflow', 'Etl Pipelines', 'Github', 'google BigQuery', 'DBT', 'Data Security', 'Data Modeling', 'Elt', 'Data Governance']",2025-06-12 05:37:16
"ETL Developer - ODI, Oracle Data Integrator - 5+ yrs - Chennai",MNC Client,6 - 11 years,15-25 Lacs P.A.,"['Chennai( Okkiyam Thuraipakkam, Pallavaram, Sholinganallur, Perungudi, Pallikaranai )']","Hi,\n\nWishes from GSN!!! Pleasure connecting with you!!!\n\nWe been into Corporate Search Services for Identifying & Bringing in Stellar Talented Professionals for our reputed IT / Non-IT clients in India. We have been successfully providing results to various potential needs of our clients for the last 20 years.\n\nAt present, GSN is hiring ETL - Oracle ODI developers for one of our leading MNC client. PFB the details for your better understanding:\n\n\n******* Looking for SHORT JOINERS *******\n\n\n\nWORK LOCATION: CHENNAI\nJob Role: ETL ODI Developer\nEXPERIENCE: 5+ yrs\nCTC Range: 15 LPA to 25 LPA\nWork Type: WFO\n\n\nMandatory Skills :\nHands on development experience in ETL using ODI 11G/12C is MUST\nProficient in Data migration technique and Data Integration\nOracle SQL and PL/SQL programming experience\nExperience in Data Warehouse and/or Data Marts\n\n\n******* Looking for SHORT JOINERS *******\n\n\nAppreciate your valuable references, if any.\n\nThanks & Regards\nSathya K\nGSN Consulting\nMob: 8939666794\nMail ID: sathya@gsnhr.net; Web: https://g.co/kgs/UAsF9W",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data migration', 'Oracle Data Integrator', 'ODI', 'Data Warehouse', 'ETL', 'Data Marts', 'Data Integration']",2025-06-12 05:37:19
Data Model Architect,Prodapt Solutions,6 - 10 years,Not Disclosed,['Chennai'],"Overview\n\nProdapt is looking for a Data Model Architect. The candidate should be good with design and data architecture in Telecom domain.    \n\n\nResponsibilities\n\n Deliverables  \nDesign & Document – Data Model for CMDB, P-S-R Catalogue (Product, Service and Resource management layer)\nDesign & Document Build Interface Speciation for Data Integration.\n\n Activities  \n\n Data Architecture and Modeling  \nDesign and maintain conceptual, logical, and physical data models\nEnsure scalability and adaptability of data models for future organizational needs.\n\n Data Model   P-S-R catalogs   in the existing Catalogs,SOM,COM systems\n\n CMDB Design and Management  \nArchitect and optimize the CMDB to accurately reflect infrastructure components, telecom assets, and their relationships.\nDefine data governance standards and enforce data consistency across the CMDB.\n\n Design    data integrations   between across systems (e.g., OSS/BSS, network monitoring tools, billing systems).\n\n\nGood Communication skills.\n\nBachelors Degree.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data warehousing', 'data architecture', 'sql', 'data governance', 'data integration', 'python', 'oracle', 'performance tuning', 'power bi', 'microsoft azure', 'erwin', 'business intelligence', 'sql server', 'plsql', 'data quality', 'tableau', 'data modeling', 'dwbi', 'etl', 'ssis', 'aws', 'informatica', 'unix']",2025-06-12 05:37:22
Data Science Instructor,TransOrg,1 - 4 years,Not Disclosed,['Gurugram'],"Pickl.AI (TransOrgs education brand) is looking for an instructor who is technically immersed in data science/data engineering as subjects. We are looking for a creative instructor who wants to accelerate their exposure to many areas in Machine Learning, loves wearing multiple hats and can take full ownership of their work.\n\nResponsibilities:\n• Design and deliver data science and data engineering training programs to students at Pickl.AI partner institutions\n• Teach and mentor students on topics such as data analysis, machine learning, statistics, data visualisation, and other relevant topics\n• Create and develop instructional materials, including lesson plans, presentations, assignments, and assessments\n• Keep up-to-date with the latest developments in data science and incorporate new and emerging trends into the curriculum\n• Include hands-on and relevant case studies in the topics that you are teaching\n• Provide guidance and support to students throughout their learning journey, including answering questions and providing feedback on assignments and projects\n• Collaborate with other instructors and team members to continuously improve the curriculum and training programs\n• Participate in meetings and training sessions to enhance instructional skills and techniques\n• Maintain accurate and up-to-date records of student attendance, progress, and grades\n\nRequirements:\n• Master's degree or Ph.D. in Computer Science, Data Science, Statistics, or a related field would be preferred\n• Excellent knowledge and understanding of data science concepts, techniques, and tools\n• Strong presentation and communication skills\n• Ability to work independently and in a team environment\n• Experience in teaching or mentoring students in a classroom or online setting is a plus\n• Passion for teaching and helping others learn.\n\nAbout the company: TransOrg Analytics has over a decade of specialization in machine learning and data science consulting. Pickl.AI is the education brand of TransOrg Analytics. Visit us at https://pickl.ai and www.transorg,com for details",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'tutoring', 'Education', 'Statistics', 'Machine Learning']",2025-06-12 05:37:24
Senior Principal Engineer - IT Business Analysis,Mercer,5 - 8 years,11-16 Lacs P.A.,['Mumbai (All Areas)'],"Job Description For Posting\nWe are seeking a talented individual to join our team at Marsh .This role will be based in Mumbai .This is a hybrid role that has a requirement of working at least three days a week in the office.\n\nSenior Principal Engineer - IT Business Analysis\n\nWe will count on you to:\nBe a highly motivated team player working within MMC Agile culture, within a specified Agile framework of Scrum or KANBAN and maintain a willingness for continuously improving your agile mindset. \nWork with the Product Owner to communicate the product vision, roadmap, value, and MVP to the Agile team to enable empathy and a shared understanding thereby helping the team to formulate an appropriate solution. \nCollaborate with the Pod Leadership and Product Owner to create Personas, Story Maps, and a Release Plan for the project.\nWork with the Product Owner to communicate the product vision, roadmap, value, and MVP to the agile teams to enable empathy and a shared understanding thereby helping the team to formulate an appropriate solution.\nWork in partnership with the Product Owner and agile teams in the creation and maintenance of Product Backlog Items ensuring that Epics and User Stories are continuously prioritized and aligned to the Product Roadmap and MVP.\nFacilitate refinement sessions with the Agile Teams and Business to sufficiently detail out User Stories, to include dependencies.\nFacilitate the Sprint Review ceremony by working with the agile teams, Product Owner, Business and Customer to review, assess and adapt the latest product increment by incorporating customer insights and feedback into the Product Backlog.\nActively work with Pod Leadership by running assessments processes for the POD and provide constructive feedback towards continuously improving the Systems Analyst function, standards and processes in Global Technology.\nFacilitate discussions and collaborate with data engineers, data architects, technical experts and AI engineers to integrate AI solutions into application design.\nDemonstrate an awareness of MMC and Mercer Technical, Security and Process Standards and work with the team to incorporate them in product delivery through the software development life cycle. \n\nWhat you need to have:\nHighly motivated candidate who is inquisitive, a rapid learner that is comfortable working as part of a remote team. \nPossess strong communication skills with the capability of working collaboratively within the organization, regardless of boundaries.\nAn effective communicator for both technical and business-oriented audiences.  \nDemonstrated requirements gathering skills showcasing the capture of customer needs and business drivers using a variety of techniques into product backlog items such as Epics and User Stories.\nProven quantitative, analytical, and problem-solving skills.\nAbility to find resolutions regarding own work methods requiring minimal direction.  \nBe willing to respond to emergent changes rather than focused on existing plans. \nFamiliarity with AI concepts and technologies, including machine learning, natural language processing, and data analytics, while ensuring compliance with AI governance frameworks. Leverage these technologies to enhance user experiences and improve decision-making processes within applications.\n\nWhat makes you stand out?\nAbility to craft effective prompts that optimize AI responses, enhancing the functionality of AI-driven applications for problem-solving, analysis, and content generation.\nStay open to learning about emerging AI tools and methodologies.\nAn Agile Mindset with an in-depth understanding of Agile Principles.\nPrior experience as an IT Systems Analyst or IT Business Analyst working as part of an agile team using an Agile Workflow tool such as JIRA or TFS. \nAgile Certification, such as CSM, PSM, CSPO, PSPO, PMI-ACP, Certified SAFe Practitioner, Azure AI Fundamentals is desirable.\n\n\nMarsh McLennan (NYSE: MMC) is a global leader in risk, strategy and people, advising clients in 130 countries across four businesses: Marsh, Guy Carpenter, Mercer and Oliver Wyman. With annual revenue of $24 billion and more than 90,000 colleagues, Marsh McLennan helps build the confidence to thrive through the power of perspective. For more information, visit marshmclennan.com, or follow on LinkedIn and X.\n\nMarsh McLennan is committed to embracing a diverse, inclusive and flexible work environment. We aim to attract and retain the best people and embrace diversity of age, background, caste, disability, ethnic origin, family duties, gender orientation or expression, gender reassignment, marital status, nationality, parental status, personal or social status, political affiliation, race, religion and beliefs, sex/gender, sexual orientation or expression, skin color, or any other characteristic protected by applicable law.\n\nMarsh McLennan is committed to hybrid work, which includes the flexibility of working remotely and the collaboration, connections and professional development benefits of working together in the office. All Marsh McLennan colleagues are expected to be in their local office or working onsite with clients at least three days per week. Office-based teams will identify at least one anchor day” per week on which their full team will be together in person.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analysis', 'Use Cases', 'Requirement Gathering']",2025-06-12 05:37:26
Database Engineer-Architect 7+ Years C2H,Greetings from BCforward INDIA TECHNOLOG...,7 - 12 years,15-30 Lacs P.A.,"['Bangalore Rural', 'Bengaluru']","Greetings from BCforward INDIA TECHNOLOGIES PRIVATE LIMITED.\n\nContract To Hire(C2H) Role\nLocation: Bangalore\nPayroll: BCforward\nWork Mode: Hybrid\n\nJD\n\nPreferred Skills: 5+ years relevant experience into Database Engineer-Architect(Postgres / Postgresql; Oracle; Linux)\n\nDescription:\nSkills: Postgres / Postgresql; Oracle; Linux\n\nDesirable attributes\nSkill at the Unix command line\nThe ability to write code or script, e.g. for test harnesses or other database-related tools and utilities\nExperience as a database administrator and/or a Unix system administrator\nKnowledge of virtualization and cloud infrastructures, and of implementations such as VMWare, OpenShift, Kubernetes and Docker\nKnowledge of AWS, Google Cloud, Azure or other public cloud offerings\nKnowledge of modern storage and compute technologies, including hyper-converged infrastructures\nBachelors degree preferred.\n\nPlease share your Updated Resume, PAN card soft copy, Passport size Photo & UAN History.\n\nInterested applicants can share updated resume to g.sreekanth@bcforward.com\n\nNote: Looking for Immediate to 15-Days joiners at most.\n\nAll the best",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database Engineer', 'Postgresql', 'Oracle', 'database Architect', 'Linux', 'Unix command']",2025-06-12 05:37:28
Onix is Hiring Hadoop GCP Engineers!!!,Datametica,4 - 8 years,Not Disclosed,"['Pune', 'Bengaluru']","We are looking for skilled Hadoop and Google Cloud Platform (GCP) Engineers to join our dynamic team. If you have hands-on experience with Big Data technologies and cloud ecosystems, we want to hear from you!\nKey Skills:\nHadoop Ecosystem (HDFS, MapReduce, YARN, Hive, Spark)\nGoogle Cloud Platform (BigQuery, DataProc, Cloud Composer)\nData Ingestion & ETL pipelines\nStrong programming skills (Java, Python, Scala)\nExperience with real-time data processing (Kafka, Spark Streaming)\nWhy Join Us?\nWork on cutting-edge Big Data projects\nCollaborate with a passionate and innovative team\nOpportunities for growth and learning\nInterested candidates, please share your updated resume or connect with us directly!",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['gcp', 'Big Data', 'pyspark', 'Hive', 'Sqoop', 'mapreduce', 'Bigquery', 'Hadoop', 'Spark', 'YARN', 'pig', 'bigq']",2025-06-12 05:37:31
Senior Software Engineer-7916,WebMD,5 - 10 years,Not Disclosed,['Navi Mumbai'],"Position: Senior Software Engineer (Data Engineer)\nNo. of Positions: 1\nAbout WebMD:\nHeadquartered in El Segundo, Calif., Internet Brands is a fully integrated online media and software services\norganization focused on four high-value vertical categories: Health, Automotive, Legal, and Home/Travel. The\ncompanys award-winning consumer websites lead their categories and serve more than 250 million monthly\nvisitors, while a full range of web presence offerings has established deep, long-term relationships with SMB and\nenterprise clients. Internet Brands,powerful, proprietary operating platform provides the flexibility and\nscalability to fuel the companys continued growth. Internet Brands is a portfolio company of KKR and Temasek.\nWebMD Health Corp., an Internet Brands Company, is the leading provider of health information services, serving\npatients, physicians, health care professionals, employers, and health plans through our public and private online\nportals, mobile platforms, and health-focused publications. The WebMD Health Network includes WebMD\nHealth, Medscape, Jobson Healthcare Information, prIME Oncology, MediQuality, Frontline, QxMD, Vitals\nConsumer Services, MedicineNet, eMedicineHealth, RxList, OnHealth, Medscape Education, and other owned\nWebMD sites. WebMD, Medscape, CME Circle, Medpulse, eMedicine®, MedicineNet®, theheart.org®, and\nRxList® are among the trademarks of WebMD Health Corp. or its subsidiaries.\nFor Company details, visit our website: www.webmd.com / www.internetbrands.com\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex,\nsexual orientation, gender identity, national origin, disability, or veteran status\nEducation: B.E. Computer Science/IT degree (or any other engineering discipline)\nExperience: 5+ years\nWork timings: 2:00 PM to 11:00 PM IST.\nDescription:\nWe are seeking an experienced and passionate Senior Software Developer to join our team. In this role,\nyou will work closely with cross-functional teams, developers, stakeholders, and business units to\ngather and analyze business requirements, design, build and implement ETL Solutions, and maintain\nthe infrastructure. The ideal candidate will have a strong background in business analysis, SQL, Unix,\nPython, ETL Tools to ensure the successful execution of projects.\nResponsibilities:\nLead requirements gathering sessions with key stakeholders to understand business needs and\nobjectives.\nCollaborate with and across Agile teams to design, develop, test, implement and support ETL\nprocesses for data transformation and preparation.\nManage data pipelines for analytics and operational use.\nEnsure data quality, data accuracy and integrity across multiple sources and systems.\nPerform unit tests and conduct reviews with other team members to make sure your code is\nrigorously designed, elegantly coded, and effectively tuned for performance.\nShould be able to come up with multiple approaches to any ETL\nproblem statement/solution/technical challenge and take well informed decision to pick the\nbest solution.\nAutomate ETL Processes using Cron and/or using Job Scheduler tools like AirFlow.\nAdhere to company standards and Serve as a key contributor to the design and development of\nexception handling, code/data standardization procedures, resolution steps and Quality Assurance\ncontrols.\nMaintain a version repository and ensure version control.\nCreate visual aids such as diagrams, charts, and screenshots to enhance documentation.\nWork with Infrastructure/systems team and developers to ensure all modules are up-to- date and\nare compatible with the code.",,,,"['ETL', 'SQL', 'UNIX', 'Python']",2025-06-12 05:37:33
Senior Artificial Intelligence Engineer,Ignitho,4 - 6 years,Not Disclosed,['Chennai( Sholinganallur )'],"Job Title: Senior AI Engineer\nLocation: Chennai\nReports To: Data Architect\n\nAbout the Company:\nIgnitho Inc. is a leading AI and data engineering company with a global presence, including US, UK, India, and Costa Rica offices.\nVisit our website to learn more about our work and culture: www.ignitho.com.\nIgnitho is a portfolio company of Nuivio Ventures Inc., a venture builder dedicated to developing Enterprise AI product companies across various domains, including AI, Data Engineering, and IoT.\nLearn more about Nuivio at: www.nuivio.com.\n\nJob Summary:\nAs a Senior AI Engineer, the candidate will lead the design, development, and deployment of cutting-edge machine learning and artificial intelligence solutions. The candidate will work closely with cross-functional teams to understand business needs and translate them into scalable AI-driven applications.\n\nKey Responsibilities:\nDesign and implement machine learning models and AI agents/LLMs.\nDevelop and optimize AI pipelines (LLM, RAG, fine-tuning)\nCollaborate with product, engineering, and data teams to define and implement AI-driven features.\nEvaluate model performance and iterate to improve accuracy and efficiency.\nMentor junior engineers and contribute to team best practices.\nStay up to date with state-of-the-art AI technologies and research.\n\nRequired Qualifications:\nBachelors or master’s in computer science, AI, or related field.\n5+ years of experience in AI/ML development.\nExperience working with LLMs, Agentic AI, RAG, and model fine-tuning\nStrong programming skills in Python and familiarity with frameworks such as TensorFlow, PyTorch, and Scikit-learn.\nExperience with cloud platforms (Azure preferred).\nSolid understanding of data preprocessing, model training, validation, and deployment.",Industry Type: Emerging Technologies (AI/ML),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Python', 'Pytorch', 'Azure Cloud', 'RAG', 'LLM', 'Ai Platform', 'AWS', 'Scikit-Learn', 'TensorFlow']",2025-06-12 05:37:35
Senior Machine Learning Engineer,"Technology, Information and Internet",5 - 10 years,30-35 Lacs P.A.,['Gurugram'],"We are seeking a highly skilled Senior Machine Learning Engineer to join our dynamic team. The ideal candidate will have a deep understanding of Machine Learning, AI, and cloud platforms like Azure. You will play a key role in designing, developing, and deploying ML models and Generative AI solutions to solve complex business problems.\n\nRole overview\nEngineering Masters degree or PhD in Data Science, Statistics, Mathematics, or related fields\n5 years+ experience in a Machine Learning Engineer role into large corporate organizations\nExperience of working with ML models in a cloud ecosystem.\nStatistics & amp; Machine Learning\nStatistics: Strong understanding of statistical analysis and modelling techniques (e.g., regression analysis, hypothesis testing, time series analysis)\nClassical ML: Very strong knowledge in classical ML algorithms for regression & classification, supervised and unsupervised machine learning, both theoretical and practical (e.g. using scikit-learn, xgboost)\nML niche: Expertise in at least one of the following ML specialisations: Timeseries forecasting / Natural Language Processing / Computer Vision\nDeep Learning: Good knowledge of Deep Learning fundamentals (CNN, RNN, transformer architecture, attention mechanism, ) and one of the deep learning frameworks (pytorch, tensorflow, keras)\nGenerative AI: Good understanding of Generative AI specificities and previous experience in working with Large Language Models is a plus (e.g. with openai, langchain).\nMLOps\nModel strategy: Expertise in designing, implementing, and testing machine learning strategies.\nModel integration: Very strong skills in integrating a machine learning algorithm in a data science application in production.\nModel performance: Deep understanding of model performance evaluation metrics and existing libraries (e.g., scikit-learn, evidently)\nModel deployment: Experience in deploying and managing machine learning models in production either using specific cloud platform, model serving frameworks, or containerization.\nModel monitoring: Experience with model performance monitoring tools is a plus (Grafana, Prometheus).\nSoftware Engineering\nPython: Very strong coding skills in Python including modularity, OOP, data & config manipulation frameworks (e.g., pandas, pydantic) etc.\nPython ecosystem: Strong knowledge of tooling in Python ecosystem such as dependency management tooling (venv, poetry), documentation frameworks (e.g. sphinx, mkdocs, jupyter-book), testing frameworks (unittest,pytest)\nSoftware engineering practices: Experience in putting in place good software engineering practices such as design patterns, testing (unit, integration), clean code, code formatting etc.\nDebugging: Ability to troubleshoot and debug issues within machine learning pipelines.\nData Science Experimentation and Analytics\nData Visualization: Knowledge of data visualization tools such as plotly, seaborn, matplotlib, etc. to visualise, interpret and communicate the results of machine learning models to stakeholders. Basic knowledge of PowerBI is a plus.\nData Cleaning: Experience with data cleaning and preprocessing techniques such as feature scaling, dimensionality reduction, and outlier detection (e.g. with pandas, scikit-learn).\nData Science Experiments: Understanding of experimental design and A/B testing methodologies.\nData Processing:\nDatabricks/Spark: Basic knowledge of PySpark for big data processing\nDatabases: Basic knowledge of SQL to query data in internal systems\nData Formats: Familiarity with different data storage formats such as Parquet and Delta.\nDevOps\nAzure DevOps: Experience using a DevOps platform such as Azure DevOps for using Boards, Repositories, Pipelines\nGit: Experience working with code versioning (git), branch strategies, and collaborative work with pull requests. Proficient with the most basic git commands.\nCI / CD: Experience in implementing/maintaining pipelines for continuous integration (including execution of testing strategy) and continuous deployment is preferable.\nCloud Platform:\nAzure Cloud: Previous experience with services like Azure Machine Learning Services and/or Azure Databricks on Azure is preferable.\nSoft skills:\nStrong analytical and problem-solving skills, with attention to detail\nExcellent verbal and written communication and pedagogical skills with technical and non-technical teams\nExcellent teamwork and collaboration skills\nAdaptability and reactivity to new technologies, tools, and techniques\nFluent in English.\nWhat would you do here:\nManaging the lifecycle of machine learning models\nDevelop and implement machine learning models to solve complex business problems.\nEnsure that models are accurate, efficient, reliable, and scalable.\nDeploy machine learning models to production environments, ensuring that models are integrated with software systems.\nMonitor machine learning models in production, ensuring that models are performing as expected and that any errors or performance issues are identified and resolved quickly.\nMaintain machine learning models over time. This includes updating models as new data becomes available, retraining models to improve performance, and retiring models that are no longer effective.\nDevelop and implement policies and procedures for ensuring the ethical and responsible use of machine learning models. This includes addressing issues related to bias, fairness, transparency, and accountability.\nContinuous Improvements:\nStay up to date with the latest developments in the field: read research papers, attend conferences, and participate in trainings to expand their knowledge and skills.\nIdentify and evaluate new technologies and tools that can improve the efficiency and effectiveness of machine learning projects.\nPropose and implement optimizations for current machine learning workflows and systems.\nProactively identify areas of improvement within the pipelines.\nMake sure that created code is compliant with our set of engineering standards.\nCollaboration with other data experts (Data Engineers, Platform Engineers, and Data Analysts)\nParticipate to pull requests reviews coming from other team members.\nAsk for review and comments when submitting their own work.\nActively participate to the day-to-day life of the project (Agile rituals), the data science team (DS meeting) and the rest of the Global Engineering team.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure', 'Generative AI', 'Artificial Intelligence', 'Machine Learning', 'Python']",2025-06-12 05:37:37
Sr Associate Software Engineer – Large Molecule Discovery,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"The role is responsible for designing, developing, and maintaining software solutions for Research scientists.\nAdditionally, it involves automating operations, monitoring system health, and responding to incidents to minimize downtime. You will join a multi-functional team of scientists and software professionals that enables technology and data capabilities to evaluate drug candidates and assess their abilities to affect the biology of drug targets. This team implements scientific software platforms that enable the capture, analysis, storage, and reporting for our Large Molecule Discovery Research team (Design, Make, Test and Analyze processes).\nThe team also interfaces heavily with teams supporting our in vitro assay management systems and our compound inventory platforms. The ideal candidate possesses experience in the pharmaceutical or biotech industry, strong technical skills, and full stack software engineering experience (spanning SQL, back-end, front-end web technologies, automated testing).\nRoles & Responsibilities:\nWork closely with product team, business team including scientists, and other collaborators\nAnalyze and understand the functional and technical requirements of applications, solutions and systems and translate them into software architecture and design specifications\nDesign, develop, and implement applications and modules, including custom reports, interfaces, and enhancements\nDevelop and execute unit tests, integration tests, and other testing strategies to ensure the quality of the software\nConduct code reviews to ensure code quality and adherence to standard methodologies\nCreate and maintain documentation on software architecture, design, deployment, disaster recovery, and operations\nProvide ongoing support and maintenance for applications, ensuring that they operate smoothly and efficiently\nStay updated with the latest technology and security trends and advancements\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The professional we seek is a with these qualifications.\nBasic Qualifications:\nRMasters degree with 1 - 3 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nBachelors degree with 4 - 6 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nDiploma with 7 - 9 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field\nPreferred Qualifications and Experience:\n1+ years of experience in implementing and supporting biopharma scientific software platforms\nFunctional Skills:\nProficient in Java or Python\nProficient in at least one JavaScript UI Framework (e.g. ExtJS, React, or Angular)\nProficient in SQL (e.g. Oracle, PostgreSQL, Databricks)\nPreferred Qualifications:\nExperience with event-based architecture and serverless AWS services such as EventBridge, SQS, Lambda or ECS.\nExperience with Benchling\nHands-on experience with Full Stack software development\nStrong understanding of software development methodologies, mainly Agile and Scrum\nWorking experience with DevOps practices and CI/CD pipelines\nExperience of infrastructure as code (IaC) tools (Terraform, CloudFormation)\nExperience with monitoring and logging tools (e.g., Prometheus, Grafana, Splunk)\nExperience with automated testing tools and frameworks\nExperience with big data technologies (e.g., Spark, Databricks, Kafka)\nExperience with leveraging the use of AI-assistants (e.g. GitHub Copilot) to accelerate software development and improve code quality\nProfessional Certifications :\nAWS Certified Cloud Practitioner preferred\nSoft Skills:\nExcellent problem solving, analytical, and troubleshooting skills\nStrong communication and interpersonal skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to learn quickly & work independently\nTeam-oriented, with a focus on achieving team goals\nAbility to manage multiple priorities successfully\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Software Engineering', 'SQS', 'Kafka', 'Spark', 'Databricks', 'Lambda', 'GitHub Copilot']",2025-06-12 05:37:40
Senior Cloud Infrastructure Engineer,Guidehouse,4 - 9 years,Not Disclosed,['Thiruvananthapuram'],"Senior Cloud Infrastructure Engineer\nJob Description Summary\nResponsible for supporting the business applications, integrating the application functions and resources across the product life cycle, right from planning, building, testing, and deployment to support. Accountable for installation, configuration, maintenance and monitoring of the business application hosted environment as well as ensuring its performance, capacity, scalability and uptime. Managing the DevOps, CI/CD and automation of various workflow in the business application. Designs and architect Application Infrastructure in QA/UAT/stage and production environments. Maintenance and administration of application & DB servers including patch updates and vulnerability mitigation.\n\nLevel Description\n- Entry to established level technical individual contributor\n- Assists with the implementation and executes on technical initiatives and solutions\n- May influence other technical staff through explanation of facts, policies and practices related to job area Organization Impact & Communication\n- Achieves operational targets within job area with direct impact on results\n- Executes daily work within existing procedures and guidelines\n- May influence other technical staff through explanation of facts, policies and practices related to job area\n- Communicates with contacts typically within the project area or technical department on matters that involve obtaining or providing information requiring some explanation or interpretation in order to reach agreement\nInnovation & Complexity\n- Entry to established level technical individual contributor\n- Builds on knowledge of an industry or segment and works to deepen experience across a range of projects\n- Follows operating guidelines and identifies recommended enhancements in processes to improve effectiveness of job area\n- Works under close to limited supervision on small to midsize projects or assignments\nLeadership & Talent Management\n- Requires general instructions for new lines of work or special projects\n- Assists with the implementation and executes on technical initiatives and solutions\n- Relies on limited experience and judgment to plan and accomplish assigned tasks and goals\nKnowledge & Experience\n- Requires technical knowledge of job area typically obtained through advanced education combined with experience.\n- Requires a University Degree and minimum 2-4 years of prior relevant experience; some roles may require graduate-level education Masters or Doctorate degree (Relevant experience may be substituted for formal education or advanced degree\n\nJob Posting\nGuidehouse India Private Limited is seeking a Senior Cloud Infrastructure Engineer to be part of its Platform Engineering team. The ideal candidate will have experience with infrastructure automation tools such as Terraform, Docker, Kubernetes, GitHub, and CI/CD tooling. This role requires someone comfortable writing code who can help set standards and best practices to guide cloud automation efforts across the company while building and maintaining infrastructure in a complex environment. This position will be part of the central Solution Engineering and Architecture Services (SEAS) group in the office of the CTO and will require working with users across Business segments, IT, and SEAS. The Senior DevOps Engineer will mentor team members. As such, the ideal candidate must have experience clearly explaining solutions to complex problems and a demonstrated ability to lead and impart knowledge effectively to junior team members.\nWhat You Will Do:\nWork with tech leads and product managers to plan and implement complex cloud infrastructure while mentoring junior team members.\nLook for enhancements and innovative solutions to improve existing IT Infrastructure, procedures, or processes.\nBuild standard Infrastructure-as-Code cloud service blueprints to support self service, on demand provisioning of tooling for the business\nCommit changes into git repositories, open Pull Requests, and review others code\nBuild automations in CI/CD tools like GitHub Actions\nAdminister various flavors of Linux containers running in Kubernetes and other container services in public cloud (Azure & AWS) environments.\nWhat You Will Need:\n5+ years of relevant industry experience.\n3+ years of experience in a DevOps or Cloud Infrastructure Engineering (or similar) role\nStrong knowledge of Terraform and other Infrastructure-as-Code and/or automation tools (e.g., Ansible, Chef, Puppet, GitHub Actions).\nHave a solid background in agile methodologies and DevOps practices.\nKnowledge of containerization through Docker, Kubernetes, Ansible, etc.\nExperience in creating CI/CD pipelines to deploy into multiple environments, Knowledge of Cloud services like AWS and Azure.\nWorking knowledge of ServiceNow, JIRA, and GitHub.\nStrong knowledge of infrastructure\nExperience working in public cloud (specifically Azure or AWS)\nAbility to research, plan, organize, lead, and implement new processes or technologies.\nProficiency in scripting and programming languages (e.g., Python, Bash).\nWhat Would Be Nice To Have:\nExperience configuring Helm charts, writing Terraform, administrating Azure and/or AWS, working in Docker\nExperience maintaining Big Data or analytics platforms\nExperience with agile management tools like Jira, GitHub Projects, or Azure DevOps\nAbility to clearly articulate pros and cons of various technologies and platforms and architectural options, as well as being able to document use cases, solutions and recommendations.\nCloud certifications (e.g. Azure AZ900 or AWS CCP) preferred\nSystem Administrator level experience with Linux",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Terraform', 'Ansible', 'Puppet']",2025-06-12 05:37:42
Senior Software Engineer,WebMD,5 - 10 years,Not Disclosed,['Navi Mumbai'],"Headquartered in El Segundo, Calif., Internet Brands is a fully integrated online media and software services organization focused on four high-value vertical categories: Health, Automotive, Legal, and Home/Travel. The company's award- winning consumer websites lead their categories and serve more than 250 million monthly visitors, while a full range of web presence offerings has established deep, long-term relationships with SMB and enterprise clients. Internet Brands' powerful, proprietary operating platform provides the flexibility and\nscalability to fuel the company's continued growth. Internet Brands is a portfolio company of KKR and Temasek.\n\nWebMD Health Corp., an Internet Brands Company, is the leading provider of health information services, serving patients, physicians, health care professionals, employers, and health plans through our public and private online portals, mobile platforms, and health-focused publications. The WebMD Health Network includes WebMD Health, Medscape, Jobson Healthcare Information, prIME Oncology, MediQuality, Frontline, QxMD, Vitals Consumer Services, MedicineNet, eMedicineHealth, RxList, OnHealth, Medscape Education, and other owned WebMD sites. WebMD, Medscape, CME Circle®, Medpulse®, eMedicine®, MedicineNet®, theheart.org®, and RxList® are among the trademarks of WebMD Health Corp. or its subsidiaries.\n\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status\n\nFor Company details, visit our website: www.webmd.com / www.internetbrands.com\n\nEducation: B.E. Computer Science/IT degree (or any other engineering discipline)\nExperience: 5+ years\nWork timings: 2 PM to 11 PM IST\n\n\nMinimum 5 years experience with\nMinimum 2 years experience with databases other than Oracle/MSSQL/PostgreSQL, such as Vertica, MongoDB, or HDFS/HIVE\nMinimum 5 years hands on\nExperience with\nExperience working closely with Business Intelligence, Finance, Marketing and Sales teams\nKnowledge of scripting languages such as Perl or Python is good to have\nExperience with the reporting platforms such as Tableau or Cognos will be an added advantage\nKnowledge about Web analytics or Business Intelligence is good to have\nKnowledge of GCP is good to have\nExcellent communication and documentation skills\nSelf-motivated with willingness to learn new technologies and business, and willing to take initiative beyond basic responsibilities\n\nDesign, develop and support multiple data projects in traditional relational databases such as Oracle, MSSQL, and PostgreSQL as well as non-traditional databases such as Vertica and MongoDB\nAnalyse business requirements, design, and implement required data model and ETL processes\nParticipate in data architecture and engineering decision making/planning\nCreate an enterprise-level data inventory regardless of source, format, structure\nConnect and integrate individual datasets for better analysis and transparency between the data\nTranslate complex technical subjects into terms that can be understood by both technical and non- technical audiences",,,,"['ETL', 'SQL', 'Data Integration']",2025-06-12 05:37:45
Senior Software Engineer,Joules to Watts,4 - 6 years,2-6 Lacs P.A.,['Bengaluru( Whitefield )'],"Only for Immediate Joiners\nCore Responsibility:\nThe project team will be spread between Paris and Bangalore. So, the candidate with an experience of 3-6 years is expected to work and coordinate on daily basis with the remote teams. Ability to learn new technology / framework / methodology.\n    Hands-on individual responsible for producing excellent quality of code, adhering to expected coding standards and industry best practices. \n    Must have strong knowledge and working experience on Big DATA ecosystem.\n    Must have strong experience in SPARK/SCALA, NIFI, KAFKA, HIVE, PIG.\n   Strong knowledge and experience working on HQL (hive Query Language)\n•    Must have strong strong expertise in Debugging and Fixing Production Issues on BIG DATA eco System.\n    Knowledge on code version management using Git & Jenkins, Nexus.\n•    High levels of ownership and commitment on deliverables. Strong and Adaptive Communication Skills; Should be comfortable interacting with Paris counterparts  to probe a technical problem or clarify requirement specifications. \n\nKEY SKILLS:\nSound knowledge on SPARK/SCALA, NIFI, KAFKA - Must Have\nSound Knowledge on HQL\nKnowledge on Kibana, Elastic Search Log stash Good to know\nBasic Awareness of CD/CI concepts & Technologies\nBig Data Ecosystem Good to know",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Hive', 'Apache Pig', 'Hadoop', 'SCALA', 'Kafka', 'Spark', 'Apache Nifi']",2025-06-12 05:37:47
Senior Lead business execution consultant,Wells Fargo,7 - 12 years,Not Disclosed,['Bengaluru'],"About this role:\nWells Fargo is seeking a Senior Lead business execution consultant\n\nIn this role, you will:\nAct as a Business Execution advisor to leadership to drive performance and initiatives, and develop and implement information delivery or presentations to key stakeholders and senior management",,,,"['Business execution', 'Business Implementation', 'Data Engineering', 'NLP', 'generative AI', 'Data Mining', 'machine learning', 'Strategic Planning', 'agentic AI']",2025-06-12 05:37:50
Sr Associate Software Engineer – Electronic Lab Notebook (ELN),Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"Roles & Responsibilities:\nDesign, develop, and implement applications and modules, including custom reports, interfaces, and enhancements\nAnalyze and understand the functional and technical requirements of applications, solutions and systems and translate them into software architecture and design specifications\nDevelop and execute unit tests, integration tests, and other testing strategies to ensure the quality of the software\nIdentify and resolve software bugs and performance issues\nWork closely with cross-functional teams, including product management, design, and QA, to deliver high-quality software on time\nMaintain detailed documentation of software designs, code, and development processes\nCustomize modules to meet specific business requirements\nWork on integrating with other systems and platforms to ensure seamless data flow and functionality\nProvide ongoing support and maintenance for applications, ensuring that they operate smoothly and efficiently\nPossesses strong rapid prototyping skills and can quickly translate concepts into working code\nContribute to both front-end and back-end development using cloud technology\nDevelop innovative solution using generative AI technologies\nCreate and maintain documentation on software architecture, design, deployment, disaster recovery, and operations\nIdentify and resolve technical challenges effectively\nStay updated with the latest trends and advancements\nWork closely with product team, business team including scientists, and other stakeholders\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The [vital attribute] professional we seek is a [type of person] with these qualifications.\nBasic Qualifications:\nMasters degree and 1 to 3 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nBachelors degree and 3 to 5 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nDiploma and 7 to 9 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field\nPreferred Qualifications:\nExperience in implementing and supporting biopharma scientific software platforms\nFunctional Skills:\nMust-Have Skills:\nProficient in a General Purpose High Level Language (e.g. Python, Java, C#.NET)\nProficient in a Javascript UI Framework (e.g. React, ExtJs)\nProficient in SQL (e.g. Oracle, PostGres, Databricks)\nExperience with event-based architecture (e.g. Mulesoft, AWS EventBridge, AWS Kinesis, Kafka)\nGood-to-Have Skills:\nStrong understanding of software development methodologies, mainly Agile and Scrum\nHands-on experience with Full Stack software development\nStrong understanding of cloud platforms (e.g AWS) and containerization technologies (e.g., Docker, Kubernetes)\nWorking experience with DevOps practices and CI/CD pipelines\nExperience with big data technologies (e.g., Spark, Databricks)\nExperience with API integration, serverless, microservices architecture (e.g. Mulesoft, AWS Kafka)\nExperience with monitoring and logging tools (e.g., Prometheus, Grafana, Splunk)\nExperience of infrastructure as code (IaC) tools (Terraform, CloudFormation)\nExperience with version control systems like Git\nExperience with automated testing tools and frameworks\nExperience with Benchling, Revvity, IDBS, or similar LIMS/ELN platforms\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nAWS Certified Cloud Practitioner preferred\nSoft Skills:\nExcellent problem solving, analytical, and troubleshooting skills\nStrong communication and interpersonal skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to learn quickly & work independently\nTeam-oriented, with a focus on achieving team goals\nAbility to manage multiple priorities successfully\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Software Engineering', 'LIMS', 'Terraform', 'ELN', 'CloudFormation', 'automated testing', 'Spark', 'Databricks', 'AWS']",2025-06-12 05:37:52
Senior Software Engineer,eG Innovations,5 - 8 years,Not Disclosed,['Chennai'],Responsibilities:\n\nDesign and manage application module for data Integration / ETL collection/processing/storage/retrieval) using latest technologies\nShould be completely aware of coding standards and should be able to design & develop high performance & scalable application\nAbility to prototype solutions quickly and analyse and compare multiple solutions and products based on requirements.,,,,"['Core Java', 'Spring Boot', 'JMS', 'Open AI', 'Kafka', 'J2Ee', 'ETL', 'Data Analytics', 'SOAP', 'Microservices', 'Data Integration']",2025-06-12 05:37:55
Sr. Associate Full Stack Software Engineer,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\n\n\nIn this vital role you will be responsible for designing, developing, and maintaining software applications and solutions that meet business needs and ensuring the availability and performance of critical systems and applications. This role involves working closely with product managers, designers, data engineers, and other engineers to create high-quality, scalable software solutions and automating operations, monitoring system health, and responding to incidents to minimize downtime.\n\nYou will play a key role in a regulatory submission content automation initiative which will modernize and digitize the regulatory submission process, positioning Amgen as a leader in regulatory innovation. The initiative demonstrates innovative technologies, including Generative AI, Structured Content Management, and integrated data to automate the creation, and management of regulatory content.\n\n\n\nRoles & Responsibilities:\nPossesses strong rapid prototyping skills and can quickly translate concepts into working code\nContribute to both front-end and back-end development using cloud technology\nDevelop innovative solution using generative AI technologies\nEnsure code quality and consistency to standard methodologies\nCreate and maintain documentation on software architecture, design, deployment, disaster recovery, and operations\nIdentify and resolve technical challenges effectively\nStay updated with the latest trends and advancements\nWork closely with product team, business team, and other collaborators\nDesign, develop, and implement applications and modules, including custom reports, interfaces, and enhancements\nAnalyze and understand the functional and technical requirements of applications, solutions and systems and translate them into software architecture and design specifications\nDevelop and implement unit tests, integration tests, and other testing strategies to ensure the quality of the software\nIdentify and resolve software bugs and performance issues\nWork closely with multi-functional teams, including product management, design, and QA, to deliver high-quality software on time\nCustomize modules to meet specific business requirements\nWork on integrating with other systems and platforms to ensure seamless data flow and functionality\nProvide ongoing support and maintenance for applications, ensuring that they operate smoothly and efficiently\n\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients.\nMasters degree and 1 to 3 years of experience in Computer Science, IT or related field OR\nBachelors degree and 3 to 5 years of experience in Computer Science, IT or related field OR\nDiploma and 7 to 9 years of experience in Computer Science, IT or related field\nPreferred Qualifications:\n\n\n\nFunctional\n\nSkills:\nMust-Have Skills:\nProficiency in Python/PySpark development, Fast API, PostgreSQL, Databricks, DevOps Tools, CI/CD, Data Ingestion.\nCandidates should be able to write clean, efficient, and maintainable code.\nKnowledge of HTML, CSS, and JavaScript, along with popular front-end frameworks like React or Angular, is required to build interactive and responsive web applications\nIn-depth knowledge of data engineering concepts, ETL processes, and data architecture principles. Solid understanding of cloud computing principles, particularly within the AWS ecosystem\nSolid understanding of software development methodologies, including Agile and Scrum\nExperience with version control systems like Git\nHands on experience with various cloud services, understand pros and cons of various cloud service in well architected cloud design principles\nStrong problem solving, analytical skills; Ability to learn quickly; Good communication and interpersonal skills\nExperienced with API integration, serverless, microservices architecture.\nExperience in SQL/NOSQL database, vector database for large language models\n\n\n\nGood-to-Have\n\nSkills:\nSolid understanding of cloud platforms (e.g., AWS, GCP, Azure) and containerization technologies (e.g., Docker, Kubernetes)\nExperience with monitoring and logging tools (e.g., Prometheus, Grafana, Splunk)\nExperience with data processing tools like Hadoop, Spark, or similar\n\n\n\nSoft\n\nSkills:\nExcellent analytical and solving skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'PySpark development', 'Data Ingestion', 'PostgreSQL', 'Fast API', 'CI/CD', 'DevOps Tools', 'Databricks']",2025-06-12 05:37:57
Senior JavaScript Software Engineer,Ciklum,6 - 10 years,Not Disclosed,"['Pune', 'Chennai']","Ciklum is looking for a Senior JavaScript Software Engineer to join our team full-time in India.\nWe are a custom product engineering company that supports both multinational organizations and scaling startups to solve their most complex business challenges. With a global team of over 4,000 highly skilled developers, consultants, analysts and product owners, we engineer technology that redefines industries and shapes the way people live.\n\nAbout the role:\nAs a Senior JavaScript Software Engineer, become a part of a cross-functional development team engineering experiences of tomorrow.\nClient for this project is a leading global provider of audit and assurance, consulting, financial advisory, risk advisory, tax, and related services. They are launching a digital transformation project to evaluate existing technology across the tax lifecycle and determine the best future state for that technology. This will include decomposing existing assets to determine functionality, assessment of those functionalities to determine the appropriate end state and building of new technologies to replace those functionalities.\n\nResponsibilities:\nParticipate in requirements analysis\nCollaborate with US and Vendors teams to produce software design and architecture\nWrite clean, scalable code using Angular with Typescript, HTML, CSS, and NET programming languages\nParticipate in pull request code review process\nTest and deploy applications and systems\nRevise, update, refactor and debug code\nDevelop, support and maintain applications and technology solutions\nEnsure that all development efforts meet or exceed client expectations. Applications should meet requirements of scope, functionality, and time and adhere to all defined and agreed upon standards\nBecome familiar with all development tools, testing tools, methodologies and processes\nBecome familiar with the project management methodology and processes\nEncourage collaborative efforts and camaraderie with on-shore and off-shore team members\nDemonstrate a strong working understanding of the best industry standards in software development and version controlling\nEnsure the quality and low bug rates of code released into production\nWork on agile projects, participate in daily SCRUM calls and provide task updates\nDuring design and key development phases, we might need to work a staggered shift as applicable to ensure appropriate overlap with US teams and project deliveries\n\nRequirements:\n  We know that sometimes, you cant tick every box. We would still love to hear from you if you think you will be a good fit\n6+ years of strong hands-on experience with JavaScript (ES6/ES2015+), HTML5, CSS3\n2+ years with hands-on experience with Typescript\n2+ years of hands-on experience with Angular 11+ component architecture, applying design patterns\nExperience with Angular 11+ and migrating to newer versions\nExperience with Angular State management or NgXs\nExperience with RxJS operators\nHands on experience with Kendo UI or Angular material or SpreadJS libraries\nExperience with Nx – Nrwl/Nx library for monorepos\nSkill for writing reusable components, Angular services, directives and pipes\nHands-on experience on C#, SQL Server, OOPS Concepts, Micro Services Architecture\nAt least two-year hands-on experience on .NET Core, ASP.NET Core Web API, SQL, NoSQL, Entity Framework 6 or above, Azure, Database performance tuning, Applying Design Patterns, Agile\n.Net back-end development with data engineering expertise. Experience with MS Fabric as a data platform/ Snowflake or similar tools would be a plus, but not a must need\nSkill for writing reusable libraries\nComfortable with Git & Git hooks using PowerShell, Terminal or a variation thereof\nFamiliarity with agile development methodologies\nExcellent Communication skills both oral & written\nExcellent troubleshooting and communication skills, ability to communicate clearly with US counterparts\n\nDesirable:\nExposure to micro-frontend architecture\nKnowledge on Yarn, Webpack, Mongo DB, NPM, Azure Devops Build/Release configuration\nSignalR, ASP.NET Core and WebSockets\nThis is an experienced level position, and we will train the qualified candidate in the required applications\nWillingness to work extra hours to meet deliverables\nExposure to Application Insights & Adobe Analytics\nUnderstanding of cloud infrastructure design and implementation\nExperience in CI/CD configuration\nGood knowledge of data analysis in enterprises\nExperience with Databricks, Snowflake\nExposure to Docker and its configurations, Experience with Kubernetes\n\nWhat's in it for you?\nCare: your mental and physical health is our priority. We ensure comprehensive company-paid medical insurance, as well as financial and legal consultation\nTailored education path: boost your skills and knowledge with our regular internal events (meetups, conferences, workshops), Udemy licence, language courses and company-paid certifications\nGrowth environment: share your experience and level up your expertise with a community of skilled professionals, locally and globally\nFlexibility: hybrid work mode at Chennai or Pune \nOpportunities: we value our specialists and always find the best options for them. Our Resourcing Team helps change a project if needed to help you grow, excel professionally and fulfil your potential\nGlobal impact: work on large-scale projects that redefine industries with international and fast-growing clients\nWelcoming environment: feel empowered with a friendly team, open-door policy, informal atmosphere within the company and regular team-building events\n\nAbout us:\nAt Ciklum, we are always exploring innovations, empowering each other to achieve more, and engineering solutions that matter. With us, you’ll work with cutting-edge technologies, contribute to impactful projects, and be part of a One Team culture that values collaboration and progress.\nIndia is a strategic innovation hub for Ciklum, with growing teams in Chennai and Pune leading advancements in EdgeTech, AR/VR, IoT, and beyond. Join us to collaborate on game-changing solutions and take your career to the next level.\nWant to learn more about us? Follow us on Instagram, Facebook, LinkedIn\n\nExplore, empower, engineer with Ciklum!\nExperiences of tomorrow. Engineered together\nInterested already?\nWe would love to get to know you! Submit your application. Can’t wait to see you at Ciklum.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Typescript', 'Angular', 'CSS', '.Net', 'HTML']",2025-06-12 05:38:00
Senior QA Engineer - Digital & IT,Skillsoft Software Services,2 - 8 years,Not Disclosed,['Hyderabad'],"We are seeking a passionate technology enthusiast with a strong background in ensuring the delivery of high-quality software applications. If you thrive in a dynamic, fast-paced environment where accountability and creativity are valued, then this opportunity might be the right fit for you. We are looking for an experienced QA Engineer to join our dynamic team.\n\nAs a Senior QA Engineer, you will be an integral part of our agile scrum development process, leading automation efforts within the sprint and ensuring the robustness of our diverse Software-as-a-Service (SaaS) and custom applications. If ensuring software quality is your forte, we are eager to connect with you.",,,,"['automation framework', 'workday', 'business requirements', 'test cases', 'api automation', 'salesforce', 'scripting', 'dynamics', 'java', 'git', 'automation', 'selenium', 'api', 'communication skills', 'python', 'development', 'bdd', 'sap', 'software testing', 'javascript', 'tdd', 'web technologies', 'salesforce testing', 'scrum', 'gui', 'agile']",2025-06-12 05:38:03
Sr Software Engineer: Integration Engineer,HMH,5 - 7 years,Not Disclosed,['Pune'],"The Data Integration Engineer will play a key role in designing, building, and maintaining data integrations between core business systems such as Salesforce and SAP and our enterprise data warehouse on Snowflake. This position is ideal for an early-career professional (1 to 4 years of experience) eager to contribute to transformative data integration initiatives and learn in a collaborative, fast-paced environment.\n\nDuties & Responsibilities:\nCollaborate with cross-functional teams to understand business requirements and translate them into data integration solutions.\nDevelop and maintain ETL/ELT pipelines using modern tools like Informatica IDMC to connect source systems to Snowflake.\nEnsure data accuracy, consistency, and security in all integration workflows.\nMonitor, troubleshoot, and optimize data integration processes to meet performance and scalability goals.\nSupport ongoing integration projects, including Salesforce and SAP data pipelines, while adhering to best practices in data governance.\nDocument integration designs, workflows, and operational processes for effective knowledge sharing.\nAssist in implementing and improving data quality controls at the start of processes to ensure reliable outcomes.\nStay informed about the latest developments in integration technologies and contribute to team learning and improvement.",,,,"['GCP', 'Azure', 'IDMC', 'XML', 'CSV', 'JSON', 'SQL Server', 'AWS', 'data integration', 'data engineering']",2025-06-12 05:38:05
Global Data Steward (GDS) - Materials,MNC Group,12 - 15 years,35-40 Lacs P.A.,['Pune( Chinchwad )'],"Extensive SAP MM Module design, Material Master Data Management/Maintenance\nProcess and MM data model (S/4HANA) (including material master,\nclassification, document management system n-depth experience with SAP ECC and ideally with S/4HANA and MDG\n\nRequired Candidate profile\nExperience with data modelling, definition of data standards, data integration, data\ncleansing, data configuration and data analysis\nExp in defining, implementing and maintain master data governance",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Material Master Data Management', 'Data Stewardship', 'MDM', 'SAP MM Module', 'SAP ECC', 'Mdg', 'S4H']",2025-06-12 05:38:08
Senior High Performance Computing Engineer,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will.\nRole Description:\nThe role is responsible for the design, integration, and management of high performance computing (HPC) systems that encompass both hardware and software components into the organizations network infrastructure. This individual will be responsible for all activities related to handling and supporting the Business and platforms including system administration, as well as incorporating new technologies under the challenge of a sophisticated and constantly evolving technology landscape. This role involves ensuring that all parts of a system work together seamlessly to meet the organizations requirements.\nRoles & Responsibilities:\nImplement, and manage cloud-based infrastructure that supports HPC environments that support data science (e.g. AI/ML workflows, Image Analysis).\nCollaborate with data scientists and ML engineers to deploy scalable machine learning models into production.\nEnsure the security, scalability, and reliability of HPC systems in the cloud.\nOptimize cloud resources for cost-effective and efficient use.\nKeep abreast of the latest in cloud services and industry standard processes.\nProvide technical leadership and guidance in cloud and HPC systems management.\nDevelop and maintain CI/CD pipelines for deploying resources to multi-cloud environments.\nMonitor and fix cluster operations/applications and cloud environments.\nDocument system design and operational procedures.\nBasic Qualifications:\nMasters degree with a 4 - 6 years of experience in Computer Science, IT or related field with hands-on HPC administration OR\nBachelors degree with 6 - 8 years of experience in Computer Science, IT or related field with hands-on HPC administration OR\nDiploma with 10-12 years of experience in Computer Science, IT or related field with hands-on HPC administration\nDemonstrable experience in cloud computing (preferably AWS) and cloud architecture.\nExperience with containerization technologies (Singularity, Docker) and cloud-based HPC solutions.\nExperience with infrastructure-as-code (IaC) tools such as Terraform, CloudFormation, Packer, Ansible and Git.\nExpert with scripting (Python or Bash) and Linux/Unix system administration (preferably Red Hat or Ubuntu).\nProficiency with job scheduling and resource management tools (SLURM, PBS, LSF, etc.).\nKnowledge of storage architectures and distributed file systems (Lustre, GPFS, Ceph).\nUnderstanding of networking architecture and security best practices.\nPreferred Qualifications:\nExperience supporting research in healthcare life sciences.\nExperience with Kubernetes (EKS) and service mesh architectures.\nKnowledge of AWS Lambda and event-driven architectures.\nExposure to multi-cloud environments (Azure, GCP).\nFamiliarity with machine learning frameworks (TensorFlow, PyTorch) and data pipelines.\nCertifications in cloud architecture (AWS Certified Solutions Architect, Google Cloud Professional Cloud Architect, etc.).\nExperience in an Agile development environment.\nPrior work with distributed computing and big data technologies (Hadoop, Spark).\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nRed Hat Certified Engineer (RHCE) or Linux Professional Institute Certification (LPIC)\nAWS Certified Solutions Architect Associate or Professional\nSoft Skills:\nStrong analytical and problem-solving skills.\nAbility to work effectively with global, virtual teams\nEffective communication and collaboration with cross-functional teams.\nAbility to work in a fast-paced, cloud-first environment.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cloud computing', 'resource management', 'Ubuntu', 'Unix system administration', 'linux', 'unix production support', 'Python']",2025-06-12 05:38:10
Machine Learning Engineer,Whats On India Media,3 - 8 years,Not Disclosed,"['Mumbai', 'Gurugram', 'Bengaluru']","Nielsen is seeking an organized, detail oriented, team player, to join the Engineering team in the role of Software Machine Learning Engineer. Nielsen's Audience Measurement Engineering platforms support the measurement of television viewing in more than 30 countries around the world. The Software Engineer will be responsible to define, develop, test, analyze, and deliver technology solutions within Nielsen's Collections platforms.\nRequired Skills\nBachelor's degree in Computer Science or equivalent degree.\n3+ years of software experience\nExperience with Machine learning frameworks and models. Pytorch experience preferred\nStrong understanding of statistical analysis and mathematical data manipulation\nWork with web technology including Java, Python, JavaScript, React/Redux, Kotlin.\nFollow best practices for software development and deployment\nUnderstanding of relational database, big data, and experience in SQL\nProficient at using GIT, GitFlow, JIRA, Gitlab and Confluence.\nStrong analytical and problem solving skills.\nOpen-minded and passionate to learn and grow technology skills\nStrong sense of accountability\nSolution-focused and ability to drive change within the organization\nExperience in writing unit/integration tests including test automation.\nStrong testing and debugging abilities, functional, analytical and technical abilities, ability to find bugs, attention to detail, troubleshooting\nAdditional Useful Skills\nA fundamental understanding of the AWS ecosystem (EC2, S3, EMR, Lambda, etc)\nExperienced in building RESTful APIs.\nExperience in writing unit/integration tests including test automation.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Ml Algorithms', 'Python', 'Pytorch']",2025-06-12 05:38:12
Ml Engineer,Sightspectrum,5 - 10 years,1-6 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Description:\n\n\n\nStrong experience in ETL development, data modeling, and managing data in large-scale environments. - Proficient in AWS services including SageMaker, S3, Glue, Lambda, and CloudFormation/Terraform. - Hands-on expertise with MLOps best practices, including model versioning, monitoring, and CI/CD for ML pipelines.\n\n- Proficiency in Python and SQL; experience with Java is a plus for streaming jobs.\n- Deep understanding of cloud infrastructure automation using Terraform or similar IaC tools. - Excellent problem-solving skills with the ability to troubleshoot data processing and deployment issues.\n- Experience in fast-paced, agile development environments with frequent delivery cycles.\n- Strong communication and collaboration skills to work effectively across cross-functional team Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MLOps', 'Aws Sagemaker', 'Rasa', 'Serverless', 'Ml']",2025-06-12 05:38:14
Sales Engineer - CLOUD Products ( Cochin Location ),Redington Group,0 - 2 years,2.75-6 Lacs P.A.,['Kochi'],Role & responsibilities\n\nTO Handle field sales ( CLOUD PRODUCTS )\n\n1. Primarily a hunter and hustler personality with 0-5 years of experience in SME & Enterprise Segment. Strong enterprise sales background in solutions / SaaS space ideally with knowledge of BI / analytics / Big data / AI/Azure /AWS.,,,,"['Cloud Technologies', 'Sales Engineering', 'Cloud Applications', 'Technical Sales']",2025-06-12 05:38:17
A reputed FMCG company hiring software Engineer Bangalore location,ontimesolutions,3 - 5 years,4-9 Lacs P.A.,['Bengaluru'],"Greetings\n\nWe are currently hiring software engineers for our FMCG client\n\nPermanent position\n\nQualification: Btech Computers - from Tier 1/Tier 2 Engineering colleges\n\nKnowledge/experience with web development frameworks (e.g., React, Angular, or Vue.js) and back-end technologies (e.g., Node.js, .NET, Django) is an added advantage.\nFamiliarity with database management systems (e.g., SQL, NoSQL) is important.\nKnowledge of cloud platforms (e.g., AWS, Azure, Google Cloud) & Data Engineering technology is important\nStrong problem-solving skills and the ability to work both independently and collaboratively in a team environment.\nExcellent communication skills to effectively convey technical concepts to non-technical stakeholders.\nBachelors degree in Computer Science, Engineering, or a related field, from a leading engineering college in India.\nProficiency in one or more programming languages, such as Python. • Medium to high proficiency in AI tech stack (TensorFlow, LLMs, conversational AI etc.) as well as data processing frameworks.\nExperience with AI and ML platforms such as Google AI Platform, AWS SageMaker, or Microsoft Azure AI services.\nFamiliarity with tools and libraries specifically designed for generative AI, such as GPT/Gemini/Claude etc. WHAT WE EXPECT\n\nKindly share cv to susmitha@ontimesolutions.in",Industry Type: FMCG,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Machine Learning', 'Python', 'SQL', 'Artificial Intelligence']",2025-06-12 05:38:19
AI/ML Lead Engineer (Senior AI Engineer),Conversehr Business Solutions,7 - 12 years,30-45 Lacs P.A.,['Hyderabad'],"What is AI Engineer team responsible for?\nAs a Senior AI Engineer, youll be a key member of the Data & AI team. This team is responsible for designing and delivering data engineering, analytics, and generative AI solutions that drive meaningful business impact. Were looking for a pragmatic, results-driven problem solver who thrives in a fast-paced environment and is passionate about building solutions at scale.\nThe ideal candidate has a strong technical foundation, a collaborative mindset, and the ability to navigate complex challenges. You should be comfortable working in a fast-moving, startup-like environment within an established enterprise, and should bring strong skill sets to adapt new solutions fast. You will play a crucial role in integrating AI solutions in our existing digital solutions, optimizing our data infrastructure, and enabling insights through data #MID_SENIOR_LEVEL\nWhat is a Digital & AI/ML Lead Engineer (Senior AI Engineer) responsible for?\nServe as a hands-on technical lead, driving project execution and delivery in our growing AI team based in the Hyderabad office.\nCollaborate closely with the U.S.-based team and cross-functional stakeholders to understand business needs and deliver scalable, AI-powered solutions.\nDesign and build AI applications leveraging best smart solutions.\nProvide quick prototype and evaluation AI/ML solutions aligned with business objectives.\nStay current with emerging trends in AI, and machine learning and help implement best practices within the team.\nMentor and support junior engineers, fostering a culture of learning and technical excellence.\nManage unstructured data and generate embeddings that can further be leveraged into AI products.\nWhat ideal qualifications, skills & experience would help someone to be successful?\nBachelors or master’s degree in computer science, data science, engineering, or a related field from a premium institute.\n7+ years of experience in engineering, software engineering, data science, or machine learning, including 3+ years in a technical leadership role.\nStrong understanding with data pipelines, Snowflake ecosystem and master data management.\nProficiency in Python.\nExperience working with unstructured data, large language models (LLMs), embeddings, and building generative AI prototypes.\nSelf-starter with a passion for learning new tools and technologies.\nStrong communication skills and a collaborative, ownership-driven mindset.\nWork Shift Timings - 2:00 PM - 11:00 PM IST",Industry Type: Investment Banking / Venture Capital / Private Equity,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Aiml', 'Python', 'ai/ml', 'genAI', 'Generative Ai', 'Large Language Model', 'Snowflake', 'Master Data Management', 'llm']",2025-06-12 05:38:21
ETL Engineer,Jasper Colin Research,3 - 8 years,10-20 Lacs P.A.,['Gurugram'],"Role Overview\nWe are looking for a Senior ETL Engineer with deep expertise in Apache Airflow to design, build, and manage complex data workflows and pipelines across cloud platforms. The ideal candidate will bring strong experience in Python, SQL, and cloud-native tools (AWS/GCP) to deliver scalable and reliable data infrastructure, supporting analytics, reporting, and operational systems.\nKey Responsibilities\nDesign, implement, and optimize scalable ETL/ELT workflows using Apache Airflow DAGs.\nBuild and maintain data pipelines with Python and SQL, integrating multiple data sources.\nDevelop robust solutions for pipeline orchestration, failure recovery, retries, and notifications.\nLeverage AWS or GCP services (e.g., S3, Lambda, BigQuery, Cloud Functions, IAM).\nIntegrate with internal and external data systems via secure REST APIs and Webhooks.\nMonitor Airflow performance, manage DAG scheduling, and resolve operational issues.\nImplement observability features like logging, metrics, alerts, and pipeline health checks.\nCollaborate with analytics, data science, and engineering teams to support data needs.\nDrive automation and reusability across pipeline frameworks and templates.\nEnsure data quality, governance, compliance, and lineage across ETL processes.\nRequired Skills\n5+ years of experience in ETL/Data Engineering with hands-on Airflow expertise.\nStrong programming skills in Python, with solid experience writing production scripts.\nProficiency in SQL for data manipulation, joins, and performance tuning.\nDeep knowledge of Apache Airflowscheduling, sensors, operators, XComs, and hooks.\nExperience working on cloud platforms like AWS or GCP in data-heavy environments.\nComfort with REST APIs, authentication protocols, and data integration techniques.\nKnowledge of CI/CD tools, Git, and containerization (Docker, Kubernetes).\nNice to Have\nFamiliarity with dbt, Snowflake, Redshift, BigQuery, or other modern data platforms.\nExperience with Terraform or infrastructure-as-code for Airflow deployment.\nUnderstanding of data privacy, regulatory compliance (GDPR, HIPAA), and metadata tracking.",Industry Type: Miscellaneous,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Apache Airflow', 'Data Engineering', 'ETL', 'SQL', 'Python']",2025-06-12 05:38:23
Associate Software Engineer – Biological Studies ( In Vivo ),Amgen Inc,0 - 3 years,Not Disclosed,['Hyderabad'],"In this vital role you will join a multi-functional team of scientists and software professionals that enables technology and data capabilities to evaluate drug candidates and assess their abilities to affect the biology of drug targets.\nThis team implements scientific software platforms that enable the capture, analysis, storage, and report of in vitro assays and in vivo / pre-clinical studies as well as those that manage compound inventories / biological sample banks. The ideal candidate possesses experience in the pharmaceutical or biotech industry, strong technical skills, and full stack software engineering experience (spanning SQL, back-end, front-end web technologies, automated testing).\nRoles & Responsibilities:\nDesign, develop, and implement applications and modules, including custom reports, interfaces, and enhancements\nAnalyze and understand the functional and technical requirements of applications, solutions and systems and translate them into software architecture and design specifications\nDevelop and execute unit tests, integration tests, and other testing strategies to ensure the quality of the software\nIdentify and resolve software bugs and performance issues\nWork closely with cross-functional teams, including product management, design, and QA, to deliver high-quality software on time\nMaintain documentation of software designs, code, and development processes\nCustomize modules to meet specific business requirements\nWork on integrating with other systems and platforms to ensure seamless data flow and functionality\nProvide ongoing support and maintenance for applications, ensuring that they operate smoothly and efficiently\nContribute to both front-end and back-end development using cloud technology\nDevelop innovative solution using generative AI technologies\nIdentify and resolve technical challenges effectively\nWork closely with product team, business team including scientists, and other stakeholders\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The [vital attribute] professional we seek is a [type of person] with these qualifications.\nBasic Qualifications:\nBachelors degree and 0 to 3 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nDiploma and 4 to 7 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field\nPreferred Qualifications:\nExperience in implementing and supporting biopharma scientific software platforms\nFunctional Skills:\nMust-Have Skills:\nProficient in a General Purpose High Level Language (e.g. Python, Java, C#.NET)\nProficient in a Javascript UI Framework (e.g. React, ExtJs)\nProficient in a SQL (e.g. Oracle, PostGres, Databricks)\nExperience with event-based architecture (e.g. Mulesoft, AWS EventBridge, AWS Kinesis, Kafka)\nGood-to-Have Skills:\nStrong understanding of software development methodologies, mainly Agile and Scrum\nHands-on experience with Full Stack software development\nStrong understanding of cloud platforms (e.g AWS) and containerization technologies (e.g., Docker, Kubernetes)\nWorking experience with DevOps practices and CI/CD pipelines\nExperience with big data technologies (e.g., Spark, Databricks)\nExperience with API integration, serverless, microservices architecture (e.g. Mulesoft, AWS Kafka)\nExperience with monitoring and logging tools (e.g., Prometheus, Grafana, Splunk)\nExperience of infrastructure as code (IaC) tools (Terraform, CloudFormation)\nExperience with version control systems like Git\nExperience with automated testing tools and frameworks\nExperience with Benchling\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nAWS Certified Cloud Practitioner preferred\nSoft Skills:\nExcellent problem solving, analytical, and troubleshooting skills\nStrong communication and interpersonal skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to learn quickly & work independently\nTeam-oriented, with a focus on achieving team goals\nAbility to manage multiple priorities successfully\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'C#', 'Java', 'JavaScript', 'Kafka', '.NET', 'React', 'ExtJs', 'SQL', 'Mulesoft', 'AWS EventBridge', 'AWS Kinesis']",2025-06-12 05:38:26
Senior Copy Writer- WFH,Aegis Softtech,6 - 7 years,6.5-9 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Immediate Openings for Senior Copy Writer (Permanent WFH)\n\nCopywriter - Job Description\n\nAbout Aegis :\nAegis Softtech is a global technology services firm delivering customized software solutions in AI, ML, Cloud, Data Engineering, CRM Consulting, and more. We work with tech leaders, enterprises, and fast-scaling startups to help them solve business problems with scalable, future-ready software.\n\nWe are building a lean, quality-first content team.\nIf you're a copywriter who knows how to write for humans, doesn't hide behind jargon, and loves shaping complex tech stories into simple, compelling narratives, lets work together.\n\nWhat Youll Do:\n1.Craft compelling, conversion-driven content across formats: website pages, landing pages, email copy, social posts, and ad creatives\n2. Own copy development for key service areas like AI/ML, Data & Cloud, and CRM solutions (Microsoft Dynamics, Salesforce, etc.)\n3. Translate technical inputs into benefit-focused, client-first messaging that aligns with our authoritative yet approachable voice\n4.Collaborate with the Content Lead, designers, and developers to align messaging across touchpoints.\n5. Edit and refine content for clarity, brevity, tone, and SEO, without diluting meaning.\n6.Bring consistency to brand tone across different regions and verticals.\n\nWhat Were Looking For\n> 5–8 years of proven experience as a copywriter, ideally in the B2B tech or SaaS industry.\n> Comfort working across multiple formats and content lengths—from short CTAs to full-service pages.\n> A storytelling mindset with a keen understanding of buyer psychology and content structure.\n> Strong grasp of SEO principles and how to write for both humans and search engines.\n> Ability to simplify complex tech ideas without dumbing them down.\n> Self-driven, collaborative, and comfortable with remote async work.\n\nWhy Join Us\n> Flexible remote work with a global team of tech thinkers and builders\n> A chance to influence messaging at a strategic level, not just execute briefs\n> Open and transparent communication culture\n> Opportunity to work closely with a Lead who values content quality as much as delivery speed.\n\nTo Apply:\nSend your resume, a short note about why this role speaks to you, and 2–3 samples that show your ability to:\nTranslate tech to value.\nBuild momentum with words.\nWrite with clarity and character.\nEmail your application to hr@aegissofttech.com with the subject line: Copywriter Application – [Your Name].",Industry Type: IT Services & Consulting,"Department: Content, Editorial & Journalism","Employment Type: Full Time, Permanent","['SEO Writing', 'technical content', 'copy writing', 'Content Writing', 'Content Strategy']",2025-06-12 05:38:28
Devops Engineer,NetApp,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Summary\nEngineering Tools and Services organization responsible in bringing efficiency and consistency in the way we automate, execute, triage tests and report results. We support the core ONTAP product team supporting more than 2500 engineers.\n \nAbout the Team\nSoftware Tools and Build Systems Engineer with developing and supporting software builds, Build operations or software tools in a UNIX environment. In this position you will work as part of the team developing and enhancing NetApp’s Best-in-Class build system, driving improvements in the development environment and improved development productivity by improving the tools, build architecture and processes.",,,,"['container', 'hive', 'kubernetes', 'continuous integration', 'release', 'golang', 'docker', 'sql', 'cloud', 'java', 'computer science', 'gcp', 'spark', 'devops', 'linux', 'design', 'web development', 'perl', 'hadoop', 'agile principles', 'big data', 'programming', 'hbase', 'architecture', 'python', 'microsoft azure', 'cloud api', 'mapreduce', 'agile', 'aws', 'unix']",2025-06-12 05:38:30
Databrics Engineer,Fortune India 500 IT Services Firm,5 - 10 years,Not Disclosed,[],"Detailed job description - Skill Set:\nStrong Knowledge in Databricks. This includes creating scalable ETL (Extract, Transform, Load) processes, data lakes\nStrong knowledge in Python and SQL\nStrong experience with AWS cloud platforms is a must\nGood understanding of data modeling principles and data warehousing concepts\nStrong knowledge of optimizing ETL jobs, batch processing jobs to ensure high performance and efficiency\nImplementing data quality checks, monitoring data pipelines, and ensuring data consistency and security\nHands on experience with Databricks features like Unity Catalog\nMandatory Skills\nDatabricks, AWS",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Databricks Engineer', 'AWS', 'Data Pipeline', 'Data Lake', 'ETL', 'Unity Catalog', 'Python', 'SQL']",2025-06-12 05:38:32
Database Engineer-5+ Years- Bangalore/Hyd- Immediate joiner,ThoughtFocus,5 - 10 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","**URGENT hiring**\n\nNote: This is work from office opportunity. Apply only of your okay with it\n\nMust have Skills: MySQL, PostgreSQL, NoSQL, and Redshift\nLocation: Bangalore/Hyderabad\nYears of experience: 5+ Years\nNotice period: immediate to 15 days",,,,"['Postgresql', 'Redshift Aws', 'MySQL', 'Nosql Databases', 'MongoDB', 'database engineer']",2025-06-12 05:38:35
Infrastructure Automation Engineer – ServiceNow,Amgen Inc,0 - 2 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will be responsible for developing innovative self-service solutions for our global workforce and further enhancing our self-service automation built on the ServiceNow platform.\nAs part of a scaled Agile product delivery team, the Developer works closely with product feature owners, project stakeholders, operational support teams, peer developers and testers to develop solutions to enhance self-service capabilities and solve business problems by identifying requirements, conducting feasibility analysis, proof of concepts and design sessions.\nThe Developer serves as a subject matter expert on the design, integration and operability of solutions to support innovation initiatives with business partners and shared services technology teams.\nThis role sits within the Digital, Technology and Innovation Infrastructure Automation product team and is tasked with delivering solutions that will integrate and facilitate the automation of various processes and enterprise systems. Please note, this is an onsite role based in Hyderabad.\nKey Responsibilities:\nDeliver outstanding self-service and automation experiences for our global workforce\nCreate ServiceNow catalog items, workflows, and cross-platform API integrations\nCreate and configure Business Rules, UI Policies, UI Actions, Client Scripts, REST APIs and ACLs including advanced scripting logic.\nCreate and configure Notifications, UI pages, UI Macros, Script Includes, Formatters, etc.\nCreate and maintain data integrations between ServiceNow and other systems\nDevelop system integrations and process automation\nParticipate in design review, client requirements sessions and development teams to deliver features and capabilities supporting automation initiatives\nCollaborate with product owners, stakeholders, testers and other developers to understand, estimate, prioritize and implement solutions\nDesign, code, debug, document, deploy and maintain solutions in a highly efficient and effective manner\nParticipate in problem analysis, code review, and system design\nRemain current on new technology and apply innovation to improve functionality\nCollaborate closely with stakeholders and team members to configure, improve and maintain current applications\nWork directly with users to resolve support issues within product team responsibilities\nMonitor health, performance and usage of developed solutions\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients.\nBasic Qualifications:\nMasters degree and 0 to 2 years of experience in computer science, IT, or related field OR\nBachelors degree and 2 to 5 years of experience in computer science, IT, or related field OR\nDiploma and 4 to 7 years of experience in computer science, IT, or related field\nRequired Skills & Qualifications:\n3+ years of deep hands-on experience with ServiceNow administration and development in two or more products: ITSM, ITBM, ITOM, GRC, HRSD, or Security Operations\nServiceNow development using JavaScript, AngularJS, AJAX, HTML, CSS, and Bootstrap;\nStrong understanding of user-centered design and building scalable, high-performing web and mobile interfaces on the ServiceNow platform\nExperience creating and managing Scoped Applications\nWorkflow automation and integration development using REST, SOAP, or MID servers\nScripting skills in Python, Bash, or other programming languages\nWorking in an Agile (SAFe, Scrum, and Kanban) environment\nPreferred Qualifications:\nGood-to-Have Skills:\nExperience with other configuration management tools (e.g., Puppet, Chef).\nExperience with Linux administration, scripting (Python, Bash), and CI/CD tools (GitHub Actions, CodePipeline, etc.)\nExperience with Terraform & CloudFormation for AWS infrastructure automation\nKnowledge of AWS Lambda and event-driven architectures.\nExposure to multi-cloud environments (Azure, GCP)\nExperience operating within a validated systems environment (FDA, European Agency for the Evaluation of Medicinal Products, Ministry of Health, etc.)\nProfessional Certifications (preferred):\nService Now Certified System Administrator\nService Now Certified Application Developer\nService Now Certified Technical Architect\nSoft Skills:\nStrong analytical and problem-solving skills.\nAbility to work effectively with global, virtual teams\nEffective communication and collaboration with cross-functional teams.\nAbility to work in a fast-paced environment.\nShift Information: This position is required to be onsite and participate in 24/5 and weekend on call in rotation fashion and may require you to work a later shift. Candidates must be willing and able to work off hours, as required based on business requirements.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ServiceNow', 'AngularJS', 'HTML', 'SAFe', 'ITBM', 'JavaScript', 'code review', 'Scrum', 'Python', 'CSS', 'Azure', 'UI pages', 'ITSM', 'system design', 'ITOM', 'Bash', 'SOAP', 'REST', 'UI Macros', 'problem analysis', 'GCP', 'GRC', 'HRSD', 'Script Includes', 'AJAX']",2025-06-12 05:38:38
Claims Business Process Analyst Senior,Optum,1 - 4 years,Not Disclosed,['Bengaluru'],"Optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health optimization on a global scale. Join us to start Caring. Connecting. Growing together.\n\nPrimary Responsibilities:\nReporting Development and Data Integration\nAssist with data projects related to integration with our core claims adjudication engines, eligibility, and other database items as necessary\nSupport the data leads by producing ad hoc reports as needed based on requirements from the business\nReport on key milestones to our project leads\nEnsuring all reporting aligns with brand standards\nEnsuring PADU guidelines for tools, connections, and data security\nBuild a network with internal partners to assist with validating data quality\nAnalytical Skills Utilization\nApplying analytical skills and developing business knowledge to support operations\nIdentify automation opportunities through the trends and day to day tasks to help create efficiencies within the team\nPerform root cause analysis via the 5 why root causing to identify process gaps and initiate process improvement efforts\nAssist with user testing for reports, business insights dashboards, and assist with automation validation review\nComply with the terms and conditions of the employment contract, company policies and procedures, and any and all directives (such as, but not limited to, transfer and/or re-assignment to different work locations, change in teams and/or work shifts, policies in regards to flexibility of work benefits and/or work environment, alternative work arrangements, and other decisions that may arise due to the changing business environment). The Company may adopt, vary or rescind these policies and directives in its absolute discretion and without any limitation (implied or otherwise) on its ability to do so\n\nRequired Qualifications:\nDegree or equivalent data science, analysis, mathematics experience\nExperience supporting operational teams' performance with reports and analytics\nExperience using Word (creating templates/documents), PowerPoint (creation and presentation), Teams, and SharePoint (document access/storage, sharing, List development and management)\nBasic understanding of reporting using Business Insights tools including Tableau and PowerBI\nExpertise in Excel (data entry, sorting/filtering) and VBA\nProven solid communication skills including oral, written, and organizational skills\nProven ability to manage emotions effectively in high-pressure situations, maintaining composure, and fosters a positive work environment conducive to collaboration and productivity\nPreferred Qualifications:\nExperience leveraging and creating automation such as macros, PowerAutomate, Alteryx/ETL Applications\nExperience working with cloud-based servers, knowledge of database structure, stored procedures\nExperience performing root cause analysis and demonstrated problem solving skills\nKnowledge of R/Python, SQL, DAX or other coding languages\nKnowledge of multiple lines of business, benefit structures and claims processing systems",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['power bi', 'sql', 'alteryx', 'tableau', 'vba', 'python', 'macros', 'dbms', 'sharepoint', 'process improvement', 'business process analysis', 'root cause analysis', 'dashboards', 'stored procedures', 'data quality', 'r', 'data science', 'business insights', 'dax', 'etl', 'data integration']",2025-06-12 05:38:40
ERP Senior Analyst,"NTT DATA, Inc.",2 - 5 years,Not Disclosed,['Bengaluru'],"Experience with Hyperion Planning & Oracle Hyperion Essbase; Hyperion Planning Modules (I.E. Workforce Planning, Capex Planning, Project Planning); Hyperion Financials Reporting (HFR) and data integration tools (DRM, FDMEE, Essbase load rules, etc.)\nDemonstrate strong analytical skills, problem solving/debugging skills\nExperience with software full lifecycle methodology\nAble to work in a fast-paced environment with a diverse team; work independently under minimal supervision and flexible to participate as a team member with a willingness to help junior staff where needed\nHighly Organized and detailed oriented; Adapt to projects quickly with a can-do attitude\nAble to fully utilize Microsoft Office (Word, Excel, PowerPoint)\nWork with Solution Architects and Project Managers to define the right solution for the problem at hand\nApplication of interpersonal skills to build relationships with Client; Effective communications at project team level, meeting facilitation and presentations, persuasive written communication skills\nRequires understanding of financial budgeting and forecasting processes\nUnderstanding of P&L, balance sheet and cash flow development across multiple industries\nAbility to advise clients on best practices in planning, budgeting and forecasting\nTravelThis position does not require any regular travel. Otherwise, may require work from home (internet access, phone access and private location with no interruptions).\n\n\n\nDegree: Undergraduate degree in Finance or equivalent combination of education and work experience.\n\n\n\nNice to Have\nGraduate degree in Finance preferred\nHeavy emphasis on implementation experience with Oracle Hyperion Planning & Budgeting products\nAble to demonstrate an ability to be an advisor to the client in a functional capacity; a consultant to the client and projects in a technical capacity; Recent working experience with PBCS/EPBCS implementations and migrations",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['epbcs', 'pvs', 'hyperion', 'debugging', 'hyperion planning', 'erp', 'oracle', 'accounting', 'oracle bi apps', 'budgeting', 'oracle hyperion', 'workforce planning', 'financial budgeting', 'financial reporting', 'financial planning', 'capex planning', 'drm', 'project planning', 'finance']",2025-06-12 05:38:43
ETL Test Engineer - Automation Testing,Stanra Tech Solutions,6 - 7 years,Not Disclosed,['Thiruvananthapuram'],"ETL Test Automation Engineer Trivandrum (WFO)-Immediate Joiner ONLY\n\nRole : ETL Test Automation Engineer to design, implement, and maintain test automation frameworks for real-time streaming ETL pipelines in a cloud based environment.\n\nRequired Experience : 6-7 Year Mandatory in ETL Automation Testing\n\nWorking Hours : Normal working hours (Monday to Friday, )\n\nAdditional Information :\n\nStart Date : Immediate-Immediate Joiners Only Mandatory\n\nBackground Verification : Mandatory through a third-party verification process.\n\nRequired Experience : 6-7 Yr Mandatory in ETL Automation Testing\n\nPlease Note : We are looking for Immediate Joiner Only\n\nSkills :\nStrong knowledge of ETL Testing with a focus on streaming data pipelines (Apache Flink, Kafka).\nExperience with database testing and validation (PostgreSQL, NoSQL).\nProficiency in Java for writing test automation scripts.\nHands-on experience with automation scripting for ETL workflows, data validation, and transformation checks.\nExperience in performance testing and optimizing ETL jobs to handle large-scale data streams.\nProficient in writing and executing complex database queries for data validation and reconciliation.\nExperience with CI/CD tools (Jenkins, GitHub Actions, Azure DevOps) for integrating test automation.\nFamiliarity with cloud-based ETL testing in Azure.\nAbility to troubleshoot data flow and transformation issues in real-time.\nNice to Have:\nExperience with Karate DSL, NiFi and CLI-based testing tools.\nExperience in developing self-healing mechanisms for data integrity issues.\nHands-on experience with automated data drift detection.\nKnowledge of data observability tools.\nFamiliarity with containerization (Docker, Kubernetes) and Infrastructure as Code (Terraform, Ansible) for ETL deployment.\nExperience with testing message queues (Kafka, Pulsar, RabbitMQ, etc.).\nDomain knowledge of banking and financial institutions and/or large enterprise IT environment will be considered a strong asset .",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ETL Testing', 'Datawarehouse Testing', 'Performance Testing', 'Integration Testing', 'Automation Testing', 'Test Strategy']",2025-06-12 05:38:45
Consultant - Software Engineer (with C#),Affine Analytics,5 - 8 years,Not Disclosed,['Bengaluru'],"We are looking for a highly skilled API & Pixel Tracking Integration Engineer to lead the development and deployment of server-side tracking and attribution solutions across multiple platforms. The ideal candidate brings deep expertise in CAPI integrations (Meta, Google, and other platforms), secure data handling using cryptographic techniques, and experience working within privacy-first environments like Azure Clean Rooms.\nThis role requires strong hands-on experience in C# development, Azure cloud services, OCI (Oracle Cloud Infrastructure), and marketing technology stacks including Adobe Tag Management and Pixel Management. You will work closely with engineering, analytics, and marketing teams to deliver scalable, compliant, and secure data tracking solutions that drive business insights and performance.",,,,"['C#', 'Azure Data Factory', 'Adobe Tag Management', 'ADF', 'ADLS', 'Azure Functions', 'cryptography', 'Key Vault', 'Cosmos DB']",2025-06-12 05:38:47
EPM Principal/Sr Principal Consultant FCCS/ARCS/TRCS/PBCS/EPBCS/PCMCS,Oracle,10 - 20 years,Not Disclosed,"['Pune', 'Bengaluru', 'Mumbai (All Areas)']","Hiring for Oracle Experts at different level from 5-20 years.\n\nJob locations - Bangalore, Mumbai, Pune, Hyderabad, Chennai, Kolkata, Noida, Gurgaon, Gandhinagar\n\nEPM Products - ARCS, TRCS, FCCS, EPBCS, PBCS, EPRCS, PCMCS\n\nPlanning - PBCS/ EPBCS\nExperience in Implementation of Hyperion with strong Application Development process experience on Hyperion EPM Product Suite.\nExperience in Requirement Gathering & Solution Design\nSound knowledge on Hyperion Planning/PBCS/EPBCS\nSound functional knowledge (Understand of planning modelling like P&L, BS, Workforce, Capex planning etc.. and inter dependencies)\nSound Knowledge on Business Rules/Forms / Task Lists / Reports.\nHands on Experience on Planning Modules is must.\nGood communication Skills\n\nFCCS\n\nFunction as applications design architect/Lead for Oracle FCCS\nApplication Design point of contact for FCCS Analyst Teams\nProvide Solutions to existing Architecture Design on the current system\nCollaborate effectively with other groups\nAdditional Requirements:\nEPM Experience 5+ Years\nExperience in Implementation of EPM cloud with strong Application Development process, experience on FCCS/HFM and good knowledge on consolidation process.\nExperience in Requirement Gathering & Solution Design\nDesired functional knowledge (Understand of Income statement, Balance Sheet, different methods of consolidation and their calculations and disclosure in financial statements)\nSound functional knowledge Finance/accounting/ General Ledger/Sub Ledgers\nSound Knowledge on Financial Reports and SmartView Reports\n\nARCS\nExperience implementing ARCS from design, configuration, data integration, and testing\nSound knowledge on ARM/ARCS including Reconciliation Compliance & Transaction Matching\nFunctional knowledge of Finance/accounting and account reconciliation is a must\nKnowledge and experience working with a consolidation tool and general ledger is a plus\nProvide Solutions to existing Architecture Design on current system\nCollaborate effectively with other groups\nAdditional Requirements:\nEPM Experience 5+ Years\nExperience in Implementation of Hyperion with strong Application Development process experience on Hyperion EPM Product Suite.\nExperience in Requirement Gathering & Solution Design\nSound functional knowledge Finance/accounting/ General Ledger/Sub Ledgers\nSound Knowledge on standard and custom reports\n\nTRCS\nFunction as applications design architect/Lead for Tax Reporting Cloud application development\nApplication Design point of contact for Tax Reporting Teams\nProvide Solutions to existing Architecture Design on current system\nCollaborate effectively with other groups\n\nEPM Experience 5+ Years\nExperience in Implementation of Hyperion with strong Application Development process experience on Hyperion EPM Product Suite.\nExperience in Requirement Gathering & Solution Design\nSound knowledge on Tax reporting and compliance processes, tax accounting and direct tax functions, CBCR and Deferred Tax Calculations\nSound knowledge on Hyperion Consolidation\nDesired functional knowledge (Understand of Income statement, Balance Sheet, different methods of consolidation and their calculations and disclosure in financial statements)\nSound Knowledge on Business Rules/Forms / Task Lists / Reports.\nGood communication Skills\n\nPCMCS\nFunction as applications design architect/Lead for Profitability and Cost Management\nApplication Design point of contact Profitability and Cost Management\nProvide Solutions to existing Architecture Design on current system\nAble to understand functional requirement of client and build the solution accordingly.\nCollaborate effectively with other groups\nAdditional Requirements:\nEPM Experience 7+ Years\nShould have completed at least 3 implementations on PCMCS.\nIn depth understanding of Oracle Hyperion Essbase (ASO and BSO)\nIn depth knowledge in Integration (Data Management)\nAbility to design and develop complex Reports using Web Reporting Studio.\nExperience in Implementation of Hyperion with strong Application Development process experience on Hyperion EPM Product Suite.\nExperience in Requirement Gathering & Solution Design\nAble to leverage the modern best practices for design-development and automation process wherever required.\nSound functional knowledge\nKnowledge on Microsoft office tools including Excel, Word and Power point, leverage Smartview to build report and ad hoc analysis.\nPCMCS Certification is a value add",Industry Type: Software Product,Department: Consulting,"Employment Type: Full Time, Permanent","['EPM', 'Cloud EPM', 'solution design', 'consulting', 'implementation', 'EPBCS', 'EPRCS', 'PBCS', 'TRCS', 'PCMCS', 'FCCS', 'ARCS']",2025-06-12 05:38:50
Cloud Solution Delivery Sr Advisor,"NTT DATA, Inc.",5 - 10 years,Not Disclosed,['Bengaluru'],"Req ID: 306668\n\nWe are currently seeking a Cloud Solution Delivery Sr Advisor to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\n Position Overview  We are seeking a highly skilled and experienced Lead Data Engineer to join our dynamic team. The ideal candidate will have a strong background in implementing data solutions using AWS infrastructure and a variety of core and supplementary technologies; leading teams and directing engineering workloads. This role requires a deep understanding of data engineering, cloud services, and the ability to implement high quality solutions.\n\n\n\n Key Responsibilities  \n\nLead and direct a small team of engineers engaged in\n\n- Engineer end-to-end data solutions using AWS services, including Lambda, S3, Snowflake, DBT, Apache Airflow\n\n- Cataloguing data\n\n- Collaborate with cross-functional teams to understand business requirements and translate them into technical solutions\n\n- Providing best in class documentation for downstream teams to develop, test and run data products built using our tools\n\n- Testing our tooling, and providing a framework for downstream teams to test their utilisation of our products\n\n- Helping to deliver CI, CD and IaC for both our own tooling, and as templates for downstream teams\n\n- Use DBT projects to define re-usable pipelines\n\n\n\n\n\n Required Skills and Qualifications  \n\n\n\n- Bachelor's degree in Computer Science, Engineering, or related field\n\n- 5+ years of experience in data engineering\n\n- 2+ years of experience inleading a team of data engineers\n\n- Experience in AWS cloud services\n\n- Expertise with Python and SQL\n\n- Experience of using Git / Github for source control management\n\n- Experience with Snowflake\n\n- Strong understanding of lakehouse architectures and best practices\n\n- Strong problem-solving skills and ability to think critically\n\n- Excellent communication skills to convey complex technical concepts to both technical and non-technical stakeholders\n\n- Strong use of version control and proven ability to govern a team in the best practice use of version control\n\n- Strong understanding of Agile and proven ability to govern a team in the best practice use of Agile methodologies\n\n\n\n Preferred Skills and Qualifications  \n\n- An understanding of Lakehouses\n\n- An understanding of Apache Iceberg tables\n\n- An understanding of data cataloguing\n\n- Knowledge of Apache Airflow for data orchestration\n\n- An understanding of DBT\n\n- SnowPro Core certification",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'cloud services', 'data engineering', 'sql', 'agile', 'snowflake', 'kubernetes', 'github', 'aws iam', 'version control', 'solution delivery', 'microsoft azure', 'docker', 'ansible', 'git', 'apache', 'java', 'lambda expressions', 'aws cloud', 'devops', 'linux', 'jenkins', 'aws', 'agile methodology']",2025-06-12 05:38:53
Senior ODI Developer (OCI PaaS/IaaS Expertise),Oracle,5 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru', 'Mumbai (All Areas)']","Role Overview:\nWe are seeking a highly skilled Senior ODI Developer with strong hands-on experience in SQL, PL/SQL, and Oracle Data Integrator (ODI) projects, particularly on OCI (Oracle Cloud Infrastructure) PaaS or IaaS platforms. The ideal candidate will design, implement, and optimize ETL processes, leveraging cloud-based solutions to meet evolving business needs. Prior experience in banking or insurance projects is a significant advantage.\nKey Responsibilities:\nDesign, develop, and deploy ETL processes using Oracle Data Integrator (ODI) on OCI PaaS/IaaS.\nConfigure and manage ODI instances on OCI, ensuring optimal performance and scalability.\nDevelop and optimize complex SQL and PL/SQL scripts for data extraction, transformation, and loading.\nImplement data integration solutions, connecting diverse data sources like cloud databases, on-premise systems, APIs, and flat files.\nMonitor and troubleshoot ODI jobs running on OCI to ensure seamless data flow and resolve any issues promptly.\nCollaborate with data architects and business analysts to understand integration requirements and deliver robust solutions.\nConduct performance tuning of ETL processes, SQL queries, and PL/SQL procedures.\nPrepare and maintain detailed technical documentation for developed solutions.\nAdhere to data security and compliance standards, particularly in cloud-based environments.\nProvide guidance and best practices for ODI and OCI-based data integration projects.\nSkills and Qualifications:\nMandatory Skills:\nStrong hands-on experience with Oracle Data Integrator (ODI) development and administration.\nProficiency in SQL and PL/SQL for complex data manipulation and query optimization.\nExperience deploying and managing ODI solutions on OCI PaaS/IaaS environments.\nDeep understanding of ETL processes, data warehousing concepts, and cloud data integration.\nPreferred Experience:\nHands-on experience in banking or insurance domain projects, with knowledge of domain-specific data structures.\nFamiliarity with OCI services like Autonomous Database, Object Storage, Compute, and Networking.\nExperience in integrating on-premise and cloud-based data sources.\nOther Skills:\nStrong problem-solving and debugging skills.\nExcellent communication and teamwork abilities.\nKnowledge of Agile methodologies and cloud-based DevOps practices.\nEducation and Experience:\nBachelors degree in computer science, Information Technology, or a related field.\n5 to 8 years of experience in ODI development, with at least 2 years of experience in OCI-based projects.\nDomain experience in banking or insurance is an added advantage.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Oracle Data Integrator', 'OCI', 'Data Integrator', 'Data Warehousing', 'ETL', 'SQL']",2025-06-12 05:38:56
o9 Demand Planning Implementation Senior Consultant,"NTT DATA, Inc.",6 - 11 years,Not Disclosed,['Bengaluru'],"Job Titleo9 Demand Planning Implementation Senior Consultant\nLocationBangalore\nDepartmentSupply Chain Consulting\n\n\n\nAbout the Role:\nWe are seeking a highly motivated and skilled o9 Solutions Implementation Consultant to join our dynamic team. In this role, you will play a critical part in delivering successful end-to-end implementations of o9 Solutions, focusing specifically on the Demand Planning module. You will work closely with cross-functional teams including client stakeholders, business analysts, and technical teams to ensure seamless deployment of the o9 platform.\n\n\n\nKey Responsibilities:\n""¢ Lead the functional implementation of o9's Demand Planning solution for global and regional supply chain firms.\n""¢ Work closely with clients to gather business requirements and translate them into effective o9 platform configurations.\n""¢ Drive data integration by collaborating with client""™s IT teams and o9 technical consultants (data ingestion, transformation, modeling).\n""¢ Develop and execute test scenarios to validate functional and system performance.\n""¢ Support user training and change management initiatives during deployment.\n""¢ Act as a trusted advisor, guiding clients through system adoption and continuous improvement post-implementation.\n""¢ Coordinate with o9 technical teams and client-side stakeholders to troubleshoot and resolve issues.\n\nRequired Qualifications:\n""¢ 4""“6 years of relevant experience in Supply Chain Planning or IT consulting, with at least 1""“2 full-cycle implementations of the o9 platform, specifically in Demand Planning.\n""¢ Strong understanding of demand forecasting concepts, statistical modeling, and S&OP processes.\n""¢ Hands-on experience in configuring o9 Demand Planning modules including DP foundational blocks, setting up master data and associated hierarchies, IBPL, creating active rules and procedures, setting up UIs and user access roles.\n""¢ Knowledge of SQL, Python, or integration tools (e.g. Informatica, SSIS) is a strong advantage.\n""¢ Strong analytical, problem-solving, and communication skills.\n""¢ Bachelor""™s or Master""™s degree in Engineering, Supply Chain, Operations, or related disciplines.\n\n\n\nPreferred Qualifications:\n""¢ Experience with other advanced planning systems (e.g., Kinaxis, Blue Yonder, SAP IBP) is a plus.\n""¢ Familiarity with Agile project management methodologies.\n""¢ o9 certification(s) on Technical Level 1 & 2, DP Ref Model.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['demand forecasting', 'it consulting', 'sql', 'supply chain planning', 'statistical modeling', 'python', 'logistics', 'supply planning', 'demand planning', 'supply chain management', 'procurement', 'agile', 'ssis', 'informatica', 'inventory management', 'inventory optimization']",2025-06-12 05:38:59
Jr.AI Engineer,Tekone It Services,1 - 3 years,1.5-6.5 Lacs P.A.,['Hyderabad'],"Position Overview\nWe are hiring five AI Engineers with 12 years of experience to join our dynamic team in Hyderabad. The ideal candidates will have a solid foundation in Large Language Models (LLMs), LangChain, and Generative AI (GenAI) frameworks. This is a great opportunity to work on innovative AI solutions, contributing to projects that integrate LLMs, prompt engineering, RAG pipelines, and cloud-based deployments.\nKey Responsibilities\nContribute to the design and development of AI-powered applications utilizing LLMs (GPT-3.5, GPT-4, Gemini).\nAssist in building LangChain-based pipelines and workflows, including LangSmith and LangGraph.\nSupport the implementation of Retrieval-Augmented Generation (RAG) frameworks using vector databases such as ChromaDB.\nApply prompt engineering techniques to optimize model responses and improve contextual accuracy.\nDevelop RESTful APIs using Flask or FastAPI to enable model consumption in production environments.\nWrite and manage data workflows using SQL, PySpark, and Spark SQL.\nDeploy and monitor models on Azure Machine Learning or AWS Bedrock platforms.\nCollaborate with cross-functional teams, including data scientists, engineers, and business stakeholders.\nRequired Skills\nProficiency in Python, SQL, PySpark, and Spark SQL\nHands-on experience with LLMs: GPT-3.5, GPT-4, Gemini\nKnowledge of LangChain, LangSmith, LangGraph\nFamiliarity with Vector Databases (e.g., ChromaDB) and embeddings\nExperience with prompt engineering and RAG-based architectures\nExposure to cloud platforms such as Azure ML or AWS Bedrock\nStrong understanding of REST APIs and version control systems (Git/GitHub)\nPreferred Qualifications\nBachelor's degree in Computer Science, Artificial Intelligence, Data Science, or a related field\nInternship or academic project experience in NLP, LLMs, or GenAI technologies\nFamiliarity with MLOps tools and practices (e.g., CI/CD, Airflow)\nStrong problem-solving abilities, attention to detail, and a collaborative mindset",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'Prompt Engineering', 'Artificial Intelligence', 'llm']",2025-06-12 05:39:01
Machine Learning Engineer,Draft N Craft Legal Outsourcing,2 - 3 years,10-15 Lacs P.A.,['New Delhi'],"Role Overview:\n\nAs an ML Engineer you will embark on a wonderful journey of developing and implementing various Machine Learning models that will ultimately act as an enabler in the companys growth.\nSince Draft Craft is a legal services-oriented company in which we serve clients from across the border, your work will be majorly aimed towards creating ML workflows that will serve the legal industry and help improve efficiency of the in-house teams. Draft n Craft offers you the perfect opportunity to grow and hone your ML/Data Engineering skills while contributing towards building a worthwhile product.\n\nKey Responsibilities:\nDeveloping data ingestion & data preprocessing pipelines for transforming data presented for legal requirements to extract key and actionable insights. \nData cleaning for supplying accurate, consistent & relevant content to ML models.\nExploring and experimenting with different ML models and architectures that can be used with data from the legal industry in a safe and compliant manner.\nDeveloping and deploying ML models that can function in production environments for the use-cases required by the company.\nAnalyzing key metrics for model performance and devising methods to improve efficiencies of the models.\nDocument the steps involved in data preprocessing, model development, and optimizations undertaken.\nExplore techniques for feature extraction, transformation, and selection to improve model performance.\nUnderstanding software development related terminologies to collaborate with the existing team of software engineers within the company.\nDevise methods of integration of the ML models within the company’s already developed software solutions and employee workflows.\n\nRequired Qualifications:\nDegree holder from Computer Science Engineering, Data Science or related fields.\nMinimum experience of 2 years working as an ML engineer or Data Scientist in a professional capacity.\nStrong programming skills including python and familiarity with related libraries Tensorflow, PyTorch, Pandas etc.\nDatabase Querying in terms of SQL/NoSQL.\nWorking with data extracting and ETL pipelines for pre-processing of data/documents.\nRelevant experience working in NLP, Neural Networks, and Gen AI technologies.\nFamiliarity with working or applying transfer-learning on LLM models like Llama, etc.\nSome experience in software development is preferred.\nKnowledge of deploying ML models and data pipelines to cloud services like AWS/Azure.",Industry Type: Legal,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Generative Ai', 'Retrieval Augmented Generation', 'Python', 'NoSQL', 'Large Language Model', 'Data Extraction', 'Machine Learning', 'SQL']",2025-06-12 05:39:03
AI Ml Engineer,Fulcrum Worldwide Software,4 - 8 years,15-30 Lacs P.A.,['Pune'],"The Role\nMachine learning and data science are interdisciplinary fields that require a combination of skills in mathematics, statistics, programming, and domain-specific knowledge to extract meaningful insights from data and develop predictive models.\nSkills Requirements\nMandatory Skillset - ML Techniques, any ML Framework, Gen AI COncepts, Azure, Python\nSecondary Skillset - AWS, or Google Cloud\n\nRequirements\nWork closely with cross-functional teams to understand business requirements and translate them into machine learning solutions.\nCollect, clean, and pre-process large datasets to prepare them for machine learning models.\nDevelop and implement machine learning algorithms and models for solving specific business problems.\nCollaborate with data engineers to ensure seamless integration of machine learning models into production systems.\nFine-tune models for optimal performance and conduct thorough testing and validation.\nStay updated on the latest advancements in machine learning and artificial intelligence and assess their potential impact on our projects.\nEffectively communicate complex technical concepts and findings to non-technical stakeholders.\nMonitor and maintain deployed models and update them as needed to adapt to changes in data or business requirements.\nAdhere to ethical standards and ensure that machine learning solutions comply with relevant regulations.\nProficiency in Python with hands-on experience in cloud platforms including GCP, AWS, and Azure; strong skills in API integration.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['NLP', 'Generative ai', 'Machine Learning', 'Deep Learning']",2025-06-12 05:39:06
Technical Lead - L1,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of the role is to support process delivery by ensuring daily performance of the Production Specialists, resolve technical escalations and develop technical capability within the Production Specialists.\n\n\nDo\nOversee and support process by reviewing daily transactions on performance parameters\nReview performance dashboard and the scores for the team\nSupport the team in improving performance parameters by providing technical support and process guidance\nRecord, track, and document all queries received, problem-solving steps taken and total successful and unsuccessful resolutions\nEnsure standard processes and procedures are followed to resolve all client queries\nResolve client queries as per the SLAs defined in the contract\nDevelop understanding of process/ product for the team members to facilitate better client interaction and troubleshooting\nDocument and analyze call logs to spot most occurring trends to prevent future problems\nIdentify red flags and escalate serious client issues to Team leader in cases of untimely resolution\nEnsure all product information and disclosures are given to clients before and after the call/email requests\nAvoids legal challenges by monitoring compliance with service agreements\n\n\n\nHandle technical escalations through effective diagnosis and troubleshooting of client queries\nManage and resolve technical roadblocks/ escalations as per SLA and quality requirements\nIf unable to resolve the issues, timely escalate the issues to TA & SES\nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers and clients business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract SLAs\n\n\n\nBuild people capability to ensure operational excellence and maintain superior customer service levels of the existing account/client\nMentor and guide Production Specialists on improving technical knowledge\nCollate trainings to be conducted as triage to bridge the skill gaps identified through interviews with the Production Specialist\nDevelop and conduct trainings (Triages) within products for production specialist as per target\nInform client about the triages being conducted\nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nIdentify and document most common problems and recommend appropriate resolutions to the team\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\n\n\nDeliver\n\nNoPerformance ParameterMeasure\n1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback, NSAT/ ESAT\n2Team ManagementProductivity, efficiency, absenteeism\n3Capability developmentTriages completed, Technical Test performance\nMandatory Skills: Data Engineering Full Stack.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Team Management', 'technical support', 'Technical leadership', 'troubleshooting', 'operational excellence']",2025-06-12 05:39:08
Ml Engineer,Solix Technologies,5 - 10 years,15-25 Lacs P.A.,['Hyderabad'],"Solix Technologies Inc. is a leading provider of big data applications for enterprise archiving, data privacy, and advanced analytics. We are on a mission to help organizations manage and leverage their data for maximum value, efficiency, and compliance.\nJob Summary:\nWe are seeking a highly skilled and motivated Machine Learning Engineer with hands-on experience in Natural Language Processing (NLP) and Large Language Models (LLMs). You will play a key role in designing, developing, and deploying scalable ML/NLP solutions that drive intelligent automation and data insight across our platforms.\nKey Responsibilities:\nDesign and develop machine learning models, particularly in the domain of NLP and LLMs.\nFine-tune, evaluate, and deploy transformer-based models (e.g., BERT, GPT, T5, LLaMA, etc.).\nApply techniques such as named entity recognition (NER), text classification, semantic search, summarization, and question answering.\nWork with large-scale datasets to extract insights and build data pipelines.\nCollaborate with cross-functional teams including data engineers, product managers, and software developers.\nConduct experiments, model training, and optimization to improve accuracy and performance.\nStay up-to-date with the latest research in NLP, LLMs, and machine learning.\nRequired Skills and Experience:\nBachelor's or Masters degree in Computer Science, Data Science, AI/ML, or a related field.\nMinimum 5+ years of hands-on experience with NLP and LLMs\nProficient in Python and ML frameworks like TensorFlow, PyTorch, Hugging Face Transformers.\nStrong understanding of modern NLP techniques (tokenization, embeddings, attention mechanisms, etc.).\nExperience with ML lifecycle including model development, evaluation, and deployment (MLOps).\nFamiliarity with data handling libraries (Pandas, NumPy) and cloud platforms (AWS, GCP, or Azure).\nGood understanding of data preprocessing, feature engineering, and model validation techniques.\nExperience with open-source LLM fine-tuning and deployment.\nKnowledge of vector databases (e.g., FAISS, Pinecone) and retrieval-augmented generation (RAG).\nPrior experience with large-scale data systems or enterprise data environments.\nPublished papers or open-source contributions in the ML/NLP space.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['LLm', 'Large Language Model', 'Natural Language Processing', 'Ml Algorithms']",2025-06-12 05:39:10
Staff Quality Engineer,Fanatics,9 - 14 years,40-50 Lacs P.A.,['Hyderabad'],"As a Staff Quality Engineer at Fanatics, you will specialize in building automated testing including test suites, reporting capabilities and continuous integration to ensure the accuracy and consistency of the systems driving our inventory lifecycle, planning and product performance. Youll be part of a multi-disciplinary Quality Engineering team while also working with our data engineers to ensure we have tools and processes that warrant the correctness of the functionality and data and quickly identify any problems. The teams within our inventory organization own the source of truth for all inventory position related info, develop the systems that handle the inventory lifecycle of our products and the integrations that keep our partners updated with an accurate picture of what we can sell to our fans and what they demand the most. Our teams also work on models to drive efficiency of our inventory management. Multiple teams across Fanatics Commerce, including Fulfillment, Merchandising and Planning, Finance, Business Intelligence and Analytics rely on these datasets and several KPIs dashboards used by our Senior Leadership aid data-driven business decisions. The team takes pride in ensuring that these datasets and the processes that generate the data are accurate, reliable and available on time by building and operating near real-time & batch data pipelines, exploring emerging technologies in this area, and helping other teams consume and realize the value of the data. \nIf you are an experienced Quality Engineer with a passion for delivering quality products and are curious and eager to keep learning and discovering new tools to improve the efficiency of our quality cycle this opportunity is for you. \n\nResponsibilities:\nNurture a culture of quality through collaboration with teammates across the engineering function to make sure quality is embedded in both processes and technology. \nDevelop quality focused test strategies that help the team deliver software that provides maximum quality without sacrificing business value. \nDefine quality metrics and build quality monitoring solutions and dashboards. \nMentor/coach team members to ensure appropriate testing coverage within the team with a focus on continuous testing and a shift-left approach. \nContribute to process improvements and refinement of Quality Engineering practices across the QE function. \nRecommend best test practices and evangelize a testing early culture to empower the team to make sound engineering decisions. \nWork with multiple teams of engineers and product managers to gather requirements and data sources to design and build test plans. \nParticipate in projects developed with agile methodology. \nDesign and document test cases and test data to ensure proper coverage. \nPerform exploratory/manual tests as needed to ensure quality of the data across all data platform components. \nCollaborates with data partners to triage issues and works to ensure the validity, timeliness, consistency, completeness and accuracy of our data across all data platform components. \nWrite, execute, and monitor automated test suites for integration and regression testing. \nIntegrate tests as part of continuous delivery pipelines. \nResearch new tools that can improve our quality process and implement them. \n\nA suitable candidate would have:\nMinimum 8 years of testing experience certifying the quality of with applications developed in languages like Python, Golang, Java, C#. \nExperience in a programming language like Python, Java, Golang, Node,js using it for automation and data validation. \nMust understand databases and ORMs, experienced with at least one RDBMS and DB Query language. \nAbility to define technical standards and guidelines for test framework development. \nExperience on modern Quality Engineering principles such as Continuous Testing and Shift Left. \nClear understanding of different testing principles like data driven testing, behavior driven development, etc. \nSolid experience in writing clear, concise, and comprehensive test plans and test cases. \nExperience in building automated test suites for API's REST and gRPC with focus on data validation. \nExtensive experience with OpenAPI Specifications and tools like Swagger for designing, documenting, and validating APIs. \nProficiency in using schema validation tools for ensuring compliance with API contracts. \nStrong understanding of RESTful principles, HTTP protocols, and JSON schema validation. \nProficiency in managing test data and mocking/stubbing for complex test scenarios. \nExperience testing applications using producer/consumer models and event streaming like Kafka, RabbitMQ, Pulsar is a plus. \nProficiency in testing tools such as Postman, K6 and JMeter for API performance and functional testing. \nHands-on experience with test automation tools like Playwright, Selenium, and other modern test frameworks. \nGood understanding of service oriented and microservices architecture. \nExperience with cloud environments like AWS, GCP, source control tools like Github and continuous integration and delivery software, preferably, Gitlab. \nExperience with AWS services like S3, SQS, RDS, Elasticache, EMR \nProven track record of mentoring team members on automation best practices, framework design, and testing methodologies \nAbility to lead technical discussions and drive decisions on test automation strategies and implementation. \nStrong debugging, root cause analysis, and problem-solving skills. \nAbility to troubleshoot issues in distributed systems, APIs, and automation tools. \nGreat communication skills \nExperience on inventory management and fulfillment processes and workflow orchestration tools like Maestro, Temporal, etc. are a plus.",Industry Type: Internet (E-Commerce),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Playwright', 'Data Testing', 'Automation Testing', 'Selenium', 'Python', 'java', 'API Testing', 'Functional Testing', 'SQL']",2025-06-12 05:39:13
AIML Engineer,Throughbit Technologies,1 - 6 years,2-7 Lacs P.A.,['Coimbatore'],"Education:\nBachelor's degree in Computer Science, Data Science, Artificial Intelligence, or a related certifications or experience in NLP & CV.\n\nYears of Experience:\nMinimum 2 years of experience in Deep Learning, NLP,CV, MLOps and its related technologies.\n\nResponsibilities:\n- Design, develop, and deploy state-of-the-art NLP and CV models and algorithm\n- Collaborate with cross-functional teams to understand requirements and develop customised NLP & CV solutions and have experience in building the python backend using Flask / Django.\n- Database integration preferably using MongoDB or any other vector databases.\n- Maintain and improve the performance, accuracy, and efficiency of existing AI/ML models and their deployment on Cloud platforms (AWS) and monitor their performance using MLOps tools such as MLFlow, DVC.\n- Experience in building End to End data pipelines\n- Stay updated with emerging AI/ML technologies, LLMs ,RAG\n- Conduct regular performance evaluations of AI/ML models using production grade MLOps solutions.\n- Troubleshoot and resolve any issues arising from the implementation of NLP & CV models.\n- Develop and monitor the code that runs in production environments using MLOps practices.\n\nRequirements:\n- Strong experience with Deep learning and NLP frameworks such as TensorFlow or other open-source machine learning frameworks.\n- Experience of using both tensorflow and pytorch frameworks\n- Proficient in programming languages, such as Python or Java, and experience with AI/ML libraries.\n- Familiarity with the integration of APIs, such as REST API, OpenAI API, for implementing advanced AI-driven features.\n- Solid understanding of Machine learning and Deep learning algorithms, concepts, and best practices in a production environment using MLOps.\n- Experience with big data technologies, such as Hadoop and Spark, is a plus.\n- Strong problem-solving skills.\n- Excellent communication and teamwork skills, with the ability to collaborate effectively with team members from various disciplines.\n- Eagerness to learn and adapt to new technologies and industry trends.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Natural Language Processing', 'Py', 'Deep Learning', 'Java', 'Deep', 'Python']",2025-06-12 05:39:15
Software Engineer-8910,WebMD,3 - 8 years,Not Disclosed,['Navi Mumbai'],"Position: Software Engineer\npositions: 1\nNo.of\nAbout Company:\nHeadquartered in El Segundo, Calif., Internet Brands is a fully integrated online media and\nsoftware services organization focused on four high-value vertical categories: Health,\nAutomotive, Legal, and Home/Travel. The company's award-winning consumer websites\nlead their categories and serve more than 250 million monthly visitors, while a full range of\nweb presence offerings has established deep, long-term relationships with SMB and\nenterprise clients. Internet Brands' powerful, proprietary operating platform provides the\nflexibility and scalability to fuel the company's continued growth. Internet Brands is a portfolio\ncompany of KKR and Temasek.\nWebMD Health Corp., an Internet Brands Company, is the leading provider of health\ninformation services, serving patients, physicians, health care professionals, employers, and\nhealth plans through our public and private online portals, mobile platforms, and health-\nfocused publications. The WebMD Health Network includes WebMD Health, Medscape,\nJobson Healthcare Information, prIME Oncology, MediQuality, Frontline, QxMD, Vitals\nConsumer Services, MedicineNet, eMedicineHealth, RxList, OnHealth, Medscape\nEducation, and other owned WebMD sites. WebMD, Medscape, CME Circle,\nMedpulse®, eMedicine®, MedicineNet®, theheart.org®, and RxList® are among the\ntrademarks of WebMD Health Corp. or its subsidiaries.\nAll qualified applicants will receive consideration for employment without regard to race,\ncolor, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran\nstatus.\nFor Company details, visit our website: www.webmd.com and www.internetbrands.com\nAll qualified applicants will receive consideration for employment without regard to\nrace, color, religion, sex, sexual orientation, gender identity, national origin, disability,\nor veteran status.\nFor Company details, visit our website: www.webmd.com\nhttps://www.webmd.com/corporate/physicians-interactive",,,,"['ETL', 'SQL', 'Data Warehousing']",2025-06-12 05:39:17
BI Engineer,Amgen Inc,2 - 5 years,Not Disclosed,['Hyderabad'],"Role Description:\nLets do this. We are seeking an experienced Senior BI Engineer to lead the design, development, and optimization of scalable business intelligence (BI) solutions that empower data-driven decision-making across the organization. The ideal candidate is highly skilled in data modeling, dashboard development, ETL design, and cloud-based BI platforms, with a passion for turning complex data into clear, actionable insights. As a senior member of the BI team, you will work closely with data engineers, analysts, business stakeholders, and product teams to deliver robust, user-friendly analytics solutions that support strategic and operational goals.\nRoles & Responsibilities:\nDesign, develop, and maintain enterprise-grade BI dashboards and reports using tools like Power BI, Tableau, or Looker.\nBuild and optimize semantic models, tabular data structures, and reusable datasets for self-service BI users.\nPartner with business stakeholders to translate requirements into technical solutions, delivering accurate, relevant, and timely insights.\nWork closely with data engineering teams to integrate BI solutions into data lake, warehouse, or lakehouse architectures (e.g., Snowflake, Redshift, Databricks, BigQuery).\nImplement best practices for BI development, including version control, performance optimization, and data governance.\nEnsure BI solutions are secure, scalable, and aligned with enterprise data governance standards.\nMentor junior BI developers and analysts, setting standards for dashboard usability, data visualization, and design consistency.\nCollaborate with cross-functional teams to promote self-service BI adoption and data literacy throughout the organization.\nMonitor BI performance, usage, and adoption, providing continuous improvements and training to enhance impact.\nMust-Have Skills:\n5 - 8 years of experience in BI development and data visualization, with deep expertise in tools such as Power BI, Tableau, or Looker.\nStrong knowledge of SQL, data modeling techniques, and BI architecture best practices.\nExperience working with data warehouses and cloud data platforms\nProficiency in building dashboards, KPIs, and executive-level reporting that align with business priorities.\nSolid understanding of ETL/ELT processes, data pipelines, and integration with BI tools.\nStrong collaboration skills with the ability to work effectively across engineering, product, finance, and business teams.\nExcellent communication skills, with a proven ability to translate technical concepts into business value.\nGood-to-Have Skills:\nExperience in cloud platforms (AWS, Azure, or GCP) and modern data stack environments.\nFamiliarity with data governance, data cataloging, metadata management, and access control.\nExposure to Agile methodologies, CI/CD for BI, and DevOps practices.\nBI or data certifications (e.g., Microsoft Certified: Power BI Data Analyst, Tableau Certified Professional).\nEducation and Professional Certifications\nMasters degree and 3 to 4 + years of Computer Science, IT or related field experience\nOR\nBachelors degree and 5 to 8 + years of Computer Science, IT or related field experience\nPowerBI / Tableau certifications preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['BI development', 'Azure', 'Power BI', 'BI tools', 'Agile methodologies', 'data modeling techniques', 'Tableau', 'SQL', 'BI architecture', 'GCP', 'CI/CD', 'cloud data platforms', 'data visualization', 'data warehouses', 'AWS']",2025-06-12 05:39:19
Database Engineer,TechStar Group,9 - 12 years,25-35 Lacs P.A.,['Hyderabad'],"Experience 12yrs only\nNotice period :- immediate / 15days\nLocation :- Hyderabad\nClient :- Tech Star Group\nPlease highlight the mandatory sill in resume .\nClient Feedback :-\nIn short, the client is primarily looking for a candidate with strong expertise in data-related skills, including:\n\nSQL & Database Management: Deep knowledge of relational databases (PostgreSQL), cloud-hosted data platforms (AWS, Azure, GCP), and data warehouses like Snowflake.\nETL/ELT Tools: Experience with SnapLogic, StreamSets, or DBT for building and maintaining data pipelines. / ETL Tools Extensive Experience on data Pipelines\nData Modeling & Optimization: Strong understanding of data modeling, OLAP systems, query optimization, and performance tuning.\nCloud & Security: Familiarity with cloud platforms and SQL security techniques (e.g., data encryption, TDE).\nData Warehousing: Experience managing large datasets, data marts, and optimizing databases for performance.\nAgile & CI/CD: Knowledge of Agile methodologies and CI/CD automation tools.\n\nImp :-The candidate should have a strong data engineering background with hands-on experience in handling large volumes of data, data pipelines, and cloud-based data systems",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Data Warehousing', 'ETL', 'Elt', 'SQL', 'Azure', 'Data Pipeline', 'GCP', 'Postgresql', 'AWS', 'Python']",2025-06-12 05:39:22
Engineering Manager,Amgen Inc,10 - 14 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are seeking a seasoned Engineering Manager (Data Engineering) to lead the end-to-end management of enterprise data assets and operational data workflows. This role is critical in ensuring the availability, quality, consistency, and timeliness of data across platforms and functions, supporting analytics, reporting, compliance, and digital transformation initiatives. You will be responsible for the day-to-day data operations, manage a team of data professionals, and drive process excellence in data intake, transformation, validation, and delivery. You will work closely with cross-functional teams including data engineering, analytics, IT, governance, and business stakeholders to align operational data capabilities with enterprise needs.\nRoles & Responsibilities:\nLead and manage the enterprise data operations team, responsible for data ingestion, processing, validation, quality control, and publishing to various downstream systems.\nDefine and implement standard operating procedures for data lifecycle management, ensuring accuracy, completeness, and integrity of critical data assets.\nOversee and continuously improve daily operational workflows, including scheduling, monitoring, and troubleshooting data jobs across cloud and on-premise environments.\nEstablish and track key data operations metrics (SLAs, throughput, latency, data quality, incident resolution) and drive continuous improvements.\nPartner with data engineering and platform teams to optimize pipelines, support new data integrations, and ensure scalability and resilience of operational data flows.\nCollaborate with data governance, compliance, and security teams to maintain regulatory compliance, data privacy, and access controls.\nServe as the primary escalation point for data incidents and outages, ensuring rapid response and root cause analysis.\nBuild strong relationships with business and analytics teams to understand data consumption patterns, prioritize operational needs, and align with business objectives.\nDrive adoption of best practices for documentation, metadata, lineage, and change management across data operations processes.\nMentor and develop a high-performing team of data operations analysts and leads.\nFunctional Skills:\nMust-Have Skills:\nExperience managing a team of data engineers in biotech/pharma domain companies.\nExperience in designing and maintaining data pipelines and analytics solutions that extract, transform, and load data from multiple source systems.\nDemonstrated hands-on experience with cloud platforms (AWS) and the ability to architect cost-effective and scalable data solutions.\nExperience managing data workflows in cloud environments such as AWS, Azure, or GCP.\nStrong problem-solving skills with the ability to analyze complex data flow issues and implement sustainable solutions.\nWorking knowledge of SQL, Python, or scripting languages for process monitoring and automation.\nExperience collaborating with data engineering, analytics, IT operations, and business teams in a matrixed organization.\nFamiliarity with data governance, metadata management, access control, and regulatory requirements (e.g., GDPR, HIPAA, SOX).\nExcellent leadership, communication, and stakeholder engagement skills.\nWell versed with full stack development & DataOps automation, logging frameworks, and pipeline orchestration tools.\nStrong analytical and problem-solving skills to address complex data challenges.\nEffective communication and interpersonal skills to collaborate with cross-functional teams.\nGood-to-Have Skills:\nData Engineering Management experience in Biotech/Life Sciences/Pharma\nExperience using graph databases such as Stardog or Marklogic or Neo4J or Allegrograph, etc.\nEducation and Professional Certifications\n9 to 12 years of experience in Computer Science, IT or related field\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data operations', 'fullstack development', 'GCP', 'stakeholder engagement', 'troubleshooting', 'cloud platforms', 'AWS']",2025-06-12 05:39:24
Principal Machine Learning Engineer,Amgen Inc,2 - 7 years,Not Disclosed,['Hyderabad'],"What you will do\nWe are seeking a highly skilled Machine Learning Engineer with a strong MLOps background to join our team. You will play a pivotal role in building and scaling our machine learning models from development to production. Your expertise in both machine learning and operations will be essential in creating efficient and reliable ML pipelines.\nRoles & Responsibilities:\nCollaborate with data scientists to develop, train, and evaluate machine learning models.\nBuild and maintain MLOps pipelines, including data ingestion, feature engineering, model training, deployment, and monitoring.\nLeverage cloud platforms (AWS, GCP, Azure) for ML model development, training, and deployment.\nImplement DevOps/MLOps best practices to automate ML workflows and improve efficiency.\nDevelop and implement monitoring systems to track model performance and identify issues.\nConduct A/B testing and experimentation to optimize model performance.\nWork closely with data scientists, engineers, and product teams to deliver ML solutions.\nGuide and mentor junior engineers in the team\nStay updated with the latest trends and advancements\n\nBasic Qualifications:\nDoctorate degree and 2 years of Computer Science, Statistics, and Data Science, Machine Learning experience OR\nMasters degree and 8 to 10 years of Computer Science, Statistics, and Data Science, Machine Learning experience OR\nBachelors degree and 10 to 14 years of Computer Science, Statistics, and Data Science, Machine Learning experience OR\nDiploma and 14 to 18 years of years of Computer Science, Statistics, and Data Science, Machine Learning experience\nPreferred Qualifications:\nMust-Have Skills:\nStrong foundation in machine learning algorithms and techniques\nExperience in MLOps practices and tools (e.g., MLflow, Kubeflow, Airflow); Experience in DevOps tools (e.g., Docker, Kubernetes, CI/CD)\nProficiency in Python and relevant ML libraries (e.g., TensorFlow, PyTorch, Scikit-learn)\nOutstanding analytical and problem-solving skills; Ability to learn quickly; Excellent communication and interpersonal skills\nGood-to-Have Skills:\nExperience with big data technologies (e.g., Spark), and performance tuning in query and data processing\nExperience with data engineering and pipeline development\nExperience in statistical techniques and hypothesis testing, experience with regression analysis, clustering and classification\nKnowledge of NLP techniques for text analysis and sentiment analysis\nExperience in analyzing time-series data for forecasting and trend analysis\nFamiliar with AWS, Azure, or Google Cloud;\nFamiliar with Databricks platform for data analytics and MLOps\nProfessional Certifications\nCloud Computing and Databricks certificate preferred\nSoft Skills:\nExcellent analytical and fixing skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Azure', 'NLP', 'MLOps', 'Databricks', 'AWS', 'Google Cloud']",2025-06-12 05:39:27
Python Django Microservices Lead,"NTT DATA, Inc.",8 - 13 years,Not Disclosed,['Bengaluru'],"Req ID: 327855\n\nWe are currently seeking a Python Django Microservices Lead to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\n""Job DutiesResponsibilities:\nLead the development of backend systems using Django.\nDesign and implement scalable and secure APIs.\nIntegrate Azure Cloud services for application deployment and management.\nUtilize Azure Databricks for big data processing and analytics.\nImplement data processing pipelines using PySpark.\nCollaborate with front-end developers, product managers, and other stakeholders to deliver comprehensive solutions.\nConduct code reviews and ensure adherence to best practices.\nMentor and guide junior developers.\nOptimize database performance and manage data storage solutions.\nEnsure high performance and security standards for applications.\nParticipate in architecture design and technical decision-making.\n\nMinimum Skills RequiredQualifications:\nBachelor's degree in Computer Science, Information Technology, or a related field.\n8+ years of experience in backend development.\n8+ years of experience with Django.\nProven experience with Azure Cloud services.\nExperience with Azure Databricks and PySpark.\nStrong understanding of RESTful APIs and web services.\nExcellent communication and problem-solving skills.\nFamiliarity with Agile methodologies.\nExperience with database management (SQL and NoSQL).\n\nSkills:\nDjango, Python, Azure Cloud, Azure Databricks, Delta Lake and Delta tables, PySpark, SQL/NoSQL databases, RESTful APIs, Git, and Agile methodologies""",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure databricks', 'rest', 'python', 'pyspark', 'django', 'azure cloud services', 'web services', 'data processing', 'backend development', 'azure cloud', 'sql', 'nosql', 'microservices', 'git', 'code review', 'agile', 'big data', 'application deployment', 'agile methodology', 'nosql databases']",2025-06-12 05:39:30
Etl Engineer,Global Energy,4 - 9 years,Not Disclosed,[],"Role & responsibilities\n\nWe are looking for 5 years of experience in ETL\n\nWork mode :Remote\n\nMandatory skills : CData, ETl\n\nNeed a dedicated ETL engineer to manage all Extract, Transform, Load (ETL) processes.\nStrong expertise in ETL workflows is required.\nMust have hands-on experience with C-Data, as it is the primary tool used by the client.\nPreferably someone with advanced or in-depth experience in C-Data, not just basic knowledge.\nSome exposure or understanding of Salesforce is expected, since it is the client's main system.\nFamiliarity with complex healthcare data is preferred, as the data can be intricate and challenging to work with.\nData handling with C-Data drivers & knowledge should be there",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Cdata sync', 'ETL', 'Salesforce Integration', 'Etl Process', 'Informatica']",2025-06-12 05:39:32
Lead Power BI Developer,Conduent,8 - 13 years,Not Disclosed,['Bengaluru'],"Job Overview: \nWe are looking for a BI & Visualization Developer who will be part of our Analytics Practice and will be expected to actively work in a multi-disciplinary fast paced environment. This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project; its primary responsibility is to support the design, development and maintainance of business intelligence and analytics solutions.\n\n\n Responsibilities: \nDevelop reports, dashboards, and advanced visualizations. Works closely with the product managers, business analysts, clients etc. to understand the needs / requirements and develop visualizations needed.\nProvide support to new of existing applications while recommending best practices and leading projects to implement new functionality.\nLearn and develop new visualization techniques as required to keep up with the contemporary visualization design and presentation.\nReviews the solution requirements and architecture to ensure selection of appropriate technology, efficient use of resources and integration of multiple systems and technology.\nCollaborate in design reviews and code reviews to ensure standards are met. Recommend new standards for visualizations.\nBuild and reuse template/components/web services across multiple dashboards\nSupport presentations to Customers and Partners\nAdvising on new technology trends and possible adoption to maintain competitive advantage\nMentoring Associates\n\n\n Experience Needed: \n8+ years of related experience is required.\nA Bachelor degree or Masters degree in Computer Science or related technical discipline is required\nHighly skilled in data visualization tools like PowerBI, Tableau, Qlikview etc.\nVery Good Understanding of PowerBI Tabular Model/Azure Analysis Services using large datasets.\nStrong SQL coding experience with performance optimization experience for data queries.\nUnderstands different data models like normalized, de-normalied, stars, and snowflake models.\nWorked in big data environments, cloud data stores, different RDBMS and OLAP solutions.\nExperience in design, development, and deployment of BI systems.\nCandidates with ETL experience preferred.\nIs familiar with the principles and practices involved in development and maintenance of software solutions and architectures and in service delivery.\nHas strong technical background and remains evergreen with technology and industry developments.\nAdditional\nDemonstrated ability to have successfully completed multiple, complex technical projects\nPrior experience with application delivery using an Onshore/Offshore model\nExperience with business processes across multiple Master data domains in a services based company\nDemonstrates a rational and organized approach to the tasks undertaken and an awareness of the need to achieve quality.\nDemonstrates high standards of professional behavior in dealings with clients, colleagues and staff.\nStrong written communication skills. Is effective and persuasive in both written and oral communication.\nExperience with gathering end user requirements and writing technical documentation\nTime management and multitasking skills to effectively meet deadlines under time-to-market pressure\nMay require occasional travel",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql coding', 'azure analysis services', 'sql', 'tableau', 'tabular model', 'snowflake', 'rdbms', 'web services', 'bi', 'power bi', 'business analysis', 'business intelligence', 'dashboards', 'qlikview', 'report development', 'azure analysis', 'olap', 'code review', 'data visualization', 'etl', 'big data']",2025-06-12 05:39:34
SRE - Site Reliability Engineer,TechBlocks,6 - 11 years,20-25 Lacs P.A.,"['Hyderabad', 'Ahmedabad']","Hi Aspirant,\n\nGreetings from TechBlocks - IT Software of Global Digital Product Development - Hyderabad !!!\n\nAbout us : TechBlocks is a global digital product engineering company with 16+ years of experience helping Fortune 500 enterprises and high-growth brands accelerate innovation, modernize technology, and drive digital transformation. From cloud solutions and data engineering to experience design and platform modernization, we help businesses solve complex challenges and unlock new growth opportunities. \n\n\nJob Title:  Senior DevOps Site Reliability Engineer (SRE)\n\nLocation: Hyderabad & Ahmedabad \n\nEmployment Type: Full-Time \n\nWork Model - 3 Days from office\n\nJob Overview \nDynamic, motivated individuals deliver exceptional solutions for the production resiliency of the systems. The role incorporates aspects of software engineering and operations, DevOps skills to come up with efficient ways of managing and operating applications. The role will require a high level of responsibility and accountability to deliver technical solutions.\n\nSummary:\nAs a Senior SRE, you will ensure platform reliability, incident management, and performance optimization. You'll define SLIs/SLOs, contribute to robust observability practices, and drive proactive reliability engineering across services.\n\nExperience Required:\n610 years of SRE or infrastructure engineering experience in cloud-native environments.\n\nMandatory:\nCloud: GCP (GKE, Load Balancing, VPN, IAM)\nObservability: Prometheus, Grafana, ELK, Datadog\nContainers & Orchestration: Kubernetes, Docker\nIncident Management: On-call, RCA, SLIs/SLOs\nIaC: Terraform, Helm\nIncident Tools: PagerDuty, OpsGenie\n\nNice to Have:\nGCP Monitoring, Skywalking\nService Mesh, API Gateway\nGCP Spanner,\n\nScope:\nDrive operational excellence and platform resilience\nReduce MTTR, increase service availability\nOwn incident and RCA processes\n\nRoles and Responsibilities:\n\nDefine and measure Service Level Indicators (SLIs), Service Level Objectives (SLOs), and manage error budgets across services.\nLead incident management for critical production issues drive Root Cause Analysis (RCA) and postmortems.\nCreate and maintain runbooks and standard operating procedures for high availability services.\nDesign and implement observability frameworks using ELK, Prometheus, and Grafana; drive telemetry adoption.\nCoordinate cross-functional war-room sessions during major incidents and maintain response logs.\nDevelop and improve automated System Recovery, Alert Suppression, and Escalation logic.\nUse GCP tools like GKE, Cloud Monitoring, and Cloud Armor to improve performance and security posture.\nCollaborate with DevOps and Infrastructure teams to build highly available and scalable systems.\nAnalyze performance metrics and conduct regular reliability reviews with engineering leads.\nParticipate in capacity planning, failover testing, and resilience architecture reviews.\n\nIf you are interested , then please share me your updated resume to kranthikt@tblocks.com\n\n\nWarm Regards,\n\nKranthi Kumar\nkranthikt@tblocks.com\nContact: 8522804902\nSenior Talent Acquisition Specialist\nToronto | Ahmedabad | Hyderabad | Pune\nwww.tblocks.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SRE', 'Gcp Cloud', 'Monitoring Tools', 'Site Reliability Engineering', 'Log Management', 'Kibana', 'Cloud Monitoring', 'Prometheus', 'Logstash', 'Datadog', 'Grafana', 'Elk Cluster', 'Docker', 'Dynatrace', 'Alerts', 'Kubernates', 'Appdynamics']",2025-06-12 05:39:37
Lead Pyspark Developer,Synechron,5 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","job requisition idJR1027452\n\n\n\nOverall Responsibilities:\nData Pipeline Development:Design, develop, and maintain highly scalable and optimized ETL pipelines using PySpark on the Cloudera Data Platform, ensuring data integrity and accuracy.\nData Ingestion:Implement and manage data ingestion processes from a variety of sources (e.g., relational databases, APIs, file systems) to the data lake or data warehouse on CDP.\nData Transformation and Processing:Use PySpark to process, cleanse, and transform large datasets into meaningful formats that support analytical needs and business requirements.\nPerformance Optimization:Conduct performance tuning of PySpark code and Cloudera components, optimizing resource utilization and reducing runtime of ETL processes.\nData Quality and Validation:Implement data quality checks, monitoring, and validation routines to ensure data accuracy and reliability throughout the pipeline.\nAutomation and Orchestration:Automate data workflows using tools like Apache Oozie, Airflow, or similar orchestration tools within the Cloudera ecosystem.\nMonitoring and Maintenance:Monitor pipeline performance, troubleshoot issues, and perform routine maintenance on the Cloudera Data Platform and associated data processes.\nCollaboration:Work closely with other data engineers, analysts, product managers, and other stakeholders to understand data requirements and support various data-driven initiatives.\nDocumentation:Maintain thorough documentation of data engineering processes, code, and pipeline configurations.\n\n\n\nSoftware :\nAdvanced proficiency in PySpark, including working with RDDs, DataFrames, and optimization techniques.\nStrong experience with Cloudera Data Platform (CDP) components, including Cloudera Manager, Hive, Impala, HDFS, and HBase.\nKnowledge of data warehousing concepts, ETL best practices, and experience with SQL-based tools (e.g., Hive, Impala).\nFamiliarity with Hadoop, Kafka, and other distributed computing tools.\nExperience with Apache Oozie, Airflow, or similar orchestration frameworks.\nStrong scripting skills in Linux.\n\n\n\nCategory-wise Technical\n\nSkills:\nPySpark:Advanced proficiency in PySpark, including working with RDDs, DataFrames, and optimization techniques.\nCloudera Data Platform:Strong experience with Cloudera Data Platform (CDP) components, including Cloudera Manager, Hive, Impala, HDFS, and HBase.\nData Warehousing:Knowledge of data warehousing concepts, ETL best practices, and experience with SQL-based tools (e.g., Hive, Impala).\nBig Data Technologies:Familiarity with Hadoop, Kafka, and other distributed computing tools.\nOrchestration and Scheduling:Experience with Apache Oozie, Airflow, or similar orchestration frameworks.\nScripting and Automation:Strong scripting skills in Linux.\n\n\n\nExperience:\n5-12 years of experience as a Data Engineer, with a strong focus on PySpark and the Cloudera Data Platform.\nProven track record of implementing data engineering best practices.\nExperience in data ingestion, transformation, and optimization on the Cloudera Data Platform.\n\n\n\nDay-to-Day Activities:\nDesign, develop, and maintain ETL pipelines using PySpark on CDP.\nImplement and manage data ingestion processes from various sources.\nProcess, cleanse, and transform large datasets using PySpark.\nConduct performance tuning and optimization of ETL processes.\nImplement data quality checks and validation routines.\nAutomate data workflows using orchestration tools.\nMonitor pipeline performance and troubleshoot issues.\nCollaborate with team members to understand data requirements.\nMaintain documentation of data engineering processes and configurations.\n\n\n\nQualifications:\nBachelors or Masters degree in Computer Science, Data Engineering, Information Systems, or a related field.\nRelevant certifications in PySpark and Cloudera technologies are a plus.\n\n\n\nSoft\n\nSkills:\nStrong analytical and problem-solving skills.\nExcellent verbal and written communication abilities.\nAbility to work independently and collaboratively in a team environment.\nAttention to detail and commitment to data quality.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cloudera', 'hive', 'pyspark', 'linux', 'hadoop', 'scala', 'amazon redshift', 'data warehousing', 'emr', 'sql', 'docker', 'apache', 'java', 'spark', 'gcp', 'etl', 'big data', 'hbase', 'data lake', 'python', 'oozie', 'airflow', 'microsoft azure', 'impala', 'data engineering', 'nosql', 'amazon ec2', 'mapreduce', 'kafka', 'sqoop', 'aws']",2025-06-12 05:39:39
Specialist Software Engineer - Large Molecule Discovery,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will.\nRole Description:\nThe role is responsible for designing, developing, and maintaining software solutions for Research scientists. Additionally, it involves automating operations, monitoring system health, and responding to incidents to minimize downtime.\nYou will join a multi-functional team of scientists and software professionals that enables technology and data capabilities to evaluate drug candidates and assess their abilities to affect the biology of drug targets. This team implements scientific software platforms that enable the capture, analysis, storage, and reporting for our Large Molecule Discovery Research team (Design, Make, Test and Analyze processes).\nThe team also interfaces heavily with teams supporting our in vitro assay management systems and our compound inventory platforms. The ideal candidate possesses experience in the pharmaceutical or biotech industry, strong technical skills, and full stack software engineering experience (spanning SQL, back-end, front-end web technologies, automated testing).\nRoles & Responsibilities:\nTake ownership of complex software projects from conception to deployment\nWork closely with product team, business team including scientists, and other collaborators\nAnalyze and understand the functional and technical requirements of applications, solutions and systems and translate them into software architecture and design specifications\nDesign, develop, and implement applications and modules, including custom reports, interfaces, and enhancements\nDevelop and execute unit tests, integration tests, and other testing strategies to ensure the quality of the software\nConduct code reviews to ensure code quality and alignment to standard methodologies\nCreate and maintain documentation on software architecture, design, deployment, disaster recovery, and operations\nProvide ongoing support and maintenance for applications, ensuring that they operate smoothly and efficiently\nStay updated with the latest technology and security trends and advancements\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The professional we seek is a [type of person] with these qualifications.\nBasic Qualifications:\nDoctorate Degree OR Masters degree with 4 - 6 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nBachelors degree with 6 - 8 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/ Bioinformatics or related field OR\nDiploma with 10 - 12 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/ Bioinformatics or related field\nPreferred Qualifications and Experience:\n3+ years of experience in implementing and supporting biopharma scientific software platforms\nSome experience with ML or generative AI technologies\nProficient in Java or Python\nProficient in at least one JavaScript UI Framework (e.g. ExtJS, React, or Angular)\nProficient in SQL (e.g. Oracle, PostgreSQL, Databricks)\nExperience with event-based architecture and serverless AWS services such as EventBridge, SQS, Lambda or ECS.\nPreferred Qualifications:\nExperience with Benchling\nHands-on experience with Full Stack software development\nStrong understanding of software development methodologies, mainly Agile and Scrum\nWorking experience with DevOps practices and CI/CD pipelines\nExperience of infrastructure as code (IaC) tools (Terraform, CloudFormation)\nExperience with monitoring and logging tools (e.g., Prometheus, Grafana, Splunk)\nExperience with automated testing tools and frameworks\nExperience with big data technologies (e.g., Spark, Databricks, Kafka)\nExperience with leveraging the use of AI-assistants (e.g. GitHub Copilot) to accelerate software development and improve code quality\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nAWS Certified Cloud Practitioner preferred\nSoft Skills:\nExcellent problem solving, analytical, and troubleshooting skills\nStrong communication and interpersonal skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to learn quickly & work independently\nTeam-oriented, with a focus on achieving team goals\nAbility to manage multiple priorities successfully\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Large Molecule Discovery', 'Java', 'DevOps', 'PostgreSQL', 'JavaScript', 'Databricks', 'Oracle', 'React', 'AWS', 'ExtJS', 'Angular', 'Python']",2025-06-12 05:39:42
Informatica IDMC Engineer,Qentelli,5 - 10 years,15-25 Lacs P.A.,['Hyderabad'],"Job Summary:\nWe are seeking a highly skilled Informatica IDMC & Data Governance Specialist to join our dynamic team. This role requires hands-on expertise in various IDMC modules, including Snowflake, Cloud Data Quality (CDQ), Cloud Application Integration (CAI), Cloud Data Governance & Catalog (CDGC), and Cloud Data Marketplace (CDMP). The ideal candidate will have a strong background in data governance and data quality, with a proven ability to create and configure data profiles and quality rules. If you have a passion for cloud technologies and data management, this is an exciting opportunity to contribute to the ongoing success of our data initiatives.\n\nKey Responsibilities:\nIDMC Modules Management: Manage and support the following IDMC modules:\nSnowflake (including RBAC)\nCloud Data Quality (CDQ)\nCloud Application Integration (CAI)\nCloud Data Governance & Catalog (CDGC)\nCloud Data Marketplace (CDMP)\nData Profiling & Quality: Create and configure Data Profiles with appropriate quality rules to ensure data meets required standards.\nData Governance & Compliance: Collaborate with data governance teams to ensure compliance with data governance policies and standards.\nPlatform Optimization & Troubleshooting: Continuously monitor the performance of IDMC platforms, resolving issues, optimizing configurations, and supporting ongoing integrations and data workflows.\nCollaboration & Support: Work cross-functionally with infrastructure, application, and data teams to ensure efficient use of the IDMC platform and ensure business data needs are met.\n\nMandatory Skills:\nExtensive work experience is required for below areas to configure everything from scratch for any complex requirement\nCDI (Cloud Data Integration)\nCDQ (Cloud Data Quality)\nCAI (Cloud Application Integration)\nEDC (On-Prem Informatica Enterprise Data Catalog)\n\nNice-to-Have Skills:\nKnowledge of Data Integration (IPC) and Git.\nPrior experience in Data Governance initiatives, including the use of governance frameworks and best practices.\n\nQualifications:\nStrong analytical and problem-solving abilities.\nExcellent communication skills, both written and verbal, to explain technical concepts to nontechnical stakeholders.\nAbility to prioritize tasks, manage multiple projects, and work in a fast-paced environment.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Cloud Data Governance & Catalog', 'Cloud Application Integration', 'Cloud Data Quality', 'Cloud Data Marketplace', 'Snowflake']",2025-06-12 05:39:44
Cloud Solution Delivery Lead Consultant,"NTT DATA, Inc.",8 - 13 years,Not Disclosed,['Bengaluru'],"We are currently seeking a Cloud Solution Delivery Lead Consultant to join our team in bangalore, Karntaka (IN-KA), India (IN).\n\n\nData Engineer Lead Robust hands-on experience with industry standard tooling and techniques, including SQL, Git and CI/CD pipelinesmandiroty Management, administration, and maintenance with data streaming tools such as Kafka/Confluent Kafka, Flink Experienced with software support for applications written in Python & SQL Administration, configuration and maintenance of Snowflake & DBT Experience with data product environments that use tools such as Kafka Connect, Synk, Confluent Schema Registry, Atlan, IBM MQ, Sonarcube, Apache Airflow, Apache Iceberg, Dynamo DB, Terraform and GitHub Debugging issues, root cause analysis, and applying fixes Management and maintenance of ETL processes (bug fixing and batch job monitoring)Training & Certification ""¢\nApache Kafka Administration\n\nSnowflake Fundamentals/Advanced Training\n""¢ Experience 8 years of experience in a technical role working with AWSAt least 2 years in a leadership or management role\n\n\n",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'ci/cd', 'python', 'kafka', 'aws', 'hive', 'hibernate', 'sql', 'microservices', 'spring', 'git', 'apache', 'java', 'spark', 'debugging', 'hadoop', 'snowflake', 'github', 'mq series', 'dynamo db', 'sonarqube', 'airflow', 'solution delivery', 'apache flink', 'spring boot', 'kafka streams', 'terraform']",2025-06-12 05:39:47
MIS Analyst,Phonepe,3 - 6 years,Not Disclosed,['Bengaluru'],"Role & responsibilities:\n1. Data Collection and Integration: Gather and integrate data from multiple internal and external sources.\nEnsure data accuracy, integrity, and consistency across various data streams.\nDevelop and maintain databases and data systems necessary for projects and department functions.\n2. Data Analysis and Reporting: Analyze complex data sets to identify trends, patterns, and insights\n. Prepare detailed business views and reports, highlighting key metrics and performance indicators.\nCreate dashboards and visualizations to present data in an understandable and actionable manner.\n3. Business Metrics and Health Monitoring: Define and track key performance indicators (KPIs) to monitor business health.\nDevelop methodologies for measuring and reporting on business performance.\nRegularly update and maintain reports and dashboards to reflect current business status.\n4. Representation and Communication:\nRepresent the team in various forums, including management meetings, strategy sessions, and cross-functional working groups.\nCommunicate findings, insights, and recommendations effectively to stakeholders at all levels. Ensure that the team's work is visible and understood across the organization.\n5. Collaboration and Stakeholder Management: Work closely with different departments to understand their data needs and provide necessary support.\nCollaborate with IT and data engineering teams to ensure seamless data flow and integration. Foster strong relationships with key stakeholders to facilitate effective communication and collaboration.\n6. Continuous Improvement:\nStay updated with the latest industry trends, tools, and technologies in data analytics and business intelligence.\nPropose and implement process improvements to enhance data quality and reporting efficiency.\nParticipate in professional development opportunities to expand skillset and knowledge bas\n\n\nPreferred candidate profile",Industry Type: FinTech / Payments,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Excel', 'MIS', 'data', 'powerbi', 'Management Information System', 'data analyst', 'sql']",2025-06-12 05:39:49
Senior Analytics Consultant,Wells Fargo,4 - 9 years,Not Disclosed,['Hyderabad'],"About this role:\nWells Fargo is seeking a Senior Analytics Consultant with a proven track record of success preferably in the banking industry.\n\nIn this role, you will:\nConsult, review and research moderately complex business, operational, and technical challenges that require an in-depth evaluation of variable data factors",,,,"['data manipulation', 'Data Engineering', 'data analysis', 'data management', 'SQL']",2025-06-12 05:39:52
AI Engineer,Fission Labs,3 - 8 years,15-25 Lacs P.A.,['Hyderabad'],"Company Name - Fission Labs\n\nApply Here - https://app.fabrichq.ai/jobs/0e46cebe-8b91-4a96-9061-950b66dc4d54\n\nAbout Us:\nHeadquartered in Sunnyvale, with offices in Dallas & Hyderabad, Fission Labs is a leading\nsoftware development company, specializing in crafting flexible, agile, and scalable solutions\nthat propel businesses forward.With a comprehensive range of services, including product development, cloud engineering, big data analytics, QA, DevOps consulting, and AI/ML solutions, we empower clients to achieve sustainable digital transformation that aligns seamlessly with their business goals.\n\nKey Responsibilities\nDesign and architect complex Generative AI solutions using AWS technologies\nDevelop advanced AI architectures incorporating state-of-the-art GenAI technologies\nCreate and implement Retrieval Augmented Generation (RAG) and GraphRAG solutions\nArchitect scalable AI systems using AWS Bedrock and SageMaker\nDesign and implement agentic AI systems with advanced reasoning capabilities\nDevelop custom AI solutions leveraging vector databases and advanced machine learning techniques\nEvaluate and integrate emerging GenAI technologies and methodologies\n\nTechnical Expertise Requirements\n\nGenerative AI Technologies\nExpert-level understanding of:\nRetrieval Augmented Generation (RAG)\nVector Database architectures\nAgentic AI design principles\n\nAWS AI Services\nComprehensive expertise in:\nAWS Bedrock\nAmazon SageMaker\nAWS AI/ML services ecosystem\nCloud-native AI solution design\n\nTechnical Skills\nAdvanced Python programming for AI/ML applications",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['LoRA', 'Vector Database', 'RAG', 'LLM', 'Python', 'GraphRag']",2025-06-12 05:39:54
Developer - L4,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\n\n\n\nDo\n1. Instrumental in understanding the requirements and design of the product/ software\nDevelop software solutions by studying information needs, studying systems flow, data usage and work processes\nInvestigating problem areas followed by the software development life cycle\nFacilitate root cause analysis of the system issues and problem statement\nIdentify ideas to improve system performance and impact availability\nAnalyze client requirements and convert requirements to feasible design\nCollaborate with functional teams or systems analysts who carry out the detailed investigation into software requirements\nConferring with project managers to obtain information on software capabilities\n\n\n\n2. Perform coding and ensure optimal software/ module development\nDetermine operational feasibility by evaluating analysis, problem definition, requirements, software development and proposed software\nDevelop and automate processes for software validation by setting up and designing test cases/scenarios/usage cases, and executing these cases\nModifying software to fix errors, adapt it to new hardware, improve its performance, or upgrade interfaces.\nAnalyzing information to recommend and plan the installation of new systems or modifications of an existing system\nEnsuring that code is error free or has no bugs and test failure\nPreparing reports on programming project specifications, activities and status\nEnsure all the codes are raised as per the norm defined for project / program / account with clear description and replication patterns\nCompile timely, comprehensive and accurate documentation and reports as requested\nCoordinating with the team on daily project status and progress and documenting it\nProviding feedback on usability and serviceability, trace the result to quality risk and report it to concerned stakeholders\n\n\n\n3. Status Reporting and Customer Focus on an ongoing basis with respect to project and its execution\nCapturing all the requirements and clarifications from the client for better quality work\nTaking feedback on the regular basis to ensure smooth and on time delivery\nParticipating in continuing education and training to remain current on best practices, learn new programming languages, and better assist other team members.\nConsulting with engineering staff to evaluate software-hardware interfaces and develop specifications and performance requirements\nDocument and demonstrate solutions by developing documentation, flowcharts, layouts, diagrams, charts, code comments and clear code\nDocumenting very necessary details and reports in a formal way for proper understanding of software from client proposal to implementation\nEnsure good quality of interaction with customer w.r.t. e-mail content, fault report tracking, voice calls, business etiquette etc\nTimely Response to customer requests and no instances of complaints either internally or externally\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure\n1.Continuous Integration, Deployment & Monitoring of Software100% error free on boarding & implementation, throughput %, Adherence to the schedule/ release plan\n2.Quality & CSATOn-Time Delivery, Manage software, Troubleshoot queries,Customer experience, completion of assigned certifications for skill upgradation\n3.MIS & Reporting100% on time MIS & report generation\nMandatory Skills: DataBricks - Data Engineering.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['DataBricks', 'module development', 'software development', 'Data Engineering', 'report generation', 'MIS', 'CI/CD', 'SDLC']",2025-06-12 05:39:56
Proactive Hiring For Databricks,HCLTech,5 - 10 years,Not Disclosed,"['Noida', 'Chennai', 'Bengaluru']","Responsibilities\nLead the design, development, and implementation of big data solutions using Apache Spark and Databricks.\nArchitect and optimize data pipelines and workflows to process large volumes of data efficiently.\nUtilize Databricks features such as Delta Lake, Databricks SQL, and Databricks Workflows to enhance data processing and analytics capabilities.",,,,"['apache spark', 'Databricks Engineer', 'SQL']",2025-06-12 05:39:58
Etl Tester- Bangalore(Pan India Infosys),Infosys,3 - 8 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Job description\nHiring for ETL Testing with experience range 3 to 10 years\nMandatory Skills: ETL/DWH/Big data testing\nEducation: BE/B.Tech/MCA/M.Tech/MSc./MS\nResponsibilities\nA day in the life of an Infoscion\nAs part of the Infosys consulting team, your primary role would be to actively aid the consulting team in different phases of the project including problem definition, effort estimation, diagnosis, solution generation and design and deployment\nYou will explore the alternatives to the recommended solutions based on research that includes literature surveys, information available in public domains, vendor evaluation information, etc. and build POCs\nYou will create requirement specifications from the business needs, define the to-be-processes and detailed functional designs based on requirements.\nYou will support configuring solution requirements on the products; understand if any issues, diagnose the root-cause of such issues, seek clarifications, and then identify and shortlist solution alternatives\nYou will also contribute to unit-level and organizational initiatives with an objective of providing high quality value adding solutions to customers. If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ETL Testing', 'Big Data Testing', 'DWH Testing']",2025-06-12 05:40:01
Etl Tester - Pan India,Infosys,3 - 8 years,Not Disclosed,"['Kolkata', 'Chennai', 'Bengaluru']","Job description,\n\nHiring for ETL testing with experience range 3-10 years\n\nMandatory Skills: ETL Testing\n\nLocation - Bangalore/Hyderabad/Pune/kolkata/Chennai/Bhubaneswar\n\nEducation: BE/B.Tech/MCA/M.Tech/MSc./MS\n\nResponsibilities\nA day in the life of an Infoscion\nAs part of the Infosys consulting team, your primary role would be to actively aid the consulting team in different phases of the project including problem definition, effort estimation, diagnosis, solution generation and design and deployment\nYou will explore the alternatives to the recommended solutions based on research that includes literature surveys, information available in public domains, vendor evaluation information, etc. and build POCs\nYou will create requirement specifications from the business needs, define the to-be-processes and detailed functional designs based on requirements.\nYou will support configuring solution requirements on the products; understand if any issues, diagnose the root-cause of such issues, seek clarifications, and then identify and shortlist solution alternatives\nYou will also contribute to unit-level and organizational initiatives with an objective of providing high quality value adding solutions to customers. If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ETL Testing', 'ETL', 'Etl Testers']",2025-06-12 05:40:03
Sr. Technology Auditor,AMERICAN EXPRESS,2 - 4 years,13-18 Lacs P.A.,"['Gurugram', 'Delhi / NCR']","Role & responsibilities\n•       Translate business risks, controls and supporting data into analytic requirements and partners with colleagues to build effective analytics and insights\n•       Responsible for multiple simultaneous audit projects of all sizes and complexity across multiple business areas within and outside of local region, in unfamiliar areas, and for different audit leaders\n•       Link analytics and insights to ongoing strategic initiatives\n•       Apply proven/ advanced data algorithms, advanced analytic and modeling techniques to draw insights essential to driving improvement initiatives",,,,"['Natural Language Processing', 'Tableau', 'Machine Learning', 'SQL', 'Python']",2025-06-12 05:40:05
Full Stack AI Engineer (Lead),Inclusive Business Solutions,3 - 8 years,20-35 Lacs P.A.,[],"AI specialists for Full Stack, Computer Vision, and Speech Processing roles. Responsibilities include real-time data integration, emotion recognition, and speech analysis. Must have expertise in AI frameworks, ML models, and optimization techniques.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Speech Recognition', 'Natural Language Processing', 'Computer Vision', 'Machine Learning', 'Python', 'Tensorflow', 'Large Language Model', 'Artificial Intelligence', 'AWS', 'Deep Learning']",2025-06-12 05:40:07
Qliksense Developer,Oracle,3 - 8 years,Not Disclosed,"['Chennai', 'Bengaluru', 'Mumbai (All Areas)']","We are seeking a skilled FLEXCUBE Reports Developer with expertise in Qlik sense to join our team. The ideal candidate will be responsible for designing, developing, and maintaining reports and dashboards that provide valuable insights from FLEXCUBE core banking data.\nKey Responsibilities:\nReport Development: Design and create interactive reports and dashboards using Qlik Sense to visualize FLEXCUBE data for business users.\nFLEXCUBE 14.7 Backend Tables: FLEXCUBE data model knowledge is must\nData Modelling: Develop data models and relationships within Qlik Sense to ensure accurate representation of FLEXCUBE data.\nCustomization: Customize reports to meet specific business requirements and ensure they align with industry best practices.\nPerformance Optimization: Optimize report performance for efficient data retrieval and rendering.\nData Integration: Integrate data from various sources into Qlik Sense reports, including FLEXCUBE and other data repositories.\nData Security: Implement data security and access controls within Qlik Sense to protect sensitive information.\nUser Training: Provide training and support to end-users to enable them to effectively utilize Qlik Sense reports.\nDocumentation: Maintain documentation for reports, data models, and best practices.\nMastery of the FLEXCUBE 14.7 backend tables and data model is essential.\n\nQualifications:\nBachelor's degree in Computer Science, Information Technology, or a related field.\n3 to 7 Years of proven experience in developing reports and dashboards using Qlik Sense.\nFamiliarity with FLEXCUBE core banking systems.\nFamiliarity with OLAP Cubes, Data Marts, Datawarehouse\nProficiency in data modelling and data visualization concepts.\nStrong SQL skills for data extraction and transformation.\nExcellent problem-solving and analytical skills.\nStrong communication and collaboration abilities.\nBanking or financial industry experience is beneficial.\nQlik Sense certifications are a plus.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Power Bi', 'Qlik Sense Development', 'QlikView', 'Qlik', 'SQL', 'Dashboarding']",2025-06-12 05:40:10
Azure Devops Engineer,Celebal Technologies,4 - 9 years,10-20 Lacs P.A.,['Jaipur( Malviya Nagar )'],"Job Role Azure DevOps Engineer\n\nJob Location – Jaipur\nExperience Required-4+ Years\n\nAbout Us -Celebal Technologies is a premier software services company in the field of Data Science, Big Data and Enterprise Cloud. Celebal Technologies helps you to discover the competitive advantage by employing intelligent data solutions using cutting-edge technology solutions that can bring massive value to your organization. The core offerings are around ""Data to Intelligence"", wherein we leverage data to extract intelligence and patterns thereby facilitating smarter and quicker decision making for clients. With Celebal Technologies, who understands the core value of modern analytics over the enterprise, we help the business in improving business intelligence and more data-driven in architecting solutions.",,,,"['Azure Kubernetes', 'Azure Cloud', 'Azure Devops', 'Azure Pipelines', 'Ci Cd Pipeline', 'Container Orchestration', 'Aks', 'Arm Templates', 'Azure Monitoring', 'Terraform', 'Containerization', 'Docker', 'Kubernetes']",2025-06-12 05:40:13
"Walk-in For Testing Positions at Chennai, Pune, Bangalore Location",Hexaware Technologies,5 - 10 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']",Exciting Career Opportunity at Hexaware Technologies!\n\nJoin our dynamic team and advance your career with us! We are looking for committed professionals to fill the following positions:\n\n- API Automation Testing (Rest Assured) (6 to 10 Years)\n- Python Automation (6 to 10 Years)\n- ETL Testing (Bigdata/Cloud) (6 to 12 Years),,,,"['Api Automation', 'Cloud Testing', 'Python Testing', 'Automation Testing', 'ETL Testing', 'Big Data Testing', 'Rest Assured', 'Selenium Testing', 'Selenium With Java']",2025-06-12 05:40:15
Devops Engineer,Thirdeye Data Inc.,4 - 7 years,8-10 Lacs P.A.,['Kolkata'],"Job Title: Senior DevOps Engineer\nLocation: Kolkata, India\nJob Type: Full-Time\nJob Overview:\nWe are seeking a highly skilled Senior DevOps Engineer with deep expertise in Azure cloud services, network security, and CI/CD pipelines to join our dynamic team. As a Senior DevOps Engineer, you will be responsible for designing and implementing robust infrastructure solutions, automating deployment workflows, managing security policies, and optimizing cloud-native applications to drive our organization's success.\nKey Responsibilities:\nAzure VNet and Subnets: Configure and manage Azure Virtual Networks (VNets), subnets, and peering. Implement and manage network security policies, ensuring secure connectivity between resources.\nNetwork Security: Manage and configure Azure Firewall, VPN gateways, Network Security Groups (NSGs), and DDoS protection to safeguard network traffic.\nCI/CD with GitHub Actions: Automate deployments to Azure using GitHub Actions and configure secret management to ensure safe and efficient software delivery pipelines.\nAzure Functions: Design and implement serverless solutions using Azure Functions. Configure triggers, bindings, and manage scaling to meet dynamic workloads.\nAzure Static Web Apps: Deploy, manage, and optimize static web applications in Azure, ensuring CI/CD pipelines and custom domain configurations are seamlessly implemented.\nAzure Cognitive Search Service: Create, configure, and manage Azure Cognitive Search indexes. Integrate data and ensure secure and effective use of Azure Search services.\nSecurity Focus: Implement and manage Azure Identity and Access Management (IAM), Role-Based Access Control (RBAC), encryption methods, compliance protocols, and ensure the security of cloud resources.\nAzure Logic Apps: Configure and manage Azure Logic Apps for automating workflows, data integration, and orchestrating services.\nInfrastructure Management: Utilize Infrastructure as Code (IaC) to provision, manage, and scale cloud infrastructure.\nCollaboration: Work closely with cross-functional teams including developers, architects, and security professionals to build and maintain scalable cloud solutions.\nAzure Resource Groups: Manage and organize resources in Azure Resource Groups (RGs) for efficient resource governance, cost management, and access control.\nNginx: Configure and maintain Nginx web servers, ensure high availability, and optimize performance for web applications deployed on Azure.\nRequired Skills and Qualifications:\nCloud Expertise: Strong proficiency in Azure cloud services (Azure VNet, Subnets, Azure Functions, Azure Cognitive Search, Azure Static Web Apps, etc.).\nNetwork Security: Extensive knowledge of Azure Firewall, VPN Gateways, NSGs, DDoS Protection, and other network security configurations.\nCI/CD: Proven experience in GitHub Actions, including automated deployment workflows, secret management, and CI/CD best practices.\nServerless Architecture: Experience in designing and managing Azure Functions, understanding of triggers, bindings, and scaling techniques.\nWeb Applications: Hands-on experience deploying and managing static web apps using Azure, including configuring custom domains and integrating CI/CD.\nSecurity and Compliance: Solid understanding of Azure IAM, RBAC, encryption practices, and securing cloud resources in line with compliance standards.\nLinux Knowledge: Extensive experience working with Linux environments and troubleshooting issues.\nDomain Mapping & Nginx: Experience configuring domain mapping, setting up and maintaining Nginx web servers, and ensuring high availability.\nDocker & GitHub: Proficiency in using Docker for containerization and managing deployments through GitHub and other CI/CD platforms.\nAzure Logic Apps & Functions: Experience working with Azure Logic Apps and Function Apps for automating workflows and orchestrating serverless solutions.\nNice to Have Skills:\nKubernetes (K8s): Experience working with Kubernetes clusters for container orchestration and management.\nJenkins: Knowledge of Jenkins for continuous integration and deployment.\nAWS and Google Cloud: Familiarity with AWS or Google Cloud Platform (GCP) and multi-cloud architectures.\nOther DevOps Tools & Practices: Knowledge of additional DevOps tools and best practices in automation, monitoring, and scaling infrastructure.\nEducation and Experience:\nBachelor's degree in computer science, Information Technology, Engineering, or a related field (or equivalent experience).\nMinimum 4+ years of experience as a DevOps Engineer or in a similar technical role.\nStrong experience working in Azure environments with hands-on expertise in cloud infrastructure management and DevOps pipelines.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Devops', 'Nginx', 'Ci Cd Pipeline', 'Linux', 'Network Security', 'Microsoft Azure', 'Azure Logic Apps', 'Azure Security']",2025-06-12 05:40:18
Photogrammetry Engineer,MAPe IT Solution,3 - 8 years,10-13 Lacs P.A.,['Chennai'],"Role & responsibilities\nPhotogrammetric Data Processing\nProcess stereo imagery for Planimetric and DTM (Digital Terrain Model) extraction using industry-standard methodologies.\nApply concepts such as Aerial Triangulation (AT) and Block File Setup for photogrammetric project setup.\nSoftware Usage\nUtilize software tools such as MicroStation V8i, Erdas Imagine, DTMaster, DSM, and Datum for:\nData processing\nFeature extraction\nOrtho image generation\nOrtho Image Production\nConduct Ortho Image Processing to ensure spatial accuracy and clarity in the final deliverables.\nQuality Control (QA/QC)\nPerform rigorous QA/QC checks on photogrammetric outputs to ensure accuracy and adherence to client/project standards.\nData Management\nManage large volumes of image and geospatial datasets efficiently.\nMaintain organized project documentation and file structures.\nTeam Collaboration\nCollaborate with project managers, GIS analysts, and QA teams to ensure deliverables meet technical expectations and deadlines.\nPreferred candidate profile\n\nExperience:\n3 to 8 years of relevant hands-on experience in photogrammetry and geospatial data processing.\nTechnical Skills:\nProficient in tools like MicroStation V8i, Erdas, DTMaster, DSM, and Datum.\nStrong understanding of Planimetric mapping and DTM generation techniques.\nEducational Qualification:\nDiploma or Bachelor's Degree in Civil Engineering, Geomatics, Geoinformatics, or any related field.\nDomain Knowledge:\nIn-depth understanding of aerial triangulation, block file setups, and orthophoto creation.\nKnowledge of geospatial industry standards and deliverable requirements.\nSoft Skills:\nDetail-oriented with a commitment to quality and accuracy.\nGood organizational and communication skills.\nAbility to work independently as well as part of a team in a deadline-driven environment.\nWork Mode:\nMust be willing to work from office at the Chennai location.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['DTmaster', 'Data Processing', 'ERDAS', 'plainmetric', 'Datum', 'Exploration', 'Microstation', 'DSM', 'Arcgis']",2025-06-12 05:40:20
Senior Mis Executive,Thyrocare,2 - 6 years,2.5-5 Lacs P.A.,['Mumbai (All Areas)'],"Roles and Responsibilities\nAnalyse, extract, and review relevant data from various client and proprietary systems to create spreadsheet reports for use of management, efficiency, quality, and productivity analysis, employee stack-ranking, cost analysis, billing/invoicing\nAnalyse data and publish daily, weekly and monthly reports as per the pre-defined timelines\nBuild and manage the tools to collect raw data that is not available from current software and systems.\nCreate multi-level reports from the same data to serve multiple stakeholders with minimum manual re-work\nSuch other activities as may be assigned by your manager\nParticipate in cross-functional meetings to resolve recurring customer issues.\nAnalyze current business processes and make recommendations for improvements\nMaintain thorough understanding of data and information resources\nMaintain a status on all projects and proactively communicate with management\n\nDesired Candidate Profile\nCandidates with 3+ Years of experience in creating and maintaining MIS reports for Banking, Financial Services, BPO/KPO/LPO, or Back-office Operations.\nAdvanced Excel Knowledge including but not limited to: Creating Impressive Dashboards, working with large excel data running into lakhs of rows and several hundred columns, use of excel tools and formulas: pivot, xlookup, vlookup, index, sumifs, countifs, maxifs, sumproduct, offset, string and date related formulas, multiple layer nested if loops, rank, use of array formulas such as unique, filter, sort, sortby.\nAnd Knowledge of Big Data analysis tools such as SQL, Python etc\nKnowledge of Power Suite: Power Query, Power Automate and Power BI would be an added advantage\nProficiency with macros and VBA coding would be an added advantage\nHigh attention to detail\nMust have an analytical bent of mind\nShould be able to build tools with high scalability and agility\nQualifications/ Requirements:\nUG- Any Graduate\nHigh producer with attention to quality\nStrong PC skills, with demonstrated proficiency with Microsoft Office\nWilling to work in shifts as per business requirements\nWillingness to learn and invest time and effort for career development.",Industry Type: Pharmaceutical & Life Sciences,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['Purchase', 'MIS', 'Advanced Excel', 'MIS Preparation', 'MIS Operations', 'Supply Chain Management', 'MIS Reporting', 'Management Information System', 'Inventory', 'Excel Report Preparation']",2025-06-12 05:40:22
Solutions Architect,"NTT DATA, Inc.",3 - 7 years,Not Disclosed,"['New Delhi', 'Chennai', 'Bengaluru']","Your day at NTT DATA\nWe are seeking an experienced Data Architect to join our team in designing and delivering innovative data solutions to clients. The successful candidate will be responsible for architecting, developing, and implementing data management solutions and data architectures for various industries. This role requires strong technical expertise, excellent problem-solving skills, and the ability to work effectively with clients and internal teams to design and deploy scalable, secure, and efficient data solutions.\nWhat you'll be doing\nWe are seeking an experienced Data Architect to join our team in designing and delivering innovative data solutions to clients. The successful candidate will be responsible for architecting, developing, and implementing data management solutions and data architectures for various industries. This role requires strong technical expertise, excellent problem-solving skills, and the ability to work effectively with clients and internal teams to design and deploy scalable, secure, and efficient data solutions.\nExperience and Leadership:\nProven experience in data architecture, with a recent role as a Lead Data Solutions Architect, or a similar senior position in the field.\nProven experience in leading architectural design and strategy for complex data solutions and then overseeing their delivery.\nExperience in consulting roles, delivering custom data architecture solutions across various industries.\nArchitectural Expertise:\nStrong expertise in designing and overseeing delivery of data streaming and event-driven architectures, with a focus on Kafka and Confluent platforms.\nIn-depth knowledge in architecting and implementing data lakes and lakehouse platforms, including experience with Databricks and Unity Catalog.\nProficiency in conceptualising and applying Data Mesh and Data Fabric architectural patterns.\nExperience in developing data product strategies, with a strong inclination towards a product-led approach in data solution architecture.\nExtensive familiarity with cloud data architecture on platforms such as AWS, Azure, GCP, and Snowflake.\nUnderstanding of cloud platform infrastructure and its impact on data architecture.\nData Technology Skills:\nA solid understanding of big data technologies such as Apache Spark, and knowledge of Hadoop ecosystems.\nKnowledge of programming languages such as Python or R is beneficial.\nExposure to ETL/ ELT processes, SQL, NoSQL databases is a nice-to-have, providing a well-rounded background.\nExperience with data visualization tools and DevOps principles/tools is advantageous.\nFamiliarity with machine learning and AI concepts, particularly in how they integrate into data architectures.\nDesign and Lifecycle Management:\nProven background in designing modern, scalable, and robust data architectures.\nComprehensive grasp of the data architecture lifecycle, from concept to deployment and consumption.\nData Management and Governance:\nStrong knowledge of data management principles and best practices, including data governance frameworks.\nExperience with data security and compliance regulations (GDPR, CCPA, HIPAA, etc.)\nLeadership and Communication:\nExceptional leadership skills to manage and guide a team of architects and technical experts.\nExcellent communication and interpersonal skills, with a proven ability to influence architectural decisions with clients and guide best practices\nProject and Stakeholder Management:\nExperience with agile methodologies (e.g. SAFe, Scrum, Kanban) in the context of architectural projects.\nAbility to manage project budgets, timelines, and resources, maintaining focus on architectural deliverables.\nLocation: Delhi or Bangalore\nWorkplace type:\nHybrid Working",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Solution architecture', 'snowflake', 'python', 'data management', 'big data technologies', 'microsoft azure', 'data architecture', 'sql', 'data bricks', 'spark', 'gcp', 'devops', 'kanban', 'kafka', 'architectural patterns', 'scrum', 'agile', 'hadoop', 'conceptualization', 'aws', 'etl', 'data lake']",2025-06-12 05:40:25
MS Fabric Architect - Support,"NTT DATA, Inc.",6 - 11 years,Not Disclosed,['Bengaluru'],"Req ID: 319341\n\nWe are currently seeking a MS Fabric Architect - Support to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\n""Job DutiesKey Responsibilities:\nProvide technical support and troubleshooting for Microsoft Fabric services.\nAssist in the implementation, configuration, and maintenance of Microsoft Fabric environments.\nMonitor system performance and resolve issues proactively.\nCollaborate with cross-functional teams to optimize data workflows and analytics solutions.\nDocument support procedures, best practices, and troubleshooting steps.\nAssist in user training and onboarding for Microsoft Fabric-related tools and applications.\nStay up to date with the latest Microsoft Fabric updates and best practices.\nRequired Qualifications:\n6+ years of experience in IT support, with a focus on Microsoft Fabric or related technologies.\nStrong knowledge of Microsoft Fabric, Power BI, Azure Synapse, and data integration tools.\nExperience with troubleshooting and resolving issues in a cloud-based environment.\nFamiliarity with SQL, data pipelines, and ETL processes.\nExcellent problem-solving and communication skills.\nAbility to work independently and collaboratively in a team environment.\nPreferred Qualifications:\nMicrosoft certifications related to Fabric, Azure, or Power BI.\nExperience with automation and scripting (PowerShell, Python, etc.).\nUnderstanding of security and compliance considerations in cloud-based data platforms.\n\nMinimum Skills RequiredKey Responsibilities:\nProvide technical support and troubleshooting for Microsoft Fabric services.\nAssist in the implementation, configuration, and maintenance of Microsoft Fabric environments.\nMonitor system performance and resolve issues proactively.\nCollaborate with cross-functional teams to optimize data workflows and analytics solutions.\nDocument support procedures, best practices, and troubleshooting steps.\nAssist in user training and onboarding for Microsoft Fabric-related tools and applications.\nStay up to date with the latest Microsoft Fabric updates and best practices.\nRequired Qualifications:\n6+ years of experience in IT support, with a focus on Microsoft Fabric or related technologies.\nStrong knowledge of Microsoft Fabric, Power BI, Azure Synapse, and data integration tools.\nExperience with troubleshooting and resolving issues in a cloud-based environment.\nFamiliarity with SQL, data pipelines, and ETL processes.\nExcellent problem-solving and communication skills.\nAbility to work independently and collaboratively in a team environment.\nPreferred Qualifications:\nMicrosoft certifications related to Fabric, Azure, or Power BI.\nExperience with automation and scripting (PowerShell, Python, etc.).\nUnderstanding of security and compliance considerations in cloud-based data platforms.""",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['it support', 'azure synapse', 'power bi', 'sql', 'data integration tools', 'python', 'oracle', 'spotfire', 'microsoft azure', 'data warehousing', 'sql server', 'jquery', 'plsql', 'oracle 10g', 'node.js', 'tableau', 'java', 'etl tool', 'powershell', 'troubleshooting', 'html', 'etl', 'data integration', 'mongodb']",2025-06-12 05:40:27
Senior MSBI Developer (SQL & SSRS/SSIS Expert),Synechron,8 - 10 years,Not Disclosed,"['Pune', 'Hinjewadi']","job requisition idJR1027513\n\nJob Summary\nSynechron is seeking an experienced and detail-oriented Senior MSBI Developerexpertise in MSBI (Microsoft Business Intelligence) to join our data and analytics team. In this role, you will contribute to designing, developing, and maintaining robust reporting and data integration solutions that support our business objectives. Your expertise will help deliver actionable insights, improve decision-making processes, and enhance overall data management efficiency within the organization.\n\nSoftware\n\nRequired\n\nSkills:\nMSBI Suite (including SSIS, SSRS, SSAS)\nSQL Server (including SQL Server Management Studio and Query Performance Tuning)\nVersionsRecent versions of SQL Server (2016 or later preferred)\nProven experience in creating complex reports, data transformation, and integration workflows\nPreferred\n\nSkills:\nPower BI or other visualization tools\nExperience with cloud-based data solutions (e.g., Azure SQL, Synapse Analytics)\nOverall Responsibilities\nDevelop, implement, and maintain MSBI solutions such as SSIS packages, SSRS reports, and data models to meet business requirements\nCollaborate with business stakeholders and data teams to gather reporting needs and translate them into scalable solutions\nOptimize and troubleshoot existing reports and data pipelines to improve performance and reliability\nEnsure data accuracy, security, and compliance within reporting processes\nDocument solution architectures, workflows, and processes for ongoing support and knowledge sharing\nParticipate in team initiatives to enhance data governance and best practices\nContribute to strategic planning for data platform evolution and modernization\nTechnical Skills (By Category)\n\nProgramming Languages:\nRequiredSQL (Advanced proficiency in query writing, stored procedures, and performance tuning)\nPreferredT-SQL scripting for data transformations and automation\nDatabases / Data Management:\nRequiredDeep knowledge of relational database concepts with extensive experience in SQL Server databases\nPreferredFamiliarity with data warehouse concepts, OLAP cubes, and data mart design\nCloud Technologies:\nDesiredBasic understanding of cloud-based data platforms like Azure Data Factory, Azure Synapse\nFrameworks and Libraries:\nNot directly applicable, focus on MSBI tools\nDevelopment Tools and Methodologies:\nExperience working within Agile development environments\nData pipeline development and testing best practices\nSecurity Protocols:\nImplement data security measures, role-based access controls, and ensure compliance with data privacy policies\nExperience\n8 to 10 years of professional experience in software development with substantial hands-on MSBI expertise\nDemonstrated experience in designing and deploying enterprise-level BI solutions\nDomain experience in finance, healthcare, retail, or similar industries is preferred\nAlternative candidacyExtensive prior experience with BI tools and proven success in similar roles may be considered in lieu of exact industry background\nDay-to-Day Activities\nDesign and develop SSIS data integration workflows to automate data loading processes\nCreate and optimize SSRS reports and dashboards for various organizational units\nEngage in troubleshooting and resolving technical issues in existing BI solutions\nCollaborate with data architects, developers, and business analysts to align data solutions with business needs\nConduct code reviews, testing, and validation of reports and data pipelines\nParticipate in scrum meetings, planning sessions, and stakeholder discussions\nEnsure documentation of solutions, processes, and workflows for ease of maintenance and scalability\nQualifications\nBachelors degree or equivalent in Computer Science, Information Technology, or related field\nRelevant certifications in Microsoft BI or SQL Server (e.g., Microsoft Certified Data Engineer Associate) preferred\nOngoing engagement in professional development related to BI, data management, and analytics tools\nProfessional Competencies\nAnalytical mindset with strong problem-solving abilities in data solution development\nCapable of working collaboratively across diverse teams and communicating technical concepts effectively\nStakeholder management skills to interpret and prioritize reporting needs\nAdaptability to evolving technologies and continuous learning mindset\nFocus on delivering high-quality, sustainable data solutions with attention to detail\nEffective time management, prioritizing tasks to meet project deadlines",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['performance tuning', 'stored procedures', 'sql scripting', 'query writing', 'msbi', 'sql server database', 'software development', 'ssas', 'power bi', 'microsoft azure', 'data mart design', 'olap cubes', 'sql server', 'sql azure', 'ssrs', 'data warehousing concepts', 'data transformation', 'ssis']",2025-06-12 05:40:30
Dot Net Architect,Conduent,10 - 15 years,Not Disclosed,['Bengaluru'],"Responsibilities \nDesign and develop highly scalable web based applications based on business needs\nDesign and customize software for client use with the aim of optimizing operational efficiency\nA deep understanding of, and ability to use and explain all aspects of application integration in .NET and data integration with SQL Server and associated technologies and standards\nDesign and provide architecture solutions based on the business needs\nStrong background in building and operating SAAS platforms using the Microsoft technology stack with modern services based architectures\nAbility to recommend and configure Azure subscriptions and establish connectivity\nWork with IT teams to setup new application architecture requirements\nCoordinate releases with Quality Assurance Team and implement SDLC work flows and better source code integration\nImplement build process and continuous build integration with Unit Testing framework\nDevelop and maintain a thorough understanding of business needs from both technical and business perspectives\nAssist and mentor junior team members to enforce development guidelines\nTake technical ownership of products and provide support with quick turnaround\nEffectively prioritize and execute tasks in a high-pressure environment\n\n\n Qualifications / Experience \nBachelor\\u2019s/Master\\u2019s degree in Computer Science / Computer Engineering\nMinimum of 10+ years\\u2019 experience in building enterprise scale windows and web application using Microsoft .NET technologies\n5+ years of experience in C#, ASP.NET MVC and Microsoft Web API\n1+ years of experience in Angular 2 or higher\nExperience in solution architecture in .Net technologies\nExperience in any of the following are also desirableBootstrap, Knockout, entity framework nhibernate, Subversion, Linq, Asynchronous Module Definition (such as requirejs)\nIn depth knowledge on design patterns and unit testing frameworks\nExperience with Agile application development\nSQL server performance tuning (SQL Server 2014/2016) and troubleshooting\nAbility to work with a sense of urgency and attention to detail\nExcellent oral and written communication skills",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql server', 'application integration', 'design patterns', '.net', 'data integration', 'c#', 'unit testing framework', 'web application', 'unit testing', 'scale', 'knockoutjs', 'net mvc', 'angular', 'asp.net core mvc', 'linq', 'saas applications', 'web api', 'agile', 'sdlc', 'asp', 'microsoft net']",2025-06-12 05:40:32
Informatica Developer,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\nRequirement\n1.We are looking 7 + yrs of Experience In Informatica Developement with IICS role.\n2.IICS- We are looking for candidates who has key skill set on IICS CAI (Cloud application integration)\n3.CDI(Cloud Data integration) and services.\n4.knowledge on APIs and also, strong analytical ability skills on SQL.\nMandatory Skills: Informatica iPaas.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Informatica Development', 'Informatica iPaas', 'Cloud application integration', 'IICS CAI', 'API', 'Cloud Data integration', 'SQL']",2025-06-12 05:40:34
Technical Lead - L1,Wipro,5 - 8 years,Not Disclosed,['Hyderabad'],"Role Purpose\nThe purpose of the role is to support process delivery by ensuring daily performance of the Production Specialists, resolve technical escalations and develop technical capability within the Production Specialists.\n\n\n\n\n\nDo\nOversee and support process by reviewing daily transactions on performance parameters\nReview performance dashboard and the scores for the team\nSupport the team in improving performance parameters by providing technical support and process guidance\nRecord, track, and document all queries received, problem-solving steps taken and total successful and unsuccessful resolutions\nEnsure standard processes and procedures are followed to resolve all client queries\nResolve client queries as per the SLAs defined in the contract\nDevelop understanding of process/ product for the team members to facilitate better client interaction and troubleshooting\nDocument and analyze call logs to spot most occurring trends to prevent future problems\nIdentify red flags and escalate serious client issues to Team leader in cases of untimely resolution\nEnsure all product information and disclosures are given to clients before and after the call/email requests\nAvoids legal challenges by monitoring compliance with service agreements\n\n\n\nHandle technical escalations through effective diagnosis and troubleshooting of client queries\nManage and resolve technical roadblocks/ escalations as per SLA and quality requirements\nIf unable to resolve the issues, timely escalate the issues to TA & SES\nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers and clients business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract SLAs\n\n\n\nBuild people capability to ensure operational excellence and maintain superior customer service levels of the existing account/client\nMentor and guide Production Specialists on improving technical knowledge\nCollate trainings to be conducted as triage to bridge the skill gaps identified through interviews with the Production Specialist\nDevelop and conduct trainings (Triages) within products for production specialist as per target\nInform client about the triages being conducted\nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nIdentify and document most common problems and recommend appropriate resolutions to the team\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\n\n\nDeliver\nNoPerformance ParameterMeasure\n1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback, NSAT/ ESAT\n2Team ManagementProductivity, efficiency, absenteeism\n3Capability developmentTriages completed, Technical Test performance\nMandatory Skills: Data Engineering Full Stack.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Team Management', 'technical support', 'Technical leadership', 'troubleshooting', 'operational excellence']",2025-06-12 05:40:37
Business Intelligence Manager,Phonepe,2 - 5 years,Not Disclosed,['Bengaluru'],"Role - Business Intelligence Manager\nExperience Bucket: 2 to 5 years\nWhat kind of person are we looking for?\nFirst-principle problem solvers who are passionate about working with data and uncovering the stories that the\nnumbers hide. If youre a curious mind and constantly questions the status-quo, then youd fit right in with us.\nWhat would you get to do in this role?\n\n\n\nBe an integral part of the pod strategy and will have to define the critical metrics. Youll also monitor\nperformance trends and influence any interventions, based on performance.\nCollaborate closely with the business, and product functions to understand the business problem and\ntranslate them into the right analytical problem statement\nOwn the entire insight generation phase and work on deep, thorough analysis to provide unbiased answers\non the identified problem statement. These insights and decisions will be influenced based on your\npresentation of the evidence, backed by data-driven hypothesis\n\nMove past just being the Data person and contribute with individual thoughts on how to improve the\ncritical metrics for the pod. We love people who think of themselves as the business owner and then go\nback to the data to prove or disprove the thought.\n\n\nAct as the analytics and visualization SME for the team and ensure that the dashboards are designed and\nbuilt in a way that makes it easy for the pod to consume the data they need\nMentor other analysts in the team; play an active role in hiring process & new team members onboarding\nprocess.\nWhat do you need to have to apply for this position?\n\n\n\n\nMinimum 2 years of analytics experience in relevant roles.\nStrong problem solving & analytical skills.\nPenchant for business & curiosity to understand how the product works.\nAbility to write complex queries on SQL to manipulate, consolidate multiple data sources for the purpose\nof dashboarding and analysis.\n\n\n\nIntuition for data and ability to handle big data sources\nStrong working knowledge in Excel and visualization tools like PowerBI, Tableau, QlikView\nUnderstanding of data-analysis languages such as R, Python and in core statistical concepts is expected;\nExperience in building ML models is good-to-have, but not mandatory\nAbility to clearly explain thoughts and ideas either verbally or in the written form.\nCandidates who are able to explain the story behind their analysis will find themselves at an advantage",Industry Type: FinTech / Payments,Department: Other,"Employment Type: Full Time, Permanent","['Data Analytics', 'Excel', 'Power Bi', 'Tableau', 'SQL']",2025-06-12 05:40:39
Technical Lead,PwC India,8 - 10 years,Not Disclosed,"['Mumbai', 'Navi Mumbai', 'Mumbai (All Areas)']","Loction :Mumbai\n\nJob Description:\n\nLooking for Candidates with 8 to 10 years of experience\nHands on experience of implementing data pipelines using traditional DWH, Big data & Cloud ecosystem",,,,"['Generative Ai Tools', 'Data Pipeline', 'Big Data', 'Data Governance', 'Data Quality', 'Aiml']",2025-06-12 05:40:41
Technical Lead - L1,Wipro,5 - 8 years,Not Disclosed,['Pune'],"Role Purpose\nThe purpose of the role is to support process delivery by ensuring daily performance of the Production Specialists, resolve technical escalations and develop technical capability within the Production Specialists.\n\n\n\n\n\nDo\nOversee and support process by reviewing daily transactions on performance parameters\nReview performance dashboard and the scores for the team\nSupport the team in improving performance parameters by providing technical support and process guidance\nRecord, track, and document all queries received, problem-solving steps taken and total successful and unsuccessful resolutions\nEnsure standard processes and procedures are followed to resolve all client queries\nResolve client queries as per the SLAs defined in the contract\nDevelop understanding of process/ product for the team members to facilitate better client interaction and troubleshooting\nDocument and analyze call logs to spot most occurring trends to prevent future problems\nIdentify red flags and escalate serious client issues to Team leader in cases of untimely resolution\nEnsure all product information and disclosures are given to clients before and after the call/email requests\nAvoids legal challenges by monitoring compliance with service agreements\n\n\n\nHandle technical escalations through effective diagnosis and troubleshooting of client queries\nManage and resolve technical roadblocks/ escalations as per SLA and quality requirements\nIf unable to resolve the issues, timely escalate the issues to TA & SES\nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers and clients business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract SLAs\n\n\n\nBuild people capability to ensure operational excellence and maintain superior customer service levels of the existing account/client\nMentor and guide Production Specialists on improving technical knowledge\nCollate trainings to be conducted as triage to bridge the skill gaps identified through interviews with the Production Specialist\nDevelop and conduct trainings (Triages) within products for production specialist as per target\nInform client about the triages being conducted\nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nIdentify and document most common problems and recommend appropriate resolutions to the team\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\n\n\nDeliver\n\nNoPerformance ParameterMeasure\n1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback, NSAT/ ESAT\n2Team ManagementProductivity, efficiency, absenteeism\n3Capability developmentTriages completed, Technical Test performance\nMandatory Skills: DataBricks - Data Engineering.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'DataBricks', 'hive', 'python', 'technical leadership', 'team management', 'spark', 'troubleshooting', 'hadoop', 'big data', 'sql']",2025-06-12 05:40:44
Quantitative Analytics Manager,Wells Fargo,4 - 8 years,Not Disclosed,['Bengaluru'],"In this role, you will:\nManage a team responsible for the creation and implementation of low to moderate complex financial areas\nMitigate operational risk and compute capital requirements\nDetermine scope and prioritization of work in consultation with experienced management\nParticipate in the development of strategy, policies, procedures, and organizational controls with model users, developers, validators, and technology",,,,"['Quantitative Analytics', 'strategy Planning', 'marketing', 'Git', 'GitHub', 'talent development', 'credit risk analysis']",2025-06-12 05:40:46
Lead Analytics Consultant,Wells Fargo,5 - 10 years,Not Disclosed,['Hyderabad'],"locationsHyderabad, India\nposted onPosted Yesterday\njob requisition idR-446112\nAbout this role:\nWells Fargo is seeking a Lead Analytics Consultant People Analytics. As a consultant, you will work as analytics professional in HR People Analytics and Business Insights delivery team and will be responsible for effective delivery of projects as per the business priority. The incumbent is expected to be an expert into executive summary, people strategy, HR consulting, HR advisory, advanced analytics & data science and value addition to the projects.",,,,"['Data Analytics', 'Data science tools', 'product lifecycle', 'SAS programming', 'ETL development', 'Alteryx', 'Data Management', 'Tableau Prep', 'SQL']",2025-06-12 05:40:48
Power Bi Engineer,Scalable Systems,4 - 6 years,10-15 Lacs P.A.,['Kochi'],Power Bi Engineer\nLocation : Kochi\nFulltime,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power Bi', 'Data Visualization', 'Power System', 'Dashboard Development']",2025-06-12 05:40:50
Technical Illustrator(Automotive),Cyient,4 - 9 years,Not Disclosed,"['Pune', 'Bengaluru']","Create and revise Parts Lists in the Parts Catalogs for automotive, agriculture and construction equipment.\nCreating exploded view artworks associated with parts catalogs as per given standards.\nProduce complete, clear and accurate parts catalogs content as per customer standards and guidelines.\nProcessing of information and data (engineering documents, changes, etc.) from\nEngineering, Manufacturing, Parts Marketing, Customer Service, Parts Warehouse and Suppliers.",,,,"['Iso Draw', 'Technical Illustration']",2025-06-12 05:40:52
Python/Pyspark developer,Zensar,4 - 5 years,Not Disclosed,"['Pune', 'Bengaluru']","Job Description:\nWe are seeking a highly skilled and motivated Python/PySpark Developer to join our growing team. In this role, you will be responsible for designing, developing, and maintaining high-performance data processing pipelines using Python and the PySpark framework. You will work closely with data engineers, data scientists, and other stakeholders to deliver impactful data-driven solutions.\nResponsibilities:\n- Design, develop, and implement scalable and efficient data pipelines using PySpark.\n- Write clean, well-documented, and maintainable Python code.\n- Optimize data processing performance and resource utilization.\n- Implement ETL (Extract, Transform, Load) processes to migrate and transform data across various systems.\n- Collaborate with data scientists and analysts to understand data requirements and translate them into technical solutions.\n- Troubleshoot and debug data processing issues.\n- Stay up-to-date with the latest advancements in big data technologies and best practices.\nQualifications:\n- Bachelor's degree in Computer Science, Engineering, or a related field.\n- 3+ years of experience in Python development.\n- 2+ years of experience with PySpark and Spark ecosystem.\n- Strong understanding of data structures, algorithms, and object-oriented programming.\n- Experience with SQL and relational databases.\n- Familiarity with cloud platforms such as AWS, Azure, or GCP (preferred).\n- Excellent problem-solving and analytical skills.\n- Strong communication and teamwork skills.\nBonus Points:\n- Experience with data visualization tools (e.g., Tableau, Power BI).\n- Knowledge of machine learning and data science concepts.\n- Experience with containerization technologies (e.g., Docker, Kubernetes).\n- Contributions to open-source projects.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Cloud Technologies', 'SQL', 'Python']",2025-06-12 05:40:55
Principal AI Architect,Wabtec,10 - 15 years,Not Disclosed,['Bengaluru'],"How will you make a difference?\n\nWe are seeking a collaborative and highly motivated Principal AI Architect to lead our AI team, drive innovation, and enhance customer experiences through impactful artificial intelligence solutions. As a member of the Wabtec IT Data & Analytics (DnA) Team, you will be responsible for:\n\nProviding strategic leadership and direction in the development, articulation and implementation, of a comprehensive AI/ML/ Data Transformation Roadmap for Wabtec aligned with the overall business objectives.\nWorking with other AI champions in Wabtec evaluating AI tools/ technologies/ frameworks, champion adoption in different projects and demonstrate value for business and customers.\nActively collaborating with various stakeholders to align AI initiatives in Cloud Computing environments (e.g., AWS, Azure, OCI) with business goals.\nProviding technical oversight on AI projects to drive performance output to meet KPI metrics in Productivity and Quality.\nServing as contact and interface with external partners and industry leaders for collaborations in AI/LLM/ Generative AI.\nArchitecting and deploying scalable AI solutions that integrate seamlessly with existing business and IT infrastructure.\nDesign and architect AI as a service, to enable collaboration btw multiple teams in delivering AI solutions\nOptimizing state-of-the-art algorithms in distributed environments\nCreate clear and concise communications/recommendations for senior leadership review related to AI strategic business plans and initiatives.\nStaying abreast of advancements in AI, machine learning, and data science to continuously innovate and improve solutions and bring the external best practices for adoption in Wabtec\nImplementing best practices for AI designing, testing, deployment, and maintenance\nDiving deep into complex business problems and immerse yourself in Wabtec data & outcomes.\nMentoring a team of data scientists, fostering growth and performance.\nDeveloping AI governance frameworks with ethical AI practices and ensuring compliance with data protection regulations and ensuring responsible AI development.\n\nWhat do we want to know about you?\n\nYou must have:\nThe minimum qualifications for this role include:\n\nPh.D., M.S., or Bachelor's degree in Statistics, Machine Learning, Operations Research, Computer Science, Economics, or a related quantitative field\n5+ years of experience developing and supporting AI products in a production environment with 12+ years of proven relevant experience\n8+ years of experience managing and leading data science teams initiatives at enterprise level\nProfound knowledge of modern AI and Generative AI technologies\nExtensive experience in designing, implementing, and maintaining AI systems\nEnd-to-end expertise in AI/ML project lifecycle, from conception to large-scale production deployment\nProven track record as an Architect with cloud computing environments (e.g., AWS, Azure, OCI) and distributed computing platforms, including containerized deployments using technologies such as Amazon EKS (Elastic Kubernetes Service)\nExpertise with Hands-On experience into Python, AWS AI tech-stack (Bedrock Services, Foundation models, Textract, Kendra, Knowledge Bases, Guard rails, Agents etc.), ML Flow, Image Processing, NLP/Deep Learning, PyTorch /TensorFlow, LLMs integration with applications.\n\nPreferred qualifications for this role include:\nProven track record in building and leading high-performance AI teams, with expertise in hiring, coaching, and developing engineering leaders, data scientists, and ML engineers\nDemonstrated ability to align team vision with strategic business goals, driving impactful outcomes across complex product suites for diverse, global customers\nStrong stakeholder management skills, adept at influencing and unifying cross-functional teams to achieve successful project outcomes\nExtensive hands-on experience with enterprise-level Python development, PyData stack, Big Data technologies, and machine learning model deployment at scale\nProficiency in cutting-edge AI technologies, including generative AI, open-source frameworks, and third-party solutions (e.g., OpenAI)\nMastery of data science infrastructure and tools, including code versioning (Git), containerization (Docker), and modern AI/ML tech stacks\nPreferred: AWS with AWS AI services.\n\nWe would love it if you had:\n\nFluent with experimental design and the ability to identify, compute and validate the appropriate metrics to measure success\nDemonstrated success working in a highly collaborative technical environment (e.g., code sharing, using revision control, contributing to team discussions/workshops, and collaborative documentation)\nPassion and aptitude for turning complex business problems into concrete hypotheses that can be answered through rigorous data analysis and experimentation\nDeep expertise in analytical storytelling and stellar communications skills\nDemonstrated success mentoring junior teammates & helping develop peers\n\nWhat will your typical day look like?\n\nStakeholder Engagement: Collaborate with our Internal stakeholders to understand their needs, update on a specific project progress, and align our AI initiatives with business goals.\nUse Generative AI and machine learning techniques and build LLM Models & fine-tuning, Image processing, NLP, model integration with new/existing applications, and improve model performance/accuracy along with cost effective solutions.\nSupport AI Team: Guide and mentor the AI team, resolving technical issues and provide suggestions.\nReporting & Strategy: Generate and present reports to senior leadership, develop strategic insights, and stay updated on industry trends.\nBuilding AI roadmap for Wabtec and discussion with senior leadership\nTraining, Development & Compliance: Organize training sessions, manage resources efficiently, ensure data accuracy, security, and compliance with best practices.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Artificial Intelligence', 'Cloud Architecture', 'Eks', 'Machine Learning', 'Aiml']",2025-06-12 05:40:57
AVP - Finance Analyst,Wells Fargo,2 - 7 years,Not Disclosed,['Bengaluru'],"About this role:\nWells Fargo is seeking a Finance Analyst\n\nIn this role, you will:\nParticipate in functions related to financial research and reporting\nForecast analysis of key metrics, as well as other financial consulting related to business performance, operating and strategic reviews",,,,"['financial research', 'Data analysis', 'Project management', 'documentation', 'Gap analysis', 'financial consulting', 'SQL']",2025-06-12 05:41:00
Senior Consultant/Engagement Manager - Technology Consulting,Tiger Analytics,7 - 12 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","About the role\n\n\nAs a Technology Consulting, you will be a key member of the teams working on end-to-end AI and analytics solutions being built for our clients. While your primary focus will be on the data engineering and application engineering components of the overall solution, you will collaborate with internal data science teams for the ML aspects. As the person responsible for the overall success of the program, you will:",,,,"['Program Management', 'Data Engineering', 'Technology Consulting', 'ETL', 'RFP', 'Delivery Lead', 'Azure', 'SOW']",2025-06-12 05:41:02
Facets System Integration,Cognizant,5 - 8 years,Not Disclosed,['Chennai'],Job Summary\nWe are seeking a Sr. Product Specialist (T) with 5 to 8 years of experience to join our team. The ideal candidate will have expertise in Facets-Technical ANSI SQL and MS SQL Server. Experience in the Payer domain is a plus. This hybrid role requires working during day shifts with no travel required.,,,,"['fact', 'data management', 'analytical', 'technical', 'ansi sql', 'payments', 'documentation', 'database administration', 'sql server', 'ansi', 'sql', 'database optimization', 'database management', 'system integration', 'collaboration', 'aspect', 'product development', 'alignment', 'technical skills', 'communication skills']",2025-06-12 05:41:05
EPBCS Principal Consultant//EPBCS Senior Consultant,Leading MNC,4 - 9 years,25-40 Lacs P.A.,[],"Leading MNC in Bangalore & Kochi\nHiring for EPBCS Principal Consultant/EPBCS Senior Consultant\nImmediate Joiners///Lesser notice period\nExperience 4-10 Years\n. The ideal candidate will have a strong background in Oracle Cloud EPM, particularly PBCS/EPBCS, and experience across multiple full-cycle implementation projects. You should be proficient in configuring out-of-the- box modules, writing Groovy scripts, and integrating data using tools like FDMEE, Data Management, or ODI. A solid understanding of financial and functional processes, coupled with excellent communication and documentation skills, is essential. Experience in public sector implementations and automation using EPM Automate is highly desirable.\n\nKey Responsibilities\nLead 1- 4 full-cycle Oracle Cloud EPM (PBCS/EPBCS) implementations.\nGather business requirements and create functional/technical specifications.\nConfigure and customize EPBCS modules (Financials, Workforce, Capex, Projects, Strategic Modeling).\nDevelop Groovy scripts, business rules, and automation using EPM Automate.\nManage data integrations using FDMEE, Data Management, or ODI.\nConduct system testing, troubleshoot issues, and support users post-go-live.\nBuild reports using Excel, SmartView, and Essbase tools.\nWork independently and document solutions clearly.\nCollaborate with clients and cross-functional teams to ensure project success.\nExperience in public sector implementations is a plus.\n\nRequired Skills & Qualifications\nRelevant experience ranging from 4 to 10 years\nExperience should comprise of exposure to at least 2-4 full project life cycles (Development projects)\nSolid experience with Oracle Cloud EPM (preferably PBCS/EPBCS)\nExtensive experience in analyzing requirements, writing functional specifications, conducting tests, troubleshooting issues and interfacing with business users\nExperience in Groovy scripting.\nExperience in out of the box modules Financials, Workforce, Capex, Projects and Strategic Modelling\nImplementing EPBCS at Public Sectors will be an added advantage.\nSpecific knowledge of integration tools such as; FDMEE, Data Management or ODI (Oracle Data Integrator)\nExperience with automation scripts such as EPM Automate\nExtensive knowledge of Excel, Essbase Spreadsheet Add-in, and SmartView\nExcellent written and communication skills\nKnowledge of financial process and functional processes\nAbility to work independently with minimum guidance\nStrong technical documentation skills\nExperience in a project team environment using structured methodologies for gathering business requirements, business processes, data conversions and system interfaces\n\nInterested Candidates can mail their cv at simmi@hiresquad.in or call at 8467054123",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['EPM', 'Epbcs', 'Smartview', 'Fdme', 'Oracle Epm', 'Essbase', 'Fccs']",2025-06-12 05:41:07
Campaign Management Lead,Indegene,8 - 10 years,11-18 Lacs P.A.,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']","We are a technology-led healthcare solutions provider. We are driven by our purpose to enable healthcare organizations to be future-ready. We offer accelerated, global growth opportunities for talent thats bold, industrious, and nimble. With Indegene, you gain a unique career experience that celebrates entrepreneurship and is guided by passion, innovation, collaboration, and empathy. To explore exciting opportunities at the convergence of healthcare and technology, check out www.careers.indegene.com\nLooking to jump-start your career?\nWe understand how important the first few years of your career are, which create the foundation of your entire professional journey. At Indegene, we promise you a differentiated career experience. You will not only work at the exciting intersection of healthcare and technology but also will be mentored by some of the most brilliant minds in the industry. We are offering a global fast-track career where you can grow along with Indegenes high-speed growth.\nWe are purpose-driven.  We enable healthcare organizations to be future ready and our customer obsession is our driving force. We ensure that our customers achieve what they truly want. We are bold in our actions, nimble in our decision-making, and industrious in the way we work.\n\nCampaign Management Lead - SFMC\nYou will be responsible for: \nLead the design and implementation of scalable SFMC solutions.\nDevelop and optimize data models, audience segmentation, and automation strategies.\nWork closely with marketing, sales, and IT teams to gather business requirements.\nProvide best practice recommendations for marketing automation.\nLead and mentor SFMC developers, consultants, and administrators.\nWork with REST/SOAP APIs, FTPs, and external data sources.\nConduct code reviews and enforce development best practices.\nExperience in Email, Mobile and web studio.\nLead design and development of customer journeys on marketing automation platforms.\nAbility to understand clients business objective and come up with a good campaign plan proposal that includes various options for audience creation, journey and content.\nAsk right questions and help clients come up with unambiguous campaign objectives. \nPresent information in a crisp and business friendly manner (good presentation skills PowerPoint).\nCollaborate with project management, content, data engineering and analytics teams in designing campaigns, finalizing assets and other operational activities.\nLead and nurture a team of campaign developers while maintaining operational excellence across all email/campaign projects.\nDrive innovation and thought leadership in the field of marketing automation and multi-channel marketing\nCritically analysing campaigns and identifying gaps in technical set up. \nLeverage past experience and come up with robust QA processes for campaign testing. \nExpertise in creating meaningful cross-channel campaign performance dashboards (in SFMC, Adobe, or externally) to report integrated campaign performance\nHelp business in deriving insights based on metrics reported in campaign dashboards\n\nYour impact: \nAbout you: \nDemonstrate good understanding of multi-channel marketing campaigns business processes (pharma experience would be a big plus). \nIs a very good communicator/articulator who is comfortable is switching between business and technical conversations. \nAbility to look at bigger picture (while talking to clients) and at the same time be detail-oriented (while working with internal team).\nShould be SME in audience management, content builder, journey builder and performance management of MCM campaign (email + other integrated channels such as Tele, Website and Social) \nGood understanding of domain and IP management for Email Engines. \nKnowledge of SPAM filters, tackling SPAM issues, Domain warm up, IP warm up, Spam filter criteria in major ESPs.\nStrong experience in SQL and excel is a MUST \nHas experience in managing a campaign execution technical team (a team of SFMC tech guys)\nAbility to set up campaign development and QA processes is preferred.\nMust have:\n8 years experience towards multiple marketing automation platforms like SFMC and Adobe campaigns, Marketo, etc. \nCertified ""SFMC Cloud Consultant"".\nIndegene is proud to be an Equal Employment Employer and is committed to the culture of Inclusion and Diversity. We do not discriminate on the basis of race, religion, sex, colour, age, national origin, pregnancy, sexual orientation, physical ability, or any other characteristics. All employment decisions, from hiring to separation, will be based on business requirements, the candidates merit and qualification. We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, colour, religion, sex, national origin, gender identity, sexual orientation, disability status, protected veteran status, or any other characteristics.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation Studio', 'api', 'Sfmc', 'Mobile Studio', 'Email Studio', 'Marketing Automation', 'Journey Builder', 'Soap And Rest Api']",2025-06-12 05:41:09
Manager - BIM,Axtria,10 - 15 years,Not Disclosed,['Bengaluru'],"Position Summary \n\nLooking for a Salesforce Data Cloud Engineer to design, implement, and manage data integrations and solutions using Salesforce Data Cloud (formerly Salesforce CDP). This role is essential for building a unified, 360-degree view of the customer by integrating and harmonizing data across platforms.\n\n Job Responsibilities \n\nConsolidate the Customer data to create a Unified Customer profile\nDesign and implement data ingestion pipelines into Salesforce Data Cloud from internal and third-party systems .\nWork with stakeholders to define Customer 360 data model requirements, identity resolution rules, and calculated insights.\nConfigure and manage the Data Cloud environment, including data streams, data bundles, and harmonization.\nImplement identity resolution, micro segmentation, and activation strategies.\nCollaborate with Salesforce Marketing Cloud, to enable real-time personalization and journey orchestration.\nEnsure data governance, and platform security.\nMonitor data quality, ingestion jobs, and overall platform performance.\n\n\n Education \n\nBE/B.Tech\nMaster of Computer Application\n\n Work Experience \nOverall experience of minimum 10 years in Data Management and Data Engineering role, with a minimum experience of 3 years as Salesforce Data Cloud Data Engineer\nHands-on experience with Salesforce Data Cloud (CDP), including data ingestion, harmonization, and segmentation.\nProficient in working with large datasets, data modeling, and ETL/ELT processes.\nUnderstanding of Salesforce core clouds (Sales, Service, Marketing) and how they integrate with Data Cloud.\nExperience with Salesforce tools such as Marketing Cloud.\nStrong knowledge of SQL, JSON, Apache Iceberg and data transformation logic.\nFamiliarity with identity resolution and customer 360 data unification concepts.\nSalesforce certifications (e.g., Salesforce Data Cloud Accredited Professional, Salesforce Administrator, Platform App Builder).\nExperience with CDP platforms other than Salesforce (e.g., Segment, Adobe Experience Platform (Good to have)).\nExperience with cloud data storage and processing tools (Azure, Snowflake, etc.).\n\n\n Behavioural Competencies \n\nTeamwork & Leadership\nMotivation to Learn and Grow\nOwnership\nCultural Fit\nTalent Management\n\n Technical Competencies \n\nProblem Solving\nAzure Data Factory\nAzure DevOps\nAzure SQL",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'apache', 'data modeling', 'data transformation', 'etl', 'snowflake', 'navisworks', 'data management', 'bim', 'revit architecture', 'microsoft azure', 'revit mep', 'azure data factory', 'autocad', 'azure devops', 'salesforce', 'talent management', 'sql azure', 'revit', 'json', 'salesforce core']",2025-06-12 05:41:12
Qa Lead,Impetus Technologies,10 - 13 years,Not Disclosed,"['Noida', 'Indore', 'Bengaluru']","Experience - 10 to 13 years\n\nMust Have Expertise :\nSelenium ,  RestEasy or having experience in multiple Automation framework\nAble to identify and Architect  Automation framework from scratch\nCan do high level and low-level framework development grooming, guide team technical / logistically all team members. \nProgramming Python\nProblem solving & logic and analytical abilities\nHands On QA Frameworks BDD, TDD ,Data Driven\nOS (Windows/Linux) concepts\nDBMS concepts & Ability to Write Queries\nCommunication/Confidence/Attitude/Client Interaction\nAble to handle team and client \nTracking of end to end deliverables\nOver all QA processes/Matrix/Reports\n\nGood to have :\nBig Data Technologies\nCloud Technology\nDocument Based Database {MongoDB}\nData Analysis",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Bdd', 'API Testing', 'Pytest', 'Selenium', 'SQL', 'TDD', 'Automation Testing', 'Python']",2025-06-12 05:41:15
YASH Technologies is Hiring - Sr. Consultant - SAP Build Apps,Yash Technologies,1 - 3 years,Not Disclosed,"['Hyderabad', 'Pune']","Develop applications using SAP Build Apps , SAP Build Process Automation, and SAP Build Work Zone.\nIntegrate SAP Build solutions with core SAP systems (SAP S/4HANA, SAP ECC, SAP BTP)\nConfigure and extend SAP Fiori applications using SAP Build tools.\nImplement process automation workflows using SAP Build Process Automation.\nWork with APIs (OData, REST) to enable data integration with SAP and non-SAP systems.\nEnsure application performance, quality, and responsiveness.\nTroubleshoot and resolve technical issues related to SAP Build apps and integrations.",,,,"['SAP ECC', 'SAP Build Apps', 'SAP systems', 'SAP BTP', 'SAP S/4HANA']",2025-06-12 05:41:18
Senior Software Developer - Java & Angular,S&P Global Market Intelligence,7 - 12 years,Not Disclosed,"['Mumbai', 'Maharastra']","Grade Level (for internal use) : - 10\nThe Team\nYou will be an expert contributor and part of the Rating Organizations Data Services Product Engineering Team\nThis team, who has a broad and expert knowledge on Ratings organizations critical data domains, technology stacks and architectural patterns, fosters knowledge sharing and collaboration that results in a unified strategy\nAll Data Services team members provide leadership, innovation, timely delivery, and the ability to articulate business value\nBe a part of a unique opportunity to build and evolve S&P Ratings next gen analytics platform\nResponsibilities:\nDesign and implement innovative software solutions to enhance S&P Ratings' cloud-based data platforms.\nMentor a team of engineers fostering a culture of trust, continuous growth, and collaborative problem-solving.\nCollaborate with business partners to understand requirements, ensuring technical solutions align with business goals.\nManage and improve existing software solutions, ensuring high performance and scalability.\nParticipate actively in all Agile scrum ceremonies, contributing to the continuous improvement of team processes.\nProduce comprehensive technical design documents and conduct technical walkthroughs.\nExperience & Qualifications:\nBachelors degree in computer science, Information Systems, Engineering, equivalent or more is required\nProficient with software development lifecycle (SDLC) methodologies like Agile, Test-driven development\n7+ years of development experience in enterprise products, modern web development technologies Java/J2EE, UI frameworks like Angular, React, SQL, Oracle, NoSQL Databases like MongoDB\nExperience designing transactional/data warehouse/data lake and data integrations with Big data eco system leveraging AWS cloud technologies\nExp. with Delta Lake systems like Databricks using AWS cloud technologies and PySpark is a plus\nThorough understanding of distributed computing\nPassionate, smart, and articulate developer\nQuality first mindset with a strong background and experience with developing products for a global audience at scale\nExcellent analytical thinking, interpersonal, oral and written communication skills with strong ability to influence both IT and business partners\nSuperior knowledge of system architecture, object-oriented design, and design patterns.\nGood work ethic, self-starter, and results-oriented\nExcellent communication skills are essential, with strong verbal and writing proficiencies\nAdditional Preferred Qualifications:\nExperience working AWS\nExperience with SAFe Agile Framework\nBachelor's/PG degree in Computer Science, Information Systems or equivalent.\nHands-on experience contributing to application architecture & designs, proven software/enterprise integration design principles\nAbility to prioritize and manage work to critical project timelines in a fast-paced environment\nExcellent Analytical and communication skills are essential, with strong verbal and writing proficiencies\nAbility to train and mentor\nBenefits:\nHealth & Wellness: Health care coverage designed for the mind and body.\nContinuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\nInvest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\nFamily Friendly Perks: Its not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\nBeyond the Basics: From retail discounts to referral incentive awardssmall perks can make a big difference.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'PySpark', 'Delta Lake systems', 'React', 'Angular', 'UI frameworks', 'SQL', 'NoSQL Databases', 'MongoDB', 'Databricks', 'Oracle', 'AWS', 'AWS cloud']",2025-06-12 05:41:20
EPM Principal/Sr Principal Consultant FCCS/ARCS/TRCS/PBCS/EPBCS/PCMCS,Oracle,10 - 20 years,Not Disclosed,"['Noida', 'Gurugram', 'Chennai']","Hiring for Oracle Experts at different level from 5-20 years.\n\nJob locations - Bangalore, Mumbai, Pune, Hyderabad, Chennai, Kolkata, Noida, Gurgaon, Gandhinagar\n\nEPM Products - ARCS, TRCS, FCCS, EPBCS, PBCS, EPRCS, PCMCS\n\nPlanning - PBCS/ EPBCS\nExperience in Implementation of Hyperion with strong Application Development process experience on Hyperion EPM Product Suite.\nExperience in Requirement Gathering & Solution Design\nSound knowledge on Hyperion Planning/PBCS/EPBCS\nSound functional knowledge (Understand of planning modelling like P&L, BS, Workforce, Capex planning etc.. and inter dependencies)\nSound Knowledge on Business Rules/Forms / Task Lists / Reports.\nHands on Experience on Planning Modules is must.\nGood communication Skills\n\nFCCS\n\nFunction as applications design architect/Lead for Oracle FCCS\nApplication Design point of contact for FCCS Analyst Teams\nProvide Solutions to existing Architecture Design on the current system\nCollaborate effectively with other groups\nEPM Experience 5+ Years\nExperience in Implementation of EPM cloud with strong Application Development process, experience on FCCS/HFM and good knowledge on consolidation process.\nExperience in Requirement Gathering & Solution Design\nDesired functional knowledge (Understand of Income statement, Balance Sheet, different methods of consolidation and their calculations and disclosure in financial statements)\nSound functional knowledge Finance/accounting/ General Ledger/Sub Ledgers\nSound Knowledge on Financial Reports and SmartView Reports\n\nARCS\nExperience implementing ARCS from design, configuration, data integration, and testing\nSound knowledge on ARM/ARCS including Reconciliation Compliance & Transaction Matching\nFunctional knowledge of Finance/accounting and account reconciliation is a must\nKnowledge and experience working with a consolidation tool and general ledger is a plus\nProvide Solutions to existing Architecture Design on current system\nCollaborate effectively with other groups\nEPM Experience 5+ Years\nExperience in Implementation of Hyperion with strong Application Development process experience on Hyperion EPM Product Suite.\nExperience in Requirement Gathering & Solution Design\nSound functional knowledge Finance/accounting/ General Ledger/Sub Ledgers\nSound Knowledge on standard and custom reports\n\nTRCS\nFunction as applications design architect/Lead for Tax Reporting Cloud application development\nApplication Design point of contact for Tax Reporting Teams\nProvide Solutions to existing Architecture Design on current system\nCollaborate effectively with other groups\nEPM Experience 5+ Years\nExperience in Implementation of Hyperion with strong Application Development process experience on Hyperion EPM Product Suite.\nExperience in Requirement Gathering & Solution Design\nSound knowledge on Tax reporting and compliance processes, tax accounting and direct tax functions, CBCR and Deferred Tax Calculations\nSound knowledge on Hyperion Consolidation\nDesired functional knowledge (Understand of Income statement, Balance Sheet, different methods of consolidation and their calculations and disclosure in financial statements)\nSound Knowledge on Business Rules/Forms / Task Lists / Reports.\nGood communication Skills\n\nPCMCS\nFunction as applications design architect/Lead for Profitability and Cost Management\nApplication Design point of contact Profitability and Cost Management\nProvide Solutions to existing Architecture Design on current system\nAble to understand functional requirement of client and build the solution accordingly.\nCollaborate effectively with other groups\nEPM Experience 5+ Years\nShould have completed at least 3 implementations on PCMCS.\nIn depth understanding of Oracle Hyperion Essbase (ASO and BSO)\nIn depth knowledge in Integration (Data Management)\nAbility to design and develop complex Reports using Web Reporting Studio.\nExperience in Implementation of Hyperion with strong Application Development process experience on Hyperion EPM Product Suite.\nExperience in Requirement Gathering & Solution Design\nAble to leverage the modern best practices for design-development and automation process wherever required.\nSound functional knowledge\nKnowledge on Microsoft office tools including Excel, Word and Power point, leverage Smartview to build report and ad hoc analysis.\nPCMCS Certification is a value add",Industry Type: Software Product,Department: Consulting,"Employment Type: Full Time, Permanent","['EPM', 'Cloud EPM', 'solution design', 'consulting', 'implementation', 'EPBCS', 'EPRCS', 'PBCS', 'TRCS', 'PCMCS', 'FCCS', 'ARCS']",2025-06-12 05:41:23
Walkin || Cognizant is hiring For Abinitio developer,Cognizant,6 - 9 years,Not Disclosed,['Kochi'],"Greetings from Cognizant!! #MegaWalkIn\n\nWe have an exciting opportunity for the #Abinitio Developer Role with Cognizant, join us if you are an aspirant for matching the below criteria!!\n\nPrimary Skill: Abinitio Developer\nExperience: 6-9 years\nJob Location: PAN India",,,,"['Abinitio Graphs', 'Initio', 'Ab Initio']",2025-06-12 05:41:26
Technical Architect,PwC India,10 - 15 years,Not Disclosed,"['Mumbai', 'Navi Mumbai', 'Gurugram']","Role Description\nWe are looking for a suitable candidate for the opening of Data/Technical Architect role for Data Management, preferably for one who has worked in Insurance or Banking and Financial Services domain and holds relevant experience of 10+ years. The candidate should be willing to take up the role of Senior Manager/Associate Director in an organization based on overall experience.\nLocation : Mumbai and Gurugram\nRelevant experience : 10+ years",,,,"['Data Architecture', 'Technical Architecture', 'Java', 'Bigquery', 'SCALA', 'Data Lake', 'Data Warehousing', 'Data Modeling', 'Data Bricks', 'Python']",2025-06-12 05:41:28
Solution Architect - L1,Wipro,8 - 10 years,Not Disclosed,['Pune'],"Role Purpose\n\nThe purpose of the role is to create exceptional architectural solution design and thought leadership and enable delivery teams to provide exceptional client engagement and satisfaction.\n\n\n\n\n\nDo\n1.Develop architectural solutions for the new deals/ major change requests in existing deals\nCreates an enterprise-wide architecture that ensures systems are scalable, reliable, and manageable.\nProvide solutioning of RFPs received from clients and ensure overall design assurance\nDevelop a direction to manage the portfolio of to-be-solutions including systems, shared infrastructure services, applications in order to better match business outcome objectives\nAnalyse technology environment, enterprise specifics, client requirements to set a collaboration solution design framework/ architecture\nProvide technical leadership to the design, development and implementation of custom solutions through thoughtful use of modern technology\nDefine and understand current state solutions and identify improvements, options & tradeoffs to define target state solutions\nClearly articulate, document and sell architectural targets, recommendations and reusable patterns and accordingly propose investment roadmaps\nEvaluate and recommend solutions to integrate with overall technology ecosystem\nWorks closely with various IT groups to transition tasks, ensure performance and manage issues through to resolution\nPerform detailed documentation (App view, multiple sections & views) of the architectural design and solution mentioning all the artefacts in detail\nValidate the solution/ prototype from technology, cost structure and customer differentiation point of view\nIdentify problem areas and perform root cause analysis of architectural design and solutions and provide relevant solutions to the problem\nCollaborating with sales, program/project, consulting teams to reconcile solutions to architecture\nTracks industry and application trends and relates these to planning current and future IT needs\nProvides technical and strategic input during the project planning phase in the form of technical architectural designs and recommendation\nCollaborates with all relevant parties in order to review the objectives and constraints of solutions and determine conformance with the Enterprise Architecture\nIdentifies implementation risks and potential impacts\n2.Enable Delivery Teams by providing optimal delivery solutions/ frameworks\nBuild and maintain relationships with executives, technical leaders, product owners, peer architects and other stakeholders to become a trusted advisor\nDevelops and establishes relevant technical, business process and overall support metrics (KPI/SLA) to drive results\nManages multiple projects and accurately reports the status of all major assignments while adhering to all project management standards\nIdentify technical, process, structural risks and prepare a risk mitigation plan for all the projects\nEnsure quality assurance of all the architecture or design decisions and provides technical mitigation support to the delivery teams\nRecommend tools for reuse, automation for improved productivity and reduced cycle times\nLeads the development and maintenance of enterprise framework and related artefacts\nDevelops trust and builds effective working relationships through respectful, collaborative engagement across individual product teams\nEnsures architecture principles and standards are consistently applied to all the projects\nEnsure optimal Client Engagement\nSupport pre-sales team while presenting the entire solution design and its principles to the client\nNegotiate, manage and coordinate with the client teams to ensure all requirements are met and create an impact of solution proposed\nDemonstrate thought leadership with strong technical capability in front of the client to win the confidence and act as a trusted advisor\n\n\n\n\n\n3.Competency Building and Branding\nEnsure completion of necessary trainings and certifications\nDevelop Proof of Concepts (POCs),case studies, demos etc. for new growth areas based on market and customer research\nDevelop and present a point of view of Wipro on solution design and architect by writing white papers, blogs etc.\nAttain market referencability and recognition through highest analyst rankings, client testimonials and partner credits\nBe the voice of Wipros Thought Leadership by speaking in forums (internal and external)\nMentor developers, designers and Junior architects in the project for their further career development and enhancement\nContribute to the architecture practice by conducting selection interviews etc\n\n\n\n\n\n4.Team Management\nResourcing\nAnticipating new talent requirements as per the market/ industry trends or client requirements\nHire adequate and right resources for the team\nTalent Management\nEnsure adequate onboarding and training for the team members to enhance capability & effectiveness\nBuild an internal talent pool and ensure their career progression within the organization\nManage team attrition\nDrive diversity in leadership positions\nPerformance Management\nSet goals for the team, conduct timely performance reviews and provide constructive feedback to own direct reports\nEnsure that the Performance Nxt is followed for the entire team\nEmployee Satisfaction and Engagement\nLead and drive engagement initiatives for the team\nTrack team satisfaction scores and identify initiatives to build engagement within the team\nMandatory Skills: DataBricks - Data Engineering.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['DataBricks', 'Team Management', 'Data Engineering', 'solution design', 'Employee Engagement', 'Performance Management']",2025-06-12 05:41:30
Walkin || Cognizant is hiring For AWS Services,Cognizant,6 - 9 years,Not Disclosed,['Kochi'],"Greetings from Cognizant!! #MegaWalkIn\n\nWe have an exciting opportunity for the #AWS Services Role with Cognizant, join us if you are an aspirant for matching the below criteria!!\n\nPrimary Skill: AWS Services\nExperience: 6-9 years\nJob Location: PAN India",,,,"['AWS Services', 'Aws Lambda', 'Aws Glue']",2025-06-12 05:41:33
Regulatory Affairs Specialist,HCLTech,2 - 4 years,Not Disclosed,"['Madurai', 'Chennai']","Education: B.E Mechanical/Bio-Medical\nExperience: 2.5 to 5 yrs Knowledge on Medical device regulations Good Communication skills Ability to read through the Medical Device Documents Ability to work on Microsoft tools (Excel, Word and PowerPoint) Experience on Medical Device UDI Data management will be additional preference.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MDR', 'ISO 13485', 'Regulatory Affairs', 'Medical Devices']",2025-06-12 05:41:36
Walkin || Cognizant is hiring For Databricks developer,Cognizant,4 - 9 years,Not Disclosed,['Kochi'],"Greetings from Cognizant!! #MegaWalkIn\n\nWe have an exciting opportunity for the #Databricks Developer Role with Cognizant, join us if you are an aspirant for matching the below criteria!!\n\nPrimary Skill: Databricks Developer\nExperience: 4-9 years\nJob Location: PAN India",,,,"['Azure Databricks', 'Data Bricks', 'ADB']",2025-06-12 05:41:38
Manager - Software Development,Amway,8 - 12 years,Not Disclosed,['Hyderabad'],"Primary Responsibilities\nCloud Expertise: Familiarity or hands-on experience with AWS and Google Cloud Platform (GCP) technologies to support data transformation, data structures, metadata management, dependency tracking, and workload orchestration.\nCollaboration & Independence: Self-motivated and capable of supporting the data needs of multiple teams, systems, and products within Amways data ecosystem.\nBig Data & Distributed Systems: Strong understanding of distributed systems for large-scale data processing and analytics, with a proven track record of manipulating, processing, and deriving insights from large, complex, and disconnected datasets.",,,,"['Data Transformation', 'GCP', 'Cloud', 'AWS']",2025-06-12 05:41:40
Offshore Program Manager,"NTT DATA, Inc.",12 - 14 years,Not Disclosed,['Pune'],"Req ID: 326837\n\nWe are currently seeking a Offshore Program Manager to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nJob DutiesOffshore BA / PM\nGrade 10/11\nSeeking an offshore program manager for managing data projects for Australia, India, and APAC region.\n\nThe individual must be able to guide the offshore project team located in different locations across India. The role involves planning, executing, and overseeing data related projects ensuring on time and within budget delivery.\n\nKey roles and responsibilities ""“\n""¢ Requirement Analysis ""“ Act as a bridge between technical development team and onshore team to understand and document the project requirements\n""¢ Project / Program Management ""“\no Project planning, timelines, and resource allocation / management\no Managing project risks related to quality, budget, and resources\no Team Management ""“ Lead and motivate offshore teams including data engineers, QA team, and data SMEs resolving potential issues that may arise during execution\no Budget management ""“ Manage project financials and budgets\no Compliance and standards management\no Risk management ""“ Identify and mitigate potential risks like scope creep, timelines etc.\no Ability to track and monitor project KPIs like defect density, Project health index etc.\n""¢ Stakeholder communication ""“\no Excellent communication and interpersonal skills with onshore / offshore / nearshore teams\no Client communication with F2F client meetings and presentations\no Ability to communicate with senior management within NTTDATA\n\nPreferred Skills and Qualifications ""“\n""¢ Overall, 12-14 years of work experience\n""¢ 7+ years of enterprise Technical Program Management experience supporting data projects.\n""¢ Data lifecycle management ""“ Understanding of data lifecycle management principles including data acquisition, ingestion, data quality, data consumption, and data visualization\n""¢ Exposure to AI and Gen AI fundamental concepts\n""¢ 7+ years of experience with Agile and Waterfall methodologies\n""¢ Ability to travel at least 25%\n""¢ Graduate degree or equivalent combination of education and work experience.\n""¢ Undergraduate or Graduate degree preferred\n\nMinimum Skills RequiredOffshore BA / PM\nGrade 10/11\nSeeking an offshore program manager for managing data projects for Australia, India, and APAC region.\n\nThe individual must be able to guide the offshore project team located in different locations across India. The role involves planning, executing, and overseeing data related projects ensuring on time and within budget delivery.\n\nKey roles and responsibilities ""“\n""¢ Requirement Analysis ""“ Act as a bridge between technical development team and onshore team to understand and document the project requirements\n""¢ Project / Program Management ""“\no Project planning, timelines, and resource allocation / management\no Managing project risks related to quality, budget, and resources\no Team Management ""“ Lead and motivate offshore teams including data engineers, QA team, and data SMEs resolving potential issues that may arise during execution\no Budget management ""“ Manage project financials and budgets\no Compliance and standards management\no Risk management ""“ Identify and mitigate potential risks like scope creep, timelines etc.\no Ability to track and monitor project KPIs like defect density, Project health index etc.\n""¢ Stakeholder communication ""“\no Excellent communication and interpersonal skills with onshore / offshore / nearshore teams\no Client communication with F2F client meetings and presentations\no Ability to communicate with senior management within NTTDATA\n\nPreferred Skills and Qualifications ""“\n""¢ Overall, 12-14 years of work experience\n""¢ 7+ years of enterprise Technical Program Management experience supporting data projects.\n""¢ Data lifecycle management ""“ Understanding of data lifecycle management principles including data acquisition, ingestion, data quality, data consumption, and data visualization\n""¢ Exposure to AI and Gen AI fundamental concepts\n""¢ 7+ years of experience with Agile and Waterfall methodologies\n""¢ Ability to travel at least 25%\n""¢ Graduate degree or equivalent combi",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['artificial intelligence', 'data quality', 'technical program management', 'waterfall', 'agile', 'data life cycle management', 'risk management', 'consumables', 'program management', 'budgeting', 'resource allocation', 'data acquisition', 'data visualization', 'project planning']",2025-06-12 05:41:42
CTO Analyst,Pepsico,5 - 10 years,Not Disclosed,['Hyderabad'],"Overview \n\nPrimary focus would be to perform development work within Azure Data Lake environment and other related ETL technologies, with the responsibility of ensuring on time and on budget delivery; Satisfying project requirements, while adhering to enterprise architecture standards. This role will also have L3 responsibilities for ETL processes\n\n Responsibilities \n\nDelivery of key Azure Data Lake projects within time and budget\nContribute to solution design and build to ensure scalability, performance and reuse of data and other components Delivery of key Azure Data Lake projects within time and budget\nEnsure on time and on budget delivery which satisfies project requirements, while adhering to enterprise architecture standards.\nPossess strong problem-solving abilities with a focus on managing to business outcomes through collaboration with multiple internal and external parties\nEnthusiastic, willing, able to learn and continuously develop skills and techniques enjoys change and seeks continuous improvement\nA clear communicator both written and verbal with good presentational skills, fluent and proficient in the English language\nCustomer focused and a team player\n\n Qualifications \n\nBachelors degree in Computer Science, MIS, Business Management, or related field\n5+ years experience in Information Technology\n4+ years experience in Azure Data Lake Bachelors degree in Computer Science, MIS, Business Management, or related field\n\nTechnical Skills\nProven experience development activities in Data, BI or Analytics projects\nSolutions Delivery experience - knowledge of system development lifecycle, integration, and sustainability\nStrong knowledge of Pyspark and SQL\nGood knowledge of Azure data factory or Databricks\nKnowledge of Presto / Denodo is desirable\nKnowledge of FMCG business processes is desirable\n\nNon-Technical Skills\nExcellent remote collaboration skills\nExperience working in a matrix organization with diverse priorities\nExceptional written and verbal communication skills along with collaboration and listening skills\nAbility to work with agile delivery methodologies\nAbility to ideate requirements & design iteratively with business partners without formal requirements documentation",Industry Type: Beverage,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['information technology', 'pyspark', 'sql', 'azure data lake', 'etl process', 'hive', 'azure data factory', 'matrix', 'microservices', 'plsql', 'spring', 'java', 'unix shell scripting', 'spark', 'hadoop', 'etl', 'big data', 'python', 'oracle', 'denodo', 'english language', 'data bricks', 'agile', 'aws', 'unix', 'presto']",2025-06-12 05:41:45
Consultant,Amdocs,4 - 9 years,Not Disclosed,['Pune'],"Amdocs helps those who build the future to make it amazing. With our market-leading portfolio of software products and services, we unlock our customers innovative potential, empowering them to provide next-generation communication and media experiences for both the individual end user and enterprise customers. Our employees around the globe are here to accelerate service providers migration to the cloud, enable them to differentiate in the 5G era, and digitalize and automate their operations. Listed on the NASDAQ Global Select Market, Amdocs had revenue of $5.00 billion in fiscal 2024. For more information, visit www.amdocs.com\n\n\nIn one sentence\n\nWe are seeking a Data Engineer with advanced expertise in Databricks SQL, PySpark, Spark SQL, and workflow orchestration using Airflow. The successful candidate will lead critical projects, including migrating SQL Server Stored Procedures to Databricks Notebooks, designing incremental data pipelines, and orchestrating workflows in Azure Databricks\n\n\nWhat will your job look like\n\nMigrate SQL Server Stored Procedures to Databricks Notebooks, leveraging PySpark and Spark SQL for complex transformations.\nDesign, build, and maintain incremental data load pipelines to handle dynamic updates from various sources, ensuring scalability and efficiency.\nDevelop robust data ingestion pipelines to load data into the Databricks Bronze layer from relational databases, APIs, and file systems.\nImplement incremental data transformation workflows to update silver and gold layer datasets in near real-time, adhering to Delta Lake best practices.\nIntegrate Airflow with Databricks to orchestrate end-to-end workflows, including dependency management, error handling, and scheduling.\nUnderstand business and technical requirements, translating them into scalable Databricks solutions.\nOptimize Spark jobs and queries for performance, scalability, and cost-efficiency in a distributed environment.\nImplement robust data quality checks, monitoring solutions, and governance frameworks within Databricks.\nCollaborate with team members on Databricks best practices, reusable solutions, and incremental loading strategies\n\n\nAll you need is...\n\nBachelor s degree in computer science, Information Systems, or a related discipline.\n4+ years of hands-on experience with Databricks, including expertise in Databricks SQL, PySpark, and Spark SQL.\nProven experience in incremental data loading techniques into Databricks, leveraging Delta Lake's features (e.g., time travel, MERGE INTO).\nStrong understanding of data warehousing concepts, including data partitioning, and indexing for efficient querying.\nProficiency in T-SQL and experience in migrating SQL Server Stored Procedures to Databricks.\nSolid knowledge of Azure Cloud Services, particularly Azure Databricks and Azure Data Lake Storage.\nExpertise in Airflow integration for workflow orchestration, including designing and managing DAGs.\nFamiliarity with version control systems (e.g., Git) and CI/CD pipelines for data engineering workflows.\nExcellent analytical and problem-solving skills with a focus on detail-oriented development.\n  Preferred Qualifications  \nAdvanced knowledge of Delta Lake optimizations, such as compaction, Z-ordering, and vacuuming.\nExperience with real-time streaming data pipelines using tools like Kafka or Azure Event Hubs.\nFamiliarity with advanced Airflow features, such as SLA monitoring and external task dependencies.\nCertifications such as Databricks Certified Associate Developer for Apache Spark or equivalent.\nExperience in Agile development methodologie\n\n\nWhy you will love this job:\nYou will be able to use your specific insights to lead business change on a large scale and drive transformation within our organization.\nYou will be a key member of a global, dynamic and highly collaborative team with various possibilities for personal and professional development.\nYou will have the opportunity to work in multinational environment for the global market leader in its field!\nWe offer a wide range of stellar benefits including health, dental, vision, and life insurance as well as paid time off, sick time, and parental leave!",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure databricks', 'airflow', 'pyspark', 'sql', 'spark', 'azure cloud services', 'continuous integration', 'azure data lake', 'workflow orchestration', 'ci/cd', 'warehouse', 't-sql', 'sql server', 'stored procedures', 'data bricks', 'git', 'kafka', 'data warehousing concepts', 'agile']",2025-06-12 05:41:47
Microsoft Fabric Specialist,"NTT DATA, Inc.",5 - 10 years,Not Disclosed,['Hyderabad'],"Req ID: 326727\n\nWe are currently seeking a Microsoft Fabric Specialist to join our team in Hyderabad, Telangana (IN-TG), India (IN).\n\n\n\n:\n\nWe are seeking a Mid-Level Microsoft Fabric Support Specialist to join our IT team. The ideal candidate will be responsible for providing technical support, troubleshooting, and ensuring the smooth operation of Microsoft Fabric services.\n\nThis role requires a deep understanding of Microsoft Fabric, data integration, and analytics solutions, along with strong problem-solving skills.\n\nKey Responsibilities:\n\n""¢ Provide technical support and troubleshooting for Microsoft Fabric services.\n\n""¢ Assist in the implementation, configuration, and maintenance of Microsoft Fabric environments.\n\n""¢ Monitor system performance and resolve issues proactively.\n\n""¢ Collaborate with cross-functional teams to optimize data workflows and analytics solutions.\n\n""¢ Document support procedures, best practices, and troubleshooting steps.\n\n""¢ Assist in user training and onboarding for Microsoft Fabric-related tools and applications.\n\n""¢ Stay up to date with the latest Microsoft Fabric updates and best practices.\n\n\n\nRequired Qualifications:\n\n""¢ 5+ years of experience in IT support, with a focus on Microsoft Fabric or related technologies.\n\n""¢ Strong knowledge of Microsoft Fabric, Power BI, Azure Synapse, and data integration tools.\n\n""¢ Experience with troubleshooting and resolving issues in a cloud-based environment.\n\n""¢ Familiarity with SQL, data pipelines, and ETL processes.\n\n""¢ Excellent problem-solving and communication skills.\n\n""¢ Ability to work independently and collaboratively in a team environment. Preferred Qualifications:\n\n""¢ Microsoft certifications related to Fabric, Azure, or Power BI.\n\n""¢ Experience with automation and scripting (PowerShell, Python, etc.).\n\n""¢ Understanding of security and compliance considerations in cloud-based data platforms.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['analytics services', 'azure synapse', 'power bi', 'troubleshooting', 'data integration', 'python', 'it support', 'oracle', 'data analytics', 'data warehousing', 'machine learning', 'jquery', 'sql server', 'sql', 'r', 'java', 'data science', 'data integration tools', 'powershell', 'html', 'agile', 'etl', 'unix']",2025-06-12 05:41:49
Snowflake Developer,Tata Consultancy Services,5 - 10 years,Not Disclosed,['Hyderabad'],Skill- Snowflake\nLocation- Hyderabad\nExperience- 5 to 10years\n\nJD\n\nIT development experience with min 3+ years hands-on experience in Snowflake,,,,"['Snowflake', 'Python', 'SQL']",2025-06-12 05:41:52
Sr. ETL QA /ETL QA Lead,Visionet Systems,7 - 12 years,Not Disclosed,['Bengaluru'],"Position Summary:\nWe are seeking a highly skilled ETL QA Engineer with at least 6 years of experience in ETL/data pipeline testing on the AWS cloud stack, specifically with Redshift, AWS Glue, S3, and related data integration tools. The ideal candidate should be proficient in SQL, capable of reviewing and validating stored procedures, and should have the ability to automate ETL test cases using Python or suitable automation frameworks. Strong communication skills are essential, and web application testing exposure is a plus.\nTechnical Skills Required:",,,,"['stack', 'amazon redshift', 'data pipeline', 'sql', 'stored procedures', 'selenium', 'aws cloud', 'debugging', 'api', 'etl', 'web testing', 'communication skills', 'python', 'data services', 'sql queries', 'appium', 'pytest', 'jaspersoft', 'power bi', 'root cause analysis', 'aws glue', 'etl testing', 'tableau', 's', 'aws', 'testng', 'api testing']",2025-06-12 05:41:55
Digital Consultant - Innovation Group,"NTT DATA, Inc.",18 - 23 years,Not Disclosed,['Pune'],"Req ID: 317103\n\nWe are currently seeking a Digital Consultant - Innovation Group to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nJob DutiesWe are seeking a highly skilled and experienced Digital Consultant to join our Innovation Group. The ideal candidate will have a strong background in Big Data, Cloud, and AI/ML projects, with a focus on the health insurance or retail domains or manufacturing domains. This role involves engaging with clients for architecture and design, building accelerators for cloud migration, and developing innovative solutions using GenAI technologies.\nKey Responsibilities:\n""¢ Engage with clients to understand their requirements and provide architectural and design solutions.\n""¢ Develop and implement accelerators to facilitate faster cloud migration.\n""¢ Create innovative use cases or solutions to solve day to day data engineering problems using AI and GenAI tools.\n""¢ Develop reference architectures for various use cases using modern cloud data platforms.\n""¢ Understanding of Legacy toolsets, be it ETL, reporting etc is needed.\n""¢ Create migration suites for cataloging, migrating, and verifying data from legacy systems to modern platforms like Databricks and Snowflake.\n\nMinimum Skills RequiredQualifications:\n""¢ EducationB.E. in Electronics & Telecommunication or related field.\n""¢ Experience18+ years in IT, with significant experience in Big Data, Cloud, and AI/ML projects.\n""¢ Technical\n\nSkills:\nProficiency in Databricks, Snowflake, AWS, GenAI (RAG and GANs), Python, C/C++/C",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['c++', 'big data', 'snowflake', 'python', 'aws', 'legacy', 'web services', 'soa', 'as400', 'data migration', 'artificial intelligence', 'retail', 'java', 'ms office outlook', 'etl', 'ml', 'mainframes', 'project management', 'erp', 'c', 'sap', 'sql server', 'data bricks', 'as', 'cobol', 'web technologies']",2025-06-12 05:41:57
"Spark, Java, Kafka- Hyderabad",Cognizant,12 - 15 years,Not Disclosed,['Hyderabad'],"Skill: Java, Spark, Kafka\nExperience: 10 to 16 years\nLocation: Hyderabad\n As Data Engineer, you will :\n       Support in designing and rolling out the data architecture and infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources\n       Identify data source, design  and implement data schema/models and integrate data that meet the requirements of the business stakeholders",,,,"['hive', 'cloudera', 'modeling', 'scala', 'data warehousing', 'apache pig', 'data pipeline', 'data architecture', 'scalability', 'sql', 'java', 'data modeling', 'spark', 'mysql', 'hadoop', 'etl', 'big data', 'hbase', 'python', 'oozie', 'data processing', 'airflow', 'elt', 'data engineering', 'nosql', 'mapreduce', 'kafka', 'feasibility analysis', 'hdfs', 'sqoop', 'aws']",2025-06-12 05:42:00
Tableau Admin with AWS Experience,"NTT DATA, Inc.",2 - 6 years,Not Disclosed,['Noida'],"Req ID: 324014\n\nWe are currently seeking a Tableau Admin with AWS Experience to join our team in NOIDA, Uttar Pradesh (IN-UP), India (IN).\n\nTableau Admin with AWS Experience\n\n\n\nWe are seeking a skilled Tableau Administrator with experience in AWS to join our team. The ideal candidate will be responsible for managing and optimizing our Tableau Server environment hosted on AWS, ensuring efficient operation, data security, and seamless integration with other data sources and analytics tools.\n\n\n\nKey Responsibilities\n\n\n\n- Manage, configure, and administer Tableau Server on AWS, including setting up sites and managing user access and permissions.\n\n- Monitor server activity/performance, conduct regular system maintenance, and troubleshoot issues to ensure optimal performance and minimal downtime.\n\n- Collaborate with data engineers and analysts to optimize data sources and dashboard performance.\n\n- Implement and manage security protocols, ensuring compliance with data governance and privacy policies.\n\n- Automate monitoring and server management tasks using AWS and Tableau APIs.\n\n- Assist in the design and development of complex Tableau dashboards.\nProvide technical support and training to Tableau users.\n\n- Stay updated on the latest Tableau and AWS features and best practices, recommending and implementing improvements.\n\n\n\nQualifications -\n\n- Proven experience as a Tableau Administrator, with strong skills in Tableau Server and Tableau Desktop.\n\n- Experience with AWS, particularly with services relevant to hosting and managing Tableau Server (e.g., EC2, S3, RDS).\n\n- Familiarity with SQL and experience working with various databases.\nKnowledge of data integration, ETL processes, and data warehousing principles.\n\n- Strong problem-solving skills and the ability to work in a fast-paced environment.\n\n- Excellent communication and collaboration skills.\n\n- Relevant certifications in Tableau and AWS are a plus.\n\n\n\n\n\nA Tableau Administrator, also known as a Tableau Server Administrator, is responsible for managing and maintaining Tableau Server, a platform that enables organizations to create, share, and collaborate on data visualizations and dashboards. Here's a typical job description for a Tableau Admin\n\n1.\n\nServer AdministrationInstall, configure, and maintain Tableau Server to ensure its reliability, performance, and security.\n\n2.\n\nUser ManagementManage user accounts, roles, and permissions on Tableau Server, ensuring appropriate access control.\n\n3.\n\nSecurityImplement security measures, including authentication, encryption, and access controls, to protect sensitive data and dashboards.\n\n4.\n\nData Source ConnectionsSet up and manage connections to various data sources, databases, and data warehouses for data extraction.\n\n5. L\n\nicense Management: Monitor Tableau licensing, allocate licenses as needed, and ensure compliance with licensing agreements.\n\n6.\n\nBackup and RecoveryEstablish backup and disaster recovery plans to safeguard Tableau Server data and configurations.\n\n7.\n\nPerformance OptimizationMonitor server performance, identify bottlenecks, and optimize configurations to ensure smooth dashboard loading and efficient data processing.\n\n8.\n\nScalingScale Tableau Server resources to accommodate increasing user demand and data volume.\n\n9.\n\nTroubleshootingDiagnose and resolve issues related to Tableau Server, data sources, and dashboards.\n\n10.\n\nVersion UpgradesPlan and execute server upgrades, apply patches, and stay current with Tableau releases.\n\n11.\n\nMonitoring and LoggingSet up monitoring tools and logs to track server health, user activity, and performance metrics.\n\n12.\n\nTraining and SupportProvide training and support to Tableau users, helping them with dashboard development and troubleshooting.\n\n13.\n\nCollaborationCollaborate with data analysts, data scientists, and business users to understand their requirements and assist with dashboard development.\n\n14.\n\nDocumentationMaintain documentation for server configurations, procedures, and best practices.\n\n15.\n\nGovernanceImplement data governance policies and practices to maintain data quality and consistency across Tableau dashboards.\n\n16.\n\nIntegrationCollaborate with IT teams to integrate Tableau with other data management systems and tools.\n\n17.\n\nUsage AnalyticsGenerate reports and insights on Tableau usage and adoption to inform decision-making.\n\n18.\n\nStay CurrentKeep up-to-date with Tableau updates, new features, and best practices in server administration. A Tableau Administrator plays a vital role in ensuring that Tableau is effectively utilized within an organization, allowing users to harness the power of data visualization and analytics for informed decision-making.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data warehousing', 'sql', 'tableau', 'etl', 'data integration', 'python', 'aws iam', 'amazon redshift', 'aws administration', 'amazon rds', 'aws cloudformation', 'aws lambda', 'ansible', 'docker', 'amazon ec2', 'git', 'devops', 'server administration', 'linux', 'jenkins', 'terraform', 'aws', 'amazon cloudwatch']",2025-06-12 05:42:03
"Salesforce Developer (Apex, Lightning Web Components & Salesforce)",Synechron,5 - 10 years,Not Disclosed,['Pune'],"job requisition idJR1023217\n\nJob Summary\nSynechron is seeking a skilled and dedicated\n\nSalesforce Developer to join our dynamic team. In this role, you will be responsible for designing, developing, and implementing robust Salesforce solutions that enhance business processes and improve user experience. You will work closely with business analysts, product managers, and technical teams to deliver high-quality, scalable applications and configurations within the Salesforce platform. Your contribution will directly support our organization's digital transformation initiatives and ensure our Salesforce ecosystem aligns with evolving industry standards and client needs.\n\nSoftware\n\nRequired\n\nSkills:\nExtensive hands-on experience with Salesforce development including Apex, Lightning Web Components (LWC), Visualforce, and Salesforce configuration tools.\nProficiency with Salesforce SFDX, Visual Studio Code, and Git-based version control systems (e.g., Git, Bitbucket).\nKnowledge of Salesforce's declarative tools such as Flows, Process Builder, Approval Processes, and Email Templates.\nStrong understanding of Salesforce security model (Profiles, Permission Sets, Sharing Settings, Sharing Rules).\nExperience with Salesforce Mobile and Experience Cloud setup and configurations.\nGood understanding of Salesforce Governor Limits and best practices for optimization.\nExperience with SOQL, SOSL, DML operations, and data modeling.\nKnowledge of Salesforce integration techniques, including REST/SOAP APIs, and middleware tools like MuleSoft or AutoRabbit (preferred).\nFamiliar with development tools such as Salesforce DX, VS Code, and CI/CD pipelines using Jenkins or similar tool\nPreferred\n\nSkills:\nSalesforce Admin and Developer certifications.\nExperience with Einstein Analytics, Mulesoft, or AutoRabbit.\nKnowledge of industry-specific compliance or security standards, especially within Financial Services.\nOverall Responsibilities\nDevelop, customize, and maintain Salesforce applications, including Apex classes, triggers, Lightning Web Components, and Visualforce pages.\nDesign scalable and reusable Lightning components and configurations that meet business requirements.\nCollaborate with functional stakeholders to translate business needs into technical solutions.\nEnsure the integrity, security, and performance tuning of Salesforce applications.\nMaintain continuous alignment with Salesforce best practices, including governance, security policies, and performance optimization.\nCreate technical documentation, including design specifications, test cases, and deployment instructions.\nConduct code reviews, unit testing, and participate in release management.\nSupport data integrations with external systems using APIs or middleware.\nContribute to SCRUM/Agile development cycles, providing timely updates and collaborating effectively with team members.\nTechnical Skills (By Category)\nProgramming Languages:\nEssentialApex, JavaScript\nPreferredJava, R, or additional languages relevant to integration and automation\nDatabases & Data Management:\nStrong knowledge of Salesforce objects, data modeling, SOQL, SOSL, and DML operations\nCloud Technologies:\nSalesforce Cloud (Experience Cloud, Mobile)\nOptionalMuleSoft, AutoRabbit for integrations\nFrameworks and Libraries:\nLightning Web Components (LWC), Aura Components\nDevelopment Tools & Methodologies:\nSalesforce DX, Visual Studio Code, Git/Bitbucket, Agile/Scrum methodologies\nSecurity Protocols:\nSalesforce security model, sharing settings, and best practices for data protection\nExperience\nMinimum of 5+ years of extensive Salesforce development experience including large-scale customizations and integrations.\nProven track record delivering end-to-end Salesforce solutions in complex enterprise environments.\nExperience working within Agile teams with continuous integration/deployment.\nIndustry experience in Financial Services is preferred, especially in Investment Banking, Asset Management, or Banking.\nDemonstrated ability to work collaboratively with cross-functional teams and stakeholders.\nDay-to-Day Activities\nDevelop and test Apex classes, triggers, Lightning Web Components, and other custom Salesforce components.\nCollaborate with business analysts and product owners to refine requirements.\nConduct code reviews, unit testing, and assist in user acceptance testing.\nManage Salesforce releases, deployments, and configurations.\nInvestigate and debug system issues, and optimize platform performance.\nParticipate in sprint planning, stand-ups, and retrospectives.\nStay current on Salesforce platform updates, new features, and best practices.\nSupport ongoing enhancements and user support initiatives.\nQualifications\nBachelor's degree in Computer Science, Information Technology, or a related field.\nSalesforce certifications such as Salesforce Certified Platform Developer I & II and Salesforce Certified Service Cloud Consultant are strongly preferred.\nStrong understanding of Salesforce architecture, frameworks, and best practices.\nProfessional Competencies\nExcellent problem-solving and analytical skills.\nEffective communication skills to interface with technical and non-technical stakeholders.\nAbility to work independently and as part of a team in a fast-paced environment.\nStrong organizational skills with an ability to prioritize tasks effectively.\nSelf-motivated learner committed to staying current with Salesforce updates and industry trends.\nAdaptability and eagerness to contribute to innovative solutions.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sosl', 'soql', 'salesforce', 'sales force development', 'salesforce integration', 'visualforce', 'salesforce dx', 'continuous integration', 'rest', 'dml', 'salesforce lightning', 'ci/cd', 'salesforce security model', 'javascript', 'apex', 'visual studio code', 'git', 'data modeling', 'lwc', 'agile', 'soap']",2025-06-12 05:42:06
Enterprise Resource Planning Advisor,"NTT DATA, Inc.",12 - 17 years,Not Disclosed,['Chennai'],"Req ID: 303369\n\nWe are currently seeking a Enterprise Resource Planning Advisor to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\n\nHas more than 12 years of relevant experience with Oracle ERP Cloud Data migration and minimum 4 end to end ERP cloud implementation.\n\n\n\nAnalyze Data and Mapping Work with functional teams to understand data requirements and develop mappings to enable smooth migration using FBDI (File-Based Data Import) and ADFdi.\n\n\n\nDevelop and Manage FBDI Templates Design, customize, and manage FBDI templates to facilitate data import into Oracle SaaS applications, ensuring data structure compatibility and completeness.\n\n\n\nConfigure ADFdi for Data Uploads Use ADFdi (Application Development Framework Desktop Integration) for interactive data uploads, enabling users to manipulate and validate data directly within Excel.\n\n\n\nData Validation and Quality Checks Implement data validation rules and perform pre- and post-load checks to maintain data integrity and quality, reducing errors during migration.\n\n\n\nExecute and Troubleshoot Data Loads Run data loads, monitor progress, troubleshoot errors, and optimize performance during the data migration process.\n\n\n\nCollaborate with Stakeholders Coordinate with cross-functional teams, including project managers and business analysts, to align on timelines, resolve data issues, and provide migration status updates.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['project management', 'sql', 'oracle erp', 'cloud data migration', 'erp cloud', 'snowflake', 'python', 'erp', 'oracle', 'oic', 'erp implementation', 'data warehousing', 'microsoft azure', 'oracle fusion', 'plsql', 'data modeling', 'hadoop', 'agile', 'big data', 'aws', 'informatica', 'unix', 'oracle apps']",2025-06-12 05:42:09
Cyber Security Manager // 6-10 years // Mumbai,2coms,7 - 12 years,Not Disclosed,['Mumbai'],"SUMMARY\nOur client is IT MNC part of one of the major insurance groups based out of Germany and Europe. The Group is represented in around 30 countries worldwide, with Over 40,000 people worldwide, focusing mainly on Europe and Asia. Our client offers a comprehensive range of insurances, pensions, investments and services by focusing on all cutting edge technologies majorly on Could, Digital, Robotics Automation, IoT, Voice Recognition, Big Data science, advanced mobile solutions and much more to accommodate the customers future needs around the globe thru supporting millions of internal and external customers with state of-the-art IT solutions to everyday problems & dedicated to bringing digital innovations to every aspect of the landscape of insurance.\n \nJob Location: Hiranandani Gardens, Powai, Mumbai\n \nMode: Work from Office\n\n\nRequirements\nRoles & Responsibilities:\nDefine project scope,   objectives, and deliverables in collaboration with stakeholders.\nDevelop comprehensive project plans, including timelines, budgets, and resource allocation.\nManage and coordinate project teams, including security engineers, analysts, and other technical resources.\nTrack project progress, identify and manage risks and issues, and implement effective mitigation strategies.\nEnsure adherence to project management methodologies and best practices.\nStay up-to-date with the latest cyber security trends   and technologies.\nSkill & Competencies:\nStrong track record of delivering IT projects in a large, complex environment.  (7 years), especially experience in the implementation of financial and regulatory requirements in the CFO context in Group-wide systems and their integration\nProven 5+ years experience as a PM \nBachelor's degree in Computer Science, Information Technology, or a related field.\nProven experience  (typically 5+ years) managing IT projects, with a significant focus on cyber security initiatives.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['risk management', 'it security', 'cyber security', 'information technology', 'iso', 'owasp', 'soc', 'ceh', 'information security', 'vulnerability management', 'siem', 'vulnerability assessment', 'cissp', 'nessus', 'cyber', 'security', 'it projects', 'security engineering', 'application security', 'vapt', 'penetration testing']",2025-06-12 05:42:12
Application Developer - Dot Net IFRS 17 // Mumbai // 4-8 Yrs,2coms,4 - 9 years,Not Disclosed,['Mumbai'],"SUMMARY\nAbout the client:\n\nOur client is an IT Technology & Services Management MNC , supporting millions of internal and external customers with state of-the-art IT solutions to everyday problems & dedicated to bringing digital innovations to every aspect of the landscape of insurance. Our Client is a part of one of the major insurance groups based out of Germany and Europe. The Group is represented in around 26 countries worldwide, with Over 37,000 people worldwide, focusing mainly on Europe and Asia & offers a comprehensive range of insurances, pensions, investments and services by focusing on all cutting edge technologies majorly on Could, Digital, Robotics Automation, IoT, Voice Recognition, Big Data science, advanced mobile solutions and much more to accommodate the customers future needs around the globe.\n\n\n\n\n\nRequirements\nCompentences for .NET\nNET Framework (incl. .net core), SQL Server, Entity Framework\nPrioritize business impact and urgency\nAbility to learn new technology and methodology quickly.\nKnowledge User Task API, GUI (Graphical User Interfaces) Design Compentences for NTS (New Tech Stack)\nNTS (New Tech Stack), Container runtime environment with Docker containers and Kubernetes, Cloud with AWS (Amazon Web Services), CI/CD - Continuous Integration / Continuous Deployment with Jenkins, Knowledge Source Management Github and Nexus\nOpenshift, Kerberos Authentication, Competences for Cluster Workflow\nDesign + implementation of process model, Design + implementation of input interfaces in REST format\nDevelopment of flow services below process model\n\nEducational Qualifications:\nBachelor’s or Master’s  degree in Computer Science /Engineering/Information Technology\nCandidate with non-computer science degree must have minimum 1 year of relevant experience\nMBA in IT / Insurance/Finance   can also apply for Requirements Engineer and Test Engineer role.\n\n\n\nBenefits",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'kubernetes', 'nexus', 'web services', 'openshift', 'ci/cd', 'kerberos', 'ccm', 'business objects administration', 'docker', 'profibus', 'jenkins', 'cd', 'rest', 'e-discovery', 'github', 'entity framework', 'intools', 'sql server', 'application development', 'net application', '.net core', '.net', 'abb dcs', 'aws']",2025-06-12 05:42:15
Senior Software Developer at An E-commerce Unicorn,Recruito,4 - 8 years,30-45 Lacs P.A.,[],"Responsibilities\nLead design and development of core product features.\nCollaborate with developers to deliver high-quality, timely solutions.\nOwn end-to-end development of assigned modules.\n\nRequirements\n\nSoftware development experience with strong skills in Java, data structures, and algorithms.\nHands-on experience with distributed systems, REST services, Kubernetes, and cloud platforms.\nKnowledge of JavaScript/ReactJS or Big Data tools like Spark, Kafka, Flink, Beam, etc.\nProficient in SQL/NoSQL databases and building cloud-based services.\nFamiliarity with data pipelines, ETL processes, and large-scale data processing.\nUnderstanding of Data Mesh, Data Fabric, and Infrastructure as Code (Terraform, Ansible).\nStrong problem-solving skills, quick learner, and team player with a full-stack mindset.\n\nPerks and Benefits\n\nMedical Insurance\nInternet Reimbursement\nFlexible working hours",Industry Type: Internet (E-Commerce),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Data Structures', 'Javascript']",2025-06-12 05:42:17
Tableau Developer,Wipro,3 - 5 years,Not Disclosed,['Chennai'],"Role Purpose\nThe purpose of this role is to interpret data and turn into information (reports, dashboards, interactive visualizations etc) which can offer ways to improve a business, thus affecting business decisions.\nDo\n1. Managing the technical scope of the project in line with the requirements at all stages\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\nb. Develop record management process and policies\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\nd. Providing sales data, proposals, data insights and account reviews to the client base\ne. Identify areas to increase efficiency and automation of processes\nf. Set up and maintain automated data processes\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\nh. Produce and track key performance indicators\n2. Analyze the data sets and provide adequate information\na. Liaise with internal and external clients to fully understand data content\nb. Design and carry out surveys and analyze survey data as per the customer requirement\nc. Analyze and interpret complex data sets relating to customers business and prepare reports for internal and external audiences using business analytics reporting tools\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\nf. Develop predictive models and share insights with the clients as per their requirement\n\nDeliver\n\nNoPerformance ParameterMeasure1.Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\nMandatory Skills: Tableau.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Tableau', 'data warehousing', 'business analytics', 'data visualization', 'QlikView', 'data integration', 'digital transformation']",2025-06-12 05:42:19
Power platform Developer,ITC Infotech,3 - 8 years,9-19 Lacs P.A.,['Bengaluru'],"Key Responsibilities:\nDesign, develop, test, and deploy custom business applications using Microsoft Power Platform (Power Apps, Power Automate, Power BI, Power Virtual Agents).\nCollaborate with business users and IT teams to gather and analyze requirements.\nCreate automated workflows and process automation using Power Automate to improve operational efficiency.\nDevelop interactive dashboards and reports using Power BI to provide actionable insights.",,,,"['Canvas', 'Power Platform', 'Powerapps', 'Microsoft Power Automate', 'Power Automate']",2025-06-12 05:42:22
Business Analyst,.,4 - 7 years,Not Disclosed,['Pune'],"Role Overview:\n\nThis hybrid role sits within the Distribution Data Stewardship Team and combines operational and technical responsibilities to ensure data accuracy, integrity, and process optimization across sales reporting functions.\n\nKey Responsibilities:\n\nSupport sales reporting inquiries from sales staff at all levels.\nReconcile omnibus activity with sales reporting systems.\nAnalyze data flows to assess impact on commissions and reporting.\nPerform data audits and updates to ensure integrity.\nLead process optimization and automation initiatives.\nManage wholesaler commission processes, including adjustments and manual submissions.\nOversee manual data integration from intermediaries.\nExecute territory alignment changes to meet business objectives.\nContribute to team initiatives and other responsibilities as assigned.\nGrowth Opportunities:\n\nExposure to all facets of sales reporting and commission processes.\nOpportunities to develop project and relationship management skills.\nPotential to explore leadership or technical specialist roles within the firm.\nQualifications:\n\nBachelors degree in Computer Engineering or a related field.\n4–7 years of experience with Python programming and automation.\nStrong background in SQL and data analysis.\nExperience in relationship/customer management and leading teams.\nExperience working with Salesforce is a plus.\nRequired Skills:\n\nTechnical proficiency in Python and SQL.\nStrong communication skills and stakeholder engagement.\nHigh attention to data integrity and detail.\nSelf-directed with excellent time management.\nProject coordination and documentation skills.\nProficiency in MS Office, especially Excel.",Industry Type: Investment Banking / Venture Capital / Private Equity,"Department: BFSI, Investments & Trading","Employment Type: Full Time, Permanent","['Advanced Python', 'Brd', 'FRD', 'Complex Queries', 'Advance Sql', 'Python Scripting', 'Python', 'SQL']",2025-06-12 05:42:24
Technical Specialist - Security,"NTT DATA, Inc.",2 - 5 years,Not Disclosed,['Mumbai'],"Your day at NTT DATA\nThe Security Managed Services Engineer (L3) is a seasoned engineering role, responsible for providing a service to clients by proactively identifying and resolving technical incidents and problems.\n\nThrough pre-emptive service incident and resolution activities, as well as product reviews, operational improvements, operational practices, and quality assurance this role will maintain a high level of service to clients.\n\nThe primary objective of this role is to ensure zero missed service level agreement (SLA) conditions and is responsible for managing tickets of high complexity, conducts advanced and complicated tasks, aware of client's high level and low-level security architecture and provides resolution to a diverse range of complex problems.\n\nThis position uses considerable judgment and independent analysis within defined policies and practices and applies analytical thinking and deep technical expertise in achieving client outcomes, while coaching and mentoring junior team members across functions.\n\nThe Security Managed Services Engineer (L3) may also contribute to support on project work as and when required.\nWhat you'll be doing\nKey Responsibilities:\nAdvanced Cyber Analytics, Proactively drive hunting and analysis against the dataset available for customers\nWork with our security operations center (SOC) and take the lead role in threat detection and incident response activities\nLeverage internal and external resources to research threats, vulnerabilities, and intelligence on various attackers and attack infrastructure\nUse Big Data Analytics platform to identify threats, determine root cause, scope, and severity of each and compile/report findings\nWork with Threat Intelligence and Malware solutions to identify threats, develop or recommend countermeasures, and perform advanced network and host analysis in the event of a compromise\nLeveraging tactical and technical intelligence for eradication of threats\nCharacterize suspicious binaries and be able identify traits, C2, and develop network and host-based IOCs\nIdentify potential malicious activity from memory dumps, logs, and packet captures\nThrough review and analysis of cyber threats, provide both internal & external parties key information to respond to threat\nParticipate as part of a close team of technical specialists on coordinated responses and subsequent remediation of security incidents\nInterface with customers on a daily basis to consult with them on best security practices and help them mature their security posture\nCreate Threat Models based on Mitre ATT&CK framework and cyber kill chain for customers\nLinking of threat models with SIEM use cases and hunting exercises\nBasic Malware Analysis\nWork with SOAR platforms to generate and configure orchestration workflows and responses\n\nAcademic Qualifications and Certifications:\nBachelor's degree or equivalent qualification in IT/Computing (or demonstrated equivalent work experience)\n\nRequired Experience:\nSeasoned Managed Services experience handling complex Security Infrastructure.\nSeasoned experience required in Engineering function within a medium to large ICT organization.\nSeasoned working knowledge of ITIL processes.\nSeasoned experience working with vendors and/or 3rd parties.\nWorkplace type:\nOn-site Working",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Security Engineering', 'technical support', 'troubleshooting', 'networking', 'ITIL', 'technical security']",2025-06-12 05:42:26
"Senior ML Manager, Notifications Intel Science",Uplers,9 - 14 years,Not Disclosed,['Bengaluru'],"What Youll do:\nOwn the strategy, roadmap, and execution of notification intelligence and automation solutions.\nLead the development of GenAI-powered content generation, send-time optimization, and cross-channel orchestration systems.\nBuild intelligent systems that drive significant incremental revenue while minimizing customer fatigue and unsubscribes.\nDevelop and grow technical leadership within the team, modeling a culture of continuous research and innovation.\nCollaborate with Engineering and Product teams to scale decisioning systems to millions of notifications daily.\nAct as a subject matter expert, providing mentorship and technical guidance across the broader Data Science and Engineering organization.\n\nWe Are a Match Because You Have:\nBachelor's or Masters degree in Computer Science, Mathematics, Statistics, or related field.\n9+ years of industry experience, with at least 12 years of experience managing teams, and 5+ years as an individual contributor working on production ML systems.\nStrategic thinker with a customer-centric mindset and a desire for creative problem-solving, looking to make a big impact in a growing organization.\nDemonstrated success influencing senior-level stakeholders on strategic direction, based on recommendations backed by in-depth analysis, and excellent written and verbal communication skills.\nAbility to partner cross-functionally to own and shape technical roadmaps and the organizations required to drive them.\nProficient in one or more programming languages, e.g., Python, Golang, or Rust.\n\nNice to have:\nExperience with GCP, Airflow, and containerization (Docker).\nExperience building scalable data processing pipelines with big data tools such as Hadoop, Hive, SQL, Spark, etc.\nExperience in Bayesian Learning, Multi-armed Bandits, or Reinforcement Learning.\nFamiliarity with Generative AI and agentic workflows.\n\nPS: This role is with one of our clients who is a leading name in the Retail Industry.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Jvm', 'Cloud Platform', 'Machine Learning', 'Golang', 'Airflow', 'Java', 'Rust', 'GCP', 'AWS', 'Python']",2025-06-12 05:42:29
Technical Lead-Analytics,3i Infotech,5 - 10 years,Not Disclosed,"['Navi Mumbai', 'Bengaluru', 'Greater Noida']","Analytics Tech Lead Job Description\nAnalytics Tech Lead with a 6 years of relevant experience, who will be responsible for designing, developing, and maintaining Machine Learning use cases, data visualization solutions. Experience in Banking Industry and a good understanding of Core Banking systems/flow is preferred.\nResponsibilities\n•         Experienced in develop Machine Learning Model use cases using Python and other ML tools. \n•         Enabling clients with AI/ML solutions that seamlessly cater their business needs of data-driven decision results. I\nExperienced in designing data exploration & storyline-based analytics dashboard/wireframes/prototypes using Tableau\nAdvanced Tableau development skills, including experience with Tableau Desktop and Server latest version and expert knowledge of Tableau software optimization and configuration.\nExperience working with Tableau server and user management, performance tuning, security in Tableau server.\nImplemented data modelling, data blending, and custom query joins to enhance dashboard interactivity and functionality. \nExperience with Structured Query Language,  data analysis, data profiling, data validation.\nWork with cross-functional teams, including business analysts and data engineers to understand requirements and develop solutions.\nPrepare technical documentation on the deliverables.\nSkills and Qualifications\nBachelors degree in computer science or Master of Computer Applications with 6+ years of relevant experience.\nAI/ML tool  Certification \nExcellent written, verbal and presentation skills to foster clarity and predictability.\nExperience working with a global team in different time zones.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Artificial Intelligence', 'Machine Learning', 'Analytics', 'Python', 'Ml']",2025-06-12 05:42:31
Immediate Openings For ETL Developer - Delhi,Trigyn Technologies,5 - 10 years,Not Disclosed,['New Delhi'],"We are seeking a skilled Data Engineer with at least 5 years of experience to join our data analytics team, focusing on building robust data pipelines and systems to support the creation of dynamic dashboards. The role involves designing, building, and optimizing data architecture, enabling real-time data flow for visualization and analytics. The Data Engineer will be responsible for managing ETL processes, ensuring data quality, and supporting the scalable integration of various data sources into our analytics platform.\nThe ideal candidate should have extensive experience in working with complex data architectures, managing ETL workflows, and ensuring seamless data integration across platforms. They should also have a deep understanding of cloud technologies and database management.\nKey Responsibilities:\n•Data Pipeline Development\no Design, build, and maintain scalable ETL (Extract, Transform, Load) processes for collecting, storing, and processing structured and unstructured data from multiple sources.\no Develop workflows to automate data extraction from APIs, databases, and external sources.\no Ensure data pipelines are optimized for performance and handle large data volumes with minimal latency.\n•Data Integration and Management\no Integrate data from various sources (e.g., databases, APIs, cloud storage) into the centralized daIta warehouse or data lake to support real-time dashboards.\no Ensure smooth data flow and seamless integration with analytics tools like Power BI and Tableau.\no Manage and maintain data storage solutions, including relational (SQL-based) and NoSQL databases.\nData Quality and Governance\no Implement data validation checks and quality assurance processes to ensure data accuracy, consistency, and integrity.\no Develop monitoring systems to identify and troubleshoot data inconsistencies, duplications, or errors during ingestion and processing.\no Ensure compliance with data governance policies and standards, including data protection regulations such as the Digital Personal Data Protection (DPDP) Act.\n•Database Management and Optimization\no Design and manage both relational and NoSQL databases, ensuring efficient storage, query performance, and reliability.\no Optimize database performance, ensuring fast query execution times and efficient data retrieval for dashboard visualization.\no Implement data partitioning, indexing, and replication strategies to support large-scale data operations.\n•Data Security and Compliance\no Ensure that all data processes adhere to security best practices, including encryption, authentication, and access control.\no Implement mechanisms for secure data storage and transmission, especially for sensitive government or public sector data.\no Conduct regular audits of data pipelines and storage systems to ensure compliance with relevant data protection regulations.\n• Cloud Infrastructure and Deployment\nDeploy and manage cloud-based data solutions using AWS, Azure, or GCP, including data lakes, data warehouses, and cloud-native ETL tools.\no Set up cloud infrastructure to support high availability, fault tolerance, and scalability of data systems.\no Monitor cloud usage and optimize costs for data storage, processing, and retrieval.\n•Performance Monitoring and Troubleshooting\no Continuously monitor data pipeline performance and data ingestion times to identify bottlenecks and areas for improvement\nTroubleshoot and resolve any data flow issues, ensuring high availability and reliability of data for dashboards and analytics.\no Implement logging and alerting mechanisms to detect and address any operational issues proactively.\nQualifications:\n•Education: Bachelors degree in Computer Science, Information Technology, Data Engineering, or a related field. A Master’s degree is a plus.\n•Experience: At least 5 years of hands-on experience as a Data Engineer, preferably in a data analytics or dashboarding environment.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Informatica', 'ETL', 'Azure Data Factory', 'SQL']",2025-06-12 05:42:34
Sr. Project Manager,Useready,15 - 18 years,30-40 Lacs P.A.,"['Mohali', 'Bengaluru']","Job Summary:\nWe are seeking an experienced and detail-oriented Technical Project Manager, with strong interpersonal skills to lead and manage Data, Business Intelligence (BI), and Analytics initiatives across single and multiple client engagements. The ideal candidate will have a solid background in data project delivery, knowledge of modern cloud platforms, and familiarity with tools like Snowflake, Tableau, and Power BI. Understanding of AI and machine learning projects is a strong plus.\nThis role requires strong communication and leadership skills, with the ability to translate complex technical requirements into actionable plans and ensure successful, timely, and high-quality delivery with attention to details.\nKey Responsibilities:\nProject & Program Delivery\nManage end-to-end, the full lifecycle of data engineering and analytics, projects including data platform migrations, dashboard/report development, and advanced analytics initiatives.\nDefine project scope, timelines, milestones, resource needs, and deliverables in alignment with stakeholder objectives.\nManage budgets, resource allocation, and risk mitigation strategies to ensure successful program delivery.\nUse Agile, Scrum, or hybrid methodologies to ensure iterative delivery and continuous improvement.\nMonitor performance, track KPIs, and adjust plans to maintain scope, schedule, and quality.\nExcellence in execution and ensure client satisfaction\nClient & Stakeholder Engagement\nServe as the primary point of contact for clients and internal teams across all data initiatives.\nTranslate business needs into actionable technical requirements and facilitate alignment across teams.\nConduct regular status meetings, monthly and quarterly reviews, executive updates, and retrospectives.\nManage Large teams\nAbility to manage up to 50+ resources working on different projects for different clients.\nWork with practice and talent acquisition teams for resourcing needs\nManage P & L\nManage allocation, gross margin, utilization etc effectively\nTeam Coordination\nLead and coordinate cross-functional teams including data engineers, BI developers, analysts, and QA testers.\nEnsure appropriate allocation of resources across concurrent projects and clients.\nFoster collaboration, accountability, and a results-oriented team culture.\n  Data, AI and BI Technology Oversight\nManage project delivery using modern cloud data platforms\nOversee BI development using Tableau and/or Power BI, ensuring dashboards meet user needs and follow visualization best practices. Conduct UATs\nManage initiatives involving ETL/ELT processes, data modeling, and real-time analytics pipelines.\nEnsure compatibility with data governance, security, and privacy requirements.\nManage AL ML projects\nData & Cloud Understanding\nOversee delivery of solutions involving cloud data platforms (e.g., Azure, AWS, GCP), data lakes, and modern data stacks.\nSupport planning for data migrations, ETL processes, data modeling, and analytics pipelines.\nBe conversant in tools such as Power BI, Tableau, Snowflake, Databricks, Azure Synapse, or BigQuery.\nRisk, Quality & Governance\nIdentify and mitigate risks related to data quality, project timelines, and resource availability.\nEnsure adherence to governance, compliance, and data privacy standards (e.g., GDPR, HIPAA).\nMaintain thorough project documentation including charters, RACI matrices, RAID logs, and retrospectives.\nQualifications:\n  Bachelor’s degree in Computer Science, Information Systems, Business, or a related field.\nCertifications (Preferred):\nPMP, PRINCE2, or Certified ScrumMaster (CSM)\nCloud certifications (e.g., AWS Cloud Practitioner, Azure Fundamentals, Google Cloud Certified)\nBI/analytics certifications (e.g., Tableau Desktop Specialist, Power BI Data Analyst Associate, DA-100)\nMust Have Skills:\nStrong communication skills\nStrong interpersonal\nAbility to work collaboratively\nExcellent Organizing skills\nStakeholder Management\nCustomer Management\nPeople Management\nContract Management\nRisk & Compliance Management\nC-suite reporting\nTeam Management\nResourcing\nExperience using tools like JIRA, MS Plan etc.\nDesirable Skills:\n15 years of IT experience with 8+ years of proven project management experience, in delivering data, AI Ml, BI / analytics-focused environments.\nExperience delivering projects with cloud platforms (e.g., Azure, AWS, GCP) and data platforms like Snowflake.\nProficiency in managing BI projects preferably Tableau and/or Power BI.\nKnowledge or hands on experience on legacy tools is a plus.\nSolid understanding of the data lifecycle including ingestion, transformation, visualization, and reporting.\nComfortable using PM tools like Jira, Azure DevOps, Monday.com, or Smartsheet.\nExperience managing projects involving data governance, metadata management, or master data management (MDM).",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['delivery', 'bi projects', 'project management', 'data', 'interpersonal skills', 'microsoft azure', 'power bi', 'aiml', 'machine learning', 'business intelligence', 'artificial intelligence', 'tableau', 'stakeholder management', 'gcp', 'leadership', 'project delivery', 'scrum', 'agile', 'organizing', 'aws', 'communication skills']",2025-06-12 05:42:37
Senior Power Bi Developer,Course5 Intelligence,6 - 10 years,12-20 Lacs P.A.,['Bengaluru'],"Job Summary\nWe are looking for a skilled and detail-oriented Power BI Developer to join our team. The ideal candidate will be responsible for designing, developing, and maintaining business intelligence dashboards and reports that help drive data-driven decision-making across the organization.\nJob Responsibilities\nDevelop and maintain Power BI dashboards, reports, and visualizations based on business requirements\nPerform data modeling, data analysis, and implement DAX queries for enhanced insights\nConnect Power BI to various data sources (SQL Server, Excel, cloud data platforms, etc.)\nWork closely with stakeholders to understand reporting needs and translate them into technical solutions\nOptimize Power BI datasets and dashboards for performance and usability\nEnsure data accuracy, reliability, and consistency across all reports and dashboards.\nCollaborate with data engineers and analysts to streamline data pipelines and reporting logic\nImplement role-level security (RLS) in Power BI.\n\nRequired Skills & Qualifications:\n• Proven experience (4+ years) working with Power BI in a professional setting\n• Strong proficiency in DAX, Power Query (M language), and data modeling\n• Experience working with SQL and relational databases\n• Knowledge of ETL processes, data warehousing, and performance tuning\n• Strong analytical and problem-solving skills\n• Excellent communication and collaboration abilities • Experience with Power BI Service (publishing, workspace management, gateway configuration)\n• Bachelors degree in Computer Science, Information Systems, or a related field",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Dax Queries', 'Power Bi', 'Data Warehousing', 'ETL', 'SQL', 'Power Bi Reports', 'Power Bi Dashboards', 'Power Bi Desktop']",2025-06-12 05:42:40
Associate - Business Information Management,Axtria,5 - 10 years,Not Disclosed,['Gurugram'],"Position Summary \n\nThis is the Requisition for Employee Referrals Campaign and JD is Generic.\n\nWe are looking for Associates with 5+ years of experience in delivering solutions around Data Engineering, Big data analytics and data lakes, MDM, BI, and data visualization. Experienced to Integrate and standardize structured and unstructured data to enable faster insights using cloud technology. Enabling data-driven insights across the enterprise.\n\n Job Responsibilities \n\n\nHe/she should be able to design implement and deliver complex Data Warehousing/Data Lake, Cloud Data Management, and Data Integration project assignments.\n\nTechnical Design and Development – Expertise in any of the following skills.\n\nAny ETL tools (Informatica, Talend, Matillion, Data Stage), andhosting technologies like the AWS stack (Redshift, EC2) is mandatory.\n\nAny BI toolsamong Tablau, Qlik & Power BI and MSTR.\n\nInformatica MDM, Customer Data Management.\n\nExpert knowledge of SQL with the capability to performance tune complex SQL queries in tradition and distributed RDDMS systems is must.\n\nExperience across Python, PySpark and Unix/Linux Shell Scripting.\n\nProject Managementis\n\nmust to have. Should be able create simple to complex project plans in Microsoft Project Plan and think in advance about potential risks and mitigation plans as per project plan.\n\nTask Management – Should be able to onboard team on the project plan and delegate tasks to accomplish milestones as per plan. Should be comfortable in discussing and prioritizing work items with team members in an onshore-offshore model.\n\nHandle Client Relationship – Manage client communication and client expectations independently or with support of reporting manager. Should be able to deliver results back to the Client as per plan. Should have excellent communication skills.\n\n\n Education \n\nBachelor of Technology\nMaster's Equivalent - Engineering\n\n Work Experience \n\nOverall, 5- 7years of relevant experience inData Warehousing, Data management projects with some experience in the Pharma domain.\n\nWe are hiring for following roles across Data management tech stacks -\n\nETL toolsamong Informatica, IICS/Snowflake,Python& Matillion and other Cloud ETL.\n\nBI toolsamong Power BI and Tableau.\n\nMDM - Informatica/ Raltio, Customer Data Management.\n\nAzure cloud Developer using Data Factory and Databricks\n\nData Modeler-Modelling of data - understanding source data, creating data models for landing, integration.\n\nPython/PySpark -Spark/ PySpark Design, Development, and Deployment",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['aws stack', 'sql', 'etl tool', 'data visualization', 'sql queries', 'data management', 'amazon redshift', 'bi', 'data warehousing', 'pyspark', 'spark', 'etl', 'data lake', 'snowflake', 'python', 'big data analytics', 'datastage', 'talend', 'power bi', 'data engineering', 'tableau', 'mdm', 'aws', 'informatica', 'unix']",2025-06-12 05:42:42
OAC ODI Architect (Senior Oracle Analytics Consultant),Mastek,10 - 15 years,15-30 Lacs P.A.,"['Ahmedabad', 'Chennai']","We are looking for OAC ODI Architect to be based in Ahemdabad or Chennai\nMinimum Architect exp 4 Yrs\nOracle Analytics Consultant (OAC, ODI, FDI) Tech Architect\nLocation: [Specify Location or Remote] Chennai Ahmedabad\nExpected DOJ June\nEmployment Type: Full-time\nExperience Level: 10 - 15+ Years\nJob Summary:\nWe are seeking an experienced and results-driven Senior Oracle Analytics Consultant with over 10 years of hands-on experience in Oracle Analytics Cloud (OAC), Oracle Data Integrator (ODI), and Fusion Data Intelligence (FDI). The ideal candidate will have a deep understanding of enterprise data architecture, data integration best practices, and cloud-based analytics solutions. This role involves working closely with cross-functional teams to design, implement, and support advanced analytics and data integration solutions that drive business value.\nKey Responsibilities:\nLead the design, development, and deployment of analytics solutions using Oracle Analytics Cloud (OAC).\nArchitect and implement data integration pipelines using Oracle Data Integrator (ODI) for on-prem and cloud data sources.\nCollaborate with business and IT stakeholders to design and deploy Fusion Data Intelligence (FDI) based dashboards and KPIs.\nOptimize performance of OAC dashboards and reports, including data modeling and visualization best practices.\nDevelop and manage data models, RPDs, and semantic layers within OAC.\nBuild and maintain ETL mappings, packages, and workflows in ODI.\nIntegrate Oracle Fusion Applications with OAC and FDI for near-real-time reporting.\nDrive data governance and quality initiatives across analytics platforms.\nTroubleshoot technical issues and provide solutions in a timely manner.\nMentor junior developers and provide technical leadership on complex projects.\nQualifications:\nBachelors or Masters degree in Computer Science, Information Systems, or related field.\n10+ years of relevant experience with strong focus on:\nOracle Analytics Cloud (OAC) - Must\nOracle Data Integrator (ODI) - Must\nFusion Data Intelligence (FDI) Good to Have\nExpertise in Oracle Fusion ERP/HCM data models and subject areas.\nExperience integrating multiple data sources, including on-premise and cloud systems.\nStrong understanding of SQL, PL/SQL, and performance tuning.\nFamiliarity with data lake architecture, data warehousing, and ELT/ETL design patterns.\nProven experience working in Agile/DevOps environments.\nExcellent communication, analytical thinking, and problem-solving skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Oac', 'ODI', 'Odi Architecture', 'FDI']",2025-06-12 05:42:44
Snowflake Architect,Allegis Global Solutions (AGS),9 - 14 years,Not Disclosed,[],"Snowflake Architect\nJob Location: Hyderabad / Bangalore / Chennai / Kolkata / Noida/ Gurgaon / Pune / Indore / Mumbai\n\nJob Details\n\nTechnical Expertise:\nStrong proficiency in Snowflake architecture, including data sharing, partitioning, clustering, and materialized views.\nAdvanced experience with DBT for data transformations and workflow management.\nExpertise in Azure services, including Azure Data Factory, Azure Data Lake, Azure Synapse, and Azure Functions.\nData Engineering:\nProficiency in SQL, Python, or other relevant programming languages.\nStrong understanding of data modeling concepts, including star schema and normalization.\nHands-on experience with ETL/ELT pipelines and data integration tools.\nSoft Skills:\nExcellent problem-solving and analytical skills.\nStrong communication and stakeholder management abilities.\nAbility to work in agile teams and handle multiple priorities.\nPreferred Qualifications:\nCertifications in Snowflake, DBT, or Azure Data Engineering.\nFamiliar with data visualization tools like Power BI or Tableau.\nKnowledge of CI/CD pipelines and DevOps practices for data workflows.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'Snowflake', 'Data Build Tool', 'SQL']",2025-06-12 05:42:46
Senior DevSecOps (Bangalore) - 8+ Years - Hybrid,Databuzz Ltd,8 - 10 years,5-15 Lacs P.A.,['Bengaluru'],"Databuzz is Hiring for Senior DevSecOps Engineer Dynamics 365 (Bangalore) - 8+ Years - Hybrid\n\nPlease mail your profile to jagadish.raju@databuzzltd.com with the below details, If you are Interested.\n\nAbout DatabuzzLTD: Databuzz is One stop shop for data analytics specialized in Data Science, Big Data, Data Engineering, AI & ML, Cloud Infrastructure and Devops. We are an MNC based in both UK and INDIA. We are a ISO 27001 & GDPR complaint company.\n\nCTC -\nECTC -\nNotice Period/LWD - (Candidate serving notice period will be preferred)\nDOB-\n\nPosition: Senior DevSecOps Engineer Dynamics 365 (Bangalore) - 8+ Years - Hybrid\n\nMandatory Skills:\n\nShould have experience in Azure DevOps\nShould have experience Dynamics - 365\nStrong experience with Python and PowerShell for scripting and automation tasks\nExperienced in working with Kubernetes, Terraform,\nGood to have Service Now, YAML\n\nRegards,\nJagadish Raju - Talent Acquisition Specialist\njagadish.raju@databuzzltd.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Dynamics 365', 'Powershell', 'Azure Devops', 'Python', 'Terraform', 'Docker', 'Servicenow', 'Yaml', 'Kubernetes']",2025-06-12 05:42:49
"Lead, Application Development",S&P Global Market Intelligence,10 - 13 years,Not Disclosed,"['Hyderabad', 'Gurugram', 'Ahmedabad']","\n\nAbout the Role: \n\nGrade Level (for internal use):\n11\n\nS&P Global EDO\n\nThe\n\nRole: Lead- Software Engineering IT- Application Development.\n\nJoin Our Team:Step into a dynamic team at the cutting edge of data innovation! Youll collaborate daily with talented professionals from around the world, designing and developing next-generation data products for our clients. Our team thrives on a diverse toolkit that evolves with emerging technologies, offering you the chance to work in a vibrant, global environment that fosters creativity and teamwork.\n\nThe Impact:As a Lead Software Developer at S&P Global, youll be a driving force in shaping the future of our data products. Your expertise will streamline software development and deployment, aligning cutting-edge solutions with business needs. By ensuring seamless integration and continuous delivery, youll enhance product capabilities, delivering high-quality systems that meet the highest standards of availability, security, and performance. Your work will empower our clients with impactful, data-driven solutions, making a real difference in the financial world.\n\nWhats in it for You:\n\n\nCareer Development: Build a rewarding career with a global leader in financial information and analytics, supported by continuous learning and a clear path to advancement.\n\n\nDynamic Work Environment: Thrive in a fast-paced, forward-thinking setting where your ideas fuel innovation and your contributions shape groundbreaking solutions.\n\n\nSkill Enhancement: Elevate your expertise on an enterprise-level platform, mastering the latest tools and techniques in software development.\n\n\nVersatile Experience: Dive into full-stack development with hands-on exposure to cloud computing, Bigdata, and revolutionary GenAI technologies.\n\n\nLeadership Opportunities: Guide and inspire a skilled team, steering the direction of our products and leaving your mark on the future of technology at S&P Global.\n\n\nResponsibilities:\nArchitect and develop scalable Bigdata and cloud applications, harnessing a range of cloud services to create robust, high-performing solutions.\nDesign and implement advanced CI/CD pipelines, automating software delivery for fast, reliable deployments that keep us ahead of the curve.\nTackle complex challenges head-on, troubleshooting and resolving issues to ensure our products run flawlessly for clients.\nLead by example, providing technical guidance and mentoring to your team, driving innovation and embracing new processes.\nDeliver top-tier code and detailed system design documents, setting the standard with technical walkthroughs that inspire excellence.\nBridge the gap between technical and non-technical stakeholders, turning complex requirements into elegant, actionable solutions.\nMentor junior developers, nurturing their growth and helping them build skills and careers under your leadership.\n\n\nWhat Were Looking For:Were seeking a passionate, experienced professional with:\n10-13 years of hands-on experience designing and building data-intensive solutions using distributed computing, showcasing your mastery of scalable architectures.\nProven success implementing and maintaining enterprise search solutions in large-scale environments, ensuring peak performance and reliability.\nA history of partnering with business stakeholders and users to shape research directions and craft robust, maintainable products.\nExtensive experience deploying data engineering solutions in public clouds like AWS, GCP, or Azure, leveraging cloud power to its fullest.\nAdvanced programming skills in Python, Java, .NET or Scala, backed by a portfolio of impressive projects.\nStrong knowledge of Gen AI tools (e.g., GitHub Copilot, ChatGPT, Claude, or Gemini) and their power to boost developer productivity.\nExpertise in containerization, scripting, cloud platforms, and CI/CD practices, ready to shine in a modern development ecosystem.\n5+ years working with Python, Java, .NET, Kubernetes, and data/workflow orchestration tools, proving your technical versatility.\nDeep experience with SQL, NoSQL, Apache Spark, Airflow, or similar tools, operationalizing data-driven pipelines for large-scale batch and stream processing.\nA knack for rapid prototyping and iteration, delivering high-quality solutions under tight deadlines.\nOutstanding communication and documentation skills, adept at explaining complex ideas to technical and non-technical audiences alike.\n\n\nTake the Next Step:Ready to elevate your career and make a lasting impact in data and technologyJoin us at S&P Global and help shape the future of financial information and analytics. Apply today!\n\nReturn to Work\n\nHave you taken time out for caring responsibilities and are now looking to return to workAs part of our Return-to-Work initiative (link to career site page when available), we are encouraging enthusiastic and talented returners to apply and will actively support your return to the workplace.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['airflow', 'data engineering', 'sql', 'nosql', 'spark', 'continuous integration', 'public cloud', 'kubernetes', 'python', 'github', 'scala', 'documentation', 'ci/cd', 'microsoft azure', 'cloud platforms', 'distributed computing', 'containerization', 'gen', 'java', 'gcp', '.net', 'aws', 'big data', 'cloud computing']",2025-06-12 05:42:51
Sr ETL/SSIS developer,Sagility India,6 - 11 years,Not Disclosed,['Bengaluru'],"Job Summary\nWe are seeking a highly skilled and self-driven SSIS with strong communication and client-facing skills to join our healthcare analytics team. This role requires a combination of deep technical expertise in SSIS and data integration along with the ability to consult and collaborate directly with clients to understand and address their data needs.\nThe ideal candidate will be experienced in building and maintaining scalable data pipelines, working with diverse healthcare data sources, and ensuring data quality and availability for downstream analytics. You will play a key role in delivering clean, trusted, and timely data for insights and reporting.\nKey Responsibilities\nDesign, develop, and maintain robust and scalable SSIS to support healthcare analytics and reporting platforms.\nEngage directly with clients to gather requirements, provide consultation, and translate business needs into technical solutions.\nIntegrate and normalize data from diverse healthcare data sources, including claims, EMR, lab, pharmacy, and eligibility systems.\nEnsure data accuracy, completeness, and consistency throughout ingestion and transformation processes.\nOptimize and tune data workflows for performance and scalability in a cloud or on-premise data platform.\nTroubleshoot and resolve data issues in a timely and proactive manner to support high data availability.\nCollaborate with analysts, data scientists, and business stakeholders to ensure data pipelines meet analytical needs.\nCreate and maintain comprehensive technical documentation for data pipelines, data dictionaries, and workflows.\nStay informed on healthcare compliance requirements (e.g., HIPAA), and ensure data handling practices follow regulatory standards.\nRequired Skills and Qualifications\n6+ years of experience in SSIS development and data engineering\nProven ability to interact directly with clients and translate business problems into data solutions\nStrong experience with SQL, SSIS, or PySpark for data processing\nDeep understanding of data warehousing concepts and dimensional modeling\nExperience working with healthcare datasets (e.g., claims, eligibility, clinical data)\nFamiliarity with cloud platforms (Azure, AWS, or GCP) and data lakes\nStrong troubleshooting, problem-solving, and performance tuning skills\nExcellent verbal and written communication skills\nBachelor's or Masters degree in Computer Science, Engineering, Information Systems, or a related field\nPreferred Qualifications\nProficiency in building data pipelines using tools such as Azure Data Factory, Informatica, Databricks, or equivalent\nExperience with FHIR, HL7, or other healthcare data standards\nFamiliarity with HIPAA and healthcare compliance requirements\nKnowledge of reporting tools like Power BI or Tableau\nExposure to CI/CD and data pipeline automation\nWhy Join Us?\nWork on high-impact healthcare projects with meaningful outcomes\nEngage directly with clients and make a tangible difference in their data strategy\nCollaborative team culture and continuous learning opportunities\nFlexible work arrangements and competitive compensation\n\nLocation - Bangalore\nShit Timing - 2 Pm to 11 PM\nWork - Hybrid\n\nRegards,\nnaveen.vediyappan@sagility.com",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SSIS', 'SQL', 'ETL']",2025-06-12 05:42:53
SR. Databricks Developer,Labcorp,7 - 12 years,Not Disclosed,['Bengaluru'],"Labcorp is hiring a Senior Data engineer.  This person will be an integrated member of Labcorp Data and Analytics team and work within the IT team.   Play a crucial role in designing, developing and maintaining data solutions using Databricks, Fabric, Spark, PySpark and Python.  Responsible to review business requests and translate them into technical solution and technical specification.  In addition, work with team members to mentor fellow developers to grow their knowledge and expertise.  Work in a fast paced and high-volume processing environment, where quality and attention to detail are vital.\n\nRESPONSIBILITIES:\nDesign and implement end-to-end data engineering solutions by leveraging the full suite of Databricks, Fabric tools, including data ingestion, transformation, and modeling.\nDesign, develop and maintain end-to-end data pipelines by using spark, ensuring scalability, reliability, and cost optimized solutions.\nConduct performance tuning and troubleshooting to identify and resolve any issues.\nImplement data governance and security best practices, including role-based access control, encryption, and auditing.\nWork in fast-paced environment and perform effectively in an agile development environment.\n\nREQUIREMENTS:\n8+ years of experience in designing and implementing data solutions with at least 4+ years of experience in data engineering.\nExtensive experience with Databricks, Fabric, including a deep understanding of its architecture, data modeling, and real-time analytics.\nMinimum 6+ years of experience in Spark, PySpark and Python.\nMust have strong experience in SQL, Spark SQL, data modeling & RDBMS concepts.\nStrong knowledge of Data Fabric services, particularly Data engineering, Data warehouse, Data factory, and Real- time intelligence.\nStrong problem-solving skills, with ability to perform multi-tasking.\nFamiliarity with security best practices in cloud environments, Active Directory, encryption, and data privacy compliance.\nCommunicate effectively in both oral and written.\nExperience in AGILE development, SCRUM and Application Lifecycle Management (ALM).\nPreference given to current or former Labcorp employees.\n\nEDUCATION:\nBachelors in engineering, MCA.",Industry Type: Medical Services / Hospital (Diagnostics),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Bricks', 'Python', 'Parquet', 'UDP', 'Shell Scripting', 'Microsoft SQL Server', 'DW BI project', 'Kafka', 'Mapreduce', 'EMR', 'Redshift', 'Hive', 'MySQL', 'Spark', 'Aws Databricks', 'Oracle', 'Redshift spectrum', 'Fabric', 'Lambda', 'Athena']",2025-06-12 05:42:56
Sap Cloud Platform Integration Consultant(lookingfor immediate joiner),Xforia Technologies,3 - 8 years,Not Disclosed,"['Pune', 'Bengaluru']","Role & responsibilities:\nProfessional Experience:\n\n10+ years of extensive experience in SAP integration technologies, with a strong focus on SAP Integration Suite.\nProven experience in architecting and implementing large-scale integration solutions.\nDeep understanding of various integration methodologies, tools, and platforms (e.g., SAP PI/PO, Ariba Managed Gateway for Spend and Network, SAP Cloud Integration for Data Services, Dell Boomi).\nProven track record in leading and mentoring technical teams.\n\nTechnical Skills:\n\nExpertise in SAP Integration Suite, including designing and implementing complex integration scenarios.\nStrong programming skills in Java, Groovy, or other relevant scripting languages.\nProficient in SOAP, REST, OData, and other web service protocols.\nExtensive experience with SAP APIs, BAPIs, IDocs, RFCs, and ALE.\nKnowledge of security and compliance standards, especially as they relate to data integration.\n\nSoft Skills:\nExceptional problem-solving and analytical skills.\nExcellent leadership and project management abilities.sibilities",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['client handling', 'end to end implementation', 'CPI']",2025-06-12 05:42:58
Java Microservice Developer,Oracle,4 - 9 years,Not Disclosed,['Hyderabad'],"Oracle ERP Cloud is the market leader in Financial Management Suites for Midsize, Large, and Global Enterprises. Gartner named Oracle ERP Cloud a leader in their Magic Quadrant, placing highest in the ability to execute and furthest in the completeness of vision. We are hiring for open positions in Fusion ERP Financial cloud applications development organization for senior/lead cloud-native engineers building next-generation cloud-based applications. The role involves building cloud applications with the latest cloud-native technologies. On top of building new applications, we also handled many migration projects, which helped us explore and use an efficient tech stack to gain from our experiences.If you are passionate about knowing how organizations deal with their back-office financial operations and what it takes to build such a world-class financial suite, you would be a perfect match. In this role, one would lead the project(s) by establishing guidelines, implementing good practices, designing scalable solutions, and taking it to delivery.\n\n\nPrimary Skills needed:- Minimum six years of overall experience in analyzing, designing, developing, and troubleshooting Software Applications- Working experience on Java microservices and deploying applications on the cloud (any cloud provider) using docker, Kubernetes- Strong hands-on experience building REST API using Java TechStack- Strong hands-on Java programming skills, understanding and implementation of best practices and design patterns- Good understanding and working experience in security, performance optimization, and monitoring aspects - Experience with large-scale distributed storage and database systems (SQL or NoSQL, e.g., Oracle/Mongo)- Exposure to various operational considerations like scaling considerations, throughput considerations, caching, and data modelling- Good understanding of messaging systems like Kafka - Consistently write valuable tests for all code developed and ensure quality is maintained as development moves forward- Bachelor's or Master's Degree in Computer Science or equivalent\n\nHaving the below skills would be an added advantage:- Functional knowledge and product architecture of financial ERP products- Exposure or working experience with the Helidon microservices framework, Oracle Cloud Infrastructure (OCI) - A good understanding of big data technologies (spark, data flow, etc.) \n\nOther supporting skills:- Infrastructure as code, such as Terraform, Helm, etc.- Linux, Build tools (maven, grunt, gulp, npm, etc.), Git, etc.- Well-versed with application life-cycle management for cloud-based applications- Good exposure to the DevOps/CICD model- Experience in Agile methodologies and Release management techniques- Good communication and interpersonal skills- Ability to work with cross-functional teams across geographies, I.e., Product Management, QA, and Strategy teams based out of various locations",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Cloud', 'J2Ee', 'Spring Boot', 'Microservices']",2025-06-12 05:43:00
o9 SSIS integration Consultant,Thoucentric,3 - 8 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","At Thoucentric, we work on various problem statements.\nThe most popular ones are -\nBuilding capabilities that address a market need, basis our ongoing research efforts\nSolving a specific use case for a current or potential client based on challenges on-ground\nDeveloping new systems that help be a better employer and a better partner to clients",,,,"['O9', 'SSIS', 'SQL']",2025-06-12 05:43:02
Senior Technology Recruiter/ IT Recruiter - Hybrid- Work from Office,IFIN Talent,2 - 7 years,3.5-4 Lacs P.A.,['Hyderabad( Kothaguda )'],"We are looking for an ambitious Lead HR and IT Recruitment expert (2 yr to 7yrs) with experience of working within a360 degrees role with demonstrated examples of accomplishment Have deep understanding of the complete recruiting function, including: research, sourcing, networking, behavior based interviewing and closing.\n\nSource candidates using a variety of search methods to build a robust candidate pipeline\nScreen candidates by reviewing resumes and job applications, and performing phone screenings\nStay abreast of recruiting trends and best practices\nManage the overall interview, selection, and closing process\nAbout Company-\nIFIN Global Group is an Award winning, FORBES, FORTUNE, ETNOW, CNBC featured fastest growing innovative Global IT consulting and Workforce Solutions firm with offices across USA, UK, CANADA, Netherlands, Finland, India backed by stellar team of Subject Matter experts, whos who Advisory leveraging our own proprietary technology platform which we call it the IFIN Way - Inclusiveness, Focus, Innovate and Nurture.\nRecruitment is a really interesting and varied role, where you'll always be learning new skills - its a perfect move if you're looking for a job with great financial rewards, which gets you out of bed excited to start each day and what better place to do this than the fastest growing innovative Global IT consulting and Workforce Solutions.\n(https://ifinglobalgroup.com)\n\n(https://www.linkedin.com/company/ifinglobalgroup/)\n\nJob Mode- Both Hybrid (3 days min wfo)\n\nJob summary:\nThe purpose of this role is to manage the complete recruitment life-cycle for a broad range of critical positions. Through targeted headhunting and proactive talent pool management, you will play a vital role in increasing our talent sourcing capabilities and helping our dynamic and purpose-driven organization to take the next big step in our growth journey.\nMain tasks & responsibilities:\n• Drive the end to end recruitment life cycle to ensure timely delivery on vacancies with high quality and ethical standards\n• Build talent pools of experienced candidates.\n• Proactive Sourcing and networking of sustainability and climate change talent.\n• Maintain and update our Applicant Tracking System\n• Maintain and enhance our company profile on social media\n• Recruitment operations - posting vacancies, scheduling interviews, technical tests, etc\n• Act as a company ambassador, attracting the best possible talent\n•\nRequirements:\nEssential -\n• University degree in HR, Business Administration, sustainability, or equivalent\n• 2- 7 years experience of relevant experience in recruitment agency or RPO model hiring.\n• Proactive sourcing expertise for a wide range of positions using all social media channels.\n• Expertise in sourcing for a range of positions within the Java Fullstack, C++, .net, Python, Data science, Analytics, AIML, Big Data, Business Intelligence , SDET, SRE, DEVOPS, Cloud Engineers\n• Excellent communication skills as well as a strong attention to detail\n• A people person with the ability to build relationships with ease\n• Curious, inquisitive and comfortable with changing circumstances\n• Strong time management and organisational planning\n• Ability to work in an interdisciplinary, global team as well as independently\n• Eagerness to grow and learn in a young, dynamic and fast evolving team\n•\nBenefits & Rewards:\nyoull receive a competitive salary and excellent commission structure\nFlexible working hours and Work from home\nOn the Job Trainings/Webinars",Industry Type: IT Services & Consulting,Department: Human Resources,"Employment Type: Full Time, Permanent","['Contract Staffing', 'IT Recruitment', 'End To End Recruitment', 'sourcing', 'Permanent Staffing', 'Technical Hiring', 'IT Staffing', 'Domestic Staffing', 'It Hiring', 'Technical Recruitment']",2025-06-12 05:43:04
